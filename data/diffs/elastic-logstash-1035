diff --git a/Makefile b/Makefile
index 336d3c51b42..41dbc471800 100644
--- a/Makefile
+++ b/Makefile
@@ -2,8 +2,8 @@
 #   rsync
 #   wget or curl
 #
-JRUBY_VERSION=1.7.9
-ELASTICSEARCH_VERSION=0.90.9
+JRUBY_VERSION=1.7.10
+ELASTICSEARCH_VERSION=0.90.11
 
 WITH_JRUBY=java -jar $(shell pwd)/$(JRUBY) -S
 JRUBY=vendor/jar/jruby-complete-$(JRUBY_VERSION).jar
@@ -41,7 +41,7 @@ default:
 	@echo "  flatjar -- builds the flatjar jar"
 	@echo "  flatjar-test -- runs the test suite against the flatjar"
 
-TESTS=$(wildcard spec/inputs/gelf.rb spec/support/*.rb spec/filters/*.rb spec/examples/*.rb spec/codecs/*.rb spec/conditionals/*.rb spec/event.rb spec/jar.rb)
+TESTS=$(wildcard spec/inputs/file.rb spec/inputs/gelf.rb spec/inputs/imap.rb spec/support/*.rb spec/filters/*.rb spec/examples/*.rb spec/codecs/*.rb spec/conditionals/*.rb spec/event.rb spec/jar.rb)
 
 # The 'version' is generated based on the logstash version, git revision, etc.
 .VERSION.mk: REVISION=$(shell git rev-parse --short HEAD | tr -d ' ')
diff --git a/lib/logstash/codecs/compress_spooler.rb b/lib/logstash/codecs/compress_spooler.rb
deleted file mode 100644
index 1e4f17bead8..00000000000
--- a/lib/logstash/codecs/compress_spooler.rb
+++ /dev/null
@@ -1,50 +0,0 @@
-# encoding: utf-8
-require "logstash/codecs/base"
-
-class LogStash::Codecs::CompressSpooler < LogStash::Codecs::Base
-  config_name 'compress_spooler'
-  milestone 1
-  config :spool_size, :validate => :number, :default => 50
-  config :compress_level, :validate => :number, :default => 6
-
-  public
-  def register
-    require "msgpack"
-    require "zlib"
-    @buffer = []
-  end
-
-  public
-  def decode(data)
-    z = Zlib::Inflate.new
-    data = MessagePack.unpack(z.inflate(data))
-    z.finish
-    z.close
-    data.each do |event|
-      event = LogStash::Event.new(event)
-      event["@timestamp"] = Time.at(event["@timestamp"]).utc if event["@timestamp"].is_a? Float
-      yield event
-    end
-  end # def decode
-
-  public
-  def encode(data)
-    if @buffer.length >= @spool_size
-      z = Zlib::Deflate.new(@compress_level)
-      @on_event.call z.deflate(MessagePack.pack(@buffer), Zlib::FINISH)
-      z.close
-      @buffer.clear
-    else
-      data["@timestamp"] = data["@timestamp"].to_f
-      @buffer << data.to_hash
-    end
-  end # def encode
-
-  public
-  def teardown
-    if !@buffer.nil? and @buffer.length > 0
-      @on_event.call @buffer
-    end
-    @buffer.clear
-  end
-end # class LogStash::Codecs::CompressSpooler
diff --git a/lib/logstash/filters/advisor.rb b/lib/logstash/filters/advisor.rb
deleted file mode 100644
index 46f4d9e9b26..00000000000
--- a/lib/logstash/filters/advisor.rb
+++ /dev/null
@@ -1,178 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# INFORMATION:
-# The filter Advisor is designed for capture and confrontation the events. 
-# The events must be grep by a filter first, then it can pull out a copy of it, like clone, whit tags "advisor_first",
-# this copy is the first occurrence of this event verified in time_adv.
-# After time_adv Advisor will pull out an event tagged "advisor_info" who will tell you the number of same events verified in time_adv.
-
-# INFORMATION ABOUT CLASS:
- 
-# For do this job, i used a thread that will sleep time adv. I assume that events coming on advisor are tagged, then i use an array for storing different events.
-# If an events is not present on array, then is the first and if the option is activate then  advisor push out a copy of event.
-# Else if the event is present on array, then is another same event and not the first, let's count it.  
-
-# USAGE:
-
-# This is an example of logstash config:
-
-# filter{
-#  advisor {
-#     time_adv => 1                     #(optional)
-#     send_first => true                #(optional)
-#  }
-# }
-
-# We analize this:
-
-# time_adv => 1
-# Means the time when the events matched and collected are pushed on outputs with tag "advisor_info".
-
-# send_first => true
-# Means you can push out the first events different who came in advisor like clone copy and tagged with "advisor_first"
-
-class LogStash::Filters::Advisor < LogStash::Filters::Base
-
- config_name "advisor"
- milestone 1
-
- # If you do not set time_adv the plugin does nothing.
- config :time_adv, :validate => :number, :default => 0
- 
- # If you want the first different event will be pushed out like a copy
- config :send_first, :validate => :boolean, :default => true
- 
- public
- def register
-
-  # Control the correct config
-  if (!(@time_adv == 0))
-    
-    @flag = false
-    @first = false
-    # Is used for store the different events.
-    @sarray = Array.new
-    # Is used for count the number of equals events.
-    @carray = Array.new
-
-    @thread = time_alert(@time_adv.to_i*60) do
-     # if collected any events then pushed out a new event after time_adv
-     if (@sarray.size !=0) 
-        @flag = true
-     end
-    end
-  
-  else
-   @logger.warn("Advisor: you have not specified Time_adv. This filter will do nothing!")
-  end
-
- end
- 
- # This method is used to manage sleep and awaken threads (thanks StackOverflow for the support)
-  def time_alert(interval)
-     Thread.new do
-      loop do
-       start_time = Time.now
-       yield
-       elapsed = Time.now - start_time
-       sleep([interval - elapsed, 0].max)
-     end
-   end
-  end
-
- public
- def filter(event)
-  return unless filter?(event)
-  
-  # Control the correct config
-  if(!(@time_adv == 0))
-
-    new_event = true
-    @message = event["message"]
-    
-    # control if the events are new or they are came before
-    for i in (0..@sarray.size-1)
-      if (@message == @sarray[i].to_s)
-        @logger.debug("Avisor: Event match")
-        # if came before then count it
-        new_event = false
-        @carray[i] = @carray[i].to_i+1
-        @logger.debug("Advisor: "+@carray[i].to_s+" Events matched")
-        break
-      end
-    end
-     
-    if (new_event == true)
-       # else is a new event
-
-       @sarray << @message
-       @carray << 1
-       if (send_first == true)
-           @logger.debug("Advisor: is the first to send out")
-           @first = true
-       end
-    end
-     
-  else
-   @logger.warn("Advisor: you have not specified Time_adv. This filter will do nothing!")
-  end
- end
-
-
-  # This method is used for generate events every 5 seconds (Thanks Jordan Sissel for explanation).
-  # In this case we generate an event when advisor thread trigger the flag or is the first different event. 
-
-  def flush
-      
-        if (@first == true)
-          event = LogStash::Event.new
-          event["host"] = Socket.gethostname
-          event["message"] = @message
-          event.tag "advisor_first"
-          filter_matched(event)
-         
-          @first = false
-          return [event]
-        end
-   
-         if (@flag == true)
- 
-          if (@tags.size != 0)
-            @tag_path = ""
-            for i in (0..@tags.size-1)
-              @tag_path += @tags[i].to_s+"."
-            end
-          end
-            
-          # Prepare message 
-          message = "Advisor: Found events who match: "+@tag_path.to_s+"\n\n"
-
-          # See on messagge partial part of different events
-          for i in (0..@sarray.size-1)
-            message = message+@carray[i].to_s+" events like: "+(@sarray[i].to_s).slice(0, 300)+"\n\n"
-          end
-         
-          event = LogStash::Event.new
-          event["host"] = Socket.gethostname 
-          event["message"] = message  
-          event.tag << "advisor_info"
-          filter_matched(event)
-   
-          # reset flag and counter 
-          @flag = false
-          @carray = nil
-          @sarray = nil
-          @carray = Array.new
-          @sarray = Array.new
-
-          # push the event
-          return [event]
-         end
-    return
- 
-  end
-
-end
-# By Bistic:)
diff --git a/lib/logstash/filters/alter.rb b/lib/logstash/filters/alter.rb
deleted file mode 100644
index 1f9a478ccf3..00000000000
--- a/lib/logstash/filters/alter.rb
+++ /dev/null
@@ -1,173 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# The alter filter allows you to do general alterations to fields 
-# that are not included in the normal mutate filter. 
-#
-#
-# NOTE: The functionality provided by this plugin is likely to
-# be merged into the 'mutate' filter in future versions.
-class LogStash::Filters::Alter < LogStash::Filters::Base
-  config_name "alter"
-  milestone 1
-  
-  # Change the content of the field to the specified value
-  # if the actual content is equal to the expected one.
-  #
-  # Example:
-  #
-  #     filter {
-  #       alter {
-  #         condrewrite => [ 
-  #              "field_name", "expected_value", "new_value" 
-  #              "field_name2", "expected_value2, "new_value2"
-  #              ....
-  #            ]
-  #       }
-  #     }
-  config :condrewrite, :validate => :array
-  
-  # Change the content of the field to the specified value
-  # if the content of another field is equal to the expected one.
-  #
-  # Example:
-  #
-  #     filter {
-  #       alter {
-  #         condrewriteother => [ 
-  #              "field_name", "expected_value", "field_name_to_change", "value",
-  #              "field_name2", "expected_value2, "field_name_to_change2", "value2",
-  #              ....
-  #         ]
-  #       }
-  #     }
-  config :condrewriteother, :validate => :array
-  
-  # Sets the value of field_name to the first nonnull expression among its arguments.
-  #
-  # Example:
-  #
-  #     filter {
-  #       alter {
-  #         coalesce => [
-  #              "field_name", "value1", "value2", "value3", ...
-  #         ]
-  #       }
-  #     }
-  config :coalesce, :validate => :array
-  
-  public
-  def register 
-    @condrewrite_parsed = []
-    @condrewrite.nil? or @condrewrite.each_slice(3) do |field, expected, replacement|
-      if [field, expected, replacement].any? {|n| n.nil?}
-        @logger.error("Invalid condrewrte configuration. condrewrite has to define 3 elements per config entry", :field => field, :expected => expected, :replacement => replacement)
-        raise "Bad configuration, aborting."
-      end
-      @condrewrite_parsed << {
-        :field        => field,
-        :expected       => expected,
-        :replacement  => replacement
-      }
-    end # condrewrite
-    
-    @condrewriteother_parsed = []
-    @condrewriteother.nil? or @condrewriteother.each_slice(4) do |field, expected, replacement_field, replacement_value|
-      if [field, expected, replacement_field, replacement_value].any? {|n| n.nil?}
-        @logger.error("Invalid condrewrteother configuration. condrewriteother has to define 4 elements per config entry", :field => field, :expected => expected, :replacement_field => replacement_field, :replacement_value => replacement_value)
-        raise "Bad configuration, aborting."
-      end
-      @condrewriteother_parsed << {
-        :field        => field,
-        :expected       => expected,
-        :replacement_field  => replacement_field,
-        :replacement_value => replacement_value
-      }
-    end # condrewriteother
-    
-    @coalesce_parsed = []
-    @coalesce.nil? or if not @coalesce.is_a?(Array) or @coalesce.length < 2
-      @logger.error("Invalid coalesce configuration. coalesce has to define one Array of at least 2 elements")
-      raise "Bad configuration, aborting."
-    else
-      @coalesce_parsed << {
-        :field  => @coalesce.slice!(0),
-        :subst_array => @coalesce
-      }
-    end
-    
-       
-  end # def register
-  
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    condrewrite(event) if @condrewrite
-    condrewriteother(event) if @condrewriteother
-    coalesce(event) if @coalesce
-
-    filter_matched(event)
-  end # def filter
-  
-  private
-  def condrewrite(event)
-    @condrewrite_parsed.each do |config|
-      field = config[:field]
-      expected = config[:expected]
-      replacement = config[:replacement]
-
-      if event[field].is_a?(Array)
-        event[field] = event[field].map do |v|
-          if v == event.sprintf(expected)
-            v = event.sprintf(replacement)
-          else
-            v
-          end
-        end
-      else
-        if event[field] == event.sprintf(expected)
-          event[field] = event.sprintf(replacement)
-        end
-      end
-    end # @condrewrite_parsed.each
-  end # def condrewrite
-  
-  private
-  def condrewriteother(event)
-    @condrewriteother_parsed.each do |config|
-      field = config[:field]
-      expected = config[:expected]
-      replacement_field = config[:replacement_field]
-      replacement_value = config[:replacement_value]
-
-      if event[field].is_a?(Array)
-        event[field].each do |v|
-          if v == event.sprintf(expected)
-            event[replacement_field] = event.sprintf(replacement_value)
-          end
-        end
-      else
-        if event[field] == event.sprintf(expected)
-          event[replacement_field] = event.sprintf(replacement_value)
-        end
-      end
-    end # @condrewriteother_parsed.each
-  end # def condrewriteother
-  
-  private
-  def coalesce(event)
-    @coalesce_parsed.each do |config|
-      field = config[:field]
-      subst_array = config[:subst_array]
-      
-      substitution_parsed = subst_array.map { |x| event.sprintf(x) }
-      not_nul_index = substitution_parsed.find_index { |x| not x.nil? and not x.eql?("nil") and not (not x.index("%").nil? && x.match(/%\{[^}]\}/).nil?) }
-      if not not_nul_index.nil?
-        event[field] = substitution_parsed[not_nul_index]
-      end
-    end # @coalesce_parsed.each
-  end # def coalesce
-  
-end # class LogStash::Filters::Alter
diff --git a/lib/logstash/filters/cidr.rb b/lib/logstash/filters/cidr.rb
deleted file mode 100644
index 4a4ad488497..00000000000
--- a/lib/logstash/filters/cidr.rb
+++ /dev/null
@@ -1,76 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-require "ipaddr"
-
-# The CIDR filter is for checking IP addresses in events against a list of
-# network blocks that might contain it. Multiple addresses can be checked
-# against multiple networks, any match succeeds. Upon success additional tags
-# and/or fields can be added to the event.
-
-class LogStash::Filters::CIDR < LogStash::Filters::Base
-
-  config_name "cidr"
-  milestone 1
-
-  # The IP address(es) to check with. Example:
-  #
-  #     filter {
-  #       %PLUGIN% {
-  #         add_tag => [ "testnet" ]
-  #         address => [ "%{src_ip}", "%{dst_ip}" ]
-  #         network => [ "192.0.2.0/24" ]
-  #       }
-  #     }
-  config :address, :validate => :array, :default => []
-
-  # The IP network(s) to check against. Example:
-  #
-  #     filter {
-  #       %PLUGIN% {
-  #         add_tag => [ "linklocal" ]
-  #         address => [ "%{clientip}" ]
-  #         network => [ "169.254.0.0/16", "fe80::/64" ]
-  #       }
-  #     }
-  config :network, :validate => :array, :default => []
-
-  public
-  def register
-    # Nothing
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    address = @address.collect do |a|
-      begin
-        IPAddr.new(event.sprintf(a))
-      rescue ArgumentError => e
-        @logger.warn("Invalid IP address, skipping", :address => a, :event => event)
-        nil
-      end
-    end
-    address.compact!
-
-    network = @network.collect do |n|
-      begin
-        IPAddr.new(event.sprintf(n))
-      rescue ArgumentError => e
-        @logger.warn("Invalid IP network, skipping", :network => n, :event => event)
-        nil
-      end
-    end
-    network.compact!
-
-    # Try every combination of address and network, first match wins
-    address.product(network).each do |a, n|
-      @logger.debug("Checking IP inclusion", :address => a, :network => n)
-      if n.include?(a)
-        filter_matched(event)
-        return
-      end
-    end
-  end # def filter
-end # class LogStash::Filters::CIDR
diff --git a/lib/logstash/filters/cipher.rb b/lib/logstash/filters/cipher.rb
deleted file mode 100644
index d9e55b4341c..00000000000
--- a/lib/logstash/filters/cipher.rb
+++ /dev/null
@@ -1,145 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# This filter parses a source and apply a cipher or decipher before
-# storing it in the target.
-#
-class LogStash::Filters::Cipher < LogStash::Filters::Base
-  config_name "cipher"
-  milestone 1
-
-  # The field to perform filter
-  #
-  # Example, to use the @message field (default) :
-  #
-  #     filter { cipher { source => "message" } }
-  config :source, :validate => :string, :default => "message"
-
-  # The name of the container to put the result
-  #
-  # Example, to place the result into crypt :
-  #
-  #     filter { cipher { target => "crypt" } }
-  config :target, :validate => :string, :default => "message"
-
-  # Do we have to perform a base64 decode or encode?
-  #
-  # If we are decrypting, base64 decode will be done before.
-  # If we are encrypting, base64 will be done after.
-  #
-  config :base64, :validate => :boolean, :default => true
-
-  # The key to use
-  config :key, :validate => :string
-
-  # The key size to pad
-  #
-  # It depends of the cipher algorythm.I your key don't need
-  # padding, don't set this parameter
-  #
-  # Example, for AES-256, we must have 32 char long key
-  #     filter { cipher { key_size => 32 }
-  #
-  config :key_size, :validate => :number, :default => 32
-
-  # The character used to pad the key
-  config :key_pad, :default => "\0"
-
-  # The cipher algorythm
-  #
-  # A list of supported algorithms can be obtained by
-  #
-  #     puts OpenSSL::Cipher.ciphers
-  config :algorithm, :validate => :string, :required => true
-
-  # Encrypting or decrypting some data
-  #
-  # Valid values are encrypt or decrypt
-  config :mode, :validate => :string, :required => true
-
-  # Cypher padding to use. Enables or disables padding. 
-  #
-  # By default encryption operations are padded using standard block padding 
-  # and the padding is checked and removed when decrypting. If the pad 
-  # parameter is zero then no padding is performed, the total amount of data 
-  # encrypted or decrypted must then be a multiple of the block size or an 
-  # error will occur.
-  #
-  # See EVP_CIPHER_CTX_set_padding for further information.
-  #
-  # We are using Openssl jRuby which uses default padding to PKCS5Padding
-  # If you want to change it, set this parameter. If you want to disable
-  # it, Set this parameter to 0
-  #     filter { cipher { padding => 0 }}
-  config :cipher_padding, :validate => :string
-
-  # The initialization vector to use
-  #
-  # The cipher modes CBC, CFB, OFB and CTR all need an "initialization
-  # vector", or short, IV. ECB mode is the only mode that does not require
-  # an IV, but there is almost no legitimate use case for this mode
-  # because of the fact that it does not sufficiently hide plaintext patterns.
-  config :iv, :validate => :string
-
-  def register
-    require 'base64' if @base64
-    init_cipher
-  end # def register
-
-
-  def filter(event)
-    return unless filter?(event)
-
-
-    #If decrypt or encrypt fails, we keep it it intact.
-    begin
-      #@logger.debug("Event to filter", :event => event)
-      data = event[@source]
-      if @mode == "decrypt"
-        data =  Base64.decode64(data) if @base64 == true
-      end
-      result = @cipher.update(data) + @cipher.final
-      if @mode == "encrypt"
-        data =  Base64.encode64(data) if @base64 == true
-      end
-    rescue => e
-      @logger.warn("Exception catch on cipher filter", :event => event, :error => e)
-    else
-      event[@target]= result
-      #Is it necessary to add 'if !result.nil?' ? exception have been already catched.
-      #In doubt, I keep it.
-      filter_matched(event) if !result.nil?
-      #Too much bad result can be a problem, reinit cipher prevent this.
-      init_cipher
-    end
-  end # def filter
-
-  def init_cipher
-
-    @cipher = OpenSSL::Cipher.new(@algorithm)
-    if @mode == "encrypt"
-      @cipher.encrypt
-    elsif @mode == "decrypt"
-      @cipher.decrypt
-    else
-      @logger.error("Invalid cipher mode. Valid values are \"encrypt\" or \"decrypt\"", :mode => @mode)
-      raise "Bad configuration, aborting."
-    end
-
-    if @key.length != @key_size
-      @logger.debug("key length is " + @key.length.to_s + ", padding it to " + @key_size.to_s + " with '" + @key_pad.to_s + "'")
-      @key = @key[0,@key_size].ljust(@key_size,@key_pad)
-    end
-
-    @cipher.key = @key
-
-    @cipher.iv = @iv if @iv
-
-    @cipher.padding = @cipher_padding if @cipher_padding
-
-    @logger.debug("Cipher initialisation done", :mode => @mode, :key => @key, :iv => @iv, :cipher_padding => @cipher_padding)
-  end # def init_cipher
-
-
-end # class LogStash::Filters::Cipher
diff --git a/lib/logstash/filters/collate.rb b/lib/logstash/filters/collate.rb
deleted file mode 100644
index 30afe0398a5..00000000000
--- a/lib/logstash/filters/collate.rb
+++ /dev/null
@@ -1,114 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# Collate events by time or count.
-#
-# The original goal of this filter was to merge the logs from different sources
-# by the time of log, for example, in real-time log collection, logs can be
-# collated by amount of 3000 logs or can be collated in 30 seconds.
-#
-# The config looks like this:
-#
-#     filter {
-#       collate {
-#         size => 3000
-#         interval => "30s"
-#         order => "ascending"
-#       }
-#     }
-class LogStash::Filters::Collate < LogStash::Filters::Base
-
-  config_name "collate"
-  milestone 1
-
-  # How many logs should be collated.
-  config :count, :validate => :number, :default => 1000
-
-  # The 'interval' is the time window which how long the logs should be collated. (default 1m)
-  config :interval, :validate => :string, :default => "1m"
-
-  # The 'order' collated events should appear in.
-  config :order, :validate => ["ascending", "descending"], :default => "ascending"
-
-  public
-  def register
-    require "thread"
-    require "rufus/scheduler"
-
-    @mutex = Mutex.new
-    @collatingDone = false
-    @collatingArray = Array.new
-    @scheduler = Rufus::Scheduler.start_new
-    @job = @scheduler.every @interval do
-      @logger.info("Scheduler Activated")
-      @mutex.synchronize{
-        collate
-      }
-    end
-  end # def register
-
-  public
-  def filter(event)
-    @logger.info("do collate filter")
-    if event == LogStash::SHUTDOWN
-      @job.trigger()
-      @job.unschedule()
-      @logger.info("collate filter thread shutdown.")
-      return
-    end
-
-    # if the event is collated, a "collated" tag will be marked, so for those uncollated event, cancel them first.
-    if event["tags"].nil? || !event.tags.include?("collated")
-      event.cancel
-    else
-      return
-    end
-
-    @mutex.synchronize{
-      @collatingArray.push(event.clone)
-
-      if (@collatingArray.length == @count)
-        collate
-      end
-
-      if (@collatingDone)
-        while collatedEvent = @collatingArray.pop
-          collatedEvent["tags"] = Array.new if collatedEvent["tags"].nil?
-          collatedEvent["tags"] << "collated"
-          filter_matched(collatedEvent)
-          yield collatedEvent
-        end # while @collatingArray.pop
-        # reset collatingDone flag
-        @collatingDone = false
-      end
-    }
-  end # def filter
-
-  private
-  def collate
-    if (@order == "ascending")
-      @collatingArray.sort! { |eventA, eventB| eventB.timestamp <=> eventA.timestamp }
-    else 
-      @collatingArray.sort! { |eventA, eventB| eventA.timestamp <=> eventB.timestamp }
-    end
-    @collatingDone = true
-  end # def collate
-
-  # Flush any pending messages.
-  public
-  def flush
-    events = []
-    if (@collatingDone)
-      @mutex.synchronize{
-        while collatedEvent = @collatingArray.pop
-          collatedEvent["tags"] << "collated"
-          events << collatedEvent
-        end # while @collatingArray.pop
-      }
-      # reset collatingDone flag.
-      @collatingDone = false
-    end
-    return events
-  end # def flush
-end #
diff --git a/lib/logstash/filters/elapsed.rb b/lib/logstash/filters/elapsed.rb
deleted file mode 100644
index 64881f5b95e..00000000000
--- a/lib/logstash/filters/elapsed.rb
+++ /dev/null
@@ -1,256 +0,0 @@
-# elapsed filter
-#
-# This filter tracks a pair of start/end events and calculates the elapsed
-# time between them.
-
-require "logstash/filters/base"
-require "logstash/namespace"
-require 'thread'
-
-# The elapsed filter tracks a pair of start/end events and uses their
-# timestamps to calculate the elapsed time between them.
-#
-# The filter has been developed to track the execution time of processes and
-# other long tasks.
-#
-# The configuration looks like this:
-#
-#     filter {
-#       elapsed {
-#         start_tag => "start event tag"
-#         end_tag => "end event tag"
-#         unique_id_field => "id field name"
-#         timeout => seconds
-#         new_event_on_match => true/false
-#       }
-#     }
-#
-# The events managed by this filter must have some particular properties.
-# The event describing the start of the task (the "start event") must contain
-# a tag equal to 'start_tag'. On the other side, the event describing the end
-# of the task (the "end event") must contain a tag equal to 'end_tag'. Both
-# these two kinds of event need to own an ID field which identify uniquely that
-# particular task. The name of this field is stored in 'unique_id_field'.
-#
-# You can use a Grok filter to prepare the events for the elapsed filter.
-# An example of configuration can be:
-#
-#     filter {
-#       grok {
-#         match => ["message", "%{TIMESTAMP_ISO8601} START id: (?<task_id>.*)"]
-#         add_tag => [ "taskStarted" ]
-#       }
-#
-#       grok {
-#         match => ["message", "%{TIMESTAMP_ISO8601} END id: (?<task_id>.*)"]
-#         add_tag => [ "taskTerminated"]
-#       }
-#
-#       elapsed {
-#         start_tag => "taskStarted"
-#         end_tag => "taskTerminated"
-#         unique_id_field => "task_id"
-#       }
-#     }
-#
-# The elapsed filter collects all the "start events". If two, or more, "start
-# events" have the same ID, only the first one is recorded, the others are
-# discarded.
-#
-# When an "end event" matching a previously collected "start event" is
-# received, there is a match. The configuration property 'new_event_on_match'
-# tells where to insert the elapsed information: they can be added to the
-# "end event" or a new "match event" can be created. Both events store the
-# following information:
-# - the tags "elapsed" and "elapsed.match"
-# - the field "elapsed.time" with the difference, in seconds, between
-#   the two events timestamps
-# - an ID filed with the task ID
-# - the field "elapsed.timestamp_start" with the timestamp of the "start event"
-#
-# If the "end event" does not arrive before "timeout" seconds, the
-# "start event" is discarded and an "expired event" is generated. This event
-# contains:
-# - the tags "elapsed" and "elapsed.expired_error"
-# - a field called "elapsed.time" with the age, in seconds, of the
-#   "start event"
-# - an ID filed with the task ID
-# - the field "elapsed.timestamp_start" with the timestamp of the "start event"
-#
-class LogStash::Filters::Elapsed < LogStash::Filters::Base
-  PREFIX = "elapsed."
-  ELAPSED_FIELD = PREFIX + "time"
-  TIMESTAMP_START_EVENT_FIELD = PREFIX + "timestamp_start"
-  HOST_FIELD = "host"
-
-  ELAPSED_TAG = "elapsed"
-  EXPIRED_ERROR_TAG = PREFIX + "expired_error"
-  END_WITHOUT_START_TAG = PREFIX + "end_wtihout_start"
-  MATCH_TAG = PREFIX + "match"
-
-  config_name "elapsed"
-  milestone 1
-
-  # The name of the tag identifying the "start event"
-  config :start_tag, :validate => :string, :required => true
-
-  # The name of the tag identifying the "end event"
-  config :end_tag, :validate => :string, :required => true
-
-  # The name of the field containing the task ID.
-  # This value must uniquely identify the task in the system, otherwise
-  # it's impossible to match the couple of events.
-  config :unique_id_field, :validate => :string, :required => true
-
-  # The amount of seconds after an "end event" can be considered lost.
-  # The corresponding "start event" is discarded and an "expired event"
-  # is generated. The default value is 30 minutes (1800 seconds).
-  config :timeout, :validate => :number, :required => false, :default => 1800
-
-  # This property manage what to do when an "end event" matches a "start event".
-  # If it's set to 'false' (default value), the elapsed information are added
-  # to the "end event"; if it's set to 'true' a new "match event" is created.
-  config :new_event_on_match, :validate => :boolean, :required => false, :default => false
-
-  public
-  def register
-    @mutex = Mutex.new
-    # This is the state of the filter. The keys are the "unique_id_field",
-    # the values are couples of values: <start event, age>
-    @start_events = {}
-
-    @logger.info("Elapsed, timeout: #{@timeout} seconds")
-  end
-
-  # Getter method used for the tests
-  def start_events
-    @start_events
-  end
-
-  def filter(event)
-    return unless filter?(event)
-
-    unique_id = event[@unique_id_field]
-    return if unique_id.nil?
-
-    if(start_event?(event))
-      filter_matched(event)
-      @logger.info("Elapsed, 'start event' received", start_tag: @start_tag, unique_id_field: @unique_id_field)
-
-      @mutex.synchronize do
-        unless(@start_events.has_key?(unique_id))
-          @start_events[unique_id] = LogStash::Filters::Elapsed::Element.new(event)
-        end
-      end
-
-    elsif(end_event?(event))
-      filter_matched(event)
-      @logger.info("Elapsed, 'end event' received", end_tag: @end_tag, unique_id_field: @unique_id_field)
-
-      @mutex.lock
-      if(@start_events.has_key?(unique_id))
-        start_event = @start_events.delete(unique_id).event
-        @mutex.unlock
-        elapsed = event["@timestamp"] - start_event["@timestamp"]
-        if(@new_event_on_match)
-          elapsed_event = new_elapsed_event(elapsed, unique_id, start_event["@timestamp"])
-          filter_matched(elapsed_event)
-          yield elapsed_event if block_given?
-        else
-          return add_elapsed_info(event, elapsed, unique_id, start_event["@timestamp"])
-        end
-      else
-        @mutex.unlock
-        # The "start event" did not arrive.
-        event.tag(END_WITHOUT_START_TAG)
-      end
-    end
-  end # def filter
-
-  # The method is invoked by LogStash every 5 seconds.
-  def flush()
-    expired_elements = []
-
-    @mutex.synchronize do
-      increment_age_by(5)
-      expired_elements = remove_expired_elements()
-    end
-
-    return create_expired_events_from(expired_elements)
-  end
-
-  private
-  def increment_age_by(seconds)
-    @start_events.each_pair do |key, element|
-      element.age += seconds
-    end
-  end
-
-  # Remove the expired "start events" from the internal
-  # buffer and return them.
-  def remove_expired_elements()
-    expired = []
-    @start_events.delete_if do |key, element|
-      if(element.age >= @timeout)
-        expired << element
-        next true
-      end
-      next false
-    end
-
-    return expired
-  end
-
-  def create_expired_events_from(expired_elements)
-    events = []
-    expired_elements.each do |element|
-      error_event = LogStash::Event.new
-      error_event.tag(ELAPSED_TAG)
-      error_event.tag(EXPIRED_ERROR_TAG)
-
-      error_event[HOST_FIELD] = Socket.gethostname
-      error_event[@unique_id_field] = element.event[@unique_id_field]
-      error_event[ELAPSED_FIELD] = element.age
-      error_event[TIMESTAMP_START_EVENT_FIELD] = element.event["@timestamp"]
-
-      events << error_event
-      filter_matched(error_event)
-    end
-
-    return events
-  end
-
-  def start_event?(event)
-    return (event["tags"] != nil && event["tags"].include?(@start_tag))
-  end
-
-  def end_event?(event)
-    return (event["tags"] != nil && event["tags"].include?(@end_tag))
-  end
-
-  def new_elapsed_event(elapsed_time, unique_id, timestamp_start_event)
-      new_event = LogStash::Event.new
-      new_event[HOST_FIELD] = Socket.gethostname
-      return add_elapsed_info(new_event, elapsed_time, unique_id, timestamp_start_event)
-  end
-
-  def add_elapsed_info(event, elapsed_time, unique_id, timestamp_start_event)
-      event.tag(ELAPSED_TAG)
-      event.tag(MATCH_TAG)
-
-      event[ELAPSED_FIELD] = elapsed_time
-      event[@unique_id_field] = unique_id
-      event[TIMESTAMP_START_EVENT_FIELD] = timestamp_start_event
-
-      return event
-  end
-end # class LogStash::Filters::Elapsed
-
-class LogStash::Filters::Elapsed::Element
-  attr_accessor :event, :age
-
-  def initialize(event)
-    @event = event
-    @age = 0
-  end
-end
diff --git a/lib/logstash/filters/elasticsearch.rb b/lib/logstash/filters/elasticsearch.rb
deleted file mode 100644
index 65d16660957..00000000000
--- a/lib/logstash/filters/elasticsearch.rb
+++ /dev/null
@@ -1,73 +0,0 @@
-require "logstash/filters/base"
-require "logstash/namespace"
-require "logstash/util/fieldreference"
-
-# Search elasticsearch for a previous log event and copy some fields from it
-# into the current event.  Below is a complete example of how this filter might
-# be used.  Whenever logstash receives an "end" event, it uses this elasticsearch
-# filter to find the matching "start" event based on some operation identifier.
-# Then it copies the @timestamp field from the "start" event into a new field on
-# the "end" event.  Finally, using a combination of the "date" filter and the
-# "ruby" filter, we calculate the time duration in hours between the two events.
-#
-#       if [type] == "end" {
-#          elasticsearch {
-#             hosts => ["es-server"]
-#             query => "type:start AND operation:%{[opid]}"
-#             fields => ["@timestamp", "started"]
-#          }
-#
-#          date {
-#             match => ["[started]", "ISO8601"]
-#             target => "[started]"
-#          }
-#
-#          ruby {
-#             code => "event['duration_hrs'] = (event['@timestamp'] - event['started']) / 3600 rescue nil"
-#          }
-#       }
-#
-class LogStash::Filters::Elasticsearch < LogStash::Filters::Base
-  config_name "elasticsearch"
-  milestone 1
-
-  # List of elasticsearch hosts to use for querying.
-  config :hosts, :validate => :array
-
-  # Elasticsearch query string
-  config :query, :validate => :string
-
-  # Comma-delimited list of <field>:<direction> pairs that define the sort order
-  config :sort, :validate => :string, :default => "@timestamp:desc"
-
-  # Hash of fields to copy from old event (found via elasticsearch) into new event
-  config :fields, :validate => :hash, :default => {}
-
-  public
-  def register
-    require "elasticsearch"
-
-    @logger.info("New ElasticSearch filter", :hosts => @hosts)
-    @client = Elasticsearch::Client.new hosts: @hosts
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    begin
-      query_str = event.sprintf(@query)
-
-      results = @client.search q: query_str, sort: @sort, size: 1
-
-      @fields.each do |old, new|
-        event[new] = results['hits']['hits'][0]['_source'][old]
-      end
-
-      filter_matched(event)
-    rescue => e
-      @logger.warn("Failed to query elasticsearch for previous event",
-                   :query => query_str, :event => event, :error => e)
-    end
-  end # def filter
-end # class LogStash::Filters::Elasticsearch
diff --git a/lib/logstash/filters/environment.rb b/lib/logstash/filters/environment.rb
deleted file mode 100644
index cbf6fc46443..00000000000
--- a/lib/logstash/filters/environment.rb
+++ /dev/null
@@ -1,27 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# Set fields from environment variables
-class LogStash::Filters::Environment < LogStash::Filters::Base
-  config_name "environment"
-  milestone 1
-
-  # Specify a hash of fields to the environment variable
-  # A hash of matches of field => environment variable
-  config :add_field_from_env, :validate => :hash, :default => {}
-
-  public
-  def register
-    # Nothing
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-    @add_field_from_env.each do |field, env|
-      event[field] = ENV[env]
-    end
-    filter_matched(event)
-  end # def filter
-end # class LogStash::Filters::Environment
diff --git a/lib/logstash/filters/extractnumbers.rb b/lib/logstash/filters/extractnumbers.rb
deleted file mode 100644
index 86a7cc57589..00000000000
--- a/lib/logstash/filters/extractnumbers.rb
+++ /dev/null
@@ -1,84 +0,0 @@
-# encoding: utf-8
-require 'logstash/namespace'
-require 'logstash/filters/base'
-
-
-# This filter automatically extracts all numbers found inside a string
-#
-# This is useful when you have lines that don't match a grok pattern
-# or use json but you still need to extract numbers.
-#
-# Each numbers is returned in a @fields.intX or @fields.floatX field
-# where X indicates the position in the string.
-#
-# The fields produced by this filter are extra useful used in combination
-# with kibana number plotting features.
-class LogStash::Filters::ExtractNumbers < LogStash::Filters::Base
-  config_name 'extractnumbers'
-  milestone 1
-
-  # The source field for the data. By default is message.
-  config :source, :validate => :string, :default => 'message'
-
-  public
-  def register
-  end
-
-  public
-  def filter(event)
-    integers = nil
-    floats = nil
-
-    msg = event[@source]
-
-    if not msg
-      return
-    end
-
-    # If for some reason the field is an array of values, take the first only.
-    msg = msg.first if msg.is_a?(Array)
-
-
-    fields = msg.split
-    for elem in fields
-      int = str_as_integer(elem)
-      if int != nil
-        if not integers
-          integers = Array.new
-        end
-        integers.push(int)
-        next
-      end
-      f = str_as_float(elem)
-      if f != nil
-        if not floats
-          floats = Array.new
-        end
-        floats.push(f)
-      end
-    end
-
-    if integers
-      index = 0
-      for i in integers
-        index += 1
-        event["int" + index.to_s] = i
-      end
-    end
-    if floats
-      index = 0
-      for f in floats
-        index += 1
-        event["float" + index.to_s] = f
-      end
-    end
-  end
-
-  def str_as_integer(str)
-    Integer(str) rescue nil
-  end
-
-  def str_as_float(str)
-    Float(str) rescue nil
-  end
-end # class LogStash::Filters::ExtractNumbers
diff --git a/lib/logstash/filters/gelfify.rb b/lib/logstash/filters/gelfify.rb
deleted file mode 100644
index 360ec8e460e..00000000000
--- a/lib/logstash/filters/gelfify.rb
+++ /dev/null
@@ -1,51 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# The GELFify filter parses RFC3164 severity levels to
-# corresponding GELF levels.
-class LogStash::Filters::Gelfify < LogStash::Filters::Base
-  config_name "gelfify"
-  milestone 2
-
-  SYSLOG_LEVEL_MAP = {
-    0 => 3, # Emergency => FATAL
-    1 => 5, # Alert     => WARN
-    2 => 3, # Critical  => FATAL
-    3 => 4, # Error     => ERROR
-    4 => 5, # Warning   => WARN
-    5 => 6, # Notice    => INFO
-    6 => 6, # Informat. => INFO
-    7 => 7  # Debug     => DEBUG
-  }
-
-  public
-  def register
-    # nothing
-  end # def register
-
-  public
-  def filter(event)
-    @logger.debug("GELFIFY FILTER: received event of type #{event["type"]}")
-
-    if event.include?("severity")
-      sev = event["severity"].to_i rescue nil
-      if sev.to_s != event["severity"].to_s
-        # severity isn't convertable to an integer.
-        # "foo".to_i => 0, which would default to EMERG.
-        @logger.debug("GELFIFY FILTER: existing severity field is not an int")
-      elsif SYSLOG_LEVEL_MAP[sev]
-        @logger.debug("GELFIFY FILTER: Severity level successfully mapped")
-        event["GELF_severity"] = SYSLOG_LEVEL_MAP[sev]
-      else
-        @logger.debug("GELFIFY FILTER: unknown severity #{sev}")
-      end
-    else
-      @logger.debug("GELFIFY FILTER: No 'severity' field found")
-    end
-
-    if !event.cancelled?
-      filter_matched(event)
-    end
-  end # def filter
-end # class LogStash::Filters::Gelfify
diff --git a/lib/logstash/filters/grep.rb b/lib/logstash/filters/grep.rb
deleted file mode 100644
index 21a7a2c0f01..00000000000
--- a/lib/logstash/filters/grep.rb
+++ /dev/null
@@ -1,158 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# NOTE: This filter has been deprecated and will be removed from future versions of logstash!
-#
-# For similar supported functionality, you should consider using the [drop](drop) and/or
-# [mutate](mutate) filters, in conjunction with logstash conditionals.
-#
-# This filter operates similar to the Unix `grep` utility. It is useful for
-# dropping events you don't want to pass, or adding tags or fields to events that match.
-#
-# Events not matched are dropped. If 'negate' is set to true (defaults false),
-# then matching events are dropped.
-class LogStash::Filters::Grep < LogStash::Filters::Base
-
-  config_name "grep"
-  milestone 3
-
-  # Drop events that don't match.
-  #
-  # If this is set to false, no events will be dropped at all. Rather, the
-  # requested tags and fields will be added to matching events, and
-  # non-matching events will be passed through unchanged.
-  config :drop, :validate => :boolean, :default => true
-
-  # Negate the match. Similar to 'grep -v'.
-  #
-  # If this is set to true, then any positive matches will result in the
-  # event being cancelled and dropped. Non-matching will be allowed
-  # through.
-  config :negate, :validate => :boolean, :default => false
-
-  # A hash of matches of field => regexp.  If multiple matches are specified,
-  # all must match for the grep to be considered successful.  Normal regular
-  # expressions are supported here.
-  #
-  # For example:
-  #
-  #     filter {
-  #       grep {
-  #         match => [ "message", "hello world" ]
-  #       }
-  #     }
-  #
-  # The above will drop all events with a message not matching "hello world" as
-  # a regular expression.
-  config :match, :validate => :hash, :default => {}
-
-  # Use case-insensitive matching. Similar to 'grep -i'
-  #
-  # If enabled, ignore case distinctions in the patterns.
-  config :ignore_case, :validate => :boolean, :default => false
-
-  public
-  def register
-    @logger.warn("The 'grep' plugin is no longer necessary now that you can do if/elsif/else in logstash configs. This plugin will be removed in the future. If you need to drop events, please use the drop filter. If you need to take action based on a match, use an 'if' block and the mutate filter. See the following URL for details on how to use if/elsif/else in your logstash configs:http://logstash.net/docs/#{LOGSTASH_VERSION}/configuration")
-
-    @patterns = Hash.new { |h,k| h[k] = [] }
-
-      # TODO(sissel): 
-    @match.each do |field, pattern|
-
-      pattern = [pattern] if pattern.is_a?(String)
-      pattern.each do |p|
-        re = Regexp.new(p, @ignore_case ? Regexp::IGNORECASE : 0)
-        @patterns[field] << re
-        @logger.debug? and @logger.debug("Registered grep", :type => @type, :field => field,
-                    :pattern => p, :regexp => re)
-      end
-    end # @match.merge.each
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    @logger.debug("Running grep filter", :event => event, :config => config)
-    matches = 0
-
-    # If negate is set but no patterns are given, drop the event.
-    # This is useful in cases where you want to drop all events with
-    # a given type or set of tags
-    #
-    # filter {
-    #   grep {
-    #     negate => true
-    #     type => blah
-    #   }
-    # }
-    if @negate && @patterns.empty?
-      event.cancel
-      return
-    end
-
-    @patterns.each do |field, regexes|
-      # For each match object, we have to match everything in order to
-      # apply any fields/tags.
-      match_count = 0
-      match_want = 0
-      regexes.each do |re|
-        match_want += 1
-
-        # Events without this field, with negate enabled, count as a match.
-        # With negate disabled, we can't possibly match, so skip ahead.
-        if event[field].nil?
-          if @negate
-            msg = "Field not present, but negate is true; marking as a match"
-            @logger.debug(msg, :field => field, :event => event)
-            match_count += 1
-          else
-            @logger.debug("Skipping match object, field not present",
-                          :field => field, :event => event)
-          end
-          # Either way, don't try to process -- may end up with extra unwanted
-          # +1's to match_count
-          next
-        end
-
-        (event[field].is_a?(Array) ? event[field] : [event[field]]).each do |value|
-          value = value.to_s if value.is_a?(Numeric)
-          if @negate
-            @logger.debug("negate match", :regexp => re, :value => value)
-            next if re.match(value)
-            @logger.debug("grep not-matched (negate requested)", field => value)
-          else
-            @logger.debug("want match", :regexp => re, :value => value)
-            next unless re.match(value)
-            @logger.debug("grep matched", field => value)
-          end
-          match_count += 1
-          break
-        end # each value in event[field]
-      end # regexes.each
-
-      if match_count == match_want
-        matches += 1
-        @logger.debug("matched all fields", :count => match_count)
-      else
-        @logger.debug("match failed", :count => match_count, :wanted => match_want)
-      end # match["match"].each
-    end # @patterns.each
-
-    if matches == @patterns.length
-      filter_matched(event)
-    else
-      if @drop == true
-        @logger.debug("grep: dropping event, no matches")
-        event.cancel
-      else
-        @logger.debug("grep: no matches, but drop set to false")
-      end
-      return
-    end
-
-    @logger.debug("Event after grep filter", :event => event)
-  end # def filter
-end # class LogStash::Filters::Grep
diff --git a/lib/logstash/filters/i18n.rb b/lib/logstash/filters/i18n.rb
deleted file mode 100644
index acc0456152c..00000000000
--- a/lib/logstash/filters/i18n.rb
+++ /dev/null
@@ -1,51 +0,0 @@
-# encoding: utf-8
-require "i18n"
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# The i18n filter allows you to remove special characters from
-# from a field
-class LogStash::Filters::I18n < LogStash::Filters::Base
-  config_name "i18n"
-  milestone 0
-
-  # Replaces non-ASCII characters with an ASCII approximation, or
-  # if none exists, a replacement character which defaults to “?”
-  #
-  # Example:
-  #
-  #     filter {
-  #       i18n {
-  #          transliterate => ["field1", "field2"]
-  #       }
-  #     }
-  config :transliterate, :validate => :array
-
-  public
-  def register
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    transliterate(event) if @transliterate
-
-    filter_matched(event)
-  end # def filter
-
-  private
-  def transliterate(event)
-    @transliterate.each do |field|
-      if event[field].is_a?(Array)
-        event[field].map! { |v| I18n.transliterate(v).encode('UTF-8') }
-      elsif event[field].is_a?(String)
-        event[field] = I18n.transliterate(event[field].encode('UTF-8'))
-      else
-        @logger.debug("Can't transliterate something that isn't a string",
-                      :field => field, :value => event[field])
-      end
-    end
-  end # def transliterate
-
-end # class LogStash::Filters::I18n
diff --git a/lib/logstash/filters/json_encode.rb b/lib/logstash/filters/json_encode.rb
deleted file mode 100644
index 07f4b74498d..00000000000
--- a/lib/logstash/filters/json_encode.rb
+++ /dev/null
@@ -1,52 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# JSON encode filter. Takes a field and serializes it into JSON
-#
-# If no target is specified, the source field is overwritten with the JSON
-# text.
-#
-# For example, if you have a field named 'foo', and you want to store the
-# JSON encoded string in 'bar', do this:
-#
-#     filter {
-#       json_encode {
-#         source => "foo"
-#         target => "bar"
-#       }
-#     }
-class LogStash::Filters::JSONEncode < LogStash::Filters::Base
-
-  config_name "json_encode"
-  milestone 2
-
-  # The field to convert to JSON.
-  config :source, :validate => :string, :required => true
-
-  # The field to write the JSON into. If not specified, the source
-  # field will be overwritten.
-  config :target, :validate => :string
-
-  public
-  def register
-    @target = @source if @target.nil?
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    @logger.debug("Running JSON encoder", :event => event)
-
-    begin
-      event[@target] = JSON.pretty_generate(event[@source])
-      filter_matched(event)
-    rescue => e
-      event.tag "_jsongeneratefailure"
-      @logger.warn("Trouble encoding JSON", :source => @source, :raw => event[@source].inspect, :exception => e)
-    end
-
-    @logger.debug? && @logger.debug("Event after JSON encoder", :event => event)
-  end # def filter
-end # class LogStash::Filters::JSONEncode
diff --git a/lib/logstash/filters/metaevent.rb b/lib/logstash/filters/metaevent.rb
deleted file mode 100644
index b31549e0e80..00000000000
--- a/lib/logstash/filters/metaevent.rb
+++ /dev/null
@@ -1,68 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-class LogStash::Filters::Metaevent < LogStash::Filters::Base
-  config_name "metaevent"
-  milestone 1
-
-  # syntax: `followed_by_tags => [ "tag", "tag" ]`
-  config :followed_by_tags, :validate => :array, :required => true
-
-  # syntax: `period => 60`
-  config :period, :validate => :number, :default => 5
-
-  def register
-    @logger.debug("registering")
-    @metaevents = []
-  end
-
-  def filter(event)
-    if filter?(event)
-      start_period(event)
-    elsif within_period(event)
-      if followed_by_tags_match(event)
-        trigger(event)
-      else
-        @logger.debug(["metaevent", @add_tag, "ignoring (tags don't match)", event])
-      end
-    else
-      @logger.debug(["metaevent", @add_tag, "ignoring (not in period)", event])
-    end
-  end
-
-  def flush
-    return if @metaevents.empty?
-
-    new_events = @metaevents
-    @metaevents = []
-    new_events
-  end
-
-  private
-
-  def start_period(event)
-    @logger.debug(["metaevent", @add_tag, "start_period", event])
-    @start_event = event
-  end
-
-  def trigger(event)
-    @logger.debug(["metaevent", @add_tag, "trigger", event])
-
-    event = LogStash::Event.new
-    event["source"] = Socket.gethostname
-    event["tags"] = [@add_tag]
-
-    @metaevents << event
-    @start_event = nil
-  end
-
-  def followed_by_tags_match(event)
-    (event["tags"] & @followed_by_tags).size == @followed_by_tags.size
-  end
-
-  def within_period(event)
-    time_delta = event["@timestamp"] - @start_event["@timestamp"]
-    time_delta >= 0 && time_delta <= @period
-  end
-end
diff --git a/lib/logstash/filters/prune.rb b/lib/logstash/filters/prune.rb
deleted file mode 100644
index 29baf2e6fa2..00000000000
--- a/lib/logstash/filters/prune.rb
+++ /dev/null
@@ -1,149 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# The prune filter is for pruning event data from @fileds based on whitelist/blacklist
-# of field names or their values (names and values can also be regular expressions).
-
-class LogStash::Filters::Prune < LogStash::Filters::Base
-  config_name "prune"
-  milestone 1
-
-  # Trigger whether configation fields and values should be interpolated for
-  # dynamic values.
-  # Probably adds some performance overhead. Defaults to false.
-  config :interpolate, :validate => :boolean, :default => false
-
-  # Include only fields only if their names match specified regexps, default to empty list which means include everything.
-  # 
-  #     filter { 
-  #       %PLUGIN% { 
-  #         tags            => [ "apache-accesslog" ]
-  #         whitelist_names => [ "method", "(referrer|status)", "${some}_field" ]
-  #       }
-  #     }
-  config :whitelist_names, :validate => :array, :default => []
-
-  # Exclude fields which names match specified regexps, by default exclude unresolved %{field} strings.
-  #
-  #     filter { 
-  #       %PLUGIN% { 
-  #         tags            => [ "apache-accesslog" ]
-  #         blacklist_names => [ "method", "(referrer|status)", "${some}_field" ]
-  #       }
-  #     }
-  config :blacklist_names, :validate => :array, :default => [ "%\{[^}]+\}" ]
-
-  # Include specified fields only if their values match regexps.
-  # In case field values are arrays, the fields are pruned on per array item
-  # thus only matching array items will be included.
-  # 
-  #     filter { 
-  #       %PLUGIN% { 
-  #         tags             => [ "apache-accesslog" ]
-  #         whitelist_values => [ "uripath", "/index.php",
-  #                               "method", "(GET|POST)",
-  #                               "status", "^[^2]" ]
-  #       }
-  #     }
-  config :whitelist_values, :validate => :hash, :default => {}
-
-  # Exclude specified fields if their values match regexps.
-  # In case field values are arrays, the fields are pruned on per array item
-  # in case all array items are matched whole field will be deleted.
-  #
-  #     filter { 
-  #       %PLUGIN% { 
-  #         tags             => [ "apache-accesslog" ]
-  #         blacklist_values => [ "uripath", "/index.php",
-  #                               "method", "(HEAD|OPTIONS)",
-  #                               "status", "^[^2]" ]
-  #       }
-  #     }
-  config :blacklist_values, :validate => :hash, :default => {}
-
-  public
-  def register
-    unless @interpolate
-      @whitelist_names_regexp = Regexp.union(@whitelist_names.map {|x| Regexp.new(x)})
-      @blacklist_names_regexp = Regexp.union(@blacklist_names.map {|x| Regexp.new(x)})
-      @whitelist_values.each do |key, value|
-        @whitelist_values[key] = Regexp.new(value)
-      end
-      @blacklist_values.each do |key, value|
-        @blacklist_values[key] = Regexp.new(value)
-      end
-    end
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    hash = event.to_hash
-
-    # We need to collect fields which needs to be remove ,and only in the end
-    # actually remove it since then interpolation mode you can get unexpected
-    # results as fields with dynamic values will not match since the fields to
-    # which they refer have already been removed.
-    fields_to_remove = []
-
-    unless @whitelist_names.empty?
-      @whitelist_names_regexp = Regexp.union(@whitelist_names.map {|x| Regexp.new(event.sprintf(x))}) if @interpolate
-      hash.each_key do |field|
-        fields_to_remove << field unless field.match(@whitelist_names_regexp)
-      end
-    end
-
-    unless @blacklist_names.empty?
-      @blacklist_names_regexp = Regexp.union(@blacklist_names.map {|x| Regexp.new(event.sprintf(x))}) if @interpolate
-      hash.each_key do |field|
-        fields_to_remove << field if field.match(@blacklist_names_regexp)
-      end
-    end
-
-    @whitelist_values.each do |key, value|
-      if @interpolate
-        key = event.sprintf(key)
-        value = Regexp.new(event.sprintf(value))
-      end
-      if hash[key]
-        if hash[key].is_a?(Array)
-          subvalues_to_remove = hash[key].find_all{|x| not x.match(value)}
-          unless subvalues_to_remove.empty?
-            fields_to_remove << (subvalues_to_remove.length == hash[key].length ? key : { :key => key, :values => subvalues_to_remove })
-          end
-        else
-          fields_to_remove << key if not hash[key].match(value)
-        end
-      end
-    end
-
-    @blacklist_values.each do |key, value|
-      if @interpolate
-        key = event.sprintf(key)
-        value = Regexp.new(event.sprintf(value))
-      end
-      if hash[key]
-        if hash[key].is_a?(Array)
-          subvalues_to_remove = hash[key].find_all{|x| x.match(value)}
-          unless subvalues_to_remove.empty?
-            fields_to_remove << (subvalues_to_remove.length == hash[key].length ? key : { :key => key, :values => subvalues_to_remove })
-          end
-        else
-          fields_to_remove << key if hash[key].match(value)
-        end
-      end
-    end
-
-    fields_to_remove.each do |field|
-      if field.is_a?(Hash)
-        hash[field[:key]] = hash[field[:key]] - field[:values]
-      else
-        hash.delete(field)
-      end
-    end
-
-    filter_matched(event)
-  end # def filter
-end # class LogStash::Filters::Prune
diff --git a/lib/logstash/filters/punct.rb b/lib/logstash/filters/punct.rb
deleted file mode 100644
index 1f55f3c5a00..00000000000
--- a/lib/logstash/filters/punct.rb
+++ /dev/null
@@ -1,32 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# Strip everything but punctuation from a field and store the remainder in the
-# a separate field. This is often used for fingerprinting log events.
-class LogStash::Filters::Punct < LogStash::Filters::Base
-  config_name "punct"
-  milestone 1
-
-  # The field reference to use for punctuation stripping
-  config :source, :validate => :string, :default => "message"
-
-  # The field to store the result.
-  config :target, :validate => :string, :default => "punct"
-
-  public
-  def register
-    # Nothing to do
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    original_value = event[@source]
-
-    # If for some reason the field is an array of values, take the first only.
-    original_value = original_value.first if original_value.is_a?(Array)
-    event[@target] = original_value.tr('A-Za-z0-9 \t','')
-  end # def filter
-end # class LogStash::Filters::Punct
diff --git a/lib/logstash/filters/railsparallelrequest.rb b/lib/logstash/filters/railsparallelrequest.rb
deleted file mode 100644
index c2f54cabec8..00000000000
--- a/lib/logstash/filters/railsparallelrequest.rb
+++ /dev/null
@@ -1,86 +0,0 @@
-# encoding: utf-8
-# parallel request filter
-#
-# This filter will separate out the parallel requests into separate events.
-#
-
-require "logstash/filters/base"
-require "logstash/namespace"
-require "set"
-
-class LogStash::Filters::Railsparallelrequest < LogStash::Filters::Base
-
-  config_name "railsparallelrequest"
-  milestone 1
-
-  public
-  def initialize(config = {})
-    super
-    @threadsafe = false
-    @pending = Hash.new
-    @last_event = nil
-    @recently_error = nil
-    @last_uuid = nil
-  end
-
-  def register ;end
-
-  def filter(event)
-    return unless filter?(event)
-    return if event["tags"].include? self.class.config_name
-
-    event["tags"] << self.class.config_name
-
-    line = event["message"]
-
-    if line =~ /^\[(.*?)\]/
-      uuid = $1
-      event["uuid"] = uuid
-      if @recently_error
-        if @last_uuid == uuid
-          merge_events(@recently_error, event, uuid)
-          event.cancel
-          return
-        else
-          @recently_error.uncancel
-          yield @recently_error
-          @recently_error = nil
-        end
-      end
-
-      @last_uuid = uuid
-      if @pending[uuid]
-        merge_events(@pending[uuid], event, uuid)
-      else
-        @pending[uuid] = event
-      end
-      @last_event = @pending[uuid]
-
-      if line =~ /Error/
-        event.overwrite(@pending[uuid].to_hash)
-        @pending.delete uuid
-        @recently_error = event
-      elsif line =~ /Completed/
-        event.overwrite(@pending[uuid])
-        @pending.delete uuid
-        return
-      end
-      event.cancel
-    elsif @last_event
-      @last_event.append(event)
-      event.cancel
-    end
-  end
-
-  def flush
-    events = @pending.values.each { |event| event.uncancel }
-    @pending.clear
-    events
-  end
-
-  private
-  def merge_events(dest, source, uuid)
-    source["message"].gsub!("[#{uuid}]", "")
-    dest.append(source)
-  end
-end
diff --git a/lib/logstash/filters/range.rb b/lib/logstash/filters/range.rb
deleted file mode 100644
index abdb4eddaf9..00000000000
--- a/lib/logstash/filters/range.rb
+++ /dev/null
@@ -1,142 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# This filter is used to check that certain fields are within expected size/length ranges.
-# Supported types are numbers and strings.
-# Numbers are checked to be within numeric value range.
-# Strings are checked to be within string length range.
-# More than one range can be specified for same fieldname, actions will be applied incrementally.
-# When field value is within a specified range an action will be taken.
-# Supported actions are drop event, add tag, or add field with specified value.
-#
-# Example use cases are for histogram-like tagging of events
-# or for finding anomaly values in fields or too big events that should be dropped.
-
-class LogStash::Filters::Range < LogStash::Filters::Base
-  config_name "range"
-  milestone 1
-
-  # An array of field, min, max, action tuples.
-  # Example:
-  #
-  #     filter {
-  #       %PLUGIN% {
-  #         ranges => [ "message", 0, 10, "tag:short",
-  #                     "message", 11, 100, "tag:medium",
-  #                     "message", 101, 1000, "tag:long",
-  #                     "message", 1001, 1e1000, "drop",
-  #                     "duration", 0, 100, "field:latency:fast",
-  #                     "duration", 101, 200, "field:latency:normal",
-  #                     "duration", 201, 1000, "field:latency:slow",
-  #                     "duration", 1001, 1e1000, "field:latency:outlier",
-  #                     "requests", 0, 10, "tag:too_few_%{host}_requests" ]
-  #       }
-  #     }
-  #
-  # Supported actions are drop tag or field with specified value.
-  # Added tag names and field names and field values can have %{dynamic} values.
-  #
-  # TODO(piavlo): The action syntax is ugly at the moment due to logstash grammar limitations - arrays grammar should support
-  # TODO(piavlo): simple not nested hashses as values in addition to numaric and string values to prettify the syntax.
-  config :ranges, :validate => :array, :default => []
-
-  # Negate the range match logic, events should be outsize of the specified range to match.
-  config :negate, :validate => :boolean, :default => false
-
-  public
-  def register
-    if @ranges.length % 4 != 0
-      raise "#{self.class.name}: ranges array should consist of 4 field tuples (field,min,max,action)"
-    end
-  
-    @range_tuples = {}
-
-    while !@ranges.empty?
-      fieldname, min, max, action = @ranges.shift(4)
-      
-      raise "#{self.class.name}: range field name value should be a string" if !fieldname.is_a?(String)
-      raise "#{self.class.name}: range min value should be a number" if !min.is_a?(Integer) and !min.is_a?(Float)
-      raise "#{self.class.name}: range max value should be a number" if !max.is_a?(Integer) and !max.is_a?(Float)
-      raise "#{self.class.name}: range action value should be a string" if !action.is_a?(String)
-      
-      action = action.split(':')
-      
-      case action.first
-      when "drop"
-        raise "#{self.class.name}: drop action does not accept any parameters" unless action.length == 1
-        action = { :name => :drop }
-      when "tag"
-        raise "#{self.class.name}: tag action accepts exactly one arg which is a tag name" unless action.length == 2
-        action = { :name => :add_tag, :tag => action.last }
-      when "field"
-        raise "#{self.class.name}: field action accepts exactly 2 args which are a field name and field value" unless action.length == 3
-        if action.last == action.last.to_i.to_s
-          value = action.last.to_i
-        elsif action.last == action.last.to_f.to_s
-          value = action.last.to_f
-        else
-          value = action.last
-        end
-        action = { :name => :add_field, :field => action[1], :value => value }
-      else
-        raise "#{self.class.name}: unsupported action #{action}"
-      end
-      
-      @range_tuples[fieldname] ||= []
-      @range_tuples[fieldname] << { :min => min, :max => max, :action => action }
-    end
-  end # def register
-
- 
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    @range_tuples.each_key do |fieldname|
-      if event.include?(fieldname)
-        @range_tuples[fieldname].each do |range|
-          matched = false
-        
-          field = event[fieldname]
-          case field
-          when Integer
-            matched = field.between?(range[:min], range[:max])
-          when Float
-            matched = field.between?(range[:min], range[:max])
-          when String
-            matched = field.length.between?(range[:min], range[:max])
-          else
-            @logger.warn("#{self.class.name}: action field value has unsupported type")
-          end
-
-          matched = !matched if @negate
-          next unless matched
-        
-          case range[:action][:name]
-          when :drop
-            @logger.debug? and @logger.debug("#{self.class.name}: dropping event due to range match", :event => event)
-            event.cancel
-            return
-          when :add_tag
-            @logger.debug? and @logger.debug("#{self.class.name}: adding tag due to range match",
-                                             :event => event, :tag => range[:action][:tag] )
-            event.tag(event.sprintf(range[:action][:tag]))
-          when :add_field
-            @logger.debug? and @logger.debug("#{self.class.name}: adding field due to range match",
-                                              :event => event, :field => range[:action][:field], :value => range[:action][:value])
-            new_field = event.sprintf(range[:action][:field])
-            if event[new_field]
-              event[new_field] = [event[new_field]] if !event[new_field].is_a?(Array)
-              event[new_field] << event.sprintf(range[:action][:value])
-            else
-              event[new_field] = range[:action][:value].is_a?(String) ? event.sprintf(range[:action][:value]) : range[:action][:value]
-            end
-          end
-        end
-      end
-    end
-    
-    filter_matched(event)
-  end # def filter
-end # class LogStash::Filters::Range
diff --git a/lib/logstash/filters/sumnumbers.rb b/lib/logstash/filters/sumnumbers.rb
deleted file mode 100644
index 217a5543ad9..00000000000
--- a/lib/logstash/filters/sumnumbers.rb
+++ /dev/null
@@ -1,73 +0,0 @@
-require 'logstash/namespace'
-require 'logstash/filters/base'
-
-
-# This filter automatically sum all numbers found inside a string
-#
-# The sum is returned in a new field, "sumTotal".
-# The total numbers summed will be in a new field, "sumNums"
-#
-# The fields produced by this filter are extra useful used in combination
-# with kibana number plotting features.
-#
-# If the field is an array, all of the numbers in it will be summed.
-# If the field is a hash, all of the values of the top-level keys will be summed.
-# If the field is a string, it will be split, numbers extracted, and summed.
-class LogStash::Filters::SumNumbers < LogStash::Filters::Base
-  config_name 'sumnumbers'
-  milestone 1
-
-  # The source field for the data. By default is message.
-  config :source, :validate => :string, :default => 'message'
-
-  public
-  def register
-  end
-
-  public
-  def filter(event)
-    msg = event[@source]
-    sumnums = 0
-    sumtotal = 0
-
-    if not msg
-      return
-    end
-
-    # If for some reason the field is an array of values, take the first only.
-    if msg.is_a?(Array)
-      fields = msg.first.split
-      # If msg is json, get an array from the values
-    elsif msg.is_a?(Hash)
-      fields = msg.values
-      # Else, we have a string. Split it.
-    else
-      fields = msg.split
-    end
-
-    for elem in fields
-      int = str_as_integer(elem)
-      if int != nil
-        sumtotal += int
-        sumnums += 1
-        next
-      end
-      f = str_as_float(elem)
-      if f != nil
-        sumtotal += f
-        sumnums += 1
-      end
-    end
-
-    event["sumNums"] = sumnums
-    event["sumTotal"] = sumtotal
-  end
-
-  def str_as_integer(str)
-    Integer(str) rescue nil
-  end
-
-  def str_as_float(str)
-    Float(str) rescue nil
-  end
-end # class LogStash::Filters::SumNumbers
diff --git a/lib/logstash/filters/translate.rb b/lib/logstash/filters/translate.rb
deleted file mode 100644
index f49fbec4653..00000000000
--- a/lib/logstash/filters/translate.rb
+++ /dev/null
@@ -1,195 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# A general search and replace tool which uses a configured hash
-# and/or a YAML file to determine replacement values.
-#
-# The dictionary entries can be specified in one of two ways: First,
-# the "dictionary" configuration item may contain a hash representing
-# the mapping. Second, an external YAML file (readable by logstash) may be specified
-# in the "dictionary_path" configuration item. These two methods may not be used
-# in conjunction; it will produce an error.
-#
-# Operationally, if the event field specified in the "field" configuration
-# matches the EXACT contents of a dictionary entry key (or matches a regex if
-# "regex" configuration item has been enabled), the field's value will be substituted
-# with the matched key's value from the dictionary.
-#
-# By default, the translate filter will replace the contents of the 
-# maching event field (in-place). However, by using the "destination"
-# configuration item, you may also specify a target event field to
-# populate with the new translated value.
-# 
-# Alternatively, for simple string search and replacements for just a few values
-# you might consider using the gsub function of the mutate filter.
-
-class LogStash::Filters::Translate < LogStash::Filters::Base
-  config_name "translate"
-  milestone 1
-
-  # The name of the logstash event field containing the value to be compared for a
-  # match by the translate filter (e.g. "message", "host", "response_code"). 
-  # 
-  # If this field is an array, only the first value will be used.
-  config :field, :validate => :string, :required => true
-
-  # If the destination (or target) field already exists, this configuration item specifies
-  # whether the filter should skip translation (default) or overwrite the target field
-  # value with the new translation value.
-  config :override, :validate => :boolean, :default => false
-
-  # The dictionary to use for translation, when specified in the logstash filter
-  # configuration item (i.e. do not use the @dictionary_path YAML file)
-  # Example:
-  #
-  #     filter {
-  #       %PLUGIN% {
-  #         dictionary => [ "100", "Continue",
-  #                         "101", "Switching Protocols",
-  #                         "merci", "thank you",
-  #                         "old version", "new version" ]
-  #       }
-  #     }
-  # NOTE: it is an error to specify both dictionary and dictionary_path
-  config :dictionary, :validate => :hash,  :default => {}
-
-  # The full path of the external YAML dictionary file. The format of the table
-  # should be a standard YAML file. Make sure you specify any integer-based keys
-  # in quotes. The YAML file should look something like this:
-  #
-  #     "100": Continue
-  #     "101": Switching Protocols
-  #     merci: gracias
-  #     old version: new version
-  #     
-  # NOTE: it is an error to specify both dictionary and dictionary_path
-  config :dictionary_path, :validate => :path
-
-  # When using a dictionary file, this setting will indicate how frequently
-  # (in seconds) logstash will check the YAML file for updates.
-  config :refresh_interval, :validate => :number, :default => 300
-  
-  # The destination field you wish to populate with the translated code. The default
-  # is a field named "translation". Set this to the same value as source if you want
-  # to do a substitution, in this case filter will allways succeed. This will clobber
-  # the old value of the source field! 
-  config :destination, :validate => :string, :default => "translation"
-
-  # When `exact => true`, the translate filter will populate the destination field
-  # with the exact contents of the dictionary value. When `exact => false`, the
-  # filter will populate the destination field with the result of any existing
-  # destination field's data, with the translated value substituted in-place.
-  #
-  # For example, consider this simple translation.yml, configured to check the `data` field:
-  #     foo: bar
-  #
-  # If logstash receives an event with the `data` field set to "foo", and `exact => true`,
-  # the destination field will be populated with the string "bar".
-  
-  # If `exact => false`, and logstash receives the same event, the destination field
-  # will be also set to "bar". However, if logstash receives an event with the `data` field
-  # set to "foofing", the destination field will be set to "barfing".
-  #
-  # Set both `exact => true` AND `regex => `true` if you would like to match using dictionary
-  # keys as regular expressions. A large dictionary could be expensive to match in this case. 
-  config :exact, :validate => :boolean, :default => true
-
-  # If you'd like to treat dictionary keys as regular expressions, set `exact => true`.
-  # Note: this is activated only when `exact => true`.
-  config :regex, :validate => :boolean, :default => false
-
-  # In case no translation occurs in the event (no matches), this will add a default
-  # translation string, which will always populate "field", if the match failed.
-  #
-  # For example, if we have configured `fallback => "no match"`, using this dictionary:
-  #
-  #     foo: bar
-  #
-  # Then, if logstash received an event with the field `foo` set to "bar", the destination
-  # field would be set to "bar". However, if logstash received an event with `foo` set to "nope",
-  # then the destination field would still be populated, but with the value of "no match".
-  config :fallback, :validate => :string
-
-  public
-  def register
-    if @dictionary_path
-      @next_refresh = Time.now + @refresh_interval
-      registering = true
-      load_yaml(registering)
-    end
-    
-    @logger.debug? and @logger.debug("#{self.class.name}: Dictionary - ", :dictionary => @dictionary)
-    if @exact
-      @logger.debug? and @logger.debug("#{self.class.name}: Dictionary translation method - Exact")
-    else
-      @logger.debug? and @logger.debug("#{self.class.name}: Dictionary translation method - Fuzzy")
-    end
-  end # def register
-
-  public
-  def load_yaml(registering=false)
-    if !File.exists?(@dictionary_path)
-      @logger.warn("dictionary file read failure, continuing with old dictionary", :path => @dictionary_path)
-      return
-    end
-
-    begin
-      @dictionary.merge!(YAML.load_file(@dictionary_path))
-    rescue Exception => e
-      if registering
-        raise "#{self.class.name}: Bad Syntax in dictionary file #{@dictionary_path}"
-      else
-        @logger.warn("#{self.class.name}: Bad Syntax in dictionary file, continuing with old dictionary", :dictionary_path => @dictionary_path)
-      end
-    end
-  end
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    if @dictionary_path
-      if @next_refresh < Time.now
-        load_yaml
-        @next_refresh = Time.now + @refresh_interval
-        @logger.info("refreshing dictionary file")
-      end
-    end
-    
-    return unless event.include?(@field) # Skip translation in case event does not have @event field.
-    return if event.include?(@destination) and not @override # Skip translation in case @destination field already exists and @override is disabled.
-
-    begin
-      #If source field is array use first value and make sure source value is string
-      source = event[@field].is_a?(Array) ? event[@field].first.to_s : event[@field].to_s
-      matched = false
-      if @exact
-        if @regex
-          key = @dictionary.keys.detect{|k| source.match(Regexp.new(k))}
-          if key
-            event[@destination] = @dictionary[key]
-            matched = true
-          end
-        elsif @dictionary.include?(source)
-          event[@destination] = @dictionary[source]
-          matched = true
-        end
-      else 
-        translation = source.gsub(Regexp.union(@dictionary.keys), @dictionary)
-        if source != translation
-          event[@destination] = translation
-          matched = true
-        end
-      end
-
-      if not matched and @fallback
-        event[@destination] = @fallback
-        matched = true
-      end
-      filter_matched(event) if matched or @field == @destination
-    rescue Exception => e
-      @logger.error("Something went wrong when attempting to translate from dictionary", :exception => e, :field => @field, :event => event)
-    end
-  end # def filter
-end # class LogStash::Filters::Translate
diff --git a/lib/logstash/filters/unique.rb b/lib/logstash/filters/unique.rb
deleted file mode 100644
index d55b4bc7dc3..00000000000
--- a/lib/logstash/filters/unique.rb
+++ /dev/null
@@ -1,29 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-class LogStash::Filters::Unique < LogStash::Filters::Base
-
-  config_name "unique"
-  milestone 1
-
-  # The fields on which to run the unique filter.
-  config :fields, :validate => :array, :required => true
-
-  public
-  def register
-    # Nothing to do
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    @fields.each do |field|
-      next unless event[field].class == Array
-
-      event[field] = event[field].uniq
-    end
-  end # def filter
-
-end # class Logstash::Filters::Unique
diff --git a/lib/logstash/filters/zeromq.rb b/lib/logstash/filters/zeromq.rb
deleted file mode 100644
index a144ae4e657..00000000000
--- a/lib/logstash/filters/zeromq.rb
+++ /dev/null
@@ -1,123 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# ZeroMQ filter. This is the best way to send an event externally for filtering
-# It works much like an exec filter would by sending the event "offsite"
-# for processing and waiting for a response
-#
-# The protocol here is:
-#   * REQ sent with JSON-serialized logstash event
-#   * REP read expected to be the full JSON 'filtered' event
-#   * - if reply read is an empty string, it will cancel the event.
-#
-# Note that this is a limited subset of the zeromq functionality in
-# inputs and outputs. The only topology that makes sense here is:
-# REQ/REP. 
-class LogStash::Filters::ZeroMQ < LogStash::Filters::Base
-
-  config_name "zeromq"
-  milestone 1
-
-  # 0mq socket address to connect or bind
-  # Please note that inproc:// will not work with logstash
-  # as we use a context per thread
-  # By default, filters connect
-  config :address, :validate => :string, :default => "tcp://127.0.0.1:2121"
-
-  # The field to send off-site for processing
-  # If this is unset, the whole event will be sent
-  # TODO (lusis)
-  # Allow filtering multiple fields
-  config :field, :validate => :string
-
-  # 0mq mode
-  # server mode binds/listens
-  # client mode connects
-  config :mode, :validate => ["server", "client"], :default => "client"
-
-  
-  # 0mq socket options
-  # This exposes zmq_setsockopt
-  # for advanced tuning
-  # see http://api.zeromq.org/2-1:zmq-setsockopt for details
-  #
-  # This is where you would set values like:
-  # ZMQ::HWM - high water mark
-  # ZMQ::IDENTITY - named queues
-  # ZMQ::SWAP_SIZE - space for disk overflow
-  # ZMQ::SUBSCRIBE - topic filters for pubsub
-  #
-  # example: sockopt => ["ZMQ::HWM", 50, "ZMQ::IDENTITY", "my_named_queue"]
-  config :sockopt, :validate => :hash
-
-  public
-  def initialize(params)
-    super(params)
-
-    @threadsafe = false
-  end
-
-  public
-  def register
-    require "ffi-rzmq"
-    require "logstash/util/zeromq"
-    self.class.send(:include, LogStash::Util::ZeroMQ)
-
-    @zsocket = context.socket(ZMQ::REQ)
-
-    error_check(@zsocket.setsockopt(ZMQ::LINGER, 1),
-                "while setting ZMQ::LINGER == 1)")
-
-    if @sockopt
-      setopts(@zsocket, @sockopt)
-    end
-
-    setup(@zsocket, @address)
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    # TODO (lusis)
-    # Need to set a timeout on the socket
-    # If it never gets a reply, filtering stops cold
-    begin
-      if @field
-        @logger.debug("0mq: sending", :request => event[@field])
-        error_check(@zsocket.send_string(event[@field]), "in send_string")
-      else
-        @logger.debug("0mq: sending", :request => event.to_json)
-        error_check(@zsocket.send_string(event.to_json), "in send_string")
-      end
-      reply = ''
-      rc = @zsocket.recv_string(reply)
-      error_check(rc, "in recv_string")
-
-      # If we receive an empty reply, this is an indication that the filter
-      # wishes to cancel this event.
-      if reply.empty?
-        event.cancel
-        return
-      end
-      @logger.debug("0mq: receiving", :reply => reply)
-      if @field
-        event[@field] = event.sprintf(reply)
-        filter_matched(event)
-      else
-        reply = JSON.parse(reply)
-        event.overwrite(reply)
-      end
-      filter_matched(event)
-    rescue => e
-      @logger.warn("0mq filter exception", :address => @address, :exception => e, :backtrace => e.backtrace)
-    end
-  end # def filter
-
-  private
-  def server?
-    @mode == "server"
-  end # def server?
-
-end # class LogStash::Filters::ZeroMQ
diff --git a/lib/logstash/inputs/drupal_dblog.rb b/lib/logstash/inputs/drupal_dblog.rb
deleted file mode 100644
index c2c4be6280f..00000000000
--- a/lib/logstash/inputs/drupal_dblog.rb
+++ /dev/null
@@ -1,323 +0,0 @@
-# encoding: utf-8
-require "date"
-require "logstash/inputs/base"
-require "logstash/namespace"
-
-# Retrieve watchdog log events from a Drupal installation with DBLog enabled.
-# The events are pulled out directly from the database.
-# The original events are not deleted, and on every consecutive run only new
-# events are pulled.
-#
-# The last watchdog event id that was processed is stored in the Drupal
-# variable table with the name "logstash_last_wid". Delete this variable or
-# set it to 0 if you want to re-import all events.
-#
-# More info on DBLog: http://drupal.org/documentation/modules/dblog
-#
-class LogStash::Inputs::DrupalDblog < LogStash::Inputs::Base
-  config_name "drupal_dblog"
-  milestone 1
-
-  default :codec, "plain"
-
-  # Specify all drupal databases that you whish to import from.
-  # This can be as many as you whish.
-  # The format is a hash, with a unique site name as the key, and a databse
-  # url as the value.
-  #
-  # Example:
-  # [
-  #   "site1", "mysql://user1:password@host1.com/databasename",
-  #   "other_site", "mysql://user2:password@otherhost.com/databasename",
-  #   ...
-  # ]
-  config :databases, :validate => :hash
-
-  # By default, the event only contains the current user id as a field.
-  # If you whish to add the username as an additional field, set this to true.
-  config :add_usernames, :validate => :boolean, :default => false
-
-  # Time between checks in minutes.
-  config :interval, :validate => :number, :default => 10
-
-  # The amount of log messages that should be fetched with each query.
-  # Bulk fetching is done to prevent querying huge data sets when lots of
-  # messages are in the database.
-  config :bulksize, :validate => :number, :default => 5000
-
-  # Label this input with a type.
-  # Types are used mainly for filter activation.
-  #
-  #
-  # If you create an input with type "foobar", then only filters
-  # which also have type "foobar" will act on them.
-  #
-  # The type is also stored as part of the event itself, so you
-  # can also use the type to search for in the web interface.
-  config :type, :validate => :string, :default => 'watchdog'
-
-  public
-  def register
-    require "php_serialize"
-
-    if RUBY_PLATFORM == 'java'
-      require "logstash/inputs/drupal_dblog/jdbcconnection"
-    else
-      require "mysql2"
-    end
-  end # def register
-
-  public
-  def config_init(params)
-    super
-
-    dbs = {}
-    valid = true
-
-    @databases.each do |name, rawUri|
-      uri = URI(rawUri)
-
-      dbs[name] = {
-        "site" => name,
-        "scheme" => uri.scheme,
-        "host" => uri.host,
-        "user" => uri.user,
-        "password" => uri.password,
-        "database" => uri.path.sub('/', ''),
-        "port" => uri.port.to_i
-      }
-
-      if not (
-        uri.scheme and not uri.scheme.empty?\
-        and uri.host and not uri.host.empty?\
-        and uri.user and not uri.user.empty?\
-        and uri.password\
-        and uri.path and not uri.path.sub('/', '').empty?
-      )
-        @logger.error("Drupal DBLog: Invalid database URI for #{name} : #{rawUri}")
-        valid = false
-      end
-      if not uri.scheme == 'mysql'
-        @logger.error("Drupal DBLog: Only mysql databases are supported.")
-        valid = false
-      end
-    end
-
-    if not valid
-      @logger.error("Config validation failed.")
-      exit 1
-    end
-
-    @databases = dbs
-  end #def config_init
-
-  public
-  def run(output_queue)
-    @logger.info("Initializing drupal_dblog")
-
-    loop do
-      @logger.debug("Drupal DBLog: Starting to fetch new watchdog entries")
-      start = Time.now.to_i
-
-      @databases.each do |name, db|
-        @logger.debug("Drupal DBLog: Checking database #{name}")
-        check_database(output_queue, db)
-        @logger.info("Drupal DBLog: Retrieved all new watchdog messages from #{name}")
-      end
-
-      timeTaken = Time.now.to_i - start
-      @logger.info("Drupal DBLog: Fetched all new watchdog entries in #{timeTaken} seconds")
-
-      # If fetching of all databases took less time than the interval,
-      # sleep a bit.
-      sleepTime = @interval * 60 - timeTaken
-      if sleepTime > 0
-        @logger.debug("Drupal DBLog: Sleeping for #{sleepTime} seconds")
-        sleep(sleepTime)
-      end
-    end # loop
-  end # def run
-
-  private
-  def initialize_client(db)
-    if db["scheme"] == 'mysql'
-
-      if not db["port"] > 0
-        db["port"] = 3306
-      end
-
-      if RUBY_PLATFORM == 'java'
-        @client = LogStash::DrupalDblogJavaMysqlConnection.new(
-            db["host"],
-            db["user"],
-            db["password"],
-            db["database"],
-            db["port"]
-        )
-      else
-        @client = Mysql2::Client.new(
-            :host => db["host"],
-            :port => db["port"],
-            :username => db["user"],
-            :password => db["password"],
-            :database => db["database"]
-        )
-      end
-    end
-  end #def get_client
-
-  private
-  def check_database(output_queue, db)
-
-    begin
-      # connect to the MySQL server
-      initialize_client(db)
-    rescue Exception => e
-      @logger.error("Could not connect to database: " + e.message)
-      return
-    end #begin
-
-    begin
-      @sitename = db["site"]
-
-      @usermap = @add_usernames ? get_usermap : nil
-
-      # Retrieve last pulled watchdog entry id
-      initialLastWid = get_last_wid
-      lastWid = nil
-
-
-      if initialLastWid == false
-        lastWid = 0
-        set_last_wid(0, true)
-      else
-        lastWid = initialLastWid
-      end
-
-      # Fetch new entries, and create the event
-      while true
-        results = get_db_rows(lastWid)
-        if results.length() < 1
-          break
-        end
-
-        @logger.debug("Fetched " + results.length().to_s + " database rows")
-
-        results.each do |row|
-          event = build_event(row)
-          if event
-            decorate(event)
-            output_queue << event
-            lastWid = row['wid'].to_s
-          end
-        end
-
-        set_last_wid(lastWid, false)
-      end
-    rescue Exception => e
-      @logger.error("Error while fetching messages: ", :error => e.message)
-    end # begin
-
-    # Close connection
-    @client.close
-  end # def check_database
-
-  def get_db_rows(lastWid)
-    query = 'SELECT * from watchdog WHERE wid > ' + lastWid.to_s + " ORDER BY wid asc LIMIT " + @bulksize.to_s
-    return @client.query(query)
-  end # def get_db_rows
-
-  private
-  def update_sitename
-    if @sitename == ""
-      result = @client.query('SELECT value FROM variable WHERE name="site_name"')
-      if result.first()
-        @sitename = PHP.unserialize(result.first()['value'])
-      end
-    end
-  end # def update_sitename
-
-  private
-  def get_last_wid
-    result = @client.query('SELECT value FROM variable WHERE name="logstash_last_wid"')
-    lastWid = false
-
-    if result.count() > 0
-      tmp = result.first()["value"].gsub("i:", "").gsub(";", "")
-      lastWid = tmp.to_i.to_s == tmp ? tmp : "0"
-    end
-
-    return lastWid
-  end # def get_last_wid
-
-  private
-  def set_last_wid(wid, insert)
-    wid = PHP.serialize(wid.to_i)
-
-    # Update last import wid variable
-    if insert
-      # Does not exist yet, so insert
-      @client.query('INSERT INTO variable (name, value) VALUES("logstash_last_wid", "' + wid + '")')
-    else
-      @client.query('UPDATE variable SET value="' + wid + '" WHERE name="logstash_last_wid"')
-    end
-  end # def set_last_wid
-
-  private
-  def get_usermap
-    map = {}
-
-    @client.query("SELECT uid, name FROM users").each do |row|
-      map[row["uid"]] = row["name"]
-    end
-
-    map[0] = "guest"
-    return map
-  end # def get_usermap
-
-  private
-  def build_event(row)
-    # Convert unix timestamp
-    timestamp = Time.at(row["timestamp"]).to_datetime.iso8601
-
-    msg = row["message"]
-    vars = {}
-
-    # Unserialize the variables, and construct the message
-    if row['variables'] != 'N;'
-      vars = PHP.unserialize(row["variables"])
-
-      if vars.is_a?(Hash)
-        vars.each_pair do |k, v|
-          if msg.scan(k).length() > 0
-            msg = msg.gsub(k.to_s, v.to_s)
-          else
-            # If not inside the message, add var as an additional field
-            row["variable_" + k] = v
-          end
-        end
-      end
-    end
-
-    row.delete("message")
-    row.delete("variables")
-    row.delete("timestamp")
-
-    row["severity"] = row["severity"].to_i
-
-    if @add_usernames and @usermap.has_key?(row["uid"])
-      row["user"] = @usermap[row["uid"]]
-    end
-
-    entry = {
-      "@timestamp" => timestamp,
-      "tags" => [],
-      "type" => "watchdog",
-      "site" => @sitename,
-      "message" => msg
-    }.merge(row)
-
-    return LogStash::Event.new(entry)
-  end # def build_event
-
-end # class LogStash::Inputs::DrupalDblog
diff --git a/lib/logstash/inputs/drupal_dblog/jdbcconnection.rb b/lib/logstash/inputs/drupal_dblog/jdbcconnection.rb
deleted file mode 100644
index a46cefc84d4..00000000000
--- a/lib/logstash/inputs/drupal_dblog/jdbcconnection.rb
+++ /dev/null
@@ -1,66 +0,0 @@
-# encoding: utf-8
-require "java"
-require "rubygems"
-require "jdbc/mysql"
-
-java_import "com.mysql.jdbc.Driver"
-
-# A JDBC mysql connection class.
-# The interface is compatible with the mysql2 API.
-class LogStash::DrupalDblogJavaMysqlConnection
-
-  def initialize(host, username, password, database, port = nil)
-    port ||= 3306
-
-    address = "jdbc:mysql://#{host}:#{port}/#{database}"
-    @connection = java.sql.DriverManager.getConnection(address, username, password)
-  end # def initialize
-
-  def query(sql)
-    if sql =~ /select/i
-      return select(sql)
-    else
-      return update(sql)
-    end
-  end # def query
-
-  def select(sql)
-    stmt = @connection.createStatement
-    resultSet = stmt.executeQuery(sql)
-
-    meta = resultSet.getMetaData
-    column_count = meta.getColumnCount
-
-    rows = []
-
-    while resultSet.next
-      res = {}
-
-      (1..column_count).each do |i|
-        name = meta.getColumnName(i)
-        case meta.getColumnType(i)
-        when java.sql.Types::INTEGER
-          res[name] = resultSet.getInt(name)
-        else
-          res[name] = resultSet.getString(name)
-        end
-      end
-
-      rows << res
-    end
-
-    stmt.close
-    return rows
-  end # def select
-
-  def update(sql)
-    stmt = @connection.createStatement
-    stmt.execute_update(sql)
-    stmt.close
-  end # def update
-
-  def close
-    @connection.close
-  end # def close
-
-end # class LogStash::DrupalDblogJavaMysqlConnection
diff --git a/lib/logstash/inputs/gemfire.rb b/lib/logstash/inputs/gemfire.rb
deleted file mode 100644
index 611664d4022..00000000000
--- a/lib/logstash/inputs/gemfire.rb
+++ /dev/null
@@ -1,222 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/threadable"
-require "logstash/namespace"
-
-# Push events to a GemFire region.
-#
-# GemFire is an object database.
-#
-# To use this plugin you need to add gemfire.jar to your CLASSPATH.
-# Using format=json requires jackson.jar too; use of continuous
-# queries requires antlr.jar.
-#
-# Note: this plugin has only been tested with GemFire 7.0.
-#
-class LogStash::Inputs::Gemfire < LogStash::Inputs::Threadable
-
-  config_name "gemfire"
-  milestone 1
-
-  default :codec, "plain"
-
-  # Your client cache name
-  config :cache_name, :validate => :string, :default => "logstash"
-
-  # The path to a GemFire client cache XML file.
-  #
-  # Example:
-  #
-  #      <client-cache>
-  #        <pool name="client-pool" subscription-enabled="true" subscription-redundancy="1">
-  #            <locator host="localhost" port="31331"/>
-  #        </pool>
-  #        <region name="Logstash">
-  #            <region-attributes refid="CACHING_PROXY" pool-name="client-pool" >
-  #            </region-attributes>
-  #        </region>
-  #      </client-cache>
-  #
-  config :cache_xml_file, :validate => :string, :default => nil
-
-  # The region name
-  config :region_name, :validate => :string, :default => "Logstash"
-
-  # A regexp to use when registering interest for cache events.
-  # Ignored if a :query is specified.
-  config :interest_regexp, :validate => :string, :default => ".*"
-
-  # A query to run as a GemFire "continuous query"; if specified it takes
-  # precedence over :interest_regexp which will be ignore.
-  #
-  # Important: use of continuous queries requires subscriptions to be enabled on the client pool.
-  config :query, :validate => :string, :default => nil
-
-  # How the message is serialized in the cache. Can be one of "json" or "plain"; default is plain
-  config :serialization, :validate => :string, :default => nil
-
-  public
-  def register
-    import com.gemstone.gemfire.cache.AttributesMutator
-    import com.gemstone.gemfire.cache.InterestResultPolicy
-    import com.gemstone.gemfire.cache.client.ClientCacheFactory
-    import com.gemstone.gemfire.cache.client.ClientRegionShortcut
-    import com.gemstone.gemfire.cache.query.CqQuery
-    import com.gemstone.gemfire.cache.query.CqAttributes
-    import com.gemstone.gemfire.cache.query.CqAttributesFactory
-    import com.gemstone.gemfire.cache.query.QueryService
-    import com.gemstone.gemfire.cache.query.SelectResults
-    import com.gemstone.gemfire.pdx.JSONFormatter
-
-    @logger.info("Registering input", :plugin => self)
-  end # def register
-
-  def run(queue)
-    return if terminating?
-    connect
-
-    @logstash_queue = queue
-
-    if @query
-      continuous_query(@query)
-    else
-      register_interest(@interest_regexp)
-    end
-  end # def run
-
-  def teardown
-    @cache.close if @cache
-    @cache = nil
-    finished
-  end # def teardown
-
-  protected
-  def connect
-    begin
-      @logger.debug("Connecting to GemFire #{@cache_name}")
-
-      @cache = ClientCacheFactory.new.
-        set("name", @cache_name).
-        set("cache-xml-file", @cache_xml_file).create
-      @logger.debug("Created cache #{@cache.inspect}")
-
-    rescue => e
-      if terminating?
-        return
-      else
-        @logger.error("Gemfire connection error (during connect), will reconnect",
-                      :exception => e, :backtrace => e.backtrace)
-        sleep(1)
-        retry
-      end
-    end
-
-    @region = @cache.getRegion(@region_name);
-    @logger.debug("Created region #{@region.inspect}")
-  end # def connect
-
-  protected
-  def continuous_query(query)
-    qs = @cache.getQueryService
-
-    cqAf = CqAttributesFactory.new
-    cqAf.addCqListener(self)
-    cqa = cqAf.create
-
-    @logger.debug("Running continuous query #{query}")
-    cq = qs.newCq("logstashCQ" + self.object_id.to_s, query, cqa)
-
-    cq.executeWithInitialResults
-  end
-
-  def register_interest(interest)
-    @region.getAttributesMutator.addCacheListener(self)
-    @region.registerInterestRegex(interest, InterestResultPolicy::NONE, false, true)
-  end
-
-  def deserialize_message(message)
-    if @serialization == "json"
-      message ? JSONFormatter.toJSON(message) : "{}"
-    else
-      message
-    end
-  end
-
-  def process_event(event, event_name)
-    message = deserialize_message(event)
-    @codec.decode(message) do |event|
-      decorate(event)
-      @logstash_queue << event
-    end
-  end
-
-  # multiple interfaces
-  def close
-  end
-
-  #
-  # CqListener interface
-  #
-  def onEvent(event)
-    key = event.getKey
-    newValue = event.getNewValue
-    @logger.debug("onEvent #{event.getQueryOperation} #{key} #{newValue}")
-
-    process_event(event.getNewValue, "onEvent", "gemfire://query/#{key}/#{event.getQueryOperation}")
-  end
-
-  def onError(event)
-    @logger.debug("onError #{event}")
-  end
-
-  #
-  # CacheListener interface
-  #
-  protected
-  def afterCreate(event)
-    regionName = event.getRegion.getName
-    key = event.getKey
-    newValue = event.getNewValue
-    @logger.debug("afterCreate #{regionName} #{key} #{newValue}")
-
-    process_event(event.getNewValue, "afterCreate", "gemfire://#{regionName}/#{key}/afterCreate")
-  end
-
-  def afterDestroy(event)
-    regionName = event.getRegion.getName
-    key = event.getKey
-    newValue = event.getNewValue
-    @logger.debug("afterDestroy #{regionName} #{key} #{newValue}")
-
-    process_event(nil, "afterDestroy", "gemfire://#{regionName}/#{key}/afterDestroy")
-  end
-
-  def afterUpdate(event)
-    regionName = event.getRegion.getName
-    key = event.getKey
-    oldValue = event.getOldValue
-    newValue = event.getNewValue
-    @logger.debug("afterUpdate #{regionName} #{key} #{oldValue} -> #{newValue}")
-
-    process_event(event.getNewValue, "afterUpdate", "gemfire://#{regionName}/#{key}/afterUpdate")
-  end
-
-  def afterRegionLive(event)
-    @logger.debug("afterRegionLive #{event}")
-  end
-
-  def afterRegionCreate(event)
-    @logger.debug("afterRegionCreate #{event}")
-  end
-
-  def afterRegionClear(event)
-    @logger.debug("afterRegionClear #{event}")
-  end
-
-  def afterRegionDestroy(event)
-    @logger.debug("afterRegionDestroy #{event}")
-  end
-
-  def afterRegionInvalidate(event)
-    @logger.debug("afterRegionInvalidate #{event}")
-  end
-end # class LogStash::Inputs::Gemfire
diff --git a/lib/logstash/inputs/heroku.rb b/lib/logstash/inputs/heroku.rb
deleted file mode 100644
index b27aba65968..00000000000
--- a/lib/logstash/inputs/heroku.rb
+++ /dev/null
@@ -1,51 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-
-# Stream events from a heroku app's logs.
-#
-# This will read events in a manner similar to how the `heroku logs -t` command
-# fetches logs.
-#
-# Recommended filters:
-#
-#     filter {
-#       grok {
-#         pattern => "^%{TIMESTAMP_ISO8601:timestamp} %{WORD:component}\[%{WORD:process}(?:\.%{INT:instance:int})?\]: %{DATA:message}$"
-#       }
-#       date { timestamp => ISO8601 }
-#     }
-class LogStash::Inputs::Heroku < LogStash::Inputs::Base
-  config_name "heroku"
-  milestone 1
-
-  default :codec, "plain"
-
-  # The name of your heroku application. This is usually the first part of the 
-  # the domain name 'my-app-name.herokuapp.com'
-  config :app, :validate => :string, :required => true
-
-  public
-  def register
-    require "heroku"
-    require "logstash/util/buftok"
-  end # def register
-
-  public
-  def run(queue)
-    client = Heroku::Client.new(Heroku::Auth.user, Heroku::Auth.password)
-
-    # The 'Herok::Client#read_logs' method emits chunks of text not bounded
-    # by event barriers like newlines.
-    # tail=1 means to follow logs
-    # I *think* setting num=1 means we only get 1 historical event. Setting
-    # this to 0 makes it fetch *all* events, not what I want.
-    client.read_logs(@app, ["tail=1", "num=1"]) do |chunk|
-      @codec.decode(chunk) do |event|
-        decorate(event)
-        event["app"] = @app
-        queue << event
-      end
-    end
-  end # def run
-end # class LogStash::Inputs::Heroku
diff --git a/lib/logstash/inputs/relp.rb b/lib/logstash/inputs/relp.rb
deleted file mode 100644
index a15b060b22a..00000000000
--- a/lib/logstash/inputs/relp.rb
+++ /dev/null
@@ -1,106 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "logstash/util/relp"
-require "logstash/util/socket_peer"
-
-
-# Read RELP events over a TCP socket.
-#
-# For more information about RELP, see 
-# <http://www.rsyslog.com/doc/imrelp.html>
-#
-# This protocol implements application-level acknowledgements to help protect
-# against message loss.
-#
-# Message acks only function as far as messages being put into the queue for
-# filters; anything lost after that point will not be retransmitted
-class LogStash::Inputs::Relp < LogStash::Inputs::Base
-  class Interrupted < StandardError; end
-
-  config_name "relp"
-  milestone 1
-
-  default :codec, "plain"
-
-  # The address to listen on.
-  config :host, :validate => :string, :default => "0.0.0.0"
-
-  # The port to listen on.
-  config :port, :validate => :number, :required => true
-
-  def initialize(*args)
-    super(*args)
-  end # def initialize
-
-  public
-  def register
-    @logger.info("Starting relp input listener", :address => "#{@host}:#{@port}")
-    @relp_server = RelpServer.new(@host, @port,['syslog'])
-  end # def register
-
-  private
-  def relp_stream(relpserver,socket,output_queue,client_address)
-    loop do
-      frame = relpserver.syslog_read(socket)
-      @codec.decode(frame["message"]) do |event|
-        decorate(event)
-        event["host"] = client_address
-        output_queue << event
-      end
-
-      #To get this far, the message must have made it into the queue for 
-      #filtering. I don't think it's possible to wait for output before ack
-      #without fundamentally breaking the plugin architecture
-      relpserver.ack(socket, frame['txnr'])
-    end
-  end
-
-  public
-  def run(output_queue)
-    @thread = Thread.current
-    loop do
-      begin
-        # Start a new thread for each connection.
-        Thread.start(@relp_server.accept) do |client|
-            rs = client[0]
-            socket = client[1]
-            # monkeypatch a 'peer' method onto the socket.
-            socket.instance_eval { class << self; include ::LogStash::Util::SocketPeer end }
-            peer = socket.peer
-            @logger.debug("Relp Connection to #{peer} created")
-          begin
-            relp_stream(rs,socket, output_queue, peer)
-          rescue Relp::ConnectionClosed => e
-            @logger.debug("Relp Connection to #{peer} Closed")
-          rescue Relp::RelpError => e
-            @logger.warn('Relp error: '+e.class.to_s+' '+e.message)
-            #TODO: Still not happy with this, are they all warn level?
-            #Will this catch everything I want it to?
-            #Relp spec says to close connection on error, ensure this is the case
-          end
-        end # Thread.start
-      rescue Relp::InvalidCommand,Relp::InappropriateCommand => e
-        @logger.warn('Relp client trying to open connection with something other than open:'+e.message)
-      rescue Relp::InsufficientCommands
-        @logger.warn('Relp client incapable of syslog')
-      rescue IOError, Interrupted
-        if @interrupted
-          # Intended shutdown, get out of the loop
-          @relp_server.shutdown
-          break
-        else
-          # Else it was a genuine IOError caused by something else, so propagate it up..
-          raise
-        end
-      end
-    end # loop
-  end # def run
-
-  def teardown
-    @interrupted = true
-    @thread.raise(Interrupted.new)
-  end
-end # class LogStash::Inputs::Relp
-
-#TODO: structured error logging
diff --git a/lib/logstash/inputs/sqlite.rb b/lib/logstash/inputs/sqlite.rb
deleted file mode 100644
index 012e7708c96..00000000000
--- a/lib/logstash/inputs/sqlite.rb
+++ /dev/null
@@ -1,185 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "socket"
-
-# Read rows from an sqlite database.
-#
-# This is most useful in cases where you are logging directly to a table.
-# Any tables being watched must have an 'id' column that is monotonically
-# increasing.
-#
-# All tables are read by default except:
-# * ones matching 'sqlite_%' - these are internal/adminstrative tables for sqlite
-# * 'since_table' - this is used by this plugin to track state.
-#
-# ## Example
-# 
-#     % sqlite /tmp/example.db
-#     sqlite> CREATE TABLE weblogs (
-#         id INTEGER PRIMARY KEY AUTOINCREMENT,
-#         ip STRING,
-#         request STRING,
-#         response INTEGER);
-#     sqlite> INSERT INTO weblogs (ip, request, response) 
-#         VALUES ("1.2.3.4", "/index.html", 200);
-#
-# Then with this logstash config:
-#
-#     input {
-#       sqlite {
-#         path => "/tmp/example.db"
-#         type => weblogs
-#       }
-#     }
-#     output {
-#       stdout {
-#         debug => true
-#       }
-#     }
-#
-# Sample output:
-#
-#     {
-#       "@source"      => "sqlite://sadness/tmp/x.db",
-#       "@tags"        => [],
-#       "@fields"      => {
-#         "ip"       => "1.2.3.4",
-#         "request"  => "/index.html",
-#         "response" => 200
-#       },
-#       "@timestamp"   => "2013-05-29T06:16:30.850Z",
-#       "@source_host" => "sadness",
-#       "@source_path" => "/tmp/x.db",
-#       "@message"     => "",
-#       "@type"        => "foo"
-#     }
-#
-class LogStash::Inputs::Sqlite < LogStash::Inputs::Base
-  config_name "sqlite"
-  milestone 1
-
-  # The path to the sqlite database file.
-  config :path, :validate => :string, :required => true
-
-  # Any tables to exclude by name.
-  # By default all tables are followed.
-  config :exclude_tables, :validate => :array, :default => []
-
-  # How many rows to fetch at a time from each SELECT call.
-  config :batch, :validate => :number, :default => 5
-
-  SINCE_TABLE = :since_table
-
-  public
-  def init_placeholder_table(db)
-    begin
-      db.create_table SINCE_TABLE do 
-        String :table
-        Int    :place
-      end
-    rescue
-      @logger.debug("since tables already exists")
-    end
-  end
-
-  public
-  def get_placeholder(db, table)
-    since = db[SINCE_TABLE]
-    x = since.where(:table => "#{table}")
-    if x[:place].nil?
-      init_placeholder(db, table) 
-      return 0
-    else
-      @logger.debug("placeholder already exists, it is #{x[:place]}")
-      return x[:place][:place]
-    end
-  end
-
-  public 
-  def init_placeholder(db, table)
-    @logger.debug("init placeholder for #{table}")
-    since = db[SINCE_TABLE]
-    since.insert(:table => table, :place => 0)
-  end
-
-  public
-  def update_placeholder(db, table, place)
-    @logger.debug("set placeholder to #{place}")
-    since = db[SINCE_TABLE]
-    since.where(:table => table).update(:place => place)
-  end
-
-  public 
-  def get_all_tables(db)
-    return db["SELECT * FROM sqlite_master WHERE type = 'table' AND tbl_name != '#{SINCE_TABLE}' AND tbl_name NOT LIKE 'sqlite_%'"].map { |t| t[:name] }.select { |n| !@exclude_tables.include?(n) }
-  end
-  
-  public
-  def get_n_rows_from_table(db, table, offset, limit)
-    dataset = db["SELECT * FROM #{table}"]
-    return db["SELECT * FROM #{table} WHERE (id > #{offset}) ORDER BY 'id' LIMIT #{limit}"].map { |row| row }
-  end
-  
-  public
-  def register
-    require "sequel"
-    require "jdbc/sqlite3" 
-    @host = Socket.gethostname
-    @logger.info("Registering sqlite input", :database => @path)
-    @db = Sequel.connect("jdbc:sqlite:#{@path}") 
-    @tables = get_all_tables(@db)
-    @table_data = {}
-    @tables.each do |table|
-      init_placeholder_table(@db)
-      last_place = get_placeholder(@db, table)
-      @table_data[table] = { :name => table, :place => last_place }
-    end
-  end # def register
-
-  public
-  def run(queue)
-    sleep_min = 0.01
-    sleep_max = 5
-    sleeptime = sleep_min
-
-    begin
-      @logger.debug("Tailing sqlite db", :path => @path)
-      loop do
-        count = 0
-        @table_data.each do |k, table|
-          table_name = table[:name]
-          offset = table[:place]
-          @logger.debug("offset is #{offset}", :k => k, :table => table_name)
-          rows = get_n_rows_from_table(@db, table_name, offset, @batch)
-          count += rows.count
-          rows.each do |row| 
-            event = LogStash::Event.new("host" => @host, "db" => @db)
-            decorate(event)
-            # store each column as a field in the event.
-            row.each do |column, element|
-              next if column == :id
-              event[column.to_s] = element
-            end
-            queue << event
-            @table_data[k][:place] = row[:id]
-          end
-          # Store the last-seen row in the database
-          update_placeholder(@db, table_name, @table_data[k][:place])
-        end
-
-        if count == 0
-          # nothing found in that iteration
-          # sleep a bit
-          @logger.debug("No new rows. Sleeping.", :time => sleeptime)
-          sleeptime = [sleeptime * 2, sleep_max].min
-          sleep(sleeptime)
-        else
-          sleeptime = sleep_min
-        end
-      end # loop
-    end # begin/rescue
-  end #run
-
-end # class Logtstash::Inputs::EventLog
-
diff --git a/lib/logstash/inputs/stomp.rb b/lib/logstash/inputs/stomp.rb
deleted file mode 100644
index 5c33b588890..00000000000
--- a/lib/logstash/inputs/stomp.rb
+++ /dev/null
@@ -1,84 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require 'pp'
-
-class LogStash::Inputs::Stomp < LogStash::Inputs::Base
-  config_name "stomp"
-  milestone 2
-
-  default :codec, "plain"
-
-  # The address of the STOMP server.
-  config :host, :validate => :string, :default => "localhost", :required => true
-
-  # The port to connet to on your STOMP server.
-  config :port, :validate => :number, :default => 61613
-
-  # The username to authenticate with.
-  config :user, :validate => :string, :default => ""
-
-  # The password to authenticate with.
-  config :password, :validate => :password, :default => ""
-
-  # The destination to read events from.
-  #
-  # Example: "/topic/logstash"
-  config :destination, :validate => :string, :required => true
-
-  # The vhost to use
-  config :vhost, :validate => :string, :default => nil
-
-  # Enable debugging output?
-  config :debug, :validate => :boolean, :default => false, :deprecated => "This setting was never used by this plugin. It will be removed soon."
-
-  private
-  def connect
-    begin
-      @client.connect
-      @logger.debug("Connected to stomp server") if @client.connected?
-    rescue => e
-      @logger.debug("Failed to connect to stomp server, will retry", :exception => e, :backtrace => e.backtrace)
-      sleep 2
-      retry
-    end
-  end
-
-  public
-  def register
-    require "onstomp"
-    @client = OnStomp::Client.new("stomp://#{@host}:#{@port}", :login => @user, :passcode => @password.value)
-    @client.host = @vhost if @vhost
-    @stomp_url = "stomp://#{@user}:#{@password}@#{@host}:#{@port}/#{@destination}"
-
-    # Handle disconnects 
-    @client.on_connection_closed {
-      connect
-      subscription_handler # is required for re-subscribing to the destination
-    }
-    connect
-  end # def register
-
-  private
-  def subscription_handler
-    @client.subscribe(@destination) do |msg|
-      @codec.decode(msg.body) do |event|
-        decorate(event)
-        @output_queue << event
-      end
-    end
-    #In the event that there is only Stomp input plugin instances
-    #the process ends prematurely. The above code runs, and return
-    #the flow control to the 'run' method below. After that, the
-    #method "run_input" from agent.rb marks 'done' as 'true' and calls
-    #'finish' over the Stomp plugin instance.
-    #'Sleeping' the plugin leves the instance alive.
-    sleep
-  end
-
-  public
-  def run(output_queue)
-    @output_queue = output_queue 
-    subscription_handler
-  end # def run
-end # class LogStash::Inputs::Stomp
diff --git a/lib/logstash/inputs/varnishlog.rb b/lib/logstash/inputs/varnishlog.rb
deleted file mode 100644
index 9fcc1e81d47..00000000000
--- a/lib/logstash/inputs/varnishlog.rb
+++ /dev/null
@@ -1,48 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/threadable"
-require "logstash/namespace"
-require "socket" # for Socket.gethostname
-
-# Read from varnish cache's shared memory log
-class LogStash::Inputs::Varnishlog < LogStash::Inputs::Threadable
-  config_name "varnishlog"
-  milestone 1
-
-  public
-  def register
-    require 'varnish'
-    @vd = Varnish::VSM.VSM_New
-    Varnish::VSL.VSL_Setup(@vd)
-    Varnish::VSL.VSL_Open(@vd, 1)
-
-  end # def register
-
-  def run(queue)
-    @q = queue
-    @hostname = Socket.gethostname
-    Varnish::VSL.VSL_Dispatch(@vd, self.method(:cb).to_proc, FFI::MemoryPointer.new(:pointer))
-  end # def run
-
-  private
-  def cb(priv, tag, fd, len, spec, ptr, bitmap)
-    begin
-      str = ptr.read_string(len)
-      event = LogStash::Event.new("message" => str, "host" => @host)
-      decorate(event)
-      event["varnish_tag"] = tag
-      event["varnish_fd"] = fd
-      event["varnish_spec"] = spec
-      event["varnish_bitmap"] = bitmap
-      @q << event
-    rescue => e
-      @logger.warn("varnishlog exception: #{e.inspect}")
-    ensure
-      return 0
-    end
-  end
-  
-  public
-  def teardown
-    finished
-  end # def teardown
-end # class LogStash::Inputs::Stdin
diff --git a/lib/logstash/inputs/websocket.rb b/lib/logstash/inputs/websocket.rb
deleted file mode 100644
index 9438c015bad..00000000000
--- a/lib/logstash/inputs/websocket.rb
+++ /dev/null
@@ -1,50 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "socket"
-
-# Read events over the websocket protocol.
-class LogStash::Inputs::Websocket < LogStash::Inputs::Base
-  config_name "websocket"
-  milestone 1
-
-  default :codec, "json"
-
-  # The url to connect to or serve from
-  config :url, :validate => :string, :default => "0.0.0.0"
-
-  # Operate as a client or a server.
-  #
-  # Client mode causes this plugin to connect as a websocket client
-  # to the URL given. It expects to receive events as websocket messages.
-  #
-  # (NOT IMPLEMENTED YET) Server mode causes this plugin to listen on
-  # the given URL for websocket clients. It expects to receive events
-  # as websocket messages from these clients.
-  config :mode, :validate => [ "server", "client" ], :default => "client"
-
-  def register
-    require "ftw"
-  end # def register
-
-  public
-  def run(output_queue)
-    # TODO(sissel): Implement server mode.
-    agent = FTW::Agent.new
-    begin
-      websocket = agent.websocket!(@url)
-      websocket.each do |payload|
-        @codec.decode(payload) do |event|
-          decorate(event)
-          output_queue << event
-        end
-      end
-    rescue => e
-      @logger.warn("websocket input client threw exception, restarting",
-                   :exception => e)
-      sleep(1)
-      retry
-    end # begin
-  end # def run
-
-end # class LogStash::Inputs::Websocket
diff --git a/lib/logstash/inputs/wmi.rb b/lib/logstash/inputs/wmi.rb
deleted file mode 100644
index 76ff94973a6..00000000000
--- a/lib/logstash/inputs/wmi.rb
+++ /dev/null
@@ -1,101 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "socket"
-
-# Collect data from WMI query
-#
-# This is useful for collecting performance metrics and other data
-# which is accessible via WMI on a Windows host
-#
-# Example:
-#
-#     input {
-#       wmi {
-#         query => "select * from Win32_Process"
-#         interval => 10
-#       }
-#       wmi {
-#         query => "select PercentProcessorTime from Win32_PerfFormattedData_PerfOS_Processor where name = '_Total'"
-#       }
-#       wmi { # Connect to a remote host
-#         query => "select * from Win32_Process"
-#         host => "MyRemoteHost"
-#         user => "mydomain\myuser"
-#         password => "Password"
-#        }
-#     }
-class LogStash::Inputs::WMI < LogStash::Inputs::Base
-
-  config_name "wmi"
-  milestone 1
-
-  # WMI query
-  config :query, :validate => :string, :required => true
-  # Polling interval
-  config :interval, :validate => :number, :default => 10
-  
-  # Host to connect to ( Defaults to localhost )
-  config :host, :validate => :string, :default => 'localhost'
-
-  # Username when doing remote connections
-  config :user, :validate => :string
-
-  # Password when doing remote connections
-  config :password, :validate => :password
-
-  # Namespace when doing remote connections
-  config :namespace, :validate => :string, :default => 'root\cimv2'
-
-
-  public
-  def register
-
-    @host = Socket.gethostname
-    @logger.info("Registering wmi input", :query => @query)
-
-    if RUBY_PLATFORM == "java"
-      # make use of the same fix used for the eventlog input
-      require "logstash/inputs/eventlog/racob_fix"
-      require "jruby-win32ole"
-    else
-      require "win32ole"
-    end
-
-    # If host is localhost do a local connection
-    if (@host == "127.0.0.1" || @host == "localhost" || @host == "::1")
-      @wmi = WIN32OLE.connect('winmgmts:')
-      @host = Socket.gethostname
-    else
-      locator = WIN32OLE.new("WbemScripting.SWbemLocator")
-      @host = Socket.gethostbyname(@host).first
-      @wmi = locator.ConnectServer(@host, @namespace, @user, @password.value)
-    end
-
-  end # def register
-
-  public
-  def run(queue)
-
-    begin
-      @logger.debug("Executing WMI query '#{@query}'")
-      loop do
-        @wmi.ExecQuery(@query).each do |wmiobj|
-          # create a single event for all properties in the collection
-          event = LogStash::Event.new
-          event["host"] = @host
-          decorate(event)
-          wmiobj.Properties_.each do |prop|
-            event[prop.name] = prop.value
-          end
-          queue << event
-        end
-        sleep @interval
-      end # loop
-    rescue Exception => ex
-      @logger.error("WMI query error: #{ex}\n#{ex.backtrace}")
-      sleep @interval
-      retry
-    end # begin/rescue
-  end # def run
-end # class LogStash::Inputs::WMI
diff --git a/lib/logstash/inputs/zenoss.rb b/lib/logstash/inputs/zenoss.rb
deleted file mode 100644
index e0e511c1605..00000000000
--- a/lib/logstash/inputs/zenoss.rb
+++ /dev/null
@@ -1,143 +0,0 @@
-# encoding: utf-8
-require "date"
-require "logstash/inputs/rabbitmq"
-require "zlib"
-
-# Read Zenoss events from the zenoss.zenevents fanout exchange.
-#
-class LogStash::Inputs::Zenoss < LogStash::Inputs::RabbitMQ
-
-  config_name "zenoss"
-  milestone 1
-
-  # Your rabbitmq server address
-  config :host, :validate => :string, :default => "localhost"
-
-  # Your rabbitmq username
-  config :user, :validate => :string, :default => "zenoss"
-
-  # Your rabbitmq password
-  config :password, :validate => :password, :default => "zenoss"
-
-  # The name of the exchange to bind the queue. This is analogous to the 'rabbitmq
-  # output' [config 'name'](../outputs/rabbitmq)
-  config :exchange, :validate => :string, :default => "zenoss.zenevents"
-
-  # The routing key to use. This is only valid for direct or fanout exchanges
-  #
-  # * Routing keys are ignored on topic exchanges.
-  # * Wildcards are not valid on direct exchanges.
-  config :key, :validate => :string, :default => "zenoss.zenevent.#"
-
-  # The vhost to use. If you don't know what this is, leave the default.
-  config :vhost, :validate => :string, :default => "/zenoss"
-
-  def register
-    super
-    require "logstash/util/zenoss"
-    require "bunny"
-  end # def register
-
-  def run(queue)
-    begin
-      zep = Org::Zenoss::Protobufs::Zep
-
-      @logger.debug("Connecting with RabbitMQ settings #{@rabbitmq_settings.inspect}")
-      @bunny = Bunny.new(@rabbitmq_settings)
-      return if terminating?
-      @bunny.start
-      @bunny.qos({:prefetch_count => @prefetch_count})
-
-      @arguments_hash = Hash[*@arguments]
-
-      @logger.debug("Setting up queue #{@name.inspect}")
-      @queue = @bunny.queue(@name, {
-        :durable => @durable,
-        :auto_delete => @auto_delete,
-        :exclusive => @exclusive,
-        :arguments => @arguments_hash
-      })
-
-      @queue.bind(@exchange, :key => @key)
-
-      @queue.subscribe({:ack => @ack}) do |data|
-
-        # Zenoss can optionally compress message payloads.
-        if data[:header].content_encoding == "deflate"
-          data[:payload] = Zlib::Inflate.inflate(data[:payload])
-        end
-
-        # Decode the payload into an EventSummary.
-        summary = zep::EventSummary.decode(data[:payload])
-
-        # This should never happen, but skip it if it does.
-        next unless summary.occurrence.length > 0
-
-        occurrence = summary.occurrence[0]
-        #timestamp = DateTime.strptime(occurrence.created_time.to_s, "%Q").to_s
-        timestamp = Time.at(occurrence.created_time / 1000.0)
-
-        # LogStash event properties.
-        event = LogStash::Event.new(
-          "@timestamp" => timestamp,
-          "type" => @type,
-          "host" => occurrence.actor.element_title,
-          "message" => occurrence.message,
-        )
-        decorate(event)
-
-        # Direct mappings from summary.
-        %w{uuid}.each do |property|
-          property_value = occurrence.send property
-          if !property_value.nil?
-            event[property] = property_value
-          end
-        end
-
-        # Direct mappings from occurrence.
-        %w{
-          fingerprint event_class event_class_key event_key event_group agent
-          syslog_facility nt_event_code monitor
-        }.each do |property|
-          property_value = occurrence.send property
-          if !property_value.nil?
-            event[property] = property_value
-          end
-        end
-
-        # Enum Mappings.
-        event["severity"] = zep::EventSeverity.constants[occurrence.severity]
-
-        if !occurrence.status.nil?
-          event["status"] = zep::EventStatus.constants[occurrence.status]
-        end
-
-        if !occurrence.syslog_priority.nil?
-          event["syslog_priority"] = zep::SyslogPriority.constants[
-            occurrence.syslog_priority]
-        end
-
-        # Extra Details.
-        if !occurrence.details.nil?
-          occurrence.details.each do |detail|
-            if detail.value.length == 1
-              event[detail.name] = detail.value[0]
-            else
-              event[detail.name] = detail.value
-            end
-          end
-        end
-
-        queue << event
-      end # @queue.subscribe
-
-    rescue *[Bunny::ConnectionError, Bunny::ServerDownError] => e
-      @logger.error("RabbitMQ connection error, will reconnect: #{e}")
-      # Sleep for a bit before retrying.
-      # TODO(sissel): Write 'backoff' method?
-      sleep(1)
-      retry
-    end # begin/rescue
-  end # def run
-
-end # class LogStash::Inputs::Zenoss
diff --git a/lib/logstash/outputs/boundary.rb b/lib/logstash/outputs/boundary.rb
deleted file mode 100644
index cde4a694a48..00000000000
--- a/lib/logstash/outputs/boundary.rb
+++ /dev/null
@@ -1,116 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-class LogStash::Outputs::Boundary < LogStash::Outputs::Base
-  # This output lets you send annotations to
-  # Boundary based on Logstash events
-  #
-  # Note that since Logstash maintains no state
-  # these will be one-shot events
-  #
-  # By default the start and stop time will be
-  # the event timestamp
-  #
-
-  config_name "boundary"
-  milestone 1
-
-  # Your Boundary API key
-  config :api_key, :validate => :string, :required => true
-
-  # Your Boundary Org ID
-  config :org_id, :validate => :string, :required => true
-
-  # Start time
-  # Override the start time
-  # Note that Boundary requires this to be seconds since epoch
-  # If overriding, it is your responsibility to type this correctly
-  # By default this is set to `event["@timestamp"].to_i`
-  config :start_time, :validate => :string
-
-  # End time
-  # Override the stop time
-  # Note that Boundary requires this to be seconds since epoch
-  # If overriding, it is your responsibility to type this correctly
-  # By default this is set to `event["@timestamp"].to_i`
-  config :end_time, :validate => :string
-
-  # Type
-  config :btype, :validate => :string
-
-  # Sub-Type
-  config :bsubtype, :validate => :string
-
-  # Tags
-  # Set any custom tags for this event
-  # Default are the Logstash tags if any
-  config :btags, :validate => :array
-
-  # Auto
-  # If set to true, logstash will try to pull boundary fields out
-  # of the event. Any field explicitly set by config options will
-  # override these.
-  # ['type', 'subtype', 'creation_time', 'end_time', 'links', 'tags', 'loc']
-  config :auto, :validate => :boolean, :default => false
-
-  public
-  def register
-    require "net/https"
-    require "uri"
-    @url = "https://api.boundary.com/#{@org_id}/annotations"
-    @uri = URI.parse(@url)
-    @client = Net::HTTP.new(@uri.host, @uri.port)
-    @client.use_ssl = true
-    # Boundary cert doesn't verify
-    @client.verify_mode = OpenSSL::SSL::VERIFY_NONE
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    boundary_event = Hash.new
-    boundary_keys = ['type', 'subtype', 'creation_time', 'end_time', 'links', 'tags', 'loc']
-
-    boundary_event['start_time'] = event.sprintf(@start_time) if @start_time
-    boundary_event['end_time'] = event.sprintf(@end_time) if @end_time
-    boundary_event['type'] = event.sprintf(@btype) if @btype
-    boundary_event['subtype'] = event.sprintf(@bsubtype) if @bsubtype
-    boundary_event['tags'] = @btags.collect { |x| event.sprintf(x) } if @btags
-
-    if @auto
-      boundary_fields = event['@fields'].select { |k| boundary_keys.member? k }
-      boundary_event = boundary_fields.merge boundary_event
-    end
-
-    boundary_event = {
-      'type' => event.sprintf("%{message}"),
-      'subtype' => event.sprintf("%{type}"),
-      'start_time' => event["@timestamp"].to_i,
-      'end_time' => event["@timestamp"].to_i,
-      'links' => [],
-      'tags' => event["tags"],
-    }.merge boundary_event
-
-    request = Net::HTTP::Post.new(@uri.path)
-    request.basic_auth(@api_key, '')
-
-    @logger.debug("Boundary event", :boundary_event => boundary_event)
-
-    begin
-      request.body = boundary_event.to_json
-      request.add_field("Content-Type", 'application/json')
-      response = @client.request(request)
-      @logger.warn("Boundary convo", :request => request.inspect, :response => response.inspect)
-      raise unless response.code == '201'
-    rescue Exception => e
-      @logger.warn(
-        "Unhandled exception",
-        :request => request.inspect,
-        :response => response.inspect,
-        :exception => e.inspect
-      )
-    end
-  end # def receive
-end
diff --git a/lib/logstash/outputs/circonus.rb b/lib/logstash/outputs/circonus.rb
deleted file mode 100644
index 1f8451a9f7c..00000000000
--- a/lib/logstash/outputs/circonus.rb
+++ /dev/null
@@ -1,78 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-class LogStash::Outputs::Circonus < LogStash::Outputs::Base
-  # This output lets you send annotations to
-  # Circonus based on Logstash events
-  # 
-
-  config_name "circonus"
-  milestone 1
-
-  # Your Circonus API Token
-  config :api_token, :validate => :string, :required => true
-
-  # Your Circonus App name
-  # This will be passed through `event.sprintf`
-  # so variables are allowed here:
-  #
-  # Example:
-  #  `app_name => "%{myappname}"`
-  config :app_name, :validate => :string, :required => true
-
-  # Annotations
-  # Registers an annotation with Circonus
-  # The only required field is `title` and `description`.
-  # `start` and `stop` will be set to `event["@timestamp"]`
-  # You can add any other optional annotation values as well.
-  # All values will be passed through `event.sprintf`
-  #
-  # Example:
-  #   ["title":"Logstash event", "description":"Logstash event for %{host}"]
-  # or
-  #   ["title":"Logstash event", "description":"Logstash event for %{host}", "parent_id", "1"]
-  config :annotation, :validate => :hash, :required => true, :default => {}
-
-  public
-  def register
-    require "net/https"
-    require "uri"
-    @url = "https://circonus.com/api/json/"
-    @uri = URI.parse(@url)
-    @client = Net::HTTP.new(@uri.host, @uri.port)
-    @client.use_ssl = true
-    @client.verify_mode = OpenSSL::SSL::VERIFY_NONE
-    
-  end # def register
-
-  public
-  def receive(event)
-    # TODO (lusis)
-    # batch and flush
-    return unless output?(event)
-   
-    annotation_event = Hash[*@annotation.collect{|k,v| [event.sprintf(k),event.sprintf(v)]}.flatten]
-    @logger.warn("Annotation event", :data => annotation_event)
-  
-    annotation_array = []
-    annotation_path = "#{@uri.path}annotation"
-    @logger.warn("Annotation path", :data => annotation_path)
-    request = Net::HTTP::Post.new(annotation_path)
-    annotation_event['start'] = event["@timestamp"].to_i unless annotation_event['start']
-    annotation_event['stop'] = event["@timestamp"].to_i unless annotation_event['stop']
-    @logger.warn("Annotation event", :data => annotation_event)
-    annotation_array << annotation_event
-    begin
-      request.set_form_data(:annotations => annotation_array.to_json)
-      @logger.warn(annotation_event)
-      request.add_field("X-Circonus-Auth-Token", "#{@api_token}")
-      request.add_field("X-Circonus-App-Name", "#{event.sprintf(@app_name)}")
-      response = @client.request(request)
-      @logger.warn("Circonus convo", :request => request.inspect, :response => response.inspect)
-      raise unless response.code == '200'
-    rescue Exception => e
-      @logger.warn("Unhandled exception", :request => request.inspect, :response => response.inspect, :exception => e.inspect)
-    end
-  end # def receive
-end
diff --git a/lib/logstash/outputs/datadog.rb b/lib/logstash/outputs/datadog.rb
deleted file mode 100644
index 87936921abf..00000000000
--- a/lib/logstash/outputs/datadog.rb
+++ /dev/null
@@ -1,93 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-class LogStash::Outputs::Datadog < LogStash::Outputs::Base
-  # This output lets you send events (for now. soon metrics) to
-  # DataDogHQ based on Logstash events
-  #
-  # Note that since Logstash maintains no state
-  # these will be one-shot events
-  #
-
-  config_name "datadog"
-  milestone 1
-
-  # Your DatadogHQ API key
-  config :api_key, :validate => :string, :required => true
-
-  # Title
-  config :title, :validate => :string, :default => "Logstash event for %{host}"
-
-  # Text
-  config :text, :validate => :string, :default => "%{message}"
-
-  # Date Happened
-  config :date_happened, :validate => :string
-
-  # Source type name
-  config :source_type_name, :validate => ["nagios", "hudson", "jenkins", "user", "my apps", "feed", "chef", "puppet", "git", "bitbucket", "fabric", "capistrano"], :default => "my apps"
- 
-  # Alert type
-  config :alert_type, :validate => ["info", "error", "warning", "success"]
-
-  # Priority
-  config :priority, :validate => ["normal", "low"]
-
-  # Tags
-  # Set any custom tags for this event
-  # Default are the Logstash tags if any
-  config :dd_tags, :validate => :array
-
-  public
-  def register
-    require "net/https"
-    require "uri"
-    @url = "https://app.datadoghq.com/api/v1/events"
-    @uri = URI.parse(@url)
-    @client = Net::HTTP.new(@uri.host, @uri.port)
-    @client.use_ssl = true
-    @client.verify_mode = OpenSSL::SSL::VERIFY_NONE
-    @logger.debug("Client", :client => @client.inspect)
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-
-    dd_event = Hash.new
-    dd_event['title'] = event.sprintf(@title)
-    dd_event['text'] = event.sprintf(@text)
-    dd_event['source_type_name'] = @source_type_name
-    dd_event['alert_type'] = @alert_type if @alert_type
-    dd_event['priority'] = @priority if @priority
-
-    if @date_happened
-      dd_event['date_happened'] = event.sprintf(@date_happened)
-    else
-      dd_event['date_happened'] = event["@timestamp"].to_i
-    end
-
-    if @dd_tags
-      tagz = @dd_tags.collect {|x| event.sprintf(x) }
-    else
-      tagz = event["tags"]
-    end
-    dd_event['tags'] = tagz if tagz
-
-    @logger.debug("DataDog event", :dd_event => dd_event)
-
-    request = Net::HTTP::Post.new("#{@uri.path}?api_key=#{@api_key}")
-    
-    begin
-      request.body = dd_event.to_json
-      request.add_field("Content-Type", 'application/json')
-      response = @client.request(request)
-      @logger.info("DD convo", :request => request.inspect, :response => response.inspect)
-      raise unless response.code == '200'
-    rescue Exception => e
-      @logger.warn("Unhandled exception", :request => request.inspect, :response => response.inspect, :exception => e.inspect)
-    end
-  end # def receive
-end
diff --git a/lib/logstash/outputs/datadog_metrics.rb b/lib/logstash/outputs/datadog_metrics.rb
deleted file mode 100644
index 8e0729ebcf4..00000000000
--- a/lib/logstash/outputs/datadog_metrics.rb
+++ /dev/null
@@ -1,123 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "stud/buffer"
-
-# This output lets you send metrics to
-# DataDogHQ based on Logstash events.
-
-# Default queue_size and timeframe are low in order to provide near realtime alerting.
-# If you do not use Datadog for alerting, consider raising these thresholds.
-
-class LogStash::Outputs::DatadogMetrics < LogStash::Outputs::Base
-
-  include Stud::Buffer
-
-  config_name "datadog_metrics"
-  milestone 1
-
-  # Your DatadogHQ API key. https://app.datadoghq.com/account/settings#api
-  config :api_key, :validate => :string, :required => true
-
-  # The name of the time series.
-  config :metric_name, :validate => :string, :default => "%{metric_name}"
-
-  # The value.
-  config :metric_value, :default => "%{metric_value}"
-
-  # The type of the metric.
-  config :metric_type, :validate => ["gauge", "counter", "%{metric_type}"], :default => "%{metric_type}"
-
-  # The name of the host that produced the metric.
-  config :host, :validate => :string, :default => "%{host}"
-
-  # The name of the device that produced the metric.
-  config :device, :validate => :string, :default => "%{metric_device}"
-
-  # Set any custom tags for this event,
-  # default are the Logstash tags if any.
-  config :dd_tags, :validate => :array
-
-  # How many events to queue before flushing to Datadog
-  # prior to schedule set in @timeframe
-  config :queue_size, :validate => :number, :default => 10
-
-  # How often (in seconds) to flush queued events to Datadog
-  config :timeframe, :validate => :number, :default => 10
-
-  public
-  def register
-    require 'time'
-    require "net/https"
-    require "uri"
-
-    @url = "https://app.datadoghq.com/api/v1/series"
-    @uri = URI.parse(@url)
-    @client = Net::HTTP.new(@uri.host, @uri.port)
-    @client.use_ssl = true
-    @client.verify_mode = OpenSSL::SSL::VERIFY_NONE
-    @logger.debug("Client", :client => @client.inspect)
-    buffer_initialize(
-      :max_items => @queue_size,
-      :max_interval => @timeframe,
-      :logger => @logger
-    )
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-    return unless @metric_name && @metric_value && @metric_type
-    return unless ["gauge", "counter"].include? event.sprintf(@metric_type)
-
-    dd_metrics = Hash.new
-    dd_metrics['metric'] = event.sprintf(@metric_name)
-    dd_metrics['points'] = [[to_epoch(event.timestamp), event.sprintf(@metric_value).to_f]]
-    dd_metrics['type'] = event.sprintf(@metric_type)
-    dd_metrics['host'] = event.sprintf(@host)
-    dd_metrics['device'] = event.sprintf(@device)
-
-    if @dd_tags
-      tagz = @dd_tags.collect {|x| event.sprintf(x) }
-    else
-      tagz = event["tags"]
-    end
-    dd_metrics['tags'] = tagz if tagz
-
-    @logger.info("Queueing event", :event => dd_metrics)
-    buffer_receive(dd_metrics)
-  end # def receive
-
-  public
-  def flush(events, final=false)
-    dd_series = Hash.new
-    dd_series['series'] = []
-
-    events.each do |event|
-      begin
-        dd_series['series'] << event
-      rescue
-        @logger.warn("Error adding event to series!", :exception => e)
-        next
-      end
-    end
-
-    request = Net::HTTP::Post.new("#{@uri.path}?api_key=#{@api_key}")
-
-    begin
-      request.body = dd_series.to_json
-      request.add_field("Content-Type", 'application/json')
-      response = @client.request(request)
-      @logger.info("DD convo", :request => request.inspect, :response => response.inspect)
-      raise unless response.code == '202'
-    rescue Exception => e
-      @logger.warn("Unhandled exception", :request => request.inspect, :response => response.inspect, :exception => e.inspect)
-    end
-  end # def flush
-
-  private
-  def to_epoch(t)
-    return t.is_a?(Time) ? t.to_i : Time.parse(t).to_i
-  end # def to_epoch
-
-end # class LogStash::Outputs::DatadogMetrics
diff --git a/lib/logstash/outputs/gemfire.rb b/lib/logstash/outputs/gemfire.rb
deleted file mode 100644
index 8c103189213..00000000000
--- a/lib/logstash/outputs/gemfire.rb
+++ /dev/null
@@ -1,103 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# Push events to a GemFire region.
-#
-# GemFire is an object database.
-#
-# To use this plugin you need to add gemfire.jar to your CLASSPATH;
-# using format=json requires jackson.jar too.
-#
-# Note: this plugin has only been tested with GemFire 7.0.
-#
-class LogStash::Outputs::Gemfire < LogStash::Outputs::Base
-
-  config_name "gemfire"
-  milestone 1
-
-  # Your client cache name
-  config :cache_name, :validate => :string, :default => "logstash"
-
-  # The path to a GemFire client cache XML file.
-  #
-  # Example:
-  #
-  #      <client-cache>
-  #        <pool name="client-pool">
-  #            <locator host="localhost" port="31331"/>
-  #        </pool>
-  #        <region name="Logstash">
-  #            <region-attributes refid="CACHING_PROXY" pool-name="client-pool" >
-  #            </region-attributes>
-  #        </region>
-  #      </client-cache>
-  #
-  config :cache_xml_file, :validate => :string, :default => nil
-
-  # The region name
-  config :region_name, :validate => :string, :default => "Logstash"
-
-  # A sprintf format to use when building keys
-  config :key_format, :validate => :string, :default => "%{host}-%{@timestamp}"
-
-  public
-  def register
-    import com.gemstone.gemfire.cache.client.ClientCacheFactory
-    import com.gemstone.gemfire.pdx.JSONFormatter
-
-    @logger.info("Registering output", :plugin => self)
-    connect
-  end # def register
-
-  public
-  def connect
-    begin
-      @logger.debug("Connecting to GemFire #{@cache_name}")
-
-      @cache = ClientCacheFactory.new.
-        set("name", @cache_name).
-        set("cache-xml-file", @cache_xml_file).create
-      @logger.debug("Created cache #{@cache.inspect}")
-
-    rescue => e
-      if terminating?
-        return
-      else
-        @logger.error("Gemfire connection error (during connect), will reconnect",
-                      :exception => e, :backtrace => e.backtrace)
-        sleep(1)
-        retry
-      end
-    end
-
-    @region = @cache.getRegion(@region_name);
-    @logger.debug("Created region #{@region.inspect}")
-  end # def connect
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    @logger.debug("Sending event", :destination => to_s, :event => event)
-
-    key = event.sprintf @key_format
-
-    message = JSONFormatter.fromJSON(event.to_json)
-
-    @logger.debug("Publishing message", { :destination => to_s, :message => message, :key => key })
-    @region.put(key, message)
-  end # def receive
-
-  public
-  def to_s
-    return "gemfire://#{cache_name}"
-  end
-
-  public
-  def teardown
-    @cache.close if @cache
-    @cache = nil
-    finished
-  end # def teardown
-end # class LogStash::Outputs::Gemfire
diff --git a/lib/logstash/outputs/google_bigquery.rb b/lib/logstash/outputs/google_bigquery.rb
deleted file mode 100644
index b4375f53109..00000000000
--- a/lib/logstash/outputs/google_bigquery.rb
+++ /dev/null
@@ -1,570 +0,0 @@
-# Author: Rodrigo De Castro <rdc@google.com>
-# Date: 2013-09-20
-#
-# Copyright 2013 Google Inc.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# Summary: plugin to upload log events to Google BigQuery (BQ), rolling
-# files based on the date pattern provided as a configuration setting. Events
-# are written to files locally and, once file is closed, this plugin uploads
-# it to the configured BigQuery dataset.
-#
-# VERY IMPORTANT:
-# 1 - To make good use of BigQuery, your log events should be parsed and
-# structured. Consider using grok to parse your events into fields that can
-# be uploaded to BQ.
-# 2 - You must configure your plugin so it gets events with the same structure,
-# so the BigQuery schema suits them. In case you want to upload log events
-# with different structures, you can utilize multiple configuration blocks,
-# separating different log events with Logstash conditionals. More details on
-# Logstash conditionals can be found here:
-# http://logstash.net/docs/1.2.1/configuration#conditionals
-#
-# For more info on Google BigQuery, please go to:
-# https://developers.google.com/bigquery/
-#
-# In order to use this plugin, a Google service account must be used. For
-# more information, please refer to:
-# https://developers.google.com/storage/docs/authentication#service_accounts
-#
-# Recommendations:
-
-# a - Experiment with the settings depending on how much log data you generate,
-# your needs to see "fresh" data, and how much data you could lose in the event
-# of crash. For instance, if you want to see recent data in BQ quickly, you
-# could configure the plugin to upload data every minute or so (provided you
-# have enough log events to justify that). Note also, that if uploads are too
-# frequent, there is no guarantee that they will be imported in the same order,
-# so later data may be available before earlier data.
-
-# b - BigQuery charges for storage and for queries, depending on how much data
-# it reads to perform a query. These are other aspects to consider when
-# considering the date pattern which will be used to create new tables and also
-# how to compose the queries when using BQ. For more info on BigQuery Pricing,
-# please access:
-# https://developers.google.com/bigquery/pricing
-#
-# USAGE:
-# This is an example of logstash config:
-#
-# output {
-#    google_bigquery {
-#      project_id => "folkloric-guru-278"                        (required)
-#      dataset => "logs"                                         (required)
-#      csv_schema => "path:STRING,status:INTEGER,score:FLOAT"    (required)
-#      key_path => "/path/to/privatekey.p12"                     (required)
-#      key_password => "notasecret"                              (optional)
-#      service_account => "1234@developer.gserviceaccount.com"   (required)
-#      temp_directory => "/tmp/logstash-bq"                      (optional)
-#      temp_file_prefix => "logstash_bq"                         (optional)
-#      date_pattern => "%Y-%m-%dT%H:00"                          (optional)
-#      flush_interval_secs => 2                                  (optional)
-#      uploader_interval_secs => 60                              (optional)
-#      deleter_interval_secs => 60                               (optional)
-#    }
-# }
-#
-# Improvements TODO list:
-# - Refactor common code between Google BQ and GCS plugins.
-# - Turn Google API code into a Plugin Mixin (like AwsConfig).
-# - There's no recover method, so if logstash/plugin crashes, files may not
-# be uploaded to BQ.
-class LogStash::Outputs::GoogleBigQuery < LogStash::Outputs::Base
-  config_name "google_bigquery"
-  milestone 1
-
-  # Google Cloud Project ID (number, not Project Name!).
-  config :project_id, :validate => :string, :required => true
-
-  # BigQuery dataset to which these events will be added to.
-  config :dataset, :validate => :string, :required => true
-
-  # BigQuery table ID prefix to be used when creating new tables for log data.
-  # Table name will be <table_prefix>_<date>
-  config :table_prefix, :validate => :string, :default => "logstash"
-
-  # Schema for log data. It must follow this format:
-  # <field1-name>:<field1-type>,<field2-name>:<field2-type>,...
-  # Example: path:STRING,status:INTEGER,score:FLOAT
-  config :csv_schema, :validate => :string, :required => true
-
-  # Path to private key file for Google Service Account.
-  config :key_path, :validate => :string, :required => true
-
-  # Private key password for service account private key.
-  config :key_password, :validate => :string, :default => "notasecret"
-
-  # Service account to access Google APIs.
-  config :service_account, :validate => :string, :required => true
-
-  # Directory where temporary files are stored.
-  # Defaults to /tmp/logstash-bq-<random-suffix>
-  config :temp_directory, :validate => :string, :default => ""
-
-  # Temporary local file prefix. Log file will follow the format:
-  # <prefix>_hostname_date.part?.log
-  config :temp_file_prefix, :validate => :string, :default => "logstash_bq"
-
-  # Time pattern for BigQuery table, defaults to hourly tables.
-  # Must Time.strftime patterns: www.ruby-doc.org/core-2.0/Time.html#method-i-strftime
-  config :date_pattern, :validate => :string, :default => "%Y-%m-%dT%H:00"
-
-  # Flush interval in seconds for flushing writes to log files. 0 will flush
-  # on every message.
-  config :flush_interval_secs, :validate => :number, :default => 2
-
-  # Uploader interval when uploading new files to BigQuery. Adjust time based
-  # on your time pattern (for example, for hourly files, this interval can be
-  # around one hour).
-  config :uploader_interval_secs, :validate => :number, :default => 60
-
-  # Deleter interval when checking if upload jobs are done for file deletion.
-  # This only affects how long files are on the hard disk after the job is done.
-  config :deleter_interval_secs, :validate => :number, :default => 60
-
-  public
-  def register
-    require 'csv'
-    require "fileutils"
-    require "thread"
-
-    @logger.debug("BQ: register plugin")
-
-    @fields = Array.new
-
-    CSV.parse(@csv_schema.gsub('\"', '""')).flatten.each do |field|
-      temp = field.strip.split(":")
-
-      # Check that the field in the schema follows the format (<name>:<value>)
-      if temp.length != 2
-        raise "BigQuery schema must follow the format <field-name>:<field-value>"
-      end
-
-      @fields << { "name" => temp[0], "type" => temp[1] }
-    end
-
-    # Check that we have at least one field in the schema
-    if @fields.length == 0
-      raise "BigQuery schema must contain at least one field"
-    end
-
-    @json_schema = { "fields" => @fields }
-
-    @upload_queue = Queue.new
-    @delete_queue = Queue.new
-    @last_flush_cycle = Time.now
-    initialize_temp_directory()
-    initialize_current_log()
-    initialize_google_client()
-    initialize_uploader()
-    initialize_deleter()
-  end
-
-  # Method called for each log event. It writes the event to the current output
-  # file, flushing depending on flush interval configuration.
-  public
-  def receive(event)
-    return unless output?(event)
-
-    @logger.debug("BQ: receive method called", :event => event)
-
-    # Message must be written as json
-    message = event.to_json
-    # Remove "@" from property names
-    message = message.gsub(/\"@(\w+)\"/, '"\1"')
-
-    new_base_path = get_base_path()
-
-    # Time to roll file based on the date pattern? Or are we due to upload it to BQ?
-    if (@current_base_path != new_base_path || Time.now - @last_file_time >= @uploader_interval_secs)
-      @logger.debug("BQ: log file will be closed and uploaded",
-                    :filename => File.basename(@temp_file.to_path),
-                    :size => @temp_file.size.to_s,
-                    :uploader_interval_secs => @uploader_interval_secs.to_s)
-      # Close alone does not guarantee that data is physically written to disk,
-      # so flushing it before.
-      @temp_file.fsync()
-      @temp_file.close()
-      initialize_next_log()
-    end
-
-    @temp_file.write(message)
-    @temp_file.write("\n")
-
-    sync_log_file()
-
-    @logger.debug("BQ: event appended to log file",
-                  :filename => File.basename(@temp_file.to_path))
-  end
-
-  public
-  def teardown
-    @logger.debug("BQ: teardown method called")
-
-    @temp_file.flush()
-    @temp_file.close()
-  end
-
-  private
-  ##
-  # Flushes temporary log file every flush_interval_secs seconds or so.
-  # This is triggered by events, but if there are no events there's no point
-  # flushing files anyway.
-  #
-  # Inspired by lib/logstash/outputs/file.rb (flush(fd), flush_pending_files)
-  def sync_log_file
-    if flush_interval_secs <= 0
-      @temp_file.fsync
-      return
-    end
-
-    return unless Time.now - @last_flush_cycle >= flush_interval_secs
-    @temp_file.fsync
-    @logger.debug("BQ: flushing file",
-                  :path => @temp_file.to_path,
-                  :fd => @temp_file)
-    @last_flush_cycle = Time.now
-  end
-
-  ##
-  # Creates temporary directory, if it does not exist.
-  #
-  # A random suffix is appended to the temporary directory
-  def initialize_temp_directory
-    if @temp_directory.empty?
-      require "stud/temporary"
-      @temp_directory = Stud::Temporary.directory("logstash-bq")
-      @logger.info("BQ: temporary directory generated",
-                   :directory => @temp_directory)
-    end
-
-    if !(File.directory? @temp_directory)
-      @logger.debug("BQ: directory doesn't exist. Creating it.",
-                    :directory => @temp_directory)
-      FileUtils.mkdir_p(@temp_directory)
-    end
-  end
-
-  ##
-  # Starts thread to delete uploaded log files once their jobs are done.
-  #
-  # Deleter is done in a separate thread, not holding the receive method above.
-  def initialize_deleter
-    @uploader = Thread.new do
-      @logger.debug("BQ: starting deleter")
-      while true
-        delete_item = @delete_queue.pop
-        job_id = delete_item["job_id"]
-        filename = delete_item["filename"]
-        job_status = get_job_status(job_id)
-        case job_status["state"]
-        when "DONE"
-          if job_status.has_key?("errorResult")
-            @logger.error("BQ: job failed, please enable debug and check full "\
-                          "response (probably the issue is an incompatible "\
-                          "schema). NOT deleting local file.",
-                          :job_id => job_id,
-                          :filename => filename,
-                          :job_status => job_status)
-          else
-            @logger.debug("BQ: job is done, deleting local temporary file ",
-                          :job_id => job_id,
-                          :filename => filename,
-                          :job_status => job_status)
-            File.delete(filename)
-          end
-        when "PENDING", "RUNNING"
-          @logger.debug("BQ: job is not done, NOT deleting local file yet.",
-                        :job_id => job_id,
-                        :filename => filename,
-                        :job_status => job_status)
-          @delete_queue << delete_item
-        else
-          @logger.error("BQ: unknown job status, please enable debug and "\
-                        "check full response (probably the issue is an "\
-                        "incompatible schema). NOT deleting local file yet.",
-                        :job_id => job_id,
-                        :filename => filename,
-                        :job_status => job_status)
-        end
-
-        sleep @deleter_interval_secs
-      end
-    end
-  end
-
-  ##
-  # Starts thread to upload log files.
-  #
-  # Uploader is done in a separate thread, not holding the receive method above.
-  def initialize_uploader
-    @uploader = Thread.new do
-      @logger.debug("BQ: starting uploader")
-      while true
-        filename = @upload_queue.pop
-
-        # Reenqueue if it is still the current file.
-        if filename == @temp_file.to_path
-          if @current_base_path == get_base_path()
-            if Time.now - @last_file_time < @uploader_interval_secs
-              @logger.debug("BQ: reenqueue as log file is being currently appended to.",
-                            :filename => filename)
-              @upload_queue << filename
-              # If we got here, it means that older files were uploaded, so let's
-              # wait another minute before checking on this file again.
-              sleep @uploader_interval_secs
-              next
-            else
-              @logger.debug("BQ: flush and close file to be uploaded.",
-                            :filename => filename)
-              @temp_file.flush()
-              @temp_file.close()
-              initialize_next_log()
-            end
-          end
-        end
-
-        if File.size(filename) > 0
-          job_id = upload_object(filename)
-          @delete_queue << { "filename" => filename, "job_id" => job_id }
-        else
-          @logger.debug("BQ: skipping empty file.")
-          @logger.debug("BQ: delete local temporary file ",
-                        :filename => filename)
-          File.delete(filename)
-        end
-
-        sleep @uploader_interval_secs
-      end
-    end
-  end
-
-  ##
-  # Returns undated path used to construct base path and final full path.
-  # This path only includes directory, prefix, and hostname info.
-  def get_undated_path
-    return @temp_directory + File::SEPARATOR + @temp_file_prefix + "_" +
-      Socket.gethostname()
-  end
-
-  ##
-  # Returns base path to log file that is invariant regardless of any
-  # user options.
-  def get_base_path
-    return get_undated_path() + "_" + Time.now.strftime(@date_pattern)
-  end
-
-  ##
-  # Returns full path to the log file based on global variables (like
-  # current_base_path) and configuration options (max file size).
-  def get_full_path
-    return @current_base_path + ".part" + ("%03d" % @size_counter) + ".log"
-  end
-
-  ##
-  # Returns date from a temporary log file name.
-  def get_date_pattern(filename)
-    match = /^#{get_undated_path()}_(?<date>.*)\.part(\d+)\.log$/.match(filename)
-    return match[:date]
-  end
-
-  ##
-  # Returns latest part number for a base path. This method checks all existing
-  # log files in order to find the highest part number, so this file can be used
-  # for appending log events.
-  #
-  # Only applicable if max file size is enabled.
-  def get_latest_part_number(base_path)
-    part_numbers = Dir.glob(base_path + ".part*.log").map do |item|
-      match = /^.*\.part(?<part_num>\d+).log$/.match(item)
-      next if match.nil?
-      match[:part_num].to_i
-    end
-
-    return part_numbers.max if part_numbers.any?
-    0
-  end
-
-  ##
-  # Opens current log file and updates @temp_file with an instance of IOWriter.
-  # This method also adds file to the upload queue.
-  def open_current_file()
-    path = get_full_path()
-    stat = File.stat(path) rescue nil
-    if stat and stat.ftype == "fifo" and RUBY_PLATFORM == "java"
-      fd = java.io.FileWriter.new(java.io.File.new(path))
-    else
-      fd = File.new(path, "a")
-    end
-    @temp_file = IOWriter.new(fd)
-    @upload_queue << @temp_file.to_path
-  end
-
-  ##
-  # Opens log file on plugin initialization, trying to resume from an existing
-  # file. If max file size is enabled, find the highest part number and resume
-  # from it.
-  def initialize_current_log
-    @current_base_path = get_base_path
-    @last_file_time = Time.now
-    @size_counter = get_latest_part_number(@current_base_path)
-    @logger.debug("BQ: resuming from latest part.",
-                  :part => @size_counter)
-    open_current_file()
-  end
-
-  ##
-  # Generates new log file name based on configuration options and opens log
-  # file. If max file size is enabled, part number if incremented in case the
-  # the base log file name is the same (e.g. log file was not rolled given the
-  # date pattern).
-  def initialize_next_log
-    new_base_path = get_base_path
-    @size_counter = @current_base_path == new_base_path ? @size_counter + 1 : 0
-    @logger.debug("BQ: opening next log file.",
-                  :filename => @current_base_path,
-                  :part => @size_counter)
-    @current_base_path = new_base_path
-    @last_file_time = Time.now
-    open_current_file()
-  end
-
-  ##
-  # Initializes Google Client instantiating client and authorizing access.
-  def initialize_google_client
-    require "google/api_client"
-    require "openssl"
-
-    @client = Google::APIClient.new(:application_name =>
-                                    'Logstash Google BigQuery output plugin',
-                                    :application_version => '0.1')
-    @bq = @client.discovered_api('bigquery', 'v2')
-
-
-    key = Google::APIClient::PKCS12.load_key(@key_path, @key_password)
-    # Authorization scope reference:
-    # https://developers.google.com/bigquery/docs/authorization
-    service_account = Google::APIClient::JWTAsserter.new(@service_account,
-                                                         'https://www.googleapis.com/auth/bigquery',
-                                                         key)
-    @client.authorization = service_account.authorize
-  end
-
-  ##
-  # Uploads a local file to the configured bucket.
-  def get_job_status(job_id)
-    begin
-      require 'json'
-      @logger.debug("BQ: check job status.",
-                    :job_id => job_id)
-      get_result = @client.execute(:api_method => @bq.jobs.get,
-                                   :parameters => {
-                                     'jobId' => job_id,
-                                     'projectId' => @project_id
-                                   })
-      response = JSON.parse(get_result.response.body)
-      @logger.debug("BQ: successfully invoked API.",
-                    :response => response)
-
-      if response.has_key?("error")
-        raise response["error"]
-      end
-
-      # Successful invocation
-      contents = response["status"]
-      return contents
-    rescue => e
-      @logger.error("BQ: failed to check status", :exception => e)
-      # TODO(rdc): limit retries?
-      sleep 1
-      retry
-    end
-  end
-
-  ##
-  # Uploads a local file to the configured bucket.
-  def upload_object(filename)
-    begin
-      require 'json'
-      table_id = @table_prefix + "_" + get_date_pattern(filename)
-      # BQ does not accept anything other than alphanumeric and _
-      # Ref: https://developers.google.com/bigquery/browser-tool-quickstart?hl=en
-      table_id = table_id.gsub!(':','_').gsub!('-', '_')
-
-      @logger.debug("BQ: upload object.",
-                    :filename => filename,
-                    :table_id => table_id)
-      media = Google::APIClient::UploadIO.new(filename, "application/octet-stream")
-      body = {
-        "configuration" => {
-          "load" => {
-            "sourceFormat" => "NEWLINE_DELIMITED_JSON",
-            "schema" => @json_schema,
-            "destinationTable"  =>  {
-              "projectId" => @project_id,
-              "datasetId" => @dataset,
-              "tableId" => table_id
-            },
-            'createDisposition' => 'CREATE_IF_NEEDED',
-            'writeDisposition' => 'WRITE_APPEND'
-          }
-        }
-      }
-      insert_result = @client.execute(:api_method => @bq.jobs.insert,
-                                      :body_object => body,
-                                      :parameters => {
-                                        'uploadType' => 'multipart',
-                                        'projectId' => @project_id
-                                      },
-                                      :media => media)
-
-      job_id = JSON.parse(insert_result.response.body)["jobReference"]["jobId"]
-      @logger.debug("BQ: multipart insert",
-                    :job_id => job_id)
-      return job_id
-    rescue => e
-      @logger.error("BQ: failed to upload file", :exception => e)
-      # TODO(rdc): limit retries?
-      sleep 1
-      retry
-    end
-  end
-end
-
-##
-# Wrapper class that abstracts which IO being used (for instance, regular
-# files or GzipWriter.
-#
-# Inspired by lib/logstash/outputs/file.rb.
-class IOWriter
-  def initialize(io)
-    @io = io
-  end
-  def write(*args)
-    @io.write(*args)
-  end
-  def flush
-    @io.flush
-  end
-  def method_missing(method_name, *args, &block)
-    if @io.respond_to?(method_name)
-      @io.send(method_name, *args, &block)
-    else
-      super
-    end
-  end
-  attr_accessor :active
-end
diff --git a/lib/logstash/outputs/google_cloud_storage.rb b/lib/logstash/outputs/google_cloud_storage.rb
deleted file mode 100644
index 5765f31167b..00000000000
--- a/lib/logstash/outputs/google_cloud_storage.rb
+++ /dev/null
@@ -1,431 +0,0 @@
-# encoding: utf-8
-# Author: Rodrigo De Castro <rdc@google.com>
-# Date: 2013-09-20
-#
-# Copyright 2013 Google Inc.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "zlib"
-
-# Summary: plugin to upload log events to Google Cloud Storage (GCS), rolling
-# files based on the date pattern provided as a configuration setting. Events
-# are written to files locally and, once file is closed, this plugin uploads
-# it to the configured bucket.
-#
-# For more info on Google Cloud Storage, please go to:
-# https://cloud.google.com/products/cloud-storage
-#
-# In order to use this plugin, a Google service account must be used. For
-# more information, please refer to:
-# https://developers.google.com/storage/docs/authentication#service_accounts
-#
-# Recommendation: experiment with the settings depending on how much log
-# data you generate, so the uploader can keep up with the generated logs.
-# Using gzip output can be a good option to reduce network traffic when
-# uploading the log files and in terms of storage costs as well.
-#
-# USAGE:
-# This is an example of logstash config:
-#
-# output {
-#    google_cloud_storage {
-#      bucket => "my_bucket"                                     (required)
-#      key_path => "/path/to/privatekey.p12"                     (required)
-#      key_password => "notasecret"                              (optional)
-#      service_account => "1234@developer.gserviceaccount.com"   (required)
-#      temp_directory => "/tmp/logstash-gcs"                     (optional)
-#      log_file_prefix => "logstash_gcs"                         (optional)
-#      max_file_size_kbytes => 1024                              (optional)
-#      output_format => "plain"                                  (optional)
-#      date_pattern => "%Y-%m-%dT%H:00"                          (optional)
-#      flush_interval_secs => 2                                  (optional)
-#      gzip => false                                             (optional)
-#      uploader_interval_secs => 60                              (optional)
-#    }
-# }
-#
-# Improvements TODO list:
-# - Support logstash event variables to determine filename.
-# - Turn Google API code into a Plugin Mixin (like AwsConfig).
-# - There's no recover method, so if logstash/plugin crashes, files may not
-# be uploaded to GCS.
-# - Allow user to configure file name.
-# - Allow parallel uploads for heavier loads (+ connection configuration if
-# exposed by Ruby API client)
-class LogStash::Outputs::GoogleCloudStorage < LogStash::Outputs::Base
-  config_name "google_cloud_storage"
-  milestone 1
-
-  # GCS bucket name, without "gs://" or any other prefix.
-  config :bucket, :validate => :string, :required => true
-
-  # GCS path to private key file.
-  config :key_path, :validate => :string, :required => true
-
-  # GCS private key password.
-  config :key_password, :validate => :string, :default => "notasecret"
-
-  # GCS service account.
-  config :service_account, :validate => :string, :required => true
-
-  # Directory where temporary files are stored.
-  # Defaults to /tmp/logstash-gcs-<random-suffix>
-  config :temp_directory, :validate => :string, :default => ""
-
-  # Log file prefix. Log file will follow the format:
-  # <prefix>_hostname_date<.part?>.log
-  config :log_file_prefix, :validate => :string, :default => "logstash_gcs"
-
-  # Sets max file size in kbytes. 0 disable max file check.
-  config :max_file_size_kbytes, :validate => :number, :default => 10000
-
-  # The event format you want to store in files. Defaults to plain text.
-  config :output_format, :validate => [ "json", "plain" ], :default => "plain"
-
-  # Time pattern for log file, defaults to hourly files.
-  # Must Time.strftime patterns: www.ruby-doc.org/core-2.0/Time.html#method-i-strftime
-  config :date_pattern, :validate => :string, :default => "%Y-%m-%dT%H:00"
-
-  # Flush interval in seconds for flushing writes to log files. 0 will flush
-  # on every message.
-  config :flush_interval_secs, :validate => :number, :default => 2
-
-  # Gzip output stream when writing events to log files.
-  config :gzip, :validate => :boolean, :default => false
-
-  # Uploader interval when uploading new files to GCS. Adjust time based
-  # on your time pattern (for example, for hourly files, this interval can be
-  # around one hour).
-  config :uploader_interval_secs, :validate => :number, :default => 60
-
-  public
-  def register
-    require "fileutils"
-    require "thread"
-
-    @logger.debug("GCS: register plugin")
-
-    @upload_queue = Queue.new
-    @last_flush_cycle = Time.now
-    initialize_temp_directory()
-    initialize_current_log()
-    initialize_google_client()
-    initialize_uploader()
-
-    if @gzip
-      @content_type = 'application/gzip'
-    else
-      @content_type = 'text/plain'
-    end
-  end
-
-  # Method called for each log event. It writes the event to the current output
-  # file, flushing depending on flush interval configuration.
-  public
-  def receive(event)
-    return unless output?(event)
-
-    @logger.debug("GCS: receive method called", :event => event)
-
-    if (@output_format == "json")
-      message = event.to_json
-    else
-      message = event.to_s
-    end
-
-    new_base_path = get_base_path()
-
-    # Time to roll file based on the date pattern? Or is it over the size limit?
-    if (@current_base_path != new_base_path || (@max_file_size_kbytes > 0 && @temp_file.size >= @max_file_size_kbytes * 1024))
-      @logger.debug("GCS: log file will be closed and uploaded",
-                    :filename => File.basename(@temp_file.to_path),
-                    :size => @temp_file.size.to_s,
-                    :max_size => @max_file_size_kbytes.to_s)
-      # Close does not guarantee that data is physically written to disk.
-      @temp_file.fsync()
-      @temp_file.close()
-      initialize_next_log()
-    end
-
-    @temp_file.write(message)
-    @temp_file.write("\n")
-
-    sync_log_file()
-
-    @logger.debug("GCS: event appended to log file",
-                  :filename => File.basename(@temp_file.to_path))
-  end
-
-  public
-  def teardown
-    @logger.debug("GCS: teardown method called")
-
-    @temp_file.fsync()
-    @temp_file.close()
-  end
-
-  private
-  ##
-  # Flushes temporary log file every flush_interval_secs seconds or so.
-  # This is triggered by events, but if there are no events there's no point
-  # flushing files anyway.
-  #
-  # Inspired by lib/logstash/outputs/file.rb (flush(fd), flush_pending_files)
-  def sync_log_file
-    if flush_interval_secs <= 0
-      @temp_file.fsync()
-      return
-    end
-
-    return unless Time.now - @last_flush_cycle >= flush_interval_secs
-    @temp_file.fsync()
-    @logger.debug("GCS: flushing file",
-                  :path => @temp_file.to_path,
-                  :fd => @temp_file)
-    @last_flush_cycle = Time.now
-  end
-
-  ##
-  # Creates temporary directory, if it does not exist.
-  #
-  # A random suffix is appended to the temporary directory
-  def initialize_temp_directory
-    require "stud/temporary"
-    if @temp_directory.empty?
-      @temp_directory = Stud::Temporary.directory("logstash-gcs")
-      @logger.info("GCS: temporary directory generated",
-                   :directory => @temp_directory)
-    end
-
-    if !(File.directory? @temp_directory)
-      @logger.debug("GCS: directory doesn't exist. Creating it.",
-                    :directory => @temp_directory)
-      FileUtils.mkdir_p(@temp_directory)
-    end
-  end
-
-  ##
-  # Starts thread to upload log files.
-  #
-  # Uploader is done in a separate thread, not holding the receive method above.
-  def initialize_uploader
-    @uploader = Thread.new do
-      @logger.debug("GCS: starting uploader")
-      while true
-        filename = @upload_queue.pop
-
-        # Reenqueue if it is still the current file.
-        if filename == @temp_file.to_path
-          if @current_base_path == get_base_path()
-            @logger.debug("GCS: reenqueue as log file is being currently appended to.",
-                          :filename => filename)
-            @upload_queue << filename
-            # If we got here, it means that older files were uploaded, so let's
-            # wait another minute before checking on this file again.
-            sleep @uploader_interval_secs
-            next
-          else
-            @logger.debug("GCS: flush and close file to be uploaded.",
-                          :filename => filename)
-            @temp_file.fsync()
-            @temp_file.close()
-            initialize_next_log()
-          end
-        end
-
-        upload_object(filename)
-        @logger.debug("GCS: delete local temporary file ",
-                      :filename => filename)
-        File.delete(filename)
-        sleep @uploader_interval_secs
-      end
-    end
-  end
-
-  ##
-  # Returns base path to log file that is invariant regardless of whether
-  # max file or gzip options.
-  def get_base_path
-    return @temp_directory + File::SEPARATOR + @log_file_prefix + "_" +
-      Socket.gethostname() + "_" + Time.now.strftime(@date_pattern)
-  end
-
-  ##
-  # Returns log file suffix, which will vary depending on whether gzip is
-  # enabled.
-  def get_suffix
-    return @gzip ? ".log.gz" : ".log"
-  end
-
-  ##
-  # Returns full path to the log file based on global variables (like
-  # current_base_path) and configuration options (max file size and gzip
-  # enabled).
-  def get_full_path
-    if @max_file_size_kbytes > 0
-      return @current_base_path + ".part" + ("%03d" % @size_counter) + get_suffix()
-    else
-      return @current_base_path + get_suffix()
-    end
-  end
-
-  ##
-  # Returns latest part number for a base path. This method checks all existing
-  # log files in order to find the highest part number, so this file can be used
-  # for appending log events.
-  #
-  # Only applicable if max file size is enabled.
-  def get_latest_part_number(base_path)
-    part_numbers = Dir.glob(base_path + ".part*" + get_suffix()).map do |item|
-      match = /^.*\.part(?<part_num>\d+)#{get_suffix()}$/.match(item)
-      next if match.nil?
-      match[:part_num].to_i
-    end
-
-    return part_numbers.max if part_numbers.any?
-    0
-  end
-
-  ##
-  # Opens current log file and updates @temp_file with an instance of IOWriter.
-  # This method also adds file to the upload queue.
-  def open_current_file()
-    path = get_full_path()
-    stat = File.stat(path) rescue nil
-    if stat and stat.ftype == "fifo" and RUBY_PLATFORM == "java"
-      fd = java.io.FileWriter.new(java.io.File.new(path))
-    else
-      fd = File.new(path, "a")
-    end
-    if @gzip
-      fd = Zlib::GzipWriter.new(fd)
-    end
-    @temp_file = GCSIOWriter.new(fd)
-    @upload_queue << @temp_file.to_path
-  end
-
-  ##
-  # Opens log file on plugin initialization, trying to resume from an existing
-  # file. If max file size is enabled, find the highest part number and resume
-  # from it.
-  def initialize_current_log
-    @current_base_path = get_base_path
-    if @max_file_size_kbytes > 0
-      @size_counter = get_latest_part_number(@current_base_path)
-      @logger.debug("GCS: resuming from latest part.",
-                    :part => @size_counter)
-    end
-    open_current_file()
-  end
-
-  ##
-  # Generates new log file name based on configuration options and opens log
-  # file. If max file size is enabled, part number if incremented in case the
-  # the base log file name is the same (e.g. log file was not rolled given the
-  # date pattern).
-  def initialize_next_log
-    new_base_path = get_base_path
-    if @max_file_size_kbytes > 0
-      @size_counter = @current_base_path == new_base_path ? @size_counter + 1 : 0
-      @logger.debug("GCS: opening next log file.",
-                    :filename => @current_base_path,
-                    :part => @size_counter)
-    else
-      @logger.debug("GCS: opening next log file.",
-                    :filename => @current_base_path)
-    end
-    @current_base_path = new_base_path
-    open_current_file()
-  end
-
-  ##
-  # Initializes Google Client instantiating client and authorizing access.
-  def initialize_google_client
-    require "google/api_client"
-    require "openssl"
-
-    @client = Google::APIClient.new(:application_name =>
-                                    'Logstash Google Cloud Storage output plugin',
-                                    :application_version => '0.1')
-    @storage = @client.discovered_api('storage', 'v1beta1')
-
-    key = Google::APIClient::PKCS12.load_key(@key_path, @key_password)
-    service_account = Google::APIClient::JWTAsserter.new(@service_account,
-                                                         'https://www.googleapis.com/auth/devstorage.read_write',
-                                                         key)
-    @client.authorization = service_account.authorize
-  end
-
-  ##
-  # Uploads a local file to the configured bucket.
-  def upload_object(filename)
-    begin
-      @logger.debug("GCS: upload object.", :filename => filename)
-
-      media = Google::APIClient::UploadIO.new(filename, @content_type)
-      metadata_insert_result = @client.execute(:api_method => @storage.objects.insert,
-                                               :parameters => {
-                                                 'uploadType' => 'multipart',
-                                                 'bucket' => @bucket,
-                                                 'name' => File.basename(filename)
-                                               },
-                                               :body_object => {contentType: @content_type},
-                                               :media => media)
-      contents = metadata_insert_result.data
-      @logger.debug("GCS: multipart insert",
-                    :object => contents.name,
-                    :self_link => contents.self_link)
-    rescue => e
-      @logger.error("GCS: failed to upload file", :exception => e)
-      # TODO(rdc): limit retries?
-      sleep 1
-      retry
-    end
-  end
-end
-
-##
-# Wrapper class that abstracts which IO being used (for instance, regular
-# files or GzipWriter.
-#
-# Inspired by lib/logstash/outputs/file.rb.
-class GCSIOWriter
-  def initialize(io)
-    @io = io
-  end
-  def write(*args)
-    @io.write(*args)
-  end
-  def fsync
-    if @io.class == Zlib::GzipWriter
-      @io.flush
-      @io.to_io.fsync
-    else
-      @io.fsync
-    end
-  end
-  def method_missing(method_name, *args, &block)
-    if @io.respond_to?(method_name)
-      @io.send(method_name, *args, &block)
-    else
-      if @io.class == Zlib::GzipWriter && @io.to_io.respond_to?(method_name)
-        @io.to_io.send(method_name, *args, &block)
-      else
-        super
-      end
-    end
-  end
-  attr_accessor :active
-end
diff --git a/lib/logstash/outputs/graphtastic.rb b/lib/logstash/outputs/graphtastic.rb
deleted file mode 100644
index afad0154a53..00000000000
--- a/lib/logstash/outputs/graphtastic.rb
+++ /dev/null
@@ -1,185 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# A plugin for a newly developed Java/Spring Metrics application
-# I didn't really want to code this project but I couldn't find
-# a respectable alternative that would also run on any Windows
-# machine - which is the problem and why I am not going with Graphite
-# and statsd.  This application provides multiple integration options
-# so as to make its use under your network requirements possible. 
-# This includes a REST option that is always enabled for your use
-# in case you want to write a small script to send the occasional 
-# metric data. 
-#
-# Find GraphTastic here : https://github.com/NickPadilla/GraphTastic
-class LogStash::Outputs::GraphTastic < LogStash::Outputs::Base
-  
-  config_name "graphtastic"
-  milestone 2
-  
-  # options are udp(fastest - default) - rmi(faster) - rest(fast) - tcp(don't use TCP yet - some problems - errors out on linux)
-  config :integration, :validate => ["udp","tcp","rmi","rest"], :default => "udp"
-  
-  # if using rest as your end point you need to also provide the application url
-  # it defaults to localhost/graphtastic.  You can customize the application url
-  # by changing the name of the .war file.  There are other ways to change the 
-  # application context, but they vary depending on the Application Server in use.
-  # Please consult your application server documentation for more on application
-  # contexts.
-  config :context, :validate => :string, :default => "graphtastic"
-  
-  # metrics hash - you will provide a name for your metric and the metric 
-  # data as key value pairs.  so for example:
-  #
-  # metrics => { "Response" => "%{response}" } 
-  #
-  # example for the logstash config
-  #
-  # metrics => [ "Response", "%{response}" ]
-  #
-  # NOTE: you can also use the dynamic fields for the key value as well as the actual value
-  config :metrics, :validate => :hash, :default => {}
-   
-  # host for the graphtastic server - defaults to 127.0.0.1
-  config :host, :validate => :string, :default => "127.0.0.1"
-  
-  # port for the graphtastic instance - defaults to 1199 for RMI, 1299 for TCP, 1399 for UDP, and 8080 for REST
-  config :port, :validate => :number
-  
-  # number of attempted retry after send error - currently only way to integrate
-  # errored transactions - should try and save to a file or later consumption
-  # either by graphtastic utility or by this program after connectivity is
-  # ensured to be established. 
-  config :retries, :validate => :number, :default => 1
-  
-  # the number of metrics to send to GraphTastic at one time. 60 seems to be the perfect 
-  # amount for UDP, with default packet size. 
-  config :batch_number, :validate => :number, :default => 60
-  
-  # setting allows you to specify where we save errored transactions
-  # this makes the most sense at this point - will need to decide
-  # on how we reintegrate these error metrics
-  # NOT IMPLEMENTED!
-  config :error_file, :validate => :string, :default => ""
-  
-  public
-   def register
-     @batch = []
-     begin
-       if @integration.downcase == "rmi"
-         if RUBY_ENGINE != "jruby"
-            raise Exception.new("LogStash::Outputs::GraphTastic# JRuby is needed for RMI to work!")
-         end
-         require "java"
-         if @port.nil?
-           @port = 1199
-         end
-         registry = java.rmi.registry.LocateRegistry.getRegistry(@host, @port);
-         @remote = registry.lookup("RmiMetricService")
-       elsif @integration.downcase == "rest"
-         require "net/http"         
-         if @port.nil?
-           @port = 8080
-           gem "mail" #outputs/email, # License: MIT License
-         end
-         @http = Net::HTTP.new(@host, @port)
-       end
-       @logger.info("GraphTastic Output Successfully Registered! Using #{@integration} Integration!")
-     rescue 
-       @logger.error("*******ERROR :  #{$!}")
-     end
-   end
-
-  public
-  def receive(event)
-    return unless output?(event)
-    # Set Intersection - returns a new array with the items that are the same between the two
-    if !@tags.empty? && (event["tags"] & @tags).size == 0
-       # Skip events that have no tags in common with what we were configured
-       @logger.debug("No Tags match for GraphTastic Output!")
-       return
-    end
-    @retry = 1
-    @logger.debug("Event found for GraphTastic!", :tags => @tags, :event => event)
-    @metrics.each do |name, metric|
-      postMetric(event.sprintf(name),event.sprintf(metric),(event["@timestamp"]*1000))# unix_timestamp is what I need in seconds - multiply by 1000 to make milliseconds.
-    end
-  end
-  
-  def postMetric(name, metric, timestamp)
-    message = name+","+metric+","+timestamp.to_s
-    if @batch.length < @batch_number
-      @batch.push(message)
-    else
-      flushMetrics()      
-    end    
-  end
-  
-  def flushMetrics()
-    begin
-      if @integration.downcase == "tcp"
-        flushViaTCP()
-      elsif @integration.downcase == "rmi"
-        flushViaRMI() 
-      elsif @integration.downcase == "udp"
-        flushViaUDP()
-      elsif @integration.downcase == "rest"
-        flushViaREST()
-      else
-        @logger.error("GraphTastic Not Able To Find Correct Integration - Nothing Sent - Integration Type : ", :@integration => @integration)
-      end
-      @batch.clear
-    rescue
-      @logger.error("*******ERROR :  #{$!}")
-      @logger.info("*******Attempting #{@retry} out of #{@retries}")
-      while @retry < @retries
-        @retry = @retry + 1
-        flushMetrics()
-      end
-    end
-  end
-  
-  # send metrics via udp
-  def flushViaUDP()
-    if @port.nil?
-     @port = 1399
-    end
-    udpsocket.send(@batch.join(','), 0, @host, @port)
-    @logger.debug("GraphTastic Sent Message Using UDP : #{@batch.join(',')}")
-  end
-  
-  # send metrics via REST
-  def flushViaREST()
-    request = Net::HTTP::Put.new("/#{@context}/addMetric/#{@batch.join(',')}")
-    response = @http.request(request)
-    if response == 'ERROR'
-      raise 'Error happend when sending metric to GraphTastic using REST!'
-    end
-    @logger.debug("GraphTastic Sent Message Using REST : #{@batch.join(',')}", :response => response.inspect)    
-  end
-  
-  # send metrics via RMI
-  def flushViaRMI()
-    if RUBY_ENGINE != "jruby"
-       raise Exception.new("LogStash::Outputs::GraphTastic# JRuby is needed for RMI to work!")
-    end
-    @remote.insertMetrics(@batch.join(','))
-    @logger.debug("GraphTastic Sent Message Using RMI : #{@batch.join(',')}")
-  end
-  
-  # send metrics via tcp
-  def flushViaTCP()
-    # to correctly read the line we need to ensure we send \r\n at the end of every message.
-    if @port.nil?
-      @port = 1299
-    end
-    tcpsocket = TCPSocket.open(@host, @port)
-    tcpsocket.send(@batch.join(',')+"\r\n", 0)
-    tcpsocket.close
-    @logger.debug("GraphTastic Sent Message Using TCP : #{@batch.join(',')}")
-  end
-
-  def udpsocket; @socket ||= UDPSocket.new end
-  
-end
diff --git a/lib/logstash/outputs/jira.rb b/lib/logstash/outputs/jira.rb
deleted file mode 100644
index 2cc533043d4..00000000000
--- a/lib/logstash/outputs/jira.rb
+++ /dev/null
@@ -1,109 +0,0 @@
-# encoding: utf-8
-# Origin https://groups.google.com/forum/#!msg/logstash-users/exgrB4iQ-mw/R34apku5nXsJ
-# and https://botbot.me/freenode/logstash/msg/4169496/ 
-# via https://gist.github.com/electrical/4660061e8fff11cdcf37#file-jira-rb
-
-# Uses jiralicious as the bridge to JIRA
-# By Martin Cleaver, Blended Perspectives
-# with a lot of help from 'electrical' in #logstash
-
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-#
-# This is so is most useful so you can use logstash to parse and structure
-# your logs and ship structured, json events to JIRA
-#
-# To use this, you'll need to ensure your JIRA instance allows REST calls
-
-class LogStash::Outputs::Jira < LogStash::Outputs::Base
-  config_name "jira"
-  milestone 2
-
-  # The hostname to send logs to. This should target your JIRA server 
-  # and has to have the REST interface enabled
-  config :host, :validate => :string
-
-  config :username, :validate => :string, :required => true
-  config :password, :validate => :string, :required => true
-
-  # Javalicious has no proxy support
-###
-  # JIRA Project number
-  config :projectid, :validate => :string, :required => true
-
-  # JIRA Issuetype number
-  config :issuetypeid, :validate => :string, :required => true
-
-  # JIRA Summary
-  config :summary, :validate => :string, :required => true
-
-  # JIRA Priority
-  config :priority, :validate => :string, :required => true
-
-  # JIRA Reporter
-  config :reporter, :validate => :string
-
-  # JIRA Reporter
-  config :assignee, :validate => :string
-
-### The following have not been implemented
-  # Ticket creation method
-  #config :method, :validate => :string, :default => 'new'
-  
-  # Search fields; When in 'append' method. search for a ticket that has these fields and data.
-  #config :searchfields, :validate => :hash
-  
-  # createfields; Add data to these fields at initial creation
-  #config :createfields, :validate => :hash
-  
-  # appendfields; Update data in these fields when appending data to an existing ticket
-  #config :appendfields, :validate => :hash
-  
-  # Comment; Add this in the comment field ( is for new and append method the same )
-  #config :comment, :validate => :string
-
-
-  public
-  def register
-    require "jiralicious" # 0.2.2 works for me
-    # nothing to do
-  end
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    if event == LogStash::SHUTDOWN
-      finished
-      return
-    end
-
-    Jiralicious.configure do |config|
-      config.username = @username
-      config.password = @password
-      config.uri = @host
-      config.auth_type = :cookie
-      config.api_version = "latest"
-    end
-
-
-issue = Jiralicious::Issue.new
-    issue.fields.set_id("project", @projectid) # would have prefered a project key, https://github.com/jstewart/jiralicious/issues/16
-    issue.fields.set("summary", @summary)
-    issue.fields.set_id("issuetype", @issuetypeid)
-    issue.fields.set_name("reporter", @reporter)
-    issue.fields.set_name("assignee", @assignee)
-    issue.fields.set_id("priority", @priority)
-#puts issue.fields.to_yaml
-    issue.save
-
-
-
-#    if response.is_a?(Net::HTTPSuccess)
-#      @logger.info("Event send to JIRA OK!")
-#    else
-#      @logger.warn("HTTP error", :error => response.error!)
-#    end
-  end # def receive
-end # class LogStash::Outputs::Jira
diff --git a/lib/logstash/outputs/librato.rb b/lib/logstash/outputs/librato.rb
deleted file mode 100644
index 09f9e6129a7..00000000000
--- a/lib/logstash/outputs/librato.rb
+++ /dev/null
@@ -1,146 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-class LogStash::Outputs::Librato < LogStash::Outputs::Base
-  # This output lets you send metrics, annotations and alerts to
-  # Librato based on Logstash events
-  # 
-  # This is VERY experimental and inefficient right now.
-
-  config_name "librato"
-  milestone 1
-
-  # Your Librato account
-  # usually an email address
-  config :account_id, :validate => :string, :required => true
-
-  # Your Librato API Token
-  config :api_token, :validate => :string, :required => true
-
-  # Gauges
-  # Send data to Librato as a gauge
-  #
-  # Example:
-  #   ["value", "%{bytes_recieved}", "source", "%{host}", "name", "apache_bytes"]
-  # Additionally, you can override the `measure_time` for the event. Must be a unix timestamp:
-  #   ["value", "%{bytes_recieved}", "source", "%{host}", "name", "apache_bytes","measure_time", "%{my_unixtime_field}]
-  # Default is to use the event's timestamp
-  config :gauge, :validate => :hash, :default => {}
-
-  # Counters
-  # Send data to Librato as a counter
-  #
-  # Example:
-  #   ["value", "1", "source", "%{host}", "name", "messages_received"]
-  # Additionally, you can override the `measure_time` for the event. Must be a unix timestamp:
-  #   ["value", "1", "source", "%{host}", "name", "messages_received", "measure_time", "%{my_unixtime_field}"]
-  # Default is to use the event's timestamp
-  config :counter, :validate => :hash, :default => {}
-
-  # Annotations
-  # Registers an annotation with Librato
-  # The only required field is `title` and `name`.
-  # `start_time` and `end_time` will be set to `event["@timestamp"].to_i`
-  # You can add any other optional annotation values as well.
-  # All values will be passed through `event.sprintf`
-  #
-  # Example:
-  #   ["title":"Logstash event on %{host}", "name":"logstash_stream"]
-  # or
-  #   ["title":"Logstash event", "description":"%{message}", "name":"logstash_stream"]
-  config :annotation, :validate => :hash, :default => {}
-
-  # Batch size
-  # Number of events to batch up before sending to Librato.
-  #
-  config :batch_size, :validate => :string, :default => "10"
-
-  public
-  def register
-    require "net/https"
-    require "uri"
-    @url = "https://metrics-api.librato.com/v1/"
-    @uri = URI.parse(@url)
-    @client = Net::HTTP.new(@uri.host, @uri.port)
-    @client.use_ssl = true
-    @client.verify_mode = OpenSSL::SSL::VERIFY_NONE
-    
-  end # def register
-
-  public
-  def receive(event)
-    # TODO (lusis)
-    # batch and flush
-    return unless output?(event)
-
-    metrics_event = Hash.new
-    unless @gauge.size == 0
-      g_hash = Hash[*@gauge.collect{|k,v| [k,event.sprintf(v)]}.flatten]
-      g_hash.each do |k,v|
-        g_hash[k] = v.to_f if k=="value"
-      end
-      g_hash['measure_time'] = event["@timestamp"].to_i unless g_hash['measure_time']
-      @logger.warn("Gauges hash", :data => g_hash)
-      metrics_event['gauges'] = Array.new 
-      metrics_event['gauges'] << g_hash
-      @logger.warn("Metrics hash", :data => metrics_event)
-    end
-    unless @counter.size == 0
-      c_hash = Hash[*@counter.collect{|k,v| [k,event.sprintf(v)]}.flatten]
-      c_hash.each do |k,v|
-        c_hash[k] = v.to_f if k=="value"
-      end
-      c_hash['measure_time'] = event["@timestamp"].to_i unless c_hash['measure_time']
-      @logger.warn("Counters hash", :data => c_hash)
-      metrics_event['counters'] = Array.new
-      metrics_event['counters'] << c_hash
-      @logger.warn("Metrics hash", :data => metrics_event)
-    end
-   
-    # TODO (lusis)
-    # Clean this mess up
-    unless metrics_event.size == 0
-      request = Net::HTTP::Post.new(@uri.path+"metrics")
-      request.basic_auth(@account_id, @api_token)
-      
-      begin
-        request.body = metrics_event.to_json
-        request.add_field("Content-Type", 'application/json')
-        response = @client.request(request)
-        @logger.warn("Librato convo", :request => request.inspect, :response => response.inspect)
-        raise unless response.code == '200'
-      rescue Exception => e
-        @logger.warn("Unhandled exception", :request => request.inspect, :response => response.inspect, :exception => e.inspect)
-      end
-    end
-
-    unless @annotation.size == 0
-      annotation_hash = Hash.new
-      annotation_hash['annotations'] = Array.new
-      @logger.warn("Original Annotation", :data => @annotation)
-      annotation_event = Hash[*@annotation.collect{|k,v| [event.sprintf(k),event.sprintf(v)]}.flatten]
-      @logger.warn("Annotation event", :data => annotation_event)
-      
-      annotation_path = "#{@uri.path}annotations/#{annotation_event['name']}"
-      @logger.warn("Annotation path", :data => annotation_path)
-      request = Net::HTTP::Post.new(annotation_path)
-      request.basic_auth(@account_id, @api_token)
-      annotation_event.delete('name')
-      annotation_event['start_time'] = event["@timestamp"].to_i unless annotation_event['start_time']
-      annotation_event['end_time'] = event["@timestamp"].to_i unless annotation_event['end_time']
-      annotation_hash['annotations'] << annotation_event
-      @logger.warn("Annotation event", :data => annotation_event)
-
-      begin
-        request.body = annotation_event.to_json
-        request.add_field("Content-Type", 'application/json')
-        response = @client.request(request)
-        @logger.warn("Librato convo", :request => request.inspect, :response => response.inspect)
-        raise unless response.code == '201'
-      rescue Exception => e
-        @logger.warn("Unhandled exception", :request => request.inspect, :response => response.inspect, :exception => e.inspect)
-      end
-    end
-  end # def receive
-end
diff --git a/lib/logstash/outputs/loggly.rb b/lib/logstash/outputs/loggly.rb
deleted file mode 100644
index 3546fd92313..00000000000
--- a/lib/logstash/outputs/loggly.rb
+++ /dev/null
@@ -1,99 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "uri"
-# TODO(sissel): Move to something that performs better than net/http
-require "net/http"
-require "net/https"
-
-# Ugly monkey patch to get around <http://jira.codehaus.org/browse/JRUBY-5529>
-Net::BufferedIO.class_eval do
-    BUFSIZE = 1024 * 16
-
-    def rbuf_fill
-      timeout(@read_timeout) {
-        @rbuf << @io.sysread(BUFSIZE)
-      }
-    end
-end
-
-# Got a loggly account? Use logstash to ship logs to Loggly!
-#
-# This is most useful so you can use logstash to parse and structure
-# your logs and ship structured, json events to your account at Loggly.
-#
-# To use this, you'll need to use a Loggly input with type 'http'
-# and 'json logging' enabled.
-class LogStash::Outputs::Loggly < LogStash::Outputs::Base
-  config_name "loggly"
-  milestone 2
-
-  # The hostname to send logs to. This should target the loggly http input
-  # server which is usually "logs.loggly.com"
-  config :host, :validate => :string, :default => "logs.loggly.com"
-
-  # The loggly http input key to send to.
-  # This is usually visible in the Loggly 'Inputs' page as something like this
-  #     https://logs.hoover.loggly.net/inputs/abcdef12-3456-7890-abcd-ef0123456789
-  #                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-  #                                           \---------->   key   <-------------/
-  #
-  # You can use %{foo} field lookups here if you need to pull the api key from
-  # the event. This is mainly aimed at multitenant hosting providers who want
-  # to offer shipping a customer's logs to that customer's loggly account.
-  config :key, :validate => :string, :required => true
-
-  # Should the log action be sent over https instead of plain http
-  config :proto, :validate => :string, :default => "http"
-
-  # Proxy Host
-  config :proxy_host, :validate => :string
-
-  # Proxy Port
-  config :proxy_port, :validate => :number
-
-  # Proxy Username
-  config :proxy_user, :validate => :string
-
-  # Proxy Password
-  config :proxy_password, :validate => :password, :default => ""
-
-
-  public
-  def register
-    # nothing to do
-  end
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    if event == LogStash::SHUTDOWN
-      finished
-      return
-    end
-
-    # Send the event over http.
-    url = URI.parse("#{@proto}://#{@host}/inputs/#{event.sprintf(@key)}")
-    @logger.info("Loggly URL", :url => url)
-    http = Net::HTTP::Proxy(@proxy_host, @proxy_port, @proxy_user, @proxy_password.value).new(url.host, url.port)
-    if url.scheme == 'https'
-      http.use_ssl = true
-      http.verify_mode = OpenSSL::SSL::VERIFY_NONE
-    end
-    request = Net::HTTP::Post.new(url.path)
-    request.body = event.to_json
-    begin
-      response = http.request(request)
-    rescue Exception => e
-      @logger.warn("Unhandled exception", :request => request, :response => response, :exception => e, :stacktrace => e.backtrace)
-      return
-    end
-
-    if response.is_a?(Net::HTTPSuccess)
-      @logger.info("Event send to Loggly OK!")
-    else
-      @logger.warn("HTTP error", :error => response.error!)
-    end
-  end # def receive
-end # class LogStash::Outputs::Loggly
diff --git a/lib/logstash/outputs/metriccatcher.rb b/lib/logstash/outputs/metriccatcher.rb
deleted file mode 100644
index 31e04ec4e21..00000000000
--- a/lib/logstash/outputs/metriccatcher.rb
+++ /dev/null
@@ -1,103 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "json"
-
-# This output ships metrics to MetricCatcher, allowing you to
-# utilize Coda Hale's Metrics.
-#
-# More info on MetricCatcher: https://github.com/clearspring/MetricCatcher
-#
-# At Clearspring, we use it to count the response codes from Apache logs:
-#     metriccatcher {
-#         host => "localhost"
-#         port => "1420"
-#         type => "apache-access"
-#         fields => [ "response" ]
-#         meter => [ "%{host}.apache.response.%{response}", "1" ]
-#     }
-class LogStash::Outputs::MetricCatcher < LogStash::Outputs::Base
-  config_name "metriccatcher"
-  milestone 2
-
-  # The address of the MetricCatcher
-  config :host, :validate => :string, :default => "localhost"
-  # The port to connect on your MetricCatcher
-  config :port, :validate => :number, :default => 1420
-
-  # The metrics to send. This supports dynamic strings like %{host}
-  # for metric names and also for values. This is a hash field with key
-  # of the metric name, value of the metric value.
-  #
-  # The value will be coerced to a floating point value. Values which cannot be
-  # coerced will zero (0)
-  config :gauge, :validate => :hash
-
-  # The metrics to send. This supports dynamic strings like %{host}
-  # for metric names and also for values. This is a hash field with key
-  # of the metric name, value of the metric value. Example:
-  #
-  #   counter => [ "%{host}.apache.hits.%{response}, "1" ]
-  #
-  # The value will be coerced to a floating point value. Values which cannot be
-  # coerced will zero (0)
-  config :counter, :validate => :hash
-
-  # The metrics to send. This supports dynamic strings like %{host}
-  # for metric names and also for values. This is a hash field with key
-  # of the metric name, value of the metric value.
-  #
-  # The value will be coerced to a floating point value. Values which cannot be
-  # coerced will zero (0)
-  config :meter, :validate => :hash
-
-  # The metrics to send. This supports dynamic strings like %{host}
-  # for metric names and also for values. This is a hash field with key
-  # of the metric name, value of the metric value.
-  #
-  # The value will be coerced to a floating point value. Values which cannot be
-  # coerced will zero (0)
-  config :biased, :validate => :hash
-
-  # The metrics to send. This supports dynamic strings like %{host}
-  # for metric names and also for values. This is a hash field with key
-  # of the metric name, value of the metric value.
-  #
-  # The value will be coerced to a floating point value. Values which cannot be
-  # coerced will zero (0)
-  config :uniform, :validate => :hash
-
-  # The metrics to send. This supports dynamic strings like %{host}
-  # for metric names and also for values. This is a hash field with key
-  # of the metric name, value of the metric value. Example:
-  #
-  #   timer => [ "%{host}.apache.response_time, "%{response_time}" ]
-  #
-  # The value will be coerced to a floating point value. Values which cannot be
-  # coerced will zero (0)
-  config :timer, :validate => :hash
-
-  def register
-    @socket = UDPSocket.new
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    @@metric_types.each do |metric_type|
-      if instance_variable_defined?("@#{metric_type}")
-        instance_variable_get("@#{metric_type}").each do |metric_name, metric_value|
-          message = [{
-            "name"      => event.sprintf(metric_name),
-            "type"      => event.sprintf(metric_type),
-            "value"     => event.sprintf(metric_value).to_f,
-            "timestamp" => event.sprintf("%{+%s}.") + Time.now.usec.to_s
-          }]
-
-          @socket.send(message.to_json, 0, @host, @port)
-        end # instance_variable_get("@#{metric_type}").each_slice
-      end # if
-    end # @metric_types.each
-  end # def receive
-end # class LogStash::Outputs::MetricCatcher
diff --git a/lib/logstash/outputs/mongodb.rb b/lib/logstash/outputs/mongodb.rb
deleted file mode 100644
index 2768e9e7e9b..00000000000
--- a/lib/logstash/outputs/mongodb.rb
+++ /dev/null
@@ -1,81 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-class LogStash::Outputs::Mongodb < LogStash::Outputs::Base
-
-  config_name "mongodb"
-  milestone 2
-
-  # a MongoDB URI to connect to
-  # See http://docs.mongodb.org/manual/reference/connection-string/
-  config :uri, :validate => :string, :required => true
-  
-  # The database to use
-  config :database, :validate => :string, :required => true
-   
-  # The collection to use. This value can use %{foo} values to dynamically
-  # select a collection based on data in the event.
-  config :collection, :validate => :string, :required => true
-
-  # If true, store the @timestamp field in mongodb as an ISODate type instead
-  # of an ISO8601 string.  For more information about this, see
-  # http://www.mongodb.org/display/DOCS/Dates
-  config :isodate, :validate => :boolean, :default => false
-
-  # Number of seconds to wait after failure before retrying
-  config :retry_delay, :validate => :number, :default => 3, :required => false
-
-  # If true, a _id field will be added to the document before insertion.
-  # The _id field will use the timestamp of the event and overwrite an existing
-  # _id field in the event.
-  config :generateId, :validate => :boolean, :default => false
-
-  public
-  def register
-    require "mongo"
-    uriParsed=Mongo::URIParser.new(@uri)
-    conn = uriParsed.connection({})
-    if uriParsed.auths.length > 0
-      uriParsed.auths.each do |auth|
-        if !auth['db_name'].nil?
-          conn.add_auth(auth['db_name'], auth['username'], auth['password'], nil)
-        end 
-      end
-      conn.apply_saved_authentication()
-    end
-    @db = conn.db(@database)
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    begin
-      if @isodate
-        # the mongodb driver wants time values as a ruby Time object.
-        # set the @timestamp value of the document to a ruby Time object, then.
-        document = event.to_hash
-      else
-        document = event.to_hash.merge("@timestamp" => event["@timestamp"].to_json)
-      end
-      if @generateId
-        document['_id'] = BSON::ObjectId.new(nil, event["@timestamp"])
-      end
-      @db.collection(event.sprintf(@collection)).insert(document)
-    rescue => e
-      @logger.warn("Failed to send event to MongoDB", :event => event, :exception => e,
-                   :backtrace => e.backtrace)
-      if e.error_code == 11000
-          # On a duplicate key error, skip the insert.
-          # We could check if the duplicate key err is the _id key
-          # and generate a new primary key.
-          # If the duplicate key error is on another field, we have no way
-          # to fix the issue.
-      else
-        sleep @retry_delay
-        retry
-      end
-    end
-  end # def receive
-end # class LogStash::Outputs::Mongodb
diff --git a/lib/logstash/outputs/riak.rb b/lib/logstash/outputs/riak.rb
deleted file mode 100644
index b1c799580c5..00000000000
--- a/lib/logstash/outputs/riak.rb
+++ /dev/null
@@ -1,152 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# Riak is a distributed k/v store from Basho.
-# It's based on the Dynamo model.
-
-class LogStash::Outputs::Riak < LogStash::Outputs::Base
-  config_name "riak"
-  milestone 1
-
-  # The nodes of your Riak cluster
-  # This can be a single host or
-  # a Logstash hash of node/port pairs
-  # e.g
-  # ["node1", "8098", "node2", "8098"]
-  config :nodes, :validate => :hash, :default => {"localhost" =>  "8098"}
-
-  # The protocol to use
-  # HTTP or ProtoBuf
-  # Applies to ALL backends listed above
-  # No mix and match
-  config :proto, :validate => ["http", "pb"], :default => "http"
-
-  # The bucket name to write events to
-  # Expansion is supported here as values are 
-  # passed through event.sprintf
-  # Multiple buckets can be specified here
-  # but any bucket-specific settings defined
-  # apply to ALL the buckets.
-  config :bucket, :validate => :array, :default => ["logstash-%{+YYYY.MM.dd}"]
-
-  # The event key name
-  # variables are valid here.
-  #
-  # Choose this carefully. Best to let riak decide....
-  config :key_name, :validate => :string
-
-  # Bucket properties (NYI)
-  # Logstash hash of properties for the bucket
-  # i.e.
-  # `bucket_props => ["r", "one", "w", "one", "dw", "one"]`
-  # or
-  # `bucket_props => ["n_val", "3"]`
-  # Note that the Logstash config language cannot support
-  # hash or array values
-  # Properties will be passed as-is
-  config :bucket_props, :validate => :hash
-
-  # Indices
-  # Array of fields to add 2i on
-  # e.g.
-  # `indices => ["source_host", "type"]
-  # Off by default as not everyone runs eleveldb
-  config :indices, :validate => :array
-
-  # Search
-  # Enable search on the bucket defined above
-  config :enable_search, :validate => :boolean, :default => false
-
-  # SSL
-  # Enable SSL
-  config :enable_ssl, :validate => :boolean, :default => false
-
-  # SSL Options
-  # Options for SSL connections
-  # Only applied if SSL is enabled
-  # Logstash hash that maps to the riak-client options
-  # here: https://github.com/basho/riak-ruby-client/wiki/Connecting-to-Riak
-  # You'll likely want something like this:
-  # `ssl_opts => ["pem", "/etc/riak.pem", "ca_path", "/usr/share/certificates"]
-  # Per the riak client docs, the above sample options
-  # will turn on SSL `VERIFY_PEER`
-  config :ssl_opts, :validate => :hash
-
-  # Metadata (NYI)
-  # Allow the user to set custom metadata on the object
-  # Should consider converting logstash data to metadata as well
-  #
-
-  public
-  def register
-    require 'riak'
-    riak_opts = {}
-    cluster_nodes = Array.new
-    @logger.debug("Setting protocol", :protocol => @proto)
-    proto_type = "#{@proto}_port".to_sym
-    @nodes.each do |node,port|
-      @logger.debug("Adding node", :node => node, :port => port)
-      cluster_nodes << {:host => node, proto_type => port}
-    end
-    @logger.debug("Cluster nodes", :nodes => cluster_nodes)
-    if @enable_ssl
-      @logger.debug("SSL requested")
-      if @ssl_opts
-        @logger.debug("SSL options provided", @ssl_opts)
-        riak_opts.merge!(@ssl_opts.inject({}) {|h,(k,v)| h[k.to_sym] = v; h})
-      else
-        riak_opts.merge!({:ssl => true})
-      end
-    @logger.debug("Riak options:", :riak_opts => riak_opts)
-    end
-    riak_opts.merge!({:nodes => cluster_nodes})
-    @logger.debug("Riak options:", :riak_opts => riak_opts)
-    @client = Riak::Client.new(riak_opts)
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-    
-    @bucket.each do |b|
-      # setup our bucket(s)
-      bukkit = @client.bucket(event.sprintf(b))
-      # Disable bucket props for now
-      # Need to detect params passed that should be converted to int
-      # otherwise setting props fails =(
-      # Logstash syntax only supports strings and bools
-      # likely fix is to either hack in is_numeric?
-      # or whitelist certain params and call to_i
-      ##@logger.debug("Setting bucket props", :props => @bucket_props)
-      ##bukkit.props = @bucket_props if @bucket_props
-      ##@logger.debug("Bucket", :bukkit => bukkit.inspect)
-     
-      if @enable_search
-        @logger.debug("Enable search requested", :bucket => bukkit.inspect)
-        # Check if search is enabled
-        @logger.debug("Checking bucket status", :search_enabled => bukkit.is_indexed?)
-        bukkit.enable_index! unless bukkit.is_indexed?
-        @logger.debug("Rechecking bucket status", :search_enabled => bukkit.is_indexed?)
-      end
-      @key_name.nil? ? evt_key=nil : evt_key=event.sprintf(@key_name)
-      evt = Riak::RObject.new(bukkit, evt_key)
-      @logger.debug("RObject", :robject => evt.to_s)
-      begin
-        evt.content_type = "application/json"
-        evt.data = event
-        if @indices
-          @indices.each do |k|
-            idx_name = "#{k.gsub('@','')}_bin"
-            @logger.debug("Riak index name", :idx => idx_name)
-            @logger.info("Indexes", :indexes => evt.indexes.to_s)
-            evt.indexes[idx_name] << event.sprintf("%{#{k}}")
-          end
-        end
-        evt.store
-      rescue Exception => e
-        @logger.warn("Exception storing", :message => e.message)
-      end
-    end
-  end # def receive
-end # class LogStash::Outputs::Riak
diff --git a/lib/logstash/outputs/riemann.rb b/lib/logstash/outputs/riemann.rb
deleted file mode 100644
index 899c0ec26eb..00000000000
--- a/lib/logstash/outputs/riemann.rb
+++ /dev/null
@@ -1,109 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# Riemann is a network event stream processing system.
-#
-# While Riemann is very similar conceptually to Logstash, it has
-# much more in terms of being a monitoring system replacement.
-#
-# Riemann is used in Logstash much like statsd or other metric-related
-# outputs
-#
-# You can learn about Riemann here:
-#
-# * <http://riemann.io/>
-# You can see the author talk about it here:
-# * <http://vimeo.com/38377415>
-#
-class LogStash::Outputs::Riemann < LogStash::Outputs::Base
-  config_name "riemann"
-  milestone 1
-
-  # The address of the Riemann server.
-  config :host, :validate => :string, :default => "localhost"
-
-  # The port to connect to on your Riemann server.
-  config :port, :validate => :number, :default => 5555
-
-  # The protocol to use
-  # UDP is non-blocking
-  # TCP is blocking
-  #
-  # Logstash's default output behaviour
-  # is to never lose events
-  # As such, we use tcp as default here
-  config :protocol, :validate => ["tcp", "udp"], :default => "tcp"
-
-  # The name of the sender.
-  # This sets the `host` value
-  # in the Riemann event
-  config :sender, :validate => :string, :default => "%{host}"
-
-  # A Hash to set Riemann event fields
-  # (<http://riemann.io/concepts.html>).
-  #
-  # The following event fields are supported:
-  # `description`, `state`, `metric`, `ttl`, `service`
-  #
-  # Tags found on the Logstash event will automatically be added to the
-  # Riemann event.
-  #
-  # Any other field set here will be passed to Riemann as an event attribute.
-  #
-  # Example:
-  #
-  #     riemann {
-  #         riemann_event => {
-  #             "metric"  => "%{metric}"
-  #             "service" => "%{service}"
-  #         }
-  #     }
-  #
-  # `metric` and `ttl` values will be coerced to a floating point value.
-  # Values which cannot be coerced will zero (0.0).
-  #
-  # `description`, by default, will be set to the event message
-  # but can be overridden here.
-  config :riemann_event, :validate => :hash
-
-  #
-  # Enable debugging output?
-  config :debug, :validate => :boolean, :default => false, :deprecated => "This setting was never used by this plugin. It will be removed soon."
-
-  public
-  def register
-    require 'riemann/client'
-    @client = Riemann::Client.new(:host => @host, :port => @port)
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    # Let's build us an event, shall we?
-    r_event = Hash.new
-    r_event[:host] = event.sprintf(@sender)
-    # riemann doesn't handle floats so we reduce the precision here
-    r_event[:time] = event["@timestamp"].to_i
-    r_event[:description] = event["message"]
-    if @riemann_event
-      @riemann_event.each do |key, val|
-        if ["ttl","metric"].include?(key)
-          r_event[key.to_sym] = event.sprintf(val).to_f
-        else
-          r_event[key.to_sym] = event.sprintf(val)
-        end
-      end
-    end
-    r_event[:tags] = event["tags"] if event["tags"].is_a?(Array)
-    @logger.debug("Riemann event: ", :riemann_event => r_event)
-    begin
-      proto_client = @client.instance_variable_get("@#{@protocol}")
-      @logger.debug("Riemann client proto: #{proto_client.to_s}")
-      proto_client << r_event
-    rescue Exception => e
-      @logger.debug("Unhandled exception", :error => e)
-    end
-  end # def receive
-end # class LogStash::Outputs::Riemann
diff --git a/lib/logstash/outputs/solr_http.rb b/lib/logstash/outputs/solr_http.rb
deleted file mode 100644
index 42d48a5b9a9..00000000000
--- a/lib/logstash/outputs/solr_http.rb
+++ /dev/null
@@ -1,78 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "stud/buffer"
-require "rubygems"
-
-# This output lets you index&store your logs in Solr. If you want to get
-# started quickly you should use version 4.4 or above in schemaless mode,
-# which will try and guess your fields automatically. To turn that on,
-# you can use the example included in the Solr archive:
-#
-#     tar zxf solr-4.4.0.tgz
-#     cd example
-#     mv solr solr_ #back up the existing sample conf
-#     cp -r example-schemaless/solr/ .  #put the schemaless conf in place
-#     java -jar start.jar   #start Solr
-#
-# You can learn more about Solr at <https://lucene.apache.org/solr/>
-
-class LogStash::Outputs::SolrHTTP < LogStash::Outputs::Base
-  include Stud::Buffer
-
-  config_name "solr_http"
-
-  milestone 1
-
-  # URL used to connect to Solr
-  config :solr_url, :validate => :string, :default => "http://localhost:8983/solr"
-
-  # Number of events to queue up before writing to Solr
-  config :flush_size, :validate => :number, :default => 100
-
-  # Amount of time since the last flush before a flush is done even if
-  # the number of buffered events is smaller than flush_size
-  config :idle_flush_time, :validate => :number, :default => 1
-
-  # Solr document ID for events. You'd typically have a variable here, like
-  # '%{foo}' so you can assign your own IDs
-  config :document_id, :validate => :string, :default => nil
-
-  public
-  def register
-    require "uuidtools"
-    require "rsolr"
-    @solr = RSolr.connect :url => @solr_url
-    buffer_initialize(
-      :max_items => @flush_size,
-      :max_interval => @idle_flush_time,
-      :logger => @logger
-    )
-  end #def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-    buffer_receive(event)
-  end #def receive
-
-  public
-  def flush(events, teardown=false)
-    documents = []  #this is the array of hashes that we push to Solr as documents
-
-    events.each do |event|
-        document = event.to_hash()
-        document["@timestamp"] = document["@timestamp"].iso8601 #make the timestamp ISO
-        if @document_id.nil?
-          document ["id"] = UUIDTools::UUID.random_create    #add a unique ID
-        else
-          document ["id"] = event.sprintf(@document_id)      #or use the one provided
-        end
-        documents.push(document)
-    end
-
-    @solr.add(documents)
-    rescue Exception => e
-      @logger.warn("An error occurred while indexing: #{e.message}")
-  end #def flush
-end #class LogStash::Outputs::SolrHTTP
diff --git a/lib/logstash/outputs/stomp.rb b/lib/logstash/outputs/stomp.rb
deleted file mode 100644
index 32730c1dc8f..00000000000
--- a/lib/logstash/outputs/stomp.rb
+++ /dev/null
@@ -1,67 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-class LogStash::Outputs::Stomp < LogStash::Outputs::Base
-  config_name "stomp"
-  milestone 2
-
-  # The address of the STOMP server.
-  config :host, :validate => :string, :required => true
-
-  # The port to connect to on your STOMP server.
-  config :port, :validate => :number, :default => 61613
-
-  # The username to authenticate with.
-  config :user, :validate => :string, :default => ""
-
-  # The password to authenticate with.
-  config :password, :validate => :password, :default => ""
-
-  # The destination to read events from. Supports string expansion, meaning
-  # %{foo} values will expand to the field value.
-  #
-  # Example: "/topic/logstash"
-  config :destination, :validate => :string, :required => true
-
-  # The vhost to use
-  config :vhost, :validate => :string, :default => nil
-
-  # Enable debugging output.
-  config :debug, :validate => :boolean, :default => false, :deprecated => "This setting was never used by this plugin. It will be removed soon."
-
-  private
-  def connect
-    begin
-      @client.connect
-      @logger.debug("Connected to stomp server") if @client.connected?
-    rescue => e
-      @logger.debug("Failed to connect to stomp server, will retry",
-                    :exception => e, :backtrace => e.backtrace)
-      sleep 2
-      retry
-    end
-  end
-
-
-  public
-  def register
-    require "onstomp"
-    @client = OnStomp::Client.new("stomp://#{@host}:#{@port}", :login => @user, :passcode => @password.value)
-    @client.host = @vhost if @vhost
-
-    # Handle disconnects
-    @client.on_connection_closed {
-      connect
-    }
-    
-    connect
-  end # def register
-  
-  def receive(event)
-      return unless output?(event)
-
-      @logger.debug(["stomp sending event", { :host => @host, :event => event }])
-      @client.send(event.sprintf(@destination), event.to_json)
-  end # def receive
-end # class LogStash::Outputs::Stomp
diff --git a/lib/logstash/outputs/syslog.rb b/lib/logstash/outputs/syslog.rb
deleted file mode 100644
index 988ebc9ede6..00000000000
--- a/lib/logstash/outputs/syslog.rb
+++ /dev/null
@@ -1,145 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "date"
-
-# Send events to a syslog server.
-#
-# You can send messages compliant with RFC3164 or RFC5424
-# UDP or TCP syslog transport is supported
-class LogStash::Outputs::Syslog < LogStash::Outputs::Base
-  config_name "syslog"
-  milestone 1
-
-  FACILITY_LABELS = [
-    "kernel",
-    "user-level",
-    "mail",
-    "daemon",
-    "security/authorization",
-    "syslogd",
-    "line printer",
-    "network news",
-    "uucp",
-    "clock",
-    "security/authorization",
-    "ftp",
-    "ntp",
-    "log audit",
-    "log alert",
-    "clock",
-    "local0",
-    "local1",
-    "local2",
-    "local3",
-    "local4",
-    "local5",
-    "local6",
-    "local7",
-  ]
-
-  SEVERITY_LABELS = [
-    "emergency",
-    "alert",
-    "critical",
-    "error",
-    "warning",
-    "notice",
-    "informational",
-    "debug",
-  ]
-
-  # syslog server address to connect to
-  config :host, :validate => :string, :required => true
-  
-  # syslog server port to connect to
-  config :port, :validate => :number, :required => true
-
-  # syslog server protocol. you can choose between udp and tcp
-  config :protocol, :validate => ["tcp", "udp"], :default => "udp"
-
-  # facility label for syslog message
-  config :facility, :validate => FACILITY_LABELS, :required => true
-
-  # severity label for syslog message
-  config :severity, :validate => SEVERITY_LABELS, :required => true
-
-  # source host for syslog message
-  config :sourcehost, :validate => :string, :default => "%{host}"
-
-  # timestamp for syslog message
-  config :timestamp, :validate => :string, :default => "%{@timestamp}", :deprecated => "This setting is no longer necessary. The RFC setting will determine what time format is used."
-
-  # application name for syslog message
-  config :appname, :validate => :string, :default => "LOGSTASH"
-
-  # process id for syslog message
-  config :procid, :validate => :string, :default => "-"
- 
-  # message id for syslog message
-  config :msgid, :validate => :string, :default => "-"
-
-  # syslog message format: you can choose between rfc3164 or rfc5424
-  config :rfc, :validate => ["rfc3164", "rfc5424"], :default => "rfc3164"
-
-  
-  public
-  def register
-      @client_socket = nil
-  end
-
-  private
-  def udp?
-    @protocol == "udp"
-  end
-
-  private
-  def rfc3164?
-    @rfc == "rfc3164"
-  end 
-
-  private
-  def connect
-    if udp?
-        @client_socket = UDPSocket.new
-        @client_socket.connect(@host, @port)
-    else
-        @client_socket = TCPSocket.new(@host, @port)
-    end
-  end
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    appname = event.sprintf(@appname)
-    procid = event.sprintf(@procid)
-    sourcehost = event.sprintf(@sourcehost)
-
-    facility_code = FACILITY_LABELS.index(@facility)
-
-    severity_code = SEVERITY_LABELS.index(@severity)
-
-    priority = (facility_code * 8) + severity_code
-
-    if rfc3164?
-      timestamp = event.sprintf("%{+MMM dd HH:mm:ss}")
-      syslog_msg = "<"+priority.to_s()+">"+timestamp+" "+sourcehost+" "+appname+"["+procid+"]: "+event["message"]
-    else
-      msgid = event.sprintf(@msgid)
-      timestamp = event.sprintf("%{+YYYY-MM-dd'T'HH:mm:ss.SSSZ}")
-      syslog_msg = "<"+priority.to_s()+">1 "+timestamp+" "+sourcehost+" "+appname+" "+procid+" "+msgid+" - "+event["message"]
-    end
-
-    begin
-      connect unless @client_socket
-      @client_socket.write(syslog_msg + "\n")
-    rescue => e
-      @logger.warn(@protocol+" output exception", :host => @host, :port => @port,
-                 :exception => e, :backtrace => e.backtrace)
-      @client_socket.close rescue nil
-      @client_socket = nil
-    end
-  end
-end
-
diff --git a/lib/logstash/outputs/websocket.rb b/lib/logstash/outputs/websocket.rb
deleted file mode 100644
index 67486036677..00000000000
--- a/lib/logstash/outputs/websocket.rb
+++ /dev/null
@@ -1,46 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/outputs/base"
-
-# This output runs a websocket server and publishes any 
-# messages to all connected websocket clients.
-#
-# You can connect to it with ws://<host\>:<port\>/
-#
-# If no clients are connected, any messages received are ignored.
-class LogStash::Outputs::WebSocket < LogStash::Outputs::Base
-  config_name "websocket"
-  milestone 1
-
-  # The address to serve websocket data from
-  config :host, :validate => :string, :default => "0.0.0.0"
-
-  # The port to serve websocket data from
-  config :port, :validate => :number, :default => 3232
-
-  public
-  def register
-    require "ftw"
-    require "logstash/outputs/websocket/app"
-    require "logstash/outputs/websocket/pubsub"
-    @pubsub = LogStash::Outputs::WebSocket::Pubsub.new
-    @pubsub.logger = @logger
-    @server = Thread.new(@pubsub) do |pubsub|
-      begin
-        Rack::Handler::FTW.run(LogStash::Outputs::WebSocket::App.new(pubsub, @logger),
-                               :Host => @host, :Port => @port)
-      rescue => e
-        @logger.error("websocket server failed", :exception => e)
-        sleep 1
-        retry
-      end
-    end
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-    @pubsub.publish(event.to_json)
-  end # def receive
-
-end # class LogStash::Outputs::Websocket
diff --git a/lib/logstash/outputs/websocket/app.rb b/lib/logstash/outputs/websocket/app.rb
deleted file mode 100644
index b21ec9c9039..00000000000
--- a/lib/logstash/outputs/websocket/app.rb
+++ /dev/null
@@ -1,29 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/outputs/websocket"
-require "sinatra/base"
-require "rack/handler/ftw" # from ftw
-require "ftw/websocket/rack" # from ftw
-
-class LogStash::Outputs::WebSocket::App < Sinatra::Base
-  def initialize(pubsub, logger)
-    @pubsub = pubsub
-    @logger = logger
-  end
-
-  set :reload_templates, false
-
-  get "/" do
-    # TODO(sissel): Support filters/etc.
-    ws = ::FTW::WebSocket::Rack.new(env)
-    @logger.debug("New websocket client")
-    stream(:keep_open) do |out|
-      @pubsub.subscribe do |event|
-        ws.publish(event)
-      end # pubsub
-    end # stream
-
-    ws.rack_response
-  end # get /
-end # class LogStash::Outputs::WebSocket::App
-
diff --git a/lib/logstash/outputs/websocket/pubsub.rb b/lib/logstash/outputs/websocket/pubsub.rb
deleted file mode 100644
index 04e6d58a141..00000000000
--- a/lib/logstash/outputs/websocket/pubsub.rb
+++ /dev/null
@@ -1,45 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/outputs/websocket"
-
-class LogStash::Outputs::WebSocket::Pubsub
-  attr_accessor :logger
-
-  def initialize
-    @subscribers = []
-    @subscribers_lock = Mutex.new
-  end # def initialize
-
-  def publish(object)
-    @subscribers_lock.synchronize do
-      break if @subscribers.size == 0
-
-      failed = []
-      @subscribers.each do |subscriber|
-        begin
-          subscriber.call(object)
-        rescue => e
-          @logger.error("Failed to publish to subscriber", :subscriber => subscriber, :exception => e)
-          failed << subscriber
-        end
-      end
-
-      failed.each do |subscriber|
-        @subscribers.delete(subscriber)
-      end
-    end # @subscribers_lock.synchronize
-  end # def Pubsub
-
-  def subscribe(&block)
-    queue = Queue.new
-    @subscribers_lock.synchronize do
-      @subscribers << proc do |event|
-        queue << event
-      end
-    end
-
-    while true
-      block.call(queue.pop)
-    end
-  end # def subscribe
-end # class LogStash::Outputs::WebSocket::Pubsub
diff --git a/lib/logstash/outputs/zabbix.rb b/lib/logstash/outputs/zabbix.rb
deleted file mode 100644
index 83f5b504e8d..00000000000
--- a/lib/logstash/outputs/zabbix.rb
+++ /dev/null
@@ -1,108 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/outputs/base"
- 
-# The zabbix output is used for sending item data to zabbix via the
-# zabbix_sender executable.
-#
-# For this output to work, your event must have the following fields:
-#
-# * "zabbix_host"    (the host configured in Zabbix)
-# * "zabbix_item"    (the item key on the host in Zabbix)
-#
-# In Zabbix, create your host with the same name (no spaces in the name of 
-# the host supported) and create your item with the specified key as a
-# Zabbix Trapper item.
-#
-# The easiest way to use this output is with the grep filter.
-# Presumably, you only want certain events matching a given pattern
-# to send events to zabbix, so use grep to match and also to add the required
-# fields.
-#
-#      filter {
-#        grep {
-#          type => "linux-syslog"
-#          match => [ "@message", "(error|ERROR|CRITICAL)" ]
-#          add_tag => [ "zabbix-sender" ]
-#          add_field => [
-#            "zabbix_host", "%{source_host}",
-#            "zabbix_item", "item.key"
-#          ]
-#       }
-#     }
-#      
-#     output {
-#       zabbix {
-#         # only process events with this tag
-#         tags => "zabbix-sender"
-#  
-#         # specify the hostname or ip of your zabbix server
-#         # (defaults to localhost)
-#         host => "localhost"
-#  
-#         # specify the port to connect to (default 10051)
-#         port => "10051"
-#  
-#         # specify the path to zabbix_sender
-#         # (defaults to "/usr/local/bin/zabbix_sender")
-#         zabbix_sender => "/usr/local/bin/zabbix_sender"
-#       }
-#     }
-class LogStash::Outputs::Zabbix < LogStash::Outputs::Base
- 
-  config_name "zabbix"
-  milestone 2
-
-  config :host, :validate => :string, :default => "localhost"
-  config :port, :validate => :number, :default => 10051
-  config :zabbix_sender, :validate => :path, :default => "/usr/local/bin/zabbix_sender"
- 
-  public
-  def register
-    # nothing to do
-  end # def register
- 
-  public
-  def receive(event)
-    return unless output?(event)
- 
-    if !File.exists?(@zabbix_sender)
-      @logger.warn("Skipping zabbix output; zabbix_sender file is missing",
-                   :zabbix_sender => @zabbix_sender, :missed_event => event)
-      return
-    end
- 
-    host = event["zabbix_host"]
-    if !host
-      @logger.warn("Skipping zabbix output; zabbix_host field is missing",
-                   :missed_event => event)
-      return
-    end
-    host = host.first if host.is_a?(Array)
- 
-    item = event["zabbix_item"]
-    if !item
-      @logger.warn("Skipping zabbix output; zabbix_item field is missing",
-                   :missed_event => event)
-      return
-    end
-    item = item.first if item.is_a?(Array)
- 
-    zmsg = event["message"]
-    zmsg = zmsg.gsub("\n", "\\n")
-    zmsg = zmsg.gsub(/"/, "\\\"")
- 
-    cmd = "#{@zabbix_sender} -z #{@host} -p #{@port} -s #{host} -k #{item} -o \"#{zmsg}\" 2>/dev/null >/dev/null"
- 
-    @logger.debug("Running zabbix command", :command => cmd)
-    begin
-      # TODO(sissel): Update this to use IO.popen so we can capture the output and
-      # log it accordingly.
-      system(cmd)
-    rescue => e
-      @logger.warn("Skipping zabbix output; error calling zabbix_sender",
-                   :command => cmd, :missed_event => event,
-                   :exception => e, :backtrace => e.backtrace)
-    end
-  end # def receive
-end # class LogStash::Outputs::Zabbix
diff --git a/lib/logstash/util/relp.rb b/lib/logstash/util/relp.rb
deleted file mode 100644
index 2549c019500..00000000000
--- a/lib/logstash/util/relp.rb
+++ /dev/null
@@ -1,326 +0,0 @@
-# encoding: utf-8
-require "socket"
-
-class Relp#This isn't much use on its own, but gives RelpServer and RelpClient things
-
-  RelpVersion = '0'#TODO: spec says this is experimental, but rsyslog still seems to exclusively use it
-  RelpSoftware = 'logstash,1.1.1,http://logstash.net'
-
-  class RelpError < StandardError; end
-  class InvalidCommand < RelpError; end
-  class InappropriateCommand < RelpError; end
-  class ConnectionClosed < RelpError; end
-  class InsufficientCommands < RelpError; end
-
-  def valid_command?(command)
-    valid_commands = Array.new
-    
-    #Allow anything in the basic protocol for both directions
-    valid_commands << 'open'
-    valid_commands << 'close'
-
-    #These are things that are part of the basic protocol, but only valid in one direction (rsp, close etc.) TODO: would they be invalid or just innapropriate?
-    valid_commands += @basic_relp_commands
-
-    #These are extra commands that we require, otherwise refuse the connection TODO: some of these are only valid on one direction
-    valid_commands += @required_relp_commands
-
-    #TODO: optional_relp_commands
-
-    #TODO: vague mentions of abort and starttls commands in spec need looking into
-    return valid_commands.include?(command)
-  end
-
-  def frame_write(socket, frame)
-    unless self.server? #I think we have to trust a server to be using the correct txnr
-      #Only allow txnr to be 0 or be determined automatically
-      frame['txnr'] = self.nexttxnr() unless frame['txnr']==0
-    end
-    frame['txnr'] = frame['txnr'].to_s
-    frame['message'] = '' if frame['message'].nil?
-    frame['datalen'] = frame['message'].length.to_s
-    wiredata=[
-      frame['txnr'],
-      frame['command'],
-      frame['datalen'],
-      frame['message']
-    ].join(' ').strip
-    begin
-      @logger.debug? and @logger.debug("Writing to socket", :data => wiredata)
-      socket.write(wiredata)
-      #Ending each frame with a newline is required in the specifications
-      #Doing it a separately is useful (but a bit of a bodge) because
-      #for some reason it seems to take 2 writes after the server closes the
-      #connection before we get an exception
-      socket.write("\n")
-    rescue Errno::EPIPE,IOError,Errno::ECONNRESET#TODO: is this sufficient to catch all broken connections?
-      raise ConnectionClosed
-    end
-    return frame['txnr'].to_i
-  end
-
-  def frame_read(socket)
-    begin
-      frame = Hash.new
-      frame['txnr'] = socket.readline(' ').strip.to_i
-      frame['command'] = socket.readline(' ').strip
-
-      #Things get a little tricky here because if the length is 0 it is not followed by a space.
-      leading_digit=socket.read(1)
-      if leading_digit=='0' then
-        frame['datalen'] = 0
-        frame['message'] = ''
-      else
-        frame['datalen'] = (leading_digit + socket.readline(' ')).strip.to_i
-        frame['message'] = socket.read(frame['datalen'])
-      end
-      @logger.debug? and @logger.debug("Read frame", :frame => frame)
-    rescue EOFError,Errno::ECONNRESET,IOError
-      raise ConnectionClosed
-    end
-    if ! self.valid_command?(frame['command'])#TODO: is this enough to catch framing errors?
-      if self.server?
-        self.serverclose
-      else
-        self.close
-      end
-      raise InvalidCommand,frame['command']
-    end
-    return frame
-  end
-
-  def server?
-    @server
-  end
-
-end
-
-class RelpServer < Relp
-
-  def initialize(host,port,required_commands=[])
-    @logger = Cabin::Channel.get(LogStash)
-    
-    @server=true
-
-    #These are things that are part of the basic protocol, but only valid in one direction (rsp, close etc.)
-    @basic_relp_commands = ['close']#TODO: check for others
-
-    #These are extra commands that we require, otherwise refuse the connection
-    @required_relp_commands = required_commands
-
-    begin
-      @server = TCPServer.new(host, port)
-    rescue Errno::EADDRINUSE
-      @logger.error("Could not start RELP server: Address in use",
-                    :host => host, :port => port)
-      raise
-    end
-    @logger.info? and @logger.info("Started RELP Server", :host => host, :port => port)
-  end
-
-  def accept
-    socket = @server.accept
-    frame=self.frame_read(socket)
-    if frame['command'] == 'open'
-      offer=Hash[*frame['message'].scan(/^(.*)=(.*)$/).flatten]
-      if offer['relp_version'].nil?
-        @logger.warn("No relp version specified")
-        #if no version specified, relp spec says we must close connection
-        self.serverclose(socket)
-        raise RelpError, 'No relp_version specified'
-      #subtracting one array from the other checks to see if all elements in @required_relp_commands are present in the offer
-      elsif ! (@required_relp_commands - offer['commands'].split(',')).empty?
-        @logger.warn("Not all required commands are available", :required => @required_relp_commands, :offer => offer['commands'])
-        #Tell them why we're closing the connection:
-        response_frame = Hash.new
-        response_frame['txnr'] = frame['txnr']
-        response_frame['command'] = 'rsp'
-        response_frame['message'] = '500 Required command(s) '
-            + (@required_relp_commands - offer['commands'].split(',')).join(',')
-            + ' not offered'
-        self.frame_write(socket,response_frame)
-        self.serverclose(socket)
-        raise InsufficientCommands, offer['commands']
-            + ' offered, require ' + @required_relp_commands.join(',')
-      else
-        #attempt to set up connection
-        response_frame = Hash.new
-        response_frame['txnr'] = frame['txnr']
-        response_frame['command'] = 'rsp'
-
-        response_frame['message'] = '200 OK '
-        response_frame['message'] += 'relp_version=' + RelpVersion + "\n"
-        response_frame['message'] += 'relp_software=' + RelpSoftware + "\n"
-        response_frame['message'] += 'commands=' + @required_relp_commands.join(',')#TODO: optional ones
-        self.frame_write(socket, response_frame)
-        return self, socket
-      end
-    else
-      self.serverclose(socket)
-      raise InappropriateCommand, frame['command'] + ' expecting open'
-    end
-  end
-
-  #This does not ack the frame, just reads it
-  def syslog_read(socket)
-    frame = self.frame_read(socket)
-    if frame['command'] == 'syslog'
-      return frame
-    elsif frame['command'] == 'close'
-      #the client is closing the connection, acknowledge the close and act on it
-      response_frame = Hash.new
-      response_frame['txnr'] = frame['txnr']
-      response_frame['command'] = 'rsp'
-      self.frame_write(socket,response_frame)
-      self.serverclose(socket)
-      raise ConnectionClosed
-    else
-      #the client is trying to do something unexpected
-      self.serverclose(socket)
-      raise InappropriateCommand, frame['command'] + ' expecting syslog'
-    end
-  end
-
-  def serverclose(socket)
-    frame = Hash.new
-    frame['txnr'] = 0
-    frame['command'] = 'serverclose'
-    begin
-      self.frame_write(socket,frame)
-      socket.close
-    rescue ConnectionClosed
-    end
-  end
-
-  def shutdown
-    @server.close
-  rescue Exception#@server might already be down
-  end
-
-  def ack(socket, txnr)
-    frame = Hash.new
-    frame['txnr'] = txnr
-    frame['command'] = 'rsp'
-    frame['message'] = '200 OK'
-    self.frame_write(socket, frame)
-  end
-
-end
-
-#This is only used by the tests; any problems here are not as important as elsewhere
-class RelpClient < Relp
-
-  def initialize(host,port,required_commands = [],buffer_size = 128,
-                 retransmission_timeout=10)
-    @logger = Cabin::Channel.get(LogStash)
-    @logger.info? and @logger.info("Starting RELP client", :host => host, :port => port)
-    @server = false
-    @buffer = Hash.new
-
-    @buffer_size = buffer_size
-    @retransmission_timeout = retransmission_timeout
-
-    #These are things that are part of the basic protocol, but only valid in one direction (rsp, close etc.)
-    @basic_relp_commands = ['serverclose','rsp']#TODO: check for others
-
-    #These are extra commands that we require, otherwise refuse the connection
-    @required_relp_commands = required_commands
-
-    @socket=TCPSocket.new(host,port)
-
-    #This'll start the automatic frame numbering 
-    @lasttxnr = 0
-
-    offer=Hash.new
-    offer['command'] = 'open'
-    offer['message'] = 'relp_version=' + RelpVersion + "\n"
-    offer['message'] += 'relp_software=' + RelpSoftware + "\n"
-    offer['message'] += 'commands=' + @required_relp_commands.join(',')#TODO: add optional ones
-    self.frame_write(@socket, offer)
-    response_frame = self.frame_read(@socket)
-    if response_frame['message'][0,3] != '200'
-      raise RelpError,response_frame['message']
-    end
-
-    response=Hash[*response_frame['message'][7..-1].scan(/^(.*)=(.*)$/).flatten]
-    if response['relp_version'].nil?
-      #if no version specified, relp spec says we must close connection
-      self.close()
-      raise RelpError, 'No relp_version specified; offer: '
-          + response_frame['message'][6..-1].scan(/^(.*)=(.*)$/).flatten
-
-    #subtracting one array from the other checks to see if all elements in @required_relp_commands are present in the offer
-    elsif ! (@required_relp_commands - response['commands'].split(',')).empty?
-      #if it can't receive syslog it's useless to us; close the connection 
-      self.close()
-      raise InsufficientCommands, response['commands'] + ' offered, require '
-          + @required_relp_commands.join(',')
-    end
-    #If we've got this far with no problems, we're good to go
-    @logger.info? and @logger.info("Connection establish with server")
-
-    #This thread deals with responses that come back
-    reader = Thread.start do
-      loop do
-        f = self.frame_read(@socket)
-        if f['command'] == 'rsp' && f['message'] == '200 OK'
-          @buffer.delete(f['txnr'])
-        elsif f['command'] == 'rsp' && f['message'][0,1] == '5'
-          #TODO: What if we get an error for something we're already retransmitted due to timeout?
-          new_txnr = self.frame_write(@socket, @buffer[f['txnr']])
-          @buffer[new_txnr] = @buffer[f['txnr']]
-          @buffer.delete(f['txnr'])
-        elsif f['command'] == 'serverclose' || f['txnr'] == @close_txnr
-          break
-        else
-          #Don't know what's going on if we get here, but it can't be good
-          raise RelpError#TODO: raising errors like this makes no sense
-        end
-      end
-    end
-
-    #While this one deals with frames for which we get no reply
-    Thread.start do
-      old_buffer = Hash.new
-      loop do
-        #This returns old txnrs that are still present
-        (@buffer.keys & old_buffer.keys).each do |txnr|
-          new_txnr = self.frame_write(@socket, @buffer[txnr])
-          @buffer[new_txnr] = @buffer[txnr]
-          @buffer.delete(txnr)
-        end
-        old_buffer = @buffer
-        sleep @retransmission_timeout
-      end
-    end
-  end
-
-  #TODO: have a way to get back unacked messages on close
-  def close
-    frame = Hash.new
-    frame['command'] = 'close'
-    @close_txnr=self.frame_write(@socket, frame)
-    #TODO: ought to properly wait for a reply etc. The serverclose will make it work though
-    sleep @retransmission_timeout
-    @socket.close#TODO: shutdown?
-    return @buffer
-  end
-
-  def syslog_write(logline)
-
-    #If the buffer is already full, wait until a gap opens up
-    sleep 0.1 until @buffer.length<@buffer_size
-
-    frame = Hash.new
-    frame['command'] = 'syslog'
-    frame['message'] = logline
-
-    txnr = self.frame_write(@socket, frame)
-    @buffer[txnr] = frame
-  end
-
-  def nexttxnr
-    @lasttxnr += 1
-  end
-
-end
diff --git a/lib/logstash/util/zenoss.rb b/lib/logstash/util/zenoss.rb
deleted file mode 100644
index 664331c4955..00000000000
--- a/lib/logstash/util/zenoss.rb
+++ /dev/null
@@ -1,566 +0,0 @@
-# encoding: utf-8
-require "beefcake"
-
-# Zenoss Protocol Buffers generated by beefcake then cleaned up and
-# consolidated by hand.
-#
-module Org
-  module Zenoss
-    module Protobufs
-
-      module Util
-
-        class TimestampRange
-          include Beefcake::Message
-
-          optional :start_time, :uint64, 1
-          optional :end_time, :uint64, 2
-        end
-
-        class ScheduleWindow
-          include Beefcake::Message
-
-          module RepeatType
-            NEVER = 0
-            DAILY = 1
-            EVERY_WEEKDAY = 2
-            WEEKLY = 3
-            MONTHLY = 4
-            FIRST_SUNDAY = 5
-          end
-
-          optional :uuid, :string, 1
-          optional :name, :string, 2
-          optional :enabled, :bool, 3
-          optional :created_time, :uint64, 4
-          optional :duration_seconds, :int32, 5
-          optional :repeat, ScheduleWindow::RepeatType, 6
-        end
-
-        class ScheduleWindowSet
-          include Beefcake::Message
-
-          repeated :windows, ScheduleWindow, 1
-        end
-
-        class Property
-          include Beefcake::Message
-
-          required :name, :string, 1
-          required :value, :string, 2
-        end
-
-      end # module Org::Zenoss::Protobufs::Util
-
-      module Model
-
-        module ModelElementType
-          DEVICE = 1
-          COMPONENT = 2
-          SERVICE = 3
-          ORGANIZER = 4
-        end
-
-        class Device
-          include Beefcake::Message
-
-          optional :uuid, :string, 1
-          optional :id, :string, 2
-          optional :title, :string, 3
-        end
-
-        class Component
-          include Beefcake::Message
-
-          optional :uuid, :string, 1
-          optional :id, :string, 2
-          optional :title, :string, 3
-          optional :device, Device, 4
-        end
-
-        class Organizer
-          include Beefcake::Message
-
-          optional :uuid, :string, 1
-          optional :title, :string, 2
-          optional :path, :string, 3
-        end
-
-        class Service
-          include Beefcake::Message
-
-          optional :uuid, :string, 1
-          optional :title, :string, 2
-          repeated :impacts, :string, 3
-        end
-
-      end # module Org::Zenoss::Protobufs::Util
-
-      module Zep
-
-        module EventSeverity
-          SEVERITY_CLEAR = 0
-          SEVERITY_DEBUG = 1
-          SEVERITY_INFO = 2
-          SEVERITY_WARNING = 3
-          SEVERITY_ERROR = 4
-          SEVERITY_CRITICAL = 5
-        end
-
-        module SyslogPriority
-          SYSLOG_PRIORITY_EMERG = 0
-          SYSLOG_PRIORITY_ALERT = 1
-          SYSLOG_PRIORITY_CRIT = 2
-          SYSLOG_PRIORITY_ERR = 3
-          SYSLOG_PRIORITY_WARNING= 4
-          SYSLOG_PRIORITY_NOTICE = 5
-          SYSLOG_PRIORITY_INFO = 6
-          SYSLOG_PRIORITY_DEBUG = 7
-        end
-
-        module EventStatus
-          STATUS_NEW = 0
-          STATUS_ACKNOWLEDGED = 1
-          STATUS_SUPPRESSED = 2
-          STATUS_CLOSED = 3
-          STATUS_CLEARED = 4
-          STATUS_DROPPED = 5
-          STATUS_AGED = 6
-        end
-
-        module FilterOperator
-          OR = 1
-          AND = 2
-        end
-
-        module RuleType
-          RULE_TYPE_JYTHON = 1
-        end
-
-        class EventActor
-          include Beefcake::Message
-
-          optional :element_type_id, Org::Zenoss::Protobufs::Model::ModelElementType, 1
-          optional :element_uuid, :string, 2
-          optional :element_identifier, :string, 3
-          optional :element_title, :string, 4
-          optional :element_sub_type_id, Org::Zenoss::Protobufs::Model::ModelElementType, 5
-          optional :element_sub_uuid, :string, 6
-          optional :element_sub_identifier, :string, 7
-          optional :element_sub_title, :string, 8
-        end
-
-        class EventDetail
-          include Beefcake::Message
-
-          module EventDetailMergeBehavior
-            REPLACE = 1
-            APPEND = 2
-            UNIQUE = 3
-          end
-
-          required :name, :string, 1
-          repeated :value, :string, 2
-          optional :merge_behavior, EventDetail::EventDetailMergeBehavior, 3, :default => EventDetail::EventDetailMergeBehavior::REPLACE
-        end
-
-        class EventDetailSet
-          include Beefcake::Message
-
-          repeated :details, EventDetail, 1
-        end
-
-        class EventTag
-          include Beefcake::Message
-
-          required :type, :string, 1
-          repeated :uuid, :string, 2
-        end
-
-        class EventNote
-          include Beefcake::Message
-
-          optional :uuid, :string, 1
-          optional :user_uuid, :string, 2
-          optional :user_name, :string, 3
-          optional :created_time, :uint64, 4
-          required :message, :string, 5
-        end
-
-        class Event
-          include Beefcake::Message
-
-          optional :uuid, :string, 1
-          optional :created_time, :uint64, 2
-          optional :fingerprint, :string, 3
-          optional :event_class, :string, 4
-          optional :event_class_key, :string, 5
-          optional :event_class_mapping_uuid, :string, 6
-          optional :actor, EventActor, 7
-          optional :summary, :string, 8
-          optional :message, :string, 9
-          optional :severity, EventSeverity, 10, :default => EventSeverity::SEVERITY_INFO
-          optional :event_key, :string, 12
-          optional :event_group, :string, 13
-          optional :agent, :string, 14
-          optional :syslog_priority, SyslogPriority, 15
-          optional :syslog_facility, :uint32, 16
-          optional :nt_event_code, :uint32, 17
-          optional :monitor, :string, 18
-          repeated :details, EventDetail, 19
-          repeated :tags, EventTag, 20
-          optional :summary_uuid, :string, 21
-          optional :status, EventStatus, 22, :default => EventStatus::STATUS_NEW
-          optional :apply_transforms, :bool, 23, :default => true
-          optional :count, :uint32, 24, :default => 1
-          optional :first_seen_time, :uint64, 25
-        end
-
-        class ZepRawEvent
-          include Beefcake::Message
-
-          required :event, Event, 1
-          repeated :clear_event_class, :string, 2
-        end
-
-        class EventAuditLog
-          include Beefcake::Message
-
-          required :timestamp, :uint64, 1
-          required :new_status, EventStatus, 2
-          optional :user_uuid, :string, 3
-          optional :user_name, :string, 4
-        end
-
-        class EventSummary
-          include Beefcake::Message
-
-          optional :uuid, :string, 1
-          repeated :occurrence, Event, 2
-          optional :status, EventStatus, 3, :default => EventStatus::STATUS_NEW
-          optional :first_seen_time, :uint64, 4
-          optional :status_change_time, :uint64, 5
-          optional :last_seen_time, :uint64, 6
-          optional :count, :uint32, 7, :default => 1
-          optional :current_user_uuid, :string, 8
-          optional :current_user_name, :string, 9
-          optional :cleared_by_event_uuid, :string, 10
-          repeated :notes, EventNote, 11
-          repeated :audit_log, EventAuditLog, 12
-          optional :update_time, :uint64, 13
-        end
-
-        class NumberRange
-          include Beefcake::Message
-
-          optional :from, :sint64, 1
-          optional :to, :sint64, 2
-        end
-
-        class EventTagFilter
-          include Beefcake::Message
-
-          optional :op, FilterOperator, 1, :default => FilterOperator::OR
-          repeated :tag_uuids, :string, 2
-        end
-
-        class EventDetailFilter
-          include Beefcake::Message
-
-          required :key, :string, 1
-          repeated :value, :string, 2
-          optional :op, FilterOperator, 3, :default => FilterOperator::OR
-        end
-
-        class EventFilter
-          include Beefcake::Message
-
-          repeated :severity, EventSeverity, 1
-          repeated :status, EventStatus, 2
-          repeated :event_class, :string, 3
-          repeated :first_seen, :'org::zenoss::protobufs::util::TimestampRange', 4
-          repeated :last_seen, :'org::zenoss::protobufs::util::TimestampRange', 5
-          repeated :status_change, :'org::zenoss::protobufs::util::TimestampRange', 6
-          repeated :update_time, :'org::zenoss::protobufs::util::TimestampRange', 7
-          repeated :count_range, NumberRange, 8
-          repeated :element_identifier, :string, 9
-          repeated :element_sub_identifier, :string, 10
-          repeated :uuid, :string, 11
-          repeated :event_summary, :string, 12
-          repeated :current_user_name, :string, 13
-          repeated :tag_filter, EventTagFilter, 14
-          repeated :details, EventDetailFilter, 15
-          repeated :fingerprint, :string, 16
-          repeated :agent, :string, 17
-          repeated :monitor, :string, 18
-          optional :operator, FilterOperator, 19, :default => FilterOperator::AND
-          repeated :subfilter, EventFilter, 20
-          repeated :element_title, :string, 21
-          repeated :element_sub_title, :string, 22
-          repeated :event_key, :string, 23
-          repeated :event_class_key, :string, 24
-          repeated :event_group, :string, 25
-          repeated :message, :string, 26
-        end
-
-        class EventSort
-          include Beefcake::Message
-
-          module Field
-            SEVERITY = 1
-            STATUS = 2
-            EVENT_CLASS = 3
-            FIRST_SEEN = 4
-            LAST_SEEN = 5
-            STATUS_CHANGE = 6
-            COUNT = 7
-            ELEMENT_IDENTIFIER = 8
-            ELEMENT_SUB_IDENTIFIER = 9
-            EVENT_SUMMARY = 10
-            UPDATE_TIME = 11
-            CURRENT_USER_NAME = 12
-            AGENT = 13
-            MONITOR = 14
-            UUID = 15
-            FINGERPRINT = 16
-            DETAIL = 17
-            ELEMENT_TITLE = 18
-            ELEMENT_SUB_TITLE = 19
-            EVENT_KEY = 20
-            EVENT_CLASS_KEY = 21
-            EVENT_GROUP = 22
-          end
-
-          module Direction
-            ASCENDING = 1
-            DESCENDING = 2
-          end
-
-          required :field, EventSort::Field, 1
-          optional :direction, EventSort::Direction, 2, :default => EventSort::Direction::ASCENDING
-          optional :detail_key, :string, 3
-        end
-
-        class EventSummaryRequest
-          include Beefcake::Message
-
-          optional :event_filter, EventFilter, 1
-          optional :exclusion_filter, EventFilter, 2
-          repeated :sort, EventSort, 3
-          optional :limit, :uint32, 4, :default => 1000
-          optional :offset, :uint32, 5
-        end
-
-        class EventSummaryResult
-          include Beefcake::Message
-
-          repeated :events, EventSummary, 1
-          optional :limit, :uint32, 2
-          optional :next_offset, :uint32, 3
-          optional :total, :uint32, 4
-        end
-
-        class EventSummaryUpdate
-          include Beefcake::Message
-
-          optional :status, EventStatus, 1
-          optional :current_user_uuid, :string, 2
-          optional :current_user_name, :string, 3
-        end
-
-        class EventQuery
-          include Beefcake::Message
-
-          optional :event_filter, EventFilter, 1
-          optional :exclusion_filter, EventFilter, 2
-          repeated :sort, EventSort, 3
-          optional :timeout, :uint32, 4, :default => 60
-        end
-
-        class EventSummaryUpdateRequest
-          include Beefcake::Message
-
-          optional :event_query_uuid, :string, 1
-          required :update_fields, EventSummaryUpdate, 2
-          optional :offset, :uint32, 3, :default => 0
-          optional :limit, :uint32, 4, :default => 100
-        end
-
-        class EventSummaryUpdateResponse
-          include Beefcake::Message
-
-          optional :next_request, EventSummaryUpdateRequest, 1
-          optional :total, :uint32, 2
-          required :updated, :uint32, 3
-        end
-
-        class EventDetailItem
-          include Beefcake::Message
-
-          module EventDetailType
-            STRING = 1
-            INTEGER = 2
-            FLOAT = 3
-            LONG = 4
-            DOUBLE = 5
-            IP_ADDRESS = 6
-            PATH = 7
-          end
-
-          required :key, :string, 1
-          optional :type, EventDetailItem::EventDetailType, 2, :default => EventDetailItem::EventDetailType::STRING
-          optional :name, :string, 3
-        end
-
-        class EventDetailItemSet
-          include Beefcake::Message
-
-          repeated :details, EventDetailItem, 1
-        end
-
-        class Rule
-          include Beefcake::Message
-
-          required :api_version, :int32, 1
-          required :type, RuleType, 2
-          required :source, :string, 3
-        end
-
-        class EventTriggerSubscription
-          include Beefcake::Message
-
-          optional :uuid, :string, 1
-          optional :delay_seconds, :int32, 2
-          optional :repeat_seconds, :int32, 3
-          optional :send_initial_occurrence, :bool, 4, :default => true
-          required :subscriber_uuid, :string, 5
-          required :trigger_uuid, :string, 6
-        end
-
-        class EventTriggerSubscriptionSet
-          include Beefcake::Message
-
-          repeated :subscriptions, EventTriggerSubscription, 1
-        end
-
-        class EventTrigger
-          include Beefcake::Message
-
-          optional :uuid, :string, 1
-          optional :name, :string, 2
-          optional :enabled, :bool, 3, :default => true
-          required :rule, Rule, 4
-          repeated :subscriptions, EventTriggerSubscription, 5
-        end
-
-        class EventTriggerSet
-          include Beefcake::Message
-
-          repeated :triggers, EventTrigger, 1
-        end
-
-        class Signal
-          include Beefcake::Message
-
-          required :uuid, :string, 1
-          required :created_time, :uint64, 2
-          required :trigger_uuid, :string, 3
-          required :subscriber_uuid, :string, 4
-          optional :clear, :bool, 5, :default => false
-          optional :event, EventSummary, 6
-          optional :clear_event, EventSummary, 7
-          optional :message, :string, 8
-        end
-
-        class EventTagSeverity
-          include Beefcake::Message
-
-          required :severity, EventSeverity, 1
-          optional :count, :uint32, 2, :default => 0
-          optional :acknowledged_count, :uint32, 3, :default => 0
-        end
-
-        class EventTagSeverities
-          include Beefcake::Message
-
-          required :tag_uuid, :string, 1
-          repeated :severities, EventTagSeverity, 2
-          optional :total, :uint32, 3
-        end
-
-        class EventTagSeveritiesSet
-          include Beefcake::Message
-
-          repeated :severities, EventTagSeverities, 1
-        end
-
-        class ZepConfig
-          include Beefcake::Message
-
-          optional :event_age_disable_severity, EventSeverity, 1, :default => EventSeverity::SEVERITY_ERROR
-          optional :event_age_interval_minutes, :uint32, 2, :default => 240
-          optional :event_archive_interval_minutes, :uint32, 3, :default => 4320
-          optional :event_archive_purge_interval_days, :uint32, 4, :default => 90
-          optional :event_time_purge_interval_days, :uint32, 5, :default => 1
-          optional :event_age_severity_inclusive, :bool, 6, :default => false
-          optional :event_max_size_bytes, :uint64, 7, :default => 32768
-          optional :index_summary_interval_milliseconds, :uint64, 8, :default => 1000
-          optional :index_archive_interval_milliseconds, :uint64, 9, :default => 30000
-          optional :index_limit, :uint32, 10, :default => 1000
-          optional :aging_limit, :uint32, 11, :default => 1000
-          optional :archive_limit, :uint32, 12, :default => 1000
-          optional :aging_interval_milliseconds, :uint64, 13, :default => 60000
-          optional :archive_interval_milliseconds, :uint64, 14, :default => 60000
-        end
-
-        class DaemonHeartbeat
-          include Beefcake::Message
-
-          required :monitor, :string, 1
-          required :daemon, :string, 2
-          required :timeout_seconds, :uint32, 3
-          optional :last_time, :uint64, 4
-        end
-
-        class DaemonHeartbeatSet
-          include Beefcake::Message
-
-          repeated :heartbeats, DaemonHeartbeat, 1
-        end
-
-        class EventTime
-          include Beefcake::Message
-
-          optional :summary_uuid, :string, 1
-          optional :processed_time, :uint64, 2
-          optional :created_time, :uint64, 3
-          optional :first_seen_time, :uint64, 4
-        end
-
-        class EventTimeSet
-          include Beefcake::Message
-
-          repeated :event_times, EventTime, 1
-        end
-
-        class ZepStatistic
-          include Beefcake::Message
-
-          required :name, :string, 1
-          required :description, :string, 2
-          required :value, :int64, 3
-        end
-
-        class ZepStatistics
-          include Beefcake::Message
-
-          repeated :stats, ZepStatistic, 1
-        end
-
-      end # module Org::Zenoss:Protobufs::Zep
-
-    end # module Org::Zenoss::Protobufs
-
-  end # module Org::Zenoss
-
-end # module Org
diff --git a/logstash.gemspec b/logstash.gemspec
index d904f6b7aa4..69612b72338 100644
--- a/logstash.gemspec
+++ b/logstash.gemspec
@@ -27,55 +27,39 @@ Gem::Specification.new do |gem|
   # Web dependencies
   gem.add_runtime_dependency "ftw", ["~> 0.0.39"] #(Apache 2.0 license)
   gem.add_runtime_dependency "haml"               #(MIT license)
-  gem.add_runtime_dependency "rack"               #(MIT license)
   gem.add_runtime_dependency "sass"               #(MIT license)
-  gem.add_runtime_dependency "sinatra"            #(MIT license)
   gem.add_runtime_dependency "mime-types"         #(GPL 2.0)
 
   # Input/Output/Filter dependencies
   #TODO Can these be optional?
   gem.add_runtime_dependency "awesome_print"                    #(MIT license)
   gem.add_runtime_dependency "aws-sdk"                          #{Apache 2.0 license}  
-  gem.add_runtime_dependency "google-api-client"                #{Apache 2.0 license}
-  gem.add_runtime_dependency "heroku"                           #(MIT license)
   gem.add_runtime_dependency "addressable"                      #(Apache 2.0 license)
   gem.add_runtime_dependency "extlib", ["0.9.16"]               #(MIT license)
-  gem.add_runtime_dependency "elasticsearch"                    #(Apache 2.0 license)
   gem.add_runtime_dependency "ffi"                              #(LGPL-3 license)
   gem.add_runtime_dependency "ffi-rzmq", ["1.0.0"]              #(MIT license)
   gem.add_runtime_dependency "filewatch", ["0.5.1"]             #(BSD license)
   gem.add_runtime_dependency "gelfd", ["0.2.0"]                 #(Apache 2.0 license)
   gem.add_runtime_dependency "gelf", ["1.3.2"]                  #(MIT license)
   gem.add_runtime_dependency "gmetric", ["0.1.3"]               #(MIT license)
-  gem.add_runtime_dependency "jiralicious", ["0.2.2"]           #(MIT license)
   gem.add_runtime_dependency "jls-grok", ["0.10.12"]            #(BSD license)
   gem.add_runtime_dependency "mail"                             #(MIT license)
-  gem.add_runtime_dependency "mongo"                            #(Apache 2.0 license)
   gem.add_runtime_dependency "metriks"                          #(MIT license)
-  gem.add_runtime_dependency "onstomp"                          #(Apache 2.0 license)
   gem.add_runtime_dependency "redis"                            #(MIT license)
-  gem.add_runtime_dependency "riak-client", ["1.0.3"]           #(Apache 2.0 license)
-  gem.add_runtime_dependency "riemann-client", ["0.2.1"]        #(MIT license)
   gem.add_runtime_dependency "statsd-ruby", ["1.2.0"]           #(MIT license)
-  gem.add_runtime_dependency "uuidtools"                        # For generating amqp queue names (Apache 2.0 license)
   gem.add_runtime_dependency "xml-simple"                       #(ruby license?)
   gem.add_runtime_dependency "xmpp4r", ["0.5"]                  #(ruby license)
   gem.add_runtime_dependency "jls-lumberjack", [">=0.0.19"]     #(Apache 2.0 license)
   gem.add_runtime_dependency "geoip", [">= 1.3.2"]              #(GPL license)
   gem.add_runtime_dependency "beefcake", "0.3.7"                #(MIT license)
-  gem.add_runtime_dependency "php-serialize"                    # For input drupal_dblog (MIT license)
   gem.add_runtime_dependency "murmurhash3"                      #(MIT license)
   gem.add_runtime_dependency "rufus-scheduler", "~> 2.0.24"     #(MIT license)
   gem.add_runtime_dependency "user_agent_parser", [">= 2.0.0"]  #(MIT license)
   gem.add_runtime_dependency "snmp"                             #(ruby license)
-  gem.add_runtime_dependency "varnish-rb"                       #(MIT license)
   gem.add_runtime_dependency "mail"                             #(MIT license)
   gem.add_runtime_dependency "rbnacl"                           #(MIT license)
-  gem.add_runtime_dependency "sequel"                           #(MIT license)
-  gem.add_runtime_dependency "jdbc-sqlite3"                     #(MIT license)
   gem.add_runtime_dependency "bindata", [">= 1.5.0"]            #(ruby license)
   gem.add_runtime_dependency "twitter", "5.0.0.rc.1"            #(MIT license)
-  gem.add_runtime_dependency "rsolr"                            #(Apache 2.0 license)
   gem.add_runtime_dependency "edn"                              #(MIT license)
 
   if RUBY_PLATFORM == 'java'
@@ -84,12 +68,9 @@ Gem::Specification.new do |gem|
     gem.add_runtime_dependency "jruby-httpclient"                 #(Apache 2.0 license)
     gem.add_runtime_dependency "bouncy-castle-java", "1.5.0147"   #(MIT license)
     gem.add_runtime_dependency "jruby-openssl", "0.8.7"           #(CPL/GPL/LGPL license)
-    gem.add_runtime_dependency "jruby-win32ole"                   #(unknown license)
-    gem.add_runtime_dependency "jdbc-mysql"                       # For input drupal_dblog (BSD license)
     gem.add_runtime_dependency "msgpack-jruby"                    #(Apache 2.0 license)
   else
     gem.add_runtime_dependency "excon"    #(MIT license)
-    gem.add_runtime_dependency "mysql2"   # For input drupal_dblog (MIT license)
     gem.add_runtime_dependency "msgpack"  #(Apache 2.0 license)
   end
 
diff --git a/spec/filters/alter.rb b/spec/filters/alter.rb
deleted file mode 100644
index e5a5a5fde9b..00000000000
--- a/spec/filters/alter.rb
+++ /dev/null
@@ -1,96 +0,0 @@
-require "test_utils"
-require "logstash/filters/alter"
-
-describe LogStash::Filters::Alter do
-  extend LogStash::RSpec
-
-  describe "condrewrite with static values" do
-    config <<-CONFIG
-    filter {
-      alter {
-        condrewrite => ["rewrite-me", "hello", "goodbye"]
-      }
-    }
-    CONFIG
-
-    sample("rewrite-me"  => "hello") do
-      insist { subject["rewrite-me"] } == "goodbye"
-    end
-
-    sample("rewrite-me"  => "greetings") do
-      insist { subject["rewrite-me"] } == "greetings"
-    end
-  end
-
-  describe "condrewrite with dynamic values" do
-    config <<-CONFIG
-    filter {
-      alter {
-        condrewrite => ["rewrite-me", "%{test}", "%{rewrite-value}"]
-      }
-    }
-    CONFIG
-
-    sample("rewrite-me"  => "hello", "test" => "hello",
-           "rewrite-value" => "goodbye") do
-      insist { subject["rewrite-me"] } == "goodbye"
-    end
-
-    sample("rewrite-me"  => "hello") do
-      insist { subject["rewrite-me"] } == "hello"
-    end
-
-    sample("rewrite-me"  => "%{test}") do
-      insist { subject["rewrite-me"] } == "%{rewrite-value}"
-    end
-
-    sample("rewrite-me"  => "hello", "test" => "hello") do
-      insist { subject["rewrite-me"] } == "%{rewrite-value}"
-    end
-
-    sample("rewrite-me"  => "greetings", "test" => "hello") do
-      insist { subject["rewrite-me"] } == "greetings"
-    end
-  end
-
-  describe "condrewriteother" do
-    config <<-CONFIG
-    filter {
-      alter {
-        condrewriteother => ["test-me", "hello", "rewrite-me","goodbye"]
-      }
-    }
-    CONFIG
-
-    sample("test-me"  => "hello") do
-      insist { subject["rewrite-me"] } == "goodbye"
-    end
-
-    sample("test-me"  => "hello", "rewrite-me"  => "hello2") do
-      insist { subject["rewrite-me"] } == "goodbye"
-    end
-
-    sample("test-me"  => "greetings") do
-      insist { subject["rewrite-me"] }.nil?
-    end
-
-    sample("test-me"  => "greetings",
-      "rewrite-me"  => "hello2") do
-      insist { subject["rewrite-me"] } == "hello2"
-    end
-  end
-
-  describe "coalesce" do
-    config <<-CONFIG
-    filter {
-      alter {
-        coalesce => ["coalesce-me", "%{non-existing-field}", "mydefault"]
-      }
-    }
-    CONFIG
-
-    sample("coalesce-me"  => "Hello") do
-      insist { subject["coalesce-me"] } == "mydefault" 
-    end
-  end
-end
diff --git a/spec/filters/cidr.rb b/spec/filters/cidr.rb
deleted file mode 100644
index 4cb35da5d91..00000000000
--- a/spec/filters/cidr.rb
+++ /dev/null
@@ -1,71 +0,0 @@
-require "test_utils"
-require "logstash/filters/cidr"
-
-describe LogStash::Filters::CIDR do
-  extend LogStash::RSpec
-
-  describe "IPV4 match test" do
-    config <<-CONFIG
-      filter {
-        cidr {
-          address => [ "%{clientip}" ]
-          network => [ "192.168.0.0/24" ]
-          add_tag => [ "matched" ]
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "192.168.0.30") do
-      insist { subject["tags"] }.include?("matched") 
-    end
-  end
-
-  describe "IPV4 non match" do
-   config <<-CONFIG
-       filter {
-        cidr {
-          address => [ "%{clientip}" ]
-          network => [ "192.168.0.0/24" ]
-          add_tag => [ "matched" ]
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "123.52.122.33") do
-       insist { subject["tags"] }.nil?
-    end
-  end
-
-  describe "IPV6 match test" do
-    config <<-CONFIG
-      filter {
-        cidr {
-          address => [ "%{clientip}" ]
-          network => [ "fe80::/64" ]
-          add_tag => [ "matched" ]
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "fe80:0:0:0:0:0:0:1") do
-      insist { subject["tags"] }.include?("matched") 
-    end
-  end
-
-  describe "IPV6 non match" do
-   config <<-CONFIG
-       filter {
-        cidr {
-          address => [ "%{clientip}" ]
-          network => [ "fe80::/64" ]
-          add_tag => [ "matched" ]
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "fd82:0:0:0:0:0:0:1") do
-       insist { subject["tags"] }.nil?
-    end
-  end
-
-end
diff --git a/spec/filters/collate.rb b/spec/filters/collate.rb
deleted file mode 100644
index 47da640cae1..00000000000
--- a/spec/filters/collate.rb
+++ /dev/null
@@ -1,122 +0,0 @@
-require "test_utils"
-require "logstash/filters/collate"
-
-describe LogStash::Filters::Collate do
-  extend LogStash::RSpec
-
-  describe "collate when count is full" do
-    config <<-CONFIG
-      filter {
-        collate {
-          count => 2
-        }
-      }
-    CONFIG
-
-    events = [
-      {
-        "@timestamp" => Time.iso8601("2013-01-02T00:00:00.000Z"),
-        "message" => "later message"
-      },
-      {
-        "@timestamp" => Time.iso8601("2013-01-01T00:00:00.000Z"),
-        "message" => "earlier message"
-      }
-    ]
-
-    sample(events) do
-      insist { subject }.is_a? Array
-      insist { subject.length } == 2
-      subject.each_with_index do |s,i|
-        if i == 0 # first one should be the earlier message
-          insist { s["message"] } == "earlier message"
-        end
-        if i == 1 # second one should be the later message
-          insist { s["message"]} == "later message"
-        end
-      end
-    end
-  end
-
-  describe "collate by desc" do
-    config <<-CONFIG
-      filter {
-        collate {
-          count => 3
-          order => "descending"
-        }
-      }
-    CONFIG
-
-    events = [
-      {
-        "@timestamp" => Time.iso8601("2013-01-03T00:00:00.000Z"),
-        "message" => "third message"
-      },
-      {
-        "@timestamp" => Time.iso8601("2013-01-01T00:00:00.000Z"),
-        "message" => "first message"
-      },
-      {
-        "@timestamp" => Time.iso8601("2013-01-02T00:00:00.000Z"),
-        "message" => "second message"
-      }
-    ]
-
-    sample(events) do
-      insist { subject }.is_a? Array
-      insist { subject.length } == 3
-      subject.each_with_index do |s,i|
-        if i == 0 # first one should be the third message
-          insist { s["message"] } == "third message"
-        end
-        if i == 1 # second one should be the second message
-          insist { s["message"]} == "second message"
-        end
-        if i == 2 # third one should be the third message
-          insist { s["message"]} == "first message"
-        end
-      end
-    end
-  end
-
-  # (Ignored) Currently this case can't pass because of the case depends on the flush function of the filter in the test, 
-  # there was a TODO marked in the code (test_utils.rb, # TODO(sissel): pipeline flush needs to be implemented.), 
-  # and the case wants to test the scenario which collate was triggered by a scheduler, so in this case, it needs to sleep few seconds 
-  # waiting the scheduler triggered, and after the events were flushed, then the result can be checked.
-
-  # describe "collate when interval reached" do
-  #   config <<-CONFIG
-  #     filter {
-  #       collate {
-  #         interval => "1s"
-  #       }
-  #     }
-  #   CONFIG
-
-  #   events = [
-  #     {
-  #       "@timestamp" => Time.iso8601("2013-01-02T00:00:00.000Z"),
-  #       "message" => "later message"
-  #     },
-  #     {
-  #       "@timestamp" => Time.iso8601("2013-01-01T00:00:00.000Z"),
-  #       "message" => "earlier message"
-  #     }
-  #   ]
-
-  #   sample(events) do
-  #     sleep(2)
-  #     insist { subject }.is_a? Array
-  #     insist { subject.length } == 2
-  #     subject.each_with_index do |s,i|
-  #       if i == 0 # first one should be the earlier message
-  #         insist { s["message"] } == "earlier message"
-  #       end
-  #       if i == 1 # second one should be the later message
-  #         insist { s["message"]} == "later message"
-  #       end
-  #     end
-  #   end
-  # end
-end
diff --git a/spec/filters/elapsed.rb b/spec/filters/elapsed.rb
deleted file mode 100644
index 067705a4bcd..00000000000
--- a/spec/filters/elapsed.rb
+++ /dev/null
@@ -1,294 +0,0 @@
-require 'socket'
-require "logstash/filters/elapsed"
-
-describe LogStash::Filters::Elapsed do
-  START_TAG = "startTag"
-  END_TAG   = "endTag"
-  ID_FIELD  = "uniqueIdField"
-
-  def event(data)
-    data["message"] ||= "Log message"
-    LogStash::Event.new(data)
-  end
-
-  def start_event(data)
-    data["tags"] ||= []
-    data["tags"] << START_TAG
-    event(data)
-  end
-
-  def end_event(data = {})
-    data["tags"] ||= []
-    data["tags"] << END_TAG
-    event(data)
-  end
-
-  before(:each) do
-    setup_filter()
-  end
-
-  def setup_filter(config = {})
-    @config = {"start_tag" => START_TAG, "end_tag" => END_TAG, "unique_id_field" => ID_FIELD}
-    @config.merge!(config)
-    @filter = LogStash::Filters::Elapsed.new(@config)
-    @filter.register
-  end
-
-  context "General validation" do
-    describe "receiving an event without start or end tag" do
-      it "does not record it" do
-        @filter.filter(event("message" => "Log message"))
-        insist { @filter.start_events.size } == 0
-      end
-    end
-
-    describe "receiving an event with a different start/end tag from the ones specified in the configuration" do
-      it "does not record it" do
-        @filter.filter(event("tags" => ["tag1", "tag2"]))
-        insist { @filter.start_events.size } == 0
-      end
-    end
-  end
-
-  context "Start event" do
-    describe "receiving an event with a valid start tag" do
-      describe "but without an unique id field" do
-        it "does not record it" do
-          @filter.filter(event("tags" => ["tag1", START_TAG]))
-          insist { @filter.start_events.size } == 0
-        end
-      end
-
-      describe "and a valid id field" do
-        it "records it" do
-          event = start_event(ID_FIELD => "id123")
-          @filter.filter(event)
-
-          insist { @filter.start_events.size } == 1
-          insist { @filter.start_events["id123"].event } == event
-        end
-      end
-    end
-
-    describe "receiving two 'start events' for the same id field" do
-      it "keeps the first one and does not save the second one" do
-          args = {"tags" => [START_TAG], ID_FIELD => "id123"}
-          first_event = event(args)
-          second_event = event(args)
-
-          @filter.filter(first_event)
-          @filter.filter(second_event)
-
-          insist { @filter.start_events.size } == 1
-          insist { @filter.start_events["id123"].event } == first_event
-      end
-    end
-  end
-
-  context "End event" do
-    describe "receiving an event with a valid end tag" do
-      describe "and without an id" do
-        it "does nothing" do
-          insist { @filter.start_events.size } == 0
-          @filter.filter(end_event())
-          insist { @filter.start_events.size } == 0
-        end
-      end
-
-      describe "and with an id" do
-        describe "but without a previous 'start event'" do
-          it "adds a tag 'elapsed.end_witout_start' to the 'end event'" do
-            end_event = end_event(ID_FIELD => "id_123")
-
-            @filter.filter(end_event)
-
-            insist { end_event["tags"].include?("elapsed.end_wtihout_start") } == true
-          end
-        end
-      end
-    end
-  end
-
-  context "Start/end events interaction" do
-    describe "receiving a 'start event'" do
-      before(:each) do
-        @id_value = "id_123"
-        @start_event = start_event(ID_FIELD => @id_value)
-        @filter.filter(@start_event)
-      end
-
-      describe "and receiving an event with a valid end tag" do
-        describe "and without an id" do
-          it "does nothing" do
-            @filter.filter(end_event())
-            insist { @filter.start_events.size } == 1
-            insist { @filter.start_events[@id_value].event } == @start_event
-          end
-        end
-
-        describe "and an id different from the one of the 'start event'" do
-          it "does nothing" do
-            different_id_value = @id_value + "_different"
-            @filter.filter(end_event(ID_FIELD => different_id_value))
-
-            insist { @filter.start_events.size } == 1
-            insist { @filter.start_events[@id_value].event } == @start_event
-          end
-        end
-
-        describe "and the same id of the 'start event'" do
-          it "deletes the recorded 'start event'" do
-            insist { @filter.start_events.size } == 1
-
-            @filter.filter(end_event(ID_FIELD => @id_value))
-
-            insist { @filter.start_events.size } == 0
-          end
-
-          shared_examples_for "match event" do
-            it "contains the tag 'elapsed'" do
-              insist { @match_event["tags"].include?("elapsed") } == true
-            end
-
-            it "contains the tag tag 'elapsed.match'" do
-              insist { @match_event["tags"].include?("elapsed.match") } == true
-            end
-
-            it "contains an 'elapsed.time field' with the elapsed time" do
-              insist { @match_event["elapsed.time"] } == 10
-            end
-
-            it "contains an 'elapsed.timestamp_start field' with the timestamp of the 'start event'" do
-              insist { @match_event["elapsed.timestamp_start"] } == @start_event["@timestamp"]
-            end
-
-            it "contains an 'id field'" do
-              insist { @match_event[ID_FIELD] } == @id_value
-            end
-          end
-
-          context "if 'new_event_on_match' is set to 'true'" do
-            before(:each) do
-              # I need to create a new filter because I need to set
-              # the config property 'new_event_on_match" to 'true'.
-              setup_filter("new_event_on_match" => true)
-              @start_event = start_event(ID_FIELD => @id_value)
-              @filter.filter(@start_event)
-
-              end_timestamp = @start_event["@timestamp"] + 10
-              end_event = end_event(ID_FIELD => @id_value, "@timestamp" => end_timestamp)
-              @filter.filter(end_event) do |new_event|
-                @match_event = new_event
-              end
-            end
-
-            context "creates a new event that" do
-              it_behaves_like "match event"
-
-              it "contains the 'host field'" do
-                insist { @match_event["host"] } == Socket.gethostname
-              end
-            end
-          end
-
-          context "if 'new_event_on_match' is set to 'false'" do
-            before(:each) do
-              end_timestamp = @start_event["@timestamp"] + 10
-              end_event = end_event(ID_FIELD => @id_value, "@timestamp" => end_timestamp)
-              @filter.filter(end_event)
-
-              @match_event = end_event
-            end
-
-            context "modifies the 'end event' that" do
-              it_behaves_like "match event"
-            end
-          end
-
-        end
-      end
-    end
-  end
-
-  describe "#flush" do
-    def setup(timeout = 1000)
-      @config["timeout"] = timeout
-      @filter = LogStash::Filters::Elapsed.new(@config)
-      @filter.register
-
-      @start_event_1 = start_event(ID_FIELD => "1")
-      @start_event_2 = start_event(ID_FIELD => "2")
-      @start_event_3 = start_event(ID_FIELD => "3")
-
-      @filter.filter(@start_event_1)
-      @filter.filter(@start_event_2)
-      @filter.filter(@start_event_3)
-
-      # Force recorded events to different ages
-      @filter.start_events["2"].age = 25
-      @filter.start_events["3"].age = 26
-    end
-
-    it "increments the 'age' of all the recorded 'start events' by 5 seconds" do
-      setup()
-      old_age = ages()
-
-      @filter.flush()
-
-      ages().each_with_index do |new_age, i|
-        insist { new_age } == (old_age[i] + 5)
-      end
-    end
-
-    def ages()
-      @filter.start_events.each_value.map{|element| element.age }
-    end
-
-    context "if the 'timeout interval' is set to 30 seconds" do
-      before(:each) do
-        setup(30)
-
-        @expired_events = @filter.flush()
-
-        insist { @filter.start_events.size } == 1
-        insist { @expired_events.size } == 2
-      end
-
-      it "deletes the recorded 'start events' with 'age' greater, or equal to, the timeout" do
-        insist { @filter.start_events.key?("1") } == true
-        insist { @filter.start_events.key?("2") } == false
-        insist { @filter.start_events.key?("3") } == false
-      end
-
-      it "creates a new event with tag 'elapsed.expired_error' for each expired 'start event'" do
-        insist { @expired_events[0]["tags"].include?("elapsed.expired_error") } == true
-        insist { @expired_events[1]["tags"].include?("elapsed.expired_error") } == true
-      end
-
-      it "creates a new event with tag 'elapsed' for each expired 'start event'" do
-        insist { @expired_events[0]["tags"].include?("elapsed") } == true
-        insist { @expired_events[1]["tags"].include?("elapsed") } == true
-      end
-
-      it "creates a new event containing the 'id field' of the expired 'start event'" do
-        insist { @expired_events[0][ID_FIELD] } == "2"
-        insist { @expired_events[1][ID_FIELD] } == "3"
-      end
-
-      it "creates a new event containing an 'elapsed.time field' with the age of the expired 'start event'" do
-        insist { @expired_events[0]["elapsed.time"] } == 30
-        insist { @expired_events[1]["elapsed.time"] } == 31
-      end
-
-      it "creates a new event containing an 'elapsed.timestamp_start field' with the timestamp of the expired 'start event'" do
-        insist { @expired_events[0]["elapsed.timestamp_start"] } == @start_event_2["@timestamp"]
-        insist { @expired_events[1]["elapsed.timestamp_start"] } == @start_event_3["@timestamp"]
-      end
-
-      it "creates a new event containing a 'host field' for each expired 'start event'" do
-        insist { @expired_events[0]["host"] } == Socket.gethostname
-        insist { @expired_events[1]["host"] } == Socket.gethostname
-      end
-    end
-  end
-end
diff --git a/spec/filters/environment.rb b/spec/filters/environment.rb
deleted file mode 100644
index d0d54505de7..00000000000
--- a/spec/filters/environment.rb
+++ /dev/null
@@ -1,43 +0,0 @@
-require "test_utils"
-require "logstash/filters/environment"
-
-describe LogStash::Filters::Environment do
-  extend LogStash::RSpec
-
-  describe "add a field from the environment" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        environment {
-          add_field_from_env => [ "newfield", "MY_ENV_VAR" ]
-        }
-      }
-    CONFIG
-
-    ENV["MY_ENV_VAR"] = "hello world"
-
-    sample "example" do
-      insist { subject["newfield"] } == "hello world"
-    end
-  end
-
-  describe "does nothing on non-matching events" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        environment {
-          type => "foo"
-          add_field_from_env => [ "newfield", "MY_ENV_VAR" ]
-        }
-      }
-    CONFIG
-
-    ENV["MY_ENV_VAR"] = "hello world"
-
-    sample("type" => "bar", "message" => "fizz") do
-      insist { subject["newfield"] }.nil?
-    end
-  end
-end
diff --git a/spec/filters/extractnumbers.rb b/spec/filters/extractnumbers.rb
deleted file mode 100644
index 163483f1605..00000000000
--- a/spec/filters/extractnumbers.rb
+++ /dev/null
@@ -1,24 +0,0 @@
-require "test_utils"
-require "logstash/filters/extractnumbers"
-
-describe LogStash::Filters::ExtractNumbers do
-  extend LogStash::RSpec
-
-  describe "Extract numbers test" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        extractnumbers {
-        }
-      }
-    CONFIG
-
-    sample("message" => "bla 1234 foo 5678 geek 10.43") do
-      insist { subject["int1"] } == 1234
-      insist { subject["int2"] } == 5678
-      insist { subject["float1"] } == 10.43
-    end
-  end
-
-end
diff --git a/spec/filters/gelfify.rb b/spec/filters/gelfify.rb
deleted file mode 100644
index af1c2ec9663..00000000000
--- a/spec/filters/gelfify.rb
+++ /dev/null
@@ -1,34 +0,0 @@
-require "test_utils"
-require "logstash/filters/gelfify"
-
-describe LogStash::Filters::Gelfify do
-  extend LogStash::RSpec
-
-    SYSLOG_LEVEL_MAP = {
-    0 => 3, # Emergency => FATAL
-    1 => 5, # Alert     => WARN
-    2 => 3, # Critical  => FATAL
-    3 => 4, # Error     => ERROR
-    4 => 5, # Warning   => WARN
-    5 => 6, # Notice    => INFO
-    6 => 6, # Informat. => INFO
-    7 => 7  # Debug     => DEBUG
-  }
-
-  SYSLOG_LEVEL_MAP.each do |k,v|
-
-    describe "gelfify #{k} to #{v}" do
-      config <<-CONFIG
-        filter {
-          gelfify { }
-        }
-      CONFIG
-
-      sample("severity" => k) do
-        insist { subject["GELF_severity"] } == v
-      end
-    end
-
-  end
-
-end
diff --git a/spec/filters/grep.rb b/spec/filters/grep.rb
deleted file mode 100644
index dae37a3deb6..00000000000
--- a/spec/filters/grep.rb
+++ /dev/null
@@ -1,342 +0,0 @@
-require "test_utils"
-require "logstash/filters/grep"
-
-describe LogStash::Filters::Grep do
-  extend LogStash::RSpec
-
-  describe "single grep match" do
-    config <<-CONFIG
-      filter {
-        grep {
-          match => [ "str", "test" ]
-        }
-      }
-    CONFIG
-
-    sample("str" => "test: this should not be dropped") do
-      reject { subject }.nil?
-    end
-
-    sample("str" => "foo: this should be dropped") do
-      insist { subject }.nil?
-    end
-  end
-
-  describe "single match failure cancels the event" do
-    config <<-CONFIG
-    filter {
-      grep {
-        match => [ "str",  "test" ]
-      }
-    }
-    CONFIG
-
-    sample("str" => "foo: this should be dropped") do
-      insist { subject }.nil?
-    end
-  end
-
-  describe "single match failure does not cancel the event with drop set to false" do
-    config <<-CONFIG
-    filter {
-      grep {
-        match => [ "str", "test" ]
-        drop => false
-      }
-    }
-    CONFIG
-
-    sample("str" => "foo: this should not be dropped") do
-      reject { subject }.nil?
-    end
-  end
-
-  describe "multiple match conditions" do
-    config <<-CONFIG
-    filter {
-      grep {
-        match => [
-          "str", "test",
-          "bar", "baz"
-        ]
-      }
-    }
-    CONFIG
-
-    sample("str" => "test: this should not be dropped", "bar" => "foo baz foo") do
-      reject { subject }.nil?
-    end
-  end
-
-  describe "multiple match conditions should cancel on failure" do
-    config <<-CONFIG
-    filter {
-      grep {
-        match => [ 
-          "str", "test",
-          "bar", "baz"
-        ]
-      }
-    }
-    CONFIG
-
-    sample("str" => "test: this should be dropped", "bar" => "foo bAz foo") do
-      insist { subject }.nil?
-    end
-  end
-
-  describe "single condition with regexp syntax" do
-    config <<-CONFIG
-    filter {
-      grep {
-        match => [ "str", "(?i)test.*foo"]
-      }
-    }
-    CONFIG
-
-    sample("str" => "TeST regexp match FoO") do
-      reject { subject }.nil?
-    end
-  end
-
-  describe "single condition with regexp syntax cancels on failure" do
-    config <<-CONFIG
-    filter {
-      grep {
-        match => [ "str", "test.*foo" ]
-      }
-    }
-    CONFIG
-
-    sample("str" => "TeST regexp match FoO") do
-      insist { subject }.nil?
-    end
-  end
-
-  describe "adding one field on success" do
-    config <<-CONFIG
-    filter {
-      grep {
-        match => [ "str", "test" ]
-        add_field => ["new_field", "new_value"]
-      }
-    }
-    CONFIG
-
-    sample("str" => "test") do
-      reject { subject }.nil?
-      insist { subject["new_field"]} == "new_value"
-    end
-  end
-
-  describe "adding one field with a sprintf value" do
-    config <<-CONFIG
-    filter {
-      grep {
-        match => [ "str", "test" ]
-        add_field => ["new_field", "%{type}"]
-      }
-    }
-    CONFIG
-
-    sample("type" => "grepper", "str" => "test") do
-      reject { subject }.nil?
-      insist { subject["new_field"]} == subject["type"]
-    end
-  end
-
-  # following test was __DISABLED_FOR_NOW_, remember why ?
-  # Seems to be multi-match several time on the same field not allowed
-  # maybe a clearer test on multi-match on same field could be created
-  # Also add_field behaviour tested separately in new NOOP test for add_field
-
-  # describe "adding fields on successful multiple match" do
-  #   config <<-CONFIG
-  #   filter {
-  #     grep {
-  #       match => [ "str", "test" ]
-  #       add_field => ["new_field", "new_value"]
-  #       match => [ "str", ".*" ]
-  #       add_field => ["new_field", "new_value_2"]
-  #     }
-  #   }
-  #   CONFIG
-  #
-  #   sample("type" => "grepper", "str" => "test") do
-  #     reject { subject }.nil?
-  #     insist { subject["new_field"]} == ["new_value", "new_value_2"]
-  #   end
-  # end
-
-  describe "add tags" do
-    config <<-CONFIG
-    filter {
-      grep {
-        match => [ "str", "test" ]
-        add_tag => ["new_tag"]
-      }
-    }
-    CONFIG
-
-    sample("tags" => ["tag"], "str" => "test") do
-      reject { subject }.nil?
-      insist { subject["tags"]} == ["tag", "new_tag"]
-    end
-  end
-
-  describe "add tags with drop set to false tags matching events" do
-    config <<-CONFIG
-    filter {
-      grep {
-        match => [ "str", "test" ]
-        drop => false
-        add_tag => ["new_tag"]
-      }
-    }
-    CONFIG
-
-    sample("tags" => ["tag"], "str" => "test") do
-      reject { subject }.nil?
-      insist { subject["tags"]} == ["tag", "new_tag"]
-    end
-  end
-
-  describe "add tags with drop set to false allows non-matching events through" do
-    config <<-CONFIG
-    filter {
-      grep {
-        match => [ "str", "test" ]
-        drop => false
-        add_tag => ["new_tag"]
-      }
-    }
-    CONFIG
-
-    sample("tags" => ["tag"], "str" => "non-matching") do
-      reject { subject }.nil?
-      insist { subject["tags"]} == ["tag"]
-    end
-  end
-
-  describe "add tags with sprintf value" do
-    config <<-CONFIG
-    filter {
-      grep {
-        match => [ "str", "test" ]
-        add_tag => ["%{str}"]
-      }
-    }
-    CONFIG
-
-    sample("tags" => ["tag"], "str" => "test") do
-      reject { subject }.nil?
-      insist { subject["tags"]} == ["tag", subject["str"]]
-    end
-  end
-
-  describe "negate=true should not cause drops when field is nil" do
-    # Set negate to true; the pattern being searched doesn't actually matter
-    # here. We're testing to make sure "grep -v" behavior doesn't drop events
-    # that don't even have the field being filtered for.
-    config <<-CONFIG
-    filter {
-      grep {
-        match => [ "str", "doesn't matter lol" ]
-        negate => true
-      }
-    }
-    CONFIG
-
-    sample("tags" => ["tag"], "str" => nil) do
-      reject { subject }.nil?
-    end
-  end
-
-  #LOGSTASH-599
-  describe "drop line based on type and tags 'matching' only but otherwise pattern matching" do
-    config <<-CONFIG
-    filter {
-      grep {
-        type => "testing"
-        tags => ["_grokparsefailure"]
-        negate => true
-      }
-    }
-    CONFIG
-
-    sample("type" => "testing", "tags" => ["_grokparsefailure"], "str" => "test") do
-      insist { subject }.nil?
-    end
-  end
-
-  #LOGSTASH-894 and LOGSTASH-919
-  describe "repeat a field in match config, similar to piped grep command line" do
-    config <<-CONFIG
-    filter {
-      grep {
-        match => ["message", "hello", "message", "world"]
-      }
-    }
-    CONFIG
-
-    #both match
-    sample "hello world" do
-      reject { subject }.nil?
-    end
-    #one match
-    sample "bye world" do
-      insist { subject }.nil?
-    end
-    #one match
-    sample "hello Jordan" do
-      insist { subject }.nil?
-    end
-    #no match
-    sample "WTF" do
-      insist { subject }.nil?
-    end
-  end
-
-  describe "repeat a field in match config, similar to several -e in grep command line" do
-    config <<-CONFIG
-    filter {
-      grep {
-        match => ["message", "hello", "message", "world"]
-        negate => true
-      }
-    }
-    CONFIG
-
-    #both match
-    sample "hello world" do
-      insist { subject }.nil?
-    end
-    #one match
-    sample "bye world" do
-      insist { subject }.nil?
-    end
-    #one match
-    sample "hello Jordan" do
-      insist { subject }.nil?
-    end
-    #no match
-    sample "WTF" do
-      reject { subject }.nil?
-    end
-  end
-
-  describe "case-insensitive matching" do
-    config <<-CONFIG
-      filter {
-        grep {
-          ignore_case => true
-          match => [ "str", "test" ]
-        }
-      }
-    CONFIG
-
-    sample("str" => "tEsT: this should still be matched") do
-      reject { subject }.nil?
-    end
-  end
-end
diff --git a/spec/filters/i18n.rb b/spec/filters/i18n.rb
deleted file mode 100644
index 088c68f9245..00000000000
--- a/spec/filters/i18n.rb
+++ /dev/null
@@ -1,25 +0,0 @@
-# encoding: utf-8
-require "test_utils"
-require "logstash/filters/i18n"
-
-describe LogStash::Filters::I18n do
-  extend LogStash::RSpec
-
-  describe "transliterate" do
-    config <<-CONFIG
-      filter {
-        i18n {
-          transliterate => [ "transliterateme" ]
-        }
-      }
-    CONFIG
-
-    event = {
-      "transliterateme" => [ "Ærøskøbing" ]
-    }
-
-    sample event do
-      insist { subject["transliterateme"] } == [ "AEroskobing" ]
-    end
-  end
-end
diff --git a/spec/filters/json_encode.rb b/spec/filters/json_encode.rb
deleted file mode 100644
index 93ccc039ed4..00000000000
--- a/spec/filters/json_encode.rb
+++ /dev/null
@@ -1,37 +0,0 @@
-require "test_utils"
-require "logstash/filters/json_encode"
-
-describe LogStash::Filters::JSONEncode do
-  extend LogStash::RSpec
-
-  describe "encode a field as json" do
-    config <<-CONFIG
-      filter {
-        json_encode {
-          source => "hello"
-          target => "fancy"
-        }
-      }
-    CONFIG
-
-    hash = { "hello" => { "whoa" => [ 1,2,3 ] } }
-    sample(hash) do
-      insist { JSON.parse(subject["fancy"]).to_json } == hash["hello"].to_json
-    end
-  end
-
-  describe "encode a field as json and overwrite the original" do
-    config <<-CONFIG
-      filter {
-        json_encode {
-          source => "hello"
-        }
-      }
-    CONFIG
-
-    hash = { "hello" => { "whoa" => [ 1,2,3 ] } }
-    sample(hash) do
-      insist { JSON.parse(subject["hello"]).to_json } == hash["hello"].to_json
-    end
-  end
-end
diff --git a/spec/filters/prune.rb b/spec/filters/prune.rb
deleted file mode 100644
index 9e6dc631b4c..00000000000
--- a/spec/filters/prune.rb
+++ /dev/null
@@ -1,441 +0,0 @@
-require "test_utils"
-require "logstash/filters/prune"
-
-# Currently the prune filter has bugs and I can't really tell what the intended
-# behavior is.
-#
-# See the 'whitelist field values with interpolation' test for a commented
-# explanation of my confusion.
-describe LogStash::Filters::Prune, :if => false  do
-  extend LogStash::RSpec
-
-  describe "defaults" do
-
-    config <<-CONFIG
-      filter {
-        prune { }
-      }
-    CONFIG
-    
-    sample(
-      "firstname"    => "Borat",
-      "lastname"     => "Sagdiyev",
-      "fullname"     => "Borat Sagdiyev",
-      "country"      => "Kazakhstan",
-      "location"     => "Somethere in Kazakhstan",
-      "hobby"        => "Cloud",
-      "status"       => "200",
-      "Borat_saying" => "Cloud is not ready for enterprise if is not integrate with single server running Active Directory.",
-      "%{hmm}"       => "doh"
-    ) do
-      insist { subject["firstname"] } == "Borat"
-      insist { subject["lastname"] } == "Sagdiyev"
-      insist { subject["fullname"] } == "Borat Sagdiyev"
-      insist { subject["country"] } == "Kazakhstan"
-      insist { subject["location"] } == "Somethere in Kazakhstan"
-      insist { subject["hobby"] } == "Cloud"
-      insist { subject["status"] } == "200"
-      insist { subject["Borat_saying"] } == "Cloud is not ready for enterprise if is not integrate with single server running Active Directory."
-      insist { subject["%{hmm}"] } == nil
-    end
-  end
-
-  describe "whitelist field names" do
-
-    config <<-CONFIG
-      filter {
-        prune {
-          whitelist_names => [ "firstname", "(hobby|status)", "%{firstname}_saying" ]
-        }
-      }
-    CONFIG
-    
-    sample(
-      "firstname"    => "Borat",
-      "lastname"     => "Sagdiyev",
-      "fullname"     => "Borat Sagdiyev",
-      "country"      => "Kazakhstan",
-      "location"     => "Somethere in Kazakhstan",
-      "hobby"        => "Cloud",
-      "status"       => "200",
-      "Borat_saying" => "Cloud is not ready for enterprise if is not integrate with single server running Active Directory.",
-      "%{hmm}"       => "doh"
-    ) do
-      insist { subject["firstname"] } == "Borat"
-      insist { subject["lastname"] } == nil
-      insist { subject["fullname"] } == nil
-      insist { subject["country"] } == nil
-      insist { subject["location"] } == nil
-      insist { subject["hobby"] } == "Cloud"
-      insist { subject["status"] } == "200"
-      insist { subject["Borat_saying"] } == nil
-      insist { subject["%{hmm}"] } == nil
-    end
-  end
-
-  describe "whitelist field names with interpolation" do
-
-    config <<-CONFIG
-      filter {
-        prune {
-          whitelist_names => [ "firstname", "(hobby|status)", "%{firstname}_saying" ]
-          interpolate     => true
-        }
-      }
-    CONFIG
-    
-    sample(
-      "firstname"    => "Borat",
-      "lastname"     => "Sagdiyev",
-      "fullname"     => "Borat Sagdiyev",
-      "country"      => "Kazakhstan",
-      "location"     => "Somethere in Kazakhstan",
-      "hobby"        => "Cloud",
-      "status"       => "200",
-      "Borat_saying" => "Cloud is not ready for enterprise if is not integrate with single server running Active Directory.",
-      "%{hmm}"       => "doh"
-    ) do
-      insist { subject["firstname"] } == "Borat"
-      insist { subject["lastname"] } == nil
-      insist { subject["fullname"] } == nil
-      insist { subject["country"] } == nil
-      insist { subject["location"] } == nil
-      insist { subject["hobby"] } == "Cloud"
-      insist { subject["status"] } == "200"
-      insist { subject["Borat_saying"] } == "Cloud is not ready for enterprise if is not integrate with single server running Active Directory."
-      insist { subject["%{hmm}"] } == nil
-    end
-  end
-
-  describe "blacklist field names" do
-
-    config <<-CONFIG
-      filter {
-        prune {
-          blacklist_names => [ "firstname", "(hobby|status)", "%{firstname}_saying" ]
-        }
-      }
-    CONFIG
-
-    sample(
-      "firstname"    => "Borat",
-      "lastname"     => "Sagdiyev",
-      "fullname"     => "Borat Sagdiyev",
-      "country"      => "Kazakhstan",
-      "location"     => "Somethere in Kazakhstan",
-      "hobby"        => "Cloud",
-      "status"       => "200",
-      "Borat_saying" => "Cloud is not ready for enterprise if is not integrate with single server running Active Directory.",
-      "%{hmm}"       => "doh"
-    ) do
-      insist { subject["firstname"] } == nil
-      insist { subject["lastname"] } == "Sagdiyev"
-      insist { subject["fullname"] } == "Borat Sagdiyev"
-      insist { subject["country"] } == "Kazakhstan"
-      insist { subject["location"] } == "Somethere in Kazakhstan"
-      insist { subject["hobby"] } == nil
-      insist { subject["status"] } == nil
-      insist { subject["Borat_saying"] } == "Cloud is not ready for enterprise if is not integrate with single server running Active Directory."
-      insist { subject["%{hmm}"] } == "doh"
-    end
-  end
-
-  describe "blacklist field names with interpolation" do
-
-    config <<-CONFIG
-      filter {
-        prune {
-          blacklist_names => [ "firstname", "(hobby|status)", "%{firstname}_saying" ]
-          interpolate     => true
-        }
-      }
-    CONFIG
-
-    sample(
-      "firstname"    => "Borat",
-      "lastname"     => "Sagdiyev",
-      "fullname"     => "Borat Sagdiyev",
-      "country"      => "Kazakhstan",
-      "location"     => "Somethere in Kazakhstan",
-      "hobby"        => "Cloud",
-      "status"       => "200",
-      "Borat_saying" => "Cloud is not ready for enterprise if is not integrate with single server running Active Directory.",
-      "%{hmm}"       => "doh"
-    ) do
-      insist { subject["firstname"] } == nil
-      insist { subject["lastname"] } == "Sagdiyev"
-      insist { subject["fullname"] } == "Borat Sagdiyev"
-      insist { subject["country"] } == "Kazakhstan"
-      insist { subject["location"] } == "Somethere in Kazakhstan"
-      insist { subject["hobby"] } == nil
-      insist { subject["status"] } == nil
-      insist { subject["Borat_saying"] } == nil
-      insist { subject["%{hmm}"] } == "doh"
-    end
-  end
-
-  describe "whitelist field values" do
-
-    config <<-CONFIG
-      filter {
-        prune {
-          # This should only  permit fields named 'firstname', 'fullname',
-          # 'location', 'status', etc.
-          whitelist_values => [ "firstname", "^Borat$",
-                                "fullname", "%{firstname} Sagdiyev",
-                                "location", "no no no",
-                                "status", "^2",
-                                "%{firstname}_saying", "%{hobby}.*Active" ]
-        }
-      }
-    CONFIG
-
-    sample(
-      "firstname"    => "Borat",
-      "lastname"     => "Sagdiyev",
-      "fullname"     => "Borat Sagdiyev",
-      "country"      => "Kazakhstan",
-      "location"     => "Somethere in Kazakhstan",
-      "hobby"        => "Cloud",
-      "status"       => "200",
-      "Borat_saying" => "Cloud is not ready for enterprise if is not integrate with single server running Active Directory.",
-      "%{hmm}"       => "doh"
-    ) do
-      insist { subject["firstname"] } == "Borat"
-
-      # TODO(sissel): According to the config above, this should be nil because
-      # it is not in the list of whitelisted fields, but we expect it to be
-      # "Sagdiyev" ? I am confused.
-      insist { subject["lastname"] } == "Sagdiyev"
-      insist { subject["fullname"] } == nil
-      insist { subject["country"] } == "Kazakhstan"
-      insist { subject["location"] } == nil
-      insist { subject["hobby"] } == "Cloud"
-      insist { subject["status"] } == "200"
-      insist { subject["Borat_saying"] } == "Cloud is not ready for enterprise if is not integrate with single server running Active Directory."
-
-      # TODO(sissel): Contrary to the 'lastname' check, we expect %{hmm} field
-      # to be nil because it is not whitelisted, yes? Contradictory insists 
-      # here. I don't know what the intended behavior is... Seems like
-      # whitelist means 'anything not here' but since this test is written
-      # confusingly, I dont' know how to move forward.
-      insist { subject["%{hmm}"] } == nil
-    end
-  end
-
-  describe "whitelist field values with interpolation" do
-
-    config <<-CONFIG
-      filter {
-        prune {
-          whitelist_values => [ "firstname", "^Borat$",
-                                "fullname", "%{firstname} Sagdiyev",
-                                "location", "no no no",
-                                "status", "^2",
-                                "%{firstname}_saying", "%{hobby}.*Active" ]
-          interpolate      => true
-        }
-      }
-    CONFIG
-
-    sample(
-      "firstname"    => "Borat",
-      "lastname"     => "Sagdiyev",
-      "fullname"     => "Borat Sagdiyev",
-      "country"      => "Kazakhstan",
-      "location"     => "Somethere in Kazakhstan",
-      "hobby"        => "Cloud",
-      "status"       => "200",
-      "Borat_saying" => "Cloud is not ready for enterprise if is not integrate with single server running Active Directory.",
-      "%{hmm}"       => "doh"
-    ) do
-      insist { subject["firstname"] } == "Borat"
-      insist { subject["lastname"] } == "Sagdiyev"
-      insist { subject["fullname"] } == "Borat Sagdiyev"
-      insist { subject["country"] } == "Kazakhstan"
-      insist { subject["location"] } == nil
-      insist { subject["hobby"] } == "Cloud"
-      insist { subject["status"] } == "200"
-      insist { subject["Borat_saying"] } == "Cloud is not ready for enterprise if is not integrate with single server running Active Directory."
-      insist { subject["%{hmm}"] } == nil
-    end
-  end
-
-  describe "blacklist field values" do
-
-    config <<-CONFIG
-      filter {
-        prune {
-          blacklist_values => [ "firstname", "^Borat$",
-                                "fullname", "%{firstname} Sagdiyev",
-                                "location", "no no no",
-                                "status", "^2",
-                                "%{firstname}_saying", "%{hobby}.*Active" ]
-        }
-      }
-    CONFIG
-
-    sample(
-      "firstname"    => "Borat",
-      "lastname"     => "Sagdiyev",
-      "fullname"     => "Borat Sagdiyev",
-      "country"      => "Kazakhstan",
-      "location"     => "Somethere in Kazakhstan",
-      "hobby"        => "Cloud",
-      "status"       => "200",
-      "Borat_saying" => "Cloud is not ready for enterprise if is not integrate with single server running Active Directory.",
-      "%{hmm}"       => "doh"
-    ) do
-      insist { subject["firstname"] } == nil
-      insist { subject["lastname"] } == "Sagdiyev"
-      insist { subject["fullname"] } == "Borat Sagdiyev"
-      insist { subject["country"] } == "Kazakhstan"
-      insist { subject["location"] } == "Somethere in Kazakhstan"
-      insist { subject["hobby"] } == "Cloud"
-      insist { subject["status"] } == nil
-      insist { subject["Borat_saying"] } == "Cloud is not ready for enterprise if is not integrate with single server running Active Directory."
-      insist { subject["%{hmm}"] } == nil
-    end
-  end
-
-  describe "blacklist field values with interpolation" do
-
-    config <<-CONFIG
-      filter {
-        prune {
-          blacklist_values => [ "firstname", "^Borat$",
-                                "fullname", "%{firstname} Sagdiyev",
-                                "location", "no no no",
-                                "status", "^2",
-                                "%{firstname}_saying", "%{hobby}.*Active" ]
-          interpolate      => true
-        }
-      }
-    CONFIG
-
-    sample(
-      "firstname"    => "Borat",
-      "lastname"     => "Sagdiyev",
-      "fullname"     => "Borat Sagdiyev",
-      "country"      => "Kazakhstan",
-      "location"     => "Somethere in Kazakhstan",
-      "hobby"        => "Cloud",
-      "status"       => "200",
-      "Borat_saying" => "Cloud is not ready for enterprise if is not integrate with single server running Active Directory.",
-      "%{hmm}"       => "doh"
-    ) do
-      insist { subject["firstname"] } == nil
-      insist { subject["lastname"] } == "Sagdiyev"
-      insist { subject["fullname"] } == nil
-      insist { subject["country"] } == "Kazakhstan"
-      insist { subject["location"] } == "Somethere in Kazakhstan"
-      insist { subject["hobby"] } == "Cloud"
-      insist { subject["status"] } == nil
-      insist { subject["Borat_saying"] } == nil
-      insist { subject["%{hmm}"] } == nil
-    end
-  end
-
-  describe "whitelist field values on fields witn array values" do
-
-    config <<-CONFIG
-      filter {
-        prune {
-          whitelist_values => [ "status", "^(1|2|3)",
-                                "xxx", "3",
-                                "error", "%{blah}" ]
-        }
-      }
-    CONFIG
-
-    sample(
-      "blah"   => "foo",
-      "xxx" => [ "1 2 3", "3 4 5" ],
-      "status" => [ "100", "200", "300", "400", "500" ],
-      "error"  => [ "This is foolish" , "Need smthing smart too" ]
-    ) do
-      insist { subject["blah"] } == "foo"
-      insist { subject["error"] } == nil
-      insist { subject["xxx"] } == [ "1 2 3", "3 4 5" ]
-      insist { subject["status"] } == [ "100", "200", "300" ]
-    end
-  end
-
-  describe "blacklist field values on fields witn array values" do
-
-    config <<-CONFIG
-      filter {
-        prune {
-          blacklist_values => [ "status", "^(1|2|3)",
-                                "xxx", "3",
-                                "error", "%{blah}" ]
-        }
-      }
-    CONFIG
-
-    sample(
-      "blah"   => "foo",
-      "xxx" => [ "1 2 3", "3 4 5" ],
-      "status" => [ "100", "200", "300", "400", "500" ],
-      "error"  => [ "This is foolish", "Need smthing smart too" ]
-    ) do
-      insist { subject["blah"] } == "foo"
-      insist { subject["error"] } == [ "This is foolish", "Need smthing smart too" ]
-      insist { subject["xxx"] } == nil
-      insist { subject["status"] } == [ "400", "500" ]
-    end
-  end
-
-  describe "whitelist field values with interpolation on fields witn array values" do
- 
-    config <<-CONFIG
-      filter {
-        prune {
-          whitelist_values => [ "status", "^(1|2|3)",
-                                "xxx", "3",
-                                "error", "%{blah}" ]
-          interpolate      => true
-        }
-      }
-    CONFIG
-
-    sample(
-      "blah"   => "foo",
-      "xxx" => [ "1 2 3", "3 4 5" ],
-      "status" => [ "100", "200", "300", "400", "500" ],
-      "error"  => [ "This is foolish" , "Need smthing smart too" ]
-    ) do
-      insist { subject["blah"] } == "foo"
-      insist { subject["error"] } == [ "This is foolish" ]
-      insist { subject["xxx"] } == [ "1 2 3", "3 4 5" ]
-      insist { subject["status"] } == [ "100", "200", "300" ]
-    end
-  end
-
-  describe "blacklist field values with interpolation on fields witn array values" do
-
-    config <<-CONFIG
-      filter {
-        prune {
-          blacklist_values => [ "status", "^(1|2|3)",
-                                "xxx", "3",
-                                "error", "%{blah}" ]
-          interpolate      => true
-        }
-      }
-    CONFIG
-
-    sample(
-      "blah"   => "foo",
-      "xxx" => [ "1 2 3", "3 4 5" ],
-      "status" => [ "100", "200", "300", "400", "500" ],
-      "error"  => [ "This is foolish" , "Need smthing smart too" ]
-    ) do
-      insist { subject["blah"] } == "foo"
-      insist { subject["error"] } == [ "Need smthing smart too" ]
-      insist { subject["xxx"] } == nil
-      insist { subject["status"] } == [ "400", "500" ]
-    end
-  end
-
-end
diff --git a/spec/filters/punct.rb b/spec/filters/punct.rb
deleted file mode 100644
index 655bdb2cb6a..00000000000
--- a/spec/filters/punct.rb
+++ /dev/null
@@ -1,18 +0,0 @@
-require "test_utils"
-require "logstash/filters/punct"
-
-describe LogStash::Filters::Punct do
-  extend LogStash::RSpec
-
-  describe "all defaults" do
-    config <<-CONFIG
-      filter {
-        punct { }
-      }
-    CONFIG
-
-    sample "PHP Warning:  json_encode() [<a href='function.json-encode'>function.json-encode</a>]: Invalid UTF-8 sequence in argument in /var/www/htdocs/test.php on line 233" do
-      insist { subject["punct"] } == ":_()[<='.-'>.-</>]:-////."
-    end
-  end
-end
diff --git a/spec/filters/railsparallelrequest.rb b/spec/filters/railsparallelrequest.rb
deleted file mode 100644
index a43a926a9fd..00000000000
--- a/spec/filters/railsparallelrequest.rb
+++ /dev/null
@@ -1,112 +0,0 @@
-require "test_utils"
-require "logstash/filters/railsparallelrequest"
-
-describe LogStash::Filters::Railsparallelrequest do
-
-  context :filter do
-
-    it "should not process same event twice" do
-      filter = LogStash::Filters::Railsparallelrequest.new
-      event = LogStash::Event.new({message: "hello world"})
-      event["tags"]=[]
-      filter.filter event
-      insist { event["tags"] } == ["railsparallelrequest"]
-      filter.filter event
-      insist { event["tags"] } == ["railsparallelrequest"]
-    end
-
-    it "should merge multiple events into single event based on unique UUID" do
-      filter = LogStash::Filters::Railsparallelrequest.new
-      filter.filter event({"message" => "[UUID]hello"})
-      filter.filter event({"message" => "[UUID]world"})
-      events = filter.flush
-      insist { events.first["message"]} == ["[UUID]hello","world"]
-    end
-
-    it "should cancel merged events until completed and completed event should have consolidated message" do
-      filter = LogStash::Filters::Railsparallelrequest.new
-      event1 = event({"message" => "[UUID]hello"})
-      filter.filter event1
-      insist { event1.cancelled? } == true
-      event2 = event({"message" => "[UUID]world"})
-      filter.filter event2
-      insist { event2.cancelled? } == true
-      event3 = event({"message" => "[UUID]Completed"})
-      filter.filter event3
-      insist { event3.cancelled? } == false
-      insist { event3["message"]} == ["[UUID]hello","world", "Completed"]
-    end
-
-    it "should not store the completed message" do
-      filter = LogStash::Filters::Railsparallelrequest.new
-      filter.filter event({"message" => "[UUID]hello"})
-      filter.filter event({"message" => "[UUID]Completed"})
-      insist { filter.flush.size } == 0
-    end
-
-    it "should handle interleaved messages and merge based on UUID" do
-      filter = LogStash::Filters::Railsparallelrequest.new
-      filter.filter event({"message" => "[UUID1]hello"})
-      filter.filter event({"message" => "[UUID2]new"})
-      filter.filter event({"message" => "[UUID1]world"})
-      filter.filter event({"message" => "[UUID2]world2"})
-
-      uuid1_completed = event({"message" => "[UUID1]Completed"})
-      filter.filter uuid1_completed
-      insist { uuid1_completed["message"] } == ["[UUID1]hello", "world", "Completed"]
-
-      uuid2_completed = event({"message" => "[UUID2]Completed"})
-      filter.filter uuid2_completed
-      insist { uuid2_completed["message"] } == ["[UUID2]new", "world2", "Completed"]
-    end
-
-    it "should merge message without UUID to previous message with UUID" do
-      filter = LogStash::Filters::Railsparallelrequest.new
-      filter.filter event({"message" => "[UUID1]hello"})
-      filter.filter event({"message" => "new"})
-      filter.filter event({"message" => "world"})
-      uuid1_completed = event({"message" => "[UUID1]Completed"})
-      filter.filter uuid1_completed
-      insist { uuid1_completed["message"] } == ["[UUID1]hello", "new", "world", "Completed"]
-    end
-
-    it "should not complete on error, wait until next UUID and complete" do
-      filter = LogStash::Filters::Railsparallelrequest.new
-      filter.filter event({"message" => "[UUID1]hello"})
-      filter.filter event({"message" => "new"})
-      filter.filter event({"message" => "world"})
-
-      uuid1_completed = event({"message" => "[UUID1]Error"})
-      filter.filter(uuid1_completed)
-      insist {uuid1_completed.cancelled?} == true
-
-      @error_event = nil
-      filter.filter(event({"message" => "[UUID2]Start"})) {|e| @error_event = e}
-
-      insist { @error_event["message"] } == ["[UUID1]hello", "new", "world", "Error"]
-      insist {@error_event.cancelled?} == false
-
-    end
-
-    it "should merge following messages even after error" do
-      filter = LogStash::Filters::Railsparallelrequest.new
-      filter.filter event({"message" => "[UUID1]hello"})
-      filter.filter event({"message" => "new"})
-      filter.filter event({"message" => "world"})
-      filter.filter(event({"message" => "[UUID1]Error"}))
-      filter.filter(event({"message" => "[UUID1]StackTrace"}))
-      @error_event = nil
-      filter.filter(event({"message" => "[UUID2]Start"})) {|e| @error_event = e}
-
-      insist { @error_event["message"] } == ["[UUID1]hello", "new", "world", "Error", "StackTrace"]
-    end
-
-  end
-
-end
-def event data
-  event = LogStash::Event.new(data)
-  event["tags"]=[]
-  event
-end
-
diff --git a/spec/filters/range.rb b/spec/filters/range.rb
deleted file mode 100644
index 83f4a6b469b..00000000000
--- a/spec/filters/range.rb
+++ /dev/null
@@ -1,169 +0,0 @@
-require "test_utils"
-require "logstash/filters/range"
-
-describe LogStash::Filters::Range do
-  extend LogStash::RSpec
-
-  describe "range match integer field on tag action" do
-    config <<-CONFIG
-      filter {
-        range {
-          ranges => [ "duration", 10, 100, "tag:cool",
-                      "duration", 1, 1, "tag:boring" ]
-        }
-      }
-    CONFIG
-
-    sample("duration" => 50) do
-      insist { subject["tags"] }.include?("cool")
-      reject { subject["tags"] }.include?("boring")
-    end
-  end
-
-  describe "range match float field on tag action" do
-    config <<-CONFIG
-      filter {
-        range {
-          ranges => [ "duration", 0, 100, "tag:cool",
-                      "duration", 0, 1, "tag:boring" ]
-        }
-      }
-    CONFIG
-
-    sample("duration" => 50.0) do
-      insist { subject["tags"] }.include?("cool")
-      reject { subject["tags"] }.include?("boring")
-    end
-  end
-
-  describe "range match string field on tag action" do
-    config <<-CONFIG
-      filter {
-        range {
-          ranges => [ "length", 0, 10, "tag:cool",
-                      "length", 0, 1, "tag:boring" ]
-        }
-      }
-    CONFIG
-
-    sample("length" => "123456789") do
-      insist { subject["tags"] }.include?("cool")
-      reject { subject["tags"] }.include?("boring")
-    end
-  end
-
-  describe "range match with negation" do
-    config <<-CONFIG
-      filter {
-        range {
-          ranges => [ "length", 0, 10, "tag:cool",
-                      "length", 0, 1, "tag:boring" ]
-          negate => true
-        }
-      }
-    CONFIG
-
-    sample("length" => "123456789") do
-      reject { subject["tags"] }.include?("cool")
-      insist { subject["tags"] }.include?("boring")
-    end
-  end
-
-  describe "range match on drop action" do
-    config <<-CONFIG
-      filter {
-        range {
-          ranges => [ "length", 0, 10, "drop" ]
-        }
-      }
-    CONFIG
-
-    sample("length" => "123456789") do
-      insist { subject }.nil?
-    end
-  end
-
-  describe "range match on field action with string value" do
-    config <<-CONFIG
-      filter {
-        range {
-          ranges => [ "duration", 10, 100, "field:cool:foo",
-                      "duration", 1, 1, "field:boring:foo" ]
-        }
-      }
-    CONFIG
-
-    sample("duration" => 50) do
-      insist { subject }.include?("cool")
-      insist { subject["cool"] } == "foo"
-      reject { subject }.include?("boring")
-    end
-  end
-
-  describe "range match on field action with integer value" do
-    config <<-CONFIG
-      filter {
-        range {
-          ranges => [ "duration", 10, 100, "field:cool:666",
-                      "duration", 1, 1, "field:boring:666" ]
-        }
-      }
-    CONFIG
-
-    sample("duration" => 50) do
-      insist { subject }.include?("cool")
-      insist { subject["cool"] } == 666
-      reject { subject }.include?("boring")
-    end
-  end
-
-  describe "range match on field action with float value" do
-    config <<-CONFIG
-      filter {
-        range {
-          ranges => [ "duration", 10, 100, "field:cool:3.14",
-                      "duration", 1, 1, "field:boring:3.14" ]
-        }
-      }
-    CONFIG
-
-    sample("duration" => 50) do
-      insist { subject }.include?("cool")
-      insist { subject["cool"] } == 3.14
-      reject { subject }.include?("boring")
-    end
-  end
-
-  describe "range match on tag action with dynamic string value" do
-    config <<-CONFIG
-      filter {
-        range {
-          ranges => [ "duration", 10, 100, "tag:cool_%{dynamic}_dynamic",
-                      "duration", 1, 1, "tag:boring_%{dynamic}_dynamic" ]
-        }
-      }
-    CONFIG
-
-    sample("duration" => 50, "dynamic" => "and") do
-      insist { subject["tags"] }.include?("cool_and_dynamic")
-      reject { subject["tags"] }.include?("boring_and_dynamic")
-    end
-  end
-
-  describe "range match on field action with dynamic string field and value" do
-    config <<-CONFIG
-      filter {
-        range {
-          ranges => [ "duration", 10, 100, "field:cool_%{dynamic}_dynamic:foo_%{dynamic}_bar",
-                      "duration", 1, 1, "field:boring_%{dynamic}_dynamic:foo_%{dynamic}_bar" ]
-        }
-      }
-    CONFIG
-
-    sample("duration" => 50, "dynamic" => "and") do
-      insist { subject }.include?("cool_and_dynamic")
-      insist { subject["cool_and_dynamic"] } == "foo_and_bar"
-      reject { subject }.include?("boring_and_dynamic")
-    end
-  end
-end
diff --git a/spec/filters/sumnumbers.rb b/spec/filters/sumnumbers.rb
deleted file mode 100644
index 5644497a441..00000000000
--- a/spec/filters/sumnumbers.rb
+++ /dev/null
@@ -1,39 +0,0 @@
-require "test_utils"
-require "logstash/filters/sumnumbers"
-
-describe LogStash::Filters::SumNumbers do
-  extend LogStash::RSpec
-
-  describe "sumnumbers test with default values" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        sumnumbers { }
-      }
-    CONFIG
-
-    sample("message" => "1 bla 3.25 10 100") do
-      insist { subject["sumNums"] } == 4
-      insist { subject["sumTotal"] } == 114.25
-    end
-  end
-
-  describe "sumnumbers test with other source field" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        sumnumbers {
-          source => 'mysource'
-        }
-      }
-    CONFIG
-
-    sample("mysource" => "1 foo 3.25 10 100") do
-      insist { subject["sumNums"] } == 4
-      insist { subject["sumTotal"] } == 114.25
-    end
-  end
-
-end
diff --git a/spec/filters/translate.rb b/spec/filters/translate.rb
deleted file mode 100644
index 50861239a2c..00000000000
--- a/spec/filters/translate.rb
+++ /dev/null
@@ -1,70 +0,0 @@
-require "test_utils"
-require "logstash/filters/translate"
-
-describe LogStash::Filters::Translate do
-  extend LogStash::RSpec
-
-  describe "exact translation" do
-    config <<-CONFIG
-      filter {
-        translate {
-          field       => "status"
-          destination => "translation"
-          dictionary  => [ "200", "OK",
-                           "300", "Redirect",
-                           "400", "Client Error",
-                           "500", "Server Error" ]
-          exact       => true
-          regex       => false
-        }
-      }
-    CONFIG
-
-    sample("status" => 200) do
-      insist { subject["translation"] } == "OK"
-    end
-  end
-
-  describe "multi translation" do
-    config <<-CONFIG
-      filter {
-        translate {
-          field       => "status"
-          destination => "translation"
-          dictionary  => [ "200", "OK",
-                           "300", "Redirect",
-                           "400", "Client Error",
-                          "500", "Server Error" ]
-          exact       => false
-          regex       => false
-        }
-      }
-    CONFIG
-
-    sample("status" => "200 & 500") do
-      insist { subject["translation"] } == "OK & Server Error"
-    end
-  end
-
-  describe "regex translation" do
-    config <<-CONFIG
-      filter {
-        translate {
-          field       => "status"
-          destination => "translation"
-          dictionary  => [ "^2[0-9][0-9]$", "OK",
-                           "^3[0-9][0-9]$", "Redirect",
-                           "^4[0-9][0-9]$", "Client Error",
-                           "^5[0-9][0-9]$", "Server Error" ]
-          exact       => true
-          regex       => true
-        }
-      }
-    CONFIG
-
-    sample("status" => "200") do
-      insist { subject["translation"] } == "OK"
-    end
-  end
-
-end
diff --git a/spec/filters/unique.rb b/spec/filters/unique.rb
deleted file mode 100644
index 6d9d42f9dfd..00000000000
--- a/spec/filters/unique.rb
+++ /dev/null
@@ -1,25 +0,0 @@
-require "test_utils"
-require "logstash/filters/unique"
-
-describe LogStash::Filters::Unique do
-  extend LogStash::RSpec
-
-  describe "unique when field is array" do
-    config <<-CONFIG
-    filter {
-      unique {
-        fields => ["noisy_field", "not_an_array"]
-      }
-    }
-    CONFIG
-
-    sample("noisy_field" => %w(cat dog cat cat)) do
-      insist { subject["noisy_field"] } == %w(cat dog)
-    end
-
-    sample("not_an_array" => "Hello, world!") do
-      insist { subject["not_an_array"] } == "Hello, world!"
-    end
-
-  end
-end
diff --git a/spec/inputs/relp.rb b/spec/inputs/relp.rb
deleted file mode 100644
index 7e510646868..00000000000
--- a/spec/inputs/relp.rb
+++ /dev/null
@@ -1,70 +0,0 @@
-# coding: utf-8
-require "test_utils"
-require "socket"
-require "logstash/util/relp"
-
-describe "inputs/relp" do
-  extend LogStash::RSpec
-
-  describe "Single client connection" do
-    event_count = 10
-    port = 5511
-    config <<-CONFIG
-    input {
-      relp {
-        type => "blah"
-        port => #{port}
-      }
-    }
-    CONFIG
-
-    input do |pipeline, queue|
-      th = Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      #Send events from clients
-      client = RelpClient.new("0.0.0.0", port, ["syslog"])
-      event_count.times do |value|
-        client.syslog_write("Hello #{value}")
-      end
-
-      events = event_count.times.collect { queue.pop }
-      event_count.times do |i|
-        insist { events[i]["message"] } == "Hello #{i}"
-      end
-
-      pipeline.shutdown
-      th.join
-    end # input
-  end
-  describe "Two client connection" do
-    event_count = 100
-    port = 5512
-    config <<-CONFIG
-    input {
-      relp {
-        type => "blah"
-        port => #{port}
-      }
-    }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      #Send events from clients sockets
-      client = RelpClient.new("0.0.0.0", port, ["syslog"])
-      client2 = RelpClient.new("0.0.0.0", port, ["syslog"])
-
-      event_count.times do |value|
-        client.syslog_write("Hello from client")
-        client2.syslog_write("Hello from client 2")
-      end
-
-      events = (event_count*2).times.collect { queue.pop }
-      insist { events.select{|event| event["message"]=="Hello from client" }.size } == event_count
-      insist { events.select{|event| event["message"]=="Hello from client 2" }.size } == event_count
-    end # input
-  end
-end
diff --git a/spec/sqlite-test.rb b/spec/sqlite-test.rb
deleted file mode 100644
index 4f27dad2db2..00000000000
--- a/spec/sqlite-test.rb
+++ /dev/null
@@ -1,81 +0,0 @@
-require "rubygems"
-require "sequel"
-require "jdbc/sqlite3" 
-
-    public
-    def init_placeholder_table(db)
-      begin
-        db.create_table :since_table do 
-          String :table
-          Int    :place
-        end
-      rescue
-        p 'since tables already exists'
-      end
-    end
-
-    public 
-    def init_placeholder(db, table)
-      p "init placeholder for #{table}"
-      since = db[:since_table]
-      since.insert(:table => table, :place => 1)
-    end
-
-    public
-    def get_placeholder(db, table)
-      since = db[:since_table]
-      x = since.where(:table => "#{table}")
-      p x
-      if x[:place].nil?
-        p 'place is 0'
-        init_placeholder(db, table) 
-        return 0
-      else
-        p "placeholder already exists, it is #{x[:place]}"
-        return x[:place][:place]
-      end
-    end
-
-    public
-    def update_placeholder(db, table, place)
-      since = db[:since_table]
-      since.where(:table => table).update(:place => place)
-    end
-      
-    public 
-    def get_all_tables(db)
-      tables = db["SELECT * FROM sqlite_master WHERE type = 'table'"].map {|table| table[:name]}
-      tables.delete_if { |table| table == 'since_table' }
-      return tables
-    end
-    
-    public
-    def get_n_rows_from_table(db, table, offset, limit)
-      p "Selecting from #{table} where id is at leasat #{offset}"
-      dataset = db["SELECT * FROM #{table}"]
-      return db["SELECT * FROM #{table} WHERE (id >= #{offset}) ORDER BY 'id' LIMIT #{limit}"].map { |row| row }
-    end
-      
-    @DB = Sequel.connect("jdbc:sqlite:/home/ec2-user/u2/log/log.db") 
-
-    tables = get_all_tables(@DB)
-
-    #init table stuff
-    table_data = Hash.new
-    tables.each{ |table|
-      init_placeholder_table(@DB)
-      last_place = get_placeholder(@DB, table)
-      table_data[table] = { :name => table, :place => last_place }
-      #puts table
-    }
-
-    #looped tabled stuff
-    table_data.each{ |k, table|
-      puts table
-      offset = table[:place]
-      limit = 5
-      table_name = table[:name]
-      puts get_n_rows_from_table(@DB, table_name, offset, limit)
-      update_placeholder(@DB, table_name, offset+limit)
-    }
-
