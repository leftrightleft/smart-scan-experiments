diff --git a/Gemfile.jruby-2.3.lock.release b/Gemfile.jruby-2.3.lock.release
new file mode 100644
index 00000000000..71be526b209
--- /dev/null
+++ b/Gemfile.jruby-2.3.lock.release
@@ -0,0 +1,692 @@
+PATH
+  remote: ./logstash-core
+  specs:
+    logstash-core (6.0.0.rc1-java)
+      chronic_duration (= 0.10.6)
+      clamp (~> 0.6.5)
+      concurrent-ruby (~> 1.0, >= 1.0.5)
+      elasticsearch (~> 5.0, >= 5.0.4)
+      filesize (= 0.0.4)
+      gems (~> 0.8.3)
+      i18n (= 0.6.9)
+      jar-dependencies
+      jrjackson (~> 0.4.2)
+      jruby-openssl (>= 0.9.20)
+      manticore (>= 0.5.4, < 1.0.0)
+      minitar (~> 0.5.4)
+      pry (~> 0.10.1)
+      puma (~> 2.16)
+      rack (= 1.6.6)
+      ruby-maven (~> 3.3.9)
+      rubyzip (~> 1.2.1)
+      sinatra (~> 1.4, >= 1.4.6)
+      stud (~> 0.0.19)
+      thread_safe (~> 0.3.5)
+      treetop (< 1.5.0)
+
+PATH
+  remote: ./logstash-core-plugin-api
+  specs:
+    logstash-core-plugin-api (2.1.16-java)
+      logstash-core (= 6.0.0.rc1)
+
+GEM
+  remote: https://rubygems.org/
+  specs:
+    addressable (2.3.8)
+    arr-pm (0.0.10)
+      cabin (> 0)
+    atomic (1.1.99-java)
+    avl_tree (1.2.1)
+      atomic (~> 1.1)
+    awesome_print (1.8.0)
+    aws-sdk (2.3.22)
+      aws-sdk-resources (= 2.3.22)
+    aws-sdk-core (2.3.22)
+      jmespath (~> 1.0)
+    aws-sdk-resources (2.3.22)
+      aws-sdk-core (= 2.3.22)
+    aws-sdk-v1 (1.67.0)
+      json (~> 1.4)
+      nokogiri (~> 1)
+    backports (3.8.0)
+    benchmark-ips (2.7.2)
+    bindata (2.4.1)
+    buftok (0.2.0)
+    builder (3.2.3)
+    cabin (0.9.0)
+    childprocess (0.7.1)
+      ffi (~> 1.0, >= 1.0.11)
+    chronic_duration (0.10.6)
+      numerizer (~> 0.1.1)
+    ci_reporter (2.0.0)
+      builder (>= 2.1.2)
+    ci_reporter_rspec (1.0.0)
+      ci_reporter (~> 2.0)
+      rspec (>= 2.14, < 4)
+    clamp (0.6.5)
+    coderay (1.1.2)
+    concurrent-ruby (1.0.5-java)
+    diff-lcs (1.3)
+    docile (1.1.5)
+    docker-api (1.33.4)
+      excon (>= 0.38.0)
+      json
+    domain_name (0.5.20170404)
+      unf (>= 0.0.5, < 1.0.0)
+    dotenv (2.2.1)
+    edn (1.1.1)
+    elasticsearch (5.0.4)
+      elasticsearch-api (= 5.0.4)
+      elasticsearch-transport (= 5.0.4)
+    elasticsearch-api (5.0.4)
+      multi_json
+    elasticsearch-transport (5.0.4)
+      faraday
+      multi_json
+    equalizer (0.0.10)
+    excon (0.59.0)
+    faraday (0.9.2)
+      multipart-post (>= 1.2, < 3)
+    ffi (1.9.18-java)
+    file-dependencies (0.1.6)
+      minitar
+    filesize (0.0.4)
+    filewatch (0.9.0)
+    fivemat (1.3.5)
+    flores (0.0.7)
+    fpm (1.3.3)
+      arr-pm (~> 0.0.9)
+      backports (>= 2.6.2)
+      cabin (>= 0.6.0)
+      childprocess
+      clamp (~> 0.6)
+      ffi
+      json (>= 1.7.7)
+    gelfd (0.2.0)
+    gem_publisher (1.5.0)
+    gems (0.8.3)
+    hitimes (1.2.6-java)
+    http (0.9.9)
+      addressable (~> 2.3)
+      http-cookie (~> 1.0)
+      http-form_data (~> 1.0.1)
+      http_parser.rb (~> 0.6.0)
+    http-cookie (1.0.3)
+      domain_name (~> 0.5)
+    http-form_data (1.0.1)
+    http_parser.rb (0.6.0-java)
+    i18n (0.6.9)
+    insist (1.0.0)
+    jar-dependencies (0.3.11)
+    jls-grok (0.11.4)
+      cabin (>= 0.6.0)
+    jmespath (1.3.1)
+    jrjackson (0.4.2-java)
+    jruby-openssl (0.9.21-java)
+    jruby-stdin-channel (0.2.0-java)
+    json (1.8.6-java)
+    json-schema (2.6.2)
+      addressable (~> 2.3.8)
+    kramdown (1.14.0)
+    logstash-codec-cef (5.0.1-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-collectd (3.0.7)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-dots (3.0.5)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-edn (3.0.5)
+      edn
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-edn_lines (3.0.5)
+      edn
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-es_bulk (3.0.5)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-fluent (3.1.3-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      msgpack
+    logstash-codec-graphite (3.0.4)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-json (3.0.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-json_lines (3.0.4)
+      logstash-codec-line (>= 2.1.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-line (3.0.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-msgpack (3.0.5-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      msgpack-jruby
+    logstash-codec-multiline (3.0.7)
+      jls-grok (~> 0.11.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-patterns-core
+    logstash-codec-netflow (3.5.2)
+      bindata (>= 1.5.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-plain (3.0.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-rubydebug (3.0.4)
+      awesome_print
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-devutils (1.3.4-java)
+      fivemat
+      gem_publisher
+      insist (= 1.0.0)
+      kramdown (= 1.14.0)
+      logstash-core-plugin-api (>= 2.0, <= 2.99)
+      minitar
+      rake
+      rspec (~> 3.0)
+      rspec-wait
+      stud (>= 0.0.20)
+    logstash-filter-cidr (3.1.1-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-clone (3.0.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-csv (3.0.5)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-date (3.1.8)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-dissect (1.0.12)
+      jar-dependencies
+      logstash-core-plugin-api (>= 2.1.1, <= 2.99)
+    logstash-filter-dns (3.0.6)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      lru_redux (~> 1.1.0)
+    logstash-filter-drop (3.0.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-fingerprint (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      murmurhash3
+    logstash-filter-geoip (5.0.1-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-grok (3.4.3)
+      jls-grok (~> 0.11.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-patterns-core
+      stud (~> 0.0.22)
+    logstash-filter-json (3.0.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-kv (4.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-metrics (4.0.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      metriks
+      thread_safe
+    logstash-filter-mutate (3.1.6)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-ruby (3.0.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-filter-date
+    logstash-filter-sleep (3.0.5)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-split (3.1.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-syslog_pri (3.0.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-throttle (4.0.3)
+      atomic
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      thread_safe
+    logstash-filter-translate (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-urldecode (3.0.5)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-useragent (3.2.1-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-xml (4.0.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      nokogiri
+      xml-simple
+    logstash-input-beats (5.0.1-java)
+      concurrent-ruby (~> 1.0)
+      jar-dependencies (~> 0.3.4)
+      logstash-codec-multiline (>= 2.0.5)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      thread_safe (~> 0.3.5)
+    logstash-input-dead_letter_queue (1.1.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-elasticsearch (4.0.5)
+      elasticsearch (>= 5.0.3, < 6.0.0)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-exec (3.1.4)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-file (4.0.3)
+      addressable
+      filewatch (~> 0.8, >= 0.8.1)
+      logstash-codec-multiline (~> 3.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-ganglia (3.1.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-gelf (3.0.6)
+      gelfd (= 0.2.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-generator (3.0.4)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-graphite (3.0.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-input-tcp
+    logstash-input-heartbeat (3.0.4)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud
+    logstash-input-http (3.0.6)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      puma (~> 2.16, >= 2.16.0)
+      rack (~> 1)
+      stud
+    logstash-input-http_poller (4.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-http_client (>= 6.0.0, < 7.0.0)
+      rufus-scheduler (~> 3.0.9)
+      stud (~> 0.0.22)
+    logstash-input-imap (3.0.4)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      mail (~> 2.6.3)
+      mime-types (= 2.6.2)
+      stud (~> 0.0.22)
+    logstash-input-jdbc (4.2.4)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      rufus-scheduler
+      sequel
+      tzinfo
+      tzinfo-data
+    logstash-input-kafka (8.0.2)
+      logstash-codec-json
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (>= 0.0.22, < 0.1.0)
+    logstash-input-pipe (3.0.5)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-rabbitmq (6.0.1)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-rabbitmq_connection (>= 5.0.0, < 6.0.0)
+    logstash-input-redis (3.1.5)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      redis (~> 3)
+    logstash-input-s3 (3.1.6)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws
+      stud (~> 0.0.18)
+    logstash-input-snmptrap (3.0.4)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      snmp
+    logstash-input-sqs (3.0.5)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+    logstash-input-stdin (3.2.4)
+      concurrent-ruby
+      jruby-stdin-channel
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-syslog (3.2.2)
+      concurrent-ruby
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-filter-date
+      logstash-filter-grok
+      stud (>= 0.0.22, < 0.1.0)
+      thread_safe
+    logstash-input-tcp (5.0.2-java)
+      logstash-codec-json
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-twitter (3.0.6)
+      http-form_data (<= 1.0.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      public_suffix (<= 1.4.6)
+      stud (>= 0.0.22, < 0.1)
+      twitter (= 5.15.0)
+    logstash-input-udp (3.1.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-unix (3.0.5)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-mixin-aws (4.2.3)
+      aws-sdk (~> 2.3.0)
+      aws-sdk-v1 (>= 1.61.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-mixin-http_client (6.0.1)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      manticore (>= 0.5.2, < 1.0.0)
+    logstash-mixin-rabbitmq_connection (5.0.0-java)
+      march_hare (~> 3.0.0)
+      stud (~> 0.0.22)
+    logstash-output-cloudwatch (3.0.6)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+      rufus-scheduler (~> 3.0.9)
+    logstash-output-csv (3.0.5)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-filter-json
+      logstash-input-generator
+      logstash-output-file
+    logstash-output-elasticsearch (8.1.1-java)
+      cabin (~> 0.6)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      manticore (>= 0.5.4, < 1.0.0)
+      stud (~> 0.0, >= 0.0.17)
+    logstash-output-file (4.1.1)
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-core-plugin-api (>= 2.0.0, < 2.99)
+    logstash-output-graphite (3.1.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-http (5.1.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-http_client (>= 6.0.0, < 7.0.0)
+    logstash-output-kafka (7.0.1)
+      logstash-codec-json
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-nagios (3.0.4)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-null (3.0.4)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-pagerduty (3.0.5)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-pipe (3.0.4)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-rabbitmq (5.0.1-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-rabbitmq_connection (>= 5.0.0, < 6.0.0)
+    logstash-output-redis (4.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      redis (~> 3)
+      stud
+    logstash-output-s3 (4.0.10)
+      concurrent-ruby
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws
+      stud (~> 0.0.22)
+    logstash-output-sns (4.0.5)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+    logstash-output-sqs (5.0.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+    logstash-output-stdout (3.1.2)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60.1, < 2.99)
+    logstash-output-tcp (5.0.1)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud
+    logstash-output-udp (3.0.4)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-webhdfs (3.0.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      snappy (= 0.0.12)
+      webhdfs
+    logstash-patterns-core (4.1.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    lru_redux (1.1.0)
+    mail (2.6.6)
+      mime-types (>= 1.16, < 4)
+    manticore (0.6.1-java)
+    march_hare (3.0.0-java)
+    memoizable (0.4.2)
+      thread_safe (~> 0.3, >= 0.3.1)
+    method_source (0.8.2)
+    metriks (0.9.9.8)
+      atomic (~> 1.0)
+      avl_tree (~> 1.2.0)
+      hitimes (~> 1.1)
+    mime-types (2.6.2)
+    minitar (0.5.4)
+    msgpack (1.1.0-java)
+    msgpack-jruby (1.4.1-java)
+    multi_json (1.12.2)
+    multipart-post (2.0.0)
+    murmurhash3 (0.1.6-java)
+    mustache (0.99.8)
+    naught (1.1.0)
+    nokogiri (1.8.0-java)
+    numerizer (0.1.1)
+    octokit (3.8.0)
+      sawyer (~> 0.6.0, >= 0.5.3)
+    paquet (0.2.1)
+    pleaserun (0.0.30)
+      cabin (> 0)
+      clamp
+      dotenv
+      insist
+      mustache (= 0.99.8)
+      stud
+    polyglot (0.3.5)
+    pry (0.10.4-java)
+      coderay (~> 1.1.0)
+      method_source (~> 0.8.1)
+      slop (~> 3.4)
+      spoon (~> 0.0)
+    public_suffix (1.4.6)
+    puma (2.16.0-java)
+    rack (1.6.6)
+    rack-protection (1.5.3)
+      rack
+    rack-test (0.7.0)
+      rack (>= 1.0, < 3)
+    rake (12.1.0)
+    redis (3.3.3)
+    rspec (3.6.0)
+      rspec-core (~> 3.6.0)
+      rspec-expectations (~> 3.6.0)
+      rspec-mocks (~> 3.6.0)
+    rspec-core (3.6.0)
+      rspec-support (~> 3.6.0)
+    rspec-expectations (3.6.0)
+      diff-lcs (>= 1.2.0, < 2.0)
+      rspec-support (~> 3.6.0)
+    rspec-mocks (3.6.0)
+      diff-lcs (>= 1.2.0, < 2.0)
+      rspec-support (~> 3.6.0)
+    rspec-support (3.6.0)
+    rspec-wait (0.0.9)
+      rspec (>= 3, < 4)
+    ruby-maven (3.3.12)
+      ruby-maven-libs (~> 3.3.9)
+    ruby-maven-libs (3.3.9)
+    ruby-progressbar (1.8.1)
+    rubyzip (1.2.1)
+    rufus-scheduler (3.0.9)
+      tzinfo
+    sawyer (0.6.0)
+      addressable (~> 2.3.5)
+      faraday (~> 0.8, < 0.10)
+    sequel (5.0.0)
+    simple_oauth (0.3.1)
+    simplecov (0.15.1)
+      docile (~> 1.1.0)
+      json (>= 1.8, < 3)
+      simplecov-html (~> 0.10.0)
+    simplecov-html (0.10.2)
+    sinatra (1.4.8)
+      rack (~> 1.5)
+      rack-protection (~> 1.4)
+      tilt (>= 1.3, < 3)
+    slop (3.6.0)
+    snappy (0.0.12-java)
+      snappy-jars (~> 1.1.0)
+    snappy-jars (1.1.0.1.2-java)
+    snmp (1.2.0)
+    spoon (0.0.6)
+      ffi
+    stud (0.0.23)
+    term-ansicolor (1.3.2)
+      tins (~> 1.0)
+    thread_safe (0.3.6-java)
+    tilt (2.0.8)
+    tins (1.6.0)
+    treetop (1.4.15)
+      polyglot
+      polyglot (>= 0.3.1)
+    twitter (5.15.0)
+      addressable (~> 2.3)
+      buftok (~> 0.2.0)
+      equalizer (= 0.0.10)
+      faraday (~> 0.9.0)
+      http (>= 0.4, < 0.10)
+      http_parser.rb (~> 0.6.0)
+      json (~> 1.8)
+      memoizable (~> 0.4.0)
+      naught (~> 1.0)
+      simple_oauth (~> 0.3.0)
+    tzinfo (1.2.3)
+      thread_safe (~> 0.1)
+    tzinfo-data (1.2017.2)
+      tzinfo (>= 1.0.0)
+    unf (0.1.4-java)
+    webhdfs (0.8.0)
+      addressable
+    xml-simple (1.1.5)
+
+PLATFORMS
+  java
+
+DEPENDENCIES
+  benchmark-ips
+  builder (~> 3.2.2)
+  ci_reporter_rspec (= 1.0.0)
+  docker-api (= 1.33.4)
+  file-dependencies (= 0.1.6)
+  flores (~> 0.0.6)
+  fpm (~> 1.3.3)
+  gems (~> 0.8.3)
+  json-schema (~> 2.6)
+  logstash-codec-cef
+  logstash-codec-collectd
+  logstash-codec-dots
+  logstash-codec-edn
+  logstash-codec-edn_lines
+  logstash-codec-es_bulk
+  logstash-codec-fluent
+  logstash-codec-graphite
+  logstash-codec-json
+  logstash-codec-json_lines
+  logstash-codec-line
+  logstash-codec-msgpack
+  logstash-codec-multiline
+  logstash-codec-netflow
+  logstash-codec-plain
+  logstash-codec-rubydebug
+  logstash-core!
+  logstash-core-plugin-api!
+  logstash-devutils
+  logstash-filter-cidr
+  logstash-filter-clone
+  logstash-filter-csv
+  logstash-filter-date
+  logstash-filter-dissect
+  logstash-filter-dns
+  logstash-filter-drop
+  logstash-filter-fingerprint
+  logstash-filter-geoip
+  logstash-filter-grok
+  logstash-filter-json
+  logstash-filter-kv
+  logstash-filter-metrics
+  logstash-filter-mutate
+  logstash-filter-ruby
+  logstash-filter-sleep
+  logstash-filter-split
+  logstash-filter-syslog_pri
+  logstash-filter-throttle
+  logstash-filter-translate
+  logstash-filter-urldecode
+  logstash-filter-useragent
+  logstash-filter-xml
+  logstash-input-beats
+  logstash-input-dead_letter_queue
+  logstash-input-elasticsearch
+  logstash-input-exec
+  logstash-input-file
+  logstash-input-ganglia
+  logstash-input-gelf
+  logstash-input-generator
+  logstash-input-graphite
+  logstash-input-heartbeat
+  logstash-input-http
+  logstash-input-http_poller
+  logstash-input-imap
+  logstash-input-jdbc
+  logstash-input-kafka
+  logstash-input-pipe
+  logstash-input-rabbitmq
+  logstash-input-redis
+  logstash-input-s3
+  logstash-input-snmptrap
+  logstash-input-sqs
+  logstash-input-stdin
+  logstash-input-syslog
+  logstash-input-tcp
+  logstash-input-twitter
+  logstash-input-udp
+  logstash-input-unix
+  logstash-output-cloudwatch
+  logstash-output-csv
+  logstash-output-elasticsearch
+  logstash-output-file
+  logstash-output-graphite
+  logstash-output-http
+  logstash-output-kafka
+  logstash-output-nagios
+  logstash-output-null
+  logstash-output-pagerduty
+  logstash-output-pipe
+  logstash-output-rabbitmq
+  logstash-output-redis
+  logstash-output-s3 (>= 4.0.9, < 5.0.0)
+  logstash-output-sns
+  logstash-output-sqs
+  logstash-output-stdout
+  logstash-output-tcp
+  logstash-output-udp
+  logstash-output-webhdfs
+  octokit (= 3.8.0)
+  paquet (~> 0.2.0)
+  pleaserun (~> 0.0.28)
+  rack-test
+  rspec (~> 3.5)
+  ruby-progressbar (~> 1.8.1)
+  rubyzip (~> 1.2.1)
+  simplecov
+  stud (~> 0.0.22)
+  term-ansicolor (~> 1.3.2)
+  tins (= 1.6)
diff --git a/README.md b/README.md
index ce1a24761e8..de36be2d4cb 100644
--- a/README.md
+++ b/README.md
@@ -22,10 +22,10 @@ For the daring, snapshot builds from `master` branch are available. These builds
 
 | artifact |
 | --- |
-| [tar](https://snapshots.elastic.co/downloads/logstash/logstash-6.0.0-beta1-SNAPSHOT.tar.gz) |
-| [zip](https://snapshots.elastic.co/downloads/logstash/logstash-6.0.0-beta1-SNAPSHOT.zip) |
-| [deb](https://snapshots.elastic.co/downloads/logstash/logstash-6.0.0-beta1-SNAPSHOT.deb) |
-| [rpm](https://snapshots.elastic.co/downloads/logstash/logstash-6.0.0-beta1-SNAPSHOT.rpm) |
+| [tar](https://snapshots.elastic.co/downloads/logstash/logstash-6.0.0-rc1-SNAPSHOT.tar.gz) |
+| [zip](https://snapshots.elastic.co/downloads/logstash/logstash-6.0.0-rc1-SNAPSHOT.zip) |
+| [deb](https://snapshots.elastic.co/downloads/logstash/logstash-6.0.0-rc1-SNAPSHOT.deb) |
+| [rpm](https://snapshots.elastic.co/downloads/logstash/logstash-6.0.0-rc1-SNAPSHOT.rpm) |
 
 ## Need Help?
 
diff --git a/config/logstash.yml b/config/logstash.yml
index 74b56f53036..f371a60392b 100644
--- a/config/logstash.yml
+++ b/config/logstash.yml
@@ -109,6 +109,19 @@
 #
 # modules:
 #
+# ------------ Cloud Settings ---------------
+# Define Elastic Cloud settings here.
+# Format of cloud.id is a base64 value e.g. dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyRub3RhcmVhbCRpZGVudGlmaWVy
+# and it may have an label prefix e.g. staging:dXMtZ...
+# This will overwrite 'var.elasticsearch.hosts' and 'var.kibana.host'
+# cloud.id: <identifier>
+#
+# Format of cloud.auth is: <user>:<pass>
+# This is optional
+# If supplied this will overwrite 'var.elasticsearch.username' and 'var.elasticsearch.password'
+# If supplied this will overwrite 'var.kibana.username' and 'var.kibana.password'
+# cloud.auth: elastic:<password>
+#
 # ------------ Queuing Settings --------------
 #
 # Internal queuing model, "memory" for legacy in-memory based queuing and
diff --git a/config/pipelines.yml b/config/pipelines.yml
index 8b186eda441..1940a01b638 100644
--- a/config/pipelines.yml
+++ b/config/pipelines.yml
@@ -67,3 +67,13 @@
 #
 #   # Enable Dead Letter Queueing for this pipeline.
 #   dead_letter_queue.enable: false
+#
+#   If using dead_letter_queue.enable: true, the maximum size of dead letter queue for this pipeline. Entries
+#   will be dropped if they would increase the size of the dead letter queue beyond this setting.
+#   Default is 1024mb
+#   dead_letter_queue.max_bytes: 1024mb
+#
+#   If using dead_letter_queue.enable: true, the directory path where the data files will be stored.
+#   Default is path.data/dead_letter_queue
+#
+#   path.dead_letter_queue:
\ No newline at end of file
diff --git a/docs/index-shared1.asciidoc b/docs/index-shared1.asciidoc
index bff97caf16a..e9e3fd3538d 100644
--- a/docs/index-shared1.asciidoc
+++ b/docs/index-shared1.asciidoc
@@ -1,9 +1,9 @@
 
-:branch:                master
+:branch:                6.0
 :major-version:         6.x
-:logstash_version:      6.0.0-beta1
-:elasticsearch_version: 6.0.0-beta1
-:kibana_version:        6.0.0-beta1
+:logstash_version:      6.0.0-beta2
+:elasticsearch_version: 6.0.0-beta2
+:kibana_version:        6.0.0-beta2
 :docker-image:          docker.elastic.co/logstash/logstash:{logstash_version}
 
 //////////
diff --git a/docs/index-shared2.asciidoc b/docs/index-shared2.asciidoc
index ffac5e4126e..78a410c217a 100644
--- a/docs/index-shared2.asciidoc
+++ b/docs/index-shared2.asciidoc
@@ -26,6 +26,22 @@ include::static/managing-multiline-events.asciidoc[]
 :edit_url: https://github.com/elastic/logstash/edit/master/docs/static/glob-support.asciidoc
 include::static/glob-support.asciidoc[]
 
+// Centralized configuration managements
+include::static/config-management.asciidoc[]
+
+// Working with Logstash Modules
+
+include::static/modules.asciidoc[]
+
+include::static/arcsight-module.asciidoc[]
+
+include::static/netflow-module.asciidoc[]
+
+// Working with Filebeat Modules
+
+:edit_url: https://github.com/elastic/logstash/edit/master/docs/static/filebeat-modules.asciidoc
+include::static/filebeat-modules.asciidoc[]
+
 // Data resiliency
 
 :edit_url: https://github.com/elastic/logstash/edit/master/docs/static/resiliency.asciidoc
@@ -37,11 +53,6 @@ include::static/persistent-queues.asciidoc[]
 :edit_url: https://github.com/elastic/logstash/edit/master/docs/static/dead-letter-queues.asciidoc
 include::static/dead-letter-queues.asciidoc[]
 
-// Working with Filebeat Modules
-
-:edit_url: https://github.com/elastic/logstash/edit/master/docs/static/filebeat-modules.asciidoc
-include::static/filebeat-modules.asciidoc[]
-
 // Transforming Data
 
 :edit_url: https://github.com/elastic/logstash/edit/master/docs/static/transforming-data.asciidoc
@@ -62,6 +73,11 @@ include::static/performance-checklist.asciidoc[]
 :edit_url: https://github.com/elastic/logstash/edit/master/docs/static/monitoring.asciidoc
 include::static/monitoring.asciidoc[]
 
+// Pipeline viewer
+
+:edit_url: https://github.com/elastic/logstash/edit/master/docs/static/pipeline-viewer.asciidoc
+include::static/pipeline-viewer.asciidoc[]
+
 // Monitoring APIs
 
 :edit_url: https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc
diff --git a/docs/static/arcsight-module.asciidoc b/docs/static/arcsight-module.asciidoc
new file mode 100644
index 00000000000..e4cd93cbc27
--- /dev/null
+++ b/docs/static/arcsight-module.asciidoc
@@ -0,0 +1,306 @@
+[role="xpack"]
+[[arcsight-module]]
+=== Logstash ArcSight Module
+
+++++
+<titleabbrev>ArcSight Module</titleabbrev>
+++++
+
+NOTE: The Logstash ArcSight module is an
+https://www.elastic.co/products/x-pack[X-Pack] feature under the Basic License
+and is therefore free to use. Please contact
+mailto:arcsight@elastic.co[arcsight@elastic.co] for questions or more
+information.
+
+The Logstash ArcSight module enables you to easily integrate your ArcSight data with the Elastic Stack. 
+With a single command, the module taps directly into the ArcSight Smart Connector or the Event Broker, 
+parses and indexes the security events into Elasticsearch, and installs a suite of Kibana dashboards 
+to get you exploring your data immediately. 
+
+[[arcsight-architecture]]
+==== Deployment Architecture
+
+The Logstash ArcSight module understands CEF (Common Event Format), and can
+accept, enrich, and index these events for analysis on the Elastic Stack. ADP
+contains two core data collection components for data streaming:
+
+* The _Smart Connectors (SC)_ are edge log collectors that parse and normalize
+data to CEF prior to publishing to the Logstash receiver.
+* The _Event Broker (EB)_ is the central hub for incoming data and is based on
+open source Apache Kafka. The Logstash ArcSight module can consume directly from
+EB topics.
+
+[[arcsight-getting-started-smartconnector]]
+==== Getting Started With The Smart Connector
+
+To get started, you can use a basic Elastic Stack setup that reads events from
+the Smart Connector directly.
+
+[role="screenshot"]
+image::static/images/arcsight-architecture-diagram-smartconnector-2017.png[ArcSight Smart Connector architecture]
+
+[[arcsight-requirements-smartconnector]]
+===== Requirements
+
+* These instructions assume you have part of the Elastic Stack (Logstash, Elasticsearch,
+Kibana) already installed. The products you need are
+https://www.elastic.co/downloads[available to download] and easy to install. The
+Elastic Stack 5.6 or higher is required for this module.
+* The Elastic Stack is running locally with default ports exposed, namely
+Elasticsearch as "localhost:9200" and Kibana as "localhost:5601". Note that you can also run 
+Elasticsearch, Kibana and Logstash on separate hosts to consume data from ArcSight.
+* Smart Connector has been configured to publish ArcSight data (to TCP port `5000`) using the CEF syslog 
+destination.
+
+[[arcsight-instructions-smartconnector]]
+===== Instructions
+
+. {ref}/installing-xpack-es.html[Install X-Pack on Elasticsearch] and then start
+Elasticsearch.
+
+. {kibana-ref}/installing-xpack-kb.html[Install X-Pack on Kibana] and then start
+Kibana.
+
+. {logstash-ref}/installing-xpack-log.html[Install X-Pack on Logstash], which
+includes the Logstash ArcSight module.
+. Start the Logstash ArcSight module by running the following command in the
+Logstash install directory with your respective EB host and port:
++
+[source,shell]
+-----
+bin/logstash --modules arcsight --setup
+  -M "arcsight.var.inputs=smartconnector" 
+  -M "arcsight.var.elasticsearch.username=elastic" 
+  -M "arcsight.var.elasticsearch.password=changeme" 
+  -M "arcsight.var.kibana.username=elastic" 
+  -M "arcsight.var.kibana.password=changeme"
+-----
++
+--
+TIP: The command in this example is formatted for readability. Remove the line
+breaks before running this command.
+
+The `--modules arcsight` option spins up an ArcSight CEF-aware Logstash
+pipeline for ingestion. The `--setup` option creates an `arcsight-*` index
+pattern in Elasticsearch and imports Kibana dashboards and visualizations. On
+subsequent module runs or when scaling out the Logstash deployment,
+the `--setup` option should be omitted to avoid overwriting the existing Kibana
+dashboards.
+--
+
+. Explore your data with Kibana:
+.. Open browser @ http://localhost:5601[http://localhost:5601] (username:
+  "elastic"; password: "changeme")
+.. Open the *[ArcSight] Network Overview Dashboard*
+.. See <<exploring-data-arcsight>> for additional details on data exploration.
+
+See <<configuring-arcsight>> if you want to specify additional options that
+control the behavior of the ArcSight module.
+
+[[arcsight-getting-started-eventbroker]]
+==== Getting Started With The Event Broker
+
+To get started, you can use a basic Elastic Stack setup that reads events from
+the EB event stream.
+
+[role="screenshot"]
+image::static/images/arcsight-architecture-diagram-eventbroker-2017.png[ArcSight Event Broker architecture]
+
+[[arcsight-requirements-eventbroker]]
+===== Requirements
+
+* These instructions assume you have the Elastic Stack (Logstash, Elasticsearch,
+Kibana) already installed. The products you need are
+https://www.elastic.co/downloads[available to download] and easy to install. The
+Elastic Stack 5.6 or higher is required for this module.
+* The Elastic Stack is running locally with default ports exposed, namely
+Elasticsearch as "localhost:9200" and Kibana as "localhost:5601".
+* By default, the Logstash ArcSight module consumes from the EB "eb-cef" topic.
+For additional EB settings, see <<arcsight-module-config>>. Consuming from a
+secured EB port is not currently available.
+
+[[arcsight-instructions-eventbroker]]
+===== Instructions
+
+. {ref}/installing-xpack-es.html[Install X-Pack on Elasticsearch] and then start
+Elasticsearch.
+
+. {kibana-ref}/installing-xpack-kb.html[Install X-Pack on Kibana] and then start
+Kibana.
+
+. {logstash-ref}/installing-xpack-log.html[Install X-Pack on Logstash], which
+includes the Logstash ArcSight module. Then update the Logstash
+<<plugins-inputs-kafka,Kafka input plugin>> to an EB compatible version. In the
+Logstash install directory, run:
++
+[source,shell]
+-----
+bin/logstash-plugin install x-pack
+bin/logstash-plugin install --version 6.2.7 logstash-input-kafka
+-----
+
+. Start the Logstash ArcSight module by running the following command in the
+Logstash install directory with your respective EB host and port:
++
+[source,shell]
+-----
+bin/logstash --modules arcsight --setup
+  -M "arcsight.var.input.eventbroker.bootstrap_servers={eb_host}:{eb_port}" 
+  -M "arcsight.var.elasticsearch.username=elastic" 
+  -M "arcsight.var.elasticsearch.password=changeme" 
+  -M "arcsight.var.kibana.username=elastic" 
+  -M "arcsight.var.kibana.password=changeme"
+-----
++
+--
+TIP: The command in this example is formatted for readability. Remove the line
+breaks before running this command.
+
+The `--modules arcsight` option spins up an ArcSight CEF-aware Logstash
+pipeline for ingestion. The `--setup` option creates an `arcsight-*` index
+pattern in Elasticsearch and imports Kibana dashboards and visualizations. On
+subsequent module runs or when scaling out the Logstash deployment,
+the `--setup` option should be omitted to avoid overwriting the existing Kibana
+dashboards.
+--
+
+. Explore your data with Kibana:
+.. Open browser @ http://localhost:5601[http://localhost:5601] (username:
+  "elastic"; password: "changeme")
+.. Open the *[ArcSight] Network Overview Dashboard*
+.. See <<exploring-data-arcsight>> for additional details on data exploration.
+
+See <<configuring-arcsight>> if you want to specify additional options that
+control the behavior of the ArcSight module.
+
+[[exploring-data-arcsight]]
+==== Exploring Your Security Data
+Once the Logstash ArcSight module starts receiving events, you can immediately
+begin using the packaged Kibana dashboards to explore and visualize your
+security data. The dashboards rapidly accelerate the time and effort required
+for security analysts and operators to gain situational and behavioral insights
+on network, endpoint, and DNS events flowing through the environment. You can
+use the dashboards as-is, or tailor them to work better with existing use cases
+and business requirements.
+
+The dashboards have a navigation pane for context switching and drill downs
+across three core use cases:
+
+* *Network Data*
+** Dashboards: Network Overview, Network Suspicious Activity
+** Data Types: Network firewalls, intrusion systems, VPN devices
+
+* *Endpoint Data*
+** Dashboards: Endpoint Overview, Endpoint OS Activity
+** Data Types: Operating systems, applications, host intrusion systems
+
+* *DNS Data*
+** Dashboards: Microsoft DNS Overview
+** Data Types: Microsoft DNS devices
+
+[[network-dashboards-arsight]]
+===== Example Network Dashboards
+
+[role="screenshot"]
+image::static/images/arcsight-network-overview.png[Network overview dashboard]
+
+[role="screenshot"]
+image::static/images/arcsight-network-suspicious.png[Network suspicious activity dashboard]
+
+These Kibana visualizations enable you to quickly understand the top devices,
+endpoints, attackers, and targets. This insight, along with the ability to
+instantly drill down on a particular host, port, device, or time range, offers a
+holistic view across the entire environment to identify specific segments that
+may require immediate attention or action. You can easily discover answers to
+questions like:
+
+* Who are my attackers and what are they targeting?
+* Which of my devices or endpoints are the busiest and what services were
+rendered?
+* How many unique attackers, techniques, signatures, or targets were triggered
+at any given point in time?
+* What are the top sources, destinations, protocols, and behaviors that are
+causing the elevated count of failures?
+
+[[configuring-arcsight]]
+==== Configuring the Module
+
+You can specify additional options for the Logstash ArcSight module in the
+`logstash.yml` configuration file or with overrides through the command line
+like in the getting started. For more information about configuring modules, see
+<<logstash-modules>>.
+
+As an example, the following settings can be appended to `logstash.yml` to
+configure your module:
+
+[source,yaml]
+-----
+modules:
+  - name: arcsight
+    var.input.eventbroker.bootstrap_servers: "eb_host:39092"
+    var.input.eventbroker.topics: "eb_topic"
+    var.elasticsearch.hosts: "localhost:9200"
+    var.elasticsearch.username: "elastic"
+    var.elasticsearch.password: "changeme"
+    var.kibana.host: “localhost:5601”
+    var.kibana.username: "elastic"
+    var.kibana.password: "changeme"
+-----
+
+[[arcsight-module-config]]
+===== Logstash ArcSight Module Configuration Options
+
+The ArcSight module provides the following settings for configuring the behavior
+of the module. These settings include ArcSight-specific options plus common
+options that are supported by all Logstash modules. 
+
+When you override a setting at the command line, remember to prefix the setting
+with the module name, for example, `arcsight.var.inputs` instead of `var.inputs`.
+
+If you don't specify configuration settings, Logstash uses the defaults.
+
+*ArcSight Module Options*
+
+*`var.inputs`*::
++
+--
+* Value type is <<string,string>>
+* Default value is "eventbroker"
+--
++
+Set the input(s) to expose for the Logstash ArcSight module. Valid settings are
+"eventbroker", "smartconnector", or "eventbroker,smartconnector" (exposes both
+inputs concurrently).
+
+*`var.input.eventbroker.bootstrap_servers`*::
++
+--
+* Value type is <<string,string>>
+* Default value is "localhost:39092"
+--
++
+A list of EB URLs to use for establishing the initial connection to the cluster.
+This list should be in the form of `host1:port1,host2:port2`. These URLs are
+just used for the initial connection to discover the full cluster membership
+(which may change dynamically), so this list need not contain the full set of
+servers (you may want more than one, though, in case a server is down).
+
+*`var.input.eventbroker.topics`*::
++
+--
+* Value type is <<array,array>>
+* Default value is ["eb-cef"]
+--
++
+A list of EB topics to subscribe to.
+
+*`var.input.smartconnector.port`*::
++
+--
+* Value type is <<number,number>>
+* Default value is 5000
+--
++
+The TCP port to listen on when receiving data from SCs.
+
+include::shared-module-options.asciidoc[]
diff --git a/docs/static/breaking-changes.asciidoc b/docs/static/breaking-changes.asciidoc
index c7297fef294..babc748cb34 100644
--- a/docs/static/breaking-changes.asciidoc
+++ b/docs/static/breaking-changes.asciidoc
@@ -27,6 +27,7 @@ These changes can impact any instance of Logstash and are plugin agnostic, but o
   `path.config`. This means any configurations  provided via `-e` will no longer be appended to the configurations provided via `-f`.
 * Configurations provided with `-f` or `config.path` will not be appended with `stdin` input and `stdout` output automatically.
 
+[float]
 ==== List of plugins bundled with Logstash
 
 The following plugins were removed from the 5.0 default bundle based on usage data. You can still install these plugins manually:
diff --git a/docs/static/config-management.asciidoc b/docs/static/config-management.asciidoc
new file mode 100644
index 00000000000..be6945242b7
--- /dev/null
+++ b/docs/static/config-management.asciidoc
@@ -0,0 +1,94 @@
+[[config-management]]
+== Configuration Management
+
+Logstash provides configuration management features to make it easier for you to
+manage updates to your configuration over time.
+
+The topics in this section describe Logstash configuration management features
+only. For information about other config management tools, such as Puppet and
+Chef, see the documentation for those projects. Also take a look at the
+https://forge.puppet.com/elastic/logstash[Logstash Puppet module documentation]. 
+
+[role="xpack"]
+[[logstash-centralized-pipeline-management]]
+=== Centralized Pipeline Configuration Management
+
+NOTE: Centralized pipeline configuration management is an {xpack} feature that
+requires a paid {xpack} license. See the
+https://www.elastic.co/subscriptions[Elastic Subscriptions] page for information
+about obtaining a license.
+
+The pipeline configuration management feature in {xpack} centralizes the
+creation and management of Logstash configuration pipelines. From within the
+pipeline management UI, you can control multiple Logstash instances. You can
+add, edit, and delete pipeline configurations. On the Logstash side, you simply
+need to register Logstash to use the centrally managed pipeline configurations. 
+
+The pipeline configurations, along with some metadata, are stored in
+Elasticsearch. Any changes that you make to a pipeline definition in the UI are
+picked up and loaded automatically by all Logstash instances registered to use
+the pipeline. The changes are applied immediately; you do not have to restart
+Logstash to pick up the changes, as long as Logtash is already registered to
+use the pipeline. 
+
+To use centralized pipeline configuration management, you must install {xpack}
+and specify the
+{logstash-ref}/settings-xpack.html#configuration-management-settings[configuration management settings]
+described under {logstash-ref}/setup-xpack.html[Setting up {xpack}].
+
+IMPORTANT: After you've configured Logstash to use centralized pipeline
+configuration management, you can no longer specify local pipeline
+configurations. This means that the `pipelines.yml` file and settings like
+`path.config` and `config.string` are inactive when this feature is enabled.
+
+==== Pipeline management UI
+
+To access the pipeline management UI, open {kib} in your browser and go to
+the Management tab. If you've set up configuration management correctly, you'll
+see an area for managing Logstash. Click the *Pipelines* link.
+
+image::static/images/centralized_config.png[]
+
+Here you can add, edit, or delete Logstash pipeline configurations.
+
+To add a new pipeline, click the *Add* button and specify values for the
+following fields:
+
+[horizontal]
+Pipeline ID::
+A name that uniquely identifies the pipeline. You use this ID when you
+{logstash-ref}/settings-xpack.html[configure {xpack}] and specify a list of
+pipeline IDs in the `xpack.management.pipeline.id` setting.
+
+Version::
+A string value that you can use as metadata to track the version of the pipeline
+configuration. For example, `v1.0.0`. This information is for your use. Logtash
+doesn't currently manage or validate the version information.
+
+Description::
+A description of the pipeline configuration. This information is for your use.
+
+Pipeline::
+The pipeline configuration. You can treat the editor in the pipeline management
+UI like any other editor. You don't have to worry about whitespace or indentation. 
+
+image::static/images/new_pipeline.png[]
+
+When you click *Save*, the pipeline runs on all Logstash instances that are
+registered to use the pipeline. There is no validation done at the UI level.
+The UI will save the new configuration, and Logstash will attempt to load it.
+You need to check the local Logstash logs for configuration errors. If you're
+using the Logstash monitoring feature in {xpack}, you can also navigate to the
+Monitoring tab to check the status of your Logstash nodes.
+
+You can specify multiple pipeline configurations that run in parallel on the
+same Logstash node.
+
+If you edit a pipeline configuration and save the changes, Logstash reloads
+the configuration in the background and continues processing events.
+
+If you delete a pipeline (for example, `apache`) from the UI, Logstash will
+attempt to stop the pipeline if it's running. Logstash will wait until all
+events have been fully processed by the pipeline. Before deleting a pipeline,
+make sure you understand your data sources because stopping a pipeline may
+lead to data loss. 
diff --git a/docs/static/dead-letter-queues.asciidoc b/docs/static/dead-letter-queues.asciidoc
index 5cd13acadae..cfbf511d6bb 100644
--- a/docs/static/dead-letter-queues.asciidoc
+++ b/docs/static/dead-letter-queues.asciidoc
@@ -56,13 +56,13 @@ path.dead_letter_queue: "path/to/data/dead_letter_queue"
 ===== File Rotation
 
 Dead letter queues have a built-in file rotation policy that manages the file
-size of the queue. When the file size reaches a preconfigured threshold,  a new
-file is created automatically. The size limit of the dead letter queue is
-constrained only by the amount of space that you have available on disk.
+size of the queue. When the file size reaches a preconfigured threshold, a new
+file is created automatically.
 
-NOTE: Dead letter queues retain all the events that are written to them.
-Currently, you cannot configure the size of the queue or the size of the files
-that are used to store the queue. 
+By default, the maximum size of each dead letter queue is set to 1024mb. To
+change this setting, use the `dead_letter_queue.max_bytes` option.  Entries
+will be dropped if they would increase the size of the dead letter queue beyond
+this setting. 
 
 [[processing-dlq-events]]
 ==== Processing Events in the Dead Letter Queue
@@ -117,6 +117,9 @@ queue, it will continue to run and process new events as they stream into the
 queue. This means that you do not need to stop your production system to handle
 events in the dead letter queue. 
 
+NOTE: Events emitted from the dead letter queue input plugin will not be resubmitted to the
+dead letter queue if they cannot be processed correctly
+
 [[dlq-timestamp]]
 ==== Reading From a Timestamp
 
diff --git a/docs/static/deploying.asciidoc b/docs/static/deploying.asciidoc
index de935e15d88..33a96a93fa8 100644
--- a/docs/static/deploying.asciidoc
+++ b/docs/static/deploying.asciidoc
@@ -120,7 +120,7 @@ Enterprise-grade security is available across the entire delivery chain.
 
 * Wire encryption is recommended for both the transport from
 {filebeat}configuring-ssl-logstash.html[Beats to Logstash] and from
-{xpack-ref}/logstash.html[Logstash to Elasticsearch].
+{logstash-ref}/ls-security.html[Logstash to Elasticsearch].
 * There’s a wealth of security options when communicating with Elasticsearch
 including basic authentication, TLS, PKI, LDAP, AD, and other custom realms.
 To enable Elasticsearch security, consult the
diff --git a/docs/static/docker.asciidoc b/docs/static/docker.asciidoc
index e470a285d39..83b463bdf7c 100644
--- a/docs/static/docker.asciidoc
+++ b/docs/static/docker.asciidoc
@@ -10,7 +10,7 @@ https://github.com/elastic/logstash-docker/tree/{branch}[GitHub].
 The images are shipped with https://www.elastic.co/products/x-pack[X-Pack]
 installed.
 
-=== Pulling the image
+==== Pulling the image
 Obtaining Logstash for Docker is as simple as issuing a +docker
 pull+ command against the Elastic Docker registry.
 
diff --git a/docs/static/getting-started-with-logstash.asciidoc b/docs/static/getting-started-with-logstash.asciidoc
index d95ceb7b2c2..d90bf0d14b7 100644
--- a/docs/static/getting-started-with-logstash.asciidoc
+++ b/docs/static/getting-started-with-logstash.asciidoc
@@ -9,8 +9,8 @@ This section includes the following topics:
 
 * <<installing-logstash>>
 * <<first-event>>
-* {logstash}advanced-pipeline.html[Advanced Pipeline]
-* {logstash}multiple-input-output-plugins.html[Multiple Output Plugins]
+* {logstash-ref}/advanced-pipeline.html[Advanced Pipeline]
+* {logstash-ref}/multiple-input-output-plugins.html[Multiple Output Plugins]
 
 [[installing-logstash]]
 === Installing Logstash
@@ -121,7 +121,7 @@ it with:
 sudo apt-get update && sudo apt-get install logstash
 --------------------------------------------------
 
-See {logstash}running-logstash.html[Running Logstash] for details about managing Logstash as a system service.
+See {logstash-ref}/running-logstash.html[Running Logstash] for details about managing Logstash as a system service.
 
 endif::[]
 
@@ -168,14 +168,14 @@ sudo yum install logstash
 WARNING: The repositories do not work with older rpm based distributions
          that still use RPM v3, like CentOS5.
 
-See the {logstash}running-logstash.html[Running Logstash] document for managing Logstash as a system service.
+See the {logstash-ref}/running-logstash.html[Running Logstash] document for managing Logstash as a system service.
 
 endif::[]
 
 ==== Docker
 
 An image is available for running Logstash as a Docker container. It is
-available from the Elastic Docker registry. See {logstash}docker.html[Running Logstash on Docker] for
+available from the Elastic Docker registry. See {logstash-ref}/docker.html[Running Logstash on Docker] for
 details on how to configure and run Logstash Docker containers.
 
 [[first-event]]
@@ -200,7 +200,7 @@ cd logstash-{logstash_version}
 bin/logstash -e 'input { stdin { } } output { stdout {} }'
 --------------------------------------------------
 
-NOTE: The location of the `bin` directory varies by platform. See {logstash}dir-layout.html[Directory layout]
+NOTE: The location of the `bin` directory varies by platform. See {logstash-ref}/dir-layout.html[Directory layout]
 to find the location of `bin\logstash` on your system.
 
 The `-e` flag enables you to specify a configuration directly from the command line. Specifying configurations at the
diff --git a/docs/static/images/arcsight-architecture-diagram-eventbroker-2017.png b/docs/static/images/arcsight-architecture-diagram-eventbroker-2017.png
new file mode 100644
index 00000000000..cf169f510cf
Binary files /dev/null and b/docs/static/images/arcsight-architecture-diagram-eventbroker-2017.png differ
diff --git a/docs/static/images/arcsight-architecture-diagram-smartconnector-2017.png b/docs/static/images/arcsight-architecture-diagram-smartconnector-2017.png
new file mode 100644
index 00000000000..12f62090c7c
Binary files /dev/null and b/docs/static/images/arcsight-architecture-diagram-smartconnector-2017.png differ
diff --git a/docs/static/images/arcsight-network-overview.png b/docs/static/images/arcsight-network-overview.png
new file mode 100644
index 00000000000..c350fd3bfbf
Binary files /dev/null and b/docs/static/images/arcsight-network-overview.png differ
diff --git a/docs/static/images/arcsight-network-suspicious.png b/docs/static/images/arcsight-network-suspicious.png
new file mode 100644
index 00000000000..348bfe96482
Binary files /dev/null and b/docs/static/images/arcsight-network-suspicious.png differ
diff --git a/docs/static/images/centralized_config.png b/docs/static/images/centralized_config.png
new file mode 100644
index 00000000000..c0307afdc91
Binary files /dev/null and b/docs/static/images/centralized_config.png differ
diff --git a/docs/static/images/logstash-module-overview.png b/docs/static/images/logstash-module-overview.png
new file mode 100644
index 00000000000..67e6609d68d
Binary files /dev/null and b/docs/static/images/logstash-module-overview.png differ
diff --git a/docs/static/images/monitoring-ui.png b/docs/static/images/monitoring-ui.png
new file mode 100644
index 00000000000..e8c42b9ecff
Binary files /dev/null and b/docs/static/images/monitoring-ui.png differ
diff --git a/docs/static/images/netflow-conversation-partners.png b/docs/static/images/netflow-conversation-partners.png
new file mode 100644
index 00000000000..b0ae38be4ac
Binary files /dev/null and b/docs/static/images/netflow-conversation-partners.png differ
diff --git a/docs/static/images/netflow-geo-location.png b/docs/static/images/netflow-geo-location.png
new file mode 100644
index 00000000000..f5d5cb8e44d
Binary files /dev/null and b/docs/static/images/netflow-geo-location.png differ
diff --git a/docs/static/images/netflow-overview.png b/docs/static/images/netflow-overview.png
new file mode 100644
index 00000000000..fa97ed6ba9d
Binary files /dev/null and b/docs/static/images/netflow-overview.png differ
diff --git a/docs/static/images/netflow-traffic-analysis.png b/docs/static/images/netflow-traffic-analysis.png
new file mode 100644
index 00000000000..2977e07f361
Binary files /dev/null and b/docs/static/images/netflow-traffic-analysis.png differ
diff --git a/docs/static/images/new_pipeline.png b/docs/static/images/new_pipeline.png
new file mode 100644
index 00000000000..23efd83cc98
Binary files /dev/null and b/docs/static/images/new_pipeline.png differ
diff --git a/docs/static/images/pipeline-diagram.png b/docs/static/images/pipeline-diagram.png
new file mode 100644
index 00000000000..63f85b8f447
Binary files /dev/null and b/docs/static/images/pipeline-diagram.png differ
diff --git a/docs/static/images/pipeline-filter-detail.png b/docs/static/images/pipeline-filter-detail.png
new file mode 100644
index 00000000000..dc9d1cf22ea
Binary files /dev/null and b/docs/static/images/pipeline-filter-detail.png differ
diff --git a/docs/static/images/pipeline-input-detail.png b/docs/static/images/pipeline-input-detail.png
new file mode 100644
index 00000000000..4f01346c3fe
Binary files /dev/null and b/docs/static/images/pipeline-input-detail.png differ
diff --git a/docs/static/images/pipeline-output-detail.png b/docs/static/images/pipeline-output-detail.png
new file mode 100644
index 00000000000..83392eaab53
Binary files /dev/null and b/docs/static/images/pipeline-output-detail.png differ
diff --git a/docs/static/images/pipeline-viewer-overview.png b/docs/static/images/pipeline-viewer-overview.png
new file mode 100644
index 00000000000..815538f938a
Binary files /dev/null and b/docs/static/images/pipeline-viewer-overview.png differ
diff --git a/docs/static/introduction.asciidoc b/docs/static/introduction.asciidoc
index 7e48c9a3739..b16e87fd61e 100644
--- a/docs/static/introduction.asciidoc
+++ b/docs/static/introduction.asciidoc
@@ -27,25 +27,25 @@ Collect more, so you can know more. Logstash welcomes data of all shapes and siz
 Where it all started.
 
 * Handle all types of logging data
-** Easily ingest a multitude of web logs like {logstash}advanced-pipeline.html[Apache], and application
-logs like {logstash}plugins-inputs-log4j.html[log4j] for Java
-** Capture many other log formats like {logstash}plugins-inputs-syslog.html[syslog],
-{logstash}plugins-inputs-eventlog.html[Windows event logs], networking and firewall logs, and more
+** Easily ingest a multitude of web logs like {logstash-ref}/advanced-pipeline.html[Apache], and application
+logs like {logstash-ref}/plugins-inputs-log4j.html[log4j] for Java
+** Capture many other log formats like {logstash-ref}/plugins-inputs-syslog.html[syslog],
+{logstash-ref}/plugins-inputs-eventlog.html[Windows event logs], networking and firewall logs, and more
 * Enjoy complementary secure log forwarding capabilities with https://www.elastic.co/products/beats/filebeat[Filebeat]
-* Collect metrics from {logstash}plugins-inputs-ganglia.html[Ganglia], {logstash}plugins-codecs-collectd.html[collectd],
-{logstash}plugins-codecs-netflow.html[NetFlow], {logstash}plugins-inputs-jmx.html[JMX], and many other infrastructure
-and application platforms over {logstash}plugins-inputs-tcp.html[TCP] and {logstash}plugins-inputs-udp.html[UDP]
+* Collect metrics from {logstash-ref}/plugins-inputs-ganglia.html[Ganglia], {logstash-ref}/plugins-codecs-collectd.html[collectd],
+{logstash-ref}/plugins-codecs-netflow.html[NetFlow], {logstash-ref}/plugins-inputs-jmx.html[JMX], and many other infrastructure
+and application platforms over {logstash-ref}/plugins-inputs-tcp.html[TCP] and {logstash-ref}/plugins-inputs-udp.html[UDP]
 
 [float]
 === The Web
 
 Unlock the World Wide Web.
 
-* Transform {logstash}plugins-inputs-http.html[HTTP requests] into events
-** Consume from web service firehoses like {logstash}plugins-inputs-twitter.html[Twitter] for social sentiment analysis
+* Transform {logstash-ref}/plugins-inputs-http.html[HTTP requests] into events
+** Consume from web service firehoses like {logstash-ref}/plugins-inputs-twitter.html[Twitter] for social sentiment analysis
 ** Webhook support for GitHub, HipChat, JIRA, and countless other applications
 ** Enables many https://www.elastic.co/products/x-pack/alerting[Watcher] alerting use cases
-* Create events by polling {logstash}plugins-inputs-http_poller.html[HTTP endpoints] on demand
+* Create events by polling {logstash-ref}/plugins-inputs-http_poller.html[HTTP endpoints] on demand
 ** Universally capture health, performance, metrics, and other types of data from web application interfaces
 ** Perfect for scenarios where the control of polling is preferred over receiving
 
@@ -55,9 +55,9 @@ Unlock the World Wide Web.
 Discover more value from the data you already own.
 
 * Better understand your data from any relational database or NoSQL store with a
-{logstash}plugins-inputs-jdbc.html[JDBC] interface 
-* Unify diverse data streams from messaging queues like Apache {logstash}plugins-outputs-kafka.html[Kafka],
-{logstash}plugins-outputs-rabbitmq.html[RabbitMQ], {logstash}plugins-outputs-sqs.html[Amazon SQS], and {logstash}plugins-outputs-zeromq.html[ZeroMQ]
+{logstash-ref}/plugins-inputs-jdbc.html[JDBC] interface 
+* Unify diverse data streams from messaging queues like Apache {logstash-ref}/plugins-outputs-kafka.html[Kafka],
+{logstash-ref}/plugins-outputs-rabbitmq.html[RabbitMQ], {logstash-ref}/plugins-outputs-sqs.html[Amazon SQS], and {logstash-ref}/plugins-outputs-zeromq.html[ZeroMQ]
 
 [float]
 === Sensors and IoT
@@ -76,18 +76,18 @@ The better the data, the better the knowledge. Clean and transform your data dur
 insights immediately at index or output time. Logstash comes out-of-box with many aggregations and mutations along
 with pattern matching, geo mapping, and dynamic lookup capabilities.
 
-* {logstash}plugins-filters-grok.html[Grok] is the bread and butter of Logstash filters and is used ubiquitously to derive
+* {logstash-ref}/plugins-filters-grok.html[Grok] is the bread and butter of Logstash filters and is used ubiquitously to derive
 structure out of unstructured data. Enjoy a wealth of integrated patterns aimed to help quickly resolve web, systems,
 networking, and other types of event formats.
-* Expand your horizons by deciphering {logstash}plugins-filters-geoip.html[geo coordinates] from IP addresses, normalizing
-{logstash}plugins-filters-date.html[date] complexity, simplifying {logstash}plugins-filters-kv.html[key-value pairs] and
-{logstash}plugins-filters-csv.html[CSV] data, {logstash}plugins-filters-fingerprint.html[fingerprinting](anonymizing) sensitive information,
-and further enriching your data with {logstash}plugins-filters-translate.html[local lookups] or Elasticsearch
-{logstash}plugins-filters-elasticsearch.html[queries].
-* Codecs are often used to ease the processing of common event structures like {logstash}plugins-codecs-json.html[JSON]
-and {logstash}plugins-codecs-multiline.html[multiline] events.
+* Expand your horizons by deciphering {logstash-ref}/plugins-filters-geoip.html[geo coordinates] from IP addresses, normalizing
+{logstash-ref}/plugins-filters-date.html[date] complexity, simplifying {logstash-ref}/plugins-filters-kv.html[key-value pairs] and
+{logstash-ref}/plugins-filters-csv.html[CSV] data, {logstash-ref}/plugins-filters-fingerprint.html[fingerprinting](anonymizing) sensitive information,
+and further enriching your data with {logstash-ref}/plugins-filters-translate.html[local lookups] or Elasticsearch
+{logstash-ref}/plugins-filters-elasticsearch.html[queries].
+* Codecs are often used to ease the processing of common event structures like {logstash-ref}/plugins-codecs-json.html[JSON]
+and {logstash-ref}/plugins-codecs-multiline.html[multiline] events.
 
-See {logstash}transformation.html[Transforming Data] for an overview of some of the popular data processing plugins.
+See {logstash-ref}/transformation.html[Transforming Data] for an overview of some of the popular data processing plugins.
 
 [float]
 == Choose Your Stash
@@ -95,43 +95,30 @@ See {logstash}transformation.html[Transforming Data] for an overview of some of
 Route your data where it matters most. Unlock various downstream analytical and operational use cases by storing,
 analyzing, and taking action on your data.
 
-[cols="a,a"]
-|=======================================================================
-|
-
 *Analysis*
 
-* {logstash}plugins-outputs-elasticsearch.html[Elasticsearch]
-* Data stores such as {logstash}plugins-outputs-mongodb.html[MongoDB] and {logstash}plugins-outputs-riak.html[Riak]
-
-|
+* {logstash-ref}/plugins-outputs-elasticsearch.html[Elasticsearch]
+* Data stores such as {logstash-ref}/plugins-outputs-mongodb.html[MongoDB] and {logstash-ref}/plugins-outputs-riak.html[Riak]
 
 *Archiving*
 
-* {logstash}plugins-outputs-webhdfs.html[HDFS]
-* {logstash}plugins-outputs-s3.html[S3]
-* {logstash}plugins-outputs-google_cloud_storage.html[Google Cloud Storage]
-
-|
+* {logstash-ref}/plugins-outputs-webhdfs.html[HDFS]
+* {logstash-ref}/plugins-outputs-s3.html[S3]
+* {logstash-ref}/plugins-outputs-google_cloud_storage.html[Google Cloud Storage]
 
 *Monitoring*
 
-* {logstash}plugins-outputs-nagios.html[Nagios]
-* {logstash}plugins-outputs-ganglia.html[Ganglia]
-* {logstash}plugins-outputs-zabbix.html[Zabbix]
-* {logstash}plugins-outputs-graphite.html[Graphite]
-* {logstash}plugins-outputs-datadog.html[Datadog]
-* {logstash}plugins-outputs-cloudwatch.html[CloudWatch]
-
-|
+* {logstash-ref}/plugins-outputs-nagios.html[Nagios]
+* {logstash-ref}/plugins-outputs-ganglia.html[Ganglia]
+* {logstash-ref}/plugins-outputs-zabbix.html[Zabbix]
+* {logstash-ref}/plugins-outputs-graphite.html[Graphite]
+* {logstash-ref}/plugins-outputs-datadog.html[Datadog]
+* {logstash-ref}/plugins-outputs-cloudwatch.html[CloudWatch]
 
 *Alerting*
 
 * https://www.elastic.co/products/watcher[Watcher] with Elasticsearch
-* {logstash}plugins-outputs-email.html[Email]
-* {logstash}plugins-outputs-pagerduty.html[Pagerduty]
-* {logstash}plugins-outputs-hipchat.html[HipChat]
-* {logstash}plugins-outputs-irc.html[IRC]
-* {logstash}plugins-outputs-sns.html[SNS]
-
-|=======================================================================
+* {logstash-ref}/plugins-outputs-email.html[Email]
+* {logstash-ref}/plugins-outputs-pagerduty.html[Pagerduty]
+* {logstash-ref}/plugins-outputs-irc.html[IRC]
+* {logstash-ref}/plugins-outputs-sns.html[SNS]
diff --git a/docs/static/logging.asciidoc b/docs/static/logging.asciidoc
index bdf569cec16..540992e1138 100644
--- a/docs/static/logging.asciidoc
+++ b/docs/static/logging.asciidoc
@@ -19,7 +19,7 @@ You can specify the log file location using `--path.logs` setting.
 
 Logstash ships with a `log4j2.properties` file with out-of-the-box settings. You  can modify this file directly to change the 
 rotation policy, type, and other https://logging.apache.org/log4j/2.x/manual/configuration.html#Loggers[log4j2 configuration]. 
-You must restart Lostash to apply any changes that you make to this file.
+You must restart Logstash to apply any changes that you make to this file.
 
 ==== Slowlog
 
diff --git a/docs/static/modules.asciidoc b/docs/static/modules.asciidoc
new file mode 100644
index 00000000000..66eec641562
--- /dev/null
+++ b/docs/static/modules.asciidoc
@@ -0,0 +1,195 @@
+[[logstash-modules]]
+== Working with Logstash Modules
+
+Logstash modules provide a quick, end-to-end solution for ingesting data and
+visualizing it with purpose-built dashboards.
+
+Each module comes pre-packaged with Logstash configurations, Kibana dashboards,
+and other meta files that make it easier for you to set up the Elastic Stack for
+specific use cases or data sources.
+
+You can think of modules as providing three essential functions that make it
+easier for you to get started. When you run a module, it will:
+
+. Create the Elasticsearch index.
+
+. Set up the Kibana dashboards, including the index pattern, searches, and
+visualizations required to visualize your data in Kibana.
+
+. Run the Logstash pipeline with the configurations required to read and parse
+the data.
+
+image::static/images/logstash-module-overview.png[Logstash modules overview]
+
+[float]
+[[running-logstash-modules]]
+=== Running modules
+
+To run a module and set up dashboards, you specify the following options:
+
+[source,shell]
+----
+bin/logstash --modules MODULE_NAME --setup [-M "CONFIG_SETTING=VALUE"]
+----
+
+
+//TODO: For 6.0, show how to run mutliple modules
+
+Where:
+
+* `--modules` runs the Logstash module specified by `MODULE_NAME`.
+
+* `-M "CONFIG_SETTING=VALUE"` is optional and overrides the specified
+configuration setting. You can specify multiple overrides. Each override must
+start with `-M`. See <<overriding-logstash-module-settings>> for more info.
+
+* `--setup` creates an index pattern in Elasticsearch and imports Kibana
+dashboards and visualizations. Running `--setup` is a one-time setup step. Omit
+this option for subsequent runs of the module to avoid overwriting existing
+Kibana dashboards.
+
+For example, the following command runs the Netflow module with the default
+settings, and sets up the netflow index pattern and dashboards:
+
+[source,shell]
+----
+bin/logstash --modules netflow --setup 
+----
+
+The following command runs the Netflow module and overrides the Elasticsearch
+`host` setting. Here it's assumed that you've already run the setup step.
+
+[source,shell]
+----
+bin/logstash --modules netflow -M "netflow.var.elasticsearch.host=es.mycloud.com"
+----
+
+
+[float]
+[[configuring-logstash-modules]]
+=== Configuring modules
+
+To configure a module, you can either
+<<setting-logstash-module-config,specify configuration settings>> in the
+`logstash.yml` <<logstash-settings-file,settings file>>, or use command-line overrides to
+<<overriding-logstash-module-settings,specify settings at the command line>>.
+
+[float]
+[[setting-logstash-module-config]]
+==== Specify module settings in `logstash.yml`
+
+To specify module settings in the `logstash.yml`
+<<logstash-settings-file,settings file>> file, you add a module definition to
+the modules array. Each module definition begins with a dash (-) and is followed
+by `name: module_name` then a series of name/value pairs that specify module
+settings. For example:
+
+[source,shell]
+----
+modules:
+- name: netflow
+  var.elasticsearch.hosts: "es.mycloud.com"
+  var.elasticsearch.username: "foo"
+  var.elasticsearch.password: "password"
+  var.kibana.host: "kb.mycloud.com"
+  var.kibana.username: "foo"
+  var.kibana.password: "password"
+  var.input.tcp.port: 5606
+----
+
+For a list of available module settings, see the documentation for the module.
+
+[float]
+[[overriding-logstash-module-settings]]
+==== Specify module settings at the command line
+
+You can override module settings by specifying one or more configuration
+overrides when you start Logstash. To specify an override, you use the `-M`
+command line option:
+
+[source,shell]
+----
+-M MODULE_NAME.var.PLUGINTYPE1.PLUGINNAME1.KEY1=VALUE
+----
+
+Notice that the fully-qualified setting name includes the module name.
+
+You can specify multiple overrides. Each override must start with `-M`. 
+
+The following command runs the Netflow module and overrides both the
+Elasticsearch `host` setting and the `udp.port` setting:
+
+[source,shell]
+----
+bin/logstash --modules netflow -M "netflow.var.input.udp.port=3555" -M "netflow.var.elasticsearch.hosts=my-es-cloud"
+----
+
+Any settings defined in the command line are ephemeral and will not persist across
+subsequent runs of Logstash. If you want to persist a configuration, you need to
+set it in the `logstash.yml` <<logstash-settings-file,settings file>>.
+
+Settings that you specify at the command line are merged with any settings 
+specified in the `logstash.yml` file. If an option is set in both
+places, the value specified at the command line takes precedence. 
+
+[[connecting-to-cloud]]
+=== Using Elastic Cloud
+
+Logstash comes with two settings that simplify using modules with https://cloud.elastic.co/[Elastic Cloud].
+The Elasticsearch and Kibana hostnames in Elastic Cloud may be hard to set
+in the Logstash config or on the commandline, so a Cloud ID can be used instead.
+
+==== Cloud ID
+
+The Cloud ID, which can be found in the Elastic Cloud web console, is used by
+Logstash to build the Elasticsearch and Kibana hosts settings.
+It is a base64 encoded text value of about 120 characters made up of upper and
+lower case letters and numbers.
+If you have several Cloud IDs, you can add a label, which is ignored
+internally, to help you tell them apart. To add a label you should prefix your
+Cloud ID with a label and a `:` separator in this format "<label>:<cloud-id>"
+
+`cloud.id` will overwrite these settings:
+----
+var.elasticsearch.hosts
+var.kibana.host
+----
+
+==== Cloud Auth
+This is optional. Construct this value by following this format "<username>:<password>".
+Use your Cloud username for the first part. Use your Cloud password for the second part,
+which is given once in the Cloud UI when you create a cluster.
+As your Cloud password is changeable, if you change it in the Cloud UI remember to change it here too.
+
+`cloud.auth` when specified will overwrite these settings:
+----
+var.elasticsearch.username
+var.elasticsearch.password
+var.kibana.username
+var.kibana.password
+----
+
+Example:
+
+These settings can be specified in the `logstash.yml` <<logstash-settings-file,settings file>>.
+They should be added separately from any module configuration settings you may have added before.
+[source,yaml]
+----
+# example with a label
+cloud.id: "staging:dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyRub3RhcmVhbCRpZGVudGlmaWVy"
+cloud.auth: "elastic:changeme"
+----
+----
+# example without a label
+cloud.id: "dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyRub3RhcmVhbCRpZGVudGlmaWVy"
+cloud.auth: "elastic:changeme"
+----
+
+These settings can be also specified at the command line, like this:
+
+["source","sh",subs="attributes,callouts"]
+----
+bin/logstash --modules netflow -M "netflow.var.input.udp.port=3555" --cloud.id <cloud-id> --cloud.auth <cloud.auth>
+----
+
+
diff --git a/docs/static/monitoring-apis.asciidoc b/docs/static/monitoring-apis.asciidoc
index e101ee71fbf..829b000337a 100644
--- a/docs/static/monitoring-apis.asciidoc
+++ b/docs/static/monitoring-apis.asciidoc
@@ -1,4 +1,3 @@
-[float]
 [[monitoring]]
 === Monitoring APIs
 
diff --git a/docs/static/monitoring.asciidoc b/docs/static/monitoring.asciidoc
index b6fe6cb609f..d9ee9870d8f 100644
--- a/docs/static/monitoring.asciidoc
+++ b/docs/static/monitoring.asciidoc
@@ -12,15 +12,21 @@ The metrics collected by Logstash include:
 runtime stats.
 * Hot threads.
 
-You can use the <<logstash-monitoring-ui,monitoring UI>> in X-Pack to view
+You can use the <<logstash-monitoring-ui,monitoring UI>> in {xpack} to view
 these metrics and gain insight into how your Logstash deployment is running.
 
 Or you can use the basic <<monitoring,monitoring APIs>> providing by Logstash
 to retrieve these metrics.
 
-[float]
+The <<logstash-pipeline-viewer,pipeline viewer>> in {xpack} offers additional
+visibility into the behavior and performance of complex pipeline configurations.
+It shows a graph representation of the overall pipeline topology, data flow, and
+branching logic, overlayed with important metrics, like events per second, for
+each plugin in the view.
+
+[role="xpack"]
 [[logstash-monitoring-ui]]
-=== [xpack]#Monitoring UI#
+=== Monitoring UI
 
 NOTE: Monitoring is an X-Pack feature under the Basic License and is therefore
 *free to use*. To get started, consult the
diff --git a/docs/static/multiple-pipelines.asciidoc b/docs/static/multiple-pipelines.asciidoc
index 7794c0e3b45..59abeb40828 100644
--- a/docs/static/multiple-pipelines.asciidoc
+++ b/docs/static/multiple-pipelines.asciidoc
@@ -27,3 +27,5 @@ Using multiple pipelines is especially useful if your current configuration has
 Having multiple pipelines in a single instance also allows these event flows to have different performance and durability parameters (for example, different settings for pipeline workers and persistent queues). This separation means that a blocked output in one pipeline won't exert backpressure in the other.
 
 That said, it's important to take into account resource competition between the pipelines, given that the default values are tuned for a single pipeline. So, for example, consider reducing the number of pipeline workers used by each pipeline, because each pipeline will use 1 worker per CPU core by default.
+
+Persistent queues and dead letter queues are isolated per pipeline, with their locations namespaced by the `pipeline.id` value.
\ No newline at end of file
diff --git a/docs/static/netflow-module.asciidoc b/docs/static/netflow-module.asciidoc
new file mode 100644
index 00000000000..ccb55d4f99a
--- /dev/null
+++ b/docs/static/netflow-module.asciidoc
@@ -0,0 +1,158 @@
+[[netflow-module]]
+=== Logstash Netflow Module
+
+++++
+<titleabbrev>Netflow Module</titleabbrev>
+++++
+
+The Logstash Netflow module simplifies the collection, normalization, and
+visualization of network flow data. With a single command, the module parses
+network flow data, indexes the events into Elasticsearch, and installs a suite
+of Kibana dashboards to get you exploring your data immediately. 
+
+Logstash modules support Netflow Version 5 and 9.
+
+==== What is Flow Data?
+
+Netflow is a type of data record streamed from capable network devices. It
+contains information about connections traversing the device, and includes
+source IP addresses and ports, destination IP addresses and ports, types of
+service, VLANs, and other information that can be encoded into frame and
+protocol headers. With Netflow data, network operators can go beyond monitoring
+simply the volume of data crossing their networks. They can understand where the
+traffic originated, where it is going, and what services or applications it is
+part of.
+
+[[netflow-requirements]]
+===== Requirements
+
+These instructions assume you have already installed Elastic Stack
+(Logstash, Elasticsearch, and Kibana) version 5.6 or higher. The products you
+need are https://www.elastic.co/downloads[available to download] and easy to
+install.
+
+[[netflow-getting-started]]
+==== Getting Started
+
+. Start the Logstash Netflow module by running the following command in the
+Logstash installation directory:
++
+[source,shell]
+-----
+bin/logstash --modules netflow --setup -M netflow.var.input.udp.port=NNNN
+-----
++
+Where `NNNN` is the UDP port on which Logstash will listen for network traffic
+data. If you don't specify a port, Logstash listens on port 2055 by default.
++
+The `--modules netflow` option spins up a Netflow-aware Logstash pipeline
+for ingestion.
++
+The `--setup` option creates a `netflow-*` index pattern in Elasticsearch and
+imports Kibana dashboards and visualizations. Running `--setup` is a one-time
+setup step. Omit this option for subsequent runs of the module to avoid
+overwriting existing Kibana dashboards.
++
+The command shown here assumes that you're running Elasticsearch and Kibana on
+your localhost. If you're not, you need to specify additional connection
+options. See <<configuring-netflow>>.
+
+. Explore your data in Kibana:
+.. Open your browser and navigate to
+http://localhost:5601[http://localhost:5601]. If security is enabled, you'll
+need to specify the Kibana username and password that you used when you set up
+security.
+.. Open *Netflow: Network Overview Dashboard*.
+.. See <<exploring-data-netflow>> for additional details on data exploration.
+
+[[exploring-data-netflow]]
+==== Exploring Your Data
+
+Once the Logstash Netflow module starts processing events, you can immediately
+begin using the packaged Kibana dashboards to explore and visualize your
+network flow data. 
+
+You can use the dashboards as-is, or tailor them to work better with existing
+use cases and business requirements.
+
+[[network-dashboards-netflow]]
+===== Example Dashboards
+
+On the *Overview* dashboard, you can see a summary of basic traffic data and set
+up filters before you drill down to gain deeper insight into the data.
+
+[role="screenshot"]
+image::static/images/netflow-overview.png[Netflow overview dashboard]
+
+For example, on the *Conversation Partners* dashboard, you can see the source
+and destination addresses of the client and server in any conversation.
+
+[role="screenshot"]
+image::static/images/netflow-conversation-partners.png[Netflow conversation partners dashboard]
+
+On the *Traffic Analysis* dashboard, you can identify high volume conversations
+by viewing the traffic volume in bytes.
+
+[role="screenshot"]
+image::static/images/netflow-traffic-analysis.png[Netflow traffic analysis dashboard]
+
+Then you can go to the *Geo Location* dashboard where you can visualize the
+location of destinations and sources on a heat map. 
+
+[role="screenshot"]
+image::static/images/netflow-geo-location.png[Netflow geo location dashboard]
+
+
+[[configuring-netflow]]
+==== Configuring the Module
+
+You can further refine the behavior of the Logstash Netflow module by specifying
+settings in the `logstash.yml` settings file, or overriding settings at the
+command line. 
+
+For example, the following configuration in the `settings.yml` file sets
+Logstash to listen on port 9996 for network traffic data: 
+[source,yaml]
+-----
+modules:
+  - name: netflow
+    var.input.udp.port: 9996
+-----
+
+To specify the same settings at the command line, you use:
+
+[source,shell]
+-----
+bin/logstash --modules netflow -M netflow.var.input.udp.port=9996
+-----
+
+For more information about configuring modules, see
+<<logstash-modules>>.
+
+[[netflow-module-config]]
+===== Configuration Options
+
+The Netflow module provides the following settings for configuring the behavior
+of the module. These settings include Netflow-specific options plus common
+options that are supported by all Logstash modules. 
+
+When you override a setting at the command line, remember to prefix the setting
+with the module name, for example,  `netflow.var.input.udp.port` instead of
+`var.input.udp.port`.
+
+If you don't specify configuration settings, Logstash uses the defaults.
+
+*Netflow Options*
+
+*`var.input.udp.port:`*::
++
+--
+* Value type is <<number,number>>
+* Default value is 2055. 
+--
++
+Sets the UDP port on which Logstash listens for network traffic data. Although
+2055 is the default for this setting, some devices use ports in the range of
+9995 through 9998, with 9996 being the most commonly used alternative.
+
+include::shared-module-options.asciidoc[]
diff --git a/docs/static/persistent-queues.asciidoc b/docs/static/persistent-queues.asciidoc
index 36ae6f2a425..3630f065b28 100644
--- a/docs/static/persistent-queues.asciidoc
+++ b/docs/static/persistent-queues.asciidoc
@@ -84,6 +84,7 @@ Logstash <<logstash-settings-file,settings file>>:
 * `queue.type`: Specify `persisted` to enable persistent queues. By default, persistent queues are disabled (default: `queue.type: memory`).
 * `path.queue`: The directory path where the data files will be stored. By default, the files are stored in `path.data/queue`. 
 * `queue.page_capacity`: The maximum size of a queue page in bytes. The queue data consists of append-only files called "pages". The default size is 250mb. Changing this value is unlikely to have performance benefits.
+* `queue.drain`: Specify `true` if you want Logstash to wait until the persistent queue is drained before shutting down. The amount of time it takes to drain the queue depends on the number of events that have accumulated in the queue. Therefore, you should avoid using this setting unless the queue, even when full, is relatively small and can be drained quickly. 
 // Technically, I know, this isn't "maximum number of events" it's really maximum number of events not yet read by the pipeline worker. We only use this for testing and users generally shouldn't be setting this.
 * `queue.max_events`:  The maximum number of events that are allowed in the queue. The default is 0 (unlimited). This value is used internally for the Logstash test suite.
 * `queue.max_bytes`: The total capacity of the queue in number of bytes. The
diff --git a/docs/static/pipeline-viewer.asciidoc b/docs/static/pipeline-viewer.asciidoc
new file mode 100644
index 00000000000..0192d8f3f10
--- /dev/null
+++ b/docs/static/pipeline-viewer.asciidoc
@@ -0,0 +1,117 @@
+[role="xpack"]
+[[logstash-pipeline-viewer]]
+=== Pipeline Viewer UI
+
+NOTE: The pipeline viewer is an {xpack} feature that requires a
+paid {xpack} license. See the
+https://www.elastic.co/subscriptions[Elastic Subscriptions] page for
+information about obtaining a license.
+
+The pipeline viewer in {xpack} provides a simple way for you to visualize and
+monitor the behavior of complex Logstash pipeline configurations. Within the
+pipeline viewer, you can explore a directed acyclic graph (DAG) representation
+of the overall pipeline topology, data flow, and branching logic. The diagram
+is overlayed with important metrics, like events per second and time spent in
+milliseconds, for each plugin in the view.
+
+The diagram includes visual indicators to draw your attention to potential
+bottlenecks in the pipeline, making it easy for you to diagnose and fix
+problems.
+
+[IMPORTANT]
+==========================================================================
+When you configure the stages in your Logstash pipeline, make sure you specify
+semantic IDs. If you don't specify IDs, Logstash generates them for you.
+
+Using semantic IDs makes it easier to identify the configurations that are
+causing bottlenecks. For example, you may have several grok filters running
+in your pipeline. If you haven't specified semantic IDs, you won't be able
+to tell at a glance which filters are slow. If you specify semantic IDs,
+such as `apacheParsingGrok` and `cloudwatchGrok`, you'll know exactly which
+grok filters are causing bottlenecks.
+
+==========================================================================
+
+Before using the pipeline viewer, you need to
+{logstash-ref}/setup-xpack.html[set up {xpack}] and configure
+{xpack-ref}/monitoring-logstash.html[Logstash monitoring]. 
+
+[float]
+==== View the pipeline diagram
+
+To view the pipeline diagram:
+
+. In Logstash, start the Logstash pipeline that you want to monitor.
++
+Assuming that you've set up Logstash monitoring, Logstash will begin shipping
+metrics to the monitoring cluster.
+
+. Navigate to the Monitoring tab in Kibana.
++
+You should see a Logstash section. 
++
+image::static/images/monitoring-ui.png[Monitoring UI]
+
+. Click the *Pipelines* link under Logstash to see all the pipelines that are
+being monitored.
++
+Each pipeline is identified by a pipeline ID (`main` by default). For each
+pipeline, you'll see a list of all versions of the pipeline stats that were
+captured during the specified time range.
++
+image::static/images/pipeline-viewer-overview.png[Pipeline Overview]
++
+The version information is auto-generated by Logstash. Each time you modify a
+pipeline, Logstash generates a new version hash. Viewing different versions
+of the pipeline stats allows you see how changes to the pipeline over time
+affect throughput and other metrics. Note that Logstash stores multiple versions
+of the pipeline stats; it does not store multiple versions of the pipeline
+configurations themselves.
+
+. Click a pipeline version in the list to drill down and explore the pipeline
+diagram.
++
+The diagram shows all the stages feeding data through the pipeline. It also shows
+conditional logic.
++
+image::static/images/pipeline-diagram.png[Pipeline Diagram]
++
+The information displayed on each node varies depending on the plugin type.
++
+Here's an example of an *input* node:
++
+image::static/images/pipeline-input-detail.png[Input node]
++
+The *I* badge indicates that this is an input stage. The node shows:
++
+--
+* input type - *stdin*
+* user-supplied ID - *logfileRead*
+* throughput expressed in events per second - *0.7 e/s*
+
+Here's an example of a *filter* node.
+
+image::static/images/pipeline-filter-detail.png[Filter node]
+
+The filter icon indicates that this is a filter stage. The node shows:
+
+* filter type - *sleep*
+* user-supplied ID - *caSleep*
+* worker usage expressed as the percentage of total execution time - *0%*
+* performance - the number of milliseconds spent processing each event - *20.00 ms/e*
+* throughput - the number of events sent per second - *0.0 e/s*
+
+Stats that are anomalously slow appear highlighted in the pipeline viewer.
+This doesn't necessarily indicate a problem, but it highlights potential
+bottle necks so that you can find them quickly.
+
+An *output* node shows the same information as a filter node, but it has an
+*O* badge to indicate that it is an output stage:
+
+image::static/images/pipeline-output-detail.png[Output node]
+--
+
+. Hover over a node in the diagram, and you'll see only the related nodes that
+are ancestors or descendants of the current node. 
+
+. Explore the diagram and look for performance anomalies.
diff --git a/docs/static/releasenotes.asciidoc b/docs/static/releasenotes.asciidoc
index da3b38a98d5..887f0f24fc4 100644
--- a/docs/static/releasenotes.asciidoc
+++ b/docs/static/releasenotes.asciidoc
@@ -3,19 +3,38 @@
 
 This section summarizes the changes in the following releases:
 
+* <<logstash-6-0-0-rc1,Logstash 6.0.0-rc1>>
+* <<logstash-6-0-0-beta2,Logstash 6.0.0-beta2>>
 * <<logstash-6-0-0-beta1,Logstash 6.0.0-beta1>>
 * <<logstash-6-0-0-alpha2,Logstash 6.0.0-alpha2>>
 * <<logstash-6-0-0-alpha1,Logstash 6.0.0-alpha1>>
 
-[[logstash-6-0-0-beta1]]
-=== Logstash 6.0.0-beta1 Release Notes
+[[logstash-6-0-0-rc1]]
+=== Logstash 6.0.0-rc1 Release Notes
+
+Placeholder for RC1 release notes. 
+
 
-Placeholder for beta1 release notes
+[[logstash-6-0-0-beta2]]
+=== Logstash 6.0.0-beta2 Release Notes
 
-* Added new `logstash.yml` setting: `config.support_escapes`. When
-  enabled, Logstash will interpret escape sequences in strings in the pipeline
-  configuration.
+* Added `cloud.id` and `cloud.auth` support for modules to provide an easy way to integrate with Elastic Cloud service.
+* Added an explicit `--setup` phase for modules which is used to install dashboards and Elasticsearch template.
+* Enabled DLQ support for multipipelines
+
+[[logstash-6-0-0-beta1]]
+=== Logstash 6.0.0-beta1 Release Notes
 
+* Added new `logstash.yml` setting: `config.support_escapes`. When enabled, Logstash will interpret escape sequences in 
+  strings in the pipeline configuration.
+* Breaking: The setting `config.reload.interval` has been changed to use time value strings such as `5m`, `10s`, etc for 
+  convenience. Previously, users had to convert this to a second time value themselves.
+* Breaking: The list of plugins bundled with Logstash 6.0.0-beta1 release has changed. Please consult the breaking changes document 
+  for a complete list.
+* Added an {xpack} feature to manage Logstash configurations centrally in Elasticsearch. We've also added a UI to manage 
+  configurations directly in Kibana without having to restart Logstash.
+* Users can now visualize the Logstash configuration pipeline as part of the {xpack} monitoring feature.
+* We now report an error if config file referenced in the `pipelines.yml` is missing.
 
 [[logstash-6-0-0-alpha2]]
 === Logstash 6.0.0-alpha2 Release Notes
diff --git a/docs/static/settings-file.asciidoc b/docs/static/settings-file.asciidoc
index a3449f4825d..0c63e4d2cea 100644
--- a/docs/static/settings-file.asciidoc
+++ b/docs/static/settings-file.asciidoc
@@ -96,8 +96,8 @@ The `logstash.yml` file includes the following settings:
 | `125`
 
 | `pipeline.batch.delay`
-| When creating pipeline event batches, how long in milliseconds to wait before dispatching an undersized
-  batch to filters and workers.
+| When creating pipeline event batches, how long in milliseconds to wait for
+  each event before dispatching an undersized batch to pipeline workers.
 | `5`
 
 | `pipeline.unsafe_shutdown`
@@ -178,10 +178,19 @@ The `logstash.yml` file includes the following settings:
 | The interval in milliseconds when a checkpoint is forced on the head page when persistent queues are enabled (`queue.type: persisted`). Specify `queue.checkpoint.interval: 0` for no periodic checkpoint.
 | 1000
 
+| `queue.drain`
+| When enabled, Logstash waits until the persistent queue is drained before shutting down.
+| false
+
 | `dead_letter_queue.enable`
 | Flag to instruct Logstash to enable the DLQ feature supported by plugins.
 | `false`
 
+| `dead_letter_queue.max_bytes`
+| The maximum size of each dead letter queue. Entries will be dropped if they
+  would increase the size of the dead letter queue beyond this setting.
+| `1024mb`
+
 | `path.dead_letter_queue`
 | The directory path where the data files will be stored for the dead-letter queue.
 | `path.data/dead_letter_queue`
diff --git a/docs/static/shared-module-options.asciidoc b/docs/static/shared-module-options.asciidoc
new file mode 100644
index 00000000000..a8898933e19
--- /dev/null
+++ b/docs/static/shared-module-options.asciidoc
@@ -0,0 +1,171 @@
+*Common options*
+
+The following configuration options are supported by all modules:
+
+*`var.elasticsearch.hosts`*::
++
+--
+* Value type is <<uri,uri>>
+* Default value is "localhost:9200"
+--
++
+Sets the host(s) of the Elasticsearch cluster. For each host, you must specify
+the hostname and port. For example, "myhost:9200". If given an <<array,array>>,
+Logstash will load balance requests across the hosts specified in the hosts
+parameter. It is important to exclude {ref}/modules-node.html[dedicated master
+nodes] from the hosts list to prevent Logstash from sending bulk requests to the
+master nodes. So this parameter should only reference either data or client
+nodes in Elasticsearch.
++
+Any special characters present in the URLs here MUST be URL escaped! This means #
+should be put in as %23 for instance.
+
+*`var.elasticsearch.username`*::
++
+--
+* Value type is <<string,string>>
+* Default value is "elastic"
+--
++
+The username to authenticate to a secure Elasticsearch cluster.
+
+*`var.elasticsearch.password`*::
++
+--
+* Value type is <<string,string>>
+* Default value is "changeme"
+--
++
+The password to authenticate to a secure Elasticsearch cluster.
+
+*`var.elasticsearch.ssl.enabled`*::
++
+--
+* Value type is <<boolean,boolean>>
+* There is no default value for this setting.
+--
++
+Enable SSL/TLS secured communication to the Elasticsearch cluster. Leaving this
+unspecified will use whatever scheme is specified in the URLs listed in `hosts`.
+If no explicit protocol is specified, plain HTTP will be used. If SSL is
+explicitly disabled here, the plugin will refuse to start if an HTTPS URL is
+given in hosts.
+
+*`var.elasticsearch.ssl.verification_mode`*::
++
+--
+* Value type is <<string,string>>
+* Default value is "strict"
+--
++
+The hostname verification setting when communicating with Elasticsearch. Set to
+`disable` to turn off hostname verification. Disabling this has serious security
+concerns.
+
+*`var.elasticsearch.ssl.certificate_authority`*::
++
+--
+* Value type is <<string,string>>
+* There is no default value for this setting
+--
++
+The path to an X.509 certificate to use to validate SSL certificates when
+communicating with Kibana.
+
+*`var.elasticsearch.ssl.certificate`*::
++
+--
+* Value type is <<string,string>>
+* There is no default value for this setting
+--
++
+The path to an X.509 certificate to use for client authentication when
+communicating with Elasticsearch.
+
+*`var.elasticsearch.ssl.key`*::
++
+--
+* Value type is <<string,string>>
+* There is no default value for this setting
+--
++
+The path to the certificate key for client authentication when communicating
+with Elasticsearch.
+
+*`var.kibana.host`*::
++
+--
+* Value type is <<string,string>>
+* Default value is "localhost:5601"
+--
++
+Sets the hostname and port of the Kibana instance to use for importing
+dashboards and visualizations. For example: "myhost:5601".
+
+*`var.kibana.username`*::
++
+--
+* Value type is <<string,string>>
+* Default value is "elastic"
+--
++
+The username to authenticate to a secured Kibana instance.
+
+*`var.kibana.password`*::
++
+--
+* Value type is <<string,string>>
+* Default value is "changeme"
+--
++
+The password to authenticate to a secure Kibana instance.
+
+*`var.kibana.ssl.enabled`*::
++
+--
+* Value type is <<boolean,boolean>>
+* Default value is false
+--
++
+Enable SSL/TLS secured communication to the Kibana instance.
+
+*`var.kibana.ssl.verification_mode`*::
++
+--
+* Value type is <<string,string>>
+* Default value is "strict"
+--
++
+The hostname verification setting when communicating with Kibana. Set to
+`disable` to turn off hostname verification. Disabling this has serious security
+concerns.
+
+*`var.kibana.ssl.certificate_authority`*::
++
+--
+* Value type is <<string,string>>
+* There is no default value for this setting
+--
++
+The path to an X.509 certificate to use to validate SSL certificates when
+communicating with Kibana.
+
+*`var.kibana.ssl.certificate`*::
++
+--
+* Value type is <<string,string>>
+* There is no default value for this setting
+--
++
+The path to an X.509 certificate to use for client authentication when
+communicating with Kibana.
+
+*`var.kibana.ssl.key`*::
++
+--
+* Value type is <<string,string>>
+* There is no default value for this setting
+--
++
+The path to the certificate key for client authentication when communicating
+with Kibana.
diff --git a/lib/pluginmanager/gemfile.rb b/lib/pluginmanager/gemfile.rb
index 86af9a40826..d0ab77048e5 100644
--- a/lib/pluginmanager/gemfile.rb
+++ b/lib/pluginmanager/gemfile.rb
@@ -16,6 +16,9 @@ def initialize(io)
     end
 
     def load(with_backup = true)
+      # encoding must be set to UTF-8 here to avoid ending up with Windows-1252 encoding on Windows
+      # which will break the DSL instance_eval of that string
+      @io.set_encoding(Encoding::UTF_8)
       @gemset ||= DSL.parse(@io.read)
       backup if with_backup
       self
diff --git a/lib/pluginmanager/utils/downloader.rb b/lib/pluginmanager/utils/downloader.rb
index 0d520febfaa..d2092f22188 100644
--- a/lib/pluginmanager/utils/downloader.rb
+++ b/lib/pluginmanager/utils/downloader.rb
@@ -68,7 +68,7 @@ def fetch(redirect_count = 0)
           downloaded_file.path
         end
       rescue => e
-        downloaded_file.close rescue nil
+        downloaded_file.close unless downloaded_file.closed?
         FileUtils.rm_rf(download_to)
         raise e
       end
diff --git a/logstash-core-plugin-api/logstash-core-plugin-api.gemspec b/logstash-core-plugin-api/logstash-core-plugin-api.gemspec
index 89877be4ac9..82deaf96f75 100644
--- a/logstash-core-plugin-api/logstash-core-plugin-api.gemspec
+++ b/logstash-core-plugin-api/logstash-core-plugin-api.gemspec
@@ -17,7 +17,7 @@ Gem::Specification.new do |gem|
   gem.require_paths = ["lib"]
   gem.version       = LOGSTASH_CORE_PLUGIN_API
 
-  gem.add_runtime_dependency "logstash-core", "6.0.0.beta1"
+  gem.add_runtime_dependency "logstash-core", "6.0.0.rc1"
 
   # Make sure we dont build this gem from a non jruby
   # environment.
diff --git a/logstash-core/lib/logstash-core/version.rb b/logstash-core/lib/logstash-core/version.rb
index da3b637eea1..579e9f2da44 100644
--- a/logstash-core/lib/logstash-core/version.rb
+++ b/logstash-core/lib/logstash-core/version.rb
@@ -5,4 +5,4 @@
 # Note to authors: this should not include dashes because 'gem' barfs if
 # you include a dash in the version string.
 
-LOGSTASH_CORE_VERSION = "6.0.0-beta1"
+LOGSTASH_CORE_VERSION = "6.0.0-rc1"
diff --git a/logstash-core/lib/logstash/agent.rb b/logstash-core/lib/logstash/agent.rb
index 9221dd34afd..7e3dcc88ddd 100644
--- a/logstash-core/lib/logstash/agent.rb
+++ b/logstash-core/lib/logstash/agent.rb
@@ -190,13 +190,6 @@ def shutdown
     converge_result
   end
 
-  def force_shutdown!
-    stop_collecting_metrics
-    stop_webserver
-    transition_to_stopped
-    force_shutdown_pipelines!
-  end
-
   def id
     return @id if @id
 
@@ -344,12 +337,13 @@ def converge_state(pipeline_actions)
 
           unless action_result.successful?
             logger.error("Failed to execute action", :id => action.pipeline_id,
-                        :action_type => action_result.class, :message => action_result.message)
+                        :action_type => action_result.class, :message => action_result.message,
+                        :backtrace => action_result.backtrace)
           end
         rescue SystemExit => e
           converge_result.add(action, e)
         rescue Exception => e
-          logger.error("Failed to execute action", :action => action, :exception => e.class.name, :message => e.message)
+          logger.error("Failed to execute action", :action => action, :exception => e.class.name, :message => e.message, :backtrace => e.backtrace)
           converge_result.add(action, e)
         end
       end
@@ -427,15 +421,6 @@ def collect_metrics?
     @collect_metric
   end
 
-  def force_shutdown_pipelines!
-    with_pipelines do |pipelines|
-      pipelines.each do |_, pipeline|
-        # TODO(ph): should it be his own action?
-        pipeline.force_shutdown!
-      end
-    end
-  end
-
   def shutdown_pipelines
     logger.debug("Shutting down all pipelines", :pipelines_count => pipelines_count)
 
diff --git a/logstash-core/lib/logstash/api/commands/default_metadata.rb b/logstash-core/lib/logstash/api/commands/default_metadata.rb
index 4436adf1350..5bb177f4273 100644
--- a/logstash-core/lib/logstash/api/commands/default_metadata.rb
+++ b/logstash-core/lib/logstash/api/commands/default_metadata.rb
@@ -12,7 +12,7 @@ def all
         end
 
         def host
-          Socket.gethostname
+          @@host ||= Socket.gethostname
         end
 
         def version
diff --git a/logstash-core/lib/logstash/compiler/lscl.rb b/logstash-core/lib/logstash/compiler/lscl.rb
index 7bc3e117604..6bbba9c8ea1 100644
--- a/logstash-core/lib/logstash/compiler/lscl.rb
+++ b/logstash-core/lib/logstash/compiler/lscl.rb
@@ -2,6 +2,7 @@
 require 'logstash/errors'
 require "treetop"
 require "logstash/compiler/treetop_monkeypatches"
+require "logstash/compiler/lscl/helpers"
 require "logstash/config/string_escape"
 
 java_import org.logstash.config.ir.DSL
@@ -10,59 +11,7 @@
 module LogStashCompilerLSCLGrammar; module LogStash; module Compiler; module LSCL; module AST
   PROCESS_ESCAPE_SEQUENCES = :process_escape_sequences
 
-  # Helpers for parsing LSCL files
-  module Helpers
-    def source_meta
-      line, column = line_and_column
-      org.logstash.common.SourceWithMetadata.new(base_protocol, base_id, line, column, self.text_value)
-    end
-
-    def base_source_with_metadata=(value)
-      set_meta(:base_source_with_metadata, value)
-    end
-    
-    def base_source_with_metadata
-      get_meta(:base_source_with_metadata)
-    end
-
-    def base_protocol
-      self.base_source_with_metadata.protocol
-    end
-
-    def base_id
-      self.base_source_with_metadata.id
-    end
-
-    def compose(*statements)
-      compose_for(section_type.to_sym).call(source_meta, *statements)
-    end
-
-    def compose_for(section_sym)
-      if section_sym == :filter
-        jdsl.method(:iComposeSequence)
-      else
-        jdsl.method(:iComposeParallel)
-      end
-    end
-
-    def line_and_column
-      start = self.interval.first
-      [self.input.line_of(start), self.input.column_of(start)]
-    end
-
-    def jdsl
-      org.logstash.config.ir.DSL
-    end
-
-    def self.jdsl
-      org.logstash.config.ir.DSL
-    end
-    
-    AND_METHOD = jdsl.method(:eAnd)
-    OR_METHOD = jdsl.method(:eOr)
-  end
-  
-  class Node < Treetop::Runtime::SyntaxNode
+    class Node < Treetop::Runtime::SyntaxNode
     include Helpers
     
     def section_type
diff --git a/logstash-core/lib/logstash/compiler/lscl/helpers.rb b/logstash-core/lib/logstash/compiler/lscl/helpers.rb
new file mode 100644
index 00000000000..b9f2bf029f6
--- /dev/null
+++ b/logstash-core/lib/logstash/compiler/lscl/helpers.rb
@@ -0,0 +1,55 @@
+# encoding: utf-8
+
+module LogStashCompilerLSCLGrammar; module LogStash; module Compiler; module LSCL; module AST
+  # Helpers for parsing LSCL files
+  module Helpers
+    def source_meta
+      line, column = line_and_column
+      org.logstash.common.SourceWithMetadata.new(base_protocol, base_id, line, column, self.text_value)
+    end
+
+    def base_source_with_metadata=(value)
+      set_meta(:base_source_with_metadata, value)
+    end
+    
+    def base_source_with_metadata
+      get_meta(:base_source_with_metadata)
+    end
+
+    def base_protocol
+      self.base_source_with_metadata ? self.base_source_with_metadata.protocol : 'config_ast'
+    end
+
+    def base_id
+      self.base_source_with_metadata ? self.base_source_with_metadata.id : 'config_ast'
+    end
+
+    def compose(*statements)
+      compose_for(section_type.to_sym).call(source_meta, *statements)
+    end
+
+    def compose_for(section_sym)
+      if section_sym == :filter
+        jdsl.method(:iComposeSequence)
+      else
+        jdsl.method(:iComposeParallel)
+      end
+    end
+
+    def line_and_column
+      start = self.interval.first
+      [self.input.line_of(start), self.input.column_of(start)]
+    end
+
+    def jdsl
+      org.logstash.config.ir.DSL
+    end
+
+    def self.jdsl
+      org.logstash.config.ir.DSL
+    end
+    
+    AND_METHOD = jdsl.method(:eAnd)
+    OR_METHOD = jdsl.method(:eOr)
+  end
+end; end; end; end; end
\ No newline at end of file
diff --git a/logstash-core/lib/logstash/config/config_ast.rb b/logstash-core/lib/logstash/config/config_ast.rb
index 750550074e8..f4cd5d1c4c6 100644
--- a/logstash-core/lib/logstash/config/config_ast.rb
+++ b/logstash-core/lib/logstash/config/config_ast.rb
@@ -1,5 +1,6 @@
 # encoding: utf-8
 require 'logstash/errors'
+require "logstash/compiler/lscl/helpers"
 require "treetop"
 
 require "logstash/compiler/treetop_monkeypatches"
@@ -32,6 +33,8 @@ def self.plugin_instance_index=(val)
   end
 
   class Node < Treetop::Runtime::SyntaxNode
+    include LogStashCompilerLSCLGrammar::LogStash::Compiler::LSCL::AST::Helpers
+
     def text_value_for_comments
       text_value.gsub(/[\r\n]/, " ")
     end
@@ -189,12 +192,12 @@ def compile_initializer
       # If any parent is a Plugin, this must be a codec.
 
       if attributes.elements.nil?
-        return "plugin(#{plugin_type.inspect}, #{plugin_name.inspect})" << (plugin_type == "codec" ? "" : "\n")
+        return "plugin(#{plugin_type.inspect}, #{plugin_name.inspect}, #{source_meta.line}, #{source_meta.column})" << (plugin_type == "codec" ? "" : "\n")
       else
         settings = attributes.recursive_select(Attribute).collect(&:compile).reject(&:empty?)
 
         attributes_code = "LogStash::Util.hash_merge_many(#{settings.map { |c| "{ #{c} }" }.join(", ")})"
-        return "plugin(#{plugin_type.inspect}, #{plugin_name.inspect}, #{attributes_code})" << (plugin_type == "codec" ? "" : "\n")
+        return "plugin(#{plugin_type.inspect}, #{plugin_name.inspect}, #{source_meta.line}, #{source_meta.column}, #{attributes_code})" << (plugin_type == "codec" ? "" : "\n")
       end
     end
 
@@ -211,7 +214,7 @@ def compile
       when "codec"
         settings = attributes.recursive_select(Attribute).collect(&:compile).reject(&:empty?)
         attributes_code = "LogStash::Util.hash_merge_many(#{settings.map { |c| "{ #{c} }" }.join(", ")})"
-        return "plugin(#{plugin_type.inspect}, #{plugin_name.inspect}, #{attributes_code})"
+        return "plugin(#{plugin_type.inspect}, #{plugin_name.inspect}, #{source_meta.line}, #{source_meta.column}, #{attributes_code})"
       end
     end
 
diff --git a/logstash-core/lib/logstash/config/modules_common.rb b/logstash-core/lib/logstash/config/modules_common.rb
index 7415d64a040..2cca7a30963 100644
--- a/logstash-core/lib/logstash/config/modules_common.rb
+++ b/logstash-core/lib/logstash/config/modules_common.rb
@@ -64,9 +64,12 @@ def self.pipeline_configs(settings)
           alt_name = "module-#{module_name}"
           pipeline_id = alt_name
           module_settings.set("pipeline.id", pipeline_id)
+          LogStash::Modules::SettingsMerger.merge_cloud_settings(module_hash, module_settings)
           current_module.with_settings(module_hash)
           config_test = settings.get("config.test_and_exit")
-          if !config_test
+          modul_setup = settings.get("modules_setup")
+          # Only import data if it's not a config test and --setup is true
+          if !config_test && modul_setup
             esclient = LogStash::ElasticsearchClient.build(module_hash)
             kbnclient = LogStash::Modules::KibanaClient.new(module_hash)
             esconnected = esclient.can_connect?
diff --git a/logstash-core/lib/logstash/config/source/base.rb b/logstash-core/lib/logstash/config/source/base.rb
index e19e50ca099..b9a5e553038 100644
--- a/logstash-core/lib/logstash/config/source/base.rb
+++ b/logstash-core/lib/logstash/config/source/base.rb
@@ -41,7 +41,7 @@ def config_string
     end
 
     def config_string?
-      !(config_string.nil? || config_string.empty?)
+      !config_string.nil?
     end
 
     def config_path_setting
diff --git a/logstash-core/lib/logstash/config/source/local.rb b/logstash-core/lib/logstash/config/source/local.rb
index a682f6468a1..897738e12f6 100644
--- a/logstash-core/lib/logstash/config/source/local.rb
+++ b/logstash-core/lib/logstash/config/source/local.rb
@@ -18,8 +18,27 @@ module LogStash module Config module Source
   #
   class Local < Base
     class ConfigStringLoader
+      INPUT_BLOCK_RE = /input *{/
+      OUTPUT_BLOCK_RE = /output *{/
+      EMPTY_RE = /^\s*$/
+
       def self.read(config_string)
-        [org.logstash.common.SourceWithMetadata.new("string", "config_string", 0, 0, config_string)]
+        config_parts = [org.logstash.common.SourceWithMetadata.new("string", "config_string", 0, 0, config_string)]
+
+        # Make sure we have an input and at least 1 output
+        # if its not the case we will add stdin and stdout
+        # this is for backward compatibility reason
+        if !INPUT_BLOCK_RE.match(config_string)
+          config_parts << org.logstash.common.SourceWithMetadata.new(self.class.name, "default input", 0, 0, LogStash::Config::Defaults.input)
+
+        end
+
+        # include a default stdout output if no outputs given
+        if !OUTPUT_BLOCK_RE.match(config_string)
+          config_parts << org.logstash.common.SourceWithMetadata.new(self.class.name, "default output", 0, 0, LogStash::Config::Defaults.output)
+        end
+
+        config_parts
       end
     end
 
@@ -52,8 +71,9 @@ def read
           end
 
           config_string = ::File.read(file)
-
-          if valid_encoding?(config_string)
+          config_string.force_encoding("UTF-8")
+          
+          if config_string.valid_encoding?
             part = org.logstash.common.SourceWithMetadata.new("file", file, 0, 0, config_string)
             config_parts << part
           else
@@ -101,10 +121,6 @@ def get_unmatched_files
         all_files - get_matched_files
       end
 
-      def valid_encoding?(content)
-        content.ascii_only? && content.valid_encoding?
-      end
-
       def temporary_file?(filepath)
         filepath.match(TEMPORARY_FILE_RE)
       end
@@ -139,8 +155,6 @@ def self.read(uri)
 
     PIPELINE_ID = LogStash::SETTINGS.get("pipeline.id").to_sym
     HTTP_RE = /^http(s)?/
-    INPUT_BLOCK_RE = /input *{/
-    OUTPUT_BLOCK_RE = /output *{/
 
     def pipeline_configs
       if config_conflict?
@@ -182,9 +196,7 @@ def local_pipeline_configs
         []
       end
 
-      return if config_parts.empty?
-
-      add_missing_default_inputs_or_outputs(config_parts) if config_string?
+      return [] if config_parts.empty?
 
       [PipelineConfig.new(self.class, @settings.get("pipeline.id").to_sym, config_parts, @settings)]
     end
@@ -193,20 +205,6 @@ def automatic_reload_with_config_string?
       config_reload_automatic? && !config_path? && config_string?
     end
 
-    # Make sure we have an input and at least 1 output
-    # if its not the case we will add stdin and stdout
-    # this is for backward compatibility reason
-    def add_missing_default_inputs_or_outputs(config_parts)
-      if !config_parts.any? { |part| INPUT_BLOCK_RE.match(part.text) }
-        config_parts << org.logstash.common.SourceWithMetadata.new(self.class.name, "default input", 0, 0, LogStash::Config::Defaults.input)
-      end
-
-      # include a default stdout output if no outputs given
-      if !config_parts.any? { |part| OUTPUT_BLOCK_RE.match(part.text) }
-        config_parts << org.logstash.common.SourceWithMetadata.new(self.class.name, "default output", 0, 0, LogStash::Config::Defaults.output)
-      end
-    end
-
     def local_config?
       return false unless config_path?
 
diff --git a/logstash-core/lib/logstash/config/source/multi_local.rb b/logstash-core/lib/logstash/config/source/multi_local.rb
index 19b35c83567..599c48a9b18 100644
--- a/logstash-core/lib/logstash/config/source/multi_local.rb
+++ b/logstash-core/lib/logstash/config/source/multi_local.rb
@@ -19,13 +19,15 @@ def pipeline_configs
         ::LogStash::PipelineSettings.from_settings(@original_settings.clone).merge(pipeline_settings)
       end
       detect_duplicate_pipelines(pipelines_settings)
-      pipelines_settings.map do |pipeline_settings|
+      pipeline_configs = pipelines_settings.map do |pipeline_settings|
         @settings = pipeline_settings
         # this relies on instance variable @settings and the parent class' pipeline_configs
         # method. The alternative is to refactor most of the Local source methods to accept
         # a settings object instead of relying on @settings.
         local_pipeline_configs # create a PipelineConfig object based on @settings
       end.flatten
+      @settings = @original_settings
+      pipeline_configs
     end
 
     def match?
diff --git a/logstash-core/lib/logstash/elasticsearch_client.rb b/logstash-core/lib/logstash/elasticsearch_client.rb
index 7fc9e0f5827..fa11c4df95e 100644
--- a/logstash-core/lib/logstash/elasticsearch_client.rb
+++ b/logstash-core/lib/logstash/elasticsearch_client.rb
@@ -39,8 +39,11 @@ def initialize(settings, logger)
       @client_args[:ssl] = ssl_options
 
       username = @settings["var.elasticsearch.username"]
-      password = @settings["var.elasticsearch.password"]
       if username
+        password = @settings["var.elasticsearch.password"]
+        if password.is_a?(LogStash::Util::Password)
+          password = password.value
+        end
         @client_args[:transport_options] = { :headers => { "Authorization" => 'Basic ' + Base64.encode64( "#{username}:#{password}" ).chomp } }
       end
 
diff --git a/logstash-core/lib/logstash/environment.rb b/logstash-core/lib/logstash/environment.rb
index 46e58ee602c..4df9972e5b5 100644
--- a/logstash-core/lib/logstash/environment.rb
+++ b/logstash-core/lib/logstash/environment.rb
@@ -3,6 +3,9 @@
 require "logstash/java_integration"
 require "logstash/config/cpu_core_strategy"
 require "logstash/settings"
+require "logstash/util/cloud_setting_id"
+require "logstash/util/cloud_setting_auth"
+require "logstash/util/modules_setting_array"
 require "socket"
 require "stud/temporary"
 
@@ -20,8 +23,11 @@ module Environment
     Setting::NullableString.new("path.config", nil, false),
  Setting::WritableDirectory.new("path.data", ::File.join(LogStash::Environment::LOGSTASH_HOME, "data")),
     Setting::NullableString.new("config.string", nil, false),
-                    Setting.new("modules.cli", Array, []),
-                    Setting.new("modules", Array, []),
+           Setting::Modules.new("modules.cli", LogStash::Util::ModulesSettingArray, []),
+           Setting::Modules.new("modules", LogStash::Util::ModulesSettingArray, []),
+           Setting::Modules.new("cloud.id", LogStash::Util::CloudSettingId),
+           Setting::Modules.new("cloud.auth",LogStash::Util::CloudSettingAuth),
+           Setting::Boolean.new("modules_setup", false),
            Setting::Boolean.new("config.test_and_exit", false),
            Setting::Boolean.new("config.reload.automatic", false),
            Setting::TimeValue.new("config.reload.interval", "3s"), # in seconds
diff --git a/logstash-core/lib/logstash/filter_delegator.rb b/logstash-core/lib/logstash/filter_delegator.rb
index fb0569ae67e..907552078bd 100644
--- a/logstash-core/lib/logstash/filter_delegator.rb
+++ b/logstash-core/lib/logstash/filter_delegator.rb
@@ -14,6 +14,8 @@ class FilterDelegator
     ]
     def_delegators :@filter, *DELEGATED_METHODS
 
+    attr_reader :id
+
     def initialize(logger, klass, metric, execution_context, plugin_args)
       @logger = logger
       @klass = klass
@@ -26,6 +28,9 @@ def initialize(logger, klass, metric, execution_context, plugin_args)
       @filter.execution_context = execution_context
 
       @metric_events = namespaced_metric.namespace(:events)
+      @metric_events_in = @metric_events.counter(:in)
+      @metric_events_out = @metric_events.counter(:out)
+      @metric_events_time = @metric_events.counter(:duration_in_millis)
       namespaced_metric.gauge(:name, config_name)
 
       # Not all the filters will do bufferings
@@ -37,19 +42,19 @@ def config_name
     end
 
     def multi_filter(events)
-      @metric_events.increment(:in, events.size)
+      @metric_events_in.increment(events.size)
 
-      clock = @metric_events.time(:duration_in_millis)
+      start_time = java.lang.System.current_time_millis
       new_events = @filter.multi_filter(events)
-      clock.stop
+      @metric_events_time.increment(java.lang.System.current_time_millis - start_time)
 
       # There is no guarantee in the context of filter
       # that EVENTS_INT == EVENTS_OUT, see the aggregates and
       # the split filter
       c = new_events.count { |event| !event.cancelled? }
-      @metric_events.increment(:out, c) if c > 0
 
-      return new_events
+      @metric_events_out.increment(c) if c > 0
+      new_events
     end
 
     private
@@ -61,7 +66,7 @@ def define_flush_method
 
         # Filter plugins that does buffering or spooling of events like the
         # `Logstash-filter-aggregates` can return `NIL` and will flush on the next flush ticks.
-        @metric_events.increment(:out, new_events.size) if new_events && new_events.size > 0
+        @metric_events_out.increment(new_events.size) if new_events && new_events.size > 0
         new_events
       end
     end
diff --git a/logstash-core/lib/logstash/instrument/collector.rb b/logstash-core/lib/logstash/instrument/collector.rb
index 08e72599f3d..4971695a2c9 100644
--- a/logstash-core/lib/logstash/instrument/collector.rb
+++ b/logstash-core/lib/logstash/instrument/collector.rb
@@ -33,11 +33,7 @@ def initialize
     #
     def push(namespaces_path, key, type, *metric_type_params)
       begin
-        metric = @metric_store.fetch_or_store(namespaces_path, key) do
-          LogStash::Instrument::MetricType.create(type, namespaces_path, key)
-        end
-
-        metric.execute(*metric_type_params)
+        get(namespaces_path, key, type).execute(*metric_type_params)
       rescue MetricStore::NamespacesExpectedError => e
         logger.error("Collector: Cannot record metric", :exception => e)
       rescue NameError => e
@@ -51,6 +47,12 @@ def push(namespaces_path, key, type, *metric_type_params)
       end
     end
 
+    def get(namespaces_path, key, type)
+      @metric_store.fetch_or_store(namespaces_path, key) do
+        LogStash::Instrument::MetricType.create(type, namespaces_path, key)
+      end
+    end
+
     # Snapshot the current Metric Store and return it immediately,
     # This is useful if you want to get access to the current metric store without
     # waiting for a periodic call.
diff --git a/logstash-core/lib/logstash/instrument/namespaced_metric.rb b/logstash-core/lib/logstash/instrument/namespaced_metric.rb
index 1f056bd0735..40afa45424a 100644
--- a/logstash-core/lib/logstash/instrument/namespaced_metric.rb
+++ b/logstash-core/lib/logstash/instrument/namespaced_metric.rb
@@ -43,6 +43,10 @@ def time(key, &block)
     def collector
       @metric.collector
     end
+    
+    def counter(key)
+      collector.get(@namespace_name, key, :counter)
+    end
 
     def namespace(name)
       NamespacedMetric.new(metric, namespace_name + Array(name))
diff --git a/logstash-core/lib/logstash/instrument/namespaced_null_metric.rb b/logstash-core/lib/logstash/instrument/namespaced_null_metric.rb
index c4e8e762c23..1a3b6f9c1d1 100644
--- a/logstash-core/lib/logstash/instrument/namespaced_null_metric.rb
+++ b/logstash-core/lib/logstash/instrument/namespaced_null_metric.rb
@@ -44,6 +44,10 @@ def collector
       @metric.collector
     end
 
+    def counter(_)
+      ::LogStash::Instrument::NullMetric::NullGauge
+    end
+
     def namespace(name)
       NamespacedNullMetric.new(metric, namespace_name + Array(name))
     end
diff --git a/logstash-core/lib/logstash/instrument/null_metric.rb b/logstash-core/lib/logstash/instrument/null_metric.rb
index 56bd0feac19..f56028d4580 100644
--- a/logstash-core/lib/logstash/instrument/null_metric.rb
+++ b/logstash-core/lib/logstash/instrument/null_metric.rb
@@ -39,6 +39,10 @@ def time(namespace, key)
       end
     end
 
+    def counter(_)
+      NullGauge
+    end
+
     def namespace(name)
       raise MetricNoNamespaceProvided if name.nil? || name.empty?
       NamespacedNullMetric.new(self, name)
@@ -49,6 +53,12 @@ def self.validate_key!(key)
     end
 
     private
+
+    class NullGauge
+      def self.increment(_)
+      end
+    end
+
     # Null implementation of the internal timer class
     #
     # @see LogStash::Instrument::TimedExecution`
diff --git a/logstash-core/lib/logstash/instrument/wrapped_write_client.rb b/logstash-core/lib/logstash/instrument/wrapped_write_client.rb
index 5da275c9f29..82f0539e287 100644
--- a/logstash-core/lib/logstash/instrument/wrapped_write_client.rb
+++ b/logstash-core/lib/logstash/instrument/wrapped_write_client.rb
@@ -10,7 +10,12 @@ def initialize(write_client, pipeline, metric, plugin)
       @events_metrics = metric.namespace([:stats, :events])
       @pipeline_metrics = metric.namespace([:stats, :pipelines, pipeline_id, :events])
       @plugin_events_metrics = metric.namespace([:stats, :pipelines, pipeline_id, :plugins, plugin_type, plugin.id.to_sym, :events])
-
+      @events_metrics_counter = @events_metrics.counter(:in)
+      @events_metrics_time = @events_metrics.counter(:queue_push_duration_in_millis)
+      @pipeline_metrics_counter = @pipeline_metrics.counter(:in)
+      @pipeline_metrics_time = @pipeline_metrics.counter(:queue_push_duration_in_millis)
+      @plugin_events_metrics_counter = @plugin_events_metrics.counter(:out)
+      @plugin_events_metrics_time = @plugin_events_metrics.counter(:queue_push_duration_in_millis)
       define_initial_metrics_values
     end
 
@@ -19,41 +24,45 @@ def get_new_batch
     end
 
     def push(event)
-      record_metric { @write_client.push(event) }
+      increment_counters(1)
+      start_time = java.lang.System.current_time_millis
+      result = @write_client.push(event)
+      report_execution_time(start_time)
+      result
     end
+
     alias_method(:<<, :push)
 
     def push_batch(batch)
-      record_metric(batch.size) { @write_client.push_batch(batch) }
+      increment_counters(batch.size)
+      start_time = java.lang.System.current_time_millis
+      result = @write_client.push_batch(batch)
+      report_execution_time(start_time)
+      result
     end
 
     private
-    def record_metric(size = 1)
-      @events_metrics.increment(:in, size)
-      @pipeline_metrics.increment(:in, size)
-      @plugin_events_metrics.increment(:out, size)
-
-      clock = @events_metrics.time(:queue_push_duration_in_millis)
 
-      result = yield
-
-      # Reuse the same values for all the endpoints to make sure we don't have skew in times.
-      execution_time = clock.stop
-
-      @pipeline_metrics.report_time(:queue_push_duration_in_millis, execution_time)
-      @plugin_events_metrics.report_time(:queue_push_duration_in_millis, execution_time)
+    def increment_counters(size)
+      @events_metrics_counter.increment(size)
+      @pipeline_metrics_counter.increment(size)
+      @plugin_events_metrics_counter.increment(size)
+    end
 
-      result
+    def report_execution_time(start_time)
+      execution_time = java.lang.System.current_time_millis - start_time
+      @events_metrics_time.increment(execution_time)
+      @pipeline_metrics_time.increment(execution_time)
+      @plugin_events_metrics_time.increment(execution_time)
     end
 
     def define_initial_metrics_values
-      @events_metrics.increment(:in, 0)
-      @pipeline_metrics.increment(:in, 0)
-      @plugin_events_metrics.increment(:out, 0)
-
-      @events_metrics.report_time(:queue_push_duration_in_millis, 0)
-      @pipeline_metrics.report_time(:queue_push_duration_in_millis, 0)
-      @plugin_events_metrics.report_time(:queue_push_duration_in_millis, 0)
+      @events_metrics_counter.increment(0)
+      @pipeline_metrics_counter.increment(0)
+      @plugin_events_metrics_counter.increment(0)
+      @events_metrics_time.increment(0)
+      @pipeline_metrics_time.increment(0)
+      @plugin_events_metrics_time.increment(0)
     end
   end
 end end
diff --git a/logstash-core/lib/logstash/logging/logger.rb b/logstash-core/lib/logstash/logging/logger.rb
index 5af0886bf84..cec53bfbc1b 100644
--- a/logstash-core/lib/logstash/logging/logger.rb
+++ b/logstash-core/lib/logstash/logging/logger.rb
@@ -73,7 +73,7 @@ def self.configure_logging(level, path = LogManager::ROOT_LOGGER_NAME)
         raise ArgumentError, "invalid level[#{level}] for logger[#{path}]"
       end
 
-      def self.initialize(config_location)
+      def self.reconfigure(config_location)
         @@config_mutex.synchronize do
           config_location_uri = URI.create(config_location)
           file_path = config_location_uri.path
@@ -92,6 +92,9 @@ def self.initialize(config_location)
         end
       end
 
+      # until dev_utils/rspec/spec_helper is changed, we need to have both methods
+      singleton_class.send(:alias_method, :initialize, :reconfigure)
+
       def self.get_logging_context
         return  LoggerContext.getContext(false)
       end
diff --git a/logstash-core/lib/logstash/modules/kibana_client.rb b/logstash-core/lib/logstash/modules/kibana_client.rb
index cf806899ef7..6d15fdfa88a 100644
--- a/logstash-core/lib/logstash/modules/kibana_client.rb
+++ b/logstash-core/lib/logstash/modules/kibana_client.rb
@@ -50,12 +50,14 @@ def initialize(settings)
 
     @client = Manticore::Client.new(client_options)
     @host = @settings.fetch("var.kibana.host", "localhost:5601")
-    username = @settings["var.kibana.username"]
-    password = @settings["var.kibana.password"]
-
     @scheme = @settings.fetch("var.kibana.scheme", "http")
     @http_options = {:headers => {'Content-Type' => 'application/json'}}
+    username = @settings["var.kibana.username"]
     if username
+      password = @settings["var.kibana.password"]
+      if password.is_a?(LogStash::Util::Password)
+        password = password.value
+      end
       @http_options[:headers]['Authorization'] = 'Basic ' + Base64.encode64( "#{username}:#{password}" ).chomp
     end
 
diff --git a/logstash-core/lib/logstash/modules/kibana_config.rb b/logstash-core/lib/logstash/modules/kibana_config.rb
index b3ec2ffe40e..03d4ea72a4b 100644
--- a/logstash-core/lib/logstash/modules/kibana_config.rb
+++ b/logstash-core/lib/logstash/modules/kibana_config.rb
@@ -11,7 +11,6 @@ module LogStash module Modules class KibanaConfig
   include LogStash::Util::Loggable
 
   ALLOWED_DIRECTORIES = ["search", "visualization"]
-  METRICS_MAX_BUCKETS = (24 * 60 * 60).freeze # 24 hours of events/sec buckets.
   attr_reader :index_name # not used when importing via kibana but for BWC with ElastsearchConfig
 
   # We name it `modul` here because `module` has meaning in Ruby.
@@ -21,10 +20,8 @@ def initialize(modul, settings)
     @settings = settings
     @index_name = "kibana"
     @pattern_name = "#{@name}-*"
-    @metrics_max_buckets = @settings.fetch("dashboards.metrics_max_buckets", METRICS_MAX_BUCKETS).to_i
     @kibana_settings = [
-      KibanaSettings::Setting.new("defaultIndex", @pattern_name),
-      KibanaSettings::Setting.new("metrics:max_buckets", @metrics_max_buckets)
+      KibanaSettings::Setting.new("defaultIndex", @pattern_name)
     ]
   end
 
diff --git a/logstash-core/lib/logstash/modules/logstash_config.rb b/logstash-core/lib/logstash/modules/logstash_config.rb
index 6b6d838e817..fd238bf3401 100644
--- a/logstash-core/lib/logstash/modules/logstash_config.rb
+++ b/logstash-core/lib/logstash/modules/logstash_config.rb
@@ -69,7 +69,7 @@ def elasticsearch_output_config(type_string = nil)
     password = @settings["var.elasticsearch.password"]
     lines = ["hosts => #{hosts}", "index => \"#{index}\""]
     lines.push(user ? "user => \"#{user}\"" : nil)
-    lines.push(password ? "password => \"#{password}\"" : nil)
+    lines.push(password ? "password => \"#{password.value}\"" : nil)
     lines.push(type_string ? "document_type => #{type_string}" : nil)
     lines.push("ssl => #{@settings.fetch('var.elasticsearch.ssl.enabled', false)}")
     if cacert = @settings["var.elasticsearch.ssl.certificate_authority"]
diff --git a/logstash-core/lib/logstash/modules/scaffold.rb b/logstash-core/lib/logstash/modules/scaffold.rb
index 5572b1b372d..8ddccafc8ee 100644
--- a/logstash-core/lib/logstash/modules/scaffold.rb
+++ b/logstash-core/lib/logstash/modules/scaffold.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/namespace"
 require "logstash/logging"
+require "logstash/util/loggable"
 require "erb"
 
 require_relative "elasticsearch_config"
@@ -17,6 +18,7 @@ def initialize(name, directory)
     @module_name = name
     @directory = directory  # this is the 'configuration folder in the GEM root.'
     @kibana_version_parts = "6.0.0".split('.') # this is backup in case kibana client fails to connect
+    logger.info("Initializing module", :module_name => name, :directory => directory)
   end
 
   def add_kibana_version(version_parts)
diff --git a/logstash-core/lib/logstash/modules/settings_merger.rb b/logstash-core/lib/logstash/modules/settings_merger.rb
index 5a852735de6..c78f992ae37 100644
--- a/logstash-core/lib/logstash/modules/settings_merger.rb
+++ b/logstash-core/lib/logstash/modules/settings_merger.rb
@@ -1,8 +1,13 @@
 # encoding: utf-8
 require "logstash/namespace"
+require "logstash/util"
+require "logstash/util/loggable"
 
-module LogStash module Modules class SettingsMerger
-  def self.merge(cli_settings, yml_settings)
+module LogStash module Modules module SettingsMerger
+  include LogStash::Util::Loggable
+  extend self
+
+  def merge(cli_settings, yml_settings)
     # both args are arrays of hashes, e.g.
     # [{"name"=>"mod1", "var.input.tcp.port"=>"3333"}, {"name"=>"mod2"}]
     # [{"name"=>"mod1", "var.input.tcp.port"=>2222, "var.kibana.username"=>"rupert", "var.kibana.password"=>"fotherington"}, {"name"=>"mod3", "var.input.tcp.port"=>4445}]
@@ -11,13 +16,56 @@ def self.merge(cli_settings, yml_settings)
     # union will also coalesce identical hashes
     union_of_settings = (cli_settings | yml_settings)
     grouped_by_name = union_of_settings.group_by{|e| e["name"]}
-    grouped_by_name.each do |name, array|
+    grouped_by_name.each do |_, array|
       if array.size == 2
-        merged << array.first.merge(array.last)
+        merged << array.last.merge(array.first)
       else
         merged.concat(array)
       end
     end
     merged
   end
+
+  def merge_cloud_settings(module_settings, logstash_settings)
+    cloud_id = logstash_settings.get("cloud.id")
+    cloud_auth = logstash_settings.get("cloud.auth")
+    if cloud_id.nil?
+      if cloud_auth.nil?
+        return # user did not specify cloud settings
+      else
+        raise ArgumentError.new("Cloud Auth without Cloud Id")
+      end
+    end
+    if logger.debug?
+      settings_copy = LogStash::Util.deep_clone(module_settings)
+    end
+
+    module_settings["var.kibana.scheme"] = "https"
+    module_settings["var.kibana.host"] = cloud_id.kibana_host
+    module_settings["var.elasticsearch.hosts"] = cloud_id.elasticsearch_host
+    unless cloud_auth.nil?
+      module_settings["var.elasticsearch.username"] = cloud_auth.username
+      module_settings["var.elasticsearch.password"] = cloud_auth.password
+      module_settings["var.kibana.username"] = cloud_auth.username
+      module_settings["var.kibana.password"] = cloud_auth.password
+    end
+    if logger.debug?
+      format_module_settings(settings_copy, module_settings).each {|line| logger.debug(line)}
+    end
+  end
+
+  def format_module_settings(settings_before, settings_after)
+    output = []
+    output << "-------- Module Settings ---------"
+    settings_after.each do |setting_name, setting|
+      setting_before = settings_before.fetch(setting_name, "")
+      line = "#{setting_name}: '#{setting}'"
+      if setting_before != setting
+        line.concat(", was: '#{setting_before}'")
+      end
+      output << line
+    end
+    output << "-------- Module Settings ---------"
+    output
+  end
 end end end
diff --git a/logstash-core/lib/logstash/output_delegator.rb b/logstash-core/lib/logstash/output_delegator.rb
index fa34187c227..dba5fbd013a 100644
--- a/logstash-core/lib/logstash/output_delegator.rb
+++ b/logstash-core/lib/logstash/output_delegator.rb
@@ -19,7 +19,9 @@ def initialize(logger, output_class, metric, execution_context, strategy_registr
     @namespaced_metric = metric.namespace(id.to_sym)
     @namespaced_metric.gauge(:name, config_name)
     @metric_events = @namespaced_metric.namespace(:events)
-
+    @in_counter = @metric_events.counter(:in)
+    @out_counter = @metric_events.counter(:out)
+    @time_metric = @metric_events.counter(:duration_in_millis)
     @strategy = strategy_registry.
                   class_for(self.concurrency).
                   new(@logger, @output_class, @namespaced_metric, execution_context, plugin_args)
@@ -42,11 +44,11 @@ def register
   end
 
   def multi_receive(events)
-    @metric_events.increment(:in, events.length)
-    clock = @metric_events.time(:duration_in_millis)
+    @in_counter.increment(events.length)
+    start_time = java.lang.System.current_time_millis
     @strategy.multi_receive(events)
-    clock.stop
-    @metric_events.increment(:out, events.length)
+    @time_metric.increment(java.lang.System.current_time_millis - start_time)
+    @out_counter.increment(events.length)
   end
 
   def do_close
diff --git a/logstash-core/lib/logstash/pipeline.rb b/logstash-core/lib/logstash/pipeline.rb
index 72f81894692..60619f385ec 100644
--- a/logstash-core/lib/logstash/pipeline.rb
+++ b/logstash-core/lib/logstash/pipeline.rb
@@ -38,7 +38,7 @@ module LogStash; class BasePipeline
 
   def initialize(pipeline_config, namespaced_metric = nil, agent = nil)
     @logger = self.logger
-
+    @mutex = Mutex.new
     @ephemeral_id = SecureRandom.uuid
 
     @pipeline_config = pipeline_config
@@ -93,6 +93,13 @@ def dlq_writer
     end
   end
 
+  def close_dlq_writer
+    @dlq_writer.close
+    if settings.get_value("dead_letter_queue.enable")
+      DeadLetterQueueFactory.release(pipeline_id)
+    end
+  end
+
   def compile_lir
     sources_with_metadata = [
       SourceWithMetadata.new("str", "pipeline", 0, 0, self.config_str)
@@ -100,16 +107,27 @@ def compile_lir
     LogStash::Compiler.compile_sources(sources_with_metadata, @settings)
   end
 
-  def plugin(plugin_type, name, *args)
+  def plugin(plugin_type, name, line, column, *args)
     @plugin_counter += 1
 
     # Collapse the array of arguments into a single merged hash
     args = args.reduce({}, &:merge)
 
-    id = if args["id"].nil? || args["id"].empty?
-      args["id"] = "#{@config_hash}-#{@plugin_counter}"
+    if plugin_type == "codec"
+      id = SecureRandom.uuid # codecs don't really use their IDs for metrics, so we can use anything here
     else
-      args["id"]
+      # Pull the ID from LIR to keep IDs consistent between the two representations
+      id = lir.graph.vertices.filter do |v| 
+        v.source_with_metadata && 
+        v.source_with_metadata.line == line && 
+        v.source_with_metadata.column == column
+      end.findFirst.get.id
+    end
+
+    args["id"] = id # some code pulls the id out of the args
+
+    if !id
+      raise ConfigurationError, "Could not determine ID for #{plugin_type}/#{plugin_name}"
     end
 
     raise ConfigurationError, "Two plugins have the id '#{id}', please fix this conflict" if @plugins_by_id[id]
@@ -224,6 +242,7 @@ def initialize(pipeline_config, namespaced_metric = nil, agent = nil)
     @running = Concurrent::AtomicBoolean.new(false)
     @flushing = Concurrent::AtomicReference.new(false)
     @force_shutdown = Concurrent::AtomicBoolean.new(false)
+    @outputs_registered = Concurrent::AtomicBoolean.new(false)
   end # def initialize
 
   def ready?
@@ -339,7 +358,7 @@ def run
   def close
     @filter_queue_client.close
     @queue.close
-    @dlq_writer.close
+    close_dlq_writer
   end
 
   def transition_to_running
@@ -385,9 +404,9 @@ def register_plugins(plugins)
 
   def start_workers
     @worker_threads.clear # In case we're restarting the pipeline
+    @outputs_registered.make_false
     begin
-      register_plugins(@outputs)
-      register_plugins(@filters)
+      maybe_setup_out_plugins
 
       pipeline_workers = safe_pipeline_worker_count
       batch_size = @settings.get("pipeline.batch.size")
@@ -453,16 +472,17 @@ def worker_loop(batch_size, batch_delay)
       shutdown_requested |= signal.shutdown? # latch on shutdown signal
 
       batch = @filter_queue_client.read_batch # metrics are started in read_batch
-      if (batch.size > 0)
+      if batch.size > 0
         @events_consumed.increment(batch.size)
         filter_batch(batch)
-        flush_filters_to_batch(batch, :final => false) if signal.flush?
+      end
+      flush_filters_to_batch(batch, :final => false) if signal.flush?
+      if batch.size > 0
         output_batch(batch)
         unless @force_shutdown.true? # ack the current batch
           @filter_queue_client.close_batch(batch)
         end
       end
-
       # keep break at end of loop, after the read_batch operation, some pipeline specs rely on this "final read_batch" before shutdown.
       break if (shutdown_requested && !draining_queue?) || @force_shutdown.true?
     end
@@ -564,17 +584,12 @@ def inputworker(plugin)
       end
 
       # otherwise, report error and restart
-      if @logger.debug?
-        @logger.error(I18n.t("logstash.pipeline.worker-error-debug",
-                             default_logging_keys(
-                               :plugin => plugin.inspect,
-                               :error => e.message,
-                               :exception => e.class,
-                               :stacktrace => e.backtrace.join("\n"))))
-      else
-        @logger.error(I18n.t("logstash.pipeline.worker-error",
-                             default_logging_keys(:plugin => plugin.inspect, :error => e.message)))
-      end
+      @logger.error(I18n.t("logstash.pipeline.worker-error-debug",
+                            default_logging_keys(
+                              :plugin => plugin.inspect,
+                              :error => e.message,
+                              :exception => e.class,
+                              :stacktrace => e.backtrace.join("\n"))))
 
       # Assuming the failure that caused this exception is transient,
       # let's sleep for a bit and execute #run again
@@ -645,11 +660,11 @@ def shutdown_workers
   # for backward compatibility in devutils for the rspec helpers, this method is not used
   # in the pipeline anymore.
   def filter(event, &block)
+    maybe_setup_out_plugins
     # filter_func returns all filtered events, including cancelled ones
-    filter_func(event).each { |e| block.call(e) }
+    filter_func(event).each {|e| block.call(e)}
   end
 
-
   # perform filters flush and yield flushed event to the passed block
   # @param options [Hash]
   # @option options [Boolean] :final => true to signal a final shutdown flush
@@ -784,9 +799,16 @@ def inspect
 
   private
 
+  def maybe_setup_out_plugins
+    if @outputs_registered.make_true
+      register_plugins(@outputs)
+      register_plugins(@filters)
+    end
+  end
+
   def default_logging_keys(other_keys = {})
     keys = super
-    keys[:thread] = thread.inspect if thread
+    keys[:thread] ||= thread.inspect if thread
     keys
   end
 
@@ -795,6 +817,9 @@ def draining_queue?
   end
 
   def wrapped_write_client(plugin)
-    LogStash::Instrument::WrappedWriteClient.new(@input_queue_client, self, metric, plugin)
+    #need to ensure that metrics are initialized one plugin at a time, else a race condition can exist.
+    @mutex.synchronize do
+      LogStash::Instrument::WrappedWriteClient.new(@input_queue_client, self, metric, plugin)
+    end
   end
 end; end
diff --git a/logstash-core/lib/logstash/pipeline_action/stop.rb b/logstash-core/lib/logstash/pipeline_action/stop.rb
index 79298b7e47f..7ee45f76af1 100644
--- a/logstash-core/lib/logstash/pipeline_action/stop.rb
+++ b/logstash-core/lib/logstash/pipeline_action/stop.rb
@@ -14,6 +14,7 @@ def initialize(pipeline_id)
     def execute(agent, pipelines)
       pipeline = pipelines[pipeline_id]
       pipeline.shutdown { LogStash::ShutdownWatcher.start(pipeline) }
+      pipeline.thread.join
       pipelines.delete(pipeline_id)
       # If we reach this part of the code we have succeeded because
       # the shutdown call will block.
diff --git a/logstash-core/lib/logstash/pipeline_settings.rb b/logstash-core/lib/logstash/pipeline_settings.rb
index 1d595605141..7984b5481db 100644
--- a/logstash-core/lib/logstash/pipeline_settings.rb
+++ b/logstash-core/lib/logstash/pipeline_settings.rb
@@ -12,8 +12,10 @@ class PipelineSettings < Settings
       "config.reload.interval",
       "config.string",
       "dead_letter_queue.enable",
+      "dead_letter_queue.max_bytes",
       "metric.collect",
       "path.config",
+      "path.dead_letter_queue",
       "path.queue",
       "pipeline.batch.delay",
       "pipeline.batch.size",
diff --git a/logstash-core/lib/logstash/runner.rb b/logstash-core/lib/logstash/runner.rb
index 835723cdd6d..49fc9b4e50c 100644
--- a/logstash-core/lib/logstash/runner.rb
+++ b/logstash-core/lib/logstash/runner.rb
@@ -74,6 +74,19 @@ class LogStash::Runner < Clamp::StrictCommand
     :multivalued => true,
     :attribute_name => "modules_variable_list"
 
+  option ["--setup"], :flag,
+    I18n.t("logstash.runner.flag.modules_setup"),
+    :default => LogStash::SETTINGS.get_default("modules_setup"),
+    :attribute_name => "modules_setup"
+
+  option ["--cloud.id"], "CLOUD_ID",
+    I18n.t("logstash.runner.flag.cloud_id"),
+    :attribute_name => "cloud.id"
+
+  option ["--cloud.auth"], "CLOUD_AUTH",
+    I18n.t("logstash.runner.flag.cloud_auth"),
+    :attribute_name => "cloud.auth"
+
   # Pipeline settings
   option ["-w", "--pipeline.workers"], "COUNT",
     I18n.t("logstash.runner.flag.pipeline-workers"),
@@ -236,7 +249,7 @@ def execute
     java.lang.System.setProperty("ls.log.level", setting("log.level"))
     unless java.lang.System.getProperty("log4j.configurationFile")
       log4j_config_location = ::File.join(setting("path.settings"), "log4j2.properties")
-      LogStash::Logging::Logger::initialize("file:///" + log4j_config_location)
+      LogStash::Logging::Logger::reconfigure("file:///" + log4j_config_location)
     end
     # override log level that may have been introduced from a custom log4j config file
     LogStash::Logging::Logger::configure_logging(setting("log.level"))
@@ -468,8 +481,7 @@ def trap_sigint
     Stud::trap("INT") do
       if @interrupted_once
         logger.fatal(I18n.t("logstash.agent.forced_sigint"))
-        @agent.force_shutdown!
-        exit
+        exit(1)
       else
         logger.warn(I18n.t("logstash.agent.sigint"))
         Thread.new(logger) {|lg| sleep 5; lg.warn(I18n.t("logstash.agent.slow_shutdown")) }
diff --git a/logstash-core/lib/logstash/settings.rb b/logstash-core/lib/logstash/settings.rb
index cc9ef23bf43..cff527e9bf1 100644
--- a/logstash-core/lib/logstash/settings.rb
+++ b/logstash-core/lib/logstash/settings.rb
@@ -255,6 +255,7 @@ def initialize(name, klass, default=nil, strict=true, &validator_proc)
           @default = default
         end
       end
+
       def set(value)
         coerced_value = coerce(value)
         validate(coerced_value)
@@ -557,7 +558,32 @@ def coerce(value)
         end
       end
     end
+
+    class Modules < Coercible
+      def initialize(name, klass, default = nil)
+        super(name, klass, default, false)
+      end
+
+      def set(value)
+        @value = coerce(value)
+        @value_is_set = true
+        @value
+      end
+
+      def coerce(value)
+        if value.is_a?(@klass)
+          return value
+        end
+        @klass.new(value)
+      end
+
+      protected
+      def validate(value)
+        coerce(value)
+      end
+    end
   end
 
+
   SETTINGS = Settings.new
 end
diff --git a/logstash-core/lib/logstash/util/cloud_setting_auth.rb b/logstash-core/lib/logstash/util/cloud_setting_auth.rb
new file mode 100644
index 00000000000..7a5d4f16066
--- /dev/null
+++ b/logstash-core/lib/logstash/util/cloud_setting_auth.rb
@@ -0,0 +1,29 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/util/password"
+
+module LogStash module Util class CloudSettingAuth
+  attr_reader :original, :username, :password
+
+  def initialize(value)
+    return if value.nil?
+
+    unless value.is_a?(String)
+      raise ArgumentError.new("Cloud Auth must be String. Received: #{value.class}")
+    end
+    @original = value
+    @username, sep, password = @original.partition(":")
+    if @username.empty? || sep.empty? || password.empty?
+      raise ArgumentError.new("Cloud Auth username and password format should be \"<username>:<password>\".")
+    end
+    @password = LogStash::Util::Password.new(password)
+  end
+
+  def to_s
+    "#{@username}:#{@password}"
+  end
+
+  def inspect
+    to_s
+  end
+end end end
diff --git a/logstash-core/lib/logstash/util/cloud_setting_id.rb b/logstash-core/lib/logstash/util/cloud_setting_id.rb
new file mode 100644
index 00000000000..9f50bc93ca4
--- /dev/null
+++ b/logstash-core/lib/logstash/util/cloud_setting_id.rb
@@ -0,0 +1,41 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "base64"
+
+module LogStash module Util class CloudSettingId
+  attr_reader :original, :decoded, :label, :elasticsearch_host, :kibana_host
+
+  def initialize(value)
+    return if value.nil?
+
+    unless value.is_a?(String)
+      raise ArgumentError.new("Cloud Id must be String. Received: #{value.class}")
+    end
+    @original = value
+    @label, sep, last = value.partition(":")
+    if last.empty?
+      @decoded = Base64.urlsafe_decode64(@label) rescue ""
+      @label = ""
+    else
+      @decoded = Base64.urlsafe_decode64(last) rescue ""
+    end
+    unless @decoded.count("$") == 2
+      raise ArgumentError.new("Cloud Id does not decode. Received: \"#{@original}\".")
+    end
+    parts = @decoded.split("$")
+    if parts.any?(&:empty?)
+      raise ArgumentError.new("Cloud Id, after decoding, is invalid. Format: '<part1>$<part2>$<part3>'. Received: \"#{@decoded}\".")
+    end
+    cloud_host, es_server, kb_server = parts
+    @elasticsearch_host = sprintf("%s.%s:443", es_server, cloud_host)
+    @kibana_host  = sprintf("%s.%s:443", kb_server, cloud_host)
+  end
+
+  def to_s
+    @original.to_s
+  end
+
+  def inspect
+    to_s
+  end
+end end end
\ No newline at end of file
diff --git a/logstash-core/lib/logstash/util/modules_setting_array.rb b/logstash-core/lib/logstash/util/modules_setting_array.rb
new file mode 100644
index 00000000000..ec895968a06
--- /dev/null
+++ b/logstash-core/lib/logstash/util/modules_setting_array.rb
@@ -0,0 +1,28 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/util/password"
+
+module LogStash module Util class ModulesSettingArray
+  extend Forwardable
+  DELEGATED_METHODS = [].public_methods.reject{|symbol| symbol.to_s.end_with?('__')}
+
+  def_delegators :@original, *DELEGATED_METHODS
+
+  attr_reader :original
+  def initialize(value)
+    unless value.is_a?(Array)
+      raise ArgumentError.new("Module Settings must be an Array. Received: #{value.class}")
+    end
+    @original = value
+    # wrap passwords
+    @original.each do |hash|
+      hash.keys.select{|key| key.to_s.end_with?('password')}.each do |key|
+        hash[key] = LogStash::Util::Password.new(hash[key])
+      end
+    end
+  end
+
+  def __class__
+    LogStash::Util::ModulesSettingArray
+  end
+end end end
\ No newline at end of file
diff --git a/logstash-core/lib/logstash/util/password.rb b/logstash-core/lib/logstash/util/password.rb
index 6cd9beb7593..355da471cc5 100644
--- a/logstash-core/lib/logstash/util/password.rb
+++ b/logstash-core/lib/logstash/util/password.rb
@@ -1,10 +1,9 @@
 # encoding: utf-8
 require "logstash/namespace"
-require "logstash/util"
 
 # This class exists to quietly wrap a password string so that, when printed or
 # logged, you don't accidentally print the password itself.
-class LogStash::Util::Password
+module LogStash module Util class Password
   attr_reader :value
 
   public
@@ -21,5 +20,4 @@ def to_s
   def inspect
     return to_s
   end # def inspect
-end # class LogStash::Util::Password
-
+end end end # class LogStash::Util::Password
diff --git a/logstash-core/lib/logstash/util/wrapped_acked_queue.rb b/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
index b9892df3ae5..f3ae8647e09 100644
--- a/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
+++ b/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
@@ -205,19 +205,18 @@ def close_batch(batch)
       end
 
       def start_clock
-        @inflight_clocks[Thread.current] = [
-          @event_metric.time(:duration_in_millis),
-          @pipeline_metric.time(:duration_in_millis)
-        ]
+        @inflight_clocks[Thread.current] = java.lang.System.current_time_millis
       end
 
       def stop_clock(batch)
         unless @inflight_clocks[Thread.current].nil?
           if batch.size > 0
-            # onl/y stop (which also records) the metrics if the batch is non-empty.
+            # only stop (which also records) the metrics if the batch is non-empty.
             # start_clock is now called at empty batch creation and an empty batch could
             # stay empty all the way down to the close_batch call.
-            @inflight_clocks[Thread.current].each(&:stop)
+            time_taken = java.lang.System.current_time_millis - @inflight_clocks[Thread.current]
+            @event_metric.report_time(:duration_in_millis, time_taken)
+            @pipeline_metric.report_time(:duration_in_millis, time_taken)
           end
           @inflight_clocks.delete(Thread.current)
         end
diff --git a/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
index 78f530760f5..a2bf8b16da4 100644
--- a/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
+++ b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
@@ -68,11 +68,17 @@ def set_batch_dimensions(batch_size, wait_for)
 
       def set_events_metric(metric)
         @event_metric = metric
+        @event_metric_out = @event_metric.counter(:out)
+        @event_metric_filtered = @event_metric.counter(:filtered)
+        @event_metric_time = @event_metric.counter(:duration_in_millis)
         define_initial_metrics_values(@event_metric)
       end
 
       def set_pipeline_metric(metric)
         @pipeline_metric = metric
+        @pipeline_metric_out = @pipeline_metric.counter(:out)
+        @pipeline_metric_filtered = @pipeline_metric.counter(:filtered)
+        @pipeline_metric_time = @pipeline_metric.counter(:duration_in_millis)
         define_initial_metrics_values(@pipeline_metric)
       end
 
@@ -140,10 +146,7 @@ def close_batch(batch)
       end
 
       def start_clock
-        @inflight_clocks[Thread.current] = [
-          @event_metric.time(:duration_in_millis),
-          @pipeline_metric.time(:duration_in_millis)
-        ]
+        @inflight_clocks[Thread.current] = java.lang.System.current_time_millis
       end
 
       def stop_clock(batch)
@@ -152,20 +155,22 @@ def stop_clock(batch)
             # only stop (which also records) the metrics if the batch is non-empty.
             # start_clock is now called at empty batch creation and an empty batch could
             # stay empty all the way down to the close_batch call.
-            @inflight_clocks[Thread.current].each(&:stop)
+            time_taken = java.lang.System.current_time_millis - @inflight_clocks[Thread.current]
+            @event_metric_time.increment(time_taken)
+            @pipeline_metric_time.increment(time_taken)
           end
           @inflight_clocks.delete(Thread.current)
         end
       end
 
       def add_filtered_metrics(batch)
-        @event_metric.increment(:filtered, batch.filtered_size)
-        @pipeline_metric.increment(:filtered, batch.filtered_size)
+        @event_metric_filtered.increment(batch.filtered_size)
+        @pipeline_metric_filtered.increment(batch.filtered_size)
       end
 
       def add_output_metrics(batch)
-        @event_metric.increment(:out, batch.filtered_size)
-        @pipeline_metric.increment(:out, batch.filtered_size)
+        @event_metric_out.increment(batch.filtered_size)
+        @pipeline_metric_out.increment(batch.filtered_size)
       end
     end
 
diff --git a/logstash-core/lib/logstash/version.rb b/logstash-core/lib/logstash/version.rb
index 7e808e34b96..ce8c2084d4f 100644
--- a/logstash-core/lib/logstash/version.rb
+++ b/logstash-core/lib/logstash/version.rb
@@ -11,4 +11,4 @@
 #       eventually this file should be in the root logstash lib fir and dependencies in logstash-core should be
 #       fixed.
 
-LOGSTASH_VERSION = "6.0.0-beta1"
+LOGSTASH_VERSION = "6.0.0-rc1"
diff --git a/logstash-core/locales/en.yml b/logstash-core/locales/en.yml
index 1f2a214fa09..4f48c006a2b 100644
--- a/logstash-core/locales/en.yml
+++ b/logstash-core/locales/en.yml
@@ -242,6 +242,22 @@ en:
           '-M "MODULE_NAME.var.PLUGIN_TYPE.PLUGIN_NAME.VARIABLE_NAME=VALUE"'
           as in
           '-M "example.var.filter.mutate.fieldname=fieldvalue"'
+        modules_setup: |+
+          Load index template into Elasticsearch, and saved searches, 
+          index-pattern, visualizations, and dashboards into Kibana when
+          running modules.
+        cloud_id: |+
+          Sets the elasticsearch and kibana host settings for
+          module connections in Elastic Cloud.
+          Your Elastic Cloud User interface or the Cloud support
+          team should provide this.
+          Add an optional label prefix '<label>:' to help you
+          identify multiple cloud.ids.
+          e.g. 'staging:dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyRub3RhcmVhbCRpZGVudGlmaWVy'
+        cloud_auth: |+
+          Sets the elasticsearch and kibana username and password
+          for module connections in Elastic Cloud
+          e.g. 'username:<password>'
         configtest: |+
           Check configuration for valid syntax and then exit.
         http_host: Web API binding host
diff --git a/logstash-core/logstash-core.gemspec b/logstash-core/logstash-core.gemspec
index da5a93903e7..fb1e89612fc 100644
--- a/logstash-core/logstash-core.gemspec
+++ b/logstash-core/logstash-core.gemspec
@@ -31,7 +31,7 @@ Gem::Specification.new do |gem|
 
   gem.add_runtime_dependency "sinatra", '~> 1.4', '>= 1.4.6'
   gem.add_runtime_dependency 'puma', '~> 2.16'
-  gem.add_runtime_dependency "jruby-openssl", "0.9.20" # >= 0.9.13 Required to support TLSv1.2
+  gem.add_runtime_dependency "jruby-openssl", ">= 0.9.20" # >= 0.9.13 Required to support TLSv1.2
   gem.add_runtime_dependency "chronic_duration", "0.10.6"
 
   # TODO(sissel): Treetop 1.5.x doesn't seem to work well, but I haven't
diff --git a/logstash-core/spec/logstash/agent/converge_spec.rb b/logstash-core/spec/logstash/agent/converge_spec.rb
index bfc990cedbc..1c50bde042b 100644
--- a/logstash-core/spec/logstash/agent/converge_spec.rb
+++ b/logstash-core/spec/logstash/agent/converge_spec.rb
@@ -74,19 +74,18 @@
         describe "#running_user_defined_pipelines" do
           it "returns the user defined pipelines" do
             start_agent(subject)
-            subject.with_running_user_defined_pipelines do |pipelines|
-              expect(pipelines).to include(:main)
-              expect(pipelines).not_to include(:system_pipeline)
-            end
-            subject.shutdown
+            wait_for do
+              subject.with_running_user_defined_pipelines {|pipelines| pipelines.keys }
+            end.to eq([:main])
           end
         end
 
         describe "#running_user_defined_pipelines?" do
           it "returns true" do
             start_agent(subject)
-            expect(subject.running_user_defined_pipelines?).to be_truthy
-            subject.shutdown
+            wait_for do
+              subject.running_user_defined_pipelines?
+            end.to be_truthy
           end
         end
       end
diff --git a/logstash-core/spec/logstash/agent_spec.rb b/logstash-core/spec/logstash/agent_spec.rb
index 23769d48a51..3b1f54ba226 100644
--- a/logstash-core/spec/logstash/agent_spec.rb
+++ b/logstash-core/spec/logstash/agent_spec.rb
@@ -9,6 +9,8 @@
 require_relative "../support/helpers"
 require_relative "../support/matchers"
 
+java_import org.logstash.Timestamp
+
 describe LogStash::Agent do
   let(:agent_settings) { mock_settings({}) }
   let(:agent_args) { {} }
@@ -24,6 +26,7 @@
     sl
   end
   let(:logger) { double("logger") }
+  let(:timeout) {120} #seconds
 
   subject { LogStash::Agent.new(agent_settings, default_source_loader) }
 
@@ -31,7 +34,7 @@
     # This MUST run first, before `subject` is invoked to ensure clean state
     clear_data_dir
 
-    File.open(config_file, "w") { |f| f.puts config_file_txt }
+    File.open(config_file, "w") { |f| f.puts(config_file_txt) }
 
     agent_args.each do |key, value|
       agent_settings.set(key, value)
@@ -48,8 +51,9 @@
   after :each do
     subject.shutdown
     LogStash::SETTINGS.reset
-    File.unlink(config_file)
-    File.unlink(subject.id_path)
+
+    FileUtils.rm(config_file)
+    FileUtils.rm_rf(subject.id_path)
   end
 
   it "fallback to hostname when no name is provided" do
@@ -312,8 +316,10 @@
   context "metrics after config reloading" do
 
     let(:initial_generator_threshold) { 1000 }
-    let(:temporary_file) { Stud::Temporary.file.path }
-    let(:config_file_txt) { "input { generator { count => #{initial_generator_threshold*2} } } output { file { path => '#{temporary_file}'} }" }
+    let(:original_config_output) { Stud::Temporary.pathname }
+    let(:new_config_output) { Stud::Temporary.pathname }
+
+    let(:config_file_txt) { "input { generator { count => #{initial_generator_threshold*2} } } output { file { path => '#{original_config_output}'} }" }
 
     let(:agent_args) do
       {
@@ -324,14 +330,25 @@
 
     subject { described_class.new(agent_settings, default_source_loader) }
 
+    let(:agent_thread) do
+      # subject has to be called for the first time outside the thread because it could create a race condition
+      # with subsequent subject calls
+      s = subject
+      Thread.new { s.execute }
+    end
+
     before(:each) do
       @abort_on_exception = Thread.abort_on_exception
       Thread.abort_on_exception = true
 
-      @t = Thread.new { subject.execute }
+      agent_thread
 
       # wait for some events to reach the dummy_output
-      sleep(0.01) until IO.readlines(temporary_file).size > initial_generator_threshold
+      Timeout.timeout(timeout) do
+        # wait for file existence otherwise it will raise exception on Windows
+        sleep(0.1) until ::File.exist?(original_config_output)
+        sleep(0.1) until IO.readlines(original_config_output).size > initial_generator_threshold
+      end
 
       # write new config
       File.open(config_file, "w") { |f| f.write(new_config) }
@@ -339,10 +356,14 @@
 
     after :each do
       begin
+        Stud.stop!(agent_thread) rescue nil # it may be dead already
+        agent_thread.join
         subject.shutdown
-        Stud.stop!(@t) rescue nil # it may be dead already
-        @t.join
-        File.unlink(temporary_file)
+
+        FileUtils.rm(original_config_output)
+        FileUtils.rm(new_config_output) if File.exist?(new_config_output)
+      rescue
+          #don't care about errors here.
       ensure
         Thread.abort_on_exception = @abort_on_exception
       end
@@ -350,19 +371,20 @@
 
     context "when reloading a good config" do
       let(:new_config_generator_counter) { 500 }
-      let(:new_file) { Stud::Temporary.file.path }
-      let(:new_config) { "input { generator { count => #{new_config_generator_counter} } } output { file { path => '#{new_file}'} }" }
+      let(:new_config) { "input { generator { count => #{new_config_generator_counter} } } output { file { path => '#{new_config_output}'} }" }
 
       before :each do
         subject.converge_state_and_update
-        sleep(0.01) while ::File.read(new_file).chomp.empty?
+        Timeout.timeout(timeout) do
+          # wait for file existence otherwise it will raise exception on Windows
+          sleep(0.1) until ::File.exist?(new_config_output)
+          sleep(0.1) while ::File.read(new_config_output).chomp.empty?
+        end
         # ensure the converge_state_and_update method has updated metrics by
         # invoking the mutex
         subject.running_pipelines?
       end
 
-      after(:each) { File.unlink(new_file) }
-
       it "resets the pipeline metric collector" do
         snapshot = subject.metric.collector.snapshot_metric
         value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:events][:in].value
@@ -392,7 +414,7 @@
       it "sets the success reload timestamp" do
         snapshot = subject.metric.collector.snapshot_metric
         value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_success_timestamp].value
-        expect(value).to be_a(LogStash::Timestamp)
+        expect(value).to be_a(Timestamp)
       end
 
       it "does not set the last reload error" do
@@ -421,7 +443,7 @@
       it "sets the failure reload timestamp" do
         snapshot = subject.metric.collector.snapshot_metric
         value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_failure_timestamp].value
-        expect(value).to be_a(LogStash::Timestamp)
+        expect(value).to be_a(Timestamp)
       end
 
       it "sets the last reload error" do
diff --git a/logstash-core/spec/logstash/api/modules/node_stats_spec.rb b/logstash-core/spec/logstash/api/modules/node_stats_spec.rb
index 340c680f3a4..729f9aee901 100644
--- a/logstash-core/spec/logstash/api/modules/node_stats_spec.rb
+++ b/logstash-core/spec/logstash/api/modules/node_stats_spec.rb
@@ -69,7 +69,7 @@
       "cpu"=>{
         "total_in_millis"=>Numeric,
         "percent"=>Numeric,
-        "load_average" => { "1m" => Numeric }
+        # load_average is not supported on Windows, set it below
       }
     },
    "pipelines" => {
@@ -89,5 +89,9 @@
    }
   }
 
+  unless LogStash::Environment.windows?
+    root_structure["process"]["cpu"]["load_average"] = { "1m" => Numeric }
+  end
+
   test_api_and_resources(root_structure)
 end
diff --git a/logstash-core/spec/logstash/config/source/local_spec.rb b/logstash-core/spec/logstash/config/source/local_spec.rb
index 9e3bf8659ab..2d557483938 100644
--- a/logstash-core/spec/logstash/config/source/local_spec.rb
+++ b/logstash-core/spec/logstash/config/source/local_spec.rb
@@ -79,7 +79,7 @@
 
         parts.each do |part|
           basename = ::File.basename(part.id)
-          file_path = ::File.join(directory, basename)
+          file_path = ::File.expand_path(::File.join(directory, basename))
           content = files[basename]
           expect(part).to be_a_source_with_metadata("file", file_path, content)
         end
@@ -99,7 +99,8 @@
       end
 
       it "raises an exception" do
-        expect { subject.read(file_path) }.to raise_error LogStash::ConfigLoadingError, /#{file_path}/
+        # check against base name because on Windows long paths are shrinked in the exception message
+        expect { subject.read(file_path) }.to raise_error LogStash::ConfigLoadingError, /.+#{::File.basename(file_path)}/
       end
     end
 
diff --git a/logstash-core/spec/logstash/config/source/multi_local_spec.rb b/logstash-core/spec/logstash/config/source/multi_local_spec.rb
index a384d7abd0e..b4d6de0ca60 100644
--- a/logstash-core/spec/logstash/config/source/multi_local_spec.rb
+++ b/logstash-core/spec/logstash/config/source/multi_local_spec.rb
@@ -155,5 +155,16 @@
         expect { subject.pipeline_configs }.to raise_error(ArgumentError)
       end
     end
+
+    context 'using dead letter queue settings' do
+      let(:retrieved_pipelines) do [
+          { "pipeline.id" => "main", "path.dead_letter_queue" => "/tmp", "dead_letter_queue.max_bytes" => 10000 },
+      ]
+      end
+      it "should not raise an error" do
+        expect { subject.pipeline_configs }.not_to raise_error(ArgumentError)
+      end
+
+    end
   end
 end
diff --git a/logstash-core/spec/logstash/filter_delegator_spec.rb b/logstash-core/spec/logstash/filter_delegator_spec.rb
index 7a073b622fc..87368626001 100644
--- a/logstash-core/spec/logstash/filter_delegator_spec.rb
+++ b/logstash-core/spec/logstash/filter_delegator_spec.rb
@@ -7,20 +7,32 @@
 require "support/shared_contexts"
 
 describe LogStash::FilterDelegator do
+
+  class MockGauge
+    def increment(_)
+    end
+  end
+
   include_context "execution_context"
-  
+
   let(:logger) { double(:logger) }
   let(:filter_id) { "my-filter" }
   let(:config) do
     { "host" => "127.0.0.1", "id" => filter_id }
   end
   let(:collector) { [] }
+  let(:counter_in) { MockGauge.new }
+  let(:counter_out) { MockGauge.new }
+  let(:counter_time) { MockGauge.new }
   let(:metric) { LogStash::Instrument::NamespacedNullMetric.new(collector, :null) }
   let(:events) { [LogStash::Event.new, LogStash::Event.new] }
 
   before :each do
     allow(pipeline).to receive(:id).and_return(pipeline_id)
     allow(metric).to receive(:namespace).with(anything).and_return(metric)
+    allow(metric).to receive(:counter).with(:in).and_return(counter_in)
+    allow(metric).to receive(:counter).with(:out).and_return(counter_out)
+    allow(metric).to receive(:counter).with(:duration_in_millis).and_return(counter_time)
   end
 
   let(:plugin_klass) do
@@ -60,7 +72,7 @@ def filter(event)
     context "when the flush return events" do
       it "increments the out" do
         subject.multi_filter([LogStash::Event.new])
-        expect(metric).to receive(:increment).with(:out, 1)
+        expect(counter_out).to receive(:increment).with(1)
         subject.flush({})
       end
     end
@@ -78,12 +90,12 @@ def filter(event)
       end
 
       it "has incremented :in" do
-        expect(metric).to receive(:increment).with(:in, events.size)
+        expect(counter_in).to receive(:increment).with(events.size)
         subject.multi_filter(events)
       end
 
       it "has not incremented :out" do
-        expect(metric).not_to receive(:increment).with(:out, anything)
+        expect(counter_out).not_to receive(:increment).with(anything)
         subject.multi_filter(events)
       end
     end
@@ -109,8 +121,8 @@ def filter(event)
       end
 
       it "increments the in/out of the metric" do
-        expect(metric).to receive(:increment).with(:in, events.size)
-        expect(metric).to receive(:increment).with(:out, events.size * 2)
+        expect(counter_in).to receive(:increment).with(events.size)
+        expect(counter_out).to receive(:increment).with(events.size * 2)
 
         subject.multi_filter(events)
       end
@@ -138,8 +150,8 @@ def filter(event)
     end
 
     it "increments the in/out of the metric" do
-      expect(metric).to receive(:increment).with(:in, events.size)
-      expect(metric).to receive(:increment).with(:out, events.size)
+      expect(counter_in).to receive(:increment).with(events.size)
+      expect(counter_out).to receive(:increment).with(events.size)
 
       subject.multi_filter(events)
     end
diff --git a/logstash-core/spec/logstash/legacy_ruby_event_spec.rb b/logstash-core/spec/logstash/legacy_ruby_event_spec.rb
index 0c4432ede2a..7dd8cfb10eb 100644
--- a/logstash-core/spec/logstash/legacy_ruby_event_spec.rb
+++ b/logstash-core/spec/logstash/legacy_ruby_event_spec.rb
@@ -399,17 +399,17 @@
       end
 
       it "should assign current time when no timestamp" do
-        expect(LogStash::Event.new({}).timestamp.to_i).to be_within(1).of (Time.now.to_i)
+        expect(LogStash::Event.new({}).timestamp.to_i).to be_within(2).of (Time.now.to_i)
       end
 
       it "should tag for invalid value" do
         event = LogStash::Event.new("@timestamp" => "foo")
-        expect(event.timestamp.to_i).to be_within(1).of Time.now.to_i
+        expect(event.timestamp.to_i).to be_within(2).of Time.now.to_i
         expect(event.get("tags")).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
         expect(event.get(LogStash::Event::TIMESTAMP_FAILURE_FIELD)).to eq("foo")
 
         event = LogStash::Event.new("@timestamp" => 666)
-        expect(event.timestamp.to_i).to be_within(1).of Time.now.to_i
+        expect(event.timestamp.to_i).to be_within(2).of Time.now.to_i
         expect(event.get("tags")).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
         expect(event.get(LogStash::Event::TIMESTAMP_FAILURE_FIELD)).to eq(666)
       end
@@ -421,7 +421,7 @@
 
       it "should tag for invalid string format" do
         event = LogStash::Event.new("@timestamp" => "foo")
-        expect(event.timestamp.to_i).to be_within(1).of Time.now.to_i
+        expect(event.timestamp.to_i).to be_within(2).of Time.now.to_i
         expect(event.get("tags")).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
         expect(event.get(LogStash::Event::TIMESTAMP_FAILURE_FIELD)).to eq("foo")
       end
diff --git a/logstash-core/spec/logstash/modules/logstash_config_spec.rb b/logstash-core/spec/logstash/modules/logstash_config_spec.rb
index 796d89d819d..a862f89571e 100644
--- a/logstash-core/spec/logstash/modules/logstash_config_spec.rb
+++ b/logstash-core/spec/logstash/modules/logstash_config_spec.rb
@@ -3,7 +3,7 @@
 
 describe LogStash::Modules::LogStashConfig do
   let(:mod) { instance_double("module", :directory => Stud::Temporary.directory, :module_name => "testing") }
-  let(:settings) { {"var.logstash.testing.pants" => "fancy" }}
+  let(:settings) { {"var.logstash.testing.pants" => "fancy", "var.elasticsearch.password" => LogStash::Util::Password.new('correct_horse_battery_staple') }}
   subject { described_class.new(mod, settings) }
 
   describe "configured inputs" do
@@ -36,6 +36,12 @@
     end
   end
 
+  describe 'elastic_search_config' do
+    it 'should put the password in correctly' do
+      expect(subject.elasticsearch_output_config()).to include("password => \"correct_horse_battery_staple\"")
+    end
+  end
+
   describe "alias modules options" do
     let(:alias_table) do
       { "var.logstash.testing" => "var.logstash.better" }
diff --git a/logstash-core/spec/logstash/modules/scaffold_spec.rb b/logstash-core/spec/logstash/modules/scaffold_spec.rb
index 2112e3848d6..a67c3937472 100644
--- a/logstash-core/spec/logstash/modules/scaffold_spec.rb
+++ b/logstash-core/spec/logstash/modules/scaffold_spec.rb
@@ -81,7 +81,7 @@
 
     it "provides a logstash config" do
       expect(test_module.logstash_configuration).to be_nil
-      test_module.with_settings(module_settings)
+      test_module.with_settings(LogStash::Util::ModulesSettingArray.new([module_settings]).first)
       expect(test_module.logstash_configuration).not_to be_nil
       config_string = test_module.config_string
       expect(config_string).to include("port => 5606")
@@ -124,18 +124,13 @@
       expect(resource2).to be_a(LogStash::Modules::KibanaDashboards)
       expect(resource1.import_path).to eq("api/kibana/settings")
       expect(resource1.content).to be_a(Array)
-      expect(resource1.content.size).to eq(2)
+      expect(resource1.content.size).to eq(1)
 
       test_object = resource1.content[0]
       expect(test_object).to be_a(LogStash::Modules::KibanaSettings::Setting)
       expect(test_object.name).to eq("defaultIndex")
       expect(test_object.value).to eq("foo-*")
 
-      test_object = resource1.content[1]
-      expect(test_object).to be_a(LogStash::Modules::KibanaSettings::Setting)
-      expect(test_object.name).to eq("metrics:max_buckets")
-      expect(test_object.value).to eq(86400)
-
       expect(resource2.import_path).to eq("api/kibana/dashboards/import")
       expect(resource2.content).to be_a(Array)
       expect(resource2.content.size).to eq(5)
@@ -207,7 +202,7 @@
       test_module.with_settings(module_settings)
       test_module.import(LogStash::Modules::ElasticsearchImporter.new(client), LogStash::Modules::KibanaImporter.new(kbnclient))
       expect(paths).to eq(expected_paths)
-      expect(contents[0]).to eq({"changes"=>{"defaultIndex"=>"tester-*", "metrics:max_buckets"=>"86400"}})
+      expect(contents[0]).to eq({"changes"=>{"defaultIndex"=>"tester-*"}})
       second_kbn_post = contents[1]
       expect(second_kbn_post[:version]).to eq("9.8.7-6")
       expect(second_kbn_post[:objects]).to be_a(Array)
diff --git a/logstash-core/spec/logstash/modules/settings_merger_spec.rb b/logstash-core/spec/logstash/modules/settings_merger_spec.rb
new file mode 100644
index 00000000000..815840e37c9
--- /dev/null
+++ b/logstash-core/spec/logstash/modules/settings_merger_spec.rb
@@ -0,0 +1,111 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/util/cloud_setting_id"
+require "logstash/util/cloud_setting_auth"
+require "logstash/modules/settings_merger"
+require "logstash/util/password"
+
+class SubstituteSettingsForRSpec
+  def initialize(hash = {}) @hash = hash; end
+  def put(key, value) @hash[key] = value; end
+  def get(key) @hash[key]; end
+end
+
+describe LogStash::Modules::SettingsMerger do
+  describe "#merge" do
+    let(:cli) {[{"name"=>"mod1", "var.input.tcp.port"=>"3333"}, {"name"=>"mod2"}]}
+    let(:yml) {[{"name"=>"mod1", "var.input.tcp.port"=>2222, "var.kibana.username"=>"rupert", "var.kibana.password"=>"fotherington"}, {"name"=>"mod3", "var.input.tcp.port"=>4445}]}
+    subject(:results) { described_class.merge(cli, yml) }
+    it "merges cli overwriting any common fields in yml" do
+      expect(results).to be_a(Array)
+      expect(results.size).to eq(3)
+      expect(results[0]["name"]).to eq("mod1")
+      expect(results[0]["var.input.tcp.port"]).to eq("3333")
+      expect(results[0]["var.kibana.username"]).to eq("rupert")
+      expect(results[1]["name"]).to eq("mod2")
+      expect(results[2]["name"]).to eq("mod3")
+      expect(results[2]["var.input.tcp.port"]).to eq(4445)
+    end
+  end
+
+  describe "#merge_cloud_settings" do
+    let(:cloud_id) { LogStash::Util::CloudSettingId.new("label:dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyRub3RhcmVhbCRpZGVudGlmaWVy") }
+    let(:cloud_auth) { LogStash::Util::CloudSettingAuth.new("elastix:bigwhoppingfairytail") }
+    let(:mod_settings) { {} }
+
+    context "when both are supplied" do
+      let(:expected_table) do
+        {
+          "var.kibana.scheme" => "https",
+          "var.kibana.host" => "identifier.us-east-1.aws.found.io:443",
+          "var.elasticsearch.hosts" => "notareal.us-east-1.aws.found.io:443",
+          "var.elasticsearch.username" => "elastix",
+          "var.kibana.username" => "elastix"
+        }
+      end
+      let(:ls_settings) { SubstituteSettingsForRSpec.new({"cloud.id" => cloud_id, "cloud.auth" => cloud_auth}) }
+
+      before do
+        described_class.merge_cloud_settings(mod_settings, ls_settings)
+      end
+
+      it "adds entries to module settings" do
+        expected_table.each do |key, expected|
+          expect(mod_settings[key]).to eq(expected)
+        end
+        expect(mod_settings["var.elasticsearch.password"].value).to eq("bigwhoppingfairytail")
+        expect(mod_settings["var.kibana.password"].value).to eq("bigwhoppingfairytail")
+      end
+    end
+
+    context "when cloud.id is supplied" do
+      let(:expected_table) do
+        {
+          "var.kibana.scheme" => "https",
+          "var.kibana.host" => "identifier.us-east-1.aws.found.io:443",
+          "var.elasticsearch.hosts" => "notareal.us-east-1.aws.found.io:443",
+        }
+      end
+      let(:ls_settings) { SubstituteSettingsForRSpec.new({"cloud.id" => cloud_id}) }
+
+      before do
+        described_class.merge_cloud_settings(mod_settings, ls_settings)
+      end
+
+      it "adds entries to module settings" do
+        expected_table.each do |key, expected|
+          expect(mod_settings[key]).to eq(expected)
+        end
+      end
+    end
+
+    context "when only cloud.auth is supplied" do
+      let(:ls_settings) { SubstituteSettingsForRSpec.new({"cloud.auth" => cloud_auth}) }
+      it "should raise an error" do
+        expect{ described_class.merge_cloud_settings(mod_settings, ls_settings) }.to raise_exception(ArgumentError)
+      end
+    end
+
+    context "when neither cloud.id nor cloud.auth is supplied" do
+      let(:ls_settings) { SubstituteSettingsForRSpec.new() }
+      it "should do nothing" do
+        expect(mod_settings).to be_empty
+      end
+    end
+  end
+
+  describe "#format_module_settings" do
+    let(:before_hash) { {"foo" => "red", "bar" => "blue", "qux" => "pink"} }
+    let(:after_hash) { {"foo" => "red", "bar" => "steel-blue", "baz" => LogStash::Util::Password.new("cyan"), "qux" => nil} }
+    subject(:results) { described_class.format_module_settings(before_hash, after_hash) }
+    it "yields an array of formatted lines for ease of logging" do
+      expect(results.size).to eq(after_hash.size + 2)
+      expect(results.first).to eq("-------- Module Settings ---------")
+      expect(results.last).to eq("-------- Module Settings ---------")
+      expect(results[1]).to eq("foo: 'red'")
+      expect(results[2]).to eq("bar: 'steel-blue', was: 'blue'")
+      expect(results[3]).to eq("baz: '<password>', was: ''")
+      expect(results[4]).to eq("qux: '', was: 'pink'")
+    end
+  end
+end
\ No newline at end of file
diff --git a/logstash-core/spec/logstash/output_delegator_spec.rb b/logstash-core/spec/logstash/output_delegator_spec.rb
index 2235cb047c9..5c009996e38 100644
--- a/logstash-core/spec/logstash/output_delegator_spec.rb
+++ b/logstash-core/spec/logstash/output_delegator_spec.rb
@@ -5,10 +5,19 @@
 require "support/shared_contexts"
 
 describe LogStash::OutputDelegator do
+
+  class MockGauge
+    def increment(_)
+    end
+  end
+
   let(:logger) { double("logger") }
   let(:events) { 7.times.map { LogStash::Event.new }}
   let(:plugin_args) { {"id" => "foo", "arg1" => "val1"} }
   let(:collector) { [] }
+  let(:counter_in) { MockGauge.new }
+  let(:counter_out) { MockGauge.new }
+  let(:counter_time) { MockGauge.new }
   let(:metric) { LogStash::Instrument::NamespacedNullMetric.new(collector, :null) }
 
   include_context "execution_context"
@@ -23,6 +32,9 @@
     before(:each) do
       # use the same metric instance
       allow(metric).to receive(:namespace).with(any_args).and_return(metric)
+      allow(metric).to receive(:counter).with(:in).and_return(counter_in)
+      allow(metric).to receive(:counter).with(:out).and_return(counter_out)
+      allow(metric).to receive(:counter).with(:duration_in_millis).and_return(counter_time)
 
       allow(out_klass).to receive(:new).with(any_args).and_return(out_inst)
       allow(out_klass).to receive(:name).and_return("example")
@@ -58,15 +70,13 @@
       end
 
       it "should increment the number of events received" do
-        expect(subject.metric_events).to receive(:increment).with(:in, events.length)
-        expect(subject.metric_events).to receive(:increment).with(:out, events.length)
+        expect(counter_in).to receive(:increment).with(events.length)
+        expect(counter_out).to receive(:increment).with(events.length)
         subject.multi_receive(events)
       end
 
       it "should record the `duration_in_millis`" do
-        clock = spy("clock")
-        expect(subject.metric_events).to receive(:time).with(:duration_in_millis).and_return(clock)
-        expect(clock).to receive(:stop)
+        expect(counter_time).to receive(:increment).with(Integer)
         subject.multi_receive(events)
       end
     end
diff --git a/logstash-core/spec/logstash/pipeline_action/create_spec.rb b/logstash-core/spec/logstash/pipeline_action/create_spec.rb
index 90627a47814..b5918813014 100644
--- a/logstash-core/spec/logstash/pipeline_action/create_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_action/create_spec.rb
@@ -19,7 +19,10 @@
   subject { described_class.new(pipeline_config, metric) }
 
   after do
-    pipelines.each { |_, pipeline| pipeline.shutdown }
+    pipelines.each do |_, pipeline| 
+      pipeline.shutdown 
+      pipeline.thread.join
+    end
   end
 
   it "returns the pipeline_id" do
diff --git a/logstash-core/spec/logstash/pipeline_action/reload_spec.rb b/logstash-core/spec/logstash/pipeline_action/reload_spec.rb
index fc2db33bb60..60bb59686d8 100644
--- a/logstash-core/spec/logstash/pipeline_action/reload_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_action/reload_spec.rb
@@ -22,7 +22,10 @@
   end
 
   after do
-    pipelines.each { |_, pipeline| pipeline.shutdown }
+    pipelines.each do |_, pipeline| 
+      pipeline.shutdown
+      pipeline.thread.join
+    end
   end
 
   it "returns the pipeline_id" do
diff --git a/logstash-core/spec/logstash/pipeline_dlq_commit_spec.rb b/logstash-core/spec/logstash/pipeline_dlq_commit_spec.rb
index 83d1e5d16de..c8643c52877 100644
--- a/logstash-core/spec/logstash/pipeline_dlq_commit_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_dlq_commit_spec.rb
@@ -67,7 +67,7 @@ def close() end
   end
 
   after(:each) do
-    FileUtils.remove_entry pipeline_settings["path.dead_letter_queue"]
+    FileUtils.rm_rf(pipeline_settings["path.dead_letter_queue"])
   end
 
   context "dlq is enabled" do
@@ -85,6 +85,7 @@ def close() end
       entry = dlq_reader.pollEntry(40)
       expect(entry).to_not be_nil
       expect(entry.reason).to eq("my reason")
+      subject.shutdown
     end
   end
 
@@ -101,6 +102,7 @@ def close() end
       subject.run
       dlq_path = java.nio.file.Paths.get(pipeline_settings_obj.get("path.dead_letter_queue"), pipeline_id)
       expect(java.nio.file.Files.exists(dlq_path)).to eq(false)
+      subject.shutdown
     end
   end
 
diff --git a/logstash-core/spec/logstash/pipeline_spec.rb b/logstash-core/spec/logstash/pipeline_spec.rb
index 6778299d0c8..d4dc0fe9840 100644
--- a/logstash-core/spec/logstash/pipeline_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_spec.rb
@@ -41,6 +41,8 @@ class DummyCodec < LogStash::Codecs::Base
   config_name "dummycodec"
   milestone 2
 
+  config :format, :validate => :string
+
   def decode(data)
     data
   end
@@ -93,11 +95,21 @@ def periodic_flush
     true
   end
   def flush(options)
-    return [::LogStash::Event.new("message" => "dummy_flush")]
+    [::LogStash::Event.new("message" => "dummy_flush")]
   end
   def close() end
 end
 
+class DummyFlushingFilterPeriodic < DummyFlushingFilter
+  config_name "dummyflushingfilterperiodic"
+
+  def flush(options)
+    # Don't generate events on the shutdown flush to make sure we actually test the
+    # periodic flush.
+    options[:final] ? [] : [::LogStash::Event.new("message" => "dummy_flush")]
+  end
+end
+
 class TestPipeline < LogStash::Pipeline
   attr_reader :outputs, :settings
 end
@@ -108,7 +120,7 @@ class TestPipeline < LogStash::Pipeline
   let(:override_thread_count)   { 42 }
   let(:dead_letter_queue_enabled) { false }
   let(:dead_letter_queue_path) { }
-  let(:pipeline_settings_obj) { LogStash::SETTINGS }
+  let(:pipeline_settings_obj) { LogStash::SETTINGS.clone }
   let(:pipeline_settings) { {} }
   let(:max_retry) {10} #times
   let(:timeout) {120} #seconds
@@ -124,10 +136,6 @@ class TestPipeline < LogStash::Pipeline
     pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
   end
 
-  after :each do
-    pipeline_settings_obj.reset
-  end
-
   describe "#ephemeral_id" do
     it "creates an ephemeral_id at creation time" do
       pipeline = mock_pipeline_from_string("input { generator { count =>  1 } } output { null {} }")
@@ -355,6 +363,30 @@ class TestPipeline < LogStash::Pipeline
     end
   end
 
+  context "with no explicit ids declared" do
+    before(:each) do
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
+    end
+
+    let(:config) { "input { dummyinput { codec => plain { format => 'something'  } } } filter { dummyfilter {} } output { dummyoutput {} }"}
+    let(:pipeline) { mock_pipeline_from_string(config) }
+
+    after do
+      # If you don't start/stop the pipeline it won't release the queue lock and will
+      # cause the suite to fail :(
+      pipeline.close
+    end
+    
+    it "should use LIR provided IDs" do
+      expect(pipeline.inputs.first.id).to eq(pipeline.lir.input_plugin_vertices.first.id)
+      expect(pipeline.filters.first.id).to eq(pipeline.lir.filter_plugin_vertices.first.id)
+      expect(pipeline.outputs.first.id).to eq(pipeline.lir.output_plugin_vertices.first.id)
+    end
+  end
+
   context "compiled flush function" do
     describe "flusher thread" do
       before(:each) do
@@ -627,7 +659,7 @@ class TestPipeline < LogStash::Pipeline
     before do
       allow(::LogStash::Outputs::DummyOutput).to receive(:new).with(any_args).and_return(output)
       allow(LogStash::Plugin).to receive(:lookup).with("input", "dummy_input").and_return(DummyInput)
-      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummy_flushing_filter").and_return(DummyFlushingFilter)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummy_flushing_filter").and_return(DummyFlushingFilterPeriodic)
       allow(LogStash::Plugin).to receive(:lookup).with("output", "dummy_output").and_return(::LogStash::Outputs::DummyOutput)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
     end
diff --git a/logstash-core/spec/logstash/runner_spec.rb b/logstash-core/spec/logstash/runner_spec.rb
index 93479db7d3d..22a9ecb4aa6 100644
--- a/logstash-core/spec/logstash/runner_spec.rb
+++ b/logstash-core/spec/logstash/runner_spec.rb
@@ -35,7 +35,7 @@
     allow(LogStash::Logging::Logger).to receive(:configure_logging) do |level, path|
       allow(logger).to receive(:level).and_return(level.to_sym)
     end
-
+    allow(LogStash::Logging::Logger).to receive(:reconfigure).with(any_args)
     # Make sure we don't start a real pipeline here.
     # because we cannot easily close the pipeline
     allow(LogStash::Agent).to receive(:new).with(any_args).and_return(agent)
@@ -43,10 +43,6 @@
     allow(agent).to receive(:shutdown)
   end
 
-  after :each do
-    LogStash::Logging::Logger::configure_logging("info")
-  end
-
   describe "argument precedence" do
     let(:config) { "input {} output {}" }
     let(:cli_args) { ["-e", config, "-w", "20"] }
@@ -294,6 +290,9 @@
     end
 
     describe "config.debug" do
+      after(:each) do
+        LogStash::SETTINGS.set("config.debug", false)
+      end
       it "should set 'config.debug' to false by default" do
         expect(LogStash::Agent).to receive(:new) do |settings|
           expect(settings.get("config.debug")).to eq(false)
@@ -340,7 +339,7 @@
     end
 
     describe "--modules" do
-      let(:args) { ["--modules", module_string] }
+      let(:args) { ["--modules", module_string, "--setup"] }
 
       context "with an available module specified but no connection to elasticsearch" do
         let(:module_string) { "tester" }
diff --git a/logstash-core/spec/logstash/settings/modules_spec.rb b/logstash-core/spec/logstash/settings/modules_spec.rb
new file mode 100644
index 00000000000..da71ea6a1e1
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/modules_spec.rb
@@ -0,0 +1,115 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+require "logstash/util/cloud_setting_id"
+require "logstash/util/cloud_setting_auth"
+require "logstash/util/modules_setting_array"
+
+describe LogStash::Setting::Modules do
+  describe "Modules.Cli" do
+    subject { described_class.new("mycloudid", LogStash::Util::ModulesSettingArray, []) }
+    context "when given an array of hashes that contains a password key" do
+      it "should convert password Strings to Password" do
+        source = [{"var.kibana.password" => "some_secret"}]
+        setting = subject.set(source)
+        expect(setting).to be_a(Array)
+        expect(setting.__class__).to eq(LogStash::Util::ModulesSettingArray)
+        expect(setting.first.fetch("var.kibana.password")).to be_a(LogStash::Util::Password)
+      end
+    end
+  end
+
+  describe "Cloud.Id" do
+    subject { described_class.new("mycloudid", LogStash::Util::CloudSettingId) }
+    context "when given a string which is not a cloud id" do
+      it "should raise an exception" do
+        expect { subject.set("foobarbaz") }.to raise_error(ArgumentError, /Cloud Id does not decode/)
+      end
+    end
+
+    context "when given a string which is empty" do
+      it "should raise an exception" do
+        expect { subject.set("") }.to raise_error(ArgumentError, /Cloud Id does not decode/)
+      end
+    end
+
+    context "when given a string which is has environment prefix only" do
+      it "should raise an exception" do
+        expect { subject.set("testing:") }.to raise_error(ArgumentError, /Cloud Id does not decode/)
+      end
+    end
+
+    context "when given a badly formatted encoded id" do
+      it "should not raise an error" do
+        encoded = Base64.urlsafe_encode64("foo$$bal")
+        expect { subject.set(encoded) }.to raise_error(ArgumentError, /Cloud Id, after decoding, is invalid. Format: '<part1>\$<part2>\$<part3>'/)
+      end
+    end
+
+    context "when given a nil" do
+      it "should not raise an error" do
+        expect { subject.set(nil) }.to_not raise_error
+      end
+    end
+
+    context "when given a string which is an unlabelled cloud id" do
+      it "should set a LogStash::Util::CloudId instance" do
+        expect { subject.set("dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyRub3RhcmVhbCRpZGVudGlmaWVy") }.to_not raise_error
+        expect(subject.value.elasticsearch_host).to eq("notareal.us-east-1.aws.found.io:443")
+        expect(subject.value.kibana_host).to eq("identifier.us-east-1.aws.found.io:443")
+        expect(subject.value.label).to eq("")
+      end
+    end
+
+    context "when given a string which is a labelled cloud id" do
+      it "should set a LogStash::Util::CloudId instance" do
+        expect { subject.set("staging:dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyRub3RhcmVhbCRpZGVudGlmaWVy") }.to_not raise_error
+        expect(subject.value.elasticsearch_host).to eq("notareal.us-east-1.aws.found.io:443")
+        expect(subject.value.kibana_host).to eq("identifier.us-east-1.aws.found.io:443")
+        expect(subject.value.label).to eq("staging")
+      end
+    end
+  end
+
+  describe "Cloud.Auth" do
+    subject { described_class.new("mycloudauth", LogStash::Util::CloudSettingAuth) }
+    context "when given a string without a separator or a password" do
+      it "should raise an exception" do
+        expect { subject.set("foobarbaz") }.to raise_error(ArgumentError, /Cloud Auth username and password format should be/)
+      end
+    end
+
+    context "when given a string without a password" do
+      it "should raise an exception" do
+        expect { subject.set("foo:") }.to raise_error(ArgumentError, /Cloud Auth username and password format should be/)
+      end
+    end
+
+    context "when given a string without a username" do
+      it "should raise an exception" do
+        expect { subject.set(":bar") }.to raise_error(ArgumentError, /Cloud Auth username and password format should be/)
+      end
+    end
+
+    context "when given a string which is empty" do
+      it "should raise an exception" do
+        expect { subject.set("") }.to raise_error(ArgumentError, /Cloud Auth username and password format should be/)
+      end
+    end
+
+    context "when given a nil" do
+      it "should not raise an error" do
+        expect { subject.set(nil) }.to_not raise_error
+      end
+    end
+
+    context "when given a string which is a cloud auth" do
+      it "should set the string" do
+        expect { subject.set("frodo:baggins") }.to_not raise_error
+        expect(subject.value.username).to eq("frodo")
+        expect(subject.value.password.value).to eq("baggins")
+        expect(subject.value.to_s).to eq("frodo:<password>")
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings/writable_directory_spec.rb b/logstash-core/spec/logstash/settings/writable_directory_spec.rb
index 4463ca82db1..be00ce9f04f 100644
--- a/logstash-core/spec/logstash/settings/writable_directory_spec.rb
+++ b/logstash-core/spec/logstash/settings/writable_directory_spec.rb
@@ -3,17 +3,17 @@
 require "logstash/settings"
 require "tmpdir"
 require "socket" # for UNIXSocket
+require "fileutils"
 
 describe LogStash::Setting::WritableDirectory do
-  let(:mode_rx) { 0555 }
   # linux is 108, Macos is 104, so use a safe value
   # Stud::Temporary.pathname, will exceed that size without adding anything
   let(:parent) { File.join(Dir.tmpdir, Time.now.to_f.to_s) }
   let(:path) { File.join(parent, "fancy") }
 
   before { Dir.mkdir(parent) }
-  after { Dir.exist?(path) && Dir.unlink(path) rescue nil }
-  after { Dir.unlink(parent) }
+  after { Dir.exist?(path) && FileUtils.rm_rf(path)}
+  after { FileUtils.rm_rf(parent) }
 
   shared_examples "failure" do
     before { subject.set(path) }
@@ -44,8 +44,9 @@
       end
 
       context "and the directory cannot be created" do
-        before { File.chmod(mode_rx, parent) }
         it "should fail" do
+          # using chmod does not work on Windows better mock and_raise("message")
+          expect(FileUtils).to receive(:mkdir_p).and_raise("foobar")
           expect { subject.value }.to raise_error
         end
       end
@@ -66,7 +67,8 @@
       end
 
       context "but is not writable" do
-        before { File.chmod(0, path) }
+        # chmod does not work on Windows, mock writable? instead
+        before { expect(File).to receive(:writable?).and_return(false) }
         it_behaves_like "failure"
       end
     end
@@ -84,12 +86,13 @@
         before { socket } # realize `socket` value
         after { socket.close }
         it_behaves_like "failure"
-      end
+      end unless LogStash::Environment.windows?
 
+      
       context "but is a symlink" do
-        before { File::symlink("whatever", path) }
+        before { FileUtils.symlink("whatever", path) }
         it_behaves_like "failure"
-      end
+      end unless LogStash::Environment.windows?
     end
 
     context "when the directory is missing" do
@@ -114,8 +117,8 @@
 
       context "and cannot be created" do
         before do
-          # Remove write permission on the parent
-          File.chmod(mode_rx, parent)
+          # chmod does not work on Windows, mock writable? instead
+          expect(File).to receive(:writable?).and_return(false)
         end
 
         it_behaves_like "failure"
diff --git a/logstash-core/src/main/java/org/logstash/Accessors.java b/logstash-core/src/main/java/org/logstash/Accessors.java
index f7197d1f260..86aa7ab00ea 100644
--- a/logstash-core/src/main/java/org/logstash/Accessors.java
+++ b/logstash-core/src/main/java/org/logstash/Accessors.java
@@ -1,208 +1,155 @@
 package org.logstash;
 
-import java.util.HashMap;
-import java.util.List;
 import java.util.Map;
 
-public class Accessors {
+public final class Accessors {
 
-    private Map<String, Object> data;
-    protected Map<String, Object> lut;
+    private Accessors() {
+        //Utility Class
+    }
 
-    public Accessors(Map<String, Object> data) {
-        this.data = data;
-        this.lut = new HashMap<>(); // reference -> target LUT
+    public static Object get(final ConvertedMap data, final FieldReference field) {
+        final Object target = findParent(data, field);
+        return target == null ? null : fetch(target, field.getKey());
     }
 
-    public Object get(String reference) {
-        FieldReference field = PathCache.cache(reference);
-        Object target = findTarget(field);
-        return (target == null) ? null : fetch(target, field.getKey());
+    public static Object set(final ConvertedMap data, final FieldReference field,
+        final Object value) {
+        return setChild(findCreateTarget(data, field), field.getKey(), value);
     }
 
-    public Object set(String reference, Object value) {
-        final FieldReference field = PathCache.cache(reference);
-        final Object target = findCreateTarget(field);
-        final String key = field.getKey();
-        if (target instanceof Map) {
-            ((Map<String, Object>) target).put(key, value);
-        } else if (target instanceof List) {
-            int i;
-            try {
-                i = Integer.parseInt(key);
-            } catch (NumberFormatException e) {
-                return null;
-            }
-            int size = ((List<Object>) target).size();
-            if (i >= size) {
-                // grow array by adding trailing null items
-                // this strategy reflects legacy Ruby impl behaviour and is backed by specs
-                // TODO: (colin) this is potentially dangerous, and could produce OOM using arbitrary big numbers
-                // TODO: (colin) should be guard against this?
-                for (int j = size; j < i; j++) {
-                    ((List<Object>) target).add(null);
-                }
-                ((List<Object>) target).add(value);
-            } else {
-                int offset = listIndex(i, ((List) target).size());
-                ((List<Object>) target).set(offset, value);
-            }
+    public static Object del(final ConvertedMap data, final FieldReference field) {
+        final Object target = findParent(data, field);
+        if (target instanceof ConvertedMap) {
+            return ((ConvertedMap) target).remove(field.getKey());
         } else {
-            throw newCollectionException(target);
+            return target == null ? null : delFromList((ConvertedList) target, field.getKey());
         }
-        return value;
     }
 
-    public Object del(String reference) {
-        FieldReference field = PathCache.cache(reference);
-        Object target = findTarget(field);
-        if (target != null) {
-            if (target instanceof Map) {
-                return ((Map<String, Object>) target).remove(field.getKey());
-            } else if (target instanceof List) {
-                try {
-                    int i = Integer.parseInt(field.getKey());
-                    int offset = listIndex(i, ((List) target).size());
-                    return ((List)target).remove(offset);
-                } catch (IndexOutOfBoundsException|NumberFormatException e) {
-                    return null;
-                }
-            } else {
-                throw newCollectionException(target);
-            }
-        }
-        return null;
+    public static boolean includes(final ConvertedMap data, final FieldReference field) {
+        final Object target = findParent(data, field);
+        final String key = field.getKey();
+        return target instanceof ConvertedMap && ((ConvertedMap) target).containsKey(key) ||
+            target instanceof ConvertedList && foundInList(key, (ConvertedList) target);
     }
 
-    public boolean includes(String reference) {
-        final FieldReference field = PathCache.cache(reference);
-        final Object target = findTarget(field);
-        final String key = field.getKey();
-        return target instanceof Map && ((Map<String, Object>) target).containsKey(key) ||
-            target instanceof List && foundInList(key, (List<Object>) target);
+    private static Object delFromList(final ConvertedList list, final String key) {
+        try {
+            return list.remove(listIndex(key, list.size()));
+        } catch (IndexOutOfBoundsException | NumberFormatException e) {
+            return null;
+        }
     }
 
-    private static boolean foundInList(final String key, final List<Object> target) {
+    private static Object setOnList(final String key, final Object value, final ConvertedList list) {
+        final int index;
         try {
-            return foundInList(target, Integer.parseInt(key));
+            index = Integer.parseInt(key);
         } catch (NumberFormatException e) {
-            return false;
+            return null;
+        }
+        final int size = list.size();
+        if (index >= size) {
+            appendAtIndex(list, value, index, size);
+        } else {
+            list.set(listIndex(index, size), value);
         }
+        return value;
     }
 
-    private Object findTarget(FieldReference field) {
-        Object target;
-
-        if ((target = this.lut.get(field.getReference())) != null) {
-            return target;
+    private static void appendAtIndex(final ConvertedList list, final Object value, final int index,
+        final int size) {
+        // grow array by adding trailing null items
+        // this strategy reflects legacy Ruby impl behaviour and is backed by specs
+        // TODO: (colin) this is potentially dangerous, and could produce OOM using arbitrary big numbers
+        // TODO: (colin) should be guard against this?
+        for (int i = size; i < index; i++) {
+            list.add(null);
         }
+        list.add(value);
+    }
 
-        target = this.data;
-        for (String key : field.getPath()) {
+    private static Object findParent(final ConvertedMap data, final FieldReference field) {
+        Object target = data;
+        for (final String key : field.getPath()) {
             target = fetch(target, key);
-            if (! isCollection(target)) {
+            if (!(target instanceof ConvertedMap || target instanceof ConvertedList)) {
                 return null;
             }
         }
-
-        this.lut.put(field.getReference(), target);
-
         return target;
     }
 
-    private Object findCreateTarget(FieldReference field) {
-        Object target;
-
-        // flush the @lut to prevent stale cached fieldref which may point to an old target
-        // which was overwritten with a new value. for example, if "[a][b]" is cached and we
-        // set a new value for "[a]" then reading again "[a][b]" would point in a stale target.
-        // flushing the complete @lut is suboptimal, but a hierarchical lut would be required
-        // to be able to invalidate fieldrefs from a common root.
-        // see https://github.com/elastic/logstash/pull/5132
-        this.lut.clear();
-
-        target = this.data;
-        for (String key : field.getPath()) {
-            Object result = fetch(target, key);
-            if (result == null) {
-                result = new HashMap<String, Object>();
-                if (target instanceof Map) {
-                    ((Map<String, Object>)target).put(key, result);
-                } else if (target instanceof List) {
-                    try {
-                        int i = Integer.parseInt(key);
-                        // TODO: what about index out of bound?
-                        ((List<Object>)target).set(i, result);
-                    } catch (NumberFormatException e) {
-                        continue;
-                    }
-                } else if (target != null) {
-                    throw newCollectionException(target);
+    private static Object findCreateTarget(final ConvertedMap data, final FieldReference field) {
+        Object target = data;
+        boolean create = false;
+        for (final String key : field.getPath()) {
+            Object result;
+            if (create) {
+                result = createChild((ConvertedMap) target, key);
+            } else {
+                result = fetch(target, key);
+                create = result == null;
+                if (create) {
+                    result = new ConvertedMap(1);
+                    setChild(target, key, result);
                 }
             }
             target = result;
         }
-
-        this.lut.put(field.getReference(), target);
-
         return target;
     }
 
-    private static boolean foundInList(List<Object> target, int index) {
-        try {
-            int offset = listIndex(index, target.size());
-            return target.get(offset) != null;
-        } catch (IndexOutOfBoundsException e) {
-            return false;
+    private static Object setChild(final Object target, final String key, final Object value) {
+        if (target instanceof Map) {
+            ((ConvertedMap) target).put(key, value);
+            return value;
+        } else {
+            return setOnList(key, value, (ConvertedList) target);
         }
+    }
 
+    private static Object createChild(final ConvertedMap target, final String key) {
+        final Object result = new ConvertedMap(1);
+        target.put(key, result);
+        return result;
     }
 
     private static Object fetch(Object target, String key) {
-        if (target instanceof Map) {
-            Object result = ((Map<String, Object>) target).get(key);
-            return result;
-        } else if (target instanceof List) {
-            try {
-                int offset = listIndex(Integer.parseInt(key), ((List) target).size());
-                return ((List<Object>) target).get(offset);
-            } catch (IndexOutOfBoundsException|NumberFormatException e) {
-                return null;
-            }
-        } else if (target == null) {
-            return null;
-        } else {
-            throw newCollectionException(target);
-        }
+        return target instanceof ConvertedMap
+            ? ((ConvertedMap) target).get(key) : fetchFromList((ConvertedList) target, key);
     }
 
-    private static boolean isCollection(Object target) {
-        if (target == null) {
-            return false;
+    private static Object fetchFromList(final ConvertedList list, final String key) {
+        try {
+            return list.get(listIndex(key, list.size()));
+        } catch (IndexOutOfBoundsException | NumberFormatException e) {
+            return null;
         }
-        return (target instanceof Map || target instanceof List);
     }
 
-    private static ClassCastException newCollectionException(Object target) {
-        return new ClassCastException("expecting List or Map, found "  + target.getClass());
+    private static boolean foundInList(final String key, final ConvertedList target) {
+        return fetchFromList(target, key) != null;
     }
 
-    /* 
-     * Returns a positive integer offset for a list of known size.
-     *
-     * @param i if positive, and offset from the start of the list. If negative, the offset from the end of the list, where -1 means the last element.
-     * @param size the size of the list.
-     * @return the positive integer offset for the list given by index i.
+    /**
+     * Returns a positive integer offset from a Ruby style positive or negative list index.
+     * @param i List index
+     * @param size the size of the list
+     * @return the positive integer offset for the list given by index i
      */
     public static int listIndex(int i, int size) {
-        if (i >= size || i < -size) {
-            throw new IndexOutOfBoundsException("Index " + i + " is out of bounds for a list with size " + size);
-        }
+        return i < 0 ? size + i : i;
+    }
 
-        if (i < 0) { // Offset from the end of the array.
-            return size + i;
-        } else {
-            return i;
-        }
+    /**
+     * Returns a positive integer offset for a list of known size.
+     * @param key List index (String matching /[0-9]+/)
+     * @param size the size of the list
+     * @return the positive integer offset for the list given by index i
+     */
+    private static int listIndex(final String key, final int size) {
+        return listIndex(Integer.parseInt(key), size);
     }
 }
diff --git a/logstash-core/src/main/java/org/logstash/ConvertedList.java b/logstash-core/src/main/java/org/logstash/ConvertedList.java
index a3c4695243c..ff5e6477e3c 100644
--- a/logstash-core/src/main/java/org/logstash/ConvertedList.java
+++ b/logstash-core/src/main/java/org/logstash/ConvertedList.java
@@ -9,7 +9,7 @@
 
 public final class ConvertedList extends ArrayList<Object> {
 
-    private ConvertedList(final int size) {
+    ConvertedList(final int size) {
         super(size);
     }
 
diff --git a/logstash-core/src/main/java/org/logstash/ConvertedMap.java b/logstash-core/src/main/java/org/logstash/ConvertedMap.java
index 932273de179..950459d7eff 100644
--- a/logstash-core/src/main/java/org/logstash/ConvertedMap.java
+++ b/logstash-core/src/main/java/org/logstash/ConvertedMap.java
@@ -8,7 +8,7 @@
 
 public final class ConvertedMap extends HashMap<String, Object> {
 
-    private ConvertedMap(final int size) {
+    ConvertedMap(final int size) {
         super((size << 2) / 3 + 2);
     }
     
diff --git a/logstash-core/src/main/java/org/logstash/Event.java b/logstash-core/src/main/java/org/logstash/Event.java
index 4e3baf3bb8b..da6af48b7c2 100644
--- a/logstash-core/src/main/java/org/logstash/Event.java
+++ b/logstash-core/src/main/java/org/logstash/Event.java
@@ -2,7 +2,6 @@
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.Date;
 import java.util.HashMap;
 import java.util.List;
@@ -12,6 +11,7 @@
 import org.joda.time.DateTime;
 import org.jruby.RubySymbol;
 import org.logstash.ackedqueue.Queueable;
+import org.logstash.bivalues.BiValues;
 import org.logstash.bivalues.NullBiValue;
 import org.logstash.bivalues.StringBiValue;
 import org.logstash.bivalues.TimeBiValue;
@@ -24,11 +24,9 @@
 public final class Event implements Cloneable, Queueable {
 
     private boolean cancelled;
-    private Map<String, Object> data;
-    private Map<String, Object> metadata;
+    private ConvertedMap data;
+    private ConvertedMap metadata;
     private Timestamp timestamp;
-    private Accessors accessors;
-    private Accessors metadata_accessors;
 
     public static final String METADATA = "@metadata";
     public static final String METADATA_BRACKETS = "[" + METADATA + "]";
@@ -40,18 +38,18 @@ public final class Event implements Cloneable, Queueable {
     private static final String DATA_MAP_KEY = "DATA";
     private static final String META_MAP_KEY = "META";
 
+    private static final FieldReference TAGS_FIELD = PathCache.cache("tags");
+    
     private static final Logger logger = LogManager.getLogger(Event.class);
 
     public Event()
     {
-        this.metadata = new HashMap<>();
-        this.data = new HashMap<>();
+        this.metadata = new ConvertedMap(10);
+        this.data = new ConvertedMap(10);
         this.data.put(VERSION, VERSION_ONE);
         this.cancelled = false;
         this.timestamp = new Timestamp();
         this.data.put(TIMESTAMP, this.timestamp);
-        this.accessors = new Accessors(this.data);
-        this.metadata_accessors = new Accessors(this.metadata);
     }
 
     /**
@@ -76,22 +74,17 @@ public Event(ConvertedMap data) {
         }
 
         if (this.data.containsKey(METADATA)) {
-            this.metadata = (Map<String, Object>) this.data.remove(METADATA);
+            this.metadata = ConvertedMap.newFromMap((Map) this.data.remove(METADATA));
         } else {
-            this.metadata = new HashMap<>();
+            this.metadata = new ConvertedMap(10);
         }
-        this.metadata_accessors = new Accessors(this.metadata);
-
         this.cancelled = false;
 
         Object providedTimestamp = data.get(TIMESTAMP);
         // keep reference to the parsedTimestamp for tagging below
         Timestamp parsedTimestamp = initTimestamp(providedTimestamp);
         this.timestamp = (parsedTimestamp == null) ? Timestamp.now() : parsedTimestamp;
-
-        this.data.put(TIMESTAMP, this.timestamp);
-        this.accessors = new Accessors(this.data);
-
+        Accessors.set(data, FieldReference.TIMESTAMP_REFERENCE, timestamp);
         // the tag() method has to be called after the Accessors initialization
         if (parsedTimestamp == null) {
             tag(TIMESTAMP_FAILURE_TAG);
@@ -99,18 +92,14 @@ public Event(ConvertedMap data) {
         }
     }
 
-    public Map<String, Object> getData() {
+    public ConvertedMap getData() {
         return this.data;
     }
 
-    public Map<String, Object> getMetadata() {
+    public ConvertedMap getMetadata() {
         return this.metadata;
     }
 
-    private Accessors getAccessors() {
-        return this.accessors;
-    }
-
     public void cancel() {
         this.cancelled = true;
     }
@@ -136,42 +125,55 @@ public void setTimestamp(Timestamp t) {
         this.data.put(TIMESTAMP, this.timestamp);
     }
 
-    public Object getField(String reference) {
-        Object val = getUnconvertedField(reference);
-        return Javafier.deep(val);
+    public Object getField(final String reference) {
+        final Object unconverted = getUnconvertedField(PathCache.cache(reference));
+        return unconverted == null ? null : Javafier.deep(unconverted);
     }
 
-    public Object getUnconvertedField(String reference) {
-        if (reference.equals(METADATA)) {
-            return this.metadata;
-        } else if (reference.startsWith(METADATA_BRACKETS)) {
-            return this.metadata_accessors.get(reference.substring(METADATA_BRACKETS.length()));
-        } else {
-            return this.accessors.get(reference);
+    public Object getUnconvertedField(final String reference) {
+        return getUnconvertedField(PathCache.cache(reference));
+    }
+
+    public Object getUnconvertedField(final FieldReference field) {
+        switch (field.type()) {
+            case FieldReference.META_PARENT:
+                return this.metadata;
+            case FieldReference.META_CHILD:
+                return Accessors.get(metadata, field);
+            default:
+                return Accessors.get(data, field);
         }
     }
 
-    public void setField(String reference, Object value) {
-        if (reference.equals(TIMESTAMP)) {
-            // TODO(talevy): check type of timestamp
-            this.accessors.set(reference, value);
-        } else if (reference.equals(METADATA_BRACKETS) || reference.equals(METADATA)) {
-            this.metadata = (Map<String, Object>) value;
-            this.metadata_accessors = new Accessors(this.metadata);
-        } else if (reference.startsWith(METADATA_BRACKETS)) {
-            this.metadata_accessors.set(reference.substring(METADATA_BRACKETS.length()), value);
-        } else {
-            this.accessors.set(reference, Valuefier.convert(value));
+    public void setField(final String reference, final Object value) {
+        setField(PathCache.cache(reference), value);
+    }
+
+    public void setField(final FieldReference field, final Object value) {
+        switch (field.type()) {
+            case FieldReference.META_PARENT:
+                this.metadata = ConvertedMap.newFromMap((Map) value);
+                break;
+            case FieldReference.META_CHILD:
+                Accessors.set(metadata, field, value);
+                break;
+            default:
+                Accessors.set(data, field, Valuefier.convert(value));
         }
     }
 
-    public boolean includes(String reference) {
-        if (reference.equals(METADATA_BRACKETS) || reference.equals(METADATA)) {
-            return true;
-        } else if (reference.startsWith(METADATA_BRACKETS)) {
-            return this.metadata_accessors.includes(reference.substring(METADATA_BRACKETS.length()));
-        } else {
-            return this.accessors.includes(reference);
+    public boolean includes(final String field) {
+        return includes(PathCache.cache(field));
+    }
+
+    public boolean includes(final FieldReference field) {
+        switch (field.type()) {
+            case FieldReference.META_PARENT:
+                return true;
+            case FieldReference.META_CHILD:
+                return Accessors.includes(metadata, field);
+            default:
+                return Accessors.includes(data, field);
         }
     }
 
@@ -244,7 +246,6 @@ public Map toMap() {
 
     public Event overwrite(Event e) {
         this.data = e.getData();
-        this.accessors = e.getAccessors();
         this.cancelled = e.isCancelled();
         try {
             this.timestamp = e.getTimestamp();
@@ -261,8 +262,12 @@ public Event append(Event e) {
         return this;
     }
 
-    public Object remove(String path) {
-        return this.accessors.del(path);
+    public Object remove(final String path) {
+        return remove(PathCache.cache(path));
+    }
+
+    public Object remove(final FieldReference field) {
+        return Accessors.del(data, field);
     }
 
     public String sprintf(String s) throws IOException {
@@ -271,7 +276,7 @@ public String sprintf(String s) throws IOException {
 
     @Override
     public Event clone() {
-        return new Event(Cloner.deep(this.data));
+        return new Event(Cloner.<Map>deep(this.data));
     }
 
     public String toString() {
@@ -331,33 +336,63 @@ private static Timestamp parseTimestamp(final Object o) {
         return null;
     }
 
-    public void tag(String tag) {
-        List<Object> tags;
-        Object _tags = this.getField("tags");
-
+    public void tag(final String tag) {
+        final Object tags = Accessors.get(data, TAGS_FIELD);
         // short circuit the null case where we know we won't need deduplication step below at the end
-        if (_tags == null) {
-            setField("tags", Arrays.asList(tag));
-            return;
+        if (tags == null) {
+            initTag(tag);
+        } else {
+            existingTag(Javafier.deep(tags), tag);
         }
+    }
+
+    /**
+     * Branch of {@link Event#tag(String)} that handles adding the first tag to this event.
+     * @param tag Tag to add
+     */
+    private void initTag(final String tag) {
+        final ConvertedList list = new ConvertedList(1);
+        list.add(BiValues.newBiValue(tag));
+        Accessors.set(data, TAGS_FIELD, list);
+    }
 
-        // assign to tags var the proper List of either the existing _tags List or a new List containing whatever non-List item was in the tags field
-        if (_tags instanceof List) {
-            tags = (List<Object>) _tags;
+    /**
+     * Branch of {@link Event#tag(String)} that handles adding to existing tags.
+     * @param tags Existing Tag(s)
+     * @param tag Tag to add
+     */
+    private void existingTag(final Object tags, final String tag) {
+        if (tags instanceof List) {
+            appendTag((List<String>) tags, tag);
         } else {
-            // tags field has a value but not in a List, convert in into a List
-            tags = new ArrayList<>();
-            tags.add(_tags);
+            scalarTagFallback((String) tags, tag);
         }
+    }
 
-        // now make sure the tags list does not already contain the tag
+    /**
+     * Merge the given tag into the given list of existing tags if the list doesn't already contain
+     * the tag.
+     * @param tags Existing tag list
+     * @param tag Tag to add
+     */
+    private void appendTag(final List<String> tags, final String tag) {
         // TODO: we should eventually look into using alternate data structures to do more efficient dedup but that will require properly defining the tagging API too
         if (!tags.contains(tag)) {
             tags.add(tag);
+            Accessors.set(data, TAGS_FIELD, ConvertedList.newFromList((List) tags));
         }
+    }
 
-        // set that back as a proper BiValue
-        this.setField("tags", tags);
+    /**
+     * Fallback for {@link Event#tag(String)} in case "tags" was populated by just a String value
+     * and needs to be converted to a list before appending to it.
+     * @param existing Existing Tag
+     * @param tag Tag to add
+     */
+    private void scalarTagFallback(final String existing, final String tag) {
+        final List<String> tags = new ArrayList<>(2);
+        tags.add(existing);
+        appendTag(tags, tag);
     }
 
     @Override
diff --git a/logstash-core/src/main/java/org/logstash/FieldReference.java b/logstash-core/src/main/java/org/logstash/FieldReference.java
index 440fd61772b..d087cb3ca48 100644
--- a/logstash-core/src/main/java/org/logstash/FieldReference.java
+++ b/logstash-core/src/main/java/org/logstash/FieldReference.java
@@ -1,25 +1,99 @@
 package org.logstash;
 
 import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
 import java.util.regex.Pattern;
-// TODO: implement thread-safe path cache singleton to avoid parsing
 
-public class FieldReference {
+public final class FieldReference {
+
+    /**
+     * This type indicates that the referenced that is the metadata of an {@link Event} found in
+     * {@link Event#metadata}.
+     */
+    public static final int META_PARENT = 0;
+
+    /**
+     * This type indicates that the referenced data must be looked up from {@link Event#metadata}.
+     */
+    public static final int META_CHILD = 1;
+
+    /**
+     * This type indicates that the referenced data must be looked up from {@link Event#data}.
+     */
+    private static final int DATA_CHILD = -1;
+
+    private static final String[] EMPTY_STRING_ARRAY = new String[0];
 
     private static final Pattern SPLIT_PATTERN = Pattern.compile("[\\[\\]]");
 
-    private List<String> path;
-    private String key;
-    private String reference;
+    /**
+     * Holds all existing {@link FieldReference} instances for de-duplication.
+     */
+    private static final Map<FieldReference, FieldReference> DEDUP = new HashMap<>(64);
 
-    public FieldReference(List<String> path, String key, String reference) {
-        this.path = path;
+    /**
+     * Unique {@link FieldReference} pointing at the timestamp field in a {@link Event}.
+     */
+    public static final FieldReference TIMESTAMP_REFERENCE =
+        deduplicate(new FieldReference(EMPTY_STRING_ARRAY, Event.TIMESTAMP, DATA_CHILD));
+
+    private static final FieldReference METADATA_PARENT_REFERENCE =
+        new FieldReference(EMPTY_STRING_ARRAY, Event.METADATA, META_PARENT);
+
+    private final String[] path;
+
+    private final String key;
+
+    private final int hash;
+
+    /**
+     * Either {@link FieldReference#META_PARENT}, {@link FieldReference#META_CHILD} or
+     * {@link FieldReference#DATA_CHILD}.
+     */
+    private final int type;
+
+    private FieldReference(final String[] path, final String key, final int type) {
         this.key = key;
-        this.reference = reference;
+        this.type = type;
+        this.path = path;
+        hash = calculateHash(this.key, this.path, this.type);
     }
 
-    public List<String> getPath() {
+    public static FieldReference parse(final CharSequence reference) {
+        final String[] parts = SPLIT_PATTERN.split(reference);
+        final List<String> path = new ArrayList<>(parts.length);
+        for (final String part : parts) {
+            if (!part.isEmpty()) {
+                path.add(part.intern());
+            }
+        }
+        final String key = path.remove(path.size() - 1).intern();
+        final boolean empty = path.isEmpty();
+        if (empty && key.equals(Event.METADATA)) {
+            return METADATA_PARENT_REFERENCE;
+        } else if (!empty && path.get(0).equals(Event.METADATA)) {
+            return deduplicate(new FieldReference(
+                path.subList(1, path.size()).toArray(EMPTY_STRING_ARRAY), key, META_CHILD));
+        } else {
+            return deduplicate(
+                new FieldReference(path.toArray(EMPTY_STRING_ARRAY), key, DATA_CHILD));
+        }
+    }
+
+    /**
+     * Returns the type of this instance to allow for fast switch operations in
+     * {@link Event#getUnconvertedField(FieldReference)} and
+     * {@link Event#setField(FieldReference, Object)}.
+     * @return Type of the FieldReference
+     */
+    public int type() {
+        return type;
+    }
+
+    public String[] getPath() {
         return path;
     }
 
@@ -27,19 +101,49 @@ public String getKey() {
         return key;
     }
 
-    public String getReference() {
-        return reference;
+    @Override
+    public boolean equals(final Object that) {
+        if (this == that) return true;
+        if (!(that instanceof FieldReference)) return false;
+        final FieldReference other = (FieldReference) that;
+        return type == other.type && key.equals(other.key) && Arrays.equals(path, other.path);
     }
 
-    public static FieldReference parse(String reference) {
-        final String[] parts = SPLIT_PATTERN.split(reference);
-        List<String> path = new ArrayList<>(parts.length);
-        for (final String part : parts) {
-            if (!part.isEmpty()) {
-                path.add(part);
-            }
+    @Override
+    public int hashCode() {
+        return hash;
+    }
+
+    /**
+     * De-duplicates instances using {@link FieldReference#DEDUP}. This method must be
+     * {@code synchronized} since we are running non-atomic get-put sequence on
+     * {@link FieldReference#DEDUP}.
+     * @param parsed FieldReference to de-duplicate
+     * @return De-duplicated FieldReference
+     */
+    private static synchronized FieldReference deduplicate(final FieldReference parsed) {
+        FieldReference ret = DEDUP.get(parsed);
+        if (ret == null) {
+            DEDUP.put(parsed, parsed);
+            ret = parsed;
+        }
+        return ret;
+    }
+
+    /**
+     * Effective hashcode implementation using knowledge of field types.
+     * @param key Key Field
+     * @param path Path Field
+     * @param type Type Field
+     * @return Hash Code
+     */
+    private static int calculateHash(final String key, final String[] path, final int type) {
+        final int prime = 31;
+        int hash = prime;
+        for (final String element : path) {
+            hash = prime * hash + element.hashCode();
         }
-        String key = path.remove(path.size() - 1);
-        return new FieldReference(path, key, reference);
+        hash = prime * hash + key.hashCode();
+        return prime * hash + type;
     }
 }
diff --git a/logstash-core/src/main/java/org/logstash/PathCache.java b/logstash-core/src/main/java/org/logstash/PathCache.java
index 68414cf3af2..c54c4a66991 100644
--- a/logstash-core/src/main/java/org/logstash/PathCache.java
+++ b/logstash-core/src/main/java/org/logstash/PathCache.java
@@ -1,36 +1,28 @@
 package org.logstash;
 
+import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
 
 public final class PathCache {
 
-    private static final ConcurrentHashMap<String, FieldReference> cache = new ConcurrentHashMap<>();
+    private static final Map<CharSequence, FieldReference> CACHE =
+        new ConcurrentHashMap<>(64, 0.2F, 1);
 
-    private static final FieldReference timestamp = cache(Event.TIMESTAMP);
-
-    private static final String BRACKETS_TIMESTAMP = "[" + Event.TIMESTAMP + "]";
-
-    static {
-        // inject @timestamp
-        cache(BRACKETS_TIMESTAMP, timestamp);
-    }
-
-    public static boolean isTimestamp(String reference) {
-        return cache(reference) == timestamp;
+    private PathCache() {
     }
 
-    public static FieldReference cache(String reference) {
+    public static FieldReference cache(final CharSequence reference) {
         // atomicity between the get and put is not important
-        FieldReference result = cache.get(reference);
-        if (result == null) {
-            result = FieldReference.parse(reference);
-            cache.put(reference, result);
+        final FieldReference result = CACHE.get(reference);
+        if (result != null) {
+            return result;
         }
-        return result;
+        return parseToCache(reference);
     }
 
-    public static FieldReference cache(String reference, FieldReference field) {
-        cache.put(reference, field);
-        return field;
+    private static FieldReference parseToCache(final CharSequence reference) {
+        final FieldReference result = FieldReference.parse(reference);
+        CACHE.put(reference, result);
+        return result;
     }
 }
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java
index 7ff83a1c75d..3fb6fb5101d 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java
@@ -38,6 +38,8 @@ public List<? extends Queueable> getElements() {
         return elements;
     }
 
+    public List<Long> getSeqNums() { return this.seqNums; }
+
     public Queue getQueue() {
         return queue;
     }
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
index 9a122d3d402..f2f24a06482 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
@@ -198,9 +198,7 @@ public void open() throws IOException {
                 logger.debug("opening tail page: {}, in: {}, with checkpoint: {}", pageNum, this.dirPath, cp.toString());
 
                 PageIO pageIO = this.pageIOFactory.build(pageNum, this.pageCapacity, this.dirPath);
-                pageIO.open(cp.getMinSeqNum(), cp.getElementCount());
-
-                add(cp, new TailPage(cp, this, pageIO));
+                addIO(cp, pageIO);
             }
 
             // transform the head page into a tail page only if the headpage is non-empty
@@ -235,7 +233,7 @@ public void open() throws IOException {
                 this.headPage.checkpoint();
             } else {
                 // head page is non-empty, transform it into a tail page and create a new empty head page
-                add(headCheckpoint, this.headPage.behead());
+                addPage(headCheckpoint, this.headPage.behead());
 
                 headPageNum = headCheckpoint.getPageNum() + 1;
                 newCheckpointedHeadpage(headPageNum);
@@ -261,9 +259,53 @@ public void open() throws IOException {
         }
     }
 
+    // TODO: addIO and addPage are almost identical - we should refactor to DRY it up.
+
+    // addIO is basically the same as addPage except that it avoid calling PageIO.open
+    // before actually purging the page if it is fully acked. This avoid dealing with
+    // zero byte page files that are fully acked.
+    // see issue #7809
+    private void addIO(Checkpoint checkpoint, PageIO pageIO) throws IOException {
+        if (checkpoint.isFullyAcked()) {
+            // first make sure any fully acked page per the checkpoint is purged if not already
+            try { pageIO.purge(); } catch (NoSuchFileException e) { /* ignore */ }
+
+            // we want to keep all the "middle" checkpoints between the first unacked tail page and the head page
+            // to always have a contiguous sequence of checkpoints which helps figuring queue integrity. for this
+            // we will remove any prepended fully acked tail pages but keep all other checkpoints between the first
+            // unacked tail page and the head page. we did however purge the data file to free disk resources.
+
+            if (this.tailPages.size() == 0) {
+                // this is the first tail page and it is fully acked so just purge it
+                this.checkpointIO.purge(this.checkpointIO.tailFileName(checkpoint.getPageNum()));
+            } else {
+                // create a tail page with a null PageIO and add it to tail pages but not unreadTailPages
+                // since it is fully read because also fully acked
+                // TODO: I don't like this null pageIO tail page...
+                this.tailPages.add(new TailPage(checkpoint, this, null));
+            }
+        } else {
+            pageIO.open(checkpoint.getMinSeqNum(), checkpoint.getElementCount());
+            TailPage page = new TailPage(checkpoint, this, pageIO);
+
+            this.tailPages.add(page);
+            this.unreadTailPages.add(page);
+            this.unreadCount += page.unreadCount();
+            this.currentByteSize += page.getPageIO().getCapacity();
+
+            // for now deactivate all tail pages, we will only reactivate the first one at the end
+            page.getPageIO().deactivate();
+        }
+
+        // track the seqNum as we rebuild tail pages, prevent empty pages with a minSeqNum of 0 to reset seqNum
+        if (checkpoint.maxSeqNum() > this.seqNum) {
+            this.seqNum = checkpoint.maxSeqNum();
+        }
+    }
+
     // add a read tail page into this queue structures but also verify that this tail page
     // is not fully acked in which case it will be purged
-    private void add(Checkpoint checkpoint, TailPage page) throws IOException {
+    private void addPage(Checkpoint checkpoint, TailPage page) throws IOException {
         if (checkpoint.isFullyAcked()) {
             // first make sure any fully acked page per the checkpoint is purged if not already
             try { page.getPageIO().purge(); } catch (NoSuchFileException e) { /* ignore */ }
diff --git a/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java b/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java
index 19bbd737a58..2a884b66865 100644
--- a/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java
+++ b/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java
@@ -56,13 +56,19 @@ private DeadLetterQueueFactory() {
      * @return The write manager for the specific id's dead-letter-queue context
      */
     public static DeadLetterQueueWriter getWriter(String id, String dlqPath, long maxQueueSize) {
-        return REGISTRY.computeIfAbsent(id, k -> {
-            try {
-                return new DeadLetterQueueWriter(Paths.get(dlqPath, k), MAX_SEGMENT_SIZE_BYTES, maxQueueSize);
-            } catch (IOException e) {
-                logger.error("unable to create dead letter queue writer", e);
-            }
-            return null;
-        });
+        return REGISTRY.computeIfAbsent(id, key -> newWriter(key, dlqPath, maxQueueSize));
+    }
+
+    public static DeadLetterQueueWriter release(String id) {
+        return REGISTRY.remove(id);
+    }
+
+    private static DeadLetterQueueWriter newWriter(final String id, final String dlqPath, final long maxQueueSize) {
+        try {
+            return new DeadLetterQueueWriter(Paths.get(dlqPath, id), MAX_SEGMENT_SIZE_BYTES, maxQueueSize);
+        } catch (IOException e) {
+            logger.error("unable to create dead letter queue writer", e);
+        }
+        return null;
     }
 }
diff --git a/logstash-core/src/main/java/org/logstash/common/SourceWithMetadata.java b/logstash-core/src/main/java/org/logstash/common/SourceWithMetadata.java
index 4a91748ecea..5d535d93714 100644
--- a/logstash-core/src/main/java/org/logstash/common/SourceWithMetadata.java
+++ b/logstash-core/src/main/java/org/logstash/common/SourceWithMetadata.java
@@ -60,6 +60,10 @@ public SourceWithMetadata(String protocol, String id, Integer line, Integer colu
             return false;
         }).collect(Collectors.toList());
 
+        if (!(this.getText() instanceof String)) {
+          badAttributes.add(this.getText());
+        }
+
         if (!badAttributes.isEmpty()){
             String message = "Missing attributes in SourceWithMetadata: (" + badAttributes + ") "
                     + this.toString();
@@ -72,7 +76,7 @@ public SourceWithMetadata(String protocol, String id, String text) throws Incomp
     }
 
     public int hashCode() {
-        return Objects.hash(attributes().toArray());
+        return Objects.hash(hashableAttributes().toArray());
     }
 
     public String toString() {
@@ -81,11 +85,16 @@ public String toString() {
 
     @Override
     public String hashSource() {
-        return attributes().stream().map(Object::toString).collect(Collectors.joining("|"));
+        return hashableAttributes().stream().map(Object::toString).collect(Collectors.joining("|"));
     }
 
-    // Fields used in the hashSource and hashCode methods to ensure uniqueness
+    // Fields checked for being not null and non empty String
     private Collection<Object> attributes() {
+        return Arrays.asList(this.getId(), this.getProtocol(), this.getLine(), this.getColumn());
+    }
+
+    // Fields used in the hashSource and hashCode methods to ensure uniqueness
+    private Collection<Object> hashableAttributes() {
         return Arrays.asList(this.getId(), this.getProtocol(), this.getLine(), this.getColumn(), this.getText());
     }
 }
diff --git a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java
index 5e1f287bb2d..0808f12a1ab 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java
@@ -45,6 +45,7 @@ public final class DeadLetterQueueWriter implements Closeable {
 
     static final String SEGMENT_FILE_PATTERN = "%d.log";
     static final String LOCK_FILE = ".lock";
+    public static final String DEAD_LETTER_QUEUE_METADATA_KEY = "dead_letter_queue";
     private final long maxSegmentSize;
     private final long maxQueueSize;
     private LongAdder currentQueueSize;
@@ -132,6 +133,12 @@ public synchronized void writeEntry(Event event, String pluginName, String plugi
     }
 
     private void innerWriteEntry(DLQEntry entry) throws IOException {
+        Event event = entry.getEvent();
+
+        if (alreadyProcessed(event)) {
+            logger.warn("Event previously submitted to dead letter queue. Skipping...");
+            return;
+        }
         byte[] record = entry.serialize();
         int eventPayloadSize = RECORD_HEADER_SIZE + record.length;
         if (currentQueueSize.longValue() + eventPayloadSize > maxQueueSize) {
@@ -144,14 +151,47 @@ private void innerWriteEntry(DLQEntry entry) throws IOException {
         currentQueueSize.add(currentWriter.writeEvent(record));
     }
 
+    /**
+     * Method to determine whether the event has already been processed by the DLQ - currently this
+     * just checks the metadata to see if metadata has been added to the event that indicates that
+     * it has already gone through the DLQ.
+     * TODO: Add metadata around 'depth' to enable >1 iteration through the DLQ if required.
+     * @param event Logstash Event
+     * @return boolean indicating whether the event is eligible to be added to the DLQ
+     */
+    private boolean alreadyProcessed(final Event event) {
+        return event.getMetadata() != null && event.getMetadata().containsKey(DEAD_LETTER_QUEUE_METADATA_KEY);
+    }
+
     @Override
     public synchronized void close() throws IOException {
-        this.lock.release();
         if (currentWriter != null) {
-            currentWriter.close();
+            try {
+                currentWriter.close();
+                open = false;
+            }catch (Exception e){
+                logger.debug("Unable to close dlq writer", e);
+            }
+        }
+        releaseLock();
+    }
+
+    private void releaseLock() {
+        if (this.lock != null){
+            try {
+                this.lock.release();
+                if (this.lock.channel() != null && this.lock.channel().isOpen()) {
+                    this.lock.channel().close();
+                }
+            } catch (Exception e) {
+                logger.debug("Unable to close lock channel", e);
+            }
+            try {
+                Files.deleteIfExists(queuePath.resolve(LOCK_FILE));
+            } catch (IOException e){
+                logger.debug("Unable to delete lock file", e);
+            }
         }
-        Files.deleteIfExists(queuePath.resolve(LOCK_FILE));
-        open = false;
     }
 
     public boolean isOpen() {
diff --git a/logstash-core/src/main/java/org/logstash/common/io/RecordIOReader.java b/logstash-core/src/main/java/org/logstash/common/io/RecordIOReader.java
index 2dea36ac3ca..5e74d3de8e2 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/RecordIOReader.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/RecordIOReader.java
@@ -25,6 +25,7 @@
 import java.nio.channels.FileChannel;
 import java.nio.file.Path;
 import java.nio.file.StandardOpenOption;
+import java.util.Arrays;
 import java.util.Comparator;
 import java.util.function.Function;
 import java.util.zip.CRC32;
@@ -40,7 +41,7 @@
 public final class RecordIOReader implements Closeable {
 
     private final FileChannel channel;
-    private final ByteBuffer currentBlock;
+    private ByteBuffer currentBlock;
     private int currentBlockSizeReadFromChannel;
     private final Path path;
     private long channelPosition;
@@ -80,14 +81,16 @@ public <T> byte[] seekToNextEventPosition(T target, Function<byte[], T> keyExtra
         int lowBlock = 0;
         int highBlock = (int) (channel.size() - VERSION_SIZE) / BLOCK_SIZE;
 
-        if (highBlock == 0) {
-            return null;
-        }
-
         while (lowBlock < highBlock) {
             int middle = (int) Math.ceil((highBlock + lowBlock) / 2.0);
             seekToBlock(middle);
-            T found = keyExtractor.apply(readEvent());
+            byte[] readEvent = readEvent();
+            // If the event is null, scan from the low block upwards
+            if (readEvent == null){
+                matchingBlock = lowBlock;
+                break;
+            }
+            T found = keyExtractor.apply(readEvent);
             int compare = keyComparator.compare(found, target);
             if (compare > 0) {
                 highBlock = middle - 1;
@@ -104,18 +107,21 @@ public <T> byte[] seekToNextEventPosition(T target, Function<byte[], T> keyExtra
 
         // now sequential scan to event
         seekToBlock(matchingBlock);
-        int currentPosition = 0;
         int compare = -1;
         byte[] event = null;
+        BufferState restorePoint = null;
         while (compare < 0) {
-            currentPosition = currentBlock.position();
+            // Save the buffer state when reading the next event, to restore to if a matching event is found.
+            restorePoint = saveBufferState();
             event = readEvent();
             if (event == null) {
                 return null;
             }
             compare = keyComparator.compare(keyExtractor.apply(event), target);
         }
-        currentBlock.position(currentPosition);
+        if (restorePoint != null) {
+            restoreFrom(restorePoint);
+        }
         return event;
     }
 
@@ -123,7 +129,7 @@ public long getChannelPosition() {
         return channelPosition;
     }
 
-   void consumeBlock(boolean rewind) throws IOException {
+    void consumeBlock(boolean rewind) throws IOException {
         if (rewind) {
             currentBlockSizeReadFromChannel = 0;
             currentBlock.rewind();
@@ -131,10 +137,15 @@ void consumeBlock(boolean rewind) throws IOException {
             // already read enough, no need to read more
             return;
         }
-        int originalPosition = currentBlock.position();
-        int read = channel.read(currentBlock);
-        currentBlockSizeReadFromChannel += (read > 0) ? read : 0;
-        currentBlock.position(originalPosition);
+        int processedPosition = currentBlock.position();
+        try {
+            // Move to last written to position
+            currentBlock.position(currentBlockSizeReadFromChannel);
+            channel.read(currentBlock);
+            currentBlockSizeReadFromChannel = currentBlock.position();
+        } finally {
+            currentBlock.position(processedPosition);
+        }
     }
 
     /**
@@ -149,6 +160,10 @@ public boolean isEndOfStream() {
      *
      */
      int seekToStartOfEventInBlock() {
+         // Already consumed all the bytes in this block.
+         if (currentBlock.position() >= currentBlockSizeReadFromChannel){
+             return -1;
+         }
          while (true) {
              RecordType type = RecordType.fromByte(currentBlock.array()[currentBlock.arrayOffset() + currentBlock.position()]);
              if (RecordType.COMPLETE.equals(type) || RecordType.START.equals(type)) {
@@ -156,11 +171,15 @@ int seekToStartOfEventInBlock() {
              } else if (RecordType.END.equals(type)) {
                  RecordHeader header = RecordHeader.get(currentBlock);
                  currentBlock.position(currentBlock.position() + header.getSize());
+                 // If this is the end of stream, then cannot seek to start of block
+                 if (this.isEndOfStream()){
+                     return -1;
+                 }
              } else {
                  return -1;
              }
          }
-    }
+     }
 
     /**
      *
@@ -233,4 +252,71 @@ public byte[] readEvent() throws IOException {
     public void close() throws IOException {
         channel.close();
     }
+
+
+    private BufferState saveBufferState() throws IOException {
+        return new BufferState.Builder().channelPosition(channel.position())
+                                        .blockContents(Arrays.copyOf(this.currentBlock.array(), this.currentBlock.array().length))
+                                        .currentBlockPosition(currentBlock.position())
+                                        .currentBlockSizeReadFromChannel(currentBlockSizeReadFromChannel)
+                                        .build();
+    }
+
+    private void restoreFrom(BufferState bufferState) throws IOException {
+        this.currentBlock = ByteBuffer.wrap(bufferState.blockContents);
+        this.currentBlock.position(bufferState.currentBlockPosition);
+        this.channel.position(bufferState.channelPosition);
+        this.channelPosition = channel.position();
+        this.currentBlockSizeReadFromChannel = bufferState.currentBlockSizeReadFromChannel;
+    }
+
+    final static class BufferState {
+        private int currentBlockPosition;
+        private int currentBlockSizeReadFromChannel;
+        private long channelPosition;
+        private byte[] blockContents;
+
+        BufferState(Builder builder){
+            this.currentBlockPosition = builder.currentBlockPosition;
+            this.currentBlockSizeReadFromChannel = builder.currentBlockSizeReadFromChannel;
+            this.channelPosition = builder.channelPosition;
+            this.blockContents = builder.blockContents;
+        }
+
+        public String toString() {
+            return String.format("CurrentBlockPosition:%d, currentBlockSizeReadFromChannel: %d, channelPosition: %d",
+                    currentBlockPosition, currentBlockSizeReadFromChannel, channelPosition);
+        }
+
+        final static class Builder{
+            private int currentBlockPosition;
+            private int currentBlockSizeReadFromChannel;
+            private long channelPosition;
+            private byte[] blockContents;
+
+            Builder currentBlockPosition(final int currentBlockPosition){
+                this.currentBlockPosition = currentBlockPosition;
+                return this;
+            }
+
+            Builder currentBlockSizeReadFromChannel(final int currentBlockSizeReadFromChannel){
+                this.currentBlockSizeReadFromChannel = currentBlockSizeReadFromChannel;
+                return this;
+            }
+
+            Builder channelPosition(final long channelPosition){
+                this.channelPosition = channelPosition;
+                return this;
+            }
+
+            Builder blockContents(final byte[] blockContents){
+                this.blockContents = blockContents;
+                return this;
+            }
+
+            BufferState build(){
+                return new BufferState(this);
+            }
+        }
+    }
 }
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/graph/Vertex.java b/logstash-core/src/main/java/org/logstash/config/ir/graph/Vertex.java
index c2a7b104527..c36dd560cd0 100644
--- a/logstash-core/src/main/java/org/logstash/config/ir/graph/Vertex.java
+++ b/logstash-core/src/main/java/org/logstash/config/ir/graph/Vertex.java
@@ -241,7 +241,7 @@ public String getId() {
         // they have no source metadata. This might also be used in the future by alternate config languages which are
         // willing to take the hit.
         if (this.getSourceWithMetadata() != null) {
-            generatedId = this.getGraph().uniqueHash() + "|" + this.getSourceWithMetadata().uniqueHash();
+            generatedId = Util.digest(this.getGraph().uniqueHash() + "|" + this.getSourceWithMetadata().uniqueHash());
         } else {
             generatedId = this.uniqueHash();
         }
diff --git a/logstash-core/src/main/java/org/logstash/ext/JrubyEventExtLibrary.java b/logstash-core/src/main/java/org/logstash/ext/JrubyEventExtLibrary.java
index 4ddb21e5873..f87e1dcc12f 100644
--- a/logstash-core/src/main/java/org/logstash/ext/JrubyEventExtLibrary.java
+++ b/logstash-core/src/main/java/org/logstash/ext/JrubyEventExtLibrary.java
@@ -22,6 +22,7 @@
 import org.jruby.runtime.load.Library;
 import org.logstash.ConvertedMap;
 import org.logstash.Event;
+import org.logstash.FieldReference;
 import org.logstash.PathCache;
 import org.logstash.Rubyfier;
 import org.logstash.Valuefier;
@@ -112,16 +113,17 @@ public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args) {
         @JRubyMethod(name = "get", required = 1)
         public IRubyObject ruby_get_field(ThreadContext context, RubyString reference)
         {
-            Object value = this.event.getUnconvertedField(reference.asJavaString());
-            return Rubyfier.deep(context.runtime, value);
+            return Rubyfier.deep(
+                context.runtime,
+                this.event.getUnconvertedField(PathCache.cache(reference.getByteList()))
+            );
         }
 
         @JRubyMethod(name = "set", required = 2)
         public IRubyObject ruby_set_field(ThreadContext context, RubyString reference, IRubyObject value)
         {
-            String r = reference.asJavaString();
-
-            if (PathCache.isTimestamp(r)) {
+            final FieldReference r = PathCache.cache(reference.getByteList());
+            if (r  == FieldReference.TIMESTAMP_REFERENCE) {
                 if (!(value instanceof JrubyTimestampExtLibrary.RubyTimestamp)) {
                     throw context.runtime.newTypeError("wrong argument type " + value.getMetaClass() + " (expected LogStash::Timestamp)");
                 }
@@ -153,15 +155,18 @@ public IRubyObject ruby_cancelled(ThreadContext context)
         }
 
         @JRubyMethod(name = "include?", required = 1)
-        public IRubyObject ruby_includes(ThreadContext context, RubyString reference)
-        {
-            return RubyBoolean.newBoolean(context.runtime, this.event.includes(reference.asJavaString()));
+        public IRubyObject ruby_includes(ThreadContext context, RubyString reference) {
+            return RubyBoolean.newBoolean(
+                context.runtime, this.event.includes(PathCache.cache(reference.getByteList()))
+            );
         }
 
         @JRubyMethod(name = "remove", required = 1)
-        public IRubyObject ruby_remove(ThreadContext context, RubyString reference)
-        {
-            return Rubyfier.deep(context.runtime, this.event.remove(reference.asJavaString()));
+        public IRubyObject ruby_remove(ThreadContext context, RubyString reference) {
+            return Rubyfier.deep(
+                context.runtime,
+                this.event.remove(PathCache.cache(reference.getByteList()))
+            );
         }
 
         @JRubyMethod(name = "clone")
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricType.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricType.java
index cc412a6bc82..df51f3e8809 100644
--- a/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricType.java
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricType.java
@@ -24,14 +24,18 @@ public enum MetricType {
      * A gauge backed by a {@link Number} type
      */
     GAUGE_NUMERIC("gauge/numeric"),
-    /**
+     /**
      * A gauge backed by a {@link Object} type.
      */
     GAUGE_UNKNOWN("gauge/unknown"),
     /**
      * A gauge backed by a {@link org.jruby.RubyHash} type. Note - Java consumers should not use this, exist for legacy Ruby code.
      */
-    GAUGE_RUBYHASH("gauge/rubyhash");
+    GAUGE_RUBYHASH("gauge/rubyhash"),
+    /**
+     * A gauge backed by a {@link org.logstash.ext.JrubyTimestampExtLibrary.RubyTimestamp} type. Note - Java consumers should not use this, exist for legacy Ruby code.
+     */
+    GAUGE_RUBYTIMESTAMP("gauge/rubytimestamp");
 
     private final String type;
 
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/BooleanGauge.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/BooleanGauge.java
index 6864c9870d7..b27557be621 100644
--- a/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/BooleanGauge.java
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/BooleanGauge.java
@@ -9,7 +9,7 @@
 /**
  * A {@link GaugeMetric} that is backed by a {@link Boolean}
  */
-public class BooleanGauge extends AbstractMetric<Boolean> implements GaugeMetric<Boolean> {
+public class BooleanGauge extends AbstractMetric<Boolean> implements GaugeMetric<Boolean,Boolean> {
 
     private volatile Boolean value;
 
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/GaugeMetric.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/GaugeMetric.java
index 4037a8ad24e..5b348d423b2 100644
--- a/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/GaugeMetric.java
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/GaugeMetric.java
@@ -6,13 +6,14 @@
 /**
  * A {@link Metric} to set/get a value. A Gauge is useful for measuring a single value that may change over time, but does not carry any additional semantics beyond simply setting
  * and getting the value.
- * @param <T> The backing Java type for the gauge. For example, a text gauge is backed by a {@link String}
+ * @param <G> The backing Java type for getting the gauge. For example, a text gauge should return a {@link String}
+ * @param <S> The backing Java type for setting the gauge. For example, a text gauge is set with a {@link String}
  */
-public interface GaugeMetric<T> extends Metric<T> {
+public interface GaugeMetric<G,S> extends Metric<G> {
 
     /**
      * Sets the value
      * @param value The value to set
      */
-    void set(T value);
+    void set(S value);
 }
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/LazyDelegatingGauge.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/LazyDelegatingGauge.java
index 8b487c8ec75..fb3d7ed0e75 100644
--- a/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/LazyDelegatingGauge.java
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/LazyDelegatingGauge.java
@@ -3,9 +3,10 @@
 import org.apache.logging.log4j.LogManager;
 import org.apache.logging.log4j.Logger;
 import org.jruby.RubyHash;
+import org.logstash.Timestamp;
+import org.logstash.ext.JrubyTimestampExtLibrary.RubyTimestamp;
 import org.logstash.instrument.metrics.AbstractMetric;
 import org.logstash.instrument.metrics.MetricType;
-import org.logstash.instrument.metrics.counter.CounterMetric;
 
 import java.util.List;
 
@@ -13,7 +14,7 @@
  * A lazy proxy to a more specific typed {@link GaugeMetric}. The metric will only be initialized if the initial value is set, or once the {@code set} operation is called.
  * <p><strong>Intended only for use with Ruby's duck typing, Java consumers should use the specific typed {@link GaugeMetric}</strong></p>
  */
-public class LazyDelegatingGauge extends AbstractMetric<Object> implements GaugeMetric<Object> {
+public class LazyDelegatingGauge extends AbstractMetric<Object> implements GaugeMetric<Object,Object> {
 
     private final static Logger LOGGER = LogManager.getLogger(LazyDelegatingGauge.class);
 
@@ -89,6 +90,8 @@ private synchronized void wakeMetric(Object value) {
                 lazyMetric = new BooleanGauge(nameSpaces, key, (Boolean) value);
             } else if (value instanceof RubyHash) {
                 lazyMetric = new RubyHashGauge(nameSpaces, key, (RubyHash) value);
+            } else if (value instanceof RubyTimestamp) {
+                lazyMetric = new RubyTimeStampGauge(nameSpaces, key, ((RubyTimestamp) value));
             } else {
                 LOGGER.warn("A gauge metric of an unknown type ({}) has been create for key: {}, namespace:{}. This may result in invalid serialization.  It is recommended to " +
                         "log an issue to the responsible developer/development team.", value.getClass().getCanonicalName(), key, nameSpaces);
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/NumericGauge.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/NumericGauge.java
index b5ee49eade4..5d0abb35541 100644
--- a/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/NumericGauge.java
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/NumericGauge.java
@@ -8,7 +8,7 @@
 /**
  * A {@link GaugeMetric} that is backed by a {@link Number}
  */
-public class NumericGauge extends AbstractMetric<Number> implements GaugeMetric<Number> {
+public class NumericGauge extends AbstractMetric<Number> implements GaugeMetric<Number,Number> {
 
     private volatile Number value;
 
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/RubyHashGauge.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/RubyHashGauge.java
index c48479a30a6..4ce4f6dedd8 100644
--- a/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/RubyHashGauge.java
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/RubyHashGauge.java
@@ -4,13 +4,14 @@
 import org.logstash.instrument.metrics.AbstractMetric;
 import org.logstash.instrument.metrics.MetricType;
 
+
 import java.util.List;
 
 /**
  * A {@link GaugeMetric} that is backed by a {@link RubyHash}.  Note - This should not be used directly from Java code and exists for passivity with legacy Ruby code. Depending
  * on the types in in the {@link RubyHash} there are no guarantees serializing properly.
  */
-public class RubyHashGauge extends AbstractMetric<RubyHash> implements GaugeMetric<RubyHash> {
+public class RubyHashGauge extends AbstractMetric<RubyHash> implements GaugeMetric<RubyHash,RubyHash> {
 
     private volatile RubyHash value;
 
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/RubyTimeStampGauge.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/RubyTimeStampGauge.java
new file mode 100644
index 00000000000..e19fc31ea92
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/RubyTimeStampGauge.java
@@ -0,0 +1,55 @@
+package org.logstash.instrument.metrics.gauge;
+
+import org.logstash.Timestamp;
+import org.logstash.bivalues.BiValues;
+import org.logstash.ext.JrubyTimestampExtLibrary.RubyTimestamp;
+import org.logstash.instrument.metrics.AbstractMetric;
+import org.logstash.instrument.metrics.MetricType;
+
+import java.util.List;
+
+/**
+ * A {@link GaugeMetric} that is set by a {@link RubyTimestamp}, and retrieved/serialized as a {@link Timestamp}.  Note - This should not be used directly from Java code and
+ * exists for passivity with legacy Ruby code.
+ */
+public class RubyTimeStampGauge extends AbstractMetric<Timestamp> implements GaugeMetric<Timestamp, RubyTimestamp> {
+
+    private volatile Timestamp value;
+
+    /**
+     * Constructor - protected so that Ruby may sub class proxy and discourage usage from Java, null initial value
+     *
+     * @param nameSpace The namespace for this metric
+     * @param key       The key <i>(with in the namespace)</i> for this metric
+     */
+    protected RubyTimeStampGauge(List<String> nameSpace, String key) {
+        this(nameSpace, key, null);
+    }
+
+    /**
+     * Constructor - protected so that Ruby may sub class proxy and discourage usage from Java
+     *
+     * @param nameSpace    The namespace for this metric
+     * @param key          The key <i>(with in the namespace)</i> for this metric
+     * @param initialValue The initial value for this {@link GaugeMetric}, may be null
+     */
+    protected RubyTimeStampGauge(List<String> nameSpace, String key, RubyTimestamp initialValue) {
+        super(nameSpace, key);
+        this.value = initialValue == null ? null : initialValue.getTimestamp();
+    }
+
+    @Override
+    public MetricType getType() {
+        return MetricType.GAUGE_RUBYTIMESTAMP;
+    }
+
+    @Override
+    public Timestamp getValue() {
+        return value;
+    }
+
+    @Override
+    public void set(RubyTimestamp value) {
+        this.value = value == null ? null : value.getTimestamp();
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/TextGauge.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/TextGauge.java
index 2c029c38718..92322a384c1 100644
--- a/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/TextGauge.java
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/TextGauge.java
@@ -9,7 +9,7 @@
 /**
  * A {@link GaugeMetric} that is backed by a {@link String}
  */
-public class TextGauge extends AbstractMetric<String> implements GaugeMetric<String> {
+public class TextGauge extends AbstractMetric<String> implements GaugeMetric<String,String> {
 
     private volatile String value;
 
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/UnknownGauge.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/UnknownGauge.java
index 418c0bc6b41..bd13e9ba8f3 100644
--- a/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/UnknownGauge.java
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/UnknownGauge.java
@@ -8,7 +8,7 @@
 /**
  * A {@link GaugeMetric} that is backed by a {@link Object}.  Note - A stronger typed {@link GaugeMetric} should be used since this makes no guarantees of serializing properly.
  */
-public class UnknownGauge extends AbstractMetric<Object> implements GaugeMetric<Object> {
+public class UnknownGauge extends AbstractMetric<Object> implements GaugeMetric<Object,Object> {
 
     private volatile Object value;
 
diff --git a/logstash-core/src/test/java/org/logstash/AccessorsTest.java b/logstash-core/src/test/java/org/logstash/AccessorsTest.java
index 28776a35d90..1a6bd2ff0ff 100644
--- a/logstash-core/src/test/java/org/logstash/AccessorsTest.java
+++ b/logstash-core/src/test/java/org/logstash/AccessorsTest.java
@@ -1,16 +1,12 @@
 package org.logstash;
 
+import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import org.junit.Rule;
 import org.junit.Test;
-import org.junit.experimental.theories.DataPoint;
-import org.junit.experimental.theories.Theories;
-import org.junit.experimental.theories.Theory;
-import org.junit.rules.ExpectedException;
-import org.junit.runner.RunWith;
+import org.logstash.bivalues.BiValues;
 
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
@@ -19,111 +15,76 @@
 
 public class AccessorsTest {
 
-    public class TestableAccessors extends Accessors {
-
-        public TestableAccessors(Map<String, Object> data) {
-            super(data);
-        }
-
-        public Object lutGet(String reference) {
-            return this.lut.get(reference);
-        }
-    }
-
     @Test
     public void testBareGet() throws Exception {
-        Map<String, Object> data = new HashMap<>();
+        Map<Serializable, Object> data = new HashMap<>();
         data.put("foo", "bar");
         String reference = "foo";
-
-        TestableAccessors accessors = new TestableAccessors(data);
-        assertNull(accessors.lutGet(reference));
-        assertEquals("bar", accessors.get(reference));
-        assertEquals(data, accessors.lutGet(reference));
+        assertEquals(
+            BiValues.newBiValue("bar"), get(ConvertedMap.newFromMap(data), reference)
+        );
     }
 
     @Test
     public void testAbsentBareGet() throws Exception {
-        Map<String, Object> data = new HashMap<>();
+        Map<Serializable, Object>  data = new HashMap<>();
         data.put("foo", "bar");
         String reference = "baz";
-
-        TestableAccessors accessors = new TestableAccessors(data);
-        assertNull(accessors.lutGet(reference));
-        assertNull(accessors.get(reference));
-        assertEquals(data, accessors.lutGet(reference));
+        assertNull(get(ConvertedMap.newFromMap(data), reference));
     }
 
     @Test
     public void testBareBracketsGet() throws Exception {
-        Map<String, Object> data = new HashMap<>();
+        Map<Serializable, Object>  data = new HashMap<>();
         data.put("foo", "bar");
         String reference = "[foo]";
-
-        TestableAccessors accessors = new TestableAccessors(data);
-        assertNull(accessors.lutGet(reference));
-        assertEquals("bar", accessors.get(reference));
-        assertEquals(data, accessors.lutGet(reference));
+        assertEquals(
+            BiValues.newBiValue("bar"), get(ConvertedMap.newFromMap(data), reference)
+        );
     }
 
     @Test
     public void testDeepMapGet() throws Exception {
-        Map<String, Object> data = new HashMap<>();
-        Map<String, Object> inner = new HashMap<>();
+        Map<Serializable, Object>  data = new HashMap<>();
+        Map<Serializable, Object>  inner = new HashMap<>();
         data.put("foo", inner);
         inner.put("bar", "baz");
-
         String reference = "[foo][bar]";
-
-        TestableAccessors accessors = new TestableAccessors(data);
-        assertNull(accessors.lutGet(reference));
-        assertEquals("baz", accessors.get(reference));
-        assertEquals(inner, accessors.lutGet(reference));
+        assertEquals(
+            BiValues.newBiValue("baz"), get(ConvertedMap.newFromMap(data), reference)
+        );
     }
 
     @Test
     public void testAbsentDeepMapGet() throws Exception {
-        Map<String, Object> data = new HashMap<>();
-        Map<String, Object> inner = new HashMap<>();
+        Map<Serializable, Object>  data = new HashMap<>();
+        Map<Serializable, Object>  inner = new HashMap<>();
         data.put("foo", inner);
         inner.put("bar", "baz");
-
         String reference = "[foo][foo]";
-
-        TestableAccessors accessors = new TestableAccessors(data);
-        assertNull(accessors.lutGet(reference));
-        assertNull(accessors.get(reference));
-        assertEquals(inner, accessors.lutGet(reference));
+        assertNull(get(ConvertedMap.newFromMap(data), reference));
     }
 
     @Test
     public void testDeepListGet() throws Exception {
-        Map<String, Object> data = new HashMap<>();
+        Map<Serializable, Object>  data = new HashMap<>();
         List inner = new ArrayList();
         data.put("foo", inner);
         inner.add("bar");
-
         String reference = "[foo][0]";
-
-        TestableAccessors accessors = new TestableAccessors(data);
-        assertNull(accessors.lutGet(reference));
-        assertEquals("bar", accessors.get(reference));
-        assertEquals(inner, accessors.lutGet(reference));
+        assertEquals(
+            BiValues.newBiValue("bar"), get(ConvertedMap.newFromMap(data), reference)
+        );
     }
 
     @Test
     public void testAbsentDeepListGet() throws Exception {
-        Map<String, Object> data = new HashMap<>();
+        Map<Serializable, Object>  data = new HashMap<>();
         List inner = new ArrayList();
         data.put("foo", inner);
         inner.add("bar");
-
         String reference = "[foo][1]";
-
-        TestableAccessors accessors = new TestableAccessors(data);
-        assertNull(accessors.lutGet(reference));
-        assertNull(accessors.get(reference));
-        assertEquals(inner, accessors.lutGet(reference));
+        assertNull(get(ConvertedMap.newFromMap(data), reference));
     }
     /*
      * Check if accessors are able to recovery from
@@ -133,105 +94,88 @@ public void testAbsentDeepListGet() throws Exception {
      */
     @Test
     public void testInvalidIdList() throws Exception {
-        Map<String, Object> data = new HashMap<>();
-        List inner = new ArrayList();
+        final ConvertedMap data = new ConvertedMap(1);
+        List inner = new ConvertedList(2);
         data.put("map1", inner);
         inner.add("obj1");
         inner.add("obj2");
 
         String reference = "[map1][IdNonNumeric]";
 
-        TestableAccessors accessors = new TestableAccessors(data);
-        assertNull(accessors.lutGet(reference));
-        assertNull(accessors.get(reference));
-        assertNull(accessors.set(reference, "obj3"));
-        assertEquals(inner, accessors.lutGet(reference));
-        assertFalse(accessors.includes(reference));
-        assertNull(accessors.del(reference));
+        assertNull(get(data, reference));
+        assertNull(set(data, reference, "obj3"));
+        assertFalse(includes(data, reference));
+        assertNull(del(data, reference));
     }
 
     @Test
     public void testBarePut() throws Exception {
-        Map<String, Object> data = new HashMap<>();
+        final ConvertedMap data = new ConvertedMap(1);
         String reference = "foo";
-
-        TestableAccessors accessors = new TestableAccessors(data);
-        assertNull(accessors.lutGet(reference));
-        assertEquals("bar", accessors.set(reference, "bar"));
-        assertEquals(data, accessors.lutGet(reference));
-        assertEquals("bar", accessors.get(reference));
+        assertEquals("bar", set(data, reference, "bar"));
+        assertEquals("bar", get(data, reference));
     }
 
     @Test
     public void testBareBracketsPut() throws Exception {
-        Map<String, Object> data = new HashMap<>();
+        final ConvertedMap data = new ConvertedMap(1);
         String reference = "[foo]";
 
-        TestableAccessors accessors = new TestableAccessors(data);
-        assertNull(accessors.lutGet(reference));
-        assertEquals("bar", accessors.set(reference, "bar"));
-        assertEquals(data, accessors.lutGet(reference));
-        assertEquals("bar", accessors.get(reference));
+        assertEquals("bar", set(data, reference, "bar"));
+        assertEquals("bar", get(data, reference));
     }
 
     @Test
     public void testDeepMapSet() throws Exception {
-        Map<String, Object> data = new HashMap<>();
+        final ConvertedMap data = new ConvertedMap(1);
 
         String reference = "[foo][bar]";
 
-        TestableAccessors accessors = new TestableAccessors(data);
-        assertNull(accessors.lutGet(reference));
-        assertEquals("baz", accessors.set(reference, "baz"));
-        assertEquals(accessors.lutGet(reference), data.get("foo"));
-        assertEquals("baz", accessors.get(reference));
+        assertEquals("baz", set(data, reference, "baz"));
+        assertEquals("baz", get(data, reference));
     }
 
     @Test
     public void testDel() throws Exception {
-        Map<String, Object> data = new HashMap<>();
-        List inner = new ArrayList();
+        final ConvertedMap data = new ConvertedMap(1);
+        List inner = new ConvertedList(1);
         data.put("foo", inner);
         inner.add("bar");
         data.put("bar", "baz");
-        TestableAccessors accessors = new TestableAccessors(data);
 
-        assertEquals("bar", accessors.del("[foo][0]"));
-        assertNull(accessors.del("[foo][0]"));
-        assertEquals(new ArrayList<>(), accessors.get("[foo]"));
-        assertEquals("baz", accessors.del("[bar]"));
-        assertNull(accessors.get("[bar]"));
+        assertEquals("bar", del(data, "[foo][0]"));
+        assertNull(del(data, "[foo][0]"));
+        assertEquals(new ConvertedList(0), get(data,"[foo]"));
+        assertEquals("baz", del(data, "[bar]"));
+        assertNull(get(data, "[bar]"));
     }
 
     @Test
     public void testNilInclude() throws Exception {
-        Map<String, Object> data = new HashMap<>();
+        final ConvertedMap data = new ConvertedMap(1);
         data.put("nilfield", null);
-        TestableAccessors accessors = new TestableAccessors(data);
-        assertTrue(accessors.includes("nilfield"));
+        assertTrue(includes(data, "nilfield"));
     }
 
     @Test
     public void testInvalidPath() throws Exception {
-        Map<String, Object> data = new HashMap<>();
-        Accessors accessors = new Accessors(data);
+        final ConvertedMap data = new ConvertedMap(1);
 
-        assertEquals(1, accessors.set("[foo]", 1));
-        assertNull(accessors.get("[foo][bar]"));
+        assertEquals(1, set(data, "[foo]", 1));
+        assertNull(get(data, "[foo][bar]"));
     }
 
     @Test
     public void testStaleTargetCache() throws Exception {
-        Map<String, Object> data = new HashMap<>();
+        final ConvertedMap data = new ConvertedMap(1);
 
-        Accessors accessors = new Accessors(data);
-        assertNull(accessors.get("[foo][bar]"));
-        assertEquals("baz", accessors.set("[foo][bar]", "baz"));
-        assertEquals("baz", accessors.get("[foo][bar]"));
+        assertNull(get(data,"[foo][bar]"));
+        assertEquals("baz", set(data,"[foo][bar]", "baz"));
+        assertEquals("baz", get(data, "[foo][bar]"));
 
-        assertEquals("boom", accessors.set("[foo]", "boom"));
-        assertNull(accessors.get("[foo][bar]"));
-        assertEquals("boom", accessors.get("[foo]"));
+        assertEquals("boom", set(data, "[foo]", "boom"));
+        assertNull(get(data, "[foo][bar]"));
+        assertEquals("boom", get(data,"[foo]"));
     }
 
     @Test
@@ -244,27 +188,20 @@ public void testListIndexOutOfBounds() {
         assertEquals(0, Accessors.listIndex(-10, 10));
     }
 
-    @RunWith(Theories.class)
-    public static class TestListIndexFailureCases {
-      private static final int size = 10;
-
-      @DataPoint
-      public static final int tooLarge = size;
-
-      @DataPoint
-      public static final int tooLarge1 = size+1;
-
-      @DataPoint
-      public static final int tooLargeNegative = -size - 1;
+    private static Object get(final ConvertedMap data, final CharSequence reference) {
+        return Accessors.get(data, PathCache.cache(reference));
+    }
 
-      @Rule
-      public ExpectedException exception = ExpectedException.none();
+    private static Object set(final ConvertedMap data, final CharSequence reference,
+        final Object value) {
+        return Accessors.set(data, PathCache.cache(reference), value);
+    }
 
-      @Theory
-      public void testListIndexOutOfBounds(int i) {
-        exception.expect(IndexOutOfBoundsException.class);
-        Accessors.listIndex(i, size);
-      }
+    private static Object del(final ConvertedMap data, final CharSequence reference) {
+        return Accessors.del(data, PathCache.cache(reference));
     }
 
+    private static boolean includes(final ConvertedMap data, final CharSequence reference) {
+        return Accessors.includes(data, PathCache.cache(reference));
+    }
 }
diff --git a/logstash-core/src/test/java/org/logstash/FieldReferenceTest.java b/logstash-core/src/test/java/org/logstash/FieldReferenceTest.java
index 280975f3230..68d1527b3c4 100644
--- a/logstash-core/src/test/java/org/logstash/FieldReferenceTest.java
+++ b/logstash-core/src/test/java/org/logstash/FieldReferenceTest.java
@@ -2,35 +2,42 @@
 
 import org.junit.Test;
 
-import static org.junit.Assert.*;
+import static org.junit.Assert.assertArrayEquals;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
 
-public class FieldReferenceTest {
+public final class FieldReferenceTest {
 
     @Test
     public void testParseSingleBareField() throws Exception {
         FieldReference f = FieldReference.parse("foo");
-        assertTrue(f.getPath().isEmpty());
+        assertEquals(0, f.getPath().length);
         assertEquals(f.getKey(), "foo");
     }
 
     @Test
     public void testParseSingleFieldPath() throws Exception {
         FieldReference f = FieldReference.parse("[foo]");
-        assertTrue(f.getPath().isEmpty());
+        assertEquals(0, f.getPath().length);
         assertEquals(f.getKey(), "foo");
     }
 
     @Test
     public void testParse2FieldsPath() throws Exception {
         FieldReference f = FieldReference.parse("[foo][bar]");
-        assertArrayEquals(f.getPath().toArray(), new String[]{"foo"});
+        assertArrayEquals(f.getPath(), new String[]{"foo"});
         assertEquals(f.getKey(), "bar");
     }
 
     @Test
     public void testParse3FieldsPath() throws Exception {
         FieldReference f = FieldReference.parse("[foo][bar]]baz]");
-        assertArrayEquals(f.getPath().toArray(), new String[]{"foo", "bar"});
+        assertArrayEquals(f.getPath(), new String[]{"foo", "bar"});
         assertEquals(f.getKey(), "baz");
     }
-}
\ No newline at end of file
+
+    @Test
+    public void deduplicatesTimestamp() throws Exception {
+        assertTrue(FieldReference.parse("@timestamp") == FieldReference.parse("[@timestamp]"));
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
index 6bd4bb987dd..bc66181c7dc 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
@@ -1,7 +1,10 @@
 package org.logstash.ackedqueue;
 
+import java.io.FileOutputStream;
 import java.io.IOException;
+import java.nio.channels.FileChannel;
 import java.nio.file.NoSuchFileException;
+import java.nio.file.Paths;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
@@ -782,4 +785,55 @@ public void inEmpty() throws IOException {
         }
     }
 
+    @Test
+    public void testZeroByteFullyAckedPageOnOpen() throws IOException {
+        Queueable element = new StringElement("0123456789"); // 10 bytes
+        int singleElementCapacity = singleElementCapacityForByteBufferPageIO(element);
+        Settings settings = TestSettings.persistedQueueSettings(singleElementCapacity, dataPath);
+
+        // the goal here is to recreate a condition where the queue has a tail page of size zero with
+        // a checkpoint that indicates it is full acknowledged
+        // see issue #7809
+
+        try(TestQueue q = new TestQueue(settings)) {
+            q.open();
+
+            Queueable element1 = new StringElement("0123456789");
+            Queueable element2 = new StringElement("9876543210");
+
+            // write 2 elements to force a new page.
+            q.write(element1);
+            q.write(element2);
+            assertThat(q.getTailPages().size(), is(1));
+
+            // work directly on the tail page and not the queue to avoid habing the queue purge the page
+            // but make sure the tail page checkpoint marks it as fully acked
+            TailPage tp = q.getTailPages().get(0);
+            Batch b = tp.readBatch(1);
+            assertThat(b.getElements().get(0), is(element1));
+            tp.ack(b.getSeqNums(), 1);
+            assertThat(tp.isFullyAcked(), is(true));
+
+        }
+        // now we have a queue state where page 0 is fully acked but not purged
+        // manually truncate page 0 to zero byte.
+
+        // TODO page.0 file name is hard coded here because we did not expose the page file naming.
+        FileChannel c = new FileOutputStream(Paths.get(dataPath, "page.0").toFile(), true).getChannel();
+        c.truncate(0);
+        c.close();
+
+        try(TestQueue q = new TestQueue(settings)) {
+            // here q.open used to crash with:
+            // java.io.IOException: Page file size 0 different to configured page capacity 27 for ...
+            // because page.0 ended up as a zero byte file but its checkpoint says it's fully acked
+            q.open();
+            assertThat(q.getUnackedCount(), is(1L));
+            assertThat(q.getTailPages().size(), is(1));
+            assertThat(q.getTailPages().get(0).isFullyAcked(), is(false));
+            assertThat(q.getTailPages().get(0).elementCount, is(1));
+            assertThat(q.getHeadPage().elementCount, is(0));
+        }
+    }
+
 }
diff --git a/logstash-core/src/test/java/org/logstash/common/DeadLetterQueueFactoryTest.java b/logstash-core/src/test/java/org/logstash/common/DeadLetterQueueFactoryTest.java
index 3b63416fb28..bbd2f249096 100644
--- a/logstash-core/src/test/java/org/logstash/common/DeadLetterQueueFactoryTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/DeadLetterQueueFactoryTest.java
@@ -32,6 +32,7 @@
 import static org.junit.Assert.assertTrue;
 
 public class DeadLetterQueueFactoryTest {
+    public static final String PIPELINE_NAME = "pipelineA";
     private Path dir;
 
     @Rule
@@ -43,12 +44,33 @@ public void setUp() throws Exception {
     }
 
     @Test
-    public void test() throws IOException {
-        Path pipelineA = dir.resolve("pipelineA");
-        DeadLetterQueueWriter writer = DeadLetterQueueFactory.getWriter("pipelineA", pipelineA.toString(), 10000);
-        assertTrue(writer.isOpen());
-        DeadLetterQueueWriter writer2 = DeadLetterQueueFactory.getWriter("pipelineA", pipelineA.toString(), 10000);
-        assertSame(writer, writer2);
-        writer.close();
+    public void testSameBeforeRelease() throws IOException {
+        try {
+            Path pipelineA = dir.resolve(PIPELINE_NAME);
+            DeadLetterQueueWriter writer = DeadLetterQueueFactory.getWriter(PIPELINE_NAME, pipelineA.toString(), 10000);
+            assertTrue(writer.isOpen());
+            DeadLetterQueueWriter writer2 = DeadLetterQueueFactory.getWriter(PIPELINE_NAME, pipelineA.toString(), 10000);
+            assertSame(writer, writer2);
+            writer.close();
+        } finally {
+            DeadLetterQueueFactory.release(PIPELINE_NAME);
+        }
     }
+
+    @Test
+    public void testOpenableAfterRelease() throws IOException {
+        try {
+            Path pipelineA = dir.resolve(PIPELINE_NAME);
+            DeadLetterQueueWriter writer = DeadLetterQueueFactory.getWriter(PIPELINE_NAME, pipelineA.toString(), 10000);
+            assertTrue(writer.isOpen());
+            writer.close();
+            DeadLetterQueueFactory.release(PIPELINE_NAME);
+            writer = DeadLetterQueueFactory.getWriter(PIPELINE_NAME, pipelineA.toString(), 10000);
+            assertTrue(writer.isOpen());
+            writer.close();
+        }finally{
+            DeadLetterQueueFactory.release(PIPELINE_NAME);
+        }
+    }
+
 }
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/common/SourceWithMetadataTest.java b/logstash-core/src/test/java/org/logstash/common/SourceWithMetadataTest.java
index ec0c9541ae9..3621f4b5489 100644
--- a/logstash-core/src/test/java/org/logstash/common/SourceWithMetadataTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/SourceWithMetadataTest.java
@@ -40,10 +40,8 @@ public static Iterable<ParameterGroup> data() {
             new ParameterGroup("proto", "path", 1, 1, null),
             new ParameterGroup("", "path", 1, 1, "foo"),
             new ParameterGroup("proto", "", 1, 1, "foo"),
-            new ParameterGroup("proto", "path", 1, 1, ""),
             new ParameterGroup(" ", "path", 1, 1, "foo"),
-            new ParameterGroup("proto", "  ", 1, 1, "foo"),
-            new ParameterGroup("proto", "path", 1, 1, "   ")
+            new ParameterGroup("proto", "  ", 1, 1, "foo")
         );
     }
 
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java
index 850fbdbe026..760dc4da1f7 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java
@@ -29,13 +29,17 @@
 import org.logstash.Timestamp;
 import org.logstash.ackedqueue.StringElement;
 
+import java.io.IOException;
 import java.nio.file.Path;
+import java.util.Arrays;
 import java.util.Collections;
+import java.util.Random;
 
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.is;
 import static org.hamcrest.CoreMatchers.nullValue;
 import static org.hamcrest.MatcherAssert.assertThat;
+import static org.logstash.common.io.RecordIOWriter.BLOCK_SIZE;
 
 public class DeadLetterQueueReaderTest {
     private Path dir;
@@ -111,27 +115,208 @@ public void testReadFromTwoSegments() throws Exception {
         manager.close();
     }
 
+
+    // This test checks that polling after a block has been mostly filled with an event is handled correctly.
+    @Test
+    public void testRereadFinalBlock() throws Exception {
+        Event event = new Event(Collections.emptyMap());
+
+        // Fill event with not quite enough characters to fill block. Fill event with valid RecordType characters - this
+        // was the cause of https://github.com/elastic/logstash/issues/7868
+        event.setField("message", generateMessageContent(32500));
+        long startTime = System.currentTimeMillis();
+        int messageSize = 0;
+        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, 1_000_000_000)) {
+            for (int i = 0; i < 2; i++) {
+                DLQEntry entry = new DLQEntry(event, "", "", "", new Timestamp(startTime++));
+                messageSize += entry.serialize().length;
+                writeManager.writeEntry(entry);
+            }
+        }
+        try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
+            for (int i = 0; i < 3;i++) {
+                readManager.pollEntry(100);
+            }
+        }
+    }
+
+
     @Test
     public void testSeek() throws Exception {
-        DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10000000, 10000000);
         Event event = new Event(Collections.emptyMap());
-        Timestamp target = null;
         long currentEpoch = System.currentTimeMillis();
-        for (int i = 0; i < 1000; i++){
-            DLQEntry entry = new DLQEntry(event, "foo", "bar", String.valueOf(i), new Timestamp(currentEpoch++));
-            writeManager.writeEntry(entry);
-            if (i == 543) {
-                target = entry.getEntryTime();
+        int TARGET_EVENT = 543;
+
+        writeEntries(event, 0, 1000, currentEpoch);
+        seekReadAndVerify(new Timestamp(currentEpoch + TARGET_EVENT),
+                          String.valueOf(TARGET_EVENT));
+    }
+
+
+    // Notes on these tests:
+    //   These tests are designed to test specific edge cases where events end at block boundaries, hence the specific
+    //    sizes of the char arrays being used to pad the events
+
+    // This test tests for a single event that ends on a block boundary
+    @Test
+    public void testBlockBoundary() throws Exception {
+
+        final int PAD_FOR_BLOCK_SIZE_EVENT = 32616;
+        Event event = new Event();
+        char[] field = new char[PAD_FOR_BLOCK_SIZE_EVENT];
+        Arrays.fill(field, 'e');
+        event.setField("T", new String(field));
+        Timestamp timestamp = new Timestamp();
+
+        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, 1_000_000_000)) {
+            for (int i = 0; i < 2; i++) {
+                DLQEntry entry = new DLQEntry(event, "", "", "", timestamp);
+                assertThat(entry.serialize().length + RecordIOWriter.RECORD_HEADER_SIZE, is(BLOCK_SIZE));
+                writeManager.writeEntry(entry);
+            }
+        }
+        try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
+            for (int i = 0; i < 2;i++) {
+                readManager.pollEntry(100);
             }
+        }
+    }
 
+    // This test has multiple messages, with a message ending on a block boundary
+    @Test
+    public void testBlockBoundaryMultiple() throws Exception {
+        Event event = new Event(Collections.emptyMap());
+        char[] field = new char[8053];
+        Arrays.fill(field, 'x');
+        event.setField("message", new String(field));
+        long startTime = System.currentTimeMillis();
+        int messageSize = 0;
+        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, 1_000_000_000)) {
+            for (int i = 1; i <= 5; i++) {
+                DLQEntry entry = new DLQEntry(event, "", "", "", new Timestamp(startTime++));
+                messageSize += entry.serialize().length;
+                writeManager.writeEntry(entry);
+                if (i == 4){
+                    assertThat(messageSize + (RecordIOWriter.RECORD_HEADER_SIZE * 4), is(BLOCK_SIZE));
+                }
+            }
+        }
+        try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
+            for (int i = 0; i < 5;i++) {
+                readManager.pollEntry(100);
+            }
         }
-        writeManager.close();
+    }
+
+
+    // This test tests for a single event that ends on a block and segment boundary
+    @Test
+    public void testBlockAndSegmentBoundary() throws Exception {
+        final int PAD_FOR_BLOCK_SIZE_EVENT = 32616;
+        Event event = new Event();
+        event.setField("T", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT));
+        Timestamp timestamp = new Timestamp();
 
-        DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir);
-        readManager.seekToNextEvent(target);
-        DLQEntry entry = readManager.pollEntry(100);
-        assertThat(entry.getEntryTime().toIso8601(), equalTo(target.toIso8601()));
-        assertThat(entry.getReason(), equalTo("543"));
+        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE, 1_000_000_000)) {
+            for (int i = 0; i < 2; i++) {
+                DLQEntry entry = new DLQEntry(event, "", "", "", timestamp);
+                assertThat(entry.serialize().length + RecordIOWriter.RECORD_HEADER_SIZE, is(BLOCK_SIZE));
+                writeManager.writeEntry(entry);
+            }
+        }
+        try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
+            for (int i = 0; i < 2;i++) {
+                readManager.pollEntry(100);
+            }
+        }
+    }
+
+
+    @Test
+    public void testWriteReadRandomEventSize() throws Exception {
+        Event event = new Event(Collections.emptyMap());
+        int eventCount = 3000;
+        int maxEventSize = BLOCK_SIZE * 2;
+        long startTime = System.currentTimeMillis();
+
+        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, 1_000_000_000L)) {
+            for (int i = 0; i < eventCount; i++) {
+                event.setField("message", generateMessageContent((int)(Math.random() * (maxEventSize))));
+                DLQEntry entry = new DLQEntry(event, "", "", String.valueOf(i), new Timestamp(startTime++));
+                writeManager.writeEntry(entry);
+            }
+        }
+        try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
+            for (int i = 0; i < eventCount;i++) {
+                DLQEntry entry = readManager.pollEntry(100);
+                assertThat(entry.getReason(), is(String.valueOf(i)));
+            }
+        }
+    }
+
+    @Test
+    public void testWriteStopSmallWriteSeekByTimestamp() throws Exception {
+        int FIRST_WRITE_EVENT_COUNT = 100;
+        int SECOND_WRITE_EVENT_COUNT = 100;
+        int OFFSET = 200;
+
+        Event event = new Event(Collections.emptyMap());
+        long startTime = System.currentTimeMillis();
+
+        writeEntries(event, 0, FIRST_WRITE_EVENT_COUNT, startTime);
+        writeEntries(event, OFFSET, SECOND_WRITE_EVENT_COUNT, startTime + 1_000);
+
+        seekReadAndVerify(new Timestamp(startTime + FIRST_WRITE_EVENT_COUNT),
+                          String.valueOf(FIRST_WRITE_EVENT_COUNT));
+    }
+
+    @Test
+    public void testWriteStopBigWriteSeekByTimestamp() throws Exception {
+        int FIRST_WRITE_EVENT_COUNT = 100;
+        int SECOND_WRITE_EVENT_COUNT = 2000;
+        int OFFSET = 200;
+
+        Event event = new Event(Collections.emptyMap());
+        long startTime = System.currentTimeMillis();
+
+        writeEntries(event, 0, FIRST_WRITE_EVENT_COUNT, startTime);
+        writeEntries(event, OFFSET, SECOND_WRITE_EVENT_COUNT, startTime + 1_000);
+
+        seekReadAndVerify(new Timestamp(startTime + FIRST_WRITE_EVENT_COUNT),
+                          String.valueOf(FIRST_WRITE_EVENT_COUNT));
+    }
+
+    private String generateMessageContent(int size) {
+        char[] valid = new char[RecordType.values().length + 1];
+        int j = 0;
+        valid[j] = 'x';
+        for (RecordType type : RecordType.values()){
+            valid[j++] = (char)type.toByte();
+        }
+        Random random = new Random();
+        char fillWith = valid[random.nextInt(valid.length)];
+
+        char[] fillArray = new char[size];
+        Arrays.fill(fillArray, fillWith);
+        return new String(fillArray);
+    }
+
+    private void seekReadAndVerify(final Timestamp seekTarget, final String expectedValue) throws Exception {
+        try(DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
+            readManager.seekToNextEvent(new Timestamp(seekTarget));
+            DLQEntry readEntry = readManager.pollEntry(100);
+            assertThat(readEntry.getReason(), equalTo(expectedValue));
+            assertThat(readEntry.getEntryTime().toIso8601(), equalTo(seekTarget.toIso8601()));
+        }
+    }
+
+    private void writeEntries(final Event event, int offset, final int numberOfEvents, long startTime) throws IOException {
+        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, 10_000_000)) {
+            for (int i = offset; i <= offset + numberOfEvents; i++) {
+                DLQEntry entry = new DLQEntry(event, "foo", "bar", String.valueOf(i), new Timestamp(startTime++));
+                writeManager.writeEntry(entry);
+            }
+        }
     }
 
     @Test
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java
index 2fd07078e43..87b9a777bf9 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java
@@ -32,11 +32,11 @@
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.StandardOpenOption;
-import java.util.Collections;
 
 import static junit.framework.TestCase.assertFalse;
-import static org.hamcrest.MatcherAssert.assertThat;
 import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.CoreMatchers.not;
+import static org.hamcrest.MatcherAssert.assertThat;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 import static org.logstash.common.io.RecordIOWriter.RECORD_HEADER_SIZE;
@@ -53,6 +53,8 @@ public void setUp() throws Exception {
         dir = temporaryFolder.newFolder().toPath();
     }
 
+    private static long EMPTY_DLQ = VERSION_SIZE; // Only the version field has been written
+
     @Test
     public void testLockFileManagement() throws Exception {
         Path lockFile = dir.resolve(".lock");
@@ -99,38 +101,51 @@ public void testWrite() throws Exception {
         writer.close();
     }
 
+    @Test
+    public void testDoesNotWriteMessagesAlreadyRoutedThroughDLQ() throws Exception {
+        Event dlqEvent = new Event();
+        dlqEvent.setField("[@metadata][dead_letter_queue][plugin_type]", "dead_letter_queue");
+        DLQEntry entry = new DLQEntry(new Event(), "type", "id", "reason");
+        DLQEntry dlqEntry = new DLQEntry(dlqEvent, "type", "id", "reason");
+
+        try (DeadLetterQueueWriter writer = new DeadLetterQueueWriter(dir, 1000, 1000000);) {
+            writer.writeEntry(entry);
+            long dlqLengthAfterEvent  = dlqLength();
+
+            assertThat(dlqLengthAfterEvent, is(not(EMPTY_DLQ)));
+            writer.writeEntry(dlqEntry);
+            assertThat(dlqLength(), is(dlqLengthAfterEvent));
+        }
+    }
+
     @Test
     public void testDoesNotWriteBeyondLimit() throws Exception {
         DLQEntry entry = new DLQEntry(new Event(), "type", "id", "reason");
 
         int payloadLength = RECORD_HEADER_SIZE + VERSION_SIZE + entry.serialize().length;
         final int MESSAGE_COUNT= 5;
-        long queueLength = payloadLength * MESSAGE_COUNT;
+        long MAX_QUEUE_LENGTH = payloadLength * MESSAGE_COUNT;
         DeadLetterQueueWriter writer = null;
 
         try{
-            writer = new DeadLetterQueueWriter(dir, payloadLength, queueLength);
+            writer = new DeadLetterQueueWriter(dir, payloadLength, MAX_QUEUE_LENGTH);
             for (int i = 0; i < MESSAGE_COUNT; i++)
                 writer.writeEntry(entry);
 
-            long size = Files.list(dir)
-                    .filter(p -> p.toString().endsWith(".log"))
-                    .mapToLong(p -> p.toFile().length())
-                    .sum();
-
-            assertThat(size, is(queueLength));
-
+            assertThat(dlqLength(), is(MAX_QUEUE_LENGTH));
             writer.writeEntry(entry);
-            size = Files.list(dir)
-                    .filter(p -> p.toString().endsWith(".log"))
-                    .mapToLong(p -> p.toFile().length())
-                    .sum();
-            assertThat(size, is(queueLength));
+            assertThat(dlqLength(), is(MAX_QUEUE_LENGTH));
         } finally {
             if (writer != null) {
                 writer.close();
             }
         }
+    }
 
+    private long dlqLength() throws IOException {
+        return Files.list(dir)
+                .filter(p -> p.toString().endsWith(".log"))
+                .mapToLong(p -> p.toFile().length())
+                .sum();
     }
 }
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/common/io/RecordIOReaderTest.java b/logstash-core/src/test/java/org/logstash/common/io/RecordIOReaderTest.java
new file mode 100644
index 00000000000..0053d99b97b
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/common/io/RecordIOReaderTest.java
@@ -0,0 +1,181 @@
+package org.logstash.common.io;
+
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.logstash.ackedqueue.StringElement;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.file.Path;
+import java.util.Arrays;
+import java.util.Comparator;
+import java.util.Random;
+import java.util.function.Function;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.CoreMatchers.not;
+import static org.hamcrest.CoreMatchers.nullValue;
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.logstash.common.io.RecordIOWriter.BLOCK_SIZE;
+import static org.logstash.common.io.RecordIOWriter.RECORD_HEADER_SIZE;
+
+public class RecordIOReaderTest {
+    private Path file;
+
+    @Rule
+    public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+    @Before
+    public void setUp() throws Exception {
+        file = temporaryFolder.newFile("test").toPath();
+    }
+
+    @Test
+    public void testReadEmptyBlock() throws Exception {
+        RecordIOWriter writer = new RecordIOWriter(file);
+        RecordIOReader reader = new RecordIOReader(file);
+        assertThat(reader.readEvent(), is(nullValue()));
+        writer.close();
+        reader.close();
+    }
+
+    @Test
+    public void testSeekToStartFromEndWithoutNextRecord() throws Exception {
+        char[] tooBig = new char[BLOCK_SIZE + 1000];
+        Arrays.fill(tooBig, 'c');
+        StringElement input = new StringElement(new String(tooBig));
+        RecordIOWriter writer = new RecordIOWriter(file);
+        writer.writeEvent(input.serialize());
+
+        RecordIOReader reader = new RecordIOReader(file);
+        reader.seekToBlock(1);
+        reader.consumeBlock(true);
+        assertThat(reader.seekToStartOfEventInBlock(), equalTo(-1));
+
+        reader.close();
+        writer.close();
+    }
+
+    @Test
+    public void testSeekToStartFromEndWithNextRecordPresent() throws Exception {
+        char[] tooBig = new char[BLOCK_SIZE + 1000];
+        Arrays.fill(tooBig, 'c');
+        StringElement input = new StringElement(new String(tooBig));
+        RecordIOWriter writer = new RecordIOWriter(file);
+        writer.writeEvent(input.serialize());
+        writer.writeEvent(input.serialize());
+
+        RecordIOReader reader = new RecordIOReader(file);
+        reader.seekToBlock(1);
+        reader.consumeBlock(true);
+        assertThat(reader.seekToStartOfEventInBlock(), equalTo(1026));
+
+        reader.close();
+        writer.close();
+    }
+
+
+    @Test
+    public void testReadMiddle() throws Exception {
+        char[] tooBig = fillArray(3 * BLOCK_SIZE + 1000);
+        StringElement input = new StringElement(new String(tooBig));
+        RecordIOWriter writer = new RecordIOWriter(file);
+        RecordIOReader reader = new RecordIOReader(file);
+        byte[] inputSerialized = input.serialize();
+
+        writer.writeEvent(inputSerialized);
+        reader.seekToBlock(1);
+        assertThat(reader.readEvent(), is(nullValue()));
+        writer.writeEvent(inputSerialized);
+        reader.seekToBlock(1);
+        assertThat(reader.readEvent(), is(not(nullValue())));
+
+        writer.close();
+        reader.close();
+    }
+
+    @Test
+    public void testFind() throws Exception {
+
+        RecordIOWriter writer = new RecordIOWriter(file);
+        RecordIOReader reader = new RecordIOReader(file);
+        ByteBuffer intBuffer = ByteBuffer.wrap(new byte[4]);
+        for (int i = 0; i < 20000; i++) {
+            intBuffer.rewind();
+            intBuffer.putInt(i);
+            writer.writeEvent(intBuffer.array());
+        }
+
+        Function<byte[], Object> toInt = (b) -> ByteBuffer.wrap(b).getInt();
+        reader.seekToNextEventPosition(34, toInt, (o1, o2) -> ((Integer) o1).compareTo((Integer) o2));
+        int nextVal = (int) toInt.apply(reader.readEvent());
+        assertThat(nextVal, equalTo(34));
+    }
+
+    @Test
+    public void testSeekBlockSizeEvents() throws Exception {
+        writeSeekAndVerify(10, BLOCK_SIZE);
+    }
+
+    @Test
+    public void testSeekHalfBlockSizeEvents() throws Exception {
+        writeSeekAndVerify(10, BLOCK_SIZE/2);
+    }
+
+    @Test
+    public void testSeekDoubleBlockSizeEvents() throws Exception {
+        writeSeekAndVerify(10, BLOCK_SIZE * 2);
+    }
+
+    private void writeSeekAndVerify(final int eventCount, final int expectedSize) throws IOException {
+        int blocks = (int)Math.ceil(expectedSize / (double)BLOCK_SIZE);
+        int fillSize = (int) (expectedSize - (blocks * RECORD_HEADER_SIZE));
+
+        try(RecordIOWriter writer = new RecordIOWriter(file)){
+            for (char i = 0; i < eventCount; i++) {
+                char[] blockSize = fillArray(fillSize);
+                blockSize[0] = i;
+                assertThat(writer.writeEvent(new StringElement(new String(blockSize)).serialize()), is((long)expectedSize));
+            }
+        }
+
+        try(RecordIOReader reader = new RecordIOReader(file)) {
+            Function<byte[], Character> toChar = (b) -> (char) ByteBuffer.wrap(b).get(0);
+
+            for (char i = 0; i < eventCount; i++) {
+                reader.seekToNextEventPosition(i, toChar, Comparator.comparing(o -> ((Character) o)));
+                assertThat(toChar.apply(reader.readEvent()), equalTo(i));
+            }
+        }
+    }
+
+    @Test
+    public void testReadWhileWriteAcrossBoundary() throws Exception {
+        char[] tooBig = fillArray( BLOCK_SIZE/4);
+        StringElement input = new StringElement(new String(tooBig));
+        byte[] inputSerialized = input.serialize();
+        try(RecordIOWriter writer = new RecordIOWriter(file);
+            RecordIOReader reader = new RecordIOReader(file)){
+
+            for (int j = 0; j < 2; j++) {
+                writer.writeEvent(inputSerialized);
+            }
+            assertThat(reader.readEvent(), equalTo(inputSerialized));
+            for (int j = 0; j < 2; j++) {
+                writer.writeEvent(inputSerialized);
+            }
+            for (int j = 0; j < 3; j++) {
+                assertThat(reader.readEvent(), equalTo(inputSerialized));
+            }
+        }
+    }
+
+    private char[] fillArray(final int fillSize) {
+        char[] blockSize= new char[fillSize];
+        Arrays.fill(blockSize, 'e');
+        return blockSize;
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/common/io/RecordIOWriterTest.java b/logstash-core/src/test/java/org/logstash/common/io/RecordIOWriterTest.java
index d3a55ac65a3..2edb068936e 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/RecordIOWriterTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/RecordIOWriterTest.java
@@ -6,15 +6,12 @@
 import org.junit.rules.TemporaryFolder;
 import org.logstash.ackedqueue.StringElement;
 
-import java.nio.ByteBuffer;
+
 import java.nio.file.Path;
 import java.util.Arrays;
-import java.util.Comparator;
-import java.util.function.Function;
 
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.is;
-import static org.hamcrest.CoreMatchers.not;
 import static org.hamcrest.CoreMatchers.nullValue;
 import static org.hamcrest.MatcherAssert.assertThat;
 import static org.logstash.common.io.RecordIOWriter.BLOCK_SIZE;
@@ -30,15 +27,6 @@ public void setUp() throws Exception {
         file = temporaryFolder.newFile("test").toPath();
     }
 
-    @Test
-    public void testReadEmptyBlock() throws Exception {
-        RecordIOWriter writer = new RecordIOWriter(file);
-        RecordIOReader reader = new RecordIOReader(file);
-        assertThat(reader.readEvent(), is(nullValue()));
-        writer.close();
-        reader.close();
-    }
-
     @Test
     public void testSingleComplete() throws Exception {
         StringElement input = new StringElement("element");
@@ -51,46 +39,9 @@ public void testSingleComplete() throws Exception {
         writer.close();
     }
 
-    @Test
-    public void testSeekToStartFromEndWithoutNextRecord() throws Exception {
-        char[] tooBig = new char[BLOCK_SIZE + 1000];
-        Arrays.fill(tooBig, 'c');
-        StringElement input = new StringElement(new String(tooBig));
-        RecordIOWriter writer = new RecordIOWriter(file);
-        writer.writeEvent(input.serialize());
-
-        RecordIOReader reader = new RecordIOReader(file);
-        reader.seekToBlock(1);
-        reader.consumeBlock(true);
-        assertThat(reader.seekToStartOfEventInBlock(), equalTo(-1));
-
-        reader.close();
-        writer.close();
-    }
-
-    @Test
-    public void testSeekToStartFromEndWithNextRecordPresent() throws Exception {
-        char[] tooBig = new char[BLOCK_SIZE + 1000];
-        Arrays.fill(tooBig, 'c');
-        StringElement input = new StringElement(new String(tooBig));
-        RecordIOWriter writer = new RecordIOWriter(file);
-        writer.writeEvent(input.serialize());
-        writer.writeEvent(input.serialize());
-
-        RecordIOReader reader = new RecordIOReader(file);
-        reader.seekToBlock(1);
-        reader.consumeBlock(true);
-        assertThat(reader.seekToStartOfEventInBlock(), equalTo(1026));
-
-        reader.close();
-        writer.close();
-    }
-
-
     @Test
     public void testFitsInTwoBlocks() throws Exception {
-        char[] tooBig = new char[BLOCK_SIZE + 1000];
-        Arrays.fill(tooBig, 'c');
+        char[] tooBig = fillArray(BLOCK_SIZE + 1000);
         StringElement input = new StringElement(new String(tooBig));
         RecordIOWriter writer = new RecordIOWriter(file);
         writer.writeEvent(input.serialize());
@@ -99,8 +50,7 @@ public void testFitsInTwoBlocks() throws Exception {
 
     @Test
     public void testFitsInThreeBlocks() throws Exception {
-        char[] tooBig = new char[2 * BLOCK_SIZE + 1000];
-        Arrays.fill(tooBig, 'r');
+        char[] tooBig = fillArray(2 * BLOCK_SIZE + 1000);
         StringElement input = new StringElement(new String(tooBig));
         RecordIOWriter writer = new RecordIOWriter(file);
         writer.writeEvent(input.serialize());
@@ -116,8 +66,7 @@ public void testFitsInThreeBlocks() throws Exception {
 
     @Test
     public void testReadWhileWrite() throws Exception {
-        char[] tooBig = new char[2 * BLOCK_SIZE + 1000];
-        Arrays.fill(tooBig, 'r');
+        char[] tooBig = fillArray(2 * BLOCK_SIZE + 1000);
         StringElement input = new StringElement(new String(tooBig));
         RecordIOWriter writer = new RecordIOWriter(file);
         RecordIOReader reader = new RecordIOReader(file);
@@ -150,41 +99,9 @@ public void testReadWhileWrite() throws Exception {
         reader.close();
     }
 
-    @Test
-    public void testReadMiddle() throws Exception {
-        char[] tooBig = new char[3 * BLOCK_SIZE + 1000];
-        Arrays.fill(tooBig, 'r');
-        StringElement input = new StringElement(new String(tooBig));
-        RecordIOWriter writer = new RecordIOWriter(file);
-        RecordIOReader reader = new RecordIOReader(file);
-        byte[] inputSerialized = input.serialize();
-
-        writer.writeEvent(inputSerialized);
-        reader.seekToBlock(1);
-        assertThat(reader.readEvent(), is(nullValue()));
-        writer.writeEvent(inputSerialized);
-        reader.seekToBlock(1);
-        assertThat(reader.readEvent(), is(not(nullValue())));
-
-        writer.close();
-        reader.close();
-    }
-
-    @Test
-    public void testFind() throws Exception {
-
-        RecordIOWriter writer = new RecordIOWriter(file);
-        RecordIOReader reader = new RecordIOReader(file);
-        ByteBuffer intBuffer = ByteBuffer.wrap(new byte[4]);
-        for (int i = 0; i < 20000; i++) {
-            intBuffer.rewind();
-            intBuffer.putInt(i);
-            writer.writeEvent(intBuffer.array());
-        }
-
-        Function<byte[], Object> toInt = (b) -> ByteBuffer.wrap(b).getInt();
-        reader.seekToNextEventPosition(34, toInt, (o1, o2) -> ((Integer) o1).compareTo((Integer) o2));
-        int nextVal = (int) toInt.apply(reader.readEvent());
-        assertThat(nextVal, equalTo(34));
+    private char[] fillArray(final int fillSize) {
+        char[] blockSize= new char[fillSize];
+        Arrays.fill(blockSize, 'e');
+        return blockSize;
     }
 }
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/instrument/metrics/MetricTypeTest.java b/logstash-core/src/test/java/org/logstash/instrument/metrics/MetricTypeTest.java
index 15dfa003c30..47e26d7cb65 100644
--- a/logstash-core/src/test/java/org/logstash/instrument/metrics/MetricTypeTest.java
+++ b/logstash-core/src/test/java/org/logstash/instrument/metrics/MetricTypeTest.java
@@ -27,6 +27,7 @@ public void ensurePassivity(){
         nameMap.put(MetricType.GAUGE_NUMERIC, "gauge/numeric");
         nameMap.put(MetricType.GAUGE_UNKNOWN, "gauge/unknown");
         nameMap.put(MetricType.GAUGE_RUBYHASH, "gauge/rubyhash");
+        nameMap.put(MetricType.GAUGE_RUBYTIMESTAMP, "gauge/rubytimestamp");
 
         //ensure we are testing all of the enumerations
         assertThat(EnumSet.allOf(MetricType.class).size()).isEqualTo(nameMap.size());
diff --git a/logstash-core/src/test/java/org/logstash/instrument/metrics/gauge/RubyTimeStampGaugeTest.java b/logstash-core/src/test/java/org/logstash/instrument/metrics/gauge/RubyTimeStampGaugeTest.java
new file mode 100644
index 00000000000..14985d71cb8
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/instrument/metrics/gauge/RubyTimeStampGaugeTest.java
@@ -0,0 +1,53 @@
+package org.logstash.instrument.metrics.gauge;
+
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.logstash.Timestamp;
+import org.logstash.ext.JrubyTimestampExtLibrary.RubyTimestamp;
+import org.logstash.instrument.metrics.MetricType;
+import org.mockito.Mock;
+import org.mockito.Mockito;
+import org.mockito.runners.MockitoJUnitRunner;
+
+import java.util.Collections;
+
+import static org.assertj.core.api.Assertions.assertThat;
+import static org.mockito.Mockito.when;
+
+/**
+ * Unit tests for {@link RubyTimeStampGauge}
+ */
+@RunWith(MockitoJUnitRunner.class)
+public class RubyTimeStampGaugeTest {
+
+    @Mock
+    private RubyTimestamp rubyTimestamp;
+
+    private final Timestamp timestamp = new Timestamp();
+
+    @Before
+    public void _setup() {
+        when(rubyTimestamp.getTimestamp()).thenReturn(timestamp);
+    }
+
+    @Test
+    public void getValue() {
+        RubyTimeStampGauge gauge = new RubyTimeStampGauge(Collections.singletonList("foo"), "bar", rubyTimestamp);
+        assertThat(gauge.getValue()).isEqualTo(rubyTimestamp.getTimestamp());
+        assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_RUBYTIMESTAMP);
+
+        //Null initialize
+        gauge = new RubyTimeStampGauge(Collections.singletonList("foo"), "bar");
+        assertThat(gauge.getValue()).isNull();
+        assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_RUBYTIMESTAMP);
+    }
+
+    @Test
+    public void set() {
+        RubyTimeStampGauge gauge = new RubyTimeStampGauge(Collections.singletonList("foo"), "bar", Mockito.mock(RubyTimestamp.class));
+        gauge.set(rubyTimestamp);
+        assertThat(gauge.getValue()).isEqualTo(rubyTimestamp.getTimestamp());
+        assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_RUBYTIMESTAMP);
+    }
+}
\ No newline at end of file
diff --git a/modules/netflow/configuration/logstash/netflow.conf.erb b/modules/netflow/configuration/logstash/netflow.conf.erb
index 06a10b43d9a..b4c90a67ae6 100644
--- a/modules/netflow/configuration/logstash/netflow.conf.erb
+++ b/modules/netflow/configuration/logstash/netflow.conf.erb
@@ -171,9 +171,9 @@ filter {
                 id => "netflow-v9-normalize-dst_port_from_tcp_dst_port"
                 rename => { "[netflow][tcp_dst_port]" => "[netflow][dst_port]" }
             }
-        } else if [netflow][udp_src_port] {
+        } else if [netflow][udp_dst_port] {
             mutate {
-                id => "netflow-v9-normalize-dst_port_from_udp_src_port"
+                id => "netflow-v9-normalize-dst_port_from_udp_dst_port"
                 rename => { "[netflow][udp_dst_port]" => "[netflow][dst_port]" }
             }
         }
@@ -603,4 +603,4 @@ filter {
 
 output {
 <%= elasticsearch_output_config() %>
-}
\ No newline at end of file
+}
diff --git a/qa/integration/fixtures/dlq_spec.yml b/qa/integration/fixtures/dlq_spec.yml
new file mode 100644
index 00000000000..8ca9e6d08bd
--- /dev/null
+++ b/qa/integration/fixtures/dlq_spec.yml
@@ -0,0 +1,38 @@
+---
+services:
+  - logstash
+  - elasticsearch
+config:
+  input {
+    generator{
+      message => '{"test":"one"}'
+      codec => "json"
+      count => 1000
+    }
+
+    dead_letter_queue {
+      path => "<%=options[:dlq_dir]%>"
+      commit_offsets => true
+    }
+  }
+
+  filter {
+    if ([geoip]) {
+        mutate {
+            remove_field => ["geoip"]
+            add_field => {
+              "mutated" => "true"
+            }
+        }
+    }else{
+      mutate {
+        add_field => {
+          "geoip" => "somewhere"
+        }
+      }
+    }
+  }
+  output {
+    elasticsearch {}
+  }
+teardown_script:
diff --git a/qa/integration/integration_tests.gemspec b/qa/integration/integration_tests.gemspec
index 7ef75c1cd13..8a95b4a015a 100644
--- a/qa/integration/integration_tests.gemspec
+++ b/qa/integration/integration_tests.gemspec
@@ -21,5 +21,4 @@ Gem::Specification.new do |s|
   s.add_development_dependency 'logstash-devutils'
   s.add_development_dependency 'flores'
   s.add_development_dependency 'rubyzip'
-  s.add_development_dependency 'docker-api'
 end
diff --git a/qa/integration/services/dockerized/Dockerfile b/qa/integration/services/dockerized/Dockerfile
deleted file mode 100644
index 045ba94de2c..00000000000
--- a/qa/integration/services/dockerized/Dockerfile
+++ /dev/null
@@ -1,32 +0,0 @@
-FROM debian:stretch
-##
-# Define a base image for all service images.
-##
-
-ENV _JAVA_OPTIONS "-Djava.net.preferIPv4Stack=true"
-ENV TERM=linux
-
-RUN apt-get update && apt-get install -y curl openjdk-8-jre-headless netcat
-
-RUN adduser --disabled-password --gecos '' tester
-USER tester
-
-# Define the work directory. Use this variable in derivated images and
-# shell scripts.
-ENV WORKDIR /home/tester
-WORKDIR $WORKDIR
-
-# Script with routines that can be used in derived images.
-ADD helpers.sh .
-
-# Expect this script in the context directory of a derived image. It should
-# contain the service setup instructions, and is going to be the first executed
-# command when a derived image is built.
-ONBUILD ADD setup.sh .
-ONBUILD RUN ./setup.sh
-
-# Expect this script in the context directory of a derived image. It should
-# contain the service start up instructions, and is going to be executed when
-# a container is started.
-ONBUILD ADD run.sh .
-CMD ["./run.sh"]
diff --git a/qa/integration/services/dockerized/elasticsearch/Dockerfile b/qa/integration/services/dockerized/elasticsearch/Dockerfile
deleted file mode 100644
index 55451ac9680..00000000000
--- a/qa/integration/services/dockerized/elasticsearch/Dockerfile
+++ /dev/null
@@ -1,3 +0,0 @@
-FROM logstash:ci_sandbox
-
-expose 9200 9300
diff --git a/qa/integration/services/dockerized/elasticsearch/run.sh b/qa/integration/services/dockerized/elasticsearch/run.sh
deleted file mode 100755
index fcf0005bf37..00000000000
--- a/qa/integration/services/dockerized/elasticsearch/run.sh
+++ /dev/null
@@ -1,6 +0,0 @@
-#!/bin/bash
-
-ES_HOME=${WORKDIR}/elasticsearch
-
-# Set "http.host" to make the service visible from outside the container.
-${ES_HOME}/bin/elasticsearch -E http.host=0.0.0.0
diff --git a/qa/integration/services/dockerized/elasticsearch/setup.sh b/qa/integration/services/dockerized/elasticsearch/setup.sh
deleted file mode 100755
index f2db6fd3bb3..00000000000
--- a/qa/integration/services/dockerized/elasticsearch/setup.sh
+++ /dev/null
@@ -1,15 +0,0 @@
-#!/bin/bash
-
-if [ -n "${ES_VERSION+1}" ]; then
-  echo "Elasticsearch version is $ES_VERSION"
-  version=$ES_VERSION
-else
-   version=5.0.1
-fi
-
-ES_HOME=${WORKDIR}/elasticsearch
-
-download_url=https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-$version.tar.gz
-curl -s -o elasticsearch.tar.gz $download_url
-mkdir -p $ES_HOME
-tar -xzf elasticsearch.tar.gz --strip-components=1 -C $ES_HOME/.
diff --git a/qa/integration/services/dockerized/helpers.sh b/qa/integration/services/dockerized/helpers.sh
deleted file mode 100644
index 7f3e9c0ffd2..00000000000
--- a/qa/integration/services/dockerized/helpers.sh
+++ /dev/null
@@ -1,23 +0,0 @@
-#!/bin/bash
-
-##
-# Add routines and/or variables that can be shared between the
-# service containers.
-##
-
-PORT_WAIT_COUNT=20
-
-# Check service responds on given port.
-# Parameters:
-#   - the port number.
-wait_for_port() {
-    count=$PORT_WAIT_COUNT
-    port=$1
-    while ! nc -z localhost $port && [[ $count -ne 0 ]]; do
-        count=$(( $count - 1 ))
-        [[ $count -eq 0 ]] && return 1
-        sleep 0.5
-    done
-    # just in case, one more time
-    nc -z localhost $port
-}
diff --git a/qa/integration/services/elasticsearch_service.rb b/qa/integration/services/elasticsearch_service.rb
index 3b45feb3688..66963aca5de 100644
--- a/qa/integration/services/elasticsearch_service.rb
+++ b/qa/integration/services/elasticsearch_service.rb
@@ -1,21 +1,12 @@
-require_relative "service_container"
 require 'elasticsearch'
-require 'docker'
 
-class ElasticsearchService < ServiceContainer
+class ElasticsearchService < Service
   def initialize(settings)
     super("elasticsearch", settings)
-
-    # Binding container to host ports.
-    @container_create_opts[:HostConfig] = {
-                                            :PortBindings => {
-                                              '9200/tcp' => [{ :HostPort => '9200' }],
-                                              '9300/tcp' => [{ :HostPort => '9300' }]
-                                          }}
   end
 
   def get_client
     Elasticsearch::Client.new(:hosts => "localhost:9200")
   end
 
-end
+end
\ No newline at end of file
diff --git a/qa/integration/services/elasticsearch_setup.sh b/qa/integration/services/elasticsearch_setup.sh
new file mode 100755
index 00000000000..30613dfa1c9
--- /dev/null
+++ b/qa/integration/services/elasticsearch_setup.sh
@@ -0,0 +1,44 @@
+#!/bin/bash
+set -ex
+current_dir="$(dirname "$0")"
+
+source "$current_dir/helpers.sh"
+
+if [ -n "${ES_VERSION+1}" ]; then
+  echo "Elasticsearch version is $ES_VERSION"
+  version=$ES_VERSION
+else
+   version=5.0.1
+fi
+
+ES_HOME=$INSTALL_DIR/elasticsearch
+
+setup_es() {
+  if [ ! -d $ES_HOME ]; then
+      local version=$1
+      download_url=https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-$version.tar.gz
+      curl -sL $download_url > $INSTALL_DIR/elasticsearch.tar.gz
+      mkdir $ES_HOME
+      tar -xzf $INSTALL_DIR/elasticsearch.tar.gz --strip-components=1 -C $ES_HOME/.
+      rm $INSTALL_DIR/elasticsearch.tar.gz
+  fi
+}
+
+start_es() {
+  es_args=$@
+  $ES_HOME/bin/elasticsearch $es_args -p $ES_HOME/elasticsearch.pid > /tmp/elasticsearch.log 2>/dev/null &
+  count=120
+  echo "Waiting for elasticsearch to respond..."
+  while ! curl --silent localhost:9200 && [[ $count -ne 0 ]]; do
+      count=$(( $count - 1 ))
+      [[ $count -eq 0 ]] && cat /tmp/elasticsearch.log && return 1
+      sleep 1
+  done
+  echo "Elasticsearch is Up !"
+  return 0
+}
+
+setup_install_dir
+setup_es $version
+export ES_JAVA_OPTS="-Xms512m -Xmx512m"
+start_es
diff --git a/qa/integration/services/elasticsearch_teardown.sh b/qa/integration/services/elasticsearch_teardown.sh
new file mode 100755
index 00000000000..f8e4dd51139
--- /dev/null
+++ b/qa/integration/services/elasticsearch_teardown.sh
@@ -0,0 +1,15 @@
+#!/bin/bash
+set -e
+current_dir="$(dirname "$0")"
+
+source "$current_dir/helpers.sh"
+
+ES_HOME=$INSTALL_DIR/elasticsearch
+
+stop_es() {
+    pid=$(cat $ES_HOME/elasticsearch.pid)
+    [ "x$pid" != "x" ] && [ "$pid" -gt 0 ]
+    kill -SIGTERM $pid
+}
+
+stop_es
\ No newline at end of file
diff --git a/qa/integration/services/kafka_dockerized/Dockerfile b/qa/integration/services/kafka_dockerized/Dockerfile
deleted file mode 100644
index 63abf910301..00000000000
--- a/qa/integration/services/kafka_dockerized/Dockerfile
+++ /dev/null
@@ -1,18 +0,0 @@
-FROM debian:stretch
-
-ENV KAFKA_HOME /kafka
-ENV KAFKA_LOGS_DIR="/kafka-logs"
-ENV KAFKA_VERSION 0.10.2.1
-ENV _JAVA_OPTIONS "-Djava.net.preferIPv4Stack=true"
-ENV TERM=linux
-
-RUN apt-get update && apt-get install -y curl openjdk-8-jre-headless netcat
-
-RUN mkdir -p ${KAFKA_LOGS_DIR} && mkdir -p ${KAFKA_HOME} && curl -s -o $INSTALL_DIR/kafka.tgz \
-    "http://ftp.wayne.edu/apache/kafka/${KAFKA_VERSION}/kafka_2.11-${KAFKA_VERSION}.tgz" && \
-    tar xzf ${INSTALL_DIR}/kafka.tgz -C ${KAFKA_HOME} --strip-components 1
-    
-ADD run.sh /run.sh
-
-EXPOSE 9092
-EXPOSE 2181
diff --git a/qa/integration/services/kafka_dockerized/run.sh b/qa/integration/services/kafka_dockerized/run.sh
deleted file mode 100644
index a01ffe32153..00000000000
--- a/qa/integration/services/kafka_dockerized/run.sh
+++ /dev/null
@@ -1,34 +0,0 @@
-#!/bin/bash
-
-KAFKA_TOPIC=logstash_topic_plain
-
-wait_for_port() {
-    count=20
-    port=$1
-    while ! nc -z localhost $port && [[ $count -ne 0 ]]; do
-        count=$(( $count - 1 ))
-        [[ $count -eq 0 ]] && return 1
-        sleep 0.5
-    done
-    # just in case, one more time
-    nc -z localhost $port
-}
-
-echo "Starting ZooKeeper"
-${KAFKA_HOME}/bin/zookeeper-server-start.sh ${KAFKA_HOME}/config/zookeeper.properties &
-wait_for_port 2181
-echo "Starting Kafka broker"
-mkdir -p ${KAFKA_LOGS_DIR}
-${KAFKA_HOME}/bin/kafka-server-start.sh ${KAFKA_HOME}/config/server.properties \
-    --override delete.topic.enable=true --override advertised.host.name=127.0.0.1 \
-    --override logs.dir=${KAFKA_LOGS_DIR} --override log.flush.interval.ms=200 &
-
-wait_for_port 9092
-
-${KAFKA_HOME}/bin/kafka-topics.sh --create --partitions 1 --replication-factor 1 --topic ${KAFKA_TOPIC} --zookeeper 127.0.0.1:2181
-
-${KAFKA_HOME}/bin/kafka-console-producer.sh --topic ${KAFKA_TOPIC} --broker-list 127.0.0.1:9092 < /how_sample.input
-
-echo "Kafka load status code $?"
-
-tail -f /dev/null
diff --git a/qa/integration/services/kafka_service.rb b/qa/integration/services/kafka_service.rb
index 0c550515560..71908f9acbb 100644
--- a/qa/integration/services/kafka_service.rb
+++ b/qa/integration/services/kafka_service.rb
@@ -1,35 +1,7 @@
 require_relative "service"
-require "docker"
-require "logstash/devutils/rspec/logstash_helpers"
 
 class KafkaService < Service
-  include LogStashHelper
-
   def initialize(settings)
     super("kafka", settings)
   end
-
-  def setup
-    try(20) do
-      @kafka_image = Docker::Image.build_from_dir(File.expand_path("../kafka_dockerized", __FILE__))
-                       .insert_local(
-                         'localPath' => File.join(TestSettings::FIXTURES_DIR, "how_sample.input"),
-                         'outputPath' => '/')
-    end
-    @kafka_container = Docker::Container.create(:Image => @kafka_image.id,
-                                                :HostConfig => {
-                                                  :PortBindings => {
-                                                    '9092/tcp' => [{ :HostPort => '9092' }],
-                                                    '2181/tcp' => [{ :HostPort => '2181' }]
-                                                  }
-                                                }, :Cmd => ["/bin/bash", "-l", "/run.sh"])
-    @kafka_container.start
-    super()
-  end
-
-  def teardown
-    @kafka_container.kill(:signal => "SIGHUP")
-    @kafka_container.delete(:force => true, :volumes => true)
-    super()
-  end
 end
diff --git a/qa/integration/services/kafka_setup.sh b/qa/integration/services/kafka_setup.sh
new file mode 100755
index 00000000000..9b4b1ca955d
--- /dev/null
+++ b/qa/integration/services/kafka_setup.sh
@@ -0,0 +1,72 @@
+#!/bin/bash
+set -ex
+current_dir="$(dirname "$0")"
+
+export _JAVA_OPTIONS="-Djava.net.preferIPv4Stack=true"
+
+source "$current_dir/helpers.sh"
+
+if [ -n "${KAFKA_VERSION+1}" ]; then
+    echo "KAFKA_VERSION is $KAFKA_VERSION"
+    version=$KAFKA_VERSION
+else
+    version=0.10.2.1
+fi
+
+KAFKA_HOME=$INSTALL_DIR/kafka
+KAFKA_TOPIC=logstash_topic_plain
+KAFKA_MESSAGES=37
+KAFKA_LOGS_DIR=/tmp/ls_integration/kafka-logs
+
+setup_kafka() {
+    local version=$1
+    if [ ! -d $KAFKA_HOME ]; then
+        echo "Downloading Kafka version $version"
+        curl -s -o $INSTALL_DIR/kafka.tgz "http://ftp.wayne.edu/apache/kafka/$version/kafka_2.11-$version.tgz"
+        mkdir $KAFKA_HOME && tar xzf $INSTALL_DIR/kafka.tgz -C $KAFKA_HOME --strip-components 1
+        rm $INSTALL_DIR/kafka.tgz
+    fi
+}
+
+start_kafka() {
+    echo "Starting ZooKeeper"
+    $KAFKA_HOME/bin/zookeeper-server-start.sh -daemon $KAFKA_HOME/config/zookeeper.properties
+    wait_for_port 2181
+    echo "Starting Kafka broker"
+    rm -rf ${KAFKA_LOGS_DIR}
+    mkdir -p ${KAFKA_LOGS_DIR}
+    $KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties --override delete.topic.enable=true --override advertised.host.name=127.0.0.1 --override log.dir=${KAFKA_LOGS_DIR} --override log.flush.interval.ms=200
+    wait_for_port 9092
+}
+
+wait_for_messages() {
+    local count=10
+    local read_lines=0
+    
+    echo "Checking if Kafka topic has been populated with data"
+    while [[ $read_lines -ne $KAFKA_MESSAGES ]] && [[ $count -ne 0 ]]; do
+        read_lines=`$KAFKA_HOME/bin/kafka-console-consumer.sh --topic $KAFKA_TOPIC --new-consumer --bootstrap-server localhost:9092 --from-beginning --max-messages $KAFKA_MESSAGES --timeout-ms 10000 | wc -l`
+        count=$(( $count - 1 ))
+        [[ $count -eq 0 ]] && return 1
+        sleep 0.5
+        #ls -lrt $KAFKA_LOGS_DIR/$KAFKA_TOPIC-0/
+    done
+    echo "Kafka topic has been populated with test data"
+}
+
+setup_install_dir
+setup_kafka $version
+start_kafka
+# Set up topics
+$KAFKA_HOME/bin/kafka-topics.sh --create --partitions 1 --replication-factor 1 --topic $KAFKA_TOPIC --zookeeper localhost:2181
+# check topic got created
+num_topic=`$KAFKA_HOME/bin/kafka-topics.sh --list --zookeeper localhost:2181 | grep $KAFKA_TOPIC | wc -l`
+[[ $num_topic -eq 1 ]]
+# Add test messages to the newly created topic
+cp $current_dir/../fixtures/how_sample.input $KAFKA_HOME
+[[ ! -s  how_sample.input ]]
+$KAFKA_HOME/bin/kafka-console-producer.sh --topic $KAFKA_TOPIC --broker-list localhost:9092 < $KAFKA_HOME/how_sample.input
+echo "Kafka load status code $?"
+# Wait until broker has all messages
+wait_for_messages
+echo "Kafka Setup complete"
diff --git a/qa/integration/services/kafka_teardown.sh b/qa/integration/services/kafka_teardown.sh
new file mode 100755
index 00000000000..0d10cffe844
--- /dev/null
+++ b/qa/integration/services/kafka_teardown.sh
@@ -0,0 +1,22 @@
+#!/bin/bash
+set -ex
+current_dir="$(dirname "$0")"
+
+source "$current_dir/helpers.sh"
+
+KAFKA_HOME=$INSTALL_DIR/kafka
+
+stop_kafka() {
+    echo "Stopping Kafka broker"
+    $KAFKA_HOME/bin/kafka-server-stop.sh
+    echo "Stopping zookeeper"
+    $KAFKA_HOME/bin/zookeeper-server-stop.sh
+}
+
+# delete test topic
+echo "Deleting test topic in Kafka"
+$KAFKA_HOME/bin/kafka-topics.sh --delete --topic logstash_topic_plain --zookeeper localhost:2181 --if-exists
+stop_kafka
+rm -rf /tmp/ls_integration/kafka-logs
+rm -rf /tmp/zookeeper
+
diff --git a/qa/integration/services/logstash_service.rb b/qa/integration/services/logstash_service.rb
index d00252b4e52..21d821c37da 100644
--- a/qa/integration/services/logstash_service.rb
+++ b/qa/integration/services/logstash_service.rb
@@ -37,7 +37,7 @@ def initialize(settings)
       ls_file = "logstash-" + ls_version_file["logstash"]
       # First try without the snapshot if it's there
       @logstash_home = File.expand_path(File.join(LS_BUILD_DIR, ls_file), __FILE__)
-      @logstash_home += "-SNAPSHOT" unless Dir.exists?(@logstash_home)
+      @logstash_home += "-SNAPSHOT" unless Dir.exist?(@logstash_home)
 
       puts "Using #{@logstash_home} as LS_HOME"
       @logstash_bin = File.join("#{@logstash_home}", LS_BIN)
@@ -77,6 +77,10 @@ def start_with_input(config, input)
     end
   end
 
+  def start_background_with_config_settings(config, settings_file)
+    spawn_logstash("-f", "#{config}", "--path.settings", settings_file)
+  end
+
   def start_with_config_string(config)
     spawn_logstash("-e", "#{config} ")
   end
diff --git a/qa/integration/services/service_container.rb b/qa/integration/services/service_container.rb
deleted file mode 100644
index 598cd04fa4c..00000000000
--- a/qa/integration/services/service_container.rb
+++ /dev/null
@@ -1,67 +0,0 @@
-require_relative "service"
-require "docker"
-require "logstash/devutils/rspec/logstash_helpers"
-
-# Represents a service running within a container.
-class ServiceContainer < Service
-  include LogStashHelper
-
-  def initialize(name, settings)
-    super(name, settings)
-
-    @base_image_context = File.expand_path("../dockerized", __FILE__)
-
-    @image_context = File.join(@base_image_context, @name)
-
-    # Options to create the container.
-    @container_create_opts = {}
-  end
-
-  def setup
-    puts "Setting up #{@name} service."
-
-    puts "Building the base container image."
-    @base_image = Docker::Image.build_from_dir(@base_image_context)
-    # Tag the base image.
-    #Caution: change this tag can cause failure to build the service container.
-    @base_image.tag('repo' => 'logstash', 'tag' => 'ci_sandbox', force: true)
-    puts "Finished building the base image."
-
-    puts "Building the container image."
-    self.build_image
-    puts "Finished building the image."
-
-    puts "Starting the container."
-    self.start_container
-    puts "Finished starting the container."
-
-    puts "Finished setting up #{@name} service."
-  end
-
-  def teardown
-    puts "Tearing down #{@name} service."
-
-    puts "Stop the container."
-    self.stop_container
-    puts "Finished stopping the container."
-
-    puts "Finished tearing down of #{@name} service."
-  end
-
-  def build_image
-    try(20) do
-      @image = Docker::Image.build_from_dir(@image_context)
-    end
-  end
-
-  def start_container
-    @container_create_opts[:Image] = @image.id
-    @container = Docker::Container.create(@container_create_opts)
-    @container.start
-  end
-
-  def stop_container
-    @container.stop
-    @container.delete(:force => true, :volumes => true)
-  end
-end
diff --git a/qa/integration/specs/dlq_spec.rb b/qa/integration/specs/dlq_spec.rb
new file mode 100644
index 00000000000..6ef2d59f592
--- /dev/null
+++ b/qa/integration/specs/dlq_spec.rb
@@ -0,0 +1,127 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+require_relative '../framework/helpers'
+require "logstash/devutils/rspec/spec_helper"
+
+describe "Test Dead Letter Queue" do
+
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+  }
+
+  after(:all) {
+      @fixture.teardown
+  }
+
+  before(:each) {
+    IO.write(config_yaml_file, config_yaml)
+  }
+
+  after(:each) do
+    es_client = @fixture.get_service("elasticsearch").get_client
+    es_client.indices.delete(index: 'logstash-*') unless es_client.nil?
+    logstash_service.teardown
+  end
+
+  let(:logstash_service) { @fixture.get_service("logstash") }
+  let(:dlq_dir) { Stud::Temporary.directory }
+  let(:dlq_config) {
+      {
+          "dead_letter_queue.enable" => true,
+          "path.dead_letter_queue" => dlq_dir,
+          "log.level" => "debug"
+      }
+  }
+  let!(:config_yaml) { dlq_config.to_yaml }
+  let!(:config_yaml_file) { ::File.join(settings_dir, "logstash.yml") }
+
+  let!(:settings_dir) { Stud::Temporary.directory }
+
+  shared_examples_for "it can send 1000 documents to and index from the dlq" do
+    it 'should index all documents' do
+      es_service = @fixture.get_service("elasticsearch")
+      es_client = es_service.get_client
+      # test if all data was indexed by ES, but first refresh manually
+      es_client.indices.refresh
+
+      logstash_service.wait_for_logstash
+      try(60) do
+        begin
+          result = es_client.search(index: 'logstash-*', size: 0, q: '*')
+          hits = result["hits"]["total"]
+        rescue Elasticsearch::Transport::Transport::Errors::ServiceUnavailable => e
+          puts "Elasticsearch unavailable #{e.inspect}"
+          hits = 0
+        end
+        expect(hits).to eq(1000)
+      end
+
+      result = es_client.search(index: 'logstash-*', size: 1, q: '*')
+      s = result["hits"]["hits"][0]["_source"]
+      expect(s["mutated"]).to eq("true")
+    end
+  end
+
+  context 'using pipelines.yml' do
+    let!(:pipelines_yaml) { pipelines.to_yaml }
+    let!(:pipelines_yaml_file) { ::File.join(settings_dir, "pipelines.yml") }
+
+    before :each do
+      IO.write(pipelines_yaml_file, pipelines_yaml)
+      logstash_service.spawn_logstash("--path.settings", settings_dir, "--log.level=debug")
+    end
+
+    context 'with multiple pipelines' do
+      let(:pipelines) {[
+          {
+              "pipeline.id" => "test",
+              "pipeline.workers" => 1,
+              "dead_letter_queue.enable" => true,
+              "pipeline.batch.size" => 1,
+              "config.string" => "input { generator { message => '{\"test\":\"one\"}' codec => \"json\" count => 1000 } } filter { mutate { add_field => { \"geoip\" => \"somewhere\" } } } output { elasticsearch {} }"
+          },
+          {
+              "pipeline.id" => "test2",
+              "pipeline.workers" => 1,
+              "dead_letter_queue.enable" => false,
+              "pipeline.batch.size" => 1,
+              "config.string" => "input { dead_letter_queue { pipeline_id => 'test' path => \"#{dlq_dir}\" commit_offsets => true } } filter { mutate { remove_field => [\"geoip\"] add_field => {\"mutated\" => \"true\" } } } output { elasticsearch {} }"
+          }
+      ]}
+
+      it_behaves_like 'it can send 1000 documents to and index from the dlq'
+    end
+
+    context 'with a single pipeline' do
+      let(:pipelines) {[
+        {
+            "pipeline.id" => "main",
+            "pipeline.workers" => 1,
+            "dead_letter_queue.enable" => true,
+            "pipeline.batch.size" => 1,
+            "config.string" => "
+                input { generator{ message => '{\"test\":\"one\"}' codec => \"json\" count => 1000 }
+                        dead_letter_queue { path => \"#{dlq_dir}\" commit_offsets => true }
+                }
+                filter {
+                  if ([geoip]) { mutate { remove_field => [\"geoip\"] add_field => { \"mutated\" => \"true\" } } }
+                  else{ mutate { add_field => { \"geoip\" => \"somewhere\" } } }
+                }
+                output { elasticsearch {} }"
+        }
+      ]}
+
+      it_behaves_like 'it can send 1000 documents to and index from the dlq'
+    end
+  end
+
+  context 'using logstash.yml and separate config file' do
+    let(:generator_config_file) { config_to_temp_file(@fixture.config("root",{ :dlq_dir => dlq_dir })) }
+
+    before :each do
+      logstash_service.start_background_with_config_settings(generator_config_file, settings_dir)
+    end
+    it_behaves_like 'it can send 1000 documents to and index from the dlq'
+  end
+end
diff --git a/rakelib/fetch.rake b/rakelib/fetch.rake
index f871cdef4c9..1e5a49fea50 100644
--- a/rakelib/fetch.rake
+++ b/rakelib/fetch.rake
@@ -10,7 +10,7 @@ def fetch(url, sha1, output)
   puts "Downloading #{url}"
   actual_sha1 = download(url, output)
 
-  if actual_sha1 != sha1
+  if sha1 != "IGNORE" && actual_sha1 != sha1
     fail "SHA1 does not match (expected '#{sha1}' but got '#{actual_sha1}')"
   end
 end # def fetch
@@ -21,7 +21,7 @@ def file_fetch(url, sha1)
   task output => [ "vendor/_" ] do
     begin
       actual_sha1 = file_sha1(output)
-      if actual_sha1 != sha1
+      if sha1 != "IGNORE" && actual_sha1 != sha1
         fetch(url, sha1, output)
       end
     rescue Errno::ENOENT
diff --git a/rakelib/modules.rake b/rakelib/modules.rake
index f866d078512..2b995ce68ee 100644
--- a/rakelib/modules.rake
+++ b/rakelib/modules.rake
@@ -23,7 +23,7 @@ namespace "modules" do
       full_path = ::File.join(partial_path, filename)
       FileUtils.rm_f(full_path)
 
-      content = JSON.pretty_generate(source)
+      content = JSON.pretty_generate(source) + "\n"
       puts "Writing #{full_path}"
       IO.write(full_path, content)
     end
@@ -42,7 +42,7 @@ namespace "modules" do
     full_path = ::File.join(dashboard_dir, "#{module_name}.json")
     FileUtils.rm_f(full_path)
 
-    content = JSON.pretty_generate(filenames)
+    content = JSON.pretty_generate(filenames) + "\n"
     puts "Writing #{full_path}"
     IO.write(full_path, content)
   end
diff --git a/rakelib/vendor.rake b/rakelib/vendor.rake
index fa562c1b48b..687a540d303 100644
--- a/rakelib/vendor.rake
+++ b/rakelib/vendor.rake
@@ -60,9 +60,12 @@ namespace "vendor" do
   end # def untar
 
   task "jruby" do |task, args|
-    name = task.name.split(":")[1]
-    info = VERSIONS[name]
+    JRUBY = "jruby"
+    JRUBY_RUNTIME = "jruby-runtime-override"
+
+    info = VERSIONS[JRUBY_RUNTIME] || VERSIONS[JRUBY]
     version = info["version"]
+    url = info["url"] || "http://jruby.org.s3.amazonaws.com/downloads/#{version}/jruby-bin-#{version}.tar.gz"
 
     discard_patterns = Regexp.union([
       /^samples/,
@@ -72,10 +75,9 @@ namespace "vendor" do
       /lib\/ruby\/shared\/rdoc/,
     ])
 
-    url = "http://jruby.org.s3.amazonaws.com/downloads/#{version}/jruby-bin-#{version}.tar.gz"
     download = file_fetch(url, info["sha1"])
 
-    parent = vendor(name).gsub(/\/$/, "")
+    parent = vendor(JRUBY).gsub(/\/$/, "")
     directory parent => "vendor" do
       next if parent =~ discard_patterns
       FileUtils.mkdir(parent)
@@ -85,7 +87,7 @@ namespace "vendor" do
     untar(download) do |entry|
       out = entry.full_name.gsub(prefix_re, "")
       next if out =~ discard_patterns
-      vendor(name, out)
+      vendor(JRUBY, out)
     end # untar
   end # jruby
 
diff --git a/spec/unit/plugin_manager/pack_fetch_strategy/uri_spec.rb b/spec/unit/plugin_manager/pack_fetch_strategy/uri_spec.rb
index 09effdb909c..210b32d5b9c 100644
--- a/spec/unit/plugin_manager/pack_fetch_strategy/uri_spec.rb
+++ b/spec/unit/plugin_manager/pack_fetch_strategy/uri_spec.rb
@@ -32,10 +32,12 @@
     let(:temporary_file) do
       f = Stud::Temporary.file
       f.write("hola")
+      f.close
       f.path
     end
 
-    let(:plugin_path) { "file://#{temporary_file}" }
+    # Windows safe way to produce a file: URI.
+    let(:plugin_path) { URI.join("file:///" + File.absolute_path(temporary_file)).to_s }
 
     it "returns a `LocalInstaller`" do
       expect(subject.get_installer_for(plugin_path)).to be_kind_of(LogStash::PluginManager::PackInstaller::Local)
diff --git a/spec/unit/plugin_manager/prepare_offline_pack_spec.rb b/spec/unit/plugin_manager/prepare_offline_pack_spec.rb
index aa9376c91cf..9c55457d0dd 100644
--- a/spec/unit/plugin_manager/prepare_offline_pack_spec.rb
+++ b/spec/unit/plugin_manager/prepare_offline_pack_spec.rb
@@ -79,6 +79,10 @@
         expect(LogStash::PluginManager::OfflinePluginPackager).not_to receive(:package).with(anything)
       end
 
+      after do
+        FileUtils.rm_rf(tmp_zip_file)
+      end
+
       it "fails to do any action" do
         expect { subject.run(cmd_args) }.to raise_error Clamp::ExecutionError, /you must specify a filename/
       end
@@ -101,13 +105,18 @@
         FileUtils.touch(tmp_zip_file)
       end
 
+      after do
+        FileUtils.rm_f(tmp_zip_file)
+      end
+
       context "without `--overwrite`" do
         before do
           expect(LogStash::PluginManager::OfflinePluginPackager).not_to receive(:package).with(anything)
         end
 
         it "should fails" do
-          expect { subject.run(cmd_args) }.to raise_error Clamp::ExecutionError, /output file destination #{tmp_zip_file} already exist/
+          # ignore the first path part of tmp_zip_file because on Windows the long path is shrinked in the exception message 
+          expect { subject.run(cmd_args) }.to raise_error Clamp::ExecutionError, /output file destination .+#{::File.basename(tmp_zip_file)} already exist/
         end
       end
 
diff --git a/spec/unit/plugin_manager/utils/downloader_spec.rb b/spec/unit/plugin_manager/utils/downloader_spec.rb
index e08e731af01..2f7a105eb95 100644
--- a/spec/unit/plugin_manager/utils/downloader_spec.rb
+++ b/spec/unit/plugin_manager/utils/downloader_spec.rb
@@ -56,7 +56,7 @@
       let(:temporary_path) { Stud::Temporary.pathname }
 
       before do
-        expect_any_instance_of(::File).to receive(:close).at_least(:twice).and_raise("Didn't work")
+        expect(Net::HTTP::Get).to receive(:new).once.and_raise("Didn't work")
         expect(Stud::Temporary).to receive(:pathname).and_return(temporary_path)
       end
 
diff --git a/tools/benchmark-cli/build.gradle b/tools/benchmark-cli/build.gradle
index e3ca5cad0ce..07ee7d305e8 100644
--- a/tools/benchmark-cli/build.gradle
+++ b/tools/benchmark-cli/build.gradle
@@ -30,6 +30,7 @@ dependencies {
   compile 'net.sf.jopt-simple:jopt-simple:5.0.3'
   compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.3'
   compile group: 'org.apache.commons', name: 'commons-compress', version: '1.14'
+  compile group: 'org.apache.commons', name: 'commons-lang3', version: '3.6'
   compile group: 'commons-io', name: 'commons-io', version: '2.5'
   compile 'com.fasterxml.jackson.core:jackson-core:2.7.4'
   compile 'com.fasterxml.jackson.core:jackson-databind:2.7.4'
diff --git a/tools/benchmark-cli/src/main/java/org/logstash/benchmark/cli/LogstashInstallation.java b/tools/benchmark-cli/src/main/java/org/logstash/benchmark/cli/LogstashInstallation.java
index 9cbf7139dca..69df041e547 100644
--- a/tools/benchmark-cli/src/main/java/org/logstash/benchmark/cli/LogstashInstallation.java
+++ b/tools/benchmark-cli/src/main/java/org/logstash/benchmark/cli/LogstashInstallation.java
@@ -127,8 +127,9 @@ public void execute(final String configuration, final File data)
             );
             final Path lsbin = location.resolve("bin").resolve("logstash");
             LsBenchFileUtil.ensureExecutable(lsbin.toFile());
+            final File output = Files.createTempFile(null, null).toFile();
             final Process process = pbuilder.command(lsbin.toString(), "-w", "2", "-f", cfg.toString()).redirectOutput(
-                ProcessBuilder.Redirect.to(new File("/dev/null"))
+                ProcessBuilder.Redirect.to(output)
             ).start();
             if (data != null) {
                 try (final InputStream file = new FileInputStream(data);
@@ -140,6 +141,7 @@ public void execute(final String configuration, final File data)
                 throw new IllegalStateException("Logstash failed to start!");
             }
             LsBenchFileUtil.ensureDeleted(cfg.toFile());
+            LsBenchFileUtil.ensureDeleted(output);
         }
 
         @Override
diff --git a/tools/benchmark-cli/src/main/java/org/logstash/benchmark/cli/ui/UserOutput.java b/tools/benchmark-cli/src/main/java/org/logstash/benchmark/cli/ui/UserOutput.java
index 7bb82a194a5..56dfa4b684d 100644
--- a/tools/benchmark-cli/src/main/java/org/logstash/benchmark/cli/ui/UserOutput.java
+++ b/tools/benchmark-cli/src/main/java/org/logstash/benchmark/cli/ui/UserOutput.java
@@ -4,9 +4,22 @@
 import java.time.ZonedDateTime;
 import java.time.format.DateTimeFormatter;
 import java.time.format.DateTimeFormatterBuilder;
+import org.apache.commons.lang3.SystemUtils;
 
 public final class UserOutput {
 
+    /**
+     * ANSI colorized green section open sequence(On Unix platform only).
+     */
+    private static final String GREEN_ANSI_OPEN = SystemUtils.IS_OS_UNIX ? "\u001B[32m" : "";
+
+    /**
+     * ANSI colorized section close sequence(On Unix platform only).
+     */
+    private static final String ANSI_CLOSE = SystemUtils.IS_OS_UNIX ? "\u001B[0m" : "";
+
+    private static final String BANNER = "Logstash Benchmark";
+
     private static final DateTimeFormatter DATE_TIME_FORMATTER = new DateTimeFormatterBuilder()
         .append(DateTimeFormatter.ofPattern("E")).appendLiteral(' ')
         .append(DateTimeFormatter.ofPattern("L")).appendLiteral(' ')
@@ -14,7 +27,7 @@ public final class UserOutput {
         .append(DateTimeFormatter.ISO_LOCAL_TIME).appendLiteral(' ')
         .append(DateTimeFormatter.ofPattern("yyyy")).appendLiteral(' ')
         .append(DateTimeFormatter.ofPattern("z")).toFormatter();
-    
+
     private final PrintStream target;
 
     public UserOutput(final PrintStream target) {
@@ -34,29 +47,15 @@ public void printLine() {
     }
 
     public void printBanner() {
-        green(
-            "██╗      ██████╗  ██████╗ ███████╗████████╗ █████╗ ███████╗██╗  ██╗          \n" +
-                "██║     ██╔═══██╗██╔════╝ ██╔════╝╚══██╔══╝██╔══██╗██╔════╝██║  ██║          \n" +
-                "██║     ██║   ██║██║  ███╗███████╗   ██║   ███████║███████╗███████║          \n" +
-                "██║     ██║   ██║██║   ██║╚════██║   ██║   ██╔══██║╚════██║██╔══██║          \n" +
-                "███████╗╚██████╔╝╚██████╔╝███████║   ██║   ██║  ██║███████║██║  ██║          \n" +
-                "╚══════╝ ╚═════╝  ╚═════╝ ╚══════╝   ╚═╝   ╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝          \n" +
-                "                                                                             \n" +
-                "██████╗ ███████╗███╗   ██╗ ██████╗██╗  ██╗███╗   ███╗ █████╗ ██████╗ ██╗  ██╗\n" +
-                "██╔══██╗██╔════╝████╗  ██║██╔════╝██║  ██║████╗ ████║██╔══██╗██╔══██╗██║ ██╔╝\n" +
-                "██████╔╝█████╗  ██╔██╗ ██║██║     ███████║██╔████╔██║███████║██████╔╝█████╔╝ \n" +
-                "██╔══██╗██╔══╝  ██║╚██╗██║██║     ██╔══██║██║╚██╔╝██║██╔══██║██╔══██╗██╔═██╗ \n" +
-                "██████╔╝███████╗██║ ╚████║╚██████╗██║  ██║██║ ╚═╝ ██║██║  ██║██║  ██║██║  ██╗\n" +
-                "╚═════╝ ╚══════╝╚═╝  ╚═══╝ ╚═════╝╚═╝  ╚═╝╚═╝     ╚═╝╚═╝  ╚═╝╚═╝  ╚═╝╚═╝  ╚═╝\n" +
-                "                                                                             ");
+        green(BANNER);
     }
 
     public void green(final String line) {
-        target.println(colorize(line, "\u001B[32m"));
+        target.println(colorize(line, GREEN_ANSI_OPEN));
     }
 
     private static String colorize(final String line, final String prefix) {
-        final String reset = "\u001B[0m";
+        final String reset = ANSI_CLOSE;
         return new StringBuilder(line.length() + 2 * reset.length())
             .append(prefix).append(line).append(reset).toString();
     }
diff --git a/tools/benchmark-cli/src/test/java/org/logstash/benchmark/cli/LsMetricsMonitorTest.java b/tools/benchmark-cli/src/test/java/org/logstash/benchmark/cli/LsMetricsMonitorTest.java
index 693c8899d0a..4af3ce4dd7b 100644
--- a/tools/benchmark-cli/src/test/java/org/logstash/benchmark/cli/LsMetricsMonitorTest.java
+++ b/tools/benchmark-cli/src/test/java/org/logstash/benchmark/cli/LsMetricsMonitorTest.java
@@ -2,14 +2,16 @@
 
 import com.github.tomakehurst.wiremock.client.WireMock;
 import com.github.tomakehurst.wiremock.junit.WireMockRule;
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
 import java.nio.charset.StandardCharsets;
-import java.nio.file.Files;
-import java.nio.file.Paths;
 import java.util.EnumMap;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
 import java.util.concurrent.Future;
 import java.util.concurrent.TimeUnit;
+import org.apache.commons.io.IOUtils;
 import org.hamcrest.CoreMatchers;
 import org.hamcrest.MatcherAssert;
 import org.junit.Rule;
@@ -30,11 +32,7 @@ public final class LsMetricsMonitorTest {
     public void parsesFilteredCount() throws Exception {
         final String path = "/_node/stats/?pretty";
         http.stubFor(WireMock.get(WireMock.urlEqualTo(path)).willReturn(WireMock.okJson(
-            new String(
-                Files.readAllBytes(
-                    Paths.get(LsMetricsMonitorTest.class.getResource("metrics.json").getPath()
-                    ))
-                , StandardCharsets.UTF_8)
+            metricsFixture()
         )));
         final ExecutorService executor = Executors.newSingleThreadExecutor();
         try {
@@ -54,11 +52,7 @@ public void parsesFilteredCount() throws Exception {
     public void parsesCpuUsage() throws Exception {
         final String path = "/_node/stats/?pretty";
         http.stubFor(WireMock.get(WireMock.urlEqualTo(path)).willReturn(WireMock.okJson(
-            new String(
-                Files.readAllBytes(
-                    Paths.get(LsMetricsMonitorTest.class.getResource("metrics.json").getPath()
-                    ))
-                , StandardCharsets.UTF_8)
+            metricsFixture()
         )));
         final ExecutorService executor = Executors.newSingleThreadExecutor();
         try {
@@ -73,4 +67,13 @@ public void parsesCpuUsage() throws Exception {
             executor.shutdownNow();
         }
     }
+
+    private static String metricsFixture() throws IOException {
+        final ByteArrayOutputStream baos = new ByteArrayOutputStream();
+        try (final InputStream input = LsMetricsMonitorTest.class
+            .getResourceAsStream("metrics.json")) {
+            IOUtils.copy(input, baos);
+        }
+        return baos.toString(StandardCharsets.UTF_8.name());
+    }
 }
diff --git a/versions.yml b/versions.yml
index 52fd4875c46..e91c085a417 100644
--- a/versions.yml
+++ b/versions.yml
@@ -1,7 +1,7 @@
 ---
-logstash: 6.0.0-beta1
-logstash-core: 6.0.0-beta1
+logstash: 6.0.0-rc1
+logstash-core: 6.0.0-rc1
 logstash-core-plugin-api: 2.1.16
 jruby:
-  version: 9.1.12.0
-  sha1: 9b6c15d42eb11db7215d94dd9842ee5184a41fea
+  version: 9.1.13.0
+  sha1: 815bac27d5daa1459a4477d6d80584f007ce6a68
