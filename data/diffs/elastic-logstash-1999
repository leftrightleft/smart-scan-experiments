diff --git a/.gitignore b/.gitignore
index 7df4327e6fb..d8e74989adf 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,7 +1,27 @@
 .*.swp
 *.gem
+pkg/*.deb
+pkg/*.rpm
 *.class
+.rbx
+Gemfile.lock
+.rbx
 *.tar.gz
 *.jar
+.bundle
+build
 local
 test/setup/elasticsearch/elasticsearch-*
+vendor
+.sass-cache
+data
+.buildpath
+.project
+.DS_Store
+*.pyc
+etc/jira-output.conf
+coverage/*
+.VERSION.mk
+.idea/*
+spec/reports
+rspec.xml
diff --git a/.rvmrc b/.rvmrc
deleted file mode 100644
index 7a3867dbb8d..00000000000
--- a/.rvmrc
+++ /dev/null
@@ -1 +0,0 @@
-rvm jruby-1.6.0
diff --git a/.tailor b/.tailor
new file mode 100644
index 00000000000..5e883dba31d
--- /dev/null
+++ b/.tailor
@@ -0,0 +1,8 @@
+Tailor.config do |config|
+  config.file_set '*.rb' do |style|
+    style.indentation_spaces 2, :level => :off
+    style.max_line_length 80, :level => :off
+    style.allow_trailing_line_spaces true, :level => :off
+    style.spaces_after_comma false, :level => :off
+  end
+end
diff --git a/CHANGELOG b/CHANGELOG
index b1c09f5bc8f..8c55f513fcf 100644
--- a/CHANGELOG
+++ b/CHANGELOG
@@ -1,3 +1,1369 @@
+1.4.2 (June 24, 2014)
+  # general
+  - fixed path issues when invoking bin/logstash outside its home directory
+
+  # input
+  - bugfix: generator: fixed stdin option support
+  - bugfix: file: fixed debian 7 path issue
+
+  # codecs
+  - improvement: stdin/tcp: automatically select json_line and line codecs with the tcp and stdin streaming imputs
+  - improvement: collectd: add support for NaN values
+
+  # outputs
+  - improvement: nagios_nsca: fix external command invocation to avoid shell escaping
+
+1.4.1 (May 6, 2014)
+  # General
+  - bumped Elasticsearch to 1.1.1 and Kibana to 3.0.1
+  - improved specs & testing (Colin Surprenant), packaging (Richard Pijnenburg) & doc (James Turnbull)
+  - better $JAVA_HOME handling (Marc Chadwick)
+  - fixed bin/plugin target dir for when installing out from form logstash home (lr1980)
+  - fixed Accessors reset bug in Event#overwrite that was causing the infamous
+    "undefined method `tv_sec'" bug with the multiline filter (Colin Surprenant)
+  - fixed agent stalling when also using web option (Colin Surprenant)
+  - fixed accessing array-indexed event fields (Jonathan Van Eenwyk)
+  - new sysv init style scripts based on pleaserun (Richard Pijnenburg)
+  - better handling of invalid command line parameters (LOGSTASH-2024, Colin Surprenant)
+  - fixed running from a path containing spaces (LOGSTASH-1983, Colin Surprenant)
+
+  # inputs
+  - improvement: rabbitmq: upgraded Bunny gem to 1.1.8, fixes a threading leak and improves
+    latency (Michael Klishin)
+  - improvement: twitter: added "full_tweet" option (Jordan Sissel)
+  - improvement: generator: fixed the example doc (LOGSTASH-2093, Jason Kendall)
+  - improvement: imap: option to disable certificate validation (Sverre Bakke)
+
+  # codecs
+  - new: collectd: better performance & error handling than collectd input (Aaron Mildenstein)
+  - improvement: graphite: removed unused charset option (Colin Surprenant)
+  - improvement: json_spooler: is now deprecated (Colin Surprenant)
+  - improvement: proper charset support in all codecs (Colin Surprenant)
+
+  # filters
+  - bugfix: date: on_success actions only when date parsing actually succeed (Philippe Weber)
+  - bugfix: multiline: "undefined method `tv_sec'" fix (Colin Surprenant)
+  - bugfix: multiline: fix for "undefined method `[]' for nil:NilClass" (#1258, Colin Surprenant)
+  - improvement: date: fix specs for non "en" locale (Olivier Le Moal)
+  - improvement: grok: better pattern for RFC-5424 syslog format (Guillaume Espanel)
+  - improvement: grok: refactored the LOGLEVEL pattern (Lorenzo Gonz√°lez)
+  - improvement: grok: fix example doc (LOGSTASH-2093, Jason Kendall)
+  - improvement: metrics: document .pXX metric (Juarez Bochi)
+
+  # outputs
+  - improvement: rabbitmq: upgraded Bunny gem to 1.1.8, fixes a threading leak and improves
+    latency (Michael Klishin)
+  - improvement: elasticsearch: start embedded server before creating a client to fix discovery
+    problems "waited for 30s ..." (Jordan Sissel)
+  - improvement: elasticsearch: have embedded ES use "bind_host" option for "network.host"
+    ES config (Jordan Sissel)
+
+1.4.0 (March 20, 2014)
+  # General
+  - We've included some upgrade-specific release notes with more details about
+    the tarball changes and contrib packaging here:
+    http://logstash.net/docs/1.4.0/release-notes
+  - Ships with Kibana 3.0.0
+  - Much faster field reference implementation (Colin Surprenant)
+  - Fix a bug in character encoding which would cause inputs using non-UTF-8
+    codecs to accidentally skip re-encoding the text to UTF-8. This should
+    solve a great number of UTF-8-related bugs. (Colin Surprenant)
+  - Fixes missing gem  for logstash web which was broken in 1.4.0 beta1
+    (LOGSTASH-1918, Jordan Sissel)
+  - Fix 'help' output being emitted twice when --help is invoked.
+    (LOGSTASH-1952, #1168)
+  - Logstash now supports deletes! See outputs section below.
+  - Update template to fit ES 1.0 API changes (untergeek)
+  - Lots of Makefile, gem and build improvements courtesy of untergeek, Faye
+    Salwin, mrsolo, ronnocol, electrical, et al
+  - Add `env` command so you can run arbitrary commands with the logstash
+    environment setup (jordansissel)
+  - Bug fixes (lots).  Did I mention bug fixes? (Thanks, community!)
+  - Elasticsearch 1.0 libraries are now included. See the Elasticsearch
+    release notes for details: http://www.elasticsearch.org/downloads/1-0-0/
+  - Kibana 3 milestone 5 is included as the 'web' process.
+  - An empty --pluginpath directory is now accepted (#917, Richard Pijnenburg)
+  - Piles of documentation improvements! A brand new introductory tutorial is
+    included, and many of the popular plugins have had their docs greatly
+    improved. This effort was lead by Kurt Hurtado with assists by James
+    Turnbull, Aaron Mildenstein, Brad Fritz, and others.
+  - Testing was another focus of this release. We added many more tests
+    to help us prevent regressions and verify expected behavior. Helping with
+    this effort was Richard Pijnenburg, Jordan Sissel, and others.
+  - The 'debug' setting was removed from most plugins. Prior to this,
+    most plugins advertised the availability of this setting but actually
+    did not use it (#996, Jordan Sissel).
+  - bugfix: --pluginpath now lets you load codecs. (#1077, Sergey Zhemzhitsky)
+
+  # inputs
+  - bugfix: collectd: Improve handling of 'NaN' values (#1015, Pieter Lexis)
+  - bugfix: snmptrap: Fixes exception when not specifying yamlmibdir (#950, Andres Koetsier)
+  - improvement: Add Multi-threaded workers and queues to UDP input (johnarnold + untergeek)
+  - improvement: log4j: port now defaults to 4560, the default log4j
+    SocketAppender port. (#757, davux)
+  - bugfix: rabbitmq: auto_delete and exclusive now default to 'false'.
+    The previous version's defaults caused data loss on logstash restarts.
+    Further, these settings are recommended by the RabbitMQ folks. (#864,
+    Michael Klishin)
+    This change breaks past default behavior, so just be aware. (Michael
+    Klishin)
+  - bugfix: collectd: fix some type calculation bugs (#905, Pieter Lexis)
+  - improvement: collectd: Now supports decryption and signature verification
+    (#905, Pieter Lexis)
+  - improvement: wmi: now supports remote hosts (#918, Richard Pijnenburg)
+  - bugfix: elasticsearch: Long scrollids now work correctly (#935, Jonathan
+    Van Eenwyk)
+  - bugfix: tcp: the 'host' field is correctly set now if you are using the
+    json codec and include a 'host' field in your events (#937, Jordan Sissel)
+  - bugfix: file: the 'host' field is correctly set now if you are using the
+    json codec and include a 'host' field in your events (#949, Piotr
+    Popieluch)
+  - bugfix: udp: the 'host' field is correctly set now if you are using the
+    json codec and include a 'host' field in your events (#965, Devin
+    Christensen)
+  - bugfix: syslog: fix regression (#986, Joshua Bussdieker)
+
+  # codecs
+  - improvement: netflow: You can now specify your own netflow field
+    definitions using the 'definitions' setting. See the netflow codec
+    docs for examples on how to do this. (#808, Matt Dainty)
+
+  # filters
+  - bugfix: clone: Correctly clone events with numeric field values.
+    (LOGSTASH-1225, #1158, Darren Holloway)
+  - bugfix: zeromq: Add `timeout` and `retries` settings for retrying on
+    request failures. Also adds `add_tag_on_timeout` so you can act on retry
+    failures. (logstash-contrib#23, Michael Hart)
+  - new: fingerprint: Checksum, anonymize, generate UUIDs, etc! A generalized
+    solution to replace the following filters: uuid, checksum, and anonymize.
+    (#907, Richard Pijnenburg)
+  - new: throttle: Allows you to tag or add fields to events that occur with a
+    given frequency. One use case is to have logstash email you only once if an
+    event occurs at least 3 times in 60 seconds. (#940, Mike Pilone) -
+  - improvement: translate: A new 'refresh_interval' setting lets you tell
+    logstash to periodically try reloading the 'dictionary_path' file
+    without requiring a restart. (#975, Kurt Hurtado)
+  - improvement: geoip: Now safe to use with multiple filter workers and
+    (#990, #997, LOGSTASH-1842; Avleen Vig, Jordan Sissel)
+  - improvement: metrics: Now safe to use with multiple filter workers (#993,
+    Bernd Ahlers)
+  - bugfix: date: Fix regression that caused times to be local time instead of
+    the intended timezone of UTC. (#1010, Jordan Sissel)
+  - bugfix: geoip: Fix encoding of fields created by geoip lookups
+    (LOGSTASH-1354, LOGSTASH-1372, LOGSTASH-1853, #1054, #1058; Jordan Sissel,
+    Nick Ethier)
+
+  # outputs
+  - bugfix: elasticsearch: flush any buffered events on logstash shutdown
+    (#1175)
+  - feature: riemann: Automatically map event fields to rieman event fields
+    (logstash-contrib#15, Byron Pezan)
+  - bugfix: lumberjack: fix off-by-one errors causing writes to another
+    logstash agent to block indefinitely
+  - bugfix: elasticsearch: Fix NameError Socket crash on startup
+    (LOGSTASH-1974, #1167)
+  - improvement: Added `action` awesomeness to elasticsearch output (#1105, jordansissel)
+  - improvement: Implement `protocol => http` in elasticsearch output (#1105, jordansissel)
+  - bugfix: fix broken pipe output to allow EBADF instead of EPIPE,
+    allowing pipe command to be restarted (#974, Pawe≈Ç Puterla)
+  - improvement: Adding dns resolution to lumberjack output (#1048, Nathan Burns )
+  - improvement: added pre- and post-messages to the IRC output (#1111, Lance O'Connor)
+  - bugfix: pipe: fix handling of command failures (#1023, #1034, LOGSTASH-1860; ronnocol, Jordan Sissel)
+  - improvement: lumberjack: now supports codecs (#1048, LOGSTASH-1680; Nathan Burns)
+
+1.3.3 (January 17, 2014)
+  # general
+  - bugfix: Fix SSL cert load problem on plugins using aws-sdk: S3, SNS, etc.
+    (LOGSTASH-1778, LOGSTASH-1787, LOGSTASH-1784, #924; Adam Peck)
+  - bugfix: Fix library load problems for aws-sdk (LOGSTASH-1718, #923; Jordan
+    Sissel)
+  - bugfix: Fix regression introduced in 1.3.2 while trying to improve time
+    parsing performance. (LOGSTASH-1732, LOGSTASH-1738, #913; Jordan Sissel)
+  - bugfix: rabbitmq: honour the passive option when creating queues.
+    (LOGSTASH-1461, Tim Potter)
+
+  # codecs
+  - bugfix: json_lines, json: Fix bug causing invalid json to be incorrectly
+    handled with respect to encoding (#920, LOGSTASH-1595; Jordan Sissel)
+
+1.3.2 (December 23, 2013)
+  # upgrade notes
+  - Users of logstash 1.3.0 or 1.3.1 should set 'template_overwrite => true' in
+    your elasticsearch (or elasticsearch_http) outputs before upgrading to this
+    version to ensure you receive the fixed index template.
+
+  # general
+  - web: don't crash if an invalid http request was sent
+    (#878, LOGSTASH-704; Jordan Sissel)
+  - Ships with Elasticsearch 0.90.9
+  - logstash will now try to make sure the @timestamp field is of the
+    correct format.
+  - Fix a bug in 1.3.1/1.3.0's elasticsearch index template causing phrase
+    searching to not work. Added tests to ensure search behavior works as
+    expected with this template. (Aaron Mildenstein, Jordan Sissel)
+  - Update README.md to be consistent with Makefile use of JRuby 1.7.8
+  - Time parsing in things like the json codec (and other similar parts of
+    logstash) are *much* faster now. This fixes a speed regression that was
+    introduced in logstash 1.2.0.
+
+  # filters
+  - improvement: date: roughly 20% faster (Jordan Sissel)
+
+  # outputs
+  - new: csv: write csv format to files output. (Matt Gray)
+    (This output will become a codec usable with file output in the next
+     major version!)
+
+1.3.1 (December 11, 2013)
+  # general
+  - Fix path to the built-in elasticsearch index template
+
+1.3.0 (December 11, 2013)
+  # general
+  - oops: The --help flag now reports help again, instead of barfing an "I need
+    help" exception (LOGSTASH-1436, LOGSTASH-1392; Jordan Sissel)
+  - Resolved encoding errors caused by environmental configurations, such as
+    'InvalidByteSequenceError ... on US-ASCII' (LOGSTASH-1595, #842;
+    Jordan Sissel)
+  - Fix bug causing "no such file to load -- base64" (LOGSTASH-1310,
+    LOGSTASH-1519, LOGSTASH-1325, LOGSTASH-1522, #834; Jordan Sissel)
+  - Elasticsearch version 0.90.7
+  - Bug fixes galore!
+
+  ## inputs
+  - new: collectd: receive metrics from collectd's network protocol
+    (#785, Aaron Mildenstein)
+  - bugfix: gelf: handle chunked gelf message properly (#718, Thomas De Smedt)
+  - bugfix: s3: fix bug in region endpoint setting (#740, Andrea Ascari)
+  - bugfix: pipe: restart the command when it finishes (#754, Jonathan Van
+    Eenwyk)
+  - bugfix: redis: if redis fails, reconnect. (#767, LOGSTASH-1475; Jordan Sissel)
+  - feature: imap: add 'content_type' setting for multipart messages and
+    choosing the part that becomes the event message. (#784, Brad Fritz)
+  - bugfix: zeromq: don't override the 'host' field if the event already
+    has one. (Jordan Sissel)
+  - bugfix: ganglia: fix regressions; plugin should work again (LOGSTASH-1655,
+    #818; Jordan Sissel)
+  - bugfix: Fix missing library in sqs input (#775, LOGSTASH-1294; Toby
+    Collier)
+
+  ## filters
+  - new: unique: removes duplicate values from a given field in an event.
+    (#676, Adam Tucker)
+  - new: elapsed: time duration between two tagged events. (#713, Andrea Forni)
+  - new: i18n: currently supports 'transliterate' which does best-effort
+    conversion of text to "plain" letters. Like '√≥' to 'o'.  (#671,
+    Juarez Bochi)
+  - bugfix: restore filter flushing thread (LOGSTASH-1284, #689; Bernd Ahlers)
+  - new: elasticsearch: query elasticsearch and update your event based on the
+    results. (#707, Jonathan Van Eenwyk)
+  - new: sumnumbers: finds all numbers in a message and sums them (#752, Avleen
+    Vig)
+  - feature: geoip: new field 'location' is GeoJSON derived from the lon/lat
+    coordinates for use with elasticsearch, kibana, and anything else that
+    understands GeoJSON (#763, Aaron Mildenstein)
+  - new: punct: Removes all text except punctuation and stores it in another
+    field. Useful for as a means for fingerprinting events. (#813, Guixing Bai)
+  - feature: metrics: Make percentiles configurable. Also make rates (1, 5,
+    15-minute) optional. (#817, Juarez Bochi)
+
+  ## codecs
+  - new: compressed_spooler: batches events and sends/receives them in
+    compressed form. Useful over high latency links or with transports
+    with higher-than-desired transmission costs. (Avleen Vig)
+  - new: fluent: receive data serialized using the Fluent::Logger for easier
+    migration away from fluentd or for folks who simply like the logger
+    library (#759, Jordan Sissel)
+  - new: edn: encode and decode the EDN serialization format. Commonly used
+    in Clojure. For more details, see: https://github.com/edn-format/edn
+    (#778, Lee Hinman)
+  - bugfix: oldlogstashjson: Fix encoding to work correctly. (#788, #795;
+    Brad Fritz)
+  - bugfix: oldlogstashjson: Fallback to plain text on invalid JSON
+    (LOGSTASH-1534, #850; Jordan Sissel)
+
+  ## outputs
+  - feature: elasticsearch and elasticsearch_http now will apply a default
+    index mapping template (included) which has the settings recommended by
+    Elasticsearch for Logstash specifically.
+    Configuration options allow disabling this feature and providing a path
+    to your own template. (#826, #839; Aaron Mildenstein)
+  - feature: elasticsearch_http: optional 'user' and 'password' settings to
+    make use of http authentication (LOGSTASH-902, #684; Ian Neubert)
+  - new: google_bigquery: upload logs to bigquery for analysis later (Rodrigo
+    De Castro)
+  - bugfix: datadog_metrics: fix validation bug (#789, Ian Paredes)
+  - feature: elasticsearch: new 'transport' setting letting you tell logstash
+    to act as a cluster node (default, prior behavior) or as a 'transport
+    client'. With the new 'transport' mode, your firewall rules may be simpler
+    (unicast, one direction) and transport clients do not show up in your
+    cluster node list. (LOGSTASH-102, #841; Jordan Sissel)
+  - feature: elasticsearch: new 'bind_port setting for 'node' protocol which
+    lets you chose the local port to bind on (#841, Jordan Sissel)
+  - bugfix: Fix missing library in sqs input (#775, LOGSTASH-1294; Toby
+    Collier)
+
+1.2.2 (October 22, 2013)
+  # general
+  - new 'worker' setting for outputs. This helps improve throughput on
+    request-oriented outputs such as redis, rabbitmq, elasticsearch,
+    elasticsearch_http, etc. Workers run in separate threads each handling
+    events as they come in. This allows you to linearly scale up outputs across
+    cores or as blocking-io permits.
+  - grok performance is up 600%
+  - lots of bug fixes
+  - bugfixes to conditionals (#682, Matt Dainty)
+  - rabbitmq now replaces the old deprecated amqp plugins. amqp plugins are
+    removed.
+  - inputs will now do their best to handle text which is encoded differently
+    than the charset you have specified (LOGSTASH-1443, Jordan Sissel)
+
+  ## inputs
+  - bugfix: udp: respects teardown requests via SIGINT, etc (LOGSTASH-1290,
+    Jordan Sissel)
+  - bugfix: rabbitmq: disable automatic connection recovery (LOGSTASH-1350,
+    #641, #642; Michael Klishin)
+  - bugfix: twitter: works again (#640, Bernd Ahlers)
+  - compatibility: Restored the old 'format' setting behavior. It is still
+    deprecated, but was accidentally removed in 1.2.0. It will be removed
+    later, but is restored as part of our backwards-compat promise (Jordan
+    Sissel)
+  - bugfix: s3: fix LOGSTASH-1321 and LOGSTASH-1319 (Richard Pijnenburg)
+  - bugfix: log4j: fix typo (Jordan Sissel)
+  - bugfix: rabbitmq: disable automatic connection recover because logstash
+    will handle it (LOGSTASH-1350, Michael Klishin)
+  - bugfix: heroku: works again (LOGSTASH-1347, #643; Bernd Ahlers)
+  - bugfix: tcp: improve detection of closed connections to reduce lost events
+    (Jordan Sissel)
+  - bugfix: elasticsearch: now works correctly (#670, Richard Pijnenburg)
+  - improvement: elasticsearch: make size and scroll time configurable (#670,
+    Richard Pijnenburg)
+  - improvement: elasticsearch: tunable search type (#670, Richard Pijnenburg)
+  - compatibility: restore 'format' setting which was accidentally removed in
+    1.2.0. This feature is still deprecated, but it has been restored
+    temporarily as part of our backwards compatibility promise. (#706, Jordan
+    Sissel)
+  - bugfix: syslog: fix socket leakage (#704, Bernd Ahlers)
+  - improvement: all aws-related plugins: Add proxy_uri setting (#714, Malthe
+    Borch)
+  - bugfix: unix: fix variable name crash (#720, Nikolay Bryskin)
+
+  ## codecs
+  - new: graphite: parse graphite formated events (Nick Ethier)
+  - new: json_lines: parse streams that are lines of json objects (#731, Nick
+    Ethier)
+  - bugfix: multiline: time is now correctly in UTC. (Jordan Sissel)
+  - bugfix: oldlogstashjson: improved conversion of old logstash json to the
+    new schema (#654, Jordan Sissel)
+  - bugfix: oldlogstashjson: fix typo breaking encoding (#665, Tom Howe)
+  - bugfix: json: now assumes json delimited by newline character
+    (LOGSTASH-1332, #710; Nick Ethier)
+  - improvements: netflow: new target and versions settings (#686, Matt Dainty)
+
+  ## filters
+  - performance: grok: 6.3x performance improvement (#681, Jordan Sissel)
+  - bugfix: geoip: empty values (nil, empty string) are not put into the event
+    anymore. (Jordan Sissel)
+  - bugfix: geoip: allow using Maxmind's ASN database (LOGSTASH-1394, #694;
+    Bernd Ahlers)
+  - improvement: kv: target will now overwrite any existing fields, including
+    the source (Jordan Sissel).
+  - improvement: Kv: 'prefix' setting now respects sprintf (LOGSTASH-913,
+    #647; Richard Pijnenburg)
+  - checksum: sha128 was not a valid digest, removed from list
+  - feature: metrics: added clear_interval and flush_interval parameters for
+    setting flush rates and when to clear metrics (#545)
+  - new: collate: group events by time and/or count into a single event. (#609,
+    Neway Liu)
+  - feature: date: now supports a 'target' field for writing the timestamp into
+    a field other than @timestamp. (#625, Jonathan Van Eenwyk)
+  - bugfix: riemann: event tagging works again (#631, Marc Fournier)
+  - improvement: grok: IPV6 pattern (#623, Matt Dainty)
+  - improvement: metrics: add clear_interval and flush_interval settings (#545,
+    Juarez Bochi)
+  - improvement: useragent: include operating system details (#656, Philip
+    Kubat)
+  - improvement: csv: new quote_char setting (#725, Alex Markham)
+
+  ## outputs
+  - feature: all outputs have a 'worker' setting  now that allows you to
+    perform more work at the same time. This is useful for plugins like
+    elasticsearch_http, redis, etc, which can bottleneck on waiting for
+    requests to complete but would otherwise be happy processing more
+    simultaneous requests. (#708, Jordan Sissel)
+  - bugfix: elasticsearch: requests are now synchronous. This avoid overloading
+    the client and server with unlimited in-flight requests. (#688, Jordan
+    Sissel)
+  - bugfix: elasticsearch_http: fix bug when sending multibyte utf-8 events
+    (LOGSTASH-1328, #678, #679, #695; Steve Merrill, Christian Winther,
+    NickEthier, Jordan Sissel)
+  - performance: elasticsearch_http: http client library uses TCP_NODELAY now
+    which dramatically improves performance. (#696, Jordan Sissel)
+  - feature: elasticsearch_http now supports a 'replication' setting to
+    allow you to choose how you wait for the response. THe default is 'sync'
+    which waits for all replica shards to be written. If you set it to 'async'
+    then all index requests will respond once only the primary shards have been
+    written and the replica shards will be written later. This can improve
+    throughput. (#700, Nick Ethier, Jordan Sissel)
+  - bugfix: elasticsearch: the default port range is now 9300-9305; the older
+    range up to 9400 was unnecessary and could cause problems for the
+    elasticsearch cluster in some cases.
+  - improvement: aws-based outputs (e.g. cloudwatch) now support proxy uri.
+  - bugfix: rabbitmq: disable automatic connection recovery (LOGSTASH-1350)
+    (#642)
+  - bugfix: riemann: fixed tagging of riemann events (#631)
+  - bugfix: s3: fix LOGSTASH-1321 and LOGSTASH-1319 (#636, #645; Richard
+    Pijnenburg)
+  - bugfix: mongodb: Fix mongodb auth (LOGSTASH-1371, #659; bitsofinfo)
+  - bugfix: datadog: Fix time conversion (LOGSTASH-1427, #690; Bernd Ahlers)
+  - bugfix: statsd: Permit plain floating point values correctly in the
+    config. Example: sample_rate => 0.5 (LOGSTASH-1441, #705; Jordan Sissel)
+  - bugfix: syslog: Fix timestamp date formation. 'timestamp' setting is now
+    deprecated and the format of the time depends on your rfc selection.
+    (LOGSTASH-1423, #692, #739; Jordan Sissel, Bernd Ahlers)
+
+  ## patterns
+  - improvement: added IPV6 suppot to IP pattern (#623)
+
+1.2.1 (September 7, 2013)
+  # general
+  - This is primarily a bugfix/stability release based on feedback from 1.2.0
+  - web: kibana's default dashboard now works with the new logstash 1.2 schema.
+  - docs: updated the tutorials to work in logstash 1.2.x
+  - agent: Restored the --configtest flag (unintentionally removed from 1.2.0)
+  - deprecation: Using deprecated plugin settings can now advise you on a
+    corrective path to take. One example is the 'type' setting on filters and
+    outputs will now advise you to use conditionals and give an example.
+  - conditionals: The "not in" operator is now supported.
+
+  ## inputs
+  - bugfix: pipe: reopen the pipe and retry on any error. (#619, Jonathan Van
+    Eenwyk)
+  - bugfix: syslog: 'message' field no longer appears as an array.
+  - bugfix: rabbitmq: can now bind the queue to the exchange (#624, #628,
+    LOGSTASH-1300, patches by Jonathan Tron and Jonathan Van Eenwyk)
+
+  ## codecs
+  - compatibility: json: if data given is not valid as json will now be used as
+    the "message" of an event . This restores the older behavior when using
+    1.1.13's "format => json" feature on inputs. (LOGSTASH-1299)
+  - new: netflow: process netflow data (#580, patches by Nikolay Bryskin and
+    Matt Dainty)
+
+  ## filters
+  - bugfix: multiline: the multiline filter returns! It was unintentionally
+    removed from the previous (1.2.0) release.
+  - bugfix: json_encode: fix a syntax error in the code. (LOGSTASH-1296)
+  - feature: kv: now captures duplicate field names as a list, so 'foo=bar
+    foo=baz' becomes the field 'foo' with value ['bar', 'baz'] (an array).
+    (#622, patch by Matt Dainty)
+
+  ## outputs
+  - new: google_cloud_storage: archive logs to Google Cloud Storage (#572,
+    Rodrigo De Castro)
+  - bugfix: fixed bug with 'tags' and 'exclude_tags' on outputs that would
+    crash if the event had no tags. (LOGSTASH-1286)
+
+1.2.0 (September 3, 2013)
+  # general
+  - The logstash json schema has changed. (LOGSTASH-675)
+    For prior logstash users, you will be impacted one of several ways:
+    * You should check your elasticsearch templates and update them accordingly.
+    * If you want to reindex old data from elasticsearch with the new schema,
+      you should be able to do this with the elasticsearch input. Just make
+      sure you set 'codec => oldlogstashjson' in your elasticsearch input.
+  - The old logstash web ui has been replaced by Kibana 3. Kibana is a far
+    superior search and analytics interface.
+  - New feature: conditionals! You can now make "if this, then ..." decisions
+    in your filters or outputs. See the docs here:
+    http://logstash.net/docs/latest/configuration#conditionals
+  - A new syntax exists for referencing fields (LOGSTASH-1153). This replaces
+    the prior and undocumented syntax for field access (was 'foo.bar' and is
+    now '[foo][bar]'). Learn more about this here:
+    http://logstash.net/docs/latest/configuration#fieldreferences
+  - A saner hash syntax in the logstash config is now supported. It uses the
+    perl/ruby hash-rocket syntax: { "key" => "value", ... } (LOGSTASH-728)
+  - ElasticSearch version 0.90.3 is included. (#486, Gang Chen)
+  - The elasticsearch plugin now uses the bulk index api which should result
+    in lower cpu usage as well as higher performance than the previous
+    logstash version.
+  - Many deprecated features have been removed. If your config caused
+    deprecation warnings on startup in logstash v1.1.13, there is a good
+    chance that these deprecated settings are now absent.
+  - 'type' is no longer a required setting on inputs.
+  - New plugin type: codec. Used to implement decoding of events for inputs and
+    encoding of events for outputs. Codecs allow us to separate transport (like
+    tcp, redis, rabbitmq) from serialization (gzip text, json, msgpack, etc).
+  - Improved error messages that try to be helpful. If you see bad or confusing
+    error messages, it is a bug, so let us know! (Patch by Nick Ethier)
+  - The old 'plugin status' concept has been replaced by 'milestones'
+    (LOGSTASH-1137)
+  - SIGHUP should cause logstash to reopen it's logfile if you are using the
+    --log flag
+
+  ## inputs
+  - new: s3: reads files from s3 (#537, patch by Mathieu Guillaume)
+  - feature: imap: now marks emails as read (#542, Raffael Schmid)
+  - feature: imap: lets you delete read email (#591, Jonathan Van Eenwyk)
+  - feature: rabbitmq: now well-supported again (patches by Michael Klishin)
+  - bugfix: gelf: work around gelf parser errors (#476, patch by Chris McCoy)
+  - broken: the twitter input is disabled because the twitter stream v1 api is
+    no longer supported and I couldn't find a replacement library that works
+    under JRuby.
+  - new: sqlite input (#484, patch by Evan Livingston)
+  - improvement: snmptrap: new 'yamlmibdir' setting for specifying an external
+    source for MIB definitions. (#477, patch by Dick Davies)
+  - improvement: stomp: vhost support (#490, patch by Matt Dainty)
+  - new: unix: unix socket input (#496, patch by Nikolay Bryskin)
+  - new: wmi: for querying wmi (windows). (#497, patch by Philip Seidel)
+  - improvement: sqs: new id_field and md5_field settings (LOGSTASH-1118, Louis
+    Zuckerman)
+
+  ## filters
+  - feature: grok: 'singles' now defaults to true.
+  - bugfix: grep: allow repeating a field in the hash config (LOGSTASH-919)
+  - feature: specify timezone in date filter (#470, patch by Philippe Weber)
+  - feature: grok setting 'overwrite' now lets you overwrite fields instead
+    of appending to them.
+  - feature: the useragent filter now defaults to writing results to the top
+    level of the event instead of "ua"
+  - feature: grok now defaults 'singles' to true, meaning captured fields are
+    stored as single values in most cases instead of the old behavior of being
+    captured as an array of values.
+  - new: json_encoder filter (#554, patch by Ralph Meijer)
+  - new: cipher: gives you many options for encrypting fields (#493, patch by
+    saez0pub)
+  - feature: kv: new settings include_fields and exclude_fields. (patch by
+    Piavlo)
+  - feature: geoip: new 'target' setting for where to write geoip results.
+    (#491, patch by Richard Pijnenburg)
+  - feature: dns: now accepts custom nameservers to query (#495, patch by
+    Nikolay Bryskin)
+  - feature: dns: now accepts a timeout setting (#507, patch by Jay Luker)
+  - bugfix: ruby: multiple ruby filter instances now work (#501, patch by
+    Nikolay Bryskin)
+  - feature: uuid: new filter to add a uuid to each event (#531, Tomas Doran)
+  - feature: useragent: added 'prefix' setting to prefix field names created
+    by this filter. (#524, patch by Jay Luker)
+  - bugfix: mutate: strip works now (#590, Jonathan Van Eenwyk)
+  - new: extractnumbers: extract all numbers from a message (#579, patch by
+    Pablo Barrera)
+
+  ## outputs
+  - new: jira: create jira tickets from an event (#536, patch by Martin Cleaver)
+  - feature: rabbitmq: now well-supported again (patches by Michael Klishin)
+  - improvement: stomp: vhost support (Patch by Matt Dainty)
+  - feature: elasticsearch: now uses the bulk index api and supports
+    a tunable bulk flushing size.
+  - feature: elasticsearch_http: will now flush when idle instead of always
+    waiting for a full buffer. This helps in slow-sender situations such
+    as testing by hand.
+  - feature: irc: add messages_per_second tunable (LOGSTASH-962)
+  - bugfix: email: restored initial really useful documentation
+  - improvement: emails: allow @message, @source, @... in match (LOGSTASH-826,
+    LOGSTASH-823)
+  - feature: email: can now set Reply-To (#540, Tim Meighen)
+  - feature: mongodb: replica sets are supported (#389, patch by Mathias Gug)
+  - new: s3: New plugin to write to amazon S3 (#439, patch by Mattia Peterle)
+  - feature: statsd: now supports 'set' metrics (#513, patch by David Warden)
+  - feature: sqs: now supports batching (#522, patch by AaronTheApe)
+  - feature: ganglia: add slope and group settings (#583, patch by divanikus)
+
+1.1.13 (May 28, 2013)
+  ## general
+  - fixed bug in static file serving for logstash web (LOGSTASH-1067)
+
+  ## outputs
+  - feature: irc: add messages_per_second tunable (LOGSTASH-962)
+
+1.1.12 (May 7, 2013)
+  ## filters
+  - bugfix: useragent filter now works correctly with the built-in regexes.yaml
+  - bugfix: mail output with smtp now works again
+
+1.1.11 (May 7, 2013)
+  ## general
+  - This release is primarily a bugfix release for bugs introduced by the
+    previous release.
+  - Support for Rubinius and MRI exists once again.
+
+  ## inputs
+  - bugfix: lumberjack now respects field data again (lumberjack --field foo=bar)
+  - bugfix: rabbitmq was broken by the previous release (LOGSTASH-1003,
+    LOGSTASH-1038; Patch by Jason Koppe)
+  - bugfix: relp: allow multiple client socket connections to RELP input
+    (LOGSTASH-707, LOGSTASH-736, LOGSTASH-921)
+
+  ## filters
+  - bugfix: geoip was broken by the previous release (LOGSTASH-1013)
+  - feature: sleep now accepts an 'every' setting which causes it to
+    sleep every N events. Example; sleep every 10 events: every => 10.
+  - feature: grok now permits dashes and dots in captures, such as
+    %{WORD:foo-bar}.
+  - bugfix: useragent filter now ships with a default regexes.yaml file
+    that is used by default unless you otherwise specify (LOGSTASH-1051)
+  - bugfix: add_field now correctly sets top-level fields like @message
+  - bugfix: mutate 'replace' now sets a field regardless of whether or not
+    it exists.
+  - feature: new mutate 'update' setting to change a field's value but
+    only if that field exists already.
+
+  ## outputs
+  - feature: irc output now supports 'secure' setting to use ssl (LOGSTASH-139)
+  - feature: nagios_nsca has new setting 'message_format'
+  - bugfix: fix graphite plugin broken in 1.1.10 (LOGSTASH-968)
+  - bugfix: elasticsearch_http was broken in 1.1.10 (LOGSTASH-1004)
+  - bugfix: rabbitmq was broken by the previous release (LOGSTASH-1003,
+    LOGSTASH-1038; Patch by Jason Koppe)
+  - feature: hipchat 'notify' setting now called 'trigger_notify' (#467, patch
+    by Richard Pijnenburg)
+
+1.1.10 (April 16, 2013)
+  ## general
+  - On linux, all threads will set their process names so you can identify
+    threads in tools like top(1).
+  - Java 5 is no longer supported (You must use Java 6 or newer).
+  - Windows line terminators (CRLF) are now accepted in config files.
+  - All AWS-related plugins now have the same configuration options:
+    region, access_key_id, secret_access_key, use_ssl, and
+    aws_credentials_file. Affected plugins: cloudwatch output,
+    sns output, sqs output, sqs input. (LOGSTASH-805)
+  - Lots of documentation fixes (James Turnbull, et al)
+  - The amqp plugins are now named 'rabbitmq' because it *only* works
+    with rabbitmq. The old 'amqp' name should still work, but it will
+    be removed soon while 'rabbitmq' will stay. (Patches by Michael Zaccari)
+  - New flag '--configtest' to test config and exit. (Patch by Darren Patterson)
+  - Improved error feedback logstash gives to you as a user.
+
+  ## inputs
+  - new: elasticsearch: this input allows you to stream search results from
+    elasticsearch; it uses the Scroll API.
+  - new: websocket. Currently supports acting as a websocket client.
+  - new: snmptrap, to receive SNMP traps (patch by Paul Czar)
+  - new: varnishlog input to read from the Varnish Cache server's shared memory
+    log (LOGSTASH-978, #422; Louis Zuckerman)
+  - new: graphite input. Supports the plain text carbon tcp protocol.
+  - new: imap input. Read mail!
+  - feature: twitter: supports http proxying now (#276, patch by Richard
+    Pijnenburg)
+  - feature: loggly: supports http proxying now (#276, patch by Richard
+    Pijnenburg)
+  - feature: tcp: ssl now supported! (#318, patch by Matthew Richardson)
+  - feature: redis: now supports 'batch_count' option for doing bulk fetches
+    from redis lists. Requires Redis 2.6.0 or higher. (#320, patch by Piavlo)
+  - feature: irc: will use ssl if you set 'secure' (#393, patch by Tomas Doran)
+  - bugfix: log4j: respect add_fields (LOGSTASH-904, #358)
+  - bugfix: eventlog: input should now work
+  - bugfix: irc: passwords now work (#412, Nick Ethier)
+
+  ## filters
+  - new: useragent: parses user agent strings in to structured data based on
+    BrowserScope data (#347, patch by Dan Everton)
+  - new: sleep: sleeps a given amount of time before passing the event.
+    Useful for rate limiting or replay simulation.
+  - new: ruby: experimental ruby plugin that lets you call custom ruby code
+    on every event.
+  - new: translate: for mapping values (#335, patch by Paul Czar)
+  - new: clone: creates a copy of the event.
+  - feature: grok: Adds tag_on_failure setting so you can prevent grok from
+    tagging events on failure. (#328, patch by Neil Prosser)
+  - deprecated: grok: deprecated the --grok-patterns-path flag (LOGSTASH-803)
+  - feature: date: nested field access is allowed now
+  - feature: csv, xml, kv, json, geoip: new common settings!
+    (LOGSTASH-756, #310, #311, #312, #383, #396; patches by Richard Pijnenburg)
+      source - what field the text comes from
+      target - where to store the parse result.
+  - feature: csv: new setting: columns - labels for each column parsed.
+  - bugfix: geoip: The built-in geoip database should work now (#326, patch
+    by Vincent Batts)
+  - bugfix: kv filter now respects add_tag, etc (LOGSTASH-935)
+
+  ## outputs
+  - new: hipchat output (#428, Cameron Stokes)
+  - bugfix: mongo would fail to load bson_java support (LOGSTASH-849)
+  - bugfix: tags support to gelf output. Returns tags as _tags field
+    (LOGSTASH-880, patch by James Turnbull)
+  - bugfix: elasticsearch: Fix a race condition. (#340, patch by Raymond Feng)
+  - improvement: http: now supports a custom 'message' format for building your
+    own http bodies from an event. (#319, patch by Christian S)
+  - bugfix: Fix opentsdb output (LOGSTASH-689, #317; patch by Emmet Murphy)
+  - improvement: http output now supports a custom message format with
+    the 'message' setting (Patch by Christian SchroÃàder)
+  - graphite output now lets you ship the whole (or part) of an event's fields
+    to graphite as metric updates. (#350, patch by Piavlo)
+  - email output now correctly defaults to not using authentication
+    (LOGSTASH-559, #365; patch by Stian Mathiassen)
+  - bugfix: file output now works correctly on fifos
+  - bugfix: irc passwords now work (#412, Nick Ethier)
+  - improvement: redis output now supports congestion detection. If
+    it appears nothing is consuming from redis, the output will stall
+    until that problem is resolved. This helps prevent a dead reader
+    from letting redis fill up memory. (Piavlo)
+  - feature: boundary: New 'auto' setting. (#413, Alden Jole)
+
+1.1.9 (January 10, 2013)
+  ## inputs
+  - bugfix: all inputs: fix bug where some @source values were not valid urls
+
+  ## filters
+  - bugfix: mutate: skip missing fields in 'convert' (#244, patch by Ralph Meijer)
+
+  ## outputs
+  - improvement: gelf: new tunable 'ignore_metadata' flag to set which fields
+    to ignore if ship_metadata is set. (#244, patch by Ralph Meijer)
+  - improvement: gelf: make short_message's field name tunable (#244, patch by
+    Ralph Meijer)
+
+1.1.8 (January 10, 2013)
+  ## general
+  - patched another work around for JRUBY-6970 (LOGSTASH-801)
+
+  ## inputs
+  - bugfix: tcp: 'Address in use' errors now report the host/port involved.
+    (LOGSTASH-831)
+  - bugfix: zeromq: fix bug where an invalid url could be given as a source
+    (LOGSTASH-821, #306)
+
+  ## outputs
+  - bugfix: elasticsearch_river: it now resolves evaluates %{} variables in
+    index and index_type settings. (LOGSTASH-819)
+
+1.1.7 (January 3, 2013)
+ ## inputs
+ - fix bug where @source_host was set to 'false' in many cases.
+
+ ## outputs
+ - improvement: redis: shuffle_hosts is now enabled by default
+
+1.1.6 (January 2, 2013)
+ ## Overview of this release:
+ - new inputs: drupal_dblog.
+ - new filters: anonymize, metrics.
+ - new outputs: syslog, cloudwatch.
+ - new 'charset' setting for all inputs. This should resolve all known encoding
+   problems. The default charset is UTF-8.
+ - grok now captures (?<somename>...) regexp into 'somename' field
+ - Elasticsearch 0.20.2 is included. This means you are required to upgrade
+   your elasticsearch cluster to 0.20.2. If you wish to continue using an old
+   version of elasticsearch, you should use the elasticsearch_http plugin
+   instead of the elasticsearch one.
+
+ ## general
+ - fixed internal dependency versioning on 'addressable' gem (LOGSTASH-694)
+ - fixed another case of 'watchdog timeout' (LOGSTASH-701)
+ - plugin flags are now deprecated. The grok filter (--grok-pattern-path) was
+   the only plugin to make use of this.
+ - the grok filter has improved documentation
+ - lots of documentation fixes (James Turnbull, Louis Zuckerman)
+ - lots of testing improvements (Philippe Weber, Laust Rud Jacobsen)
+ - all 'name' settings have been deprecated in favor of more descriptive
+   settings (LOGSTASH-755)
+ - JRuby upgraded to 1.7.1
+ - removed use of bundler
+ - Fixed timestamp parsing in MRI (patch by Rene Lengwinat)
+
+ ## inputs
+ - All inputs now have a 'charset' setting to help you inform logstash of the
+   text encoding of the input. This is useful if you have Shift_JIS or CP1251
+   encoded log files. This should help resolve the many UTF-8 bugs that were
+   reported recently. The default charset is UTF-8.
+ - new: drupal_dblog: read events from a DBLog-enabled Drupal. (#251, Patch by
+   theduke)
+ - bugfix: zeromq: 'topology' is now a required setting
+ - bugfix: lumberjack: client connection closing is now handled properly.
+   (Patch by Nick Ethier)
+ - misc: lumberjack: jls-lumberjack gem updated to 0.0.7
+ - bugfix: stomp: fix startup problems causing early termination (#226
+ - bugfix: tcp: the 'source host' for events is now the client ip:port that
+   sent it, instead of the listen address that received it. (LOGSTASH-796)
+ - improvement: tcp: the default data_timeout is now -1 (never timeout).
+   This change was made because read timeouts were causing data loss, and
+   logstash should avoid losing events by default.
+ - improvement: amqp: the 'name' setting is now called 'queue' (#274)
+ - improvement: eventlog: the 'name' setting is now called 'logfile' (#274)
+ - bugfix: log4j: fix stacktrace reading (#253, patch by Alex Arutyunyants)
+
+ ## filters
+ - new: anonymize: supports many hash mechanisms (murmur3, sha1, md5, etc) as
+   well as IP address anonymization (#280, #261; patches by Richard Pijnenburg
+   and Avishai Ish-Shalom)
+ - new: metrics: allows you to aggregate metrics from events and emit them
+   periodically. Think of this like 'statsd' but implemented as a logstash
+   filter instead of an external service.
+ - feature: date: now accepts 'match' as a setting. Use of this is preferable
+   to the old syntax. Where you previously had 'date { somefield =>
+   "somepattern" }' you should now do: 'date { match => [ "somefield",
+   "somepattern" ] }'. (#248, LOGSTASH-734, Patch by Louis Zuckerman)
+ - feature: grok: now accepts (?<foo>...) named captures. This lets you
+   compose a pattern in the grok config without needing to define it in a
+   patterns file. Example: (?<hostport>%{HOST}:%{POSINT}) to capture 'hostport'
+ - improvement: grok: allow '$' in JAVACLASS pattern (#241, patch by Corry
+   Haines)
+ - improvement: grok: can now match against number types. Example, if you're
+   sending a json format event with { "status": 403 } you can now grok that
+   field.  The number is represented as a string "403" before pattern matching.
+ - bugfix: date: Fix a bug that would crash the pipeline if no date pattern
+   matched. (LOGSTASH-705)
+ - feature: kv: Adds field_split, value_split, prefix, and container
+   settings. (#225, patch by Alex Wheeler)
+ - bugfix: mutate: rename on a nonexistant field now does nothing as expected.
+   (LOGSTASH-757)
+ - bugfix: grok: don't tag an event with _grokparsefailure if it's already so
+   (#248, patch by Greg Brockman)
+ - feature: mutate: new settings - split, join, strip. "split" splits a field
+   into an array. "join" merges an array into a string. "strip" strips leading and
+   trailing whitespace. (Patch by Avishai Ish-Shalom)
+
+ ## outputs
+ - new: syslog output supporting both RFC3164 and RFC5424 (#180, patch by
+   Rui Alves)
+ - new: cloudwatch output to emit metrics and other events to Amazon CloudWatch.
+   (LOGSTASH-461, patch by Louis Zuckerman)
+ - feature: stdout: added 'message' setting for configuring the output message
+   format. The default is same behavior as before this feature.
+ - feature: http: added 'format' option to select 'json' or form-encoded
+   request body to send with each request.
+ - feature: http: added 'content_Type' option set the Content-Type header.
+   This defaults to "application/json" if the 'format' is 'json'. Will default
+   to 'application/x-www-form-urlencoded' if the 'format' is 'form'
+ - bugfix: zeromq: 'topology' is now a required setting
+ - feature: mongodb: new setting 'isodate' that, when true, stores the
+   @timestamp field as a mongodb date instead of a string. (#224, patch by
+   Kevin Amorin)
+ - improvement: gelf: Allow full_message gelf property to be overridden (#245,
+   patch by S√©bastien Masset)
+ - misc: lumberjack: jls-lumberjack gem updated to 0.0.6
+ - feature: nagios: New 'nagios_level' setting to let you change the level
+   of the passive check result sent to nagios. (#298, Patch by James Turnbull)
+ - feature: elasticsearch, elasticsearch_http, elasticsearch_river: new setting
+   'document_id' for explicitly setting the document id in each write to
+   elasticsearch. This is useful for overwriting existing documents.
+ - improvement: elasticsearch_river: 'name' is now 'queue' (#274)
+ - improvement: amqp: 'name' is now 'exchange' (#274)
+ - bugfix: the websocket output works again (supports RFC6455)
+
+1.1.5 (November 10, 2012)
+ ## Overview of this release:
+ * New inputs: zenoss, gemfire
+ * New outputs: lumberjack, gemfire
+ * Many UTF-8 crashing bugs were resolved
+
+ ## general
+ - new runner command 'rspec' - lets you run rspec tests from the jar
+   This means you should now be able to write external tests that execute your
+   logstash configs and verify functionality.
+ - "file not found" errors related to paths that had "jar:" prefixes should
+   now work. (Fixes LOGSTASH-649, LOGSTASH-642, LOGSTASH-655)
+ - several plugins received UTF-8-related fixes (file, lumberjack, etc)
+   File bugs if you see any UTF-8 related crashes.
+ - 'json_event' format inputs will now respect 'tags' (#239, patch by
+   Tim Laszlo)
+ - logstash no longer uses nor recommends bundler (see 'gembag.rb'). The
+   Gemfile will be purged in the near future.
+ - amqp plugins are now marked 'unsupported' as there is no active maintainer
+   nor is there source of active support in the community. If you're interested
+   in maintainership, please email the mailling list or contact Jordan!
+
+ ## inputs
+ - irc: now stores irc nick
+ - new: zenoss (#232, patch by Chet Luther)
+ - new: gemfire (#235, patch by Andrea Campi)
+ - bugfix: udp: skip close() call if we're already closed (#238, patch by kcrayon)
+
+ ## filters
+ - bugfix: fix for zeromq filter initializer (#237, patch by Tom Howe)
+
+ ## outputs
+ - new: lumberjack output (patch by Nick Ethier)
+ - new: gemfire output (#234, patch by Andrea Campi)
+ - improved: nagios_ncsa (patch by Tomas Doran)
+ - improved: elasticsearch: permit setting 'host' even if embedded. Also set the
+   host default to 'localhost' when using embedded. These fixes should help resolve
+   issues new users have when their distros surprisingly block multicast by
+   default.
+ - improved: elasticsearch: failed index attempts will be retried
+ - improved: irc: new 'password' setting (#283, patch by theduke)
+
+1.1.4 (October 28, 2012)
+ ## Overview of this release:
+ - bug fixes mostly
+
+ ## filters
+ - date: Fix crashing on date filter failures. Wrote test to cover this case.
+   (LOGSTASH-641)
+ - grok: Improve QUOTEDSTRING pattern to avoid some more 'watchdog timeout' problems
+
+ ## outputs
+ - nagios_nsca: Allow check status to be set from the event (#228, patch by
+   Tomas Doran)
+ - elasticsearch_http: Fix OpenSSL::X509::StoreError (LOGSTASH-642)
+
+1.1.3 (October 22, 2012)
+ - rebuilt 1.1.2 for java 5 and 6
+
+1.1.2 (October 22, 2012)
+ ## Overview of this release:
+  * New input plugins: lumberjack, sqs, relp
+  * New output plugins: exec, sqs
+  * New filter plugins: kv, geoip, urldecode, alter
+  * file input supports backfill via 'start_position'
+  * filter watchdog timer set to 10 seconds (was 2 seconds)
+
+ ## general
+ - Stopped using 'Gemfile' for dependencies, the logstash.gemspec has returned.
+   (Patch by Grant Rogers)
+ - New 'logstash-event.gemspec' for generating logstash events in your own
+   ruby programs (Patch by Garry Shutler)
+ - Wildcard config files are now sorted properly (agent -f
+   /etc/logstash/*.conf)
+ - The old '-vvv' setting ruby's internal $DEBUG is now gone. It was causing
+   too much confusion for users due to noise.
+ - Improved 'logstash event' creation speed by 3.5x
+ - Now uses JRuby 1.7.0
+ - Now ships with Elasticsearch 0.19.10
+
+ ## inputs
+ - bugfix: redis: [LOGSTASH-526] fix bug with password passing
+ - new: lumberjack: for use with the lumberjack log shipper
+   (https://github.com/jordansissel/lumberjack)
+ - new: sqs: Amazon SQS input (Patch by Sean Laurent, #211)
+ - new: relp: RELP (rsyslog) plugin (Patch by Mike Worth, #177)
+ - file input: sincedb path is now automatically generated if not specified.
+   This helps work around a problem where two file inputs don't specify a
+   sincedb_path would clobber eachother (LOGSTASH-554)
+ - file input: no longer crashes if HOME is not set in env (LOGSTASH-458)
+ - log4j input: now supports MDC 'event properties' which are stored as fields
+   in the logstash event. (#216, #179. Patches by Charles Robertson and Jurjan
+   Woltman)
+ - pipe input: should work now.
+
+ ## filters
+ - new: kv: useful for parsing log formats taht use 'foo=bar baz=fizz' and
+   similar key-value-like things.
+ - new: urldecode: a filter for urldecoding fields in your event. (Patch by
+   Joey Imbasciano, LOGSTASH-612)
+ - new: geoip: query a local geoip database for location information (Patch by
+   Avishai Ish-Shalom, #208)
+ - improvement: zeromq: an empty reply is now considered as a 'cancel this
+   event' operation (LOGSTASH-574)
+ - bugfix: mutate: fix bug in uppercase and lowercase feature that would
+   prevent it from actually doing the uppercasing/lowercasing.
+ - improvement: mutate: do the 'remove' action last (LOGSTASH-543)
+ - feature: grok: new 'singles' config option which, when true, stores
+   single-value fields simply as a single value rather than as an array, like
+   [value]. (LOGSTASH-185)
+ - grok patterns: the URIPARAM pattern now includes pipe '|' as a valid
+   character. (Patch by Chris Mague)
+ - grok patterns: improve haproxy log patterns (Patch by Kevin Nuckolls)
+ - grok patterns: include 'FATAL' as a valid LOGLEVEL match
+   (patch by Corry Haines)
+ - grok patterns: 'ZONE' is no longer captured by name in the HTTPDATE pattern
+ - new: alter: adds some conditional field modification as well as a
+   'coalesce' feature which sets the value of a field to the first non-null
+   value given in a list. (Patch by Francesco Salbaroli)
+ - improvement: date: add TAI64N support
+ - improvement: date: subsecond precision on UNIX timestamps is retained on
+   conversion (#213, Patch by Ralph Meijer)
+ - improvement: date: Add locale setting; useful for day/month name parsing.
+   (#100, Patch by Christian Schr√∂der)
+
+ ## outputs
+ - new: exec: run arbitrary commands based on an event.
+ - new: sqs: Amazon SQS output (Patch by Sean Laurent, #211)
+ - bugfix: redis: [LOGSTASH-526] fix bug with password passing
+ - improvement: redis: [LOGSTASH-573] retry on failure even in batch-mode. This
+   also fixes a prior bug where an exception in batch mode would cause logstash
+   to crash. (Patch by Alex Dean)
+ - improvement: riemann: metric and ttl values in riemann_event now support
+   sprintf %{foo} values. (pull #174)
+ - improvement: stdout: new 'dots' debug_format value emits one dot per event
+   useful for tracking event rates.
+ - gelf output: correct severity level mappings (patch by Jason Koppe)
+ - xmpp output: users and rooms are separate config settings now (patch by
+   Parker DeBardelaben)
+ - improvement: redis: 'host' setting now accepts a list of hosts for failover
+   of writes should the current host go down. (#222, patch by Corry Haines)
+
+1.1.1 (July 14, 2012)
+ ## Overview of this release:
+  * New input plugins: generator, heroku, pipe, ganglia, irc
+  * New output plugins: juggernaut, metricscatcher, nagios_ncsa, pipe,
+                        opentsdb, pagerduty, irc
+  * New filter plugins: zeromq, environment, xml, csv, syslog_pri
+  * Fixes for gelf output
+  * Support for more than 1 filter worker (agent argument "-w")
+
+ ## IMPORTANT CHANGES FOR UPGRADES FROM 1.1.0
+  - zeromq input and output rewritten
+      The previous zeromq support was an MVP. It has now been rewritten into
+      something more flexible. The configuration options have changed entirely.
+      While this is still listed as `experimental`, we don't predict any more
+      configuration syntax changes. The next release will bump this to beta.
+  - unix_timestamp
+      Previously, several plugins did not work as expected on MRI due to the
+      usage of the JRuby-only Jodatime library. We now have a contributed fix
+      for a slower parser on MRI/CRuby!
+  - elasticsearch version is now 0.19.8
+      This means your elasticsearch cluster must be running 0.19.x for
+      compatibility reasons.
+  - grok pattern %{POSINT} used to match '0' -- now it does not. If you want
+    to match non-negative integers, there is now a %{NONNEGINT} pattern.
+  - bug in file input fixed that led to an extra leading slash in @source_path.
+    Previously, file input would have @source = 'file://host//var/log/foo' and
+    @source_path = '//var/log/foo'; now @source = 'file://host/var/log/foo'
+    and @source_path = '/var/log/foo'. [LOGSTASH-501]
+  - file input now rejects relative paths. [LOGSTASH-503]
+  - event sprintf can now look inside structured field data. %{foo.bar} will
+    look in the event field "foo" (if it is a hash) for "bar".  To preserve
+    compatibility, we first look for a top-level key that matches exactly
+    (so %{foo.bar} will first look for a field named "foo.bar", then look for
+    "bar" under "foo").
+
+  ## general
+  - NOTE: gemspec removed; deploying logstash as a gem hasn't been supported
+    for a while.
+  - feature: logstash sub-commands "irb" and "pry" for an interactive debug
+    console, useful to debug jruby when running from the monolithic jar
+  - misc: newer cabin gem for logging
+  - misc: initial support for reporting internal metrics (currently outputs
+    to INFO log; eventually will be an internal event type)
+  - misc: added a "thread watchdog" to detect hanging filter workers, and
+    crash logstash w/an informational message
+  - misc: jar is built with jruby 1.6.7.2
+  - misc: better shutdown behavior when there are no inputs/plugins running
+  - feature: logstash web now uses relative URLs; useful if you want to
+    reverseproxy with a path other than "/"
+
+  ## inputs
+  - bugfix: stdin: exit plugin gracefully on EOF
+  - feature: [LOGSTASH-410] - inputs can now be duplicated with the
+    'threads' parameter (where supported)
+  - bugfix: [LOGSTASH-490] - include cacert.pem in jar for twitter input
+  - feature: [LOGSTASH-139] - support for IRC
+
+  ## filters
+  - feature: all filters support 'remove_tag' (remove tags on success)
+  - feature: all filters support 'exclude_tags' (inverse of 'tags')
+  - bugfix: [LOGSTASH-300] - bump grok pattern replace limit to 1000,
+    fixes "deep recursion pattern compilation" problems
+  - bugfix: [LOGSTASH-375] - fix bug in grep: don't drop when field is nil
+    and negate is true
+  - bugfix: [LOGSTASH-386] - fix some grok patterns for haproxy
+  - bugfix: [LOGSTASH-446] - fix grok %{QUOTEDSTRING} pattern, should fix
+    some grok filter hangs
+  - bugfix: some enhancements to grok pattern %{COMBINEDAPACHELOG}
+  - bugfix: grok: %{URIPATH} and %{URIPARAM} enhancements
+  - feature: grok: add %{UUID} pattern
+  - bugfix: grok: better error message when expanding unknown %{pattern}
+  - feature: mutate: now supports a 'gsub' operation for applying a regexp
+    substitution on event fields
+
+  ## outputs
+  - bugfix: [LOGSTASH-351] - fix file input on windows
+  - feature: [LOGSTASH-356] - make file output flush intervals configurable
+  - feature: [LOGSTASH-392] - add 'field' attribute to restrict which fields
+    get sent to an output
+  - feature: [LOGSTASH-374] - add gzip support to file output
+  - bugfix: elastic search river now respects exchange_type and queue_name
+  - bugfix: ganglia plugin now respects metric_type
+  - bugfix: GELF output facility fixes; now defaults to 'logstash-gelf'
+  - feature: [LOGSTASH-139] - support for IRC
+  - bugfix: es_river: check river status after creation to verify status
+  - feature: es: allow setting node_name
+  - feature: redis: output batching for list mode
+
+1.1.0.1 (January 30, 2012)
+  ## Overview of this release:
+    * date filter bugfix: [LOGSTASH-438] - update joda-time to properly
+      handle leap days
+
+1.1.0 (January 30, 2012)
+  ## Overview of this release:
+    * New input plugins: zeromq, gelf
+    * New filter plugins: mutate, dns, json
+    * New output plugins: zeromq, file
+    * The logstash agent now runs also in MRI 1.9.2 and above
+
+    This is a large release due to the longevity of the 1.1.0 betas.
+    We don't like long releases and will try to avoid this in the future.
+
+  ## IMPORTANT CHANGES FOR UPGRADES FROM 1.0.x
+    - grok filter: named_captures_only now defaults to true
+        This means simple patterns %{NUMBER} without any other name will
+        now not be included in the field set. You can revert to the old
+        behavior by setting 'named_captures_only => false' in your grok
+        filter config.
+    - grok filter: now uses Ruby's regular expression engine
+        The previous engine was PCRE. It is now Oniguruma (Ruby). Their
+        syntaxes are quite similar, but it is something to be aware of.
+    - elasticsearch library upgraded to 0.18.7
+        This means you will need to upgrade your elasticsearch servers,
+        if any, to the this version: 0.18.7
+    - AMQP parameters and usage have changed for the better. You might
+      find that your old (1.0.x) AMQP logstash configs do not work.
+      If so, please consult the documentation for that plugin to find
+      the new names of the parameters.
+
+  ## general
+  - feature: [LOGSTASH-158] - MRI-1.9 compatible (except for some
+    plugins/functions which will throw a compatibility exception) This means
+    you can use most of the logstash agent under standard ruby.
+  - feature: [LOGSTASH-118] - logstash version output (--version or -V for
+    agent)
+  - feature: all plugins now have a 'plugin status' indicating the expectation
+    of stability, successful deployment, and rate of code change. If you
+    use an unstable plugin, you will now see a warning message on startup.
+  - bugfix: AMQP overhaul (input & output), please see docs for updated
+    config parameters.
+  - bugfix: [LOGSTASH-162,177,196] make sure plugin-contained global actions
+    happen serially across all plugins (with a mutex)
+  - bugfix: [LOGSTASH-286] - logstash agent should not truncate logfile on
+    startup
+  - misc: [LOGSTASH-160] - now use gnu make instead of rake.
+  - misc: now using cabin library for all internal logging
+  - test: use minitest
+  - upgrade: now using jruby in 1.9 mode
+
+  ## inputs
+  - feature: zeromq input. Requires you have libzmq installed on your system.
+  - feature, bugfix: [LOGSTASH-40,65,234,296]: much smarter file watching for
+    file inputs. now supports globs, keeps state between runs, can handle
+    truncate, log rotation, etc. no more inotify is required, either (file
+    input now works on all platforms)
+  - feature: [LOGSTASH-172,201] - syslog input accepts ISO8601 timestamps
+  - feature: [LOGSTASH-159] - TCP input lets you configure what identifies
+    an input stream to the multiline filter (unique per host, or connection)
+  - feature: [LOGSTASH-168] - add new GELF input plugin
+  - bugfix: [LOGSTASH-8,233] - fix stomp input
+  - bugfix: [LOGSTASH-136,142] - file input should behave better with log rotations
+  - bugfix: [LOGSTASH-249] - Input syslog force facility type to be an integer
+  - bugfix: [LOGSTASH-317] - fix file input not to crash when a file
+    is unreadable
+
+  ## filters
+  - feature: [LOGSTASH-66,150]: libgrok re-written in pure ruby (no more
+    FFI / external libgrok.so dependency!)
+  - feature: [LOGSTASH-292,316] - Filters should run on all events if no condition
+    is applied (type, etc).
+  - feature: [LOGSTASH-292,316] - Filters can now act on specific tags (or sets
+    of tags).
+  - bugfix: [LOGSTASH-285] - for grok, add 'keep_empty_captures' setting to
+    allow dropping of empty captures. This is true by default.
+  - feature: [LOGSTASH-219] - support parsing unix epoch times
+  - feature: [LOGSTASH-207] - new filter to parse a field as json merging it
+    into the event.
+  - feature: [LOGSTASH-267,254] - add DNS filter for doing forward or
+    reverse DNS on an event field
+  - feature: [LOGSTASH-57] - add mutate filter to help with manipulating
+    event field content and type
+
+  ## outputs
+  - feature: zeromq output. Requires you have libzmq installed on your system.
+  - feature: new file output plugin
+  - bugfix: [LOGSTASH-307] embedded elasticsearch now acts as a full ES server;
+    previously embedded was only accessible from within the logstash process.
+  - bugfix: [LOGSTASH-302] - logstash's log level (-v, -vv flags) now control
+    the log output from the elasticsearch client via log4j.
+  - bugfix: many gelf output enhancements and bugfixes
+  - feature: [LOGSTASH-281] - add https support to loggly output
+  - bugfix: [LOGSTASH-167] - limit number of in-flight requests to the
+    elasticsearch node to avoid creating too many threads (one thread per
+    pending write request)
+  - bugfix: [LOGSTASH-181] - output/statsd: set sender properly
+  - bugfix: [LOGSTASH-173] - GELF output can throw an exception during gelf notify
+  - bugfix: [LOGSTASH-182] - grep filter should act on all events if no type is
+    specified.
+  - bugfix: [LOGSTASH-309] - file output can now write to named pipes (fifo)
+
+
+1.0.17 (Aug 12, 2011)
+  - Bugs fixed
+    - [LOGSTASH-147] - grok filter incorrectly adding fields when a match failed
+    - [LOGSTASH-151] - Fix bug in routing keys on AMQP
+    - [LOGSTASH-156] - amqp issue with 1.0.16?
+
+  - Improvement
+    - [LOGSTASH-148] - AMQP input should allow queue name to be specified separately from exchange name
+    - [LOGSTASH-157] - Plugin doc generator should make regexp config names more readable
+
+  - New Feature
+    - [LOGSTASH-153] - syslog input: make timestamp an optional field
+    - [LOGSTASH-154] - Make error reporting show up in the web UI
+
+1.0.16 (Aug 18, 2011)
+  - Fix elasticsearch client problem with 1.0.15 - jruby-elasticsearch gem
+    version required is now 0.0.10 (to work with elasticsearch 0.17.6)
+
+1.0.15 (Aug 18, 2011)
+  - IMPORTANT: Upgraded to ElasticSearch 0.17.6 - this brings a number of bug
+    fixes including an OOM error caused during high index rates in some
+    conditions.
+    NOTE: You *must* use same main version of elasticsearch as logstash does,
+    so if you are still using elasticsearch server 0.16.x - you need to upgrade
+    your server before the elasticsearch output will work. If you are using
+    the 'embedded' elasticsearch feature of logstash, you do not need to make
+    any changes.
+  - feature: tcp input and output plugins can now operate in either client
+    (connect) or server (listen) modes.
+  - feature: new output plugin "statsd" which lets you increment or record
+    timings from your logs to a statsd agent
+  - feature: new redis 'pattern_channel' input support for PSUBSCRIBE
+  - feature: new output plugin "graphite" for taking metrics from events and
+    shipping them off to your graphite/carbon server.
+  - feature: new output plugin "ganglia" for shipping metrics to ganglia
+    gmond server.
+  - feature: new output plugin "xmpp" for shipping events over jabber/xmpp
+  - feature: new input plugin "xmpp" for receiving events over jabber/xmpp
+  - feature: amqp input now supports routing keys.
+    https://logstash.jira.com/browse/LOGSTASH-122
+  - feature: amqp output now supports setting routing key dynamically.
+    https://logstash.jira.com/browse/LOGSTASH-122
+  - feature: amqp input/output both now support SSL.
+    https://logstash.jira.com/browse/LOGSTASH-131
+  - feature: new input plugin "exec" for taking events from executed commands
+    like shell scripts or other tools.
+  - feature: new filter plugin "split" for splitting one event into multiple.
+    It was written primarily for the new "exec" input to allow you to split
+    the output of a single command run by line into multiple events.
+  - misc: upgraded jar releases to use JRuby 1.6.3
+  - bugfix: syslog input shouldn't crash anymore on weird network behaviors
+    like portscanning, etc.
+    https://logstash.jira.com/browse/LOGSTASH-130
+
+1.0.14 (Jul 1, 2011)
+  - feature: new output plugin "loggly" which lets you ship logs to loggly.com
+  - feature: new output plugin "zabbix" - similar to the nagios output, but
+    works with the Zabbix monitoring system. Contributed by Johan at
+    Mach Technology.
+  - feature: New agent '-e' flag which lets you specify a config in a string.
+    If you specify no 'input' plugins, default is stdin { type => stdin }
+    If you specify no 'output' plugins, default is stdout { debug => true }
+    This is intended to be used for hacking with or debugging filters, but
+    you can specify an entire config here if you choose.
+  - feature: Agent '-f' flag now supports directories and globs. If you specify
+    a directory, all files in that directory will be loaded as a single config.
+    If you specify a glob, all files matching that glob will be loaded as a
+    single config.
+  - feature: gelf output now allows you to override the 'sender'. This defaults
+    to the source host originating the event, but can be set to anything now.
+    It supports dynamic values, so you can use fields from your event as the
+    sender. Contributed by John Vincent
+    Issue: https://github.com/logstash/logstash/pull/30
+  - feature: added new feature to libgrok that allows you to define patterns
+    in-line, like "%{FOO=\d+}" defines 'FOO' match \d+ and captures as such.
+    To use this new feature, you must upgrade libgrok to at least 1.20110630
+    Issue: https://logstash.jira.com/browse/LOGSTASH-94
+  - feature: grok filter now supports 'break_on_match' defaulting to true
+    (this was the original behavior). If you set it to false, it will attempt
+    to match all patterns and create new fields as normal. If left default
+    (true), it will break after the first successful match.
+  - feature: grok filter now supports parsing any field. You can do either of
+    these: grok { match => [ "fieldname", "pattern" ] }
+    or this: grok { fieldname => "pattern" }
+    The older 'pattern' attribute still means the same thing, and is equivalent
+    to this: grok { match => [ "@message", "pattern" ] }
+    Issue: https://logstash.jira.com/browse/LOGSTASH-101
+  - feature: elasticsearch - when embedded is true, you can now set the
+    'embedded_http_port' to configure which port the embedded elasticsearch
+    server listens on. This is only valid for the embedded elasticsearch
+    configuration. https://logstash.jira.com/browse/LOGSTASH-117
+  - bugfix: amqp input now reconnects properly when the amqp broker restarts.
+  - bugfix: Fix bug in gelf output when a fields were not arrays but numbers.
+    Issue: https://logstash.jira.com/browse/LOGSTASH-113
+  - bugfix: Fix a bug in syslog udp input due to misfeatures in Ruby's URI
+    class. https://logstash.jira.com/browse/LOGSTASH-115
+  - misc: jquery and jquery ui now ship with logstash; previously they were
+    loaded externally
+  - testing: fixed some bugs in the elasticsearch test itself, all green now.
+  - testing: fixed logstash-test to now run properly
+
+1.0.12 (Jun 9, 2011)
+  - misc: clean up some excess debugging output
+  - feature: for tcp input, allow 'data_timeout => -1' to mean "never time out"
+
+1.0.11 (Jun 9, 2011)
+  - deprecated: The redis 'name' and 'queue' options for both input and output
+    are now deprecated. They will be removed in a future version.
+  - feature: The redis input and output now supports both lists and channels.
+  - feature: Refactor runner to allow you to run multiple things in a single
+    process.  You can end each instance with '--' flag. For example, to run one
+    agent and one web instance:
+      % java -jar logstash-blah.jar agent -f myconfig -- web
+  - feature: Add 'embedded' option to the elasticsearch output:
+      elasticsearch { embedded => true }
+    Default is false. If true, logstash will run an elasticsearch server
+    in the same process as logstash. This is really useful if you are just
+    starting out or only need one one elasticsearch server.
+  - feature: Added a logstash web backend feature for elasticsearch that tells
+    logstash to use the 'local' (in process) elasticsearch:
+      --backend elasticsearch:///?local
+  - feature: Added 'named_captures_only' option to grok filter. This will have
+    logstash only keep the captures you give names to - for example %{NUMBER}
+    won't be kept, but %{NUMBER:bytes} will be.
+  - feature: Add 'bind_host' option to elasticsearch output. This lets you choose the
+    address ElasticSearch client uses to bind to - useful if you have a
+    multihomed server.
+  - feature: The mongodb output now supports authentication
+  - bugfix: Fix bug in GELF output that caused the gelf short_message to be set as an
+    array if it came from a grok value. The short_message field should only
+    now be a string properly.
+  - bugfix: Fix bug in grep filter that would drop/cancel events if you had
+    more than one event type flowing through filters and didn't have a grep
+    filter defined for each type.
+  - misc: Updated gem dependencies (tests still pass)
+  - misc: With the above two points, you can now run a single logstash process
+    that includes elasticsearch server, logstash agent, and logstash web.
+
+1.0.10 (May 23, 2011)
+  - Fix tcp input bug (LOGSTASH-88) that would drop connections.
+  - Grok patterns_dir (filter config) and --grok-patterns-dir (cmdline opt)
+    are now working.
+  - GELF output now properly sends extra fields from the log event (prefixed
+    with a "_") and sets timestamp to seconds-since-epoch (millisecond
+    precision and time zone information is lost, but this is the format GELF
+    asks for).
+  - Inputs support specifying the format of input data (see "format" and
+    "message_format" input config parameters).
+  - Grok filter no longer incorrectly tags _grokparsefailure when more than
+    one grok filter is enabled (for multiple types) or when an event has
+    no grok configuration for it's type.
+  - Fix bug where an invalid HTTP Referer: would break grok parsing of the
+    log line (used to expect %{URI}). Since Referer: is not sanitized in
+    the HTTP layer, we cannot assume it will be a well formed %{URI}.
+
+1.0.9 (May 18, 2011)
+  - Fix crash bug caused by refactoring that left 'break' calls in code
+    that no longer used loops.
+
+1.0.8 (May 17, 2011)
+  - Remove beanstalk support because the library (beanstalk-client) is GPL3. I
+    am not a lawyer, but I'm not waiting around to have someone complain about
+    license incompatibilities.
+  - fix bug in jar build
+
+1.0.7 (May 16, 2011)
+  - logstash 'web' now allows you to specify the elasticsearch clustername;
+    --backend elasticsearch://[host[:port]]/[clustername]
+  - GELF output now supports dynamic strings for level and facility
+    https://logstash.jira.com/browse/LOGSTASH-83
+  - 'amqp' output supports persistent messages over AMQP, now. Tunable.
+    https://logstash.jira.com/browse/LOGSTASH-81
+  - Redis input and output are now supported. (Contributed by dokipen)
+  - Add shutdown processing. Shutdown starts when all inputs finish (like
+    stdin) The sequence progresses using the same pipeline as the
+    inputs/filters/outputs, so all in-flight events should finish getting
+    processed before the final shutdown event makes it's way to the outputs.
+  - Add retries to unhandled input exceptions (LOGSTASH-84)
+
+1.0.6 (May 11, 2011)
+  * Remove 'sigar' from monolithic jar packaging. This removes a boatload of
+    unnecessary warning messages on startup whenever you use elasticsearch
+    output or logstash-web.
+    Issue: https://logstash.jira.com/browse/LOGSTASH-79
+
+1.0.5 (May 10, 2011)
+  * fix queues when durable is set to true
+
 1.0.4 (May 9, 2011)
   * Fix bugs in syslog input
 
@@ -7,6 +1373,7 @@
 
 1.0.1 (May 7, 2011)
   * Fix password auth for amqp and stomp (Reported by Luke Macken)
+  * Fix default elasticsearch target for logstash-web (Reported by Donald Gordon)
 
 1.0.0 (May 6, 2011)
   * First major release.
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
new file mode 100644
index 00000000000..0249977e1e6
--- /dev/null
+++ b/CONTRIBUTING.md
@@ -0,0 +1,61 @@
+# Contributing to logstash
+
+All contributions are welcome: ideas, patches, documentation, bug reports,
+complaints, etc!
+
+Programming is not a required skill, and there are many ways to help out!
+It is more important to us that you are able to contribute.
+
+That said, some basic guidelines, which you are free to ignore :)
+
+## Want to learn?
+
+Want to lurk about and see what others are doing with logstash? 
+
+* The irc channel (#logstash on irc.freenode.org) is a good place for this
+* The [mailing list](http://groups.google.com/group/logstash-users) is also
+  great for learning from others.
+
+## Got Questions?
+
+Have a problem you want logstash to solve for you? 
+
+* You can email the [mailing list](http://groups.google.com/group/logstash-users)
+* alternately, you are welcome to join the IRC channel #logstash on
+irc.freenode.org and ask for help there!
+
+## Have an Idea or Feature Request?
+
+* File a ticket on [github](https://github.com/elasticsearch/logstash/issues), or email the
+  [mailing list](http://groups.google.com/group/logstash-users), or email
+  me personally (jls@semicomplete.com) if that is more comfortable.
+
+## Something Not Working? Found a Bug?
+
+If you think you found a bug, it probably is a bug.
+
+* File it on [github](https://github.com/elasticsearch/logstash/issues)
+* or the [mailing list](http://groups.google.com/group/logstash-users).
+
+# Contributing Documentation and Code Changes
+
+If you have a bugfix or new feature that you would like to contribute to
+logstash, and you think it will take more than a few minutes to produce the fix
+(ie; write code), it is worth discussing the change with the logstash users and developers first! You can reach us via [github](https://github.com/elasticsearch/logstash/issues), the [mailing list](http://groups.google.com/group/logstash-users), or via IRC (#logstash on freenode irc)
+
+## Contribution Steps
+
+1. Test your changes! Run the test suite ('make test') 
+2. Please make sure you have signed our [Contributor License
+   Agreement](http://www.elasticsearch.org/contributor-agreement/). We are not
+   asking you to assign copyright to us, but to give us the right to distribute
+   your code without restriction. We ask this of all contributors in order to
+   assure our users of the origin and continuing existence of the code. You
+   only need to sign the CLA once.
+3. Send a pull request! Push your changes to your fork of the repository and
+   [submit a pull
+   request](https://help.github.com/articles/using-pull-requests). In the pull
+   request, describe what your changes do and mention any bugs/issues related
+   to the pull request.
+
+
diff --git a/CONTRIBUTORS b/CONTRIBUTORS
index c70dbb07082..e14761e28fe 100644
--- a/CONTRIBUTORS
+++ b/CONTRIBUTORS
@@ -1,8 +1,9 @@
-Project Owners
-* Jordan Sissel (jordansissel)
-* Pete Fritchman (fetep)
+The following is a list of people who have contributed ideas, code, bug
+reports, or in general have helped logstash along its way.
 
 Contributors:
+* Jordan Sissel (jordansissel)
+* Pete Fritchman (fetep)
 * Brice Figureau (masterzen)
 * Vladimir Vuksan (vvuksan)
 * Alexandre Dulaunoy (adulau)
@@ -12,3 +13,69 @@ Contributors:
 * Naresh V (nareshov)
 * John Vincent (lusis)
 * Bob Corsaro (dokipen)
+* kjoconnor
+* Evgeny Zislis (kesor)
+* Johan Venter
+* Jeff Buchbinder (freemed)
+* Dan Peterson (dpiddy)
+* Nick Ethier (nickethier)
+* Joel Merrick (joelio)
+* Michael Leinartas (mleinart)
+* Jesse Newland (jnewland)
+* Yuval Oren (yuvaloren)
+* cmclaughlin
+* Graham Bleach (bleach)
+* Jan Seidl (jseidl)
+* Jean-Luc Geering (jlgeering)
+* Avishai Ish-Shalom (avishai-ish-shalom)
+* turtlebender
+* Nick Padilla (NickPadilla)
+* tucker250
+* gja
+* Max Horbul (mhorbul)
+* Aaron Mildenstein (untergeek)
+* Greg Mefford (gregmefford)
+* JB Barth (jbbarth)
+* Tom Hodder (tolland)
+* David Castro (arimus)
+* Parker DeBardelaben (parkerd)
+* Max Horbul (mhorbul)
+* Harlan Barnes (harlanbarnes)
+* Jeremiah Shirk (jeremiahshirk)
+* Patrick Debois (jedi4ever)
+* bodik
+* Philippe Weber
+* Marc Huffnagle (mhuffnagle)
+* Oliver Gorwits (ollyg)
+* Rashid Khan (rashidkpc)
+* squiddle
+* Andrew Rowson (growse)
+* Cameron Stokes (clstokes)
+* Francesco Salbaroli (wathrog)
+* goblin
+* Mike Worth (MikeWorth)
+* Nic Williams (drnic)
+* Tomas Doran (bobtfish)
+* Xabier de Zuazo (zuazo)
+* Louis Zuckerman (semiosis)
+* Richard Pijnenburg (electrical)
+* James Turnbull (jamesturnbull)
+* Neil Prosser (neilprosser)
+* Alex Dean (alexdean)
+* Jonathan Quail (jonathanq)
+* Kushal Pisavadia (KushalP)
+* Gang Chen (goncha)
+* Michael Klishin (michaelklishin)
+* Marc Fournier (mfournier)
+* Helmut Duregger (hduregger)
+* Matt Dainty (bodgit)
+* Juarez Bochi (jbochi)
+* Bernd Ahlers (bernd)
+* Andrea Forni (andreaforni)
+* Leandro Moreira (leandromoreira)
+* Hao Chen (haoch)
+
+Note: If you've sent me patches, bug reports, or otherwise contributed to
+logstash, and you aren't on the list above and want to be, please let me know
+and I'll make sure you're here. Contributions from folks like you are what make
+open source awesome.
diff --git a/Gemfile b/Gemfile
deleted file mode 100644
index c5b553239bf..00000000000
--- a/Gemfile
+++ /dev/null
@@ -1,26 +0,0 @@
-source :rubygems
-
-gem "bunny" # for amqp support
-gem "uuidtools" # for naming amqp queues
-gem "filewatch", "~> 0.2.3"  # for file tailing
-gem "jls-grok", "~> 0.4.7" # for grok filter
-gem "jruby-elasticsearch", "~> 0.0.7"
-gem "stomp" # for stomp protocol
-gem "json"
-gem "awesome_print"
-
-gem "rack"
-gem "mizuno"
-gem "sinatra"
-gem "haml"
-
-# TODO(sissel): Put this into a group that's only used for monolith packaging
-gem "mongo" # outputs/mongodb
-
-gem "gelf" # outputs/gelf
-
-# For testing/dev
-group :development do
-  gem "stompserver"
-  gem "spoon"
-end
diff --git a/Gemfile.lock b/Gemfile.lock
deleted file mode 100644
index 093597e458f..00000000000
--- a/Gemfile.lock
+++ /dev/null
@@ -1,59 +0,0 @@
-GEM
-  remote: http://rubygems.org/
-  specs:
-    awesome_print (0.3.2)
-    bson (1.3.0-java)
-    bunny (0.6.0)
-    daemons (1.1.2)
-    eventmachine (0.12.10-java)
-    ffi (0.6.3-java)
-    filewatch (0.2.5)
-      ffi
-    gelf (1.1.3)
-      json
-    haml (3.0.25)
-    hoe (2.9.4)
-      rake (>= 0.8.7)
-    jls-grok (0.4.7)
-      ffi (>= 0.6.3)
-    jruby-elasticsearch (0.0.7)
-    json (1.5.1-java)
-    mizuno (0.4.0)
-      rack (>= 1.0.0)
-    mongo (1.3.0)
-      bson (>= 1.3.0)
-    rack (1.2.2)
-    rake (0.8.7)
-    sinatra (1.2.2)
-      rack (~> 1.1)
-      tilt (>= 1.2.2, < 2.0)
-    spoon (0.0.1)
-    stomp (1.1.8)
-    stompserver (0.9.9)
-      daemons (>= 1.0.2)
-      eventmachine (>= 0.7.2)
-      hoe (>= 1.1.1)
-      hoe (>= 1.3.0)
-    tilt (1.2.2)
-    uuidtools (2.1.2)
-
-PLATFORMS
-  java
-
-DEPENDENCIES
-  awesome_print
-  bunny
-  filewatch (~> 0.2.3)
-  gelf
-  haml
-  jls-grok (~> 0.4.7)
-  jruby-elasticsearch (~> 0.0.7)
-  json
-  mizuno
-  mongo
-  rack
-  sinatra
-  spoon
-  stomp
-  stompserver
-  uuidtools
diff --git a/INSTALL b/INSTALL
deleted file mode 100644
index 16b78c729c0..00000000000
--- a/INSTALL
+++ /dev/null
@@ -1,40 +0,0 @@
-If you have questions/complaints/bugs, please email
-logstash-users@googlegroups.com.
-
-# Prereqs:
-These instructions assume you are on a relatively recent
-linux system, that you have a working (C) build environment,
-that you have ruby and rubygems installed.
-
-# Installation Instructions...
-You should have ruby and rubygems installed.
-After that install the following gems, via 'gem install gemname'
-
-required gems:
-- bunny
-- filewatch
-- jls-grok >= 0.4.3
-- json
-- stomp
-- stompserver
-- uuidtools
-
-For the web interface:
-- async_sinatra
-- haml
-- sass
-
-# For 'jls-grok' you will need grok installed.
-  Install the following packages (centos: sudo yum install pkgname)
-  pcre-devel, libevent-devel, gperf
-  If you are on centos download and install a newer flex:
-    ftp://mirrors.kernel.org/fedora/releases/9/Fedora/source/SRPMS/flex-2.5.35-1.fc9.src.rpm
-
-  You'll need grok >= 1.20110308
-  https://github.com/jordansissel/grok/tarball/1.20110308.1
-
-# On Linux:
-  Check your /etc/ld.so.conf, or /etc/ld.so.conf.d/* . 
-  If not already set add /usr/local/lib and run 'ldconfig' as root to update the
-  library cache list.
-
diff --git a/LICENSE b/LICENSE
index b8c3fdc3f63..b3e30706bd8 100644
--- a/LICENSE
+++ b/LICENSE
@@ -1,4 +1,4 @@
-Copyright 2009-2011 Jordan Sissel, Pete Fritchman, and contributors.
+Copyright 2009-2013 Jordan Sissel, Pete Fritchman, and contributors.
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
diff --git a/Makefile b/Makefile
new file mode 100644
index 00000000000..db263b80aff
--- /dev/null
+++ b/Makefile
@@ -0,0 +1,2 @@
+%: 
+	rake $@
diff --git a/README.md b/README.md
index b0e9e086551..a844500203a 100644
--- a/README.md
+++ b/README.md
@@ -1,15 +1,104 @@
-# logstash
+# Logstash
 
-logstash is a tool for managing events and logs. You can use it to collect logs, parse them, and store them for later use (like, for searching). Speaking of searching, logstash comes with a web interface for searching and drilling into all of your logs.
+Logstash is a tool for managing events and logs. You can use it to collect
+logs, parse them, and store them for later use (like, for searching). Speaking
+of searching, Logstash comes with a web interface for searching and drilling
+into all of your logs.
 
-It is fully free and fully open source. The license is New BSD, meaning you are pretty much free to use it however you want in whatever way.
+It is fully free and fully open source. The license is Apache 2.0, meaning you
+are pretty much free to use it however you want in whatever way.
 
 For more info, see <http://logstash.net/>
 
-File bugs here: <http://logstash.jira.com>
+## logstash-contrib
+### AKA "Where'd that plugin go??"
+
+Since version 1.4.0 of Logstash, some of the community-contributed plugins were
+moved to a new home in the
+[Elasticsearch logstash-contrib repo](https://github.com/elasticsearch/logstash-contrib).
+If you can't find a plugin here which you've previously used, odds are it is now
+located there. The good news is that these plugins are simple to install using the
+[Logstash manual plugin installation script](http://logstash.net/docs/latest/contrib-plugins).
+
+## Need Help?
+
+Need help? Try #logstash on freenode irc or the logstash-users@googlegroups.com
+mailing list.
+
+You can also find documentation on the <http://logstash.net> site.
+
+## Developing
+
+To get started, you'll need *any* ruby available and it should come with the `rake` tool.
+
+Here's how to get started with Logstash development:
+
+    rake bootstrap
+    
+Other commands:
+
+    # to use Logstash gems or libraries in irb, use the following
+    # this gets you an 'irb' shell with Logstash's environment
+    bin/logstash irb
+
+    # Run Logstash
+    bin/logstash agent [options]
+
+Notes about using other rubies. If you don't use rvm, you can probably skip
+this paragraph. Logstash works with other rubies, and if you wish to use your
+own ruby you must set `USE_RUBY=1` in your environment.
+
+We recommend using flatland/drip for faster startup times during development. To
+tell Logstash to use drip, set `USE_DRIP=1` in your environment.
+
+## Testing
+
+There are a few ways to run the tests. For development, using `bin/logstash
+rspec <some spec>` will suffice:
+
+    % bin/logstash rspec spec/filters/grok.rb
+    ...................
+
+    Finished in 0.123 seconds
+    19 examples, 0 failures
+
+If you want to run all the tests from source, do:
+
+    rake test
+
+## Building
+
+Building is not required. You are highly recommended to download the releases
+we provide from the Logstash site!
+
+If you want to build the release tarball yourself, run:
+
+    rake artifact:tar
+
+You can build rpms and debs, if you need those. Building rpms requires you have [fpm](https://github.com/jordansissel/fpm), then do this:
+
+    # Build an RPM
+    rake artifact:rpm
+
+    # Build a Debian/Ubuntu package
+    rake artifact:deb
+
+## Project Principles
+
+* Community: If a newbie has a bad time, it's a bug.
+* Software: Make it work, then make it right, then make it fast.
+* Technology: If it doesn't do a thing today, we can make it do it tomorrow.
 
 ## Contributing
 
-If you're going to send me patches, I prefer pull requests on github. However, I will also accept normal patches.
+All contributions are welcome: ideas, patches, documentation, bug reports,
+complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and
+maintainers or community members  saying "send patches or die" - you will not
+see that here.
+
+It is more important to me that you are able to contribute.
 
-If you want to request a new feature, file it on jira: <http://logstash.jira.com>
+For more information about contributing, see the
+[CONTRIBUTING](CONTRIBUTING.md) file.
diff --git a/Rakefile b/Rakefile
old mode 100755
new mode 100644
index c9cf07c7abd..e67cca5c5ec
--- a/Rakefile
+++ b/Rakefile
@@ -1,283 +1,18 @@
-require 'tempfile'
-require 'ftools' # fails in 1.9.2
 
-# TODO(sissel): load the gemspec and parse the version from it instead.
-LOGSTASH_VERSION = "1.0.2" 
+$: << File.join(File.dirname(__FILE__), "lib")
 
-# Compile config grammar (ragel -> ruby)
-file "lib/logstash/config/grammar.rb" => ["lib/logstash/config/grammar.rl"] do
-  sh "make -C lib/logstash/config grammar.rb"
-end
-
-# Taken from 'jrubyc' 
-#  Currently this code is commented out because jruby emits this:
-#     Failure during compilation of file logstash/web/helpers/require_param.rb:
-#       java.lang.RuntimeException: java.io.FileNotFoundException: File path
-#       /home/jls/projects/logstash/logstash/web/helpers/require_param.rb
-#       does not start with parent path /home/jls/projects/logstash/lib
-#
-#     org/jruby/util/JavaNameMangler.java:105:in `mangleFilenameForClasspath'
-#     org/jruby/util/JavaNameMangler.java:32:in `mangleFilenameForClasspath'
-#require 'jruby/jrubyc'
-##args = [ "-p", "net.logstash" ]
-#args = ["-d", "build"]
-#args += Dir.glob("**/*.rb")
-#status = JRuby::Compiler::compile_argv(args)
-#if (status != 0)
-  #puts "Compilation FAILED: #{status} error(s) encountered"
-  #exit status
-#end
+task "default" => "help"
 
-task :clean do
-  sh "rm -rf .bundle"
-  sh "rm -rf build-jar-thin"
-  sh "rm -rf build-jar"
-  sh "rm -rf build"
-  sh "rm -rf vendor"
-end
+task "help" do
+  puts <<HELP
+What do you want to do?
 
-task :compile => "lib/logstash/config/grammar.rb" do |t|
-  mkdir_p "build"
-  sh "rm -rf lib/net"
-  Dir.chdir("lib") do
-    args = Dir.glob("**/*.rb")
-    sh "jrubyc", "-t", "../build", *args
-  end
-  Dir.chdir("test") do
-    args = Dir.glob("**/*.rb")
-    sh "jrubyc", "-t", "../build", *args
-  end
-end
+Packaging?
+  `rake artifact:tar`  to build a deployable .tar.gz
+  `rake artifact:rpm`  to build an rpm
+  `rake artifact:deb`  to build an deb
 
-task :jar => [ "package:monolith:jar" ] do |t|
-  # Nothing
+Developing?
+  `rake bootstrap`     installs any dependencies for doing Logstash development
+HELP
 end
-
-VERSIONS = {
-  :jruby => "1.6.0", # Any of CPL1.0/GPL2.0/LGPL2.1 ? Confusing, but OK.
-  #:elasticsearch => "0.15.2", # Apache 2.0 license
-  :elasticsearch => "0.16.0", # Apache 2.0 license
-  :joda => "1.6.2",  # Apache 2.0 license
-}
-
-namespace :vendor do
-  file "vendor/jar" do |t|
-    mkdir_p t.name
-  end
-
-  # Download jruby.jar
-  file "vendor/jar/jruby-complete-#{VERSIONS[:jruby]}.jar" => "vendor/jar" do |t|
-    baseurl = "http://repository.codehaus.org/org/jruby/jruby-complete"
-    if !File.exists?(t.name)
-      sh "wget -O #{t.name} #{baseurl}/#{VERSIONS[:jruby]}/#{File.basename(t.name)}"
-    end
-  end # jruby
-
-  task :jruby => "vendor/jar/jruby-complete-#{VERSIONS[:jruby]}.jar" do
-    # nothing to do, the dep does it.
-  end
-
-  # Download elasticsearch + deps (for outputs/elasticsearch)
-  task :elasticsearch => "vendor/jar" do
-    version = VERSIONS[:elasticsearch]
-    tarball = "elasticsearch-#{version}.tar.gz"
-    url = "http://github.com/downloads/elasticsearch/elasticsearch/#{tarball}"
-    if !File.exists?(tarball)
-      # --no-check-certificate is for github and wget not supporting wildcard
-      # certs sanely.
-      sh "wget -O #{tarball} --no-check-certificate #{url}"
-    end
-
-    sh "tar -zxf #{tarball} -C vendor/jar/ elasticsearch-#{version}/lib"
-  end # elasticsearch
-
-  task :joda => "vendor/jar" do
-    version = VERSIONS[:joda] 
-    baseurl = "http://sourceforge.net/projects/joda-time/files/joda-time"
-    tarball = "joda-time-#{version}-bin.tar.gz"
-    url = "#{baseurl}/#{version}/#{tarball}/download"
-
-    if !File.exists?(tarball)
-      sh "wget -O #{tarball} #{url}"
-    end
-
-    sh "tar -zxf #{tarball} -C vendor/jar/ joda-time-#{version}/joda-time-#{version}.jar"
-  end # joda
-
-  task :gems => "vendor/jar" do
-    puts "=> Installing gems to vendor/bundle/..."
-    sh "bundle install --path #{File.join("vendor", "bundle")}"
-  end
-end # vendor namespace
-
-namespace :package do
-  task :gem do
-    sh "gem build logstash.gemspec"
-  end
-
-  monolith_deps = [ "vendor:jruby", "vendor:gems", "vendor:elasticsearch", "compile" ]
-
-  namespace :monolith do
-    task :tar => monolith_deps do
-      paths = %w{ bin CHANGELOG CONTRIBUTORS etc examples Gemfile Gemfile.lock
-                  INSTALL lib LICENSE patterns Rakefile README.md STYLE.md test
-                  TODO USAGE vendor/bundle vendor/jar }
-      sh "tar -zcf logstash-monolithic-someversion.tar.gz #{paths.join(" ")}"
-    end # package:monolith:tar
-
-    task :jar => monolith_deps do
-      mkdir_p "build-jar"
-
-      # Unpack all the 3rdparty jars and any jars in gems
-      Dir.glob("vendor/{bundle,jar}/**/*.jar").each do |jar|
-        puts "=> Unpacking #{jar} into build-jar/"
-        Dir.chdir("build-jar") do 
-          sh "jar xf ../#{jar}"
-        end
-      end
-
-      # We compile stuff to build/...
-      # TODO(sissel): Could probably just use 'jar uf' for this?
-      #Dir.glob("build/**/*.class").each do |file|
-        #target = File.join("build-jar", file.gsub("build/", ""))
-        #mkdir_p File.dirname(target)
-        #puts "=> Copying #{file} => #{target}"
-        #File.copy(file, target)
-      #end
-
-      # Purge any extra files we don't need in META-INF (like manifests and
-      # jar signatures)
-      ["INDEX.LIST", "MANIFEST.MF", "ECLIPSEF.RSA", "ECLIPSEF.SF"].each do |file|
-        File.delete(File.join("build-jar", "META-INF", file)) rescue nil
-      end
-      #FileUtils.rm_r(File.join("build-jar", "META-INF")) rescue nil
-
-      output = "logstash-#{LOGSTASH_VERSION}-monolithic.jar"
-      sh "jar cfe #{output} logstash.runner -C build-jar ."
-
-      jar_update_args = []
-
-      # Learned how to do this mostly from here:
-      # http://blog.nicksieger.com/articles/2009/01/10/jruby-1-1-6-gems-in-a-jar
-      #
-      # Add bundled gems to the jar
-      # Skip the 'cache' dir which is just the original .gem files
-      gem_dirs = %w{bin doc gems specifications}
-      gem_root = File.join(%w{vendor bundle jruby 1.8})
-      # for each dir, build args: -C vendor/bundle/jruby/1.8 bin, etc
-      gem_jar_args = gem_dirs.collect { |dir| ["-C", gem_root, dir ] }.flatten
-      jar_update_args += gem_jar_args
-
-      # Add compiled our compiled ruby code
-      jar_update_args += %w{ -C build . }
-
-      # Add web stuff
-      jar_update_args += %w{ -C lib logstash/web/public }
-      jar_update_args += %w{ -C lib logstash/web/views }
-
-      # Add test code
-      #jar_update_args += %w{ -C test logstsah }
-
-      # Add grok patterns
-      jar_update_args << "patterns"
-
-      # Update with other files and also build an index.
-      sh "jar uf #{output} #{jar_update_args.join(" ")}"
-      sh "jar i #{output}"
-    end # task package:monolith:jar
-  end # namespace monolith
-
-  task :jar => [ "vendor:jruby", "vendor:gems", "compile" ] do
-    builddir = "build-jar-thin"
-    mkdir_p builddir
-
-    # Unpack jruby
-    Dir.glob("vendor/jar/jruby-complete-1.6.0.jar").each do |jar|
-      puts "=> Unpacking #{jar} into #{builddir}/"
-      Dir.chdir(builddir) do 
-        sh "jar xf ../#{jar}"
-      end
-    end
-
-    ["INDEX.LIST", "MANIFEST.MF", "ECLIPSEF.RSA", "ECLIPSEF.SF"].each do |file|
-      File.delete(File.join(builddir, "META-INF", file)) rescue nil
-    end
-
-    output = "logstash-#{LOGSTASH_VERSION}.jar"
-    sh "jar cfe #{output} logstash.runner -C #{builddir} ."
-
-    jar_update_args = []
-
-    # Learned how to do this mostly from here:
-    # http://blog.nicksieger.com/articles/2009/01/10/jruby-1-1-6-gems-in-a-jar
-    #
-    # Add bundled gems to the jar
-    # Skip the 'cache' dir which is just the original .gem files
-    gem_dirs = %w{bin doc gems specifications}
-    gem_root = File.join(%w{vendor bundle jruby 1.8})
-    # for each dir, build args: -C vendor/bundle/jruby/1.8 bin, etc
-    gem_jar_args = gem_dirs.collect { |dir| ["-C", gem_root, dir ] }.flatten
-    jar_update_args += gem_jar_args
-
-    # Add compiled our compiled ruby code
-    jar_update_args += %w{ -C build . }
-
-    # Add web stuff
-    jar_update_args += %w{ -C lib logstash/web/public }
-    jar_update_args += %w{ -C lib logstash/web/views }
-
-    # Add test code
-    #jar_update_args += %w{ -C test logstsah }
-
-    # Add grok patterns
-    jar_update_args << "patterns"
-
-    # Update with other files and also build an index.
-    sh "jar uf #{output} #{jar_update_args.join(" ")}"
-    sh "jar i #{output}"
-  end # task package:jar
-end # namespace package
-
-task :test do
-  sh "cd test; ruby logstash_test_runner.rb"
-end
-
-task :docs => [:docgen, :doccopy, :docindex ] do
-end
-
-task :require_output_env do
-  if ENV["output"].nil?
-    raise "No output variable set. Run like: 'rake docs output=path/to/output'"
-  end
-end
-
-task :doccopy => [:require_output_env] do
-  if ENV["output"].nil?
-    raise "No output variable set. Run like: 'rake docs output=path/to/output'"
-  end
-
-  Dir.glob("docs/**/*").each do |doc|
-    dir = File.join(ENV["output"], File.dirname(doc).gsub(/docs\/?/, ""))
-    mkdir_p dir
-    puts "Copy #{doc} => #{dir}"
-    cp(doc, dir)
-  end
-end
-
-task :docindex => [:require_output_env] do
-  sh "ruby docs/generate_index.rb #{ENV["output"]} > #{ENV["output"]}/index.html"
-end
-
-task :docgen => [:require_output_env] do
-  if ENV["output"].nil?
-    raise "No output variable set. Run like: 'rake docgen output=path/to/output'"
-  end
-
-  sh "find lib/logstash/inputs lib/logstash/filters lib/logstash/outputs  -type f -not -name 'base.rb' -a -name '*.rb'| xargs ruby docs/docgen.rb -o #{ENV["output"]}"
-end
-
-task :publish do
-  latest_gem = %x{ls -t logstash-[0-9]*.gem}.split("\n").first
-  sh "gem push #{latest_gem}"
-end
-
diff --git a/STYLE.md b/STYLE.md
index 8331e635628..894a31e88f3 100644
--- a/STYLE.md
+++ b/STYLE.md
@@ -1,9 +1,8 @@
 # Style Guide 
 
 Rather than write a full style guide, please follow by examples you see in the
-code.  If you send me a patch, I will not reject it for style reasons (but I
-will fix it before it gets committed) 
-
+code. If you send me a patch, I will not reject it for style reasons (but I
+will fix it before it gets committed).
 
 ## Logging
 
@@ -15,17 +14,19 @@ Rather than this:
 
 Do this:
     
-    @logger.info(["Some error occured in this request", { :request => request, :input => input, :client => ip}])
-
+    @logger.info("Some error occured in this request", :request => request, :input => input, :client => ip)
 
 ## Code Style
 
+* comment everything you can think of.
 * indentation: 2 spaces
 * between methods: 1 line
 * sort your requires
-* long lines should wrap at 80 characters. If you wrap at an operator (or, +,
-  etc) start the next line with that operator.
+* long lines should wrap at 80 characters. If you wrap at an operator ('or',
+  '+', etc) start the next line with that operator.
+* parentheses on function definitions/calls
 * explicit is better than implicit
+  * implicit returns are forbidden except in the case of a single expression 
 
 The point is consistency and documentation. If you see inconsistencies, let me
 know, and I'll fix them :)
@@ -55,7 +56,41 @@ Short example:
           # If it seems unreasonable, wrap and indent 4 spaces.
           some_really_long_function_call_blah_blah_blah(arg1,
               arg2, arg3, arg4)
+
+          # indent the 'when' inside a 'case'.
+          case foo
+            when "bar"
+              puts "Hello world"
+            when /testing/
+              puts "testing
+            else
+              puts "I got nothin'"
+          end # case foo
             
         end # def somefunc
       end # class Foo
 
+## Specific cases
+
+### Hash Syntax
+
+Use of the "hash colon" syntax (ruby 1.9) is not accepted.
+
+    # This is NOT good.
+    { foo: "bar" }
+
+    # This is good.
+    { :foo => "bar" }
+
+### String#[]
+
+String#[] with one numeric argument must not be used due to bugs and
+inconsistencies between ruby versions.
+
+    str = "foo"
+
+    # This is NOT good
+    str[0]
+
+    # This is good.
+    str[0, 1]
diff --git a/TODO b/TODO
deleted file mode 100644
index db5c023da84..00000000000
--- a/TODO
+++ /dev/null
@@ -1,12 +0,0 @@
-- TODO: finish refactor work. Document new agent model of input+filter+output.
-  All messages are arbitrary hashes now (encoding on wire is json)
-
-  * Document it. 
-  * Move 'Agent' to LogStash::Agent
-
-  * Add inspectabilty to agents (Some kind of RPC to query status)
-  
-  Write examples:
-  * log monitor agent (react/alert to messages over amqp, for example)
-  * write storage agent (mongodb, elasticsearch, etc)
-  * web frontend using websockets for real-time search through a browser
diff --git a/USAGE b/USAGE
deleted file mode 100644
index 687eda6a8cc..00000000000
--- a/USAGE
+++ /dev/null
@@ -1,4 +0,0 @@
-See the docs online:
-
-* http://code.google.com/p/logstash/wiki/GettingStartedCentralized
-* http://code.google.com/p/logstash/wiki/GettingStartedStandalone
diff --git a/acceptance_spec/acceptance/install_spec.rb b/acceptance_spec/acceptance/install_spec.rb
new file mode 100644
index 00000000000..2492737d53d
--- /dev/null
+++ b/acceptance_spec/acceptance/install_spec.rb
@@ -0,0 +1,152 @@
+require_relative '../spec_helper_acceptance'
+
+describe "Logstash class:" do
+
+  case fact('osfamily')
+  when 'RedHat'
+    core_package_name    = 'logstash'
+    contrib_package_name = 'logstash-contrib'
+    service_name         = 'logstash'
+    core_url             = 'https://s3-us-west-2.amazonaws.com/build.elasticsearch.org/logstash/master/nightly/logstash-latest.rpm'
+    contrib_url          = 'https://s3-us-west-2.amazonaws.com/build.elasticsearch.org/logstash/master/nightly/logstash-contrib-latest.rpm'
+    pid_file             = '/var/run/logstash.pid'
+  when 'Debian'
+    core_package_name    = 'logstash'
+    contrib_package_name = 'logstash-contrib'
+    service_name         = 'logstash'
+    core_url             = 'https://s3-us-west-2.amazonaws.com/build.elasticsearch.org/logstash/master/nightly/logstash-latest.deb'
+    contrib_url          = 'https://s3-us-west-2.amazonaws.com/build.elasticsearch.org/logstash/master/nightly/logstash-contrib-latest.deb'
+    pid_file             = '/var/run/logstash.pid'
+  end
+
+  context "Install Nightly core package" do
+
+    it 'should run successfully' do
+      pp = "class { 'logstash': package_url => '#{core_url}', java_install => true }
+            logstash::configfile { 'basic_config': content => 'input { tcp { port => 2000 } } output { stdout { } } ' }
+           "
+
+      # Run it twice and test for idempotency
+      apply_manifest(pp, :catch_failures => true)
+      sleep 20
+      expect(apply_manifest(pp, :catch_failures => true).exit_code).to be_zero
+
+    end
+
+    describe package(core_package_name) do
+      it { should be_installed }
+    end
+
+    describe service(service_name) do
+      it { should be_enabled }
+      it { should be_running }
+    end
+
+    describe file(pid_file) do
+      it { should be_file }
+      its(:content) { should match /[0-9]+/ }
+    end
+
+    describe port(2000) do
+      it {
+        sleep 30
+        should be_listening
+      }
+    end
+
+  end
+
+  context "ensure we are still running" do
+
+    describe service(service_name) do
+      it {
+        sleep 30
+        should be_running
+      }
+    end
+
+    describe port(2000) do
+      it { should be_listening }
+    end
+
+  end
+
+  describe "module removal" do
+
+    it 'should run successfully' do
+      pp = "class { 'logstash': ensure => 'absent' }"
+
+      # Run it twice and test for idempotency
+      apply_manifest(pp, :catch_failures => true)
+
+    end
+
+    describe service(service_name) do
+      it { should_not be_enabled }
+      it { should_not be_running }
+    end
+
+    describe package(core_package_name) do
+      it { should_not be_installed }
+    end
+
+  end
+
+  context "Install Nightly core + contrib packages" do
+
+    it 'should run successfully' do
+      pp = "class { 'logstash': package_url => '#{core_url}', java_install => true, contrib_package_url => '#{contrib_url}', install_contrib => true }
+            logstash::configfile { 'basic_config': content => 'input { tcp { port => 2000 } } output { stdout { } } ' }
+           "
+
+      # Run it twice and test for idempotency
+      apply_manifest(pp, :catch_failures => true)
+      sleep 20
+      expect(apply_manifest(pp, :catch_failures => true).exit_code).to be_zero
+
+    end
+
+    describe package(core_package_name) do
+      it { should be_installed }
+    end
+
+    describe package(contrib_package_name) do
+      it { should be_installed }
+    end
+
+    describe service(service_name) do
+      it { should be_enabled }
+      it { should be_running }
+    end
+
+    describe file(pid_file) do
+      it { should be_file }
+      its(:content) { should match /[0-9]+/ }
+    end
+
+    describe port(2000) do
+      it {
+        sleep 30
+        should be_listening
+      }
+    end
+
+  end
+
+  context "ensure we are still running" do
+
+    describe service(service_name) do
+      it {
+        sleep 30
+        should be_running
+      }
+    end
+
+    describe port(2000) do
+      it { should be_listening }
+    end
+
+  end
+
+end
+
diff --git a/acceptance_spec/acceptance/nodesets/centos-6-x64.yml b/acceptance_spec/acceptance/nodesets/centos-6-x64.yml
new file mode 100644
index 00000000000..6429c65c7a5
--- /dev/null
+++ b/acceptance_spec/acceptance/nodesets/centos-6-x64.yml
@@ -0,0 +1,15 @@
+HOSTS:
+  centos-6-x64:
+    roles:
+      - master
+      - database
+      - dashboard
+    platform: el-6-x86_64
+    image: jordansissel/system:centos-6.4
+    hypervisor: docker
+    docker_cmd: '["/sbin/init"]'
+    docker_image_commands:
+      - 'yum install -y wget ntpdate rubygems ruby-augeas ruby-devel augeas-devel'
+      - 'touch /etc/sysconfig/network'
+CONFIG:
+  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/debian-6-x64.yml b/acceptance_spec/acceptance/nodesets/debian-6-x64.yml
new file mode 100644
index 00000000000..6468721375a
--- /dev/null
+++ b/acceptance_spec/acceptance/nodesets/debian-6-x64.yml
@@ -0,0 +1,15 @@
+HOSTS:
+  debian-6:
+    roles:
+      - master
+      - database
+      - dashboard
+    platform: debian-6-amd64
+    image: jordansissel/system:debian-6.0.8
+    hypervisor: docker
+    docker_cmd: '["/sbin/init"]'
+    docker_image_commands:
+      - 'apt-get install -yq lsb-release wget net-tools ruby rubygems ruby1.8-dev libaugeas-dev libaugeas-ruby ntpdate locales-all'
+      - 'REALLY_GEM_UPDATE_SYSTEM=1 gem update --system --no-ri --no-rdoc'
+CONFIG:
+  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/debian-7-x64.yml b/acceptance_spec/acceptance/nodesets/debian-7-x64.yml
new file mode 100644
index 00000000000..c90dafaefd8
--- /dev/null
+++ b/acceptance_spec/acceptance/nodesets/debian-7-x64.yml
@@ -0,0 +1,15 @@
+HOSTS:
+  debian-7:
+    roles:
+      - master
+      - database
+      - dashboard
+    platform: debian-7-amd64
+    image: jordansissel/system:debian-7.3
+    hypervisor: docker
+    docker_cmd: '["/sbin/init"]'
+    docker_image_commands:
+      - 'apt-get install -yq lsb-release wget net-tools ruby rubygems ruby1.8-dev libaugeas-dev libaugeas-ruby ntpdate locales-all'
+      - 'REALLY_GEM_UPDATE_SYSTEM=1 gem update --system --no-ri --no-rdoc'
+CONFIG:
+  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml b/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml
new file mode 100644
index 00000000000..e53d08047a3
--- /dev/null
+++ b/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml
@@ -0,0 +1,14 @@
+HOSTS:
+  ubuntu-12-04:
+    roles:
+      - master
+      - database
+      - dashboard
+    platform: ubuntu-12.04-amd64
+    image: jordansissel/system:ubuntu-12.04
+    hypervisor: docker
+    docker_cmd: '["/sbin/init"]'
+    docker_image_commands:
+      - 'apt-get install -yq ruby1.8-dev libaugeas-dev libaugeas-ruby ruby rubygems lsb-release wget net-tools curl'
+CONFIG:
+  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/ubuntu-server-1304-x64.yml b/acceptance_spec/acceptance/nodesets/ubuntu-server-1304-x64.yml
new file mode 100644
index 00000000000..5735afc4669
--- /dev/null
+++ b/acceptance_spec/acceptance/nodesets/ubuntu-server-1304-x64.yml
@@ -0,0 +1,14 @@
+HOSTS:
+  ubuntu-13-04:
+    roles:
+      - master
+      - database
+      - dashboard
+    platform: ubuntu-13.04-amd64
+    image: jordansissel/system:ubuntu-13.04
+    hypervisor: docker
+    docker_cmd: '["/sbin/init"]'
+    docker_image_commands:
+      - 'apt-get install -yq ruby1.8-dev libaugeas-dev libaugeas-ruby ruby rubygems lsb-release wget net-tools curl'
+CONFIG:
+  type: foss
diff --git a/acceptance_spec/spec_helper_acceptance.rb b/acceptance_spec/spec_helper_acceptance.rb
new file mode 100644
index 00000000000..a37cbf9e35f
--- /dev/null
+++ b/acceptance_spec/spec_helper_acceptance.rb
@@ -0,0 +1,64 @@
+require 'beaker-rspec'
+require 'pry'
+require 'securerandom'
+
+files_dir = ENV['files_dir'] || '/home/jenkins/puppet'
+
+proxy_host = ENV['proxy_host'] || ''
+
+gem_proxy = ''
+gem_proxy = "http_proxy=http://#{proxy_host}" unless proxy_host.empty?
+
+hosts.each do |host|
+  # Install Puppet
+  if host.is_pe?
+    install_pe
+  else
+    puppetversion = ENV['VM_PUPPET_VERSION'] || '3.4.0'
+    install_package host, 'rubygems'
+    on host, "#{gem_proxy} gem install puppet --no-ri --no-rdoc --version '~> #{puppetversion}'"
+    on host, "mkdir -p #{host['distmoduledir']}"
+
+    if fact('osfamily') == 'Suse'
+      install_package host, 'ruby-devel augeas-devel libxml2-devel'
+      on host, 'gem install ruby-augeas --no-ri --no-rdoc'
+    end
+
+  end
+
+  # Setup proxy if its enabled
+  if fact('osfamily') == 'Debian'
+          on host, "echo 'Acquire::http::Proxy \"http://#{proxy_host}/\";' >> /etc/apt/apt.conf.d/10proxy" unless proxy_host.empty?
+  end
+  if fact('osfamily') == 'RedHat'
+    on host, "echo 'proxy=http://#{proxy_host}/' >> /etc/yum.conf" unless proxy_host.empty?
+  end
+
+end
+
+RSpec.configure do |c|
+  # Project root
+  proj_root = File.expand_path(File.join(File.dirname(__FILE__), '..'))
+
+  # Readable test descriptions
+  c.formatter = :documentation
+
+  # Configure all nodes in nodeset
+  c.before :suite do
+    # Install module and dependencies
+
+    hosts.each do |host|
+
+      on host, puppet('module','install','elasticsearch-logstash'), { :acceptable_exit_codes => [0,1] }
+
+      if fact('osfamily') == 'Debian'
+        scp_to(host, "#{files_dir}/puppetlabs-apt-1.4.2.tar.gz", '/tmp/puppetlabs-apt-1.4.2.tar.gz')
+        on host, puppet('module','install','/tmp/puppetlabs-apt-1.4.2.tar.gz'), { :acceptable_exit_codes => [0,1] }
+      end
+      if fact('osfamily') == 'Suse'
+        on host, puppet('module','install','darin-zypprepo'), { :acceptable_exit_codes => [0,1] }
+      end
+
+    end
+  end
+end
diff --git a/bin/logstash b/bin/logstash
index 4c17dcb3435..21e26d9ddf5 100755
--- a/bin/logstash
+++ b/bin/logstash
@@ -1,10 +1,38 @@
-#!/usr/bin/env jruby
+#!/bin/sh
+# Run logstash from source
+#
+# This is most useful when done from a git checkout.
+#
+# Usage:
+#     bin/logstash <command> [arguments]
+#
+# See 'bin/logstash help' for a list of commands.
+#
+# Defaults you can override with environment variables
+LS_HEAP_SIZE="${LS_HEAP_SIZE:=500m}"
 
-$: << File.dirname($0) + "/../lib"
+unset CDPATH
+basedir=$(cd `dirname $0`/..; pwd)
+. "${basedir}/bin/logstash.lib.sh"
 
-require "rubygems"
-require "logstash/agent"
+setup
 
-agent = LogStash::Agent.new
-agent.argv = ARGV
-agent.run
+# Export these so that they can be picked up by file input (and others?).
+export HOME SINCEDB_DIR
+
+case $1 in
+  -*)
+    if [ -z "$VENDORED_JRUBY" ] ; then
+      exec "${RUBYCMD}" "${basedir}/lib/logstash/runner.rb" "agent" "$@"
+    else
+      exec "$JRUBY_BIN" $(jruby_opts) "${basedir}/lib/logstash/runner.rb" "agent" "$@"
+    fi
+    ;;
+  *)
+    if [ -z "$VENDORED_JRUBY" ] ; then
+      exec "${RUBYCMD}" "${basedir}/lib/logstash/runner.rb" "$@"
+    else
+      exec "$JRUBY_BIN" $(jruby_opts) "${basedir}/lib/logstash/runner.rb" "$@"
+    fi
+    ;;
+esac
diff --git a/bin/logstash-test b/bin/logstash-test
index 40642aa94d0..3af148b6c40 100755
--- a/bin/logstash-test
+++ b/bin/logstash-test
@@ -1,90 +1,4 @@
-#!/usr/bin/env ruby
-
-require "rubygems"
-require "optparse"
-$:.unshift "#{File.dirname(__FILE__)}/../lib"
-$:.unshift "#{File.dirname(__FILE__)}/../test"
-
-#require "logstash/test_syntax"
-#require "logstash/filters/test_date"
-#require "logstash/filters/test_multiline"
-
-def check_lib(lib, provider, optional=true, message=nil)
-  begin
-    require lib
-    puts "+ Found #{optional ? "optional" : "required"} library '#{lib}'"
-    return { :optional => optional, :found => true }
-  rescue LoadError => e
-    puts "- Missing #{optional ? "optional" : "required"} library '#{lib}' - try 'gem install #{provider}'#{optional ? " if you want this library" : ""}. #{message}"
-    return { :optional => optional, :found => false }
-  end
-end
-
-def report_ruby_version
-  puts "Running #{RUBY_VERSION}p#{RUBY_PATCHLEVEL} on #{RUBY_PLATFORM}"
-end
-
-def check_libraries
-  results = []
-  results << check_lib("em-http-request", "em-http-request", true,
-            "needed for ElasticSearch input, output, and logstash-web support.")
-  results << check_lib("em-mongo", "em-mongo", true,
-            "needed for the mongodb output.")
-  results << check_lib("grok", "jls-grok", true,
-            "needed for the grok filter.")
-  results << check_lib("em-websocket", "em-websocket", true,
-            "needed for websocket output")
-  results << check_lib("rack", "rack", true,
-            "needed for logstash-web")
-  results << check_lib("thin", "thin", true,
-            "needed for logstash-web")
-  results << check_lib("amqp", "amqp", true,
-            "needed for AMQP input and output")
-  results << check_lib("sinatra/async", "async_sinatra", true,
-            "needed for logstash-web")
-  results << check_lib("uuidtools", "uuidtools", true,
-            "needed for AMQP input and output")
-  results << check_lib("ap", "awesome_print", true,
-            "improve logstash debug logging output")
-  results << check_lib("eventmachine", "eventmachine", false,
-            "required for logstash to function")
-  results << check_lib("json", "json", false,
-            "required for logstash to function")
-
-  missing_required = results.count { |r| !r[:optional] and !r[:found] }
-  if missing_required == 0
-    puts "All required libraries found :)"
-  else
-    suffix = (missing_required > 1) ? "ies" : "y"
-    puts "FATAL: Missing #{missing_required} required librar#{suffix}"
-    return false
-  end
-
-  return true
-end
-
-def run_tests
-  require "#{File.dirname(__FILE__)}/../test/run"
-  return Test::Unit::AutoRunner.run
-end
-
-def main(args)
-  report_ruby_version
-  # TODO(sissel): Add a way to call out specific things to test, like
-  # logstash-web, elasticsearch, mongodb, syslog, etc.
-  if !check_libraries
-    puts "Library check failed."
-    return 1
-  end
-
-  if !run_tests
-    puts "Test suite failed."
-    return 1
-  end
-
-  return 0
-end
-
-exit(main(ARGV))
-
+#!/bin/sh
 
+basedir=$(cd `dirname $0`/..; pwd)
+exec $basedir/bin/logstash rspec "$@"
diff --git a/bin/logstash-web b/bin/logstash-web
index 3d22d57fe5f..1f765b8ce6b 100755
--- a/bin/logstash-web
+++ b/bin/logstash-web
@@ -1,8 +1,4 @@
-#!/usr/bin/env ruby
+#!/bin/sh
 
-$: << File.dirname($0) + "/../lib"
-$: << File.dirname($0) + "/../test"
-
-require "rubygems"
-require "logstash/loadlibs"
-require "logstash/web/server"
+basedir=$(cd `dirname $0`/..; pwd)
+exec $basedir/bin/logstash web "$@"
diff --git a/bin/logstash.bat b/bin/logstash.bat
new file mode 100644
index 00000000000..f90ef3673ab
--- /dev/null
+++ b/bin/logstash.bat
@@ -0,0 +1,89 @@
+@echo off
+
+SETLOCAL
+
+if not defined JAVA_HOME goto missing_java_home
+
+set SCRIPT_DIR=%~dp0
+for %%I in ("%SCRIPT_DIR%..") do set LS_HOME=%%~dpfI
+
+
+REM ***** JAVA options *****
+
+if "%LS_MIN_MEM%" == "" (
+set LS_MIN_MEM=256m
+)
+
+if "%LS_MAX_MEM%" == "" (
+set LS_MAX_MEM=1g
+)
+
+set JAVA_OPTS=%JAVA_OPTS% -Xms%LS_MIN_MEM% -Xmx%LS_MAX_MEM%
+
+REM Enable aggressive optimizations in the JVM
+REM    - Disabled by default as it might cause the JVM to crash
+REM set JAVA_OPTS=%JAVA_OPTS% -XX:+AggressiveOpts
+
+set JAVA_OPTS=%JAVA_OPTS% -XX:+UseParNewGC
+set JAVA_OPTS=%JAVA_OPTS% -XX:+UseConcMarkSweepGC
+set JAVA_OPTS=%JAVA_OPTS% -XX:+CMSParallelRemarkEnabled
+set JAVA_OPTS=%JAVA_OPTS% -XX:SurvivorRatio=8
+set JAVA_OPTS=%JAVA_OPTS% -XX:MaxTenuringThreshold=1
+set JAVA_OPTS=%JAVA_OPTS% -XX:CMSInitiatingOccupancyFraction=75
+set JAVA_OPTS=%JAVA_OPTS% -XX:+UseCMSInitiatingOccupancyOnly
+
+REM GC logging options -- uncomment to enable
+REM JAVA_OPTS=%JAVA_OPTS% -XX:+PrintGCDetails
+REM JAVA_OPTS=%JAVA_OPTS% -XX:+PrintGCTimeStamps
+REM JAVA_OPTS=%JAVA_OPTS% -XX:+PrintClassHistogram
+REM JAVA_OPTS=%JAVA_OPTS% -XX:+PrintTenuringDistribution
+REM JAVA_OPTS=%JAVA_OPTS% -XX:+PrintGCApplicationStoppedTime
+REM JAVA_OPTS=%JAVA_OPTS% -Xloggc:/var/log/logstash/gc.log
+
+REM Causes the JVM to dump its heap on OutOfMemory.
+set JAVA_OPTS=%JAVA_OPTS% -XX:+HeapDumpOnOutOfMemoryError
+REM The path to the heap dump location, note directory must exists and have enough
+REM space for a full heap dump.
+REM JAVA_OPTS=%JAVA_OPTS% -XX:HeapDumpPath=$LS_HOME/logs/heapdump.hprof
+
+set RUBYLIB=%LS_HOME%\lib
+set GEM_HOME=%LS_HOME%\vendor\bundle\jruby\1.9\
+set GEM_PATH=%GEM_HOME%
+
+for %%I in ("%LS_HOME%\vendor\jar\jruby-complete-*.jar") do set JRUBY_JAR_FILE=%%I
+if not defined JRUBY_JAR_FILE goto missing_jruby_jar
+
+set RUBY_CMD="%JAVA_HOME%\bin\java" %JAVA_OPTS% %LS_JAVA_OPTS% -jar "%JRUBY_JAR_FILE%"
+
+if "%*"=="deps" goto install_deps
+goto run_logstash
+
+:install_deps
+if not exist "%LS_HOME%\logstash.gemspec" goto missing_gemspec
+echo Installing gem dependencies. This will probably take a while the first time.
+%RUBY_CMD% "%LS_HOME%\gembag.rb"
+goto finally
+
+:run_logstash
+%RUBY_CMD% "%LS_HOME%\lib\logstash\runner.rb" %*
+goto finally
+
+:missing_java_home
+echo JAVA_HOME environment variable must be set!
+pause
+goto finally
+
+:missing_jruby_jar
+md "%LS_HOME%\vendor\jar\"
+echo Please download the JRuby Complete .jar from http://jruby.org/download to %LS_HOME%\vendor\jar\ and re-run this command.
+pause
+goto finally
+
+:missing_gemspec
+echo Cannot install dependencies; missing logstash.gemspec. This 'deps' command only works from a logstash git clone.
+pause
+goto finally
+
+:finally
+
+ENDLOCAL
diff --git a/bin/logstash.lib.sh b/bin/logstash.lib.sh
new file mode 100755
index 00000000000..9fba6dddf49
--- /dev/null
+++ b/bin/logstash.lib.sh
@@ -0,0 +1,116 @@
+basedir=$(cd `dirname $0`/..; pwd)
+
+setup_java() {
+  if [ -z "$JAVACMD" ] ; then
+    if [ -n "$JAVA_HOME" ] ; then
+      JAVACMD="$JAVA_HOME/bin/java"
+    else
+      JAVACMD="java"
+    fi
+  fi
+
+  # Resolve full path to the java command.
+  if [ ! -f "$JAVACMD" ] ; then
+    JAVACMD=$(which $JAVACMD 2>/dev/null)
+  fi
+
+  if [ ! -x "$JAVACMD" ] ; then
+    echo "Could not find any executable java binary. Please install java in your PATH or set JAVA_HOME."
+    exit 1
+  fi
+
+  JAVA_OPTS="$JAVA_OPTS -Xmx${LS_HEAP_SIZE}"
+  JAVA_OPTS="$JAVA_OPTS -XX:+UseParNewGC"
+  JAVA_OPTS="$JAVA_OPTS -XX:+UseConcMarkSweepGC"
+  JAVA_OPTS="$JAVA_OPTS -Djava.awt.headless=true"
+
+  JAVA_OPTS="$JAVA_OPTS -XX:CMSInitiatingOccupancyFraction=75"
+  JAVA_OPTS="$JAVA_OPTS -XX:+UseCMSInitiatingOccupancyOnly"
+
+  if [ ! -z "$LS_USE_GC_LOGGING" ] ; then
+    JAVA_OPTS="$JAVA_OPTS -XX:+PrintGCDetails"
+    JAVA_OPTS="$JAVA_OPTS -XX:+PrintGCTimeStamps"
+    JAVA_OPTS="$JAVA_OPTS -XX:+PrintClassHistogram"
+    JAVA_OPTS="$JAVA_OPTS -XX:+PrintTenuringDistribution"
+    JAVA_OPTS="$JAVA_OPTS -XX:+PrintGCApplicationStoppedTime"
+    JAVA_OPTS="$JAVA_OPTS -Xloggc:./logstash-gc.log"
+    echo "Writing garbage collection logs to ./logstash-gc.log"
+  fi
+
+  export JAVACMD
+  export JAVA_OPTS
+}
+
+setup_drip() {
+  if [ -z $DRIP_JAVACMD ] ; then
+    JAVACMD="drip"
+  fi
+
+  # resolve full path to the drip command.
+  if [ ! -f "$JAVACMD" ] ; then
+    JAVACMD=$(which $JAVACMD 2>/dev/null)
+  fi
+
+  if [ ! -x "$JAVACMD" ] ; then
+    echo "Could not find executable drip binary. Please install drip in your PATH"
+    exit 1
+  fi
+
+  # faster JRuby startup options https://github.com/jruby/jruby/wiki/Improving-startup-time
+  # since we are using drip to speed up, we may as well throw these in also
+  if [ "$USE_RUBY" = "1" ] ; then
+    export JRUBY_OPTS="-J-XX:+TieredCompilation -J-XX:TieredStopAtLevel=1 -J-noverify"
+  else
+    JAVA_OPTS="$JAVA_OPTS -XX:+TieredCompilation -XX:TieredStopAtLevel=1 -noverify"
+  fi
+  export JAVACMD
+  export DRIP_INIT_CLASS="org.jruby.main.DripMain"
+  export DRIP_INIT=""
+}
+
+setup_vendored_jruby() {
+  #JRUBY_JAR=$(ls "${basedir}"/vendor/jruby/jruby-complete-*.jar)
+  JRUBY_BIN="${basedir}/vendor/jruby/bin/jruby"
+
+  if [ ! -f "${JRUBY_BIN}" ] ; then
+    echo "Unable to find JRuby."
+    echo "If you are a user, this is a bug."
+    echo "If you are a developer, please run 'rake bootstrap'. Running 'rake' requires the 'ruby' program be available."
+    exit 1
+  fi
+  VENDORED_JRUBY=1
+}
+
+setup_ruby() {
+  RUBYCMD="ruby"
+  VENDORED_JRUBY=
+}
+
+jruby_opts() {
+  for i in $JAVA_OPTS ; do
+    echo "-J$i"
+  done
+}
+
+setup() {
+  # first check if we want to use drip, which can be used in vendored jruby mode
+  # and also when setting USE_RUBY=1 if the ruby interpretor is in fact jruby
+  if [ ! -z "$JAVACMD" ] ; then
+    if [ "$(basename $JAVACMD)" = "drip" ] ; then
+      DRIP_JAVACMD=1
+      USE_DRIP=1
+    fi
+  fi
+  if [ "$USE_DRIP" = "1" ] ; then
+    setup_drip
+  fi
+
+  if [ "$USE_RUBY" = "1" ] ; then
+    setup_ruby
+  else
+    setup_java
+    setup_vendored_jruby
+  fi
+
+  export RUBYLIB="${basedir}/lib"
+}
diff --git a/bin/plugin b/bin/plugin
new file mode 100755
index 00000000000..4e2601fb911
--- /dev/null
+++ b/bin/plugin
@@ -0,0 +1,61 @@
+#!/bin/sh
+# Install contrib plugins.
+#
+# Usage:
+#     bin/plugin install contrib
+#
+# Figure out if we're using wget or curl
+
+basedir=$(cd `dirname $0`/..; pwd)
+. ${basedir}/bin/logstash.lib.sh
+
+WGET=$(which wget 2>/dev/null)
+CURL=$(which curl 2>/dev/null)
+
+URLSTUB="http://download.elasticsearch.org/logstash/logstash/"
+
+if [ "x$WGET" != "x" ]; then
+	DOWNLOAD_COMMAND="wget -q --no-check-certificate -O"
+elif [ "x$CURL" != "x" ]; then
+    DOWNLOAD_COMMAND="curl -s -L -k -o"
+else
+	echo "wget or curl are required."
+	exit 1
+fi
+
+
+if [ -f "$basedir/lib/logstash/version.rb" ] ; then
+	VERSION=$(cat "$basedir/lib/logstash/version.rb" | grep LOGSTASH_VERSION | awk -F\" '{print $2}') 
+else
+	echo "ERROR: Cannot determine Logstash version.  Exiting."
+	exit 1
+fi
+
+# Placeholder for now, if other installs ever become available.
+if [ "x$2" != "xcontrib" ]; then
+	echo "Can only install contrib at this time... Exiting."
+	exit 1
+fi
+
+TARGETDIR="$basedir/vendor/logstash"
+mkdir -p $TARGETDIR
+SUFFIX=".tar.gz"
+FILEPATH="logstash-contrib-${VERSION}"
+FILENAME=${FILEPATH}${SUFFIX}
+TARGET="${TARGETDIR}/${FILENAME}"
+
+case $1 in
+  install)
+  	$DOWNLOAD_COMMAND ${TARGET} ${URLSTUB}${FILENAME}
+  	if [ ! -f "${TARGET}" ]; then
+	  	echo "ERROR: Unable to download ${URLSTUB}${FILENAME}"
+	  	echo "Exiting."
+	  	exit 1
+	fi
+  	gzip -dc ${TARGET} | tar -xC $TARGETDIR
+  	cp -R ${TARGETDIR}/$FILEPATH/* $basedir  ;; # Copy contents to local directory, adding on top of existing install
+  *) 
+  	echo "Usage: bin/plugin install contrib"
+	exit 0
+	;;
+esac
diff --git a/bot/check_pull_changelog.rb b/bot/check_pull_changelog.rb
new file mode 100644
index 00000000000..7e8ac7e1f21
--- /dev/null
+++ b/bot/check_pull_changelog.rb
@@ -0,0 +1,89 @@
+require "octokit"
+##
+# This script will validate that any pull request submitted against a github 
+# repository will contains changes to CHANGELOG file.
+#
+# If not the case, an helpful text will be commented on the pull request
+# If ok, a thanksful message will be commented also containing a @mention to 
+# acts as a trigger for review notification by a human.
+## 
+
+
+@bot="" # Put here your bot github username
+@password="" # Put here your bot github password
+
+@repository="logstash/logstash"
+@mention="@jordansissel"
+
+@missing_changelog_message = <<MISSING_CHANGELOG
+Hello, I'm #{@bot}, I'm here to help you accomplish your pull request submission quest
+
+You still need to accomplish these tasks:
+
+* Please add a changelog information
+
+Also note that your pull request name will appears in the details section 
+of the release notes, so please make it clear
+MISSING_CHANGELOG
+
+@ok_changelog_message = <<OK_CHANGELOG
+You successfully completed the pre-requisite quest (aka updating CHANGELOG)
+
+Also note that your pull request name will appears in the details section 
+of the release notes, so please make it clear, if not already done.
+
+#{@mention} Dear master, would you please have a look to this humble request
+OK_CHANGELOG
+
+#Connect to Github
+@client=Octokit::Client.new(:login => @bot, :password => @password)
+
+
+#For each open pull
+Octokit.pull_requests(@repository).each do |pull|
+  #Get botComment
+  botComment = nil
+  @client.issue_comments(@repository, pull.number, {
+    :sort => "created",
+    :direction => "desc"
+  }).each do |comment|
+    if comment.user.login == @bot
+      botComment = comment
+      break
+    end
+  end
+
+  if !botComment.nil? and botComment.body.start_with?("[BOT-OK]")
+    #Pull already validated by bot, nothing to do
+    puts "Pull request #{pull.number}, already ok for bot"
+  else
+    #Firt encounter, or previous [BOT-WARN] status
+    #Check for changelog
+    warnOnMissingChangeLog = true
+    @client.pull_request_files(@repository, pull.number).each do |changedFile|
+      if changedFile.filename  == "CHANGELOG"
+        if changedFile.additions.to_i > 0
+          #Changelog looks good
+          warnOnMissingChangeLog = false
+        else
+          #No additions, means crazy deletion
+          warnOnMissingChangeLog = true
+        end
+      end
+    end
+    if warnOnMissingChangeLog
+      if botComment.nil?
+        puts "Pull request #{pull.number}, adding bot warning"
+        @client.add_comment(@repository, pull.number, "[BOT-WARN] #{@missing_changelog_message}")
+      else
+        puts "Pull request #{pull.number}, already warned, no changes yet"
+      end
+    else
+      if !botComment.nil?
+        @client.delete_comment(@repository,botComment.id)
+      end
+      puts "Pull request #{pull.number}, adding bot ok"
+      @client.add_comment(@repository, pull.number, "[BOT-OK] #{@ok_changelog_message}")
+    end
+  end
+end
diff --git a/docs/asciidoc/static/command-line-flags.asciidoc b/docs/asciidoc/static/command-line-flags.asciidoc
new file mode 100644
index 00000000000..30935c4792f
--- /dev/null
+++ b/docs/asciidoc/static/command-line-flags.asciidoc
@@ -0,0 +1,49 @@
+= Command-line flags
+
+== Agent
+
+The Logstash agent has the following flags (also try using the '--help' flag)
+
+[source,js]
+----------------------------------
+-f, --config CONFIGFILE
+ Load the Logstash config from a specific file, directory, or a wildcard. If given a directory or wildcard, config files will be read from the directory in alphabetical order.
+
+-e CONFIGSTRING
+ Use the given string as the configuration data. Same syntax as the config file. If not input is specified, 'stdin { type => stdin }' is default. If no output is specified, 'stdout { codec => rubydebug }}' is default.
+
+-w, --filterworkers COUNT
+ Run COUNT filter workers (default: 1)
+
+--watchdog-timeout TIMEOUT
+ Set watchdog timeout value in seconds. Default is 10.
+
+-l, --log FILE 
+ Log to a given path. Default is to log to stdout 
+
+--verbose 
+ Increase verbosity to the first level, less verbose.
+
+--debug 
+ Increase verbosity to the last level, more verbose.
+
+-v  
+ *DEPRECATED: see --verbose/debug* Increase verbosity. There are multiple levels of verbosity available with
+'-vv' currently being the highest 
+
+--pluginpath PLUGIN_PATH 
+ A colon-delimited path to find other Logstash plugins in 
+----------------------------------
+
+
+== Web
+
+[source,js]
+----------------------------------
+-a, --address ADDRESS 
+ Address on which to start webserver. Default is 0.0.0.0.
+
+-p, --port PORT
+ Port on which to start webserver. Default is 9292.
+----------------------------------
+
diff --git a/docs/asciidoc/static/configuration.asciidoc b/docs/asciidoc/static/configuration.asciidoc
new file mode 100644
index 00000000000..8a1cc2d6a25
--- /dev/null
+++ b/docs/asciidoc/static/configuration.asciidoc
@@ -0,0 +1,337 @@
+= Logstash Config Language
+
+== Basic Layout
+
+The Logstash config language aims to be simple.
+
+There are 3 main sections: inputs, filters, outputs. Each section has configurations for each plugin available in that section.
+
+Example:
+
+[source,js]
+----------------------------------
+# This is a comment. You should use comments to describe
+# parts of your configuration.
+input {
+  ...
+}
+
+filter {
+  ...
+}
+
+output {
+  ...
+}
+----------------------------------
+
+== Filters and Ordering
+
+For a given event, filters are applied in the order of appearance in the configuration file.
+
+== Comments
+
+Comments are the same as in perl, ruby, and python. A comment starts with a '#' character, and does not need to be at the beginning of a line. For example:
+
+[source,js]
+----------------------------------
+# this is a comment
+
+input { # comments can appear at the end of a line, too
+  # ...
+}
+----------------------------------
+
+== Plugins
+
+The input, filter and output sections all let you configure plugins. Plugin
+configuration consists of the plugin name followed by a block of settings for
+that plugin. For example, how about two file inputs:
+
+[source,js]
+----------------------------------
+input {
+  file {
+    path => "/var/log/messages"
+    type => "syslog"
+  }
+
+  file {
+    path => "/var/log/apache/access.log"
+    type => "apache"
+  }
+}
+----------------------------------
+
+The above configures two file separate inputs. Both set two configuration settings each: 'path' and 'type'. Each plugin has different settings for configuring it; seek the documentation for your plugin to learn what settings are available and what they mean. For example, the [file input][fileinput] documentation will explain the meanings of the path and type settings.
+
+[fileinput]: inputs/file
+
+== Value Types
+
+The documentation for a plugin may enforce a configuration field having a
+certain type.  Examples include boolean, string, array, number, hash,
+etc.
+
+=== Boolean
+
+A boolean must be either `true` or `false`. Note the lack of quotes around `true` and `false`.
+
+Examples:
+
+[source,js]
+----------------------------------
+  ssl_enable => true
+----------------------------------
+
+=== String
+
+A string must be a single value.
+
+Example:
+
+[source,js]
+----------------------------------
+  name => "Hello world"
+----------------------------------
+
+You should use quotes around string values.
+
+=== Number
+
+Numbers must be valid numerics (floating point or integer are OK).
+
+Example:
+
+[source,js]
+----------------------------------
+  port => 33
+----------------------------------
+
+=== Array
+
+An array can be a single string value or multiple. If you specify the same
+field multiple times, it appends to the array.
+
+Examples:
+
+[source,js]
+----------------------------------
+  path => [ "/var/log/messages", "/var/log/*.log" ]
+  path => "/data/mysql/mysql.log"
+----------------------------------
+
+The above makes 'path' a 3-element array including all 3 strings.
+
+=== Hash
+
+A hash is basically the same syntax as Ruby hashes. 
+The key and value are simply pairs, such as:
+
+[source,js]
+----------------------------------
+match => {
+  "field1" => "value1"
+  "field2" => "value2"
+  ...
+}
+----------------------------------
+
+== Field References
+
+All events have properties. For example, an apache access log would have things
+like status code (200, 404), request path ("/", "index.html"), HTTP verb (GET, POST),
+client IP address, etc. Logstash calls these properties "fields." 
+
+In many cases, it is useful to be able to refer to a field by name. To do this,
+you can use the Logstash field reference syntax.
+
+By way of example, let us suppose we have this event:
+
+[source,js]
+----------------------------------
+{
+  "agent": "Mozilla/5.0 (compatible; MSIE 9.0)",
+  "ip": "192.168.24.44",
+  "request": "/index.html"
+  "response": {
+    "status": 200,
+    "bytes": 52353
+  },
+  "ua": {
+    "os": "Windows 7"
+  }
+}
+
+----------------------------------
+
+- the syntax to access fields is `[fieldname]`.
+- if you are only referring to a **top-level field**, you can omit the `[]` and
+simply say `fieldname`.
+- in the case of **nested fields**, like the "os" field above, you need
+the full path to that field: `[ua][os]`.
+
+=== sprintf format
+
+This syntax is also used in what Logstash calls 'sprintf format'. This format
+allows you to refer to field values from within other strings. For example, the
+statsd output has an 'increment' setting, to allow you to keep a count of
+apache logs by status code:
+
+[source,js]
+----------------------------------
+output {
+  statsd {
+    increment => "apache.%{[response][status]}"
+  }
+}
+----------------------------------
+
+You can also do time formatting in this sprintf format. Instead of specifying a field name, use the `+FORMAT` syntax where `FORMAT` is a [time format](http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html). 
+
+For example, if you want to use the file output to write to logs based on the
+hour and the 'type' field:
+
+[source,js]
+----------------------------------
+output {
+  file {
+    path => "/var/log/%{type}.%{+yyyy.MM.dd.HH}"
+  }
+}
+----------------------------------
+
+== Conditionals
+
+Sometimes you only want a filter or output to process an event under
+certain conditions. For that, you'll want to use a conditional!
+
+Conditionals in Logstash look and act the same way they do in programming
+languages. You have `if`, `else if` and `else` statements. Conditionals may be
+nested if you need that.
+
+The syntax is follows:
+
+[source,js]
+----------------------------------
+if EXPRESSION {
+  ...
+} else if EXPRESSION {
+  ...
+} else {
+  ...
+}
+----------------------------------
+
+What's an expression? Comparison tests, boolean logic, etc!
+
+The following comparison operators  are supported:
+
+* equality, etc: ==,  !=,  <,  >,  <=,  >= 
+* regexp: =~, !~ 
+* inclusion: in, not in
+
+The following boolean operators are supported:
+
+* and, or, nand, xor
+
+The following unary operators are supported:
+
+* !
+
+Expressions may contain expressions. Expressions may be negated with `!`.
+Expressions may be grouped with parentheses `(...)`. Expressions can be long
+and complex.
+
+For example, if we want to remove the field `secret` if the field
+`action` has a value of `login`:
+
+[source,js]
+----------------------------------
+filter {
+  if [action] == "login" {
+    mutate { remove => "secret" }
+  }
+}
+----------------------------------
+
+The above uses the field reference syntax to get the value of the
+`action` field. It is compared against the text `login` and, if equal,
+allows the mutate filter to delete the field named `secret`.
+
+How about a more complex example?
+
+* alert nagios of any apache events with status 5xx
+* record any 4xx status to elasticsearch
+* record all status code hits via statsd
+
+How about telling nagios of any http event that has a status code of 5xx?
+
+[source,js]
+----------------------------------
+output {
+  if [type] == "apache" {
+    if [status] =~ /^5\d\d/ {
+      nagios { ...  }
+    } else if [status] =~ /^4\d\d/ {
+      elasticsearch { ... }
+    }
+    statsd { increment => "apache.%{status}" }
+  }
+}
+----------------------------------
+
+You can also do multiple expressions in a single condition:
+
+[source,js]
+----------------------------------
+output {
+  # Send production errors to pagerduty
+  if [loglevel] == "ERROR" and [deployment] == "production" {
+    pagerduty {
+    ...
+    }
+  }
+}
+----------------------------------
+
+Here are some examples for testing with the in conditional:
+
+[source,js]
+----------------------------------
+filter {
+  if [foo] in [foobar] {
+    mutate { add_tag => "field in field" }
+  }
+  if [foo] in "foo" {
+    mutate { add_tag => "field in string" }
+  }
+  if "hello" in [greeting] {
+    mutate { add_tag => "string in field" }
+  }
+  if [foo] in ["hello", "world", "foo"] {
+    mutate { add_tag => "field in list" }
+  }
+  if [missing] in [alsomissing] {
+    mutate { add_tag => "shouldnotexist" }
+  }
+  if !("foo" in ["hello", "world"]) {
+    mutate { add_tag => "shouldexist" }
+  }
+}
+----------------------------------
+
+Or, to test if grok was successful:
+
+[source,js]
+----------------------------------
+output {
+  if "_grokparsefailure" not in [tags] {
+    elasticsearch { ... }
+  }
+}
+----------------------------------
+
+== Further Reading
+
+For more information, see [the plugin docs index](index)
diff --git a/docs/asciidoc/static/contrib-plugins.asciidoc b/docs/asciidoc/static/contrib-plugins.asciidoc
new file mode 100644
index 00000000000..1458062f3b6
--- /dev/null
+++ b/docs/asciidoc/static/contrib-plugins.asciidoc
@@ -0,0 +1,56 @@
+= contrib plugins
+
+== Why contrib?
+As Logstash has grown, we've accumulated a massive repository of plugins. Well over 100 plugins, it became difficult for the project maintainers to adequately support everything effectively.
+
+In order to improve the quality of popular plugins, we've moved the less-commonly-used plugins to a separate repository we're calling "contrib". Concentrating common plugin usage into core solves a few problems, most notably user complaints about the size of Logstash releases, support/maintenance costs, etc.
+
+It is our intent that this separation will improve life for users. If it doesn't, please file a bug so we can work to address it!
+
+If a plugin is available in the 'contrib' package, the documentation for that plugin will note this boldly at the top of that plugin's documentation.
+
+Contrib plugins reside in a [separate github project](https://github.com/elasticsearch/logstash-contrib).
+
+== Packaging
+
+At present, the contrib modules are available as a tarball.
+
+== Automated Installation
+
+The `bin/plugin` script will handle the installation for you:
+
+[source,js]
+----------------------------------
+cd /path/to/logstash
+bin/plugin install contrib
+----------------------------------
+== Manual Installation
+
+The contrib plugins can be extracted on top of an existing Logstash installation. 
+
+For example, if I've extracted `logstash-%VERSION%.tar.gz` into `/path`, e.g.
+
+[source,js]
+----------------------------------
+cd /path
+tar zxf ~/logstash-%VERSION%.tar.gz
+----------------------------------
+
+It will have a `/path/logstash-%VERSION%` directory, e.g.
+
+[source,js]
+----------------------------------
+$ ls
+logstash-%VERSION%
+----------------------------------
+
+The method to install the contrib tarball is identical.
+[source,js]
+----------------------------------
+cd /path
+wget http://download.elasticsearch.org/logstash/logstash/logstash-contrib-%VERSION%.tar.gz
+tar zxf ~/logstash-contrib-%VERSION%.tar.gz
+----------------------------------
+This will install the contrib plugins in the same directory as the core
+install. These plugins will be available to Logstash the next time it starts.
+
diff --git a/docs/asciidoc/static/contributing-to-logstash.asciidoc b/docs/asciidoc/static/contributing-to-logstash.asciidoc
new file mode 100644
index 00000000000..bbb928fcdf8
--- /dev/null
+++ b/docs/asciidoc/static/contributing-to-logstash.asciidoc
@@ -0,0 +1,125 @@
+[[contributing-to-logstash]]
+
+== Extending Logstash
+
+You can add your own input, output, or filter plugins to Logstash.
+
+If you're looking to extend Logstash today, the best way is to look at how some existing plugins are written.
+
+[float]
+=== Good examples of plugins
+
+* https://github.com/logstash/logstash/blob/master/lib/logstash/inputs/tcp.rb[inputs/tcp]
+* https://github.com/logstash/logstash/blob/master/lib/logstash/filters/multiline.rb[filters/multiline]
+* https://github.com/elasticsearch/logstash-contrib/blob/master/lib/logstash/outputs/mongodb.rb[outputs/mongodb]
+
+[float]
+=== Common concepts
+
+* The `config_name` sets the name used in the config file.
+* The `milestone` sets the milestone number of the plugin. See link:plugin-milestones[Plugin Milestones] for more info.
+* The `config` lines define this plugin's configuration options.
+* The `register` method is called per plugin instantiation. Do any of your initialization here.
+
+[float]
+==== Required modules
+
+All plugins should require the Logstash module.
+
+[source,js]
+----------------------------------
+require 'logstash/namespace'
+----------------------------------
+
+[float]
+==== Plugin name
+
+Every plugin must have a name set with the `config_name` method. If this
+is not specified plugins will fail to load with an error.
+
+[float]
+==== Milestones
+
+Every plugin needs a milestone set using `milestone`. See
+<../plugin-milestones> for more info.
+
+[float]
+==== Config lines
+
+The `config` lines define configuration options and are constructed like
+so:
+
+[source,js]
+----------------------------------
+config :host, :validate => :string, :default => "0.0.0.0"
+----------------------------------
+
+The name of the option is specified, here `:host` and then the
+attributes of the option. They can include `:validate`, `:default`,
+`:required` (a Boolean `true` or `false`), and `:deprecated` (also a
+Boolean).  
+ 
+[float]
+=== Inputs
+
+All inputs require and extend the LogStash::Inputs::Base class, like so:
+
+[source,js]
+----------------------------------
+require 'logstash/inputs/base'
+...
+
+class LogStash::Inputs::YourPlugin < LogStash::Inputs::Base
+...
+----------------------------------
+ 
+Inputs have two methods: `register` and `run`.
+
+* Each input runs as its own thread.
+* The `run` method is expected to run-forever.
+
+[float]
+=== Filters
+
+All filters require and extend the LogStash::Filters::Base class, like so:
+
+[source,js]
+----------------------------------
+require 'logstash/filters/base'
+...
+
+class LogStash::Filters::YourPlugin < LogStash::Filters::Base
+...
+----------------------------------
+ 
+Filters have two methods: `register` and `filter`.
+
+* The `filter` method gets an event. 
+* Call `event.cancel` to drop the event.
+* To modify an event, simply make changes to the event you are given.
+* The return value is ignored.
+
+[float]
+=== Outputs
+
+All outputs require and extend the LogStash::Outputs::Base class, like so:
+
+[source,js]
+----------------------------------
+require 'logstash/outputs/base'
+...
+
+class LogStash::Outputs::YourPlugin < LogStash::Outputs::Base
+...
+----------------------------------
+
+Outputs have two methods: `register` and `receive`.
+
+* The `receive` method is called when an event gets pushed to your output
+
+[float]
+=== Example: a new filter
+
+Learn by example how to [add a new filter to Logstash](example-add-a-new-filter)
+
+
diff --git a/docs/asciidoc/static/example-add-a-new-filter.asciidoc b/docs/asciidoc/static/example-add-a-new-filter.asciidoc
new file mode 100644
index 00000000000..d87af4aafe3
--- /dev/null
+++ b/docs/asciidoc/static/example-add-a-new-filter.asciidoc
@@ -0,0 +1,119 @@
+= Add a new filter
+
+== Adding a sample filter to Logstash
+
+This document shows you how to add a new filter to Logstash.
+
+For a general overview of how to add a new plugin, see [the extending Logstash](.) overview.
+
+== Write code.
+
+Let's write a 'hello world' filter. This filter will replace the 'message' in the event with "Hello world!"
+
+First, Logstash expects plugins in a certain directory structure: `logstash/TYPE/PLUGIN_NAME.rb`
+
+Since we're creating a filter, let's mkdir this:
+
+[source,js]
+----------------------------------
+mkdir -p logstash/filters/
+cd logstash/filters
+----------------------------------
+
+Now add the code:
+
+[source,js]
+----------------------------------
+# Call this file 'foo.rb' (in logstash/filters, as above)
+
+require "logstash/filters/base"
+require "logstash/namespace"
+
+class LogStash::Filters::Foo < LogStash::Filters::Base
+
+  # Setting the config_name here is required. This is how you
+  # configure this filter from your Logstash config.
+  #
+  # filter {
+  #   foo { ... }
+  # }
+  config_name "foo"
+
+  # New plugins should start life at milestone 1.
+  milestone 1
+
+  # Replace the message with this value.
+  config :message, :validate => :string
+
+  public
+    def register
+    # nothing to do
+  end # def register
+
+  public
+    def filter(event)
+
+      if @message
+        # Replace the event message with our message as configured in the
+        # config file.
+        event["message"] = @message
+      end
+
+      # filter_matched should go in the last line of our successful code 
+      filter_matched(event)
+    end # def filter
+  end # class LogStash::Filters::Foo
+
+  ## Add it to your configuration
+----------------------------------
+
+For this simple example, let's just use stdin input and stdout output.
+The config file looks like this:
+
+[source,js]
+----------------------------------
+input { 
+  stdin { type => "foo" } 
+}
+filter {
+  if [type] == "foo" {
+    foo {
+      message => "Hello world!"
+    }
+  }
+}
+output {
+  stdout { }
+}
+----------------------------------
+
+Call this file 'example.conf'
+
+== Tell Logstash about it.
+
+Depending on how you installed Logstash, you have a few ways of including this
+plugin.
+
+You can use the agent flag --pluginpath flag to specify where the root of your
+plugin tree is. In our case, it's the current directory.
+
+[source,js]
+----------------------------------
+% bin/logstash --pluginpath your/plugin/root -f example.conf
+----------------------------------
+
+## Example running
+
+In the example below, I typed in "the quick brown fox" after running the java
+command.
+
+[source,js]
+----------------------------------
+% bin/logstash --pluginpath your/plugin/root -f example.conf
+the quick brown fox   
+2011-05-12T01:05:09.495000Z mylocalhost: Hello world!
+----------------------------------
+
+The output is the standard Logstash stdout output, but in this case our "the quick brown fox" message was replaced with "Hello world!"
+
+All done! :)
diff --git a/docs/asciidoc/static/extending-logstash.asciidoc b/docs/asciidoc/static/extending-logstash.asciidoc
new file mode 100644
index 00000000000..92038b71397
--- /dev/null
+++ b/docs/asciidoc/static/extending-logstash.asciidoc
@@ -0,0 +1,104 @@
+= Extending Logstash
+
+== Extending Logstash
+
+You can add your own input, output, or filter plugins to Logstash.
+
+If you're looking to extend Logstash today, please look at the existing plugins.
+
+== Good examples of plugins
+
+* https://github.com/logstash/logstash/blob/master/lib/logstash/inputs/tcp.rb[inputs/tcp]
+* https://github.com/logstash/logstash/blob/master/lib/logstash/filters/multiline.rb[filters/multiline]
+* https://github.com/logstash/logstash/blob/master/lib/logstash/outputs/mongodb.rb[outputs/mongodb]
+
+== Common concepts
+
+* The `config_name` sets the name used in the config file.
+* The `milestone` sets the milestone number of the plugin. See <../plugin-milestones> for more info.
+* The `config` lines define config options.
+* The `register` method is called per plugin instantiation. Do any of your initialization here.
+
+=== Required modules
+
+All plugins should require the Logstash module.
+
+[source,js]
+----------------------------------
+require 'logstash/namespace'
+----------------------------------
+
+=== Plugin name
+
+Every plugin must have a name set with the `config_name` method. If this
+is not specified plugins will fail to load with an error.
+
+=== Milestones
+
+Every plugin needs a milestone set using `milestone`. See
+<../plugin-milestones> for more info.
+
+=== Config lines
+
+The `config` lines define configuration options and are constructed like
+so:
+
+[source,js]
+----------------------------------
+config :host, :validate => :string, :default => "0.0.0.0"
+----------------------------------
+
+The name of the option is specified, here `:host` and then the
+attributes of the option. They can include `:validate`, `:default`,
+`:required` (a Boolean `true` or `false`), and `:deprecated` (also a
+Boolean).  
+ 
+== Inputs
+
+All inputs require the LogStash::Inputs::Base class:
+
+[source,js]
+----------------------------------
+require 'logstash/inputs/base'
+---------------------------------- 
+
+Inputs have two methods: `register` and `run`.
+
+* Each input runs as its own thread.
+* The `run` method is expected to run-forever.
+
+== Filters
+
+All filters require the LogStash::Filters::Base class:
+
+[source,js]
+----------------------------------
+require 'logstash/filters/base'
+----------------------------------
+
+Filters have two methods: `register` and `filter`.
+
+* The `filter` method gets an event. 
+* Call `event.cancel` to drop the event.
+* To modify an event, simply make changes to the event you are given.
+* The return value is ignored.
+
+== Outputs
+
+All outputs require the LogStash::Outputs::Base class:
+
+[source,js]
+----------------------------------
+require 'logstash/outputs/base'
+----------------------------------
+
+Outputs have two methods: `register` and `receive`.
+
+* The `register` method is called per plugin instantiation. Do any of your initialization here.
+* The `receive` method is called when an event gets pushed to your output
+
+== Example: a new filter
+
+Learn by example how to http://foo.com/example-add-a-new-filter[add a new filter to Logstash]
+
+
diff --git a/docs/asciidoc/static/getting-started-with-logstash.asciidoc b/docs/asciidoc/static/getting-started-with-logstash.asciidoc
new file mode 100644
index 00000000000..62ff7a511a6
--- /dev/null
+++ b/docs/asciidoc/static/getting-started-with-logstash.asciidoc
@@ -0,0 +1,472 @@
+= Getting Started with Logstash
+
+== Introduction
+Logstash is a tool for receiving, processing and outputting logs. All kinds of logs. System logs, webserver logs, error logs, application logs and just about anything you can throw at it. Sounds great, eh?
+
+Using Elasticsearch as a backend datastore, and kibana as a frontend reporting tool, Logstash acts as the workhorse, creating a powerful pipeline for storing, querying and analyzing your logs. With an arsenal of built-in inputs, filters, codecs and outputs, you can harness some powerful functionality with a small amount of effort. So, let's get started!
+
+=== Prerequisite: Java
+The only prerequisite required by Logstash is a Java runtime. You can check that you have it installed by running the  command `java -version` in your shell. Here's something similar to what you might see:
+[source,js]
+----------------------------------
+> java -version
+java version "1.7.0_45"
+Java(TM) SE Runtime Environment (build 1.7.0_45-b18)
+Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode)
+----------------------------------
+It is recommended to run a recent version of Java in order to ensure the greatest success in running Logstash.
+
+It's fine to run an open-source version such as OpenJDK: +
+http://openjdk.java.net/
+
+Or you can use the official Oracle version: +
+http://www.oracle.com/technetwork/java/index.html
+
+Once you have verified the existence of Java on your system, we can move on!
+
+== Up and Running!
+
+=== Logstash in two commands
+First, we're going to download the 'logstash' binary and run it with a very simple configuration.
+[source,js]
+----------------------------------
+curl -O https://download.elasticsearch.org/logstash/logstash/logstash-%VERSION%.tar.gz
+----------------------------------
+Now you should have the file named 'logstash-%VERSION%.tar.gz' on your local filesystem. Let's unpack it:
+[source,js]
+----------------------------------
+tar zxvf logstash-%VERSION%.tar.gz
+cd logstash-%VERSION%
+----------------------------------
+Now let's run it:
+[source,js]
+----------------------------------
+bin/logstash -e 'input { stdin { } } output { stdout {} }'
+----------------------------------
+
+Now type something into your command prompt, and you will see it output by Logstash:
+[source,js]
+----------------------------------
+hello world
+2013-11-21T01:22:14.405+0000 0.0.0.0 hello world
+----------------------------------
+
+OK, that's interesting... We ran Logstash with an input called "stdin", and an output named "stdout", and Logstash basically echoed back whatever we typed in some sort of structured format. Note that specifying the *-e* command line flag allows Logstash to accept a configuration directly from the command line. This is especially useful for quickly testing configurations without having to edit a file between iterations.
+
+Let's try a slightly fancier example. First, you should exit Logstash by issuing a 'CTRL-C' command in the shell in which it is running. Now run Logstash again with the following command:
+[source,js]
+----------------------------------
+bin/logstash -e 'input { stdin { } } output { stdout { codec => rubydebug } }'
+----------------------------------
+
+And then try another test input, typing the text "goodnight moon":
+[source,js]
+----------------------------------
+goodnight moon
+{
+  "message" => "goodnight moon",
+  "@timestamp" => "2013-11-20T23:48:05.335Z",
+  "@version" => "1",
+  "host" => "my-laptop"
+}
+----------------------------------
+
+So, by re-configuring the "stdout" output (adding a "codec"), we can change the output of Logstash. By adding inputs, outputs and filters to your configuration, it's possible to massage the log data in many ways, in order to maximize flexibility of the stored data when you are querying it.
+
+== Storing logs with Elasticsearch
+Now, you're probably saying, "that's all fine and dandy, but typing all my logs into Logstash isn't really an option, and merely seeing them spit to STDOUT isn't very useful." Good point. First, let's set up Elasticsearch to store the messages we send into Logstash. If you don't have Elasticearch already installed, you can http://www.elasticsearch.org/download/[download the RPM or DEB package], or install manually by downloading the current release tarball, by issuing the following four commands:
+
+[source,js]
+----------------------------------
+curl -O https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-%ELASTICSEARCH_VERSION%.tar.gz
+tar zxvf elasticsearch-%ELASTICSEARCH_VERSION%.tar.gz
+cd elasticsearch-%ELASTICSEARCH_VERSION%/
+./bin/elasticsearch
+----------------------------------
+
+NOTE: This tutorial specifies running Logstash %VERSION% with Elasticsearch %ELASTICSEARCH_VERSION%. Each release of Logstash has a *recommended* version of Elasticsearch to pair with. Make sure the versions match based on the http://logstash.net/docs/latest[Logstash version] you're running!
+
+More detailed information on installing and configuring Elasticsearch can be found on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index.html[The Elasticsearch reference pages]. However, for the purposes of Getting Started with Logstash, the default installation and configuration of Elasticsearch should be sufficient.
+
+Now that we have Elasticsearch running on port 9200 (we do, right?), Logstash can be simply configured to use Elasticsearch as its backend. The defaults for both Logstash and Elasticsearch are fairly sane and well thought out, so we can omit the optional configurations within the elasticsearch output:
+
+[source,js]
+----------------------------------
+bin/logstash -e 'input { stdin { } } output { elasticsearch { host => localhost } }'
+----------------------------------
+
+Type something, and Logstash will process it as before (this time you won't see any output, since we don't have the stdout output configured)
+
+[source,js]
+----------------------------------
+you know, for logs
+----------------------------------
+
+You can confirm that ES actually received the data by making a curl request and inspecting the return:
+
+[source,js]
+----------------------------------
+curl 'http://localhost:9200/_search?pretty'
+----------------------------------
+
+which should return something like this:
+
+[source,js]
+----------------------------------
+{
+  "took" : 2,
+  "timed_out" : false,
+  "_shards" : {
+    "total" : 5,
+    "successful" : 5,
+    "failed" : 0
+  },
+  "hits" : {
+    "total" : 1,
+    "max_score" : 1.0,
+    "hits" : [ {
+      "_index" : "logstash-2013.11.21",
+      "_type" : "logs",
+      "_id" : "2ijaoKqARqGvbMgP3BspJA",
+      "_score" : 1.0, "_source" : {"message":"you know, for logs","@timestamp":"2013-11-21T18:45:09.862Z","@version":"1","host":"my-laptop"}
+    } ]
+  }
+}
+----------------------------------
+
+Congratulations! You've successfully stashed logs in Elasticsearch via Logstash.
+
+=== Elasticsearch Plugins (an aside)
+Another very useful tool for querying your Logstash data (and Elasticsearch in general) is the Elasticearch-kopf plugin. Here is more information on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html[Elasticsearch plugins]. To install elasticsearch-kopf, simply issue the following command in your Elasticsearch directory (the same one in which you ran Elasticsearch earlier):
+
+[source,js]
+----------------------------------
+bin/plugin -install lmenezes/elasticsearch-kopf
+----------------------------------
+Now you can browse to http://localhost:9200/_plugin/kopf/[http://localhost:9200/_plugin/kopf/] to browse your Elasticsearch data, settings and mappings!
+
+=== Multiple Outputs
+As a quick exercise in configuring multiple Logstash outputs, let's invoke Logstash again, using both the 'stdout' as well as the 'elasticsearch' output:
+
+[source,js]
+----------------------------------
+bin/logstash -e 'input { stdin { } } output { elasticsearch { host => localhost } stdout { } }'
+----------------------------------
+Typing a phrase will now echo back to your terminal, as well as save in Elasticsearch! (Feel free to verify this using curl or elasticsearch-kopf).
+
+=== Default - Daily Indices
+You might notice that Logstash was smart enough to create a new index in Elasticsearch... The default index name is in the form of 'logstash-YYYY.MM.DD', which essentially creates one index per day. At midnight (UTC), Logstash will automagically rotate the index to a fresh new one, with the new current day's timestamp. This allows you to keep windows of data, based on how far retroactively you'd like to query your log data. Of course, you can always archive (or re-index) your data to an alternate location, where you are able to query further into the past. If you'd like to simply delete old indices after a certain time period, you can use the https://github.com/elasticsearch/curator[Elasticsearch Curator tool].
+
+== Moving On
+Now you're ready for more advanced configurations. At this point, it makes sense for a quick discussion of some of the core features of Logstash, and how they interact with the Logstash engine.
+
+=== The Life of an Event
+
+Inputs, Outputs, Codecs and Filters are at the heart of the Logstash configuration. By creating a pipeline of event processing, Logstash is able to extract the relevant data from your logs and make it available to elasticsearch, in order to efficiently query your data. To get you thinking about the various options available in Logstash, let's discuss some of the more common configurations currently in use. For more details, read about http://logstash.net/docs/latest/life-of-an-event[the Logstash event pipeline].
+
+==== Inputs
+Inputs are the mechanism for passing log data to Logstash. Some of the more useful, commonly-used ones are:
+
+* *file*: reads from a file on the filesystem, much like the UNIX command "tail -0a"
+* *syslog*: listens on the well-known port 514 for syslog messages and parses according to RFC3164 format
+* *redis*: reads from a redis server, using both redis channels and also redis lists. Redis is often used as a "broker" in a centralized Logstash installation, which queues Logstash events from remote Logstash "shippers".
+* *lumberjack*: processes events sent in the lumberjack protocol. Now called https://github.com/elasticsearch/logstash-forwarder[logstash-forwarder].
+
+==== Filters
+Filters are used as intermediary processing devices in the Logstash chain. They are often combined with conditionals in order to perform a certain action on an event, if it matches particular criteria. Some useful filters:
+
+* *grok*: parses arbitrary text and structure it. Grok is currently the best way in Logstash to parse unstructured log data into something structured and queryable. With 120 patterns shipped built-in to Logstash, it's more than likely you'll find one that meets your needs!
+* *mutate*: The mutate filter allows you to do general mutations to fields. You can rename, remove, replace, and modify fields in your events.
+* *drop*: drop an event completely, for example, 'debug' events.
+* *clone*: make a copy of an event, possibly adding or removing fields.
+* *geoip*: adds information about geographical location of IP addresses (and displays amazing charts in kibana)
+
+==== Outputs
+Outputs are the final phase of the Logstash pipeline. An event may pass through multiple outputs during processing, but once all outputs are complete, the event has finished its execution. Some commonly used outputs include:
+
+* *elasticsearch*: If you're planning to save your data in an efficient, convenient and easily queryable format... Elasticsearch is the way to go. Period. Yes, we're biased :)
+* *file*: writes event data to a file on disk.
+* *graphite*: sends event data to graphite, a popular open source tool for storing and graphing metrics. http://graphite.wikidot.com/
+* *statsd*: a service which "listens for statistics, like counters and timers, sent over UDP and sends aggregates to one or more pluggable backend services". If you're already using statsd, this could be useful for you!
+
+==== Codecs
+Codecs are basically stream filters which can operate as part of an input, or an output. Codecs allow you to easily separate the transport of your messages from the serialization process. Popular codecs include 'json', 'msgpack' and 'plain' (text).
+
+* *json*: encode / decode data in JSON format
+* *multiline*: Takes multiple-line text events and merge them into a single event, e.g. java exception and stacktrace messages
+
+For the complete list of (current) configurations, visit the Logstash "plugin configuration" section of the http://logstash.net/docs/latest/[Logstash documentation page].
+
+
+== More fun with Logstash
+=== Persistent Configuration files
+
+Specifying configurations on the command line using '-e' is only so helpful, and more advanced setups will require more lengthy, long-lived configurations. First, let's create a simple configuration file, and invoke Logstash using it. Create a file named "logstash-simple.conf" and save it in the same directory as Logstash.
+
+[source,js]
+----------------------------------
+input { stdin { } }
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----------------------------------
+
+Then, run this command:
+
+[source,js]
+----------------------------------
+bin/logstash -f logstash-simple.conf
+----------------------------------
+
+Et voil√†! Logstash will read in the configuration file you just created and run as in the example we saw earlier. Note that we used the '-f' to read in the file, rather than the '-e' to read the configuration from the command line. This is a very simple case, of course, so let's move on to some more complex examples.
+
+=== Filters
+Filters are an in-line processing mechanism which provide the flexibility to slice and dice your data to fit your needs. Let's see one in action, namely the *grok filter*.
+
+[source,js]
+----------------------------------
+input { stdin { } }
+
+filter {
+  grok {
+    match => { "message" => "%{COMBINEDAPACHELOG}" }
+  }
+  date {
+    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
+  }
+}
+
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----------------------------------
+Run Logstash with this configuration:
+
+[source,js]
+----------------------------------
+bin/logstash -f logstash-filter.conf
+----------------------------------
+
+Now paste this line into the terminal (so it will be processed by the stdin input):
+[source,js]
+----------------------------------
+127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] "GET /xampp/status.php HTTP/1.1" 200 3891 "http://cadenza/xampp/navi.php" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0"
+----------------------------------
+
+You should see something returned to STDOUT which looks like this:
+
+[source,js]
+----------------------------------
+{
+        "message" => "127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] \"GET /xampp/status.php HTTP/1.1\" 200 3891 \"http://cadenza/xampp/navi.php\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\"",
+     "@timestamp" => "2013-12-11T08:01:45.000Z",
+       "@version" => "1",
+           "host" => "cadenza",
+       "clientip" => "127.0.0.1",
+          "ident" => "-",
+           "auth" => "-",
+      "timestamp" => "11/Dec/2013:00:01:45 -0800",
+           "verb" => "GET",
+        "request" => "/xampp/status.php",
+    "httpversion" => "1.1",
+       "response" => "200",
+          "bytes" => "3891",
+       "referrer" => "\"http://cadenza/xampp/navi.php\"",
+          "agent" => "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\""
+}
+----------------------------------
+
+As you can see, Logstash (with help from the *grok* filter) was able to parse the log line (which happens to be in Apache "combined log" format) and break it up into many different discrete bits of information. This will be extremely useful later when we start querying and analyzing our log data... for example, we'll be able to run reports on HTTP response codes, IP addresses, referrers, etc. very easily. There are quite a few grok patterns included with Logstash out-of-the-box, so it's quite likely if you're attempting to parse a fairly common log format, someone has already done the work for you. For more details, see the list of https://github.com/logstash/logstash/blob/master/patterns/[logstash grok patterns] on github.
+
+The other filter used in this example is the *date* filter. This filter parses out a timestamp and uses it as the timestamp for the event (regardless of when you're ingesting the log data). You'll notice that the @timestamp field in this example is set to December 11, 2013, even though Logstash is ingesting the event at some point afterwards. This is handy when backfilling logs, for example... the ability to tell Logstash "use this value as the timestamp for this event".
+
+== Useful Examples
+
+=== Apache logs (from files)
+Now, let's configure something actually *useful*... apache2 access log files! We are going to read the input from a file on the localhost, and use a *conditional* to process the event according to our needs. First, create a file called something like 'logstash-apache.conf' with the following contents (you'll need to change the log's file path to suit your needs):
+
+[source,js]
+----------------------------------
+input {
+  file {
+    path => "/tmp/access_log"
+    start_position => "beginning"
+  }
+}
+
+filter {
+  if [path] =~ "access" {
+    mutate { replace => { "type" => "apache_access" } }
+    grok {
+      match => { "message" => "%{COMBINEDAPACHELOG}" }
+    }
+  }
+  date {
+    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
+  }
+}
+
+output {
+  elasticsearch {
+    host => localhost
+  }
+  stdout { codec => rubydebug }
+}
+
+----------------------------------
+
+Then, create the file you configured above (in this example, "/tmp/access_log") with the following log lines as contents (or use some from your own webserver):
+
+[source,js]
+----------------------------------
+71.141.244.242 - kurt [18/May/2011:01:48:10 -0700] "GET /admin HTTP/1.1" 301 566 "-" "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3"
+134.39.72.245 - - [18/May/2011:12:40:18 -0700] "GET /favicon.ico HTTP/1.1" 200 1189 "-" "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; InfoPath.2; .NET4.0C; .NET4.0E)"
+98.83.179.51 - - [18/May/2011:19:35:08 -0700] "GET /css/main.css HTTP/1.1" 200 1837 "http://www.safesand.com/information.htm" "Mozilla/5.0 (Windows NT 6.0; WOW64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1"
+----------------------------------
+
+Now run it with the -f flag as in the last example:
+
+[source,js]
+----------------------------------
+bin/logstash -f logstash-apache.conf
+----------------------------------
+
+You should be able to see your apache log data in Elasticsearch now! You'll notice that Logstash opened the file you configured, and read through it, processing any events it encountered. Any additional lines logged to this file will also be captured, processed by Logstash as events and stored in Elasticsearch. As an added bonus, they will be stashed with the field "type" set to "apache_access" (this is done by the type => "apache_access" line in the input configuration).
+
+In this configuration, Logstash is only watching the apache access_log, but it's easy enough to watch both the access_log and the error_log (actually, any file matching '*log'), by changing one line in the above configuration, like this:
+
+[source,js]
+----------------------------------
+input {
+  file {
+    path => "/tmp/*_log"
+...
+----------------------------------
+
+Now, rerun Logstash, and you will see both the error and access logs processed via Logstash. However, if you inspect your data (using elasticsearch-kopf, perhaps), you will see that the access_log was broken up into discrete fields, but not the error_log. That's because we used a "grok" filter to match the standard combined apache log format and automatically split the data into separate fields. Wouldn't it be nice *if* we could control how a line was parsed, based on its format? Well, we can...
+
+Also, you might have noticed that Logstash did not reprocess the events which were already seen in the access_log file. Logstash is able to save its position in files, only processing new lines as they are added to the file. Neat!
+
+=== Conditionals
+Now we can build on the previous example, where we introduced the concept of a *conditional*. A conditional should be familiar to most Logstash users, in the general sense. You may use 'if', 'else if' and 'else' statements, as in many other programming languages. Let's label each event according to which file it appeared in (access_log, error_log and other random files which end with "log").
+
+[source,js]
+----------------------------------
+input {
+  file {
+    path => "/tmp/*_log"
+  }
+}
+
+filter {
+  if [path] =~ "access" {
+    mutate { replace => { type => "apache_access" } }
+    grok {
+      match => { "message" => "%{COMBINEDAPACHELOG}" }
+    }
+    date {
+      match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
+    }
+  } else if [path] =~ "error" {
+    mutate { replace => { type => "apache_error" } }
+  } else {
+    mutate { replace => { type => "random_logs" } }
+  }
+}
+
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----------------------------------
+
+You'll notice we've labeled all events using the "type" field, but we didn't actually parse the "error" or "random" files... There are so many types of error logs that it's better left as an exercise for you, depending on the logs you're seeing.
+
+=== Syslog
+OK, now we can move on to another incredibly useful example: *syslog*. Syslog is one of the most common use cases for Logstash, and one it handles exceedingly well (as long as the log lines conform roughly to RFC3164 :). Syslog is the de facto UNIX networked logging standard, sending messages from client machines to a local file, or to a centralized log server via rsyslog. For this example, you won't need a functioning syslog instance; we'll fake it from the command line, so you can get a feel for what happens.
+
+First, let's make a simple configuration file for Logstash + syslog, called 'logstash-syslog.conf'.
+
+[source,js]
+----------------------------------
+input {
+  tcp {
+    port => 5000
+    type => syslog
+  }
+  udp {
+    port => 5000
+    type => syslog
+  }
+}
+
+filter {
+  if [type] == "syslog" {
+    grok {
+      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
+      add_field => [ "received_at", "%{@timestamp}" ]
+      add_field => [ "received_from", "%{host}" ]
+    }
+    syslog_pri { }
+    date {
+      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
+    }
+  }
+}
+
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----------------------------------
+Run it as normal:
+
+[source,js]
+----------------------------------
+bin/logstash -f logstash-syslog.conf
+----------------------------------
+
+Normally, a client machine would connect to the Logstash instance on port 5000 and send its message. In this simplified case, we're simply going to telnet to Logstash and enter a log line (similar to how we entered log lines into STDIN earlier). First, open another shell window to interact with the Logstash syslog input and type the following command:
+
+[source,js]
+----------------------------------
+telnet localhost 5000
+----------------------------------
+
+You can copy and paste the following lines as samples (feel free to try some of your own, but keep in mind they might not parse if the grok filter is not correct for your data):
+
+[source,js]
+----------------------------------
+Dec 23 12:11:43 louis postfix/smtpd[31499]: connect from unknown[95.75.93.154]
+Dec 23 14:42:56 louis named[16000]: client 199.48.164.7#64817: query (cache) 'amsterdamboothuren.com/MX/IN' denied
+Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)
+Dec 22 18:28:06 louis rsyslogd: [origin software="rsyslogd" swVersion="4.2.0" x-pid="2253" x-info="http://www.rsyslog.com"] rsyslogd was HUPed, type 'lightweight'.
+----------------------------------
+
+Now you should see the output of Logstash in your original shell as it processes and parses messages!
+
+[source,js]
+----------------------------------
+{
+                 "message" => "Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)",
+              "@timestamp" => "2013-12-23T22:30:01.000Z",
+                "@version" => "1",
+                    "type" => "syslog",
+                    "host" => "0:0:0:0:0:0:0:1:52617",
+        "syslog_timestamp" => "Dec 23 14:30:01",
+         "syslog_hostname" => "louis",
+          "syslog_program" => "CRON",
+              "syslog_pid" => "619",
+          "syslog_message" => "(www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)",
+             "received_at" => "2013-12-23 22:49:22 UTC",
+           "received_from" => "0:0:0:0:0:0:0:1:52617",
+    "syslog_severity_code" => 5,
+    "syslog_facility_code" => 1,
+         "syslog_facility" => "user-level",
+         "syslog_severity" => "notice"
+}
+----------------------------------
+
+Congratulations! You're well on your way to being a real Logstash power user. You should be comfortable configuring, running and sending events to Logstash, but there's much more to explore.
diff --git a/docs/asciidoc/static/howtos-and-tutorials.asciidoc b/docs/asciidoc/static/howtos-and-tutorials.asciidoc
new file mode 100644
index 00000000000..fe98a8e0cb9
--- /dev/null
+++ b/docs/asciidoc/static/howtos-and-tutorials.asciidoc
@@ -0,0 +1,16 @@
+[[howtos-and-tutorials]]
+== Logstash HOWTOs and Tutorials
+Pretty self-explanatory, really
+
+=== Downloads and Releases
+* http://elasticsearch.org/#[Getting Started with Logstash]
+* http://elasticsearch.org/#[Configuration file overview]
+* http://elasticsearch.org/#[Command-line flags]
+* http://elasticsearch.org/#[The life of an event in Logstash]
+* http://elasticsearch.org/#[Using conditional logic]
+* http://elasticsearch.org/#[Glossary]
+* http://elasticsearch.org/#[referring to fields `[like][this]`]
+* http://elasticsearch.org/#[using the `%{fieldname}` syntax]
+* http://elasticsearch.org/#[Metrics from Logs]
+* http://elasticsearch.org/#[Using RabbitMQ]
+* http://elasticsearch.org/#[Contributing to Logstash]
diff --git a/docs/asciidoc/static/life-of-an-event.asciidoc b/docs/asciidoc/static/life-of-an-event.asciidoc
new file mode 100644
index 00000000000..75103b74bf0
--- /dev/null
+++ b/docs/asciidoc/static/life-of-an-event.asciidoc
@@ -0,0 +1,65 @@
+= the life of an event
+
+The Logstash agent is an event pipeline.
+
+== The Pipeline
+
+The Logstash agent is a processing pipeline with 3 stages: inputs -> filters -> outputs. Inputs generate events, filters modify them, outputs ship them elsewhere.
+
+Internal to Logstash, events are passed from each phase using internal queues. It is implemented with a 'SizedQueue' in Ruby. SizedQueue allows a bounded maximum of items in the queue such that any writes to the queue will block if the queue is full at maximum capacity.
+
+Logstash sets each queue size to 20. This means only 20 events can be pending into the next phase - this helps reduce any data loss and in general avoids Logstash trying to act as a data storage system. These internal queues are not for storing messages long-term.
+
+== Fault Tolerance
+
+Starting at outputs, here's what happens when things break.
+
+An output can fail or have problems because of some downstream cause, such as full disk, permissions problems, temporary network failures, or service outages. Most outputs should keep retrying to ship any events that were involved in the failure.
+
+If an output is failing, the output thread will wait until this output is healthy again and able to successfully send the message. Therefore, the output queue will stop being read from by this output and will eventually fill up with events and block new events from being written to this queue.
+
+A full output queue means filters will block trying to write to the output queue. Because filters will be stuck, blocked writing to the output queue, they will stop reading from the filter queue which will eventually cause the filter queue (input -> filter) to fill up.
+
+A full filter queue will cause inputs to block when writing to the filters. This will cause each input to block, causing each input to stop processing new data from wherever that input is getting new events.
+
+In ideal circumstances, this will behave similarly to when the tcp window closes to 0, no new data is sent because the receiver hasn't finished processing the current queue of data, but as soon as the downstream (output) problem is resolved, messages will begin flowing again..
+
+== Thread Model
+
+The thread model in Logstash is currently:
+
+[source,js]
+----------------------------------
+input threads | filter worker threads | output worker
+----------------------------------
+
+Filters are optional, so you will have this model if you have no filters defined:
+
+[source,js]
+----------------------------------
+input threads | output worker
+----------------------------------
+
+Each input runs in a thread by itself. This allows busier inputs to not be blocked by slower ones, etc. It also allows for easier containment of scope because each input has a thread.
+
+The filter thread model is a 'worker' model where each worker receives an event and applies all filters, in order, before emitting that to the output queue. This allows scalability across CPUs because many filters are CPU intensive (permitting that we have thread safety). 
+
+The default number of filter workers is 1, but you can increase this number with the '-w' flag on the agent.
+
+The output worker model is currently a single thread. Outputs will receive events in the order they are defined in the config file. 
+
+Outputs may decide to buffer events temporarily before publishing them, possibly in a separate thread. One example of this is the elasticsearch output
+which will buffer events and flush them all at once, in a separate thread. This mechanism (buffering many events + writing in a separate thread) can improve performance so the Logstash pipeline isn't stalled waiting for a response from elasticsearch.
+
+== Consequences and Expectations
+
+Small queue sizes mean that Logstash simply blocks and stalls safely during times of load or other temporary pipeline problems. There are two alternatives to this - unlimited queue length and dropping messages. Unlimited queues grow grow unbounded and eventually exceed memory causing a crash which loses all of those messages. Dropping messages is also an undesirable behavior in most cases.
+
+At a minimum, Logstash will have probably 3 threads (2 if you have no filters). One input, one filter worker, and one output thread each.
+
+If you see Logstash using multiple CPUs, this is likely why. If you want to know more about what each thread is doing, you should read this: <http://www.semicomplete.com/blog/geekery/debugging-java-performance.html>.
+
+Threads in java have names, and you can use jstack and top to figure out who is using what resources. The URL above will help you learn how to do this.
+
+On Linux platforms, Logstash will label all the threads it can with something descriptive. Inputs will show up as "<inputname" and filter workers as "|worker" and outputs as ">outputworker" (or something similar).  Other threads may be labeled as well, and are intended to help you identify their purpose should you wonder why they are consuming resources!
+
diff --git a/docs/asciidoc/static/logstash-docs-home.asciidoc b/docs/asciidoc/static/logstash-docs-home.asciidoc
new file mode 100644
index 00000000000..19bd3281184
--- /dev/null
+++ b/docs/asciidoc/static/logstash-docs-home.asciidoc
@@ -0,0 +1,30 @@
+[[logstash-docs-home]]
+== Logstash Documentation
+Pretty self-explanatory, really
+
+=== Downloads and Releases
+* http://www.elasticsearch.org/overview/logstash/download/[Download Logstash 1.4.2]
+* http://www.elasticsearch.org/blog/apt-and-yum-repositories/[package repositories]
+* http://www.elasticsearch.org/blog/logstash-1-4-2/[release notes]
+* https://github.com/elasticsearch/logstash/blob/master/CHANGELOG[view changelog]
+* https://github.com/elasticsearch/puppet-logstash[Puppet Module]
+
+=== Plugins
+* http://elasticsearch.org/#[contrib plugins]
+* http://elasticsearch.org/#[writing your own plugins]
+* http://elasticsearch.org/#[Inputs] / http://elasticsearch.org/#[Filters] / http://elasticsearch.org/#[Outputs]
+* http://elasticsearch.org/#[Codecs]
+* http://elasticsearch.org/#[(more)]
+
+=== HOWTOs, References, Information
+* http://elasticsearch.org/#[Getting Started with Logstash]
+* http://elasticsearch.org/#[Configuration file overview]
+* http://elasticsearch.org/#[Command-line flags]
+* http://elasticsearch.org/#[The life of an event in Logstash]
+* http://elasticsearch.org/#[Using conditional logic]
+* http://elasticsearch.org/#[Glossary]
+* http://elasticsearch.org/#[(more)]
+
+=== About / Videos / Blogs
+* http://elasticsearch.org/#[Videos]
+* http://elasticsearch.org/#[Blogs]
diff --git a/docs/asciidoc/static/logstash-glossary.asciidoc b/docs/asciidoc/static/logstash-glossary.asciidoc
new file mode 100644
index 00000000000..9715ffa6c76
--- /dev/null
+++ b/docs/asciidoc/static/logstash-glossary.asciidoc
@@ -0,0 +1,132 @@
+== Glossary 
+Logstash Glossary
+
+apache ::
+	A very common open source web server application, which produces logs easily consumed by Logstash (Apache Common/Combined Log Format).
+
+agent ::
+	An invocation of Logstash with a particular configuration, allowing it to operate as a "shipper", a "collector", or a combination of functionalities.
+
+
+broker ::
+	An intermediary used in a multi-tiered Logstash deployment which allows a queueing mechanism to be used. Examples of brokers are Redis, RabbitMQ, and Apache Kafka. This pattern is a common method of building fault-tolerance into a Logstash architecture. 
+
+buffer::
+	Within Logstash, a temporary storage area where events can queue up, waiting to be processed. The default queue size is 20 events, but it is not recommended to increase this, as Logstash is not designed to operate as a queueing mechanism.
+
+centralized::
+	A configuration of Logstash in which the Logstash agent, input and output sources live on multiple machines, and the pipeline passes through these tiers.
+
+codec::
+	A Logstash plugin which works within an input or output plugin, and usually aims to serialize or deserialize data flowing through the Logstash pipeline. A common example is the JSON codec, which allows Logstash inputs to receive data which arrives in JSON format, or output event data in JSON format.
+
+collector::
+	An instance of Logstash which receives external events from another instance of Logstash, or perhaps some other client, either remote or local.
+
+conditional::
+	In a computer programming context, a control flow which executes certain actions based on true/false values of a statement (called the condition). Often expressed in the form of "if ... then ... (elseif ...) else". Logstash has built-in conditionals to allow users control of the plugin pipeline.
+
+elasticsearch::
+	An open-source, Lucene-based, RESTful search and analytics engine written in Java, with supported clients in various languages such as Perl, Python, Ruby, Java, etc. 
+
+event::
+	In Logstash parlance, a single unit of information, containing a timestamp plus additional data. An event arrives via an input, and is subsequently parsed, timestamped, and passed through the Logstash pipeline.
+
+field::
+	A data point (often a key-value pair) within a full Logstash event, e.g. "timestamp", "message", "hostname", "ipaddress". Also used to describe a key-value pair in Elasticsearch.
+
+file::
+	A resource storing binary data (which might be text, image, application, etc.) on a physical storage media. In the Logstash context, a common input source which monitors a growing collection of text-based log lines.
+
+filter:
+	An intermediary processing mechanism in the Lostash pipeline. Typically, filters act upon event data after it has been ingested via inputs, by mutating, enriching, and/or modifying the data according to configuration rules. The second phase of the typical Logstash pipeline (inputs->filters->outputs). 
+
+fluentd::
+	Like Logstash, another open-source tool for collecting logs and events, with plugins to extend functionality.
+
+ganglia::
+	A scalable, distributed monitoring system suitable for large clusters. Logstash features both an input and an output to enable reading from, and writing to Ganglia.
+
+graphite::
+	A highly-scalable realtime graphing application, which presents graphs through web interfaces. Logstash provides an output which ships event data to Graphite for visualization.
+
+heka::
+	An open-source event processing system developed by Mozilla and often compared to Logstash.
+
+index::
+	An index can be seen as a named collection of documents in Elasticsearch which are available for searching and querying. It is a logical namespace which maps to one or more primary shards and can have zero or more replica shards.
+
+indexer::
+	Refers to a Logstash instance which is tasked with interfacing with an Elasticsearch cluster in order to index event data.
+
+input::
+	The means for ingesting data into Logstash. Inputs allow users to pull data from files, network sockets, other applications, etc. The initial phase of the typical Logstash pipeline (inputs->filters->outputs). 
+
+jar / jarfile::
+	A packaging method for Java libraries. Since Logstash runs on the JRuby runtime environment, it is possible to use these Java libraries to provide extra functionality to Logstash.
+
+java::
+	An object-oriented programming language popular for its flexibility, extendability and portability.
+
+jRuby:
+	JRuby is a 100% Java implementation of the Ruby programming language, which allows Ruby to run in the JVM. Logstash typically runs in JRuby, which provides it with a fast, extensible runtime environment. 
+
+kibana::
+	A visual tool for viewing time-based data which has been stored in Elasticsearch. Kibana features a powerful set of functionality based on panels which query Elasticsearch in different ways.
+
+log::
+	A snippet of textual information emitted by a device, ostensibly with some pertinent information about the status of said device.
+
+log4j::
+	A very common Java-based logging utility.
+
+Logstash::
+	An application which offers a powerful data processing pipeline, allowing users to consume information from various sources, enrich the data, and output it to any number of other sources.
+
+lumberjack::
+	A protocol for shipping logs from one location to another, in a secure and optimized manner. Also the (deprecated) name of a software application, now known as Logstash Forwarder (LSF).
+
+output::
+	The means for passing event data out of Logstash into other applications, network endpoints, files, etc. The last phase of the typical Logstash pipeline (inputs->filters->outputs). 
+
+pipeline::
+	A term used to describe the flow of events through the Logstash workflow. The pipeline typically consists of a series of inputs, filters, and outputs.
+
+plugin::
+	A generic term referring to an input, codec, filter, or output which extends basic Logstash functionality.
+
+redis::
+	An open-source key-value store and cache which is often used in conjunction with Logstash as a message broker.
+
+ruby::
+	A popular, open-source, object-oriented programming language in which Logstash is implemented.
+
+shell::
+	A command-line interface to an operating system.
+
+shipper::
+	An instance of Logstash which send events to another instance of Logstash, or some other application.
+
+statsd::
+	A network daemon for aggregating statistics, such as counters and timers, and shipping over UDP to backend services, such as Graphite or Datadog. Logstash provides an output to statsd.
+
+stdin::
+	An I/O stream providing input to a software application. In Logstash, an input which receives data from this stream.
+
+stdout::
+	An I/O stream producing output from a software application. In Logstash, an output which produces data from this stream.
+
+syslog::
+	A popular method for logging messages from a computer. The standard is somewhat loose, but Logstash has tools (input, grok patterns) to make this simpler.
+
+standalone::
+	A configuration of Logstash in which the Logstash agent, input and output sources typically live on the same host machine.
+
+thread::
+	Parallel sequences of execution within a process which allow a computer to perform several tasks simultaneously, in a multi-processor environment. Logstash takes advantage of this functionality, by specifying the "-w" flag
+
+type::
+	In Elasticsearch type, a type can be compared to a table in a relational database. Each type has a list of fields that can be specified for documents of that type. The mapping defines how each field in the document is analyzed. To index documents, it is required to specify both an index and a type.
+
+worker::
+	The filter thread model used by Logstash, where each worker receives an event and applies all filters, in order, before emitting the event to the output queue. This allows scalability across CPUs because many filters are CPU intensive (permitting that we have thread safety). 
diff --git a/docs/asciidoc/static/plugin-milestones.asciidoc b/docs/asciidoc/static/plugin-milestones.asciidoc
new file mode 100644
index 00000000000..c24dd4c0986
--- /dev/null
+++ b/docs/asciidoc/static/plugin-milestones.asciidoc
@@ -0,0 +1,24 @@
+= Plugin Milestones
+
+== Why Milestones?
+Plugins (inputs/outputs/filters/codecs) have a milestone label in Logstash. This is to provide an indicator to the end-user as to the kinds of changes a given plugin could have between Logstash releases.
+
+The desire here is to allow plugin developers to quickly iterate on possible new plugins while conveying to the end-user a set of expectations about that plugin.
+
+== Milestone 1
+
+Plugins at this milestone need your feedback to improve! Plugins at this milestone may change between releases as the community figures out the best way for the plugin to behave and be configured.
+
+== Milestone 2
+
+Plugins at this milestone are more likely to have backwards-compatibility to previous releases than do Milestone 1 plugins. This milestone also indicates a greater level of in-the-wild usage by the community than the previous milestone.
+
+== Milestone 3
+
+Plugins at this milestone have strong promises towards backwards-compatibility. This is enforced with automated tests to ensure behavior and configuration are consistent across releases.
+
+== Milestone 0
+
+This milestone appears at the bottom of the page because it is very infrequently used.
+
+This milestone marker is used to generally indicate that a plugin has no active code maintainer nor does it have support from the community in terms of getting help.
diff --git a/docs/asciidoc_index.rb b/docs/asciidoc_index.rb
new file mode 100644
index 00000000000..be1b94403f8
--- /dev/null
+++ b/docs/asciidoc_index.rb
@@ -0,0 +1,35 @@
+#!/usr/bin/env ruby
+
+require "erb"
+
+if ARGV.size != 2
+  $stderr.puts "No path given to search for plugin docs"
+  $stderr.puts "Usage: #{$0} plugin_doc_dir type"
+  exit 1
+end
+
+
+def plugins(glob)
+  plugins=Hash.new []
+  files = Dir.glob(glob)
+  files.collect { |f| File.basename(f).gsub(".asciidoc", "") }.each {|plugin|
+    first_letter = plugin[0,1]
+    plugins[first_letter] += [plugin]
+  }
+  return Hash[plugins.sort]
+end # def plugins
+
+basedir = ARGV[0]
+type = ARGV[1]
+
+docs = plugins(File.join(basedir, "#{type}/*.asciidoc"))
+template_path = File.join(File.dirname(__FILE__), "index-#{type}.asciidoc.erb")
+template = File.new(template_path).read
+erb = ERB.new(template, nil, "-")
+
+path = "#{basedir}/#{type}.asciidoc"
+
+File.open(path, "w") do |out|
+  html = erb.result(binding)
+  out.puts(html)
+end
diff --git a/docs/asciidocgen.rb b/docs/asciidocgen.rb
new file mode 100644
index 00000000000..ec7cbba5d51
--- /dev/null
+++ b/docs/asciidocgen.rb
@@ -0,0 +1,254 @@
+require "rubygems"
+require "erb"
+require "optparse"
+require "kramdown" # markdown parser
+
+$: << Dir.pwd
+$: << File.join(File.dirname(__FILE__), "..", "lib")
+
+require "logstash/config/mixin"
+require "logstash/inputs/base"
+require "logstash/codecs/base"
+require "logstash/filters/base"
+require "logstash/outputs/base"
+require "logstash/version"
+
+class LogStashConfigAsciiDocGenerator
+  COMMENT_RE = /^ *#(?: (.*)| *$)/
+
+  def initialize
+    @rules = {
+      COMMENT_RE => lambda { |m| add_comment(m[1]) },
+      /^ *class.*< *LogStash::(Outputs|Filters|Inputs|Codecs)::(Base|Threadable)/ => \
+        lambda { |m| set_class_description },
+      /^ *config +[^=].*/ => lambda { |m| add_config(m[0]) },
+      /^ *milestone .*/ => lambda { |m| set_milestone(m[0]) },
+      /^ *config_name .*/ => lambda { |m| set_config_name(m[0]) },
+      /^ *flag[( ].*/ => lambda { |m| add_flag(m[0]) },
+      /^ *(class|def|module) / => lambda { |m| clear_comments },
+    }
+
+    if File.exists?("build/contrib_plugins")
+      @contrib_list = File.read("build/contrib_plugins").split("\n")
+    else
+      @contrib_list = []
+    end
+  end
+
+  def parse(string)
+    clear_comments
+    buffer = ""
+    string.split(/\r\n|\n/).each do |line|
+      # Join long lines
+      if line =~ COMMENT_RE
+        # nothing
+      else
+        # Join extended lines
+        if line =~ /(, *$)|(\\$)|(\[ *$)/
+          buffer += line.gsub(/\\$/, "")
+          next
+        end
+      end
+
+      line = buffer + line
+      buffer = ""
+
+      @rules.each do |re, action|
+        m = re.match(line)
+        if m
+          action.call(m)
+        end
+      end # RULES.each
+    end # string.split("\n").each
+  end # def parse
+
+  def set_class_description
+    @class_description = @comments.join("\n")
+    clear_comments
+  end # def set_class_description
+
+  def add_comment(comment)
+    return if comment == "encoding: utf-8"
+    @comments << comment
+  end # def add_comment
+
+  def add_config(code)
+    # I just care about the 'config :name' part
+    code = code.sub(/,.*/, "")
+
+    # call the code, which calls 'config' in this class.
+    # This will let us align comments with config options.
+    name, opts = eval(code)
+
+    # TODO(sissel): This hack is only required until regexp configs
+    # are gone from logstash.
+    name = name.to_s unless name.is_a?(Regexp)
+
+    description = Kramdown::Document.new(@comments.join("\n")).to_kramdown
+    @attributes[name][:description] = description
+    clear_comments
+  end # def add_config
+
+  def add_flag(code)
+    # call the code, which calls 'config' in this class.
+    # This will let us align comments with config options.
+    #p :code => code
+    fixed_code = code.gsub(/ do .*/, "")
+    #p :fixedcode => fixed_code
+    name, description = eval(fixed_code)
+    @flags[name] = description
+    clear_comments
+  end # def add_flag
+
+  def set_config_name(code)
+    name = eval(code)
+    @name = name
+  end # def set_config_name
+
+  def set_milestone(code)
+    @milestone = eval(code)
+  end
+
+  # pretend to be the config DSL and just get the name
+  def config(name, opts={})
+    return name, opts
+  end # def config
+
+  # Pretend to support the flag DSL
+  def flag(*args, &block)
+    name = args.first
+    description = args.last
+    return name, description
+  end # def config
+
+  # pretend to be the config dsl's 'config_name' method
+  def config_name(name)
+    return name
+  end # def config_name
+
+  # pretend to be the config dsl's 'milestone' method
+  def milestone(m)
+    return m
+  end # def milestone
+
+  def clear_comments
+    @comments.clear
+  end # def clear_comments
+
+  def generate(file, settings)
+    @class_description = ""
+    @milestone = ""
+    @comments = []
+    @attributes = Hash.new { |h,k| h[k] = {} }
+    @flags = {}
+
+    # local scoping for the monkeypatch belowg
+    attributes = @attributes
+    # Monkeypatch the 'config' method to capture
+    # Note, this monkeypatch requires us do the config processing
+    # one at a time.
+    #LogStash::Config::Mixin::DSL.instance_eval do
+      #define_method(:config) do |name, opts={}|
+        #p name => opts
+        #attributes[name].merge!(opts)
+      #end
+    #end
+
+    # Loading the file will trigger the config dsl which should
+    # collect all the config settings.
+    load file
+
+    # parse base first
+    parse(File.new(File.join(File.dirname(file), "base.rb"), "r").read)
+
+    # Now parse the real library
+    code = File.new(file).read
+
+    # inputs either inherit from Base or Threadable.
+    if code =~ /\< LogStash::Inputs::Threadable/
+      parse(File.new(File.join(File.dirname(file), "threadable.rb"), "r").read)
+    end
+
+    if code =~ /include LogStash::PluginMixins/
+      mixin = code.gsub(/.*include LogStash::PluginMixins::(\w+)\s.*/m, '\1')
+      mixin.gsub!(/(.)([A-Z])/, '\1_\2')
+      mixin.downcase!
+      parse(File.new(File.join(File.dirname(file), "..", "plugin_mixins", "#{mixin}.rb")).read)
+    end
+
+    parse(code)
+
+    puts "Generating docs for #{file}"
+
+    if @name.nil?
+      $stderr.puts "Missing 'config_name' setting in #{file}?"
+      return nil
+    end
+
+    klass = LogStash::Config::Registry.registry[@name]
+    if klass.ancestors.include?(LogStash::Inputs::Base)
+      section = "input"
+    elsif klass.ancestors.include?(LogStash::Filters::Base)
+      section = "filter"
+    elsif klass.ancestors.include?(LogStash::Outputs::Base)
+      section = "output"
+    elsif klass.ancestors.include?(LogStash::Codecs::Base)
+      section = "codec"
+    end
+
+    template_file = File.join(File.dirname(__FILE__), "plugin-doc.asciidoc.erb")
+    template = ERB.new(File.new(template_file).read, nil, "-")
+
+    is_contrib_plugin = @contrib_list.include?(file)
+
+    # descriptions are assumed to be markdown
+    description = Kramdown::Document.new(@class_description).to_kramdown
+
+    klass.get_config.each do |name, settings|
+      @attributes[name].merge!(settings)
+      default = klass.get_default(name)
+      unless default.nil?
+        @attributes[name][:default] = default
+      end
+    end
+    sorted_attributes = @attributes.sort { |a,b| a.first.to_s <=> b.first.to_s }
+    klassname = LogStash::Config::Registry.registry[@name].to_s
+    name = @name
+
+    synopsis_file = File.join(File.dirname(__FILE__), "plugin-synopsis.asciidoc.erb")
+    synopsis = ERB.new(File.new(synopsis_file).read, nil, "-").result(binding)
+
+    if settings[:output]
+      dir = File.join(settings[:output], section + "s")
+      path = File.join(dir, "#{name}.asciidoc")
+      Dir.mkdir(settings[:output]) if !File.directory?(settings[:output])
+      Dir.mkdir(dir) if !File.directory?(dir)
+      File.open(path, "w") do |out|
+        html = template.result(binding)
+        html.gsub!("%VERSION%", LOGSTASH_VERSION)
+        html.gsub!("%PLUGIN%", @name)
+        out.puts(html)
+      end
+    else
+      puts template.result(binding)
+    end
+  end # def generate
+
+end # class LogStashConfigDocGenerator
+
+if __FILE__ == $0
+  opts = OptionParser.new
+  settings = {}
+  opts.on("-o DIR", "--output DIR",
+          "Directory to output to; optional. If not specified,"\
+          "we write to stdout.") do |val|
+    settings[:output] = val
+  end
+
+  args = opts.parse(ARGV)
+
+  args.each do |arg|
+    gen = LogStashConfigAsciiDocGenerator.new
+    gen.generate(arg, settings)
+  end
+end
diff --git a/docs/configuration.md b/docs/configuration.md
index 8b145a6a60f..77696213b9c 100644
--- a/docs/configuration.md
+++ b/docs/configuration.md
@@ -1,16 +1,18 @@
 ---
-title: Configuration Language - logstash
+title: Configuration Language - Logstash
 layout: content_right
 ---
-# LogStash Config Language
+# Logstash Config Language
 
-The logstash config language aims to be simple.
+The Logstash config language aims to be simple.
 
-There's 3 main sections: inputs, filters, outputs. Each section has
+There are 3 main sections: inputs, filters, outputs. Each section has
 configurations for each plugin available in that section.
 
 Example:
 
+    # This is a comment. You should use comments to describe
+    # parts of your configuration.
     input {
       ...
     }
@@ -23,24 +25,63 @@ Example:
       ...
     }
 
-## Filters
+## Filters and Ordering
 
-For a given event, are applied in the order of appearance in the config file.
+For a given event, are applied in the order of appearance in the
+configuration file.
 
-## Types
+## Comments
 
-The documentation for a plugin may say that a config field has a certain type.
-Examples include boolean, string, array, number, hash, etc.
+Comments are the same as in ruby, perl, and python. Starts with a '#' character. Example:
 
-### Boolean
+    # this is a comment
 
-A boolean must be either true or false.
+    input { # comments can appear at the end of a line, too
+      # ...
+    }
+
+## Plugins
+
+The input, filter and output sections all let you configure plugins. Plugin
+configuration consists of the plugin name followed by a block of settings for
+that plugin. For example, how about two file inputs:
+
+    input {
+      file {
+        path => "/var/log/messages"
+        type => "syslog"
+      }
+
+      file {
+        path => "/var/log/apache/access.log"
+        type => "apache"
+      }
+    }
+
+The above configures two file separate inputs. Both set two
+configuration settings each: 'path' and 'type'. Each plugin has different
+settings for configuring it; seek the documentation for your plugin to
+learn what settings are available and what they mean. For example, the
+[file input][fileinput] documentation will explain the meanings of the
+path and type settings.
+
+[fileinput]: inputs/file
+
+## Value Types
+
+The documentation for a plugin may enforce a configuration field having a
+certain type.  Examples include boolean, string, array, number, hash,
+etc.
+
+### <a name="boolean"></a>Boolean
+
+A boolean must be either `true` or `false`. Note the lack of quotes around `true` and `false`.
 
 Examples:
 
     debug => true
 
-### String
+### <a name="string"></a>String
 
 A string must be a single value.
 
@@ -48,17 +89,19 @@ Example:
 
     name => "Hello world"
 
-### Number
+Single, unquoted words are valid as strings, too, but you should use quotes.
 
-Numbers must be valid numerics (floating point or integer are OK)
+### <a name="number"></a>Number
+
+Numbers must be valid numerics (floating point or integer are OK).
 
 Example:
 
     port => 33
 
-### Array
+### <a name="array"></a>Array
 
-An 'array' can be a single string value or multiple. If you specify the same
+An array can be a single string value or multiple. If you specify the same
 field multiple times, it appends to the array.
 
 Examples:
@@ -68,20 +111,185 @@ Examples:
 
 The above makes 'path' a 3-element array including all 3 strings.
 
-### Hash
+### <a name="hash"></a>Hash
+
+A hash is basically the same syntax as Ruby hashes. 
+The key and value are simply pairs, such as:
+
+    match => {
+      "field1" => "value1"
+      "field2" => "value2"
+      ...
+    }
+
+## <a name="fieldreferences"></a>Field References
+
+All events have properties. For example, an apache access log would have things
+like status code (200, 404), request path ("/", "index.html"), HTTP verb (GET, POST),
+client IP address, etc. Logstash calls these properties "fields." 
+
+In many cases, it is useful to be able to refer to a field by name. To do this,
+you can use the Logstash field reference syntax.
+
+By way of example, let us suppose we have this event:
+
+    {
+      "agent": "Mozilla/5.0 (compatible; MSIE 9.0)",
+      "ip": "192.168.24.44",
+      "request": "/index.html"
+      "response": {
+        "status": 200,
+        "bytes": 52353
+      },
+      "ua": {
+        "os": "Windows 7"
+      }
+    }
+
+- the syntax to access fields is `[fieldname]`.
+- if you are only referring to a **top-level field**, you can omit the `[]` and
+simply say `fieldname`.
+- in the case of **nested fields**, like the "os" field above, you need
+the full path to that field: `[ua][os]`.
+
+## <a name="sprintf"></a>sprintf format
+
+This syntax is also used in what Logstash calls 'sprintf format'. This format
+allows you to refer to field values from within other strings. For example, the
+statsd output has an 'increment' setting, to allow you to keep a count of
+apache logs by status code:
+
+    output {
+      statsd {
+        increment => "apache.%{[response][status]}"
+      }
+    }
+
+You can also do time formatting in this sprintf format. Instead of specifying a field name, use the `+FORMAT` syntax where `FORMAT` is a [time format](http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html). 
+
+For example, if you want to use the file output to write to logs based on the
+hour and the 'type' field:
+
+    output {
+      file {
+        path => "/var/log/%{type}.%{+yyyy.MM.dd.HH}"
+      }
+    }
+
+## <a name="conditionals"></a>Conditionals
 
-A 'hash' is currently represented using the same syntax as an array (see above).
-The 'key' and 'value' are simply pairs, such as:
+Sometimes you only want a filter or output to process an event under
+certain conditions. For that, you'll want to use a conditional!
 
-    match => [ "field1", "pattern1", "field2", "pattern2" ]
+Conditionals in Logstash look and act the same way they do in programming
+languages. You have `if`, `else if` and `else` statements. Conditionals may be
+nested if you need that.
 
-The above would internally be represented as this hash: `{ "field1" =>
-"pattern1", "field2" => "pattern2" }`
+The syntax is follows:
 
-Why this syntax? Well frankly it was easier than adding additional grammar to
-the config language. Logstash may support ruby- or json-like hash syntax in the
-future, but not otday.
+    if EXPRESSION {
+      ...
+    } else if EXPRESSION {
+      ...
+    } else {
+      ...
+    }
+
+What's an expression? Comparison tests, boolean logic, etc!
+
+The following comparison operators  are supported:
+
+* equality, etc: ==,  !=,  <,  >,  <=,  >= 
+* regexp: =~, !~ 
+* inclusion: in, not in
+
+The following boolean operators are supported:
+
+* and, or, nand, xor
+
+The following unary operators are supported:
+
+* !
+
+Expressions may contain expressions. Expressions may be negated with `!`.
+Expressions may be grouped with parentheses `(...)`. Expressions can be long
+and complex.
+
+For example, if we want to remove the field `secret` if the field
+`action` has a value of `login`:
+
+    filter {
+      if [action] == "login" {
+        mutate { remove => "secret" }
+      }
+    }
+
+The above uses the field reference syntax to get the value of the
+`action` field. It is compared against the text `login` and, if equal,
+allows the mutate filter to delete the field named `secret`.
+
+How about a more complex example?
+
+* alert nagios of any apache events with status 5xx
+* record any 4xx status to elasticsearch
+* record all status code hits via statsd
+
+How about telling nagios of any http event that has a status code of 5xx?
+
+    output {
+      if [type] == "apache" {
+        if [status] =~ /^5\d\d/ {
+          nagios { ...  }
+        } else if [status] =~ /^4\d\d/ {
+          elasticsearch { ... }
+        }
+
+        statsd { increment => "apache.%{status}" }
+      }
+    }
+
+You can also do multiple expressions in a single condition:
+
+    output {
+      # Send production errors to pagerduty
+      if [loglevel] == "ERROR" and [deployment] == "production" {
+        pagerduty {
+          ...
+        }
+      }
+    }
+
+Here are some examples for testing with the in conditional:
+
+    filter {
+      if [foo] in [foobar] {
+        mutate { add_tag => "field in field" }
+      }
+      if [foo] in "foo" {
+        mutate { add_tag => "field in string" }
+      }
+      if "hello" in [greeting] {
+        mutate { add_tag => "string in field" }
+      }
+      if [foo] in ["hello", "world", "foo"] {
+        mutate { add_tag => "field in list" }
+      }
+      if [missing] in [alsomissing] {
+        mutate { add_tag => "shouldnotexist" }
+      }
+      if !("foo" in ["hello", "world"]) {
+        mutate { add_tag => "shouldexist" }
+      }
+    }
+
+Or, to test if grok was successful:
+
+    output {
+      if "_grokparsefailure" not in [tags] {
+        elasticsearch { ... }
+      }
+    }
 
-## Further reading
+## Further Reading
 
 For more information, see [the plugin docs index](index)
diff --git a/docs/contrib-plugins.md b/docs/contrib-plugins.md
new file mode 100644
index 00000000000..d4adbcee8d2
--- /dev/null
+++ b/docs/contrib-plugins.md
@@ -0,0 +1,59 @@
+---
+title: Logstash Contrib plugins
+layout: content_right
+---
+
+# contrib plugins
+
+As logstash has grown, we've accumulated a massive repository of plugins. Well
+over 100 plugins, it became difficult for the project maintainers to adequately
+support everything effectively.
+
+In order to improve the quality of popular plugins, we've moved the
+less-commonly-used plugins to a separate repository we're calling "contrib".
+Concentrating common plugin usage into core solves a few problems, most notably
+user complaints about the size of logstash releases, support/maintenance costs,
+etc.
+
+It is our intent that this separation will improve life for users. If it
+doesn't, please file a bug so we can work to address it!
+
+If a plugin is available in the 'contrib' package, the documentation for that
+plugin will note this boldly at the top of that plugin's documentation.
+
+Contrib plugins reside in a [separate github project](https://github.com/elasticsearch/logstash-contrib).
+
+# Packaging
+
+At present, the contrib modules are available as a tarball.
+
+# Automated Installation
+
+The `bin/plugin` script will handle the installation for you:
+
+    cd /path/to/logstash
+    bin/plugin install contrib
+
+# Manual Installation
+
+The contrib plugins can be extracted on top of an existing Logstash installation. 
+
+For example, if I've extracted `logstash-%VERSION%.tar.gz` into `/path`, e.g.
+ 
+    cd /path
+    tar zxf ~/logstash-%VERSION%.tar.gz
+
+It will have a `/path/logstash-%VERSION%` directory, e.g.
+
+    $ ls
+    logstash-%VERSION%
+
+The method to install the contrib tarball is identical.
+
+    cd /path
+    wget http://download.elasticsearch.org/logstash/logstash/logstash-contrib-%VERSION%.tar.gz
+    tar zxf ~/logstash-contrib-%VERSION%.tar.gz
+
+This will install the contrib plugins in the same directory as the core
+install. These plugins will be available to logstash the next time it starts.
+
diff --git a/docs/docgen.rb b/docs/docgen.rb
index 0cdaa2a7702..f64a7f8f853 100644
--- a/docs/docgen.rb
+++ b/docs/docgen.rb
@@ -1,44 +1,50 @@
 require "rubygems"
 require "erb"
 require "optparse"
-require "bluecloth" # for markdown parsing
-
-# TODO(sissel): Currently this doc generator doesn't follow ancestry, so
-# LogStash::Input::Amqp inherits Base, but we don't parse the base file.
-# We need this, though.
-#
-# TODO(sissel): Convert this to use ERB, not random bits of 'puts'
+require "kramdown" # markdown parser
 
 $: << Dir.pwd
 $: << File.join(File.dirname(__FILE__), "..", "lib")
 
+require "logstash/config/mixin"
+require "logstash/inputs/base"
+require "logstash/codecs/base"
+require "logstash/filters/base"
+require "logstash/outputs/base"
+require "logstash/version"
+
 class LogStashConfigDocGenerator
   COMMENT_RE = /^ *#(?: (.*)| *$)/
 
   def initialize
     @rules = {
       COMMENT_RE => lambda { |m| add_comment(m[1]) },
-      /^ *class.*< *LogStash::(Outputs|Filters|Inputs)::Base/ => \
+      /^ *class.*< *LogStash::(Outputs|Filters|Inputs|Codecs)::(Base|Threadable)/ => \
         lambda { |m| set_class_description },
-      /^ *config .*/ => lambda { |m| add_config(m[0]) },
+      /^ *config +[^=].*/ => lambda { |m| add_config(m[0]) },
+      /^ *milestone .*/ => lambda { |m| set_milestone(m[0]) },
       /^ *config_name .*/ => lambda { |m| set_config_name(m[0]) },
       /^ *flag[( ].*/ => lambda { |m| add_flag(m[0]) },
       /^ *(class|def|module) / => lambda { |m| clear_comments },
     }
-    @comments = []
-    @settings = {}
-    @flags = {}
+
+    if File.exists?("build/contrib_plugins")
+      @contrib_list = File.read("build/contrib_plugins").split("\n")
+    else
+      @contrib_list = []
+    end
   end
 
   def parse(string)
+    clear_comments
     buffer = ""
-    string.split("\n").each do |line|
+    string.split(/\r\n|\n/).each do |line|
       # Join long lines
       if line =~ COMMENT_RE
         # nothing
       else
         # Join extended lines
-        if line =~ /(, *$)|(\\$)/
+        if line =~ /(, *$)|(\\$)|(\[ *$)/
           buffer += line.gsub(/\\$/, "")
           next
         end
@@ -60,37 +66,49 @@ def set_class_description
     @class_description = @comments.join("\n")
     clear_comments
   end # def set_class_description
- 
+
   def add_comment(comment)
+    return if comment == "encoding: utf-8"
     @comments << comment
   end # def add_comment
 
   def add_config(code)
+    # I just care about the 'config :name' part
+    code = code.sub(/,.*/, "")
+
     # call the code, which calls 'config' in this class.
     # This will let us align comments with config options.
     name, opts = eval(code)
 
-    description = BlueCloth.new(@comments.join("\n")).to_html
-    @settings[name] = opts.merge(:description => description)
+    # TODO(sissel): This hack is only required until regexp configs
+    # are gone from logstash.
+    name = name.to_s unless name.is_a?(Regexp)
+
+    description = Kramdown::Document.new(@comments.join("\n")).to_html
+    @attributes[name][:description] = description
     clear_comments
   end # def add_config
 
   def add_flag(code)
     # call the code, which calls 'config' in this class.
     # This will let us align comments with config options.
-    p :code => code
+    #p :code => code
     fixed_code = code.gsub(/ do .*/, "")
-    p :fixedcode => fixed_code
+    #p :fixedcode => fixed_code
     name, description = eval(fixed_code)
     @flags[name] = description
     clear_comments
-  end # def add_config
+  end # def add_flag
 
   def set_config_name(code)
     name = eval(code)
     @name = name
   end # def set_config_name
 
+  def set_milestone(code)
+    @milestone = eval(code)
+  end
+
   # pretend to be the config DSL and just get the name
   def config(name, opts={})
     return name, opts
@@ -108,25 +126,56 @@ def config_name(name)
     return name
   end # def config_name
 
+  # pretend to be the config dsl's 'milestone' method
+  def milestone(m)
+    return m
+  end # def milestone
+
   def clear_comments
     @comments.clear
   end # def clear_comments
 
   def generate(file, settings)
-    require "logstash/inputs/base"
-    require "logstash/filters/base"
-    require "logstash/outputs/base"
-    require file
-
-    @comments = []
-    @settings = {}
     @class_description = ""
+    @milestone = ""
+    @comments = []
+    @attributes = Hash.new { |h,k| h[k] = {} }
+    @flags = {}
+
+    # local scoping for the monkeypatch belowg
+    attributes = @attributes
+    # Monkeypatch the 'config' method to capture
+    # Note, this monkeypatch requires us do the config processing
+    # one at a time.
+    #LogStash::Config::Mixin::DSL.instance_eval do
+      #define_method(:config) do |name, opts={}|
+        #p name => opts
+        #attributes[name].merge!(opts)
+      #end
+    #end
+
+    # Loading the file will trigger the config dsl which should
+    # collect all the config settings.
+    load file
 
     # parse base first
     parse(File.new(File.join(File.dirname(file), "base.rb"), "r").read)
 
     # Now parse the real library
     code = File.new(file).read
+
+    # inputs either inherit from Base or Threadable.
+    if code =~ /\< LogStash::Inputs::Threadable/
+      parse(File.new(File.join(File.dirname(file), "threadable.rb"), "r").read)
+    end
+
+    if code =~ /include LogStash::PluginMixins/
+      mixin = code.gsub(/.*include LogStash::PluginMixins::(\w+)\s.*/m, '\1')
+      mixin.gsub!(/(.)([A-Z])/, '\1_\2')
+      mixin.downcase!
+      parse(File.new(File.join(File.dirname(file), "..", "plugin_mixins", "#{mixin}.rb")).read)
+    end
+
     parse(code)
 
     puts "Generating docs for #{file}"
@@ -143,27 +192,40 @@ def generate(file, settings)
       section = "filter"
     elsif klass.ancestors.include?(LogStash::Outputs::Base)
       section = "output"
+    elsif klass.ancestors.include?(LogStash::Codecs::Base)
+      section = "codec"
     end
 
-    template_file = File.join(File.dirname(__FILE__), "docs.html.erb")
+    template_file = File.join(File.dirname(__FILE__), "plugin-doc.html.erb")
     template = ERB.new(File.new(template_file).read, nil, "-")
 
+    is_contrib_plugin = @contrib_list.include?(file)
+
     # descriptions are assumed to be markdown
-    description = BlueCloth.new(@class_description).to_html
+    description = Kramdown::Document.new(@class_description).to_html
 
-    sorted_settings = @settings.sort { |a,b| a.first.to_s <=> b.first.to_s }
+    klass.get_config.each do |name, settings|
+      @attributes[name].merge!(settings)
+    end
+    sorted_attributes = @attributes.sort { |a,b| a.first.to_s <=> b.first.to_s }
     klassname = LogStash::Config::Registry.registry[@name].to_s
     name = @name
 
+    synopsis_file = File.join(File.dirname(__FILE__), "plugin-synopsis.html.erb")
+    synopsis = ERB.new(File.new(synopsis_file).read, nil, "-").result(binding)
+
     if settings[:output]
       dir = File.join(settings[:output], section + "s")
       path = File.join(dir, "#{name}.html")
       Dir.mkdir(settings[:output]) if !File.directory?(settings[:output])
       Dir.mkdir(dir) if !File.directory?(dir)
       File.open(path, "w") do |out|
-        out.puts(template.result(binding))
+        html = template.result(binding)
+        html.gsub!("%VERSION%", LOGSTASH_VERSION)
+        html.gsub!("%PLUGIN%", @name)
+        out.puts(html)
       end
-    else 
+    else
       puts template.result(binding)
     end
   end # def generate
@@ -173,7 +235,7 @@ def generate(file, settings)
 if __FILE__ == $0
   opts = OptionParser.new
   settings = {}
-  opts.on("-o DIR", "--output DIR", 
+  opts.on("-o DIR", "--output DIR",
           "Directory to output to; optional. If not specified,"\
           "we write to stdout.") do |val|
     settings[:output] = val
diff --git a/docs/docs.html.erb b/docs/docs.html.erb
deleted file mode 100644
index 718a1d74fbf..00000000000
--- a/docs/docs.html.erb
+++ /dev/null
@@ -1,76 +0,0 @@
----
-title: logstash docs for <%= section %>s/<%= name %>
-layout: content_right
----
-<h2><%= name %></h2>
-
-<%= description %>
-
-<% if !@flags.empty? -%>
-<h3> Flags </h3>
-
-This plugin provides the following flags:
-
-<dl>
-<% @flags.each do |flag, description| -%>
-<%# Prefix flag with plugin name. %>
-  <dt> <%= flag.gsub(/^--/, "--#{name}-") %> </dt>
-  <dd> <%= description %> </dd>
-<% end -%>
-</dl>
-
-<% end -%>
-
-<h3> Synopsis </h3>
-
-This is what it might look like in your config file:
-
-<pre><code>
-<%= section %> {
-  <%= name %> {
-<% sorted_settings.each do |setting_name, config|
-     if config[:validate].is_a?(Array) 
-       annotation = "#{config[:validate].inspect}"
-     else 
-       annotation = "#{config[:validate]}"
-     end
-     annotation += " (required)" if config[:required]
-     annotation += ", default: #{config[:default].inspect}" if config[:default]
--%>
-    <a href="#setting_<%= setting_name %>"><%= setting_name %></a> => ... # <%= annotation %>
-<% end -%>
-  }
-}
-</code></pre>
-
-<h3> Details </h3>
-
-<% sorted_settings.each do |setting_name, config| -%>
-<h4> 
-  <a name="setting_<%= setting_name %>">
-    <%= setting_name %><%= " (required setting)" if config[:required] %>
-</a>
-</h4>
-
-<ul>
-<% if config[:validate].is_a?(Symbol) -%>
-  <li> Value type is <%= config[:validate] %> </li>
-<% elsif config[:validate].nil? -%>
-  <li> Value type is string </li>
-<% elsif config[:validate].is_a?(Array) -%>
-  <li> Value can be any of: <%= config[:validate].map(&:inspect).join(", ") %> </li>
-<% end -%>
-<% if config[:default] -%>
-  <li> Default value is <%= config[:default].inspect %> </li>
-<% else -%>
-  <li> There is no default value for this setting. </li>
-<% end -%>
-</ul>
-
-<%= config[:description] %>
-
-<% end -%>
-
-<hr>
-
-This is documentation from <a href="https://github.com/logstash/logstash/blob/master/<%= file %>"><%= file %>
diff --git a/docs/extending.md b/docs/extending.md
deleted file mode 100644
index 10494021e91..00000000000
--- a/docs/extending.md
+++ /dev/null
@@ -1,40 +0,0 @@
----
-title: How to extend - logstash
-layout: content_right
----
-# Extending logstash
-
-You can add your own input, output, or filter plugins to logstash.
-
-DOCS - TBD
-
-If you're looking to extend logstash today, please look at the existing plugins.
-
-Good examples include:
-
-* [inputs/beanstalk](https://github.com/logstash/logstash/blob/master/lib/logstash/inputs/beanstalk.rb)
-* [filters/multiline](https://github.com/logstash/logstash/blob/master/lib/logstash/filters/multiline.rb)
-* [outputs/mongodb](https://github.com/logstash/logstash/blob/master/lib/logstash/outputs/mongodb.rb)
-
-Main stuff you need to know:
-
-* 'config_name' sets the name used in the config file.
-* 'config' lines define config options
-* 'register' method is called per plugin instantiation. Do any of your initialization here.
-
-Inputs have two methods: register and run.
-
-* Each input runs as it's own thread.
-* the 'run' method is expected to run-forever.
-
-Filters have two methods: register and filter.
-
-* 'filter' method gets an event. 
-* Call 'event.cancel' to drop the event.
-* To modify an event, simply make changes to the event you are given.
-* The return value is ignored.
-
-Outputs have two methods: register and receive.
-
-* 'register' is called per plugin instantiation. Do any of your initialization here.
-* 'receive' is called when an event gets pushed to your output
diff --git a/docs/extending/example-add-a-new-filter.md b/docs/extending/example-add-a-new-filter.md
new file mode 100644
index 00000000000..6b613226735
--- /dev/null
+++ b/docs/extending/example-add-a-new-filter.md
@@ -0,0 +1,108 @@
+---
+title: How to extend - logstash
+layout: content_right
+---
+# Add a new filter
+
+This document shows you how to add a new filter to logstash.
+
+For a general overview of how to add a new plugin, see [the extending
+logstash](.) overview.
+
+## Write code.
+
+Let's write a 'hello world' filter. This filter will replace the 'message' in
+the event with "Hello world!"
+
+First, logstash expects plugins in a certain directory structure: `logstash/TYPE/PLUGIN_NAME.rb`
+
+Since we're creating a filter, let's mkdir this:
+
+    mkdir -p logstash/filters/
+    cd logstash/filters
+
+Now add the code:
+
+    # Call this file 'foo.rb' (in logstash/filters, as above)
+    require "logstash/filters/base"
+    require "logstash/namespace"
+
+    class LogStash::Filters::Foo < LogStash::Filters::Base
+
+      # Setting the config_name here is required. This is how you
+      # configure this filter from your logstash config.
+      #
+      # filter {
+      #   foo { ... }
+      # }
+      config_name "foo"
+
+      # New plugins should start life at milestone 1.
+      milestone 1
+
+      # Replace the message with this value.
+      config :message, :validate => :string
+
+      public
+      def register
+        # nothing to do
+      end # def register
+
+      public
+      def filter(event)
+        # return nothing unless there's an actual filter event
+        return unless filter?(event)
+        if @message
+          # Replace the event message with our message as configured in the
+          # config file.
+          event["message"] = @message
+        end
+        # filter_matched should go in the last line of our successful code 
+        filter_matched(event)
+      end # def filter
+    end # class LogStash::Filters::Foo
+
+## Add it to your configuration
+
+For this simple example, let's just use stdin input and stdout output.
+The config file looks like this:
+
+    input { 
+      stdin { type => "foo" } 
+    }
+    filter {
+      if [type] == "foo" {
+        foo {
+          message => "Hello world!"
+        }
+      }
+    }
+    output {
+      stdout { }
+    }
+
+Call this file 'example.conf'
+
+## Tell logstash about it.
+
+Depending on how you installed logstash, you have a few ways of including this
+plugin.
+
+You can use the agent flag --pluginpath flag to specify where the root of your
+plugin tree is. In our case, it's the current directory.
+
+    % bin/logstash --pluginpath your/plugin/root -f example.conf
+
+## Example running
+
+In the example below, I typed in "the quick brown fox" after running the java
+command.
+
+    % bin/logstash -f example.conf
+    the quick brown fox   
+    2011-05-12T01:05:09.495000Z stdin://snack.home/: Hello world!
+
+The output is the standard logstash stdout output, but in this case our "the
+quick brown fox" message was replaced with "Hello world!"
+
+All done! :)
diff --git a/docs/extending/index.md b/docs/extending/index.md
new file mode 100644
index 00000000000..014f9700337
--- /dev/null
+++ b/docs/extending/index.md
@@ -0,0 +1,91 @@
+---
+title: How to extend - logstash
+layout: content_right
+---
+# Extending logstash
+
+You can add your own input, output, or filter plugins to logstash.
+
+If you're looking to extend logstash today, please look at the existing plugins.
+
+## Good examples of plugins
+
+* [inputs/tcp](https://github.com/logstash/logstash/blob/master/lib/logstash/inputs/tcp.rb)
+* [filters/multiline](https://github.com/logstash/logstash/blob/master/lib/logstash/filters/multiline.rb)
+* [outputs/mongodb](https://github.com/logstash/logstash/blob/master/lib/logstash/outputs/mongodb.rb)
+
+## Common concepts
+
+* The `config_name` sets the name used in the config file.
+* The `milestone` sets the milestone number of the plugin. See <../plugin-milestones> for more info.
+* The `config` lines define config options.
+* The `register` method is called per plugin instantiation. Do any of your initialization here.
+
+### Required modules
+
+All plugins should require the Logstash module.
+
+    require 'logstash/namespace'
+
+### Plugin name
+
+Every plugin must have a name set with the `config_name` method. If this
+is not specified plugins will fail to load with an error.
+
+### Milestones
+
+Every plugin needs a milestone set using `milestone`. See
+<../plugin-milestones> for more info.
+
+### Config lines
+
+The `config` lines define configuration options and are constructed like
+so:
+
+    config :host, :validate => :string, :default => "0.0.0.0"
+
+The name of the option is specified, here `:host` and then the
+attributes of the option. They can include `:validate`, `:default`,
+`:required` (a Boolean `true` or `false`), and `:deprecated` (also a
+Boolean).  
+ 
+## Inputs
+
+All inputs require the LogStash::Inputs::Base class:
+
+    require 'logstash/inputs/base'
+ 
+Inputs have two methods: `register` and `run`.
+
+* Each input runs as its own thread.
+* The `run` method is expected to run-forever.
+
+## Filters
+
+All filters require the LogStash::Filters::Base class:
+
+    require 'logstash/filters/base'
+ 
+Filters have two methods: `register` and `filter`.
+
+* The `filter` method gets an event. 
+* Call `event.cancel` to drop the event.
+* To modify an event, simply make changes to the event you are given.
+* The return value is ignored.
+
+## Outputs
+
+All outputs require the LogStash::Outputs::Base class:
+
+    require 'logstash/outputs/base'
+ 
+Outputs have two methods: `register` and `receive`.
+
+* The `register` method is called per plugin instantiation. Do any of your initialization here.
+* The `receive` method is called when an event gets pushed to your output
+
+## Example: a new filter
+
+Learn by example how to [add a new filter to logstash](example-add-a-new-filter)
+
+
diff --git a/docs/flags.md b/docs/flags.md
index 68669d0bf18..508c18ddea5 100644
--- a/docs/flags.md
+++ b/docs/flags.md
@@ -9,26 +9,39 @@ layout: content_right
 The logstash agent has the following flags (also try using the '--help' flag)
 
 <dl>
-<dt> --config CONFIGFILE </dt>
-<dd> Load the logstash config from a specific file </dd>
-<dt> --log FILE </dt>
+<dt> -f, --config CONFIGFILE </dt>
+<dd> Load the logstash config from a specific file, directory, or a
+wildcard. If given a directory or wildcard, config files will be read
+from the directory in alphabetical order. </dd>
+<dt> -e CONFIGSTRING </dt>
+<dd> Use the given string as the configuration data. Same syntax as the
+config file. If not input is specified, 'stdin { type => stdin }' is
+default. If no output is specified, 'stdout { debug => true }}' is
+default. </dd>
+<dt> -w, --filterworkers COUNT </dt>
+<dd> Run COUNT filter workers (default: 1) </dd>
+<dt> --watchdog-timeout TIMEOUT </dt>
+<dd> Set watchdog timeout value in seconds. Default is 10.</dd>
+<dt> -l, --log FILE </dt>
 <dd> Log to a given path. Default is to log to stdout </dd>
-<dt> -v </dt>
-<dd> Increase verbosity. There are multiple levels of verbosity available with
-'-vvv' currently being the highest </dd>
+<dt> --verbose </dt>
+<dd> Increase verbosity to the first level, less verbose.</dd>
+<dt> --debug </dt>
+<dd> Increase verbosity to the last level, more verbose.</dd>
+<dt> -v  </dt>
+<dd> *DEPRECATED: see --verbose/debug* Increase verbosity. There are multiple levels of verbosity available with
+'-vv' currently being the highest </dd>
 <dt> --pluginpath PLUGIN_PATH </dt>
 <dd> A colon-delimted path to find other logstash plugins in </dd>
 </dl>
 
-## Web UI
+
+## Web
 
 <dl>
-<dt> --log FILE </dt>
-<dd> Log to a given path. Default is stdout. </dd>
-<dt> --address ADDRESS </dt>
-<dd> Address on which to start webserver. Default is 0.0.0.0. </dd>
-<dt> --port PORT </dt>
-<dd> Port on which to start webserver. Default is 9292. </dd>
-<dt> --backend URL </dt>
-<dd> The backend URL to use. Default is elasticserach://localhost:9200/ </dd>
+<dt> -a, --address ADDRESS </dt>
+<dd>Address on which to start webserver. Default is 0.0.0.0.</dd>
+<dt> -p, --port PORT</dt>
+<dd>Port on which to start webserver. Default is 9292.</dd>
 </dl>
+
diff --git a/docs/generate_index.rb b/docs/generate_index.rb
index 2801aacd3f0..6e7bed8e45b 100644
--- a/docs/generate_index.rb
+++ b/docs/generate_index.rb
@@ -17,6 +17,7 @@ def plugins(glob)
 basedir = ARGV[0]
 docs = {
   "inputs" => plugins(File.join(basedir, "inputs/*.html")),
+  "codecs" => plugins(File.join(basedir, "codecs/*.html")),
   "filters" => plugins(File.join(basedir, "filters/*.html")),
   "outputs" => plugins(File.join(basedir, "outputs/*.html")),
 }
diff --git a/docs/getting-started-centralized.md b/docs/getting-started-centralized.md
deleted file mode 100644
index f0d175e80cd..00000000000
--- a/docs/getting-started-centralized.md
+++ /dev/null
@@ -1,242 +0,0 @@
----
-title: Getting Started (Centralized Server) - logstash
-layout: content_right
----
-
-# Getting Started
-
-## Centralized Setup with Event Parsing
-
-This guide shows how to get you going quickly with logstash with multiple
-servers. This guide is for folks who want to ship all their logstash logs to a
-central location for indexing and search.
-
-We'll have two classes of server. First, one that ships logs. Second, one that
-collects and indexes logs.
-
-It's important to note that logstash itself has no concept of "shipper" and
-"collector" - the behavior of an agent depends entirely on how you configure
-it.
-
-On servers shipping logs:
-
-* Download and run logstash (See section 'logstash log shipper' below)
-
-On the server collecting and indexing your logs:
-
-* Download and run elasticsearch
-* Download and run an AMQP broker
-* Download and install grok (library) and jls-grok (rubygems)
-* Download and run logstash
-
-## ElasticSearch
-
-Requirements: java
-
-Paste this in your shell for easy downloadings.
-
-    ES_PACKAGE=elasticsearch-0.16.0.zip
-    ES_DIR=${ES_PACKAGE%%.zip}
-    SITE=http://github.com/downloads/elasticsearch/elasticsearch
-    if [ ! -d "$ES_DIR" ] ; then
-      wget --no-check-certificate $SITE/$ES_PACKAGE
-      unzip $ES_PACKAGE
-    fi
-
-Otherwise - Download and unpack the elasticsearch yourself; you'll want version
-0.16.0 or newer. It's written in Java and requires Java (uses Lucene on the
-backend; if you want to know more read the elasticsearch docs).
-
-To start the service, run `bin/elasticsearch`. If you want to run it in the
-foreground, use 'bin/elasticsearch -f' 
-
-== AMQP Broker ==
-
-AMQP is a standard for message-based communication. It supports publish-subscribe, queues, etc. 
-AMQP is supported way to ship your logs between servers with logstash.
-
-If you don't know what AMQP is, that's fine, you don't need to know anything
-about it for this config. If you already have an AMQP server and know how to configure it, you
-can skip this section.
-
-If you don't have an AMQP server already, you might as well download [rabbitmq
-http://www.rabbitmq.com/server.html] I recommend using the native packages
-(rpm, deb) if those are available for your system.
-
-Configuring RabbitMQ is out of scope for this doc, but know that if you use the
-RPM or Deb package you'll probably end up with a rabbitmq startup script that
-you can use, and you'll be ready to go to the next section.
-
-If you want/need to configure RabbitMQ, seek the rabbitmq docs.
-
-== grok ==
-
-Site for download and install docs: http://code.google.com/p/semicomplete/wiki/Grok
-
-You'll need to install grok. If you're on Ubuntu 10.04 64bit, you can use this
-[ubuntu package
-http://code.google.com/p/semicomplete/downloads/detail?name=grok_1.20101030.3088_amd64.deb&can=2&q=]
-
-See http://code.google.com/p/semicomplete/source/browse/grok/INSTALL for
-further installation instructions and dependency information
-
-Note: On some systems, you may need to symlink libgrok.so to libgrok.so.1 (wherever
-you installed grok to).
-
-Note: If you get segfaults from grok, it's likely becuase you are missing a
-correct dependency. Make sure you have the recent-enough versionf of libpcre
-and tokyocabinet (see above grok/INSTALL url)
-
-(This next step can be skipped if you are using a logstash jar release
-(logstash-X.Y.jar, etc) Once you have grok installed, you need to install the
-'jls-grok' rubygem, which you can do by running:
-
-{{{
-gem install jls-grok
-}}}
- 
-== logstash ==
-
-Once you have elasticsearch and rabbitmq (or any AMQP server) running, you're
-ready to configure logstash.
-
-Download the monolithic logstash release package. By 'monolithic' I mean the
-package contains all required dependencies to save you time chasing down
-requirements.
-
-    TODO(sissel): PUT DOWNLOAD LINK
-
-Since we're doing a centralized configuration, you'll have two main logstash
-agent roles: a shipper and an indexer. You will ship logs from all servers to a
-single AMQP message queue and have another agent receive those messages, parse
-them, and index them in elasticsearch.
-
-=== logstash log shipper ===
-
-This agent you will run on all of your servers you want to collect logs on.
-Here's a good sample config:
-
-    input {
-      file {
-        type => "syslog"
-
-        # Wildcards work here :)
-        path => [ "/var/log/messages", "/var/log/syslog", "/var/log/*.log" ]
-      }
-
-      file {
-        type => "apache-access"
-        path => "/var/log/apache2/access.log"
-      }
-
-      file {
-        type => "apache-error"
-        path => "/var/log/apache2/error.log"
-      }
-    }
-
-    output {
-      # Output events to stdout for debugging. Feel free to remove
-      # this output if you don't need it.
-      stdout { }
-
-      # Ship events to the amqp fanout queue named 'rawlogs"
-      amqp {
-        host => "myamqpserver"
-        exchange_type => "fanout"
-        name => "rawlogs"
-      }
-    }
-
-Put this in a file and call it 'myshipper.conf' (or anything, really), and run: 
-
-    java -jar logstash-1.0-monolithic.jar agent -f myshipper.conf
-
-This should start tailing the file inputs specified above and ships them out
-over amqp. If you included the 'stdout' output you will see events written to
-stdout as they are found.
-
-=== logstash indexer ===
-
-This agent will parse and index your logs as they come in over AMQP. Here's a
-sample config based on the previous section.
-
-We'll use grok to parse some logs. Grok is a filter in logstash. Additionally,
-after we parse with grok, we want to take any timestamps found in the log and
-parse them to use as the real timestamp value for the event.
-
-    input {
-      amqp {
-        # ship logs to the 'rawlogs' fanout queue.
-        type => "all"
-        host => "myamqpserver"
-        exchange_type => "fanout"
-        name => "rawlogs"
-      }
-    }
-
-    filter {
-      grok {
-        type => "syslog" # for logs of type "syslog"
-        pattern => "%{SYSLOGLINE}"
-        # You can specify multiple 'pattern' lines
-      }
-
-      grok {
-        type => "apache-access" # for logs of type 'apache-access'
-        pattern => "%{COMBINEDAPACHELOG}"
-      }
-
-      date {
-        type => "syslog"
-
-        # The 'timestamp' and 'timestamp8601' names are for fields in the
-        # logstash event.  The 'SYSLOGLINE' grok pattern above includes a field
-        # named 'timestamp' that is set to the normal syslog timestamp if it
-        # exists in the event.
-        timestamp => "MMM  d HH:mm:ss"   # syslog 'day' value can be space-leading
-        timestamp => "MMM dd HH:mm:ss"
-        timestamp8601 => ISO8601 # Some syslogs use ISO8601 time format
-      }
-
-      date {
-        type => "apache-access"
-        timestamp => "dd/MMM/yyyy:HH:mm:ss Z"
-      }
-    }
-    
-    output {
-      stdout { }
-
-      # If your elasticsearch server is discoverable with multicast, use this:
-      #elasticsearch { }
-
-      # If you can't discover using multicast, set the address explicitly
-      elasticsearch {
-        host => "myelasticsearchserver"
-      }
-    }
-
-
-The above config will take raw logs in over amqp, parse them with grok and date
-filters, and index them into elasticsearch.
-
-== logstash web interface ==
-
-Run this on the same server as your elasticsearch server.
-
-To run the logstash web server, just run the jar with 'web' as the first
-argument. 
-
-    % java -jar logstash-1.0-monolithic.jar web
-    >> Thin web server (v1.2.7 codename No Hup)
-    >> Maximum connections set to 1024
-    >> Listening on 0.0.0.0:9292, CTRL+C to stop
-
-Just point your browser at the http://yourserver:9292/ and start searching
-logs!
-
-Note: If your elasticsearch server is not discoverable with multicast, you can
-specify the host explicitly using the --backend flag:
-
-    % java -jar logstash-1.0-monolithic.jar web --backend elasticsearch://myserver/
diff --git a/docs/getting-started-simple.md b/docs/getting-started-simple.md
deleted file mode 100644
index 4989c27cd39..00000000000
--- a/docs/getting-started-simple.md
+++ /dev/null
@@ -1,105 +0,0 @@
----
-title: Getting Started (Standalone server) - logstash
-layout: content_right
----
-# Getting started with logstash (standalone server example)
-
-This guide shows how to get you going quickly with logstash on a single,
-standalone server collecting its own logs. By standalone, I mean that
-everything happens on a single server: log collection, indexing, and the web
-interface.
-
-logstash can be run on multiple servers (collect from many servers to a single
-indexer) if you want, but this example shows simply a standalone configuration.
-
-Steps:
-
-* Download and run elasticsearch
-* Download and run logstash
-
-## Problems?
-
-If you have problems, feel free to email the users list
-(logstash-users@googlegroups.com) or join IRC (#logstash on irc.freenode.org)
-
-## ElasticSearch
-
-Requirements: Java. I have only tested with sun java.
-
-Use this shell scriptlet to help download and unpack elasticsearch:
-
-    ES_PACKAGE=elasticsearch-0.16.0.zip
-    ES_DIR=${ES_PACKAGE%%.zip}
-    if [ ! -d "$ES_DIR" ] ; then
-      wget --no-check-certificate http://github.com/downloads/elasticsearch/elasticsearch/$ES_PACKAGE
-      unzip $ES_PACKAGE
-    fi
-
-Otherwise: Download and unpack the elasticsearch yourself; you'll want version
-0.16.0 or newer. It's written in Java and requires Java (uses Lucene on the
-backend; if you want to know more read the <a href="http://elasticsearch.org">elasticsearch docs</a>).
-
-To start the service, run bin/elasticsearch. If you want to run it in the
-foreground, use `bin/elasticsearch -f`
-
-## logstash
-
-Once you have elasticsearch running, you're ready to configure logstash.
-
-You should download the logstash 'monolithic' jar. This package includes most
-of the dependencies for logstash in it and helps you get started quicker.
-
-The configuration of any logstash agent consists of specifying inputs, filters,
-and outputs. For this example, we will not configure any filters.
-
-The inputs are your log files. The output will be elasticsearch. The config
-format should be simple to read and write. The bottom of this document includes
-links for further reading (config, etc) if you want to learn more.
-
-Here is a sample config you can start with. It defines some basic inputs
-grouped by type and two outputs.
-
-    input {
-      file {
-        type => "linux-syslog"
-
-        # Wildcards work, here :)
-        path => [ "/var/log/*.log", "/var/log/messages", "/var/log/syslog" ]
-      }
-
-      file {
-        type => "apache-access"
-        path => "/var/log/apache2/access.log"
-      }
-
-      file {
-        type => "apache-error"
-        path => "/var/log/apache2/error.log"
-      }
-    }
-
-    output {
-      # Emit events to stdout for easy debugging of what is going through
-      # logstash.
-      stdout { }
-
-      # This elasticsearch output will try to autodiscover a near-by
-      # elasticsearch cluster using multicast discovery.
-      # If multicast doesn't work, you'll need to set a 'host' setting.
-      elasticsearch { }
-    }
-
-Put this in a file called "mylogstash.conf"
-
-Now run 
-
-    java -jar logstash-1.0-monolithic.jar agent -f mylogstash.conf
-
-## the web interface
-
-To run the logstash web ui, run this:
-
-    java -jar logstash-1.0-monolithic.jar web
-
-Point your browser at <http://yourserver:9292> and start searching!
-
diff --git a/docs/index-codecs.asciidoc.erb b/docs/index-codecs.asciidoc.erb
new file mode 100644
index 00000000000..76daec1a95d
--- /dev/null
+++ b/docs/index-codecs.asciidoc.erb
@@ -0,0 +1,45 @@
+[[codec-plugins]]
+== Codec plugins
+
+Some docs about Codecs
+
+
+
+Available Codec plugins:
+
+<%-
+full_list=[]
+letters=[]
+docs.each do |doc|
+letter = doc[0]
+letters << letter
+-%>
+<<plugins-codecs-letters-<%= letter %>, <%=letter %>>>
+<%- end -%>
+
+<%-
+cols=3
+rows=(docs.count/cols)+1
+item=0
+r=0
+-%>
+[cols="asciidoc,asciidoc,asciidoc"]
+|=======================================================================
+<%- while r < rows do -%>
+<%- c=0; while c < cols do -%>|<% if letters[item].nil? %>&nbsp; <% else %>[[plugins-codecs-letters-<%=letters[item] %>]] <% end %>
+<%- letter = letters[item];
+arr = docs[letter]
+if ! arr.nil?
+arr.each do |plugin_item|
+full_list << plugin_item
+%>* <<plugins-codecs-<%=plugin_item -%>,<%=plugin_item -%>>>
+<%- end 
+end -%>
+<%- item+=1; c+=1; end; r+=1 end -%>
+|=======================================================================
+
+<%-
+full_list.each do |plugin|
+-%>
+include::codecs/<%=plugin %>.asciidoc[]
+<%- end -%>
diff --git a/docs/index-filters.asciidoc.erb b/docs/index-filters.asciidoc.erb
new file mode 100644
index 00000000000..df3a9f4d761
--- /dev/null
+++ b/docs/index-filters.asciidoc.erb
@@ -0,0 +1,45 @@
+[[filter-plugins]]
+== Filter plugins
+
+Some docs about Filter plugins
+
+
+
+Available Filter plugins:
+
+<%-
+full_list=[]
+letters=[]
+docs.each do |doc|
+letter = doc[0]
+letters << letter
+-%>
+<<plugins-filters-letters-<%= letter %>, <%=letter %>>>
+<%- end -%>
+
+<%-
+cols=3
+rows=(docs.count/cols)+1
+item=0
+r=0
+-%>
+[cols="asciidoc,asciidoc,asciidoc"]
+|=======================================================================
+<%- while r < rows do -%>
+<%- c=0; while c < cols do -%>|<% if letters[item].nil? %>&nbsp; <% else %>[[plugins-filters-letters-<%=letters[item] %>]] <% end %>
+<%- letter = letters[item];
+arr = docs[letter]
+if ! arr.nil?
+arr.each do |plugin_item|
+full_list << plugin_item
+%>* <<plugins-filters-<%=plugin_item -%>,<%=plugin_item -%>>>
+<%- end 
+end -%>
+<%- item+=1; c+=1; end; r+=1 end -%>
+|=======================================================================
+
+<%-
+full_list.each do |plugin|
+-%>
+include::filters/<%=plugin %>.asciidoc[]
+<%- end -%>
diff --git a/docs/index-inputs.asciidoc.erb b/docs/index-inputs.asciidoc.erb
new file mode 100644
index 00000000000..9e49f759ece
--- /dev/null
+++ b/docs/index-inputs.asciidoc.erb
@@ -0,0 +1,45 @@
+[[input-plugins]]
+== Input plugins
+
+Some docs about inputs
+
+
+
+Available input plugins:
+
+<%-
+full_list=[]
+letters=[]
+docs.each do |doc|
+letter = doc[0]
+letters << letter
+-%>
+<<plugins-inputs-letters-<%= letter %>, <%=letter %>>>
+<%- 
+end
+
+cols=3
+rows=(docs.count/cols)+1
+item=0
+r=0
+-%>
+[cols="asciidoc,asciidoc,asciidoc"]
+|=======================================================================
+<%- while r < rows do -%>
+<%- c=0; while c < cols do -%>|<% if letters[item].nil? %>&nbsp; <% else %>[[plugins-inputs-letters-<%=letters[item] %>]] <% end %>
+<%- letter = letters[item];
+arr = docs[letter]
+if ! arr.nil?
+arr.each do |plugin_item|
+full_list << plugin_item
+%>* <<plugins-inputs-<%=plugin_item -%>,<%=plugin_item -%>>>
+<%- end 
+end -%>
+<%- item+=1; c+=1; end; r+=1 end -%>
+|=======================================================================
+
+<%-
+full_list.each do |plugin|
+-%>
+include::inputs/<%=plugin %>.asciidoc[]
+<%- end -%>
diff --git a/docs/index-outputs.asciidoc.erb b/docs/index-outputs.asciidoc.erb
new file mode 100644
index 00000000000..ee865fe0822
--- /dev/null
+++ b/docs/index-outputs.asciidoc.erb
@@ -0,0 +1,43 @@
+[[output-plugins]]
+== Output plugins
+
+Some docs about output plugins
+
+Available output plugins:
+
+<%-
+full_list=[]
+letters=[]
+docs.each do |doc|
+letter = doc[0]
+letters << letter
+-%>
+<<plugins-outputs-letters-<%= letter %>, <%=letter %>>>
+<%- end -%>
+
+<%-
+cols=3
+rows=(docs.count/cols)+1
+item=0
+r=0
+-%>
+[cols="asciidoc,asciidoc,asciidoc"]
+|=======================================================================
+<%- while r < rows do -%>
+<%- c=0; while c < cols do -%>|<% if letters[item].nil? %>&nbsp; <% else %>[[plugins-outputs-letters-<%=letters[item] %>]] <% end %>
+<%- letter = letters[item];
+arr = docs[letter]
+if ! arr.nil?
+arr.each do |plugin_item|
+full_list << plugin_item
+%>* <<plugins-outputs-<%=plugin_item -%>,<%=plugin_item -%>>>
+<%- end 
+end -%>
+<%- item+=1; c+=1; end; r+=1 end -%>
+|=======================================================================
+
+<%-
+full_list.each do |plugin|
+-%>
+include::outputs/<%=plugin %>.asciidoc[]
+<%- end -%>
diff --git a/docs/index.html.erb b/docs/index.html.erb
index 7ab797ac598..859f4f158df 100644
--- a/docs/index.html.erb
+++ b/docs/index.html.erb
@@ -4,16 +4,42 @@ layout: content_right
 ---
 <div id="doc_index_container">
 
+  <h3> For Users </h3>
   <ul>
-    <li> <a href="getting-started-simple"> getting started (standalone) </a> </li>
-    <li> <a href="getting-started-centralized"> getting started (centralized) </a> </li>
-    <li> <a href="installation"> install </a> </li>
+    <li> <a href="https://download.elasticsearch.org/logstash/logstash/logstash-%VERSION%.tar.gz"> download logstash %VERSION% </a> </li>
+    <li> <a href="release-notes"> upgrade/release notes </a> </li>
+    <li> <a href="https://github.com/elasticsearch/logstash/blob/v%VERSION%/CHANGELOG"> view changelog </a> </li>
+    <li> <a href="contrib-plugins"> contrib plugins</a> </li>
+    <li> <a href="repositories"> package repositories</a> </li>
+    <li> <a href="configuration"> configuration file overview </a> </li>
+    <li> <a href="configuration#conditionals">conditionals</a> </li>
+    <li> <a href="configuration#fieldreferences">referring to fields [like][this]</a> </li>
+    <li> <a href="configuration#sprintf">using the %{fieldname} syntax</a> </li>
+
+    <li> <a href="life-of-an-event"> the life of an event in logstash </a> </li>
     <li> <a href="flags"> command-line flags </a> </li>
-    <li> <a href="configuration"> configure </a> </li>
-    <li> <a href="extending"> extending </a> </li>
   </ul>
 
-  <h3> plugin docs </h3>
+  <h3> For Developers </h3>
+  <ul>
+    <li> <a href="extending"> writing your own plugins </a> </li>
+  </ul>
+
+  <h3> Tutorials and Use Cases </h3>
+
+  <ul>
+    <li> <a href="tutorials/getting-started-with-logstash"> Getting started with Logstash </a> - New to Logstash? Start here! </li>
+    <li> <a href="tutorials/metrics-from-logs"> Metrics from logs </a> - take metrics from logs and ship them to graphite, ganglia, and more. </li>
+    <li> <a href="tutorials/just-enough-rabbitmq-for-logstash">Just enough RabbitMQ knowledge for Logstash </a> - Get a quick primer on RabbitMQ and how to use it in Logstash! </li>
+  </ul>
+
+  <h3> books and articles </h3>
+
+  <ul>
+    <li> <a href="http://www.logstashbook.com">The Logstash Book </a> - An introductory Logstash book. </li>
+  </ul>
+
+  <h3> plugin documentation </h3>
 <% docs.each do |type, paths| -%>
   <div class="doc_index_section">
     <h3><%= type %></h3>
diff --git a/docs/installation.md b/docs/installation.md
deleted file mode 100644
index fd2a0b6c464..00000000000
--- a/docs/installation.md
+++ /dev/null
@@ -1,49 +0,0 @@
----
-title: Installation options - logstash
-layout: content_right
----
-# LogStash Installation
-
-There are a few ways to install logstash:
-
-* standalone runnable jar file  (monolithic)
-* normal runnable jar (ruby deps + jruby included, no elasticsearch)
-* gem install logstash
-
-## 'standalone runnable jar'
-
-This jar is the normal runnable jar with elasticsearch libs included. To use
-it, use it the same way as documented below for 'normal runnable jar'
-
-## 'normal runnable jar'
-
-If you want to include elasticsearch, you'll need to download it and set the
-CLASSPATH environment variable to include any elasticsearch jar files.
-
-### web interface
-
-    java -jar logstash-0.9.1.jar web
-
-### agent 
-
-    java -jar logstash-0.9.1.jar agent -f youragent.conf
-
-## 'gem install logstash'
-
-Using this method to download logstash will install all ruby dependencies.
-
-* You must have jruby already
-
-### web interface
-
-* You have elasticsearch already
-* You'll need to know the path to your elasticsearch lib directory.
-
-    % CLASSPATH=elasticsearch-0.16.0/lib/*.jar logstash-web
-    >> Thin web server (v1.2.7 codename No Hup)
-    >> Maximum connections set to 1024
-    >> Listening on 0.0.0.0:9292, CTRL+C to stop
-
-### agent
-
-    % logstash -f youragent.conf
diff --git a/docs/learn.md b/docs/learn.md
index 7b502164584..e26bf320fa8 100644
--- a/docs/learn.md
+++ b/docs/learn.md
@@ -16,10 +16,15 @@ presentation I gave at CarolinaCon 2011:
 logstash, how you can use it, some alternatives, logging best practices,
 parsing tools, etc. Video also below:
 
+<!--
 <embed src="http://blip.tv/play/gvE9grjcdQI" type="application/x-shockwave-flash" width="480" height="296" allowscriptaccess="always" allowfullscreen="true"></embed>
 
 The slides are available online here: [slides](http://goo.gl/68c62). The slides
 include speaker notes (click 'actions' then 'speaker notes').
+-->
+<iframe width="480" height="296" src="http://www.youtube.com/embed/RuUFnog29M4" frameborder="0" allowfullscreen="allowfullscreen"></iframe>
+
+The slides are available online here: [slides](http://semicomplete.com/presentations/logstash-puppetconf-2012/).
 
 ## Getting Help
 
@@ -28,18 +33,14 @@ email the mailing list (logstash-users@googlegroups.com). Further, there is also
 an IRC channel - #logstash on irc.freenode.org.
 
 If you find a bug or have a feature request, file them
-on <http://logstash.jira.com/>. (Honestly though, if you prefer email or irc
+on [github](https://github.com/elasticsearch/logstas/issues). (Honestly though, if you prefer email or irc
 for such things, that works for me, too.)
 
-## Download it
-
-logstash releases come in a few flavors.
+## Download It
 
-* [Monolithic jar](http://semicomplete.com/files/logstash/logstash-1.0.2-monolithic.jar)
-* [rubygem](https://github.com/downloads/logstash/releases/logstash-1.0.2.gem)
-* [`gem install logstash`](http://rubygems.org/gems/logstash)
+[Download logstash-%VERSION%](https://download.elasticsearch.org/logstash/logstash/logstash-%VERSION%.tar.gz)
 
 ## What's next?
 
-Try the [standalone logstash guide](getting-started-simple) for a simple
+Try this [guide](tutorials/getting-started-with-logstash) for a simple
 real-world example getting started using logstash.
diff --git a/docs/life-of-an-event.md b/docs/life-of-an-event.md
new file mode 100644
index 00000000000..f7dd640995b
--- /dev/null
+++ b/docs/life-of-an-event.md
@@ -0,0 +1,109 @@
+---
+title: the life of an event - logstash
+layout: content_right
+---
+# the life of an event
+
+The logstash agent is an event pipeline.
+
+## The Pipeline
+
+The logstash agent is a processing pipeline with 3 stages: inputs -> filters ->
+outputs. Inputs generate events, filters modify them, outputs ship them
+elsewhere.
+
+Internal to logstash, events are passed from each phase using internal queues.
+It is implemented with a 'SizedQueue' in Ruby. SizedQueue allows a bounded
+maximum of items in the queue such that any writes to the queue will block if
+the queue is full at maximum capacity.
+
+Logstash sets each queue size to 20. This means only 20 events can be pending
+into the next phase - this helps reduce any data loss and in general avoids
+logstash trying to act as a data storage system. These internal queues are not
+for storing messages long-term.
+
+## Fault Tolerance
+
+Starting at outputs, here's what happens when things break.
+
+An output can fail or have problems because of some downstream cause, such as
+full disk, permissions problems, temporary network failures, or service
+outages. Most outputs should keep retrying to ship any events that were
+involved in the failure.
+
+If an output is failing, the output thread will wait until this output is
+healthy again and able to successfully send the message. Therefore, the output
+queue will stop being read from by this output and will eventually fill up with
+events and block new events from being written to this queue.
+
+A full output queue means filters will block trying to write to the output
+queue. Because filters will be stuck, blocked writing to the output queue, they
+will stop reading from the filter queue which will eventually cause the filter
+queue (input -> filter) to fill up.
+
+A full filter queue will cause inputs to block when writing to the filters.
+This will cause each input to block, causing each input to stop processing new
+data from wherever that input is getting new events.
+
+In ideal circumstances, this will behave similarly to when the tcp window
+closes to 0, no new data is sent because the receiver hasn't finished
+processing the current queue of data, but as soon as the downstream (output)
+problem is resolved, messages will begin flowing again..
+
+## Thread Model
+
+The thread model in logstash is currently:
+
+    input threads | filter worker threads | output worker
+
+Filters are optional, so you will have this model if you have no filters
+defined:
+
+    input threads | output worker
+
+Each input runs in a thread by itself. This allows busier inputs to not be
+blocked by slower ones, etc. It also allows for easier containment of scope
+because each input has a thread.
+
+The filter thread model is a 'worker' model where each worker receives an event
+and applies all filters, in order, before emitting that to the output queue.
+This allows scalability across CPUs because many filters are CPU intensive
+(permitting that we have thread safety). 
+
+The default number of filter workers is 1, but you can increase this number
+with the '-w' flag on the agent.
+
+The output worker model is currently a single thread. Outputs will receive
+events in the order they are defined in the config file. 
+
+Outputs may decide to buffer events temporarily before publishing them,
+possibly in a separate thread. One example of this is the elasticsearch output
+which will buffer events and flush them all at once, in a separate thread. This
+mechanism (buffering many events + writing in a separate thread) can improve
+performance so the logstash pipeline isn't stalled waiting for a response from
+elasticsearch.
+
+## Consequences and Expectations
+
+Small queue sizes mean that logstash simply blocks and stalls safely during
+times of load or other temporary pipeline problems. There are two alternatives
+to this - unlimited queue length and dropping messages. Unlimited queues grow
+grow unbounded and eventually exceed memory causing a crash which loses all of
+those messages. Dropping messages is also an undesirable behavior in most cases.
+
+At a minimum, logstash will have probably 3 threads (2 if you have no filters).
+One input, one filter worker, and one output thread each.
+
+If you see logstash using multiple CPUs, this is likely why. If you want to
+know more about what each thread is doing, you should read this:
+<http://www.semicomplete.com/blog/geekery/debugging-java-performance.html>.
+
+Threads in java have names, and you can use jstack and top to figure out who is
+using what resources. The URL above will help you learn how to do this.
+
+On Linux platforms, logstash will label all the threads it can with something
+descriptive. Inputs will show up as "<inputname" and filter workers as
+"|worker" and outputs as ">outputworker" (or something similar).  Other threads
+may be labeled as well, and are intended to help you identify their purpose
+should you wonder why they are consuming resources!
+
diff --git a/docs/logging-tool-comparisons.md b/docs/logging-tool-comparisons.md
new file mode 100644
index 00000000000..a39fea0546e
--- /dev/null
+++ b/docs/logging-tool-comparisons.md
@@ -0,0 +1,60 @@
+---
+title: Logging tools comparisons - logstash
+layout: content_right
+---
+# Logging tools comparison
+
+The information below is provided as "best effort" and is not strictly intended
+as a complete source of truth. If the information below is unclear or incorrect, please
+email the logstash-users list (or send a pull request with the fix) :)
+
+Where feasible, this document will also provide information on how you can use
+logstash with these other projects.
+
+# logstash
+
+Primary goal: Make log/event data and analytics accessible.
+
+Overview: Where your logs come from, how you store them, or what you do with
+them is up to you. Logstash exists to help make such actions easier and faster.
+
+It provides you a simple event pipeline for taking events and logs from any
+input, manipulating them with filters, and sending them to any output. Inputs
+can be files, network, message brokers, etc. Filters are date and string
+parsers, grep-like, etc. Outputs are data stores (elasticsearch, mongodb, etc),
+message systems (rabbitmq, stomp, etc), network (tcp, syslog), etc.
+
+It also provides a web interface for doing search and analytics on your
+logs.
+
+# graylog2
+
+[http://graylog2.org/](http://graylog2.org)
+
+_Overview to be written_
+
+You can use graylog2 with logstash by using the 'gelf' output to send logstash
+events to a graylog2 server. This gives you logstash's excellent input and
+filter features while still being able to use the graylog2 web interface.
+
+# whoops
+
+[whoops site](http://www.whoopsapp.com/)
+
+_Overview to be written_
+
+A logstash output to whoops is coming soon - <https://logstash.jira.com/browse/LOGSTASH-133>
+
+# flume
+
+[flume site](https://github.com/cloudera/flume/wiki)
+
+Flume is primarily a transport system aimed at reliably copying logs from
+application servers to HDFS.
+
+You can use it with logstash by having a syslog sink configured to shoot logs
+at a logstash syslog input.
+
+# scribe
+
+_Overview to be written_
diff --git a/docs/plugin-doc.asciidoc.erb b/docs/plugin-doc.asciidoc.erb
new file mode 100644
index 00000000000..3860efbf503
--- /dev/null
+++ b/docs/plugin-doc.asciidoc.erb
@@ -0,0 +1,52 @@
+<%- plugin_name = name -%>
+[[plugins-<%= section %>s-<%= name %>]]
+=== <%= name %>
+
+
+<%= description %>
+
+==== Synopsis
+
+These are the config options
+
+<%= synopsis -%>
+
+==== Details
+
+<% sorted_attributes.each do |name, config| -%>
+<%
+     if name.is_a?(Regexp)
+       name = "/" + name.to_s.gsub(/^\(\?-mix:/, "").gsub(/\)$/, "") + "/"
+       is_regexp = true
+     else
+       is_regexp = false
+     end
+-%>
+[[plugins-<%= section%>s-<%= plugin_name%>-<%= name%>]]
+===== `<%= name %>` <%= " (DEPRECATED)" if config[:deprecated] %>
+
+<% if config[:required] -%>
+  * This is a required setting.
+<% end -%>
+<% if config[:deprecated] -%>
+  * DEPRECATED WARNING: This config item is deprecated. It may be removed in a further version.
+<% end -%>
+<% if is_regexp -%>
+  * The configuration attribute name here is anything that matches the above regular expression.
+<% end -%>
+<% if config[:validate].is_a?(Symbol) -%>
+  * Value type is <<<%= config[:validate] %>,<%= config[:validate] %>>>
+<% elsif config[:validate].nil? -%>
+  <li> Value type is <<string,string>>
+<% elsif config[:validate].is_a?(Array) -%>
+  * Value can be any of: `<%= config[:validate].join('`, `') %>`
+<% end -%>
+<% if config.include?(:default) -%>
+  * Default value is `<%= config[:default].inspect %>`
+<% else -%>
+  * There is no default value for this setting.
+<% end -%>
+
+<%= config[:description] %>
+
+<% end -%>
diff --git a/docs/plugin-doc.html.erb b/docs/plugin-doc.html.erb
new file mode 100644
index 00000000000..5b9733afa52
--- /dev/null
+++ b/docs/plugin-doc.html.erb
@@ -0,0 +1,84 @@
+---
+title: logstash docs for <%= section %>s/<%= name %>
+layout: content_right
+---
+<h2><%= name %></h2>
+<h3>Milestone: <a href="../plugin-milestones"><%= @milestone %></a></h3>
+<% if is_contrib_plugin -%>
+<div class="community-plugin-notice">
+  <strong>This is a community-contributed plugin!</strong> It does not ship with logstash by default, but it is easy to install!
+  To use this, you must have <a href="../contrib-plugins">installed the contrib plugins package</a>.
+</div>
+<% end -%>
+
+<%= description %>
+
+<h3> Synopsis </h3>
+
+This is what it might look like in your config file:
+
+<pre><code><% if section == "codec" -%>
+# with an input plugin:
+# you can also use this codec with an output.
+input { 
+<% if name == "json_lines" -%>
+  udp {
+    port =&gt; 1234
+<% else -%>
+  file {
+<% end -%>
+    codec =&gt; <%= synopsis -%>
+  }
+}
+<% else -%>
+<%= section %> {
+  <%= synopsis -%>
+}
+<% end -%></code></pre>
+
+<h3> Details </h3>
+
+<% sorted_attributes.each do |name, config| -%>
+<%
+     if name.is_a?(Regexp)
+       name = "/" + name.to_s.gsub(/^\(\?-mix:/, "").gsub(/\)$/, "") + "/"
+       is_regexp = true
+     else
+       is_regexp = false
+     end
+-%>
+<h4> 
+  <a name="<%= name %>">
+    <%= name %><%= " (required setting)" if config[:required] %>
+    <%= " <strong>DEPRECATED</strong>" if config[:deprecated] %>
+</a>
+</h4>
+
+<ul>
+<% if config[:deprecated] -%>
+  <li> DEPRECATED WARNING: This config item is deprecated. It may be removed in a further version. </li>
+<% end -%>
+<% if is_regexp -%>
+  <li> The configuration attribute name here is anything that matches the above regular expression. </li>
+<% end -%>
+<% if config[:validate].is_a?(Symbol) -%>
+  <li> Value type is <a href="../configuration#<%= config[:validate] %>"><%= config[:validate] %></a> </li>
+<% elsif config[:validate].nil? -%>
+  <li> Value type is <a href="../configuration#string">string</a> </li>
+<% elsif config[:validate].is_a?(Array) -%>
+  <li> Value can be any of: <%= config[:validate].map(&:inspect).join(", ") %> </li>
+<% end -%>
+<% if config.include?(:default) -%>
+  <li> Default value is <%= config[:default].inspect %> </li>
+<% else -%>
+  <li> There is no default value for this setting. </li>
+<% end -%>
+</ul>
+
+<%= config[:description] %>
+
+<% end -%>
+
+<hr>
+
+This is documentation from <a href="https://github.com/logstash/logstash/blob/v<%= LOGSTASH_VERSION %>/<%= file %>"><%= file %></a>
diff --git a/docs/plugin-milestones.md b/docs/plugin-milestones.md
new file mode 100644
index 00000000000..5d72e9ac472
--- /dev/null
+++ b/docs/plugin-milestones.md
@@ -0,0 +1,41 @@
+---
+title: Plugin Milestones - logstash
+layout: content_right
+---
+# Plugin Milestones
+
+Plugins (inputs/outputs/filters/codecs) have a milestone label in logstash.
+This is to provide an indicator to the end-user as to the kinds of changes
+a given plugin could have between logstash releases.
+
+The desire here is to allow plugin developers to quickly iterate on possible
+new plugins while conveying to the end-user a set of expectations about that
+plugin.
+
+## Milestone 1
+
+Plugins at this milestone need your feedback to improve! Plugins at this
+milestone may change between releases as the community figures out the best way
+for the plugin to behave and be configured.
+
+## Milestone 2
+
+Plugins at this milestone are more likely to have backwards-compatibility to
+previous releases than do Milestone 1 plugins. This milestone also indicates
+a greater level of in-the-wild usage by the community than the previous
+milestone.
+
+## Milestone 3
+
+Plugins at this milestone have strong promises towards backwards-compatibility.
+This is enforced with automated tests to ensure behavior and configuration are
+consistent across releases.
+
+## Milestone 0
+
+This milestone appears at the bottom of the page because it is very
+infrequently used.
+
+This milestone marker is used to generally indicate that a plugin has no
+active code maintainer nor does it have support from the community in terms
+of getting help.
diff --git a/docs/plugin-synopsis.asciidoc.erb b/docs/plugin-synopsis.asciidoc.erb
new file mode 100644
index 00000000000..7e1efd89277
--- /dev/null
+++ b/docs/plugin-synopsis.asciidoc.erb
@@ -0,0 +1,48 @@
+<%- plugin_name = name -%>
+[source,json]
+--------------------------
+<%= name %> {
+<% sorted_attributes.each do |name, config|
+   next if config[:deprecated]
+   next if !config[:required]
+-%>
+<%= "  " if section == "codec" %>    <%= name %> => ... 
+<% end -%>
+<%= "  " if section == "codec" %>  }
+
+--------------------------
+
+[cols="<,<,<,<m",options="header",]
+|=======================================================================
+|Setting |Input type|Required/optional|Default value
+<% sorted_attributes.each do |name, config|
+   next if config[:deprecated]
+   if config[:validate].is_a?(Array) 
+     annotation = "|string, one of #{config[:validate].inspect}"
+   elsif config[:validate] == :path
+     annotation = "|a valid filesystem path"
+   else
+     annotation = "|<<#{config[:validate]},#{config[:validate]}>>"
+   end
+
+   if name.is_a?(Regexp)
+     name = "/" + name.to_s.gsub(/^\(\?-mix:/, "").gsub(/\)$/, "") + "/"
+   end
+   if config[:required]
+     annotation += "|required"
+   else
+     annotation += "|optional"
+   end
+   if config.include?(:default)
+     if config[:default].is_a?(String)
+       annotation += "|`#{config[:default]}`"
+     else
+       annotation += "|#{config[:default].inspect}"
+     end
+   else 
+     annotation += "|"
+   end
+-%>
+| <<plugins-<%= section %>s-<%=plugin_name%>-<%= name %>>> <%= annotation %>
+<% end -%>
+|=======================================================================
diff --git a/docs/plugin-synopsis.html.erb b/docs/plugin-synopsis.html.erb
new file mode 100644
index 00000000000..92465227fa8
--- /dev/null
+++ b/docs/plugin-synopsis.html.erb
@@ -0,0 +1,24 @@
+<%= name %> {
+<% sorted_attributes.each do |name, config|
+   next if config[:deprecated]
+   if config[:validate].is_a?(Array) 
+     annotation = "string, one of #{config[:validate].inspect}"
+   elsif config[:validate] == :path
+     annotation = "a valid filesystem path"
+   else 
+     annotation = "#{config[:validate]}"
+   end
+
+   if name.is_a?(Regexp)
+     name = "/" + name.to_s.gsub(/^\(\?-mix:/, "").gsub(/\)$/, "") + "/"
+   end
+   if config[:required]
+     annotation += " (required)"
+   else
+     annotation += " (optional)"
+   end
+   annotation += ", default: #{config[:default].inspect}" if config.include?(:default)
+-%>
+<%= "  " if section == "codec" %>    <a href="#<%= name %>"><%= name %></a> => ... # <%= annotation %>
+<% end -%>
+<%= "  " if section == "codec" %>  }
diff --git a/docs/release-notes.md b/docs/release-notes.md
new file mode 100644
index 00000000000..e8b3324dd7d
--- /dev/null
+++ b/docs/release-notes.md
@@ -0,0 +1,70 @@
+---
+title: release notes for %VERSION%
+layout: content_right
+---
+
+# %VERSION% - Release Notes
+
+This document is targeted at existing users of Logstash who are upgrading from
+an older version to version %VERSION%. This document is intended to supplement
+a the [changelog
+file](https://github.com/elasticsearch/logstash/blob/v%VERSION%/CHANGELOG) by
+providing more details on certain changes.
+
+### tarball 
+
+With Logstash 1.4.0, we stopped shipping the jar file and started shipping a
+tarball instead.
+
+Past releases have been a single jar file which included all Ruby and Java
+library dependencies to eliminate deployment pains. We still ship all
+the dependencies for you! The jar file served us well, but over time we found
+Java‚Äôs default heap size, garbage collector, and other settings weren‚Äôt well
+suited to Logstash.
+
+In order to provide better Java defaults, we‚Äôve changed to releasing a tarball
+(.tar.gz) that includes all the same dependencies. What does this mean to you?
+Instead of running `java -jar logstash.jar ...` you run `bin/logstash ...` (for
+Windows users, `bin/logstash.bat`)
+
+One pleasant side effect of using a tarball is that the Logstash code itself is
+much more accessible and able to satisfy any curiosity you may have.
+
+The new way to do things is:
+
+* Download logstash tarball
+* Unpack it (`tar -zxf logstash-%VERSION%.tar.gz`)
+* `cd logstash-%VERSION%`
+% Run it: `bin/logstash ...`
+
+The old way to run logstash of `java -jar logstash.jar` is now replaced with
+`bin/logstash`. The command line arguments are exactly the same after that.
+For example:
+
+    # Old way:
+    % java -jar logstash-1.3.3-flatjar.jar agent -f logstash.conf
+
+    # New way:
+    % bin/logstash agent -f logstash.conf
+
+### contrib plugins
+
+Logstash has grown brilliantly over the past few years with great contributions
+from the community. Now having 165 plugins, it became hard for us (the Logstash
+engineering team) to reliably support all the wonderful technologies in each
+contributed plugin. We combed through all the plugins and picked the ones we
+felt strongly we could support, and those now ship by default with Logstash.
+
+All the other plugins are now available in a contrib package. All plugins
+continue to be open source and free, of course! Installing plugins from the
+contrib package is very easy:
+
+    % cd /path/to/logstash-%VERSION%/
+    % bin/plugin install contrib
+
+A bonus effect of this decision is that the default Logstash download size
+shrank by 19MB compared to the previous release because we were able to shed
+some lesser-used dependencies.
+
+You can learn more about contrib plugins on the [contrib plugins
+page](http://logstash.net/docs/%VERSION%/contrib-plugins)
diff --git a/docs/repositories.md b/docs/repositories.md
new file mode 100644
index 00000000000..7eb163bde9b
--- /dev/null
+++ b/docs/repositories.md
@@ -0,0 +1,35 @@
+---
+title: repositories - logstash
+layout: content_right
+---
+# Logstash repositories
+
+We also have Logstash available as APT and YUM repositories.
+
+Our public signing key can be found on the [Elasticsearch packages apt GPG signing key page](http://packages.elasticsearch.org/GPG-KEY-elasticsearch)
+
+## Apt based distributions
+
+Add the key:
+
+     wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | apt-key add -
+
+Add the repo to /etc/apt/sources.list
+
+     deb http://packages.elasticsearch.org/logstash/1.4/debian stable main
+
+
+## YUM based distributions
+
+Add the key:
+
+     rpm --import http://packages.elasticsearch.org/GPG-KEY-elasticsearch
+
+Add the repo to /etc/yum.repos.d/ directory
+
+     [logstash-1.4]
+     name=logstash repository for 1.4.x packages
+     baseurl=http://packages.elasticsearch.org/logstash/1.4/centos
+     gpgcheck=1
+     gpgkey=http://packages.elasticsearch.org/GPG-KEY-elasticsearch
+     enabled=1
diff --git a/docs/tutorials/10-minute-walkthrough/apache-elasticsearch.conf b/docs/tutorials/10-minute-walkthrough/apache-elasticsearch.conf
new file mode 100644
index 00000000000..854fb1d9d27
--- /dev/null
+++ b/docs/tutorials/10-minute-walkthrough/apache-elasticsearch.conf
@@ -0,0 +1,35 @@
+input {
+  tcp { 
+    type => "apache"
+    port => 3333
+  } 
+}
+
+filter {
+  if [type] == "apache" {
+    grok {
+      # See the following URL for a complete list of named patterns
+      # logstash/grok ships with by default:
+      # https://github.com/logstash/logstash/tree/master/patterns
+      #
+      # The grok filter will use the below pattern and on successful match use
+      # any captured values as new fields in the event.
+      match => { "message" => "%{COMBINEDAPACHELOG}" }
+    }
+
+    date {
+      # Try to pull the timestamp from the 'timestamp' field (parsed above with
+      # grok). The apache time format looks like: "18/Aug/2011:05:44:34 -0700"
+      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
+    }
+  }
+}
+
+output {
+  elasticsearch {
+    # Setting 'embedded' will run  a real elasticsearch server inside logstash.
+    # This option below saves you from having to run a separate process just
+    # for ElasticSearch, so you can get started quicker!
+    embedded => true
+  }
+}
diff --git a/docs/tutorials/10-minute-walkthrough/apache-parse.conf b/docs/tutorials/10-minute-walkthrough/apache-parse.conf
new file mode 100644
index 00000000000..17adeaf06a0
--- /dev/null
+++ b/docs/tutorials/10-minute-walkthrough/apache-parse.conf
@@ -0,0 +1,33 @@
+input {
+  tcp { 
+    type => "apache"
+    port => 3333
+  } 
+}
+
+filter {
+  if [type] == "apache" {
+    grok {
+      # See the following URL for a complete list of named patterns
+      # logstash/grok ships with by default:
+      # https://github.com/logstash/logstash/tree/master/patterns
+      #
+      # The grok filter will use the below pattern and on successful match use
+      # any captured values as new fields in the event.
+      match => { "message" => "%{COMBINEDAPACHELOG}" }
+    }
+
+    date {
+      # Try to pull the timestamp from the 'timestamp' field (parsed above with
+      # grok). The apache time format looks like: "18/Aug/2011:05:44:34 -0700"
+      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
+    }
+  }
+}
+
+output {
+  # Use stdout in debug mode again to see what logstash makes of the event.
+  stdout {
+    debug => true
+  }
+}
diff --git a/docs/tutorials/10-minute-walkthrough/apache_log.1 b/docs/tutorials/10-minute-walkthrough/apache_log.1
new file mode 100644
index 00000000000..f7911a7eb0a
--- /dev/null
+++ b/docs/tutorials/10-minute-walkthrough/apache_log.1
@@ -0,0 +1 @@
+129.92.249.70 - - [18/Aug/2011:06:00:14 -0700] "GET /style2.css HTTP/1.1" 200 1820 "http://www.semicomplete.com/blog/geekery/bypassing-captive-portals.html" "Mozilla/5.0 (iPad; U; CPU OS 4_3_5 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8L1 Safari/6533.18.5"
diff --git a/docs/tutorials/10-minute-walkthrough/apache_log.2.bz2 b/docs/tutorials/10-minute-walkthrough/apache_log.2.bz2
new file mode 100644
index 00000000000..841e7b6b1f0
Binary files /dev/null and b/docs/tutorials/10-minute-walkthrough/apache_log.2.bz2 differ
diff --git a/docs/tutorials/10-minute-walkthrough/hello-search.conf b/docs/tutorials/10-minute-walkthrough/hello-search.conf
new file mode 100644
index 00000000000..c99f014658a
--- /dev/null
+++ b/docs/tutorials/10-minute-walkthrough/hello-search.conf
@@ -0,0 +1,25 @@
+input {
+  stdin { 
+    # A type is a label applied to an event. It is used later with filters
+    # to restrict what filters are run against each event.
+    type => "human"
+  } 
+}
+
+output {
+  # Print each event to stdout.
+  stdout {
+    # Enabling 'rubydebug' codec on the stdout output will make logstash
+    # pretty-print the entire event as something similar to a JSON representation.
+    codec => rubydebug
+  }
+  
+  # You can have multiple outputs. All events generally to all outputs.
+  # Output events to elasticsearch
+  elasticsearch {
+    # Setting 'embedded' will run  a real elasticsearch server inside logstash.
+    # This option below saves you from having to run a separate process just
+    # for ElasticSearch, so you can get started quicker!
+    embedded => true
+  }
+}
diff --git a/docs/tutorials/10-minute-walkthrough/hello.conf b/docs/tutorials/10-minute-walkthrough/hello.conf
new file mode 100644
index 00000000000..3d80679931d
--- /dev/null
+++ b/docs/tutorials/10-minute-walkthrough/hello.conf
@@ -0,0 +1,16 @@
+input {
+  stdin { 
+    # A type is a label applied to an event. It is used later with filters
+    # to restrict what filters are run against each event.
+    type => "human"
+  } 
+}
+
+output {
+  # Print each event to stdout.
+  stdout {
+    # Enabling 'rubydebug' codec on the stdout output will make logstash
+    # pretty-print the entire event as something similar to a JSON representation.
+    codec => rubydebug
+  }
+}
diff --git a/docs/tutorials/10-minute-walkthrough/index.md b/docs/tutorials/10-minute-walkthrough/index.md
new file mode 100644
index 00000000000..b9af9d36cae
--- /dev/null
+++ b/docs/tutorials/10-minute-walkthrough/index.md
@@ -0,0 +1,131 @@
+---
+title: Logstash 10-Minute Tutorial
+layout: content_right
+---
+# Logstash 10-minute Tutorial
+
+## Step 1 - Download
+
+### Download logstash:
+
+* [logstash-%VERSION%.tar.gz](https://download.elasticsearch.org/logstash/logstash/logstash-%VERSION%.tar.gz)
+
+    curl -O https://download.elasticsearch.org/logstash/logstash/logstash-%VERSION%.tar.gz
+
+### Unpack it
+
+    tar -xzf logstash-%VERSION%.tar.gz
+    cd logstash-%VERSION%
+
+### Requirements:
+
+* Java
+
+### The Secret:
+
+Logstash is written in JRuby, but I release standalone jar files for easy
+deployment, so you don't need to download JRuby or most any other dependencies.
+
+I bake as much as possible into the single release file.
+
+## Step 2 - A hello world.
+
+### Download this config file:
+
+* [hello.conf](hello.conf)
+
+### Run it:
+
+    bin/logstash agent -f hello.conf
+
+Type stuff on standard input. Press enter. Watch what event Logstash sees.
+Press ^C to kill it.
+
+## Step 3 - Add ElasticSearch
+
+### Download this config file:
+
+* [hello-search.conf](hello-search.conf)
+
+### Run it:
+
+    bin/logstash agent -f hello-search.conf
+
+Same config as step 2, but now we are also writing events to ElasticSearch. Do
+a search for `*` (all):
+
+    curl 'http://localhost:9200/_search?pretty=1&q=*'
+
+## Step 4 - Logstash web
+
+The previous step is good, but a better frontend on elasticsearch would help!
+
+The same config as step 3 is used.
+
+### Run it:
+
+    bin/logstash agent -f hello-search.conf web
+
+The above runs both the agent and the Logstash web interface in the same
+process. Useful for simple deploys.
+
+### Use it:
+
+Go to the Logstash web interface in browser: <http://localhost:9292/>
+
+Type stuff on `STDIN` on the agent, then search for it in the web interface.
+
+## Step 5 - real world example
+
+Let's backfill some old Apache logs.  First, let's use grok.
+
+Use the ['grok'](../../filters/grok) Logstash filter to parse logs. 
+
+### Download
+
+* [apache-parse.conf](apache-parse.conf)
+* [apache_log.1](apache_log.1) (a single apache log line)
+
+### Run it
+
+    bin/logstash agent -f apache-parse.conf
+
+Logstash will now be listening on TCP port 3333. Send an Apache log message at it:
+
+    nc localhost 3333 < apache_log.1
+
+The expected output can be viewed here: [step-5-output.txt](step-5-output.txt)
+
+## Step 6 - real world example + search
+
+Same as the previous step, but we'll output to ElasticSearch now.
+
+### Download
+
+* [apache-elasticsearch.conf](apache-elasticsearch.conf)
+* [apache_log.2.bz2](apache_log.2.bz2) (2 days of apache logs)
+
+### Run it
+
+    bin/logstash agent -f apache-elasticsearch.conf web
+
+Logstash should be all set for you now. Start feeding it logs:
+
+    bzip2 -d apache_log.2.bz2
+
+    nc localhost 3333 < apache_log.2 
+
+Go to the Logstash web interface in browser: <http://localhost:9292/>
+
+Try some search queries. To see all the data, search for `*` (no quotes). Click
+on some results, drill around in some logs.
+
+## Want more?
+
+For further learning, try these:
+
+* [Watch a presentation on logstash](http://www.youtube.com/embed/RuUFnog29M4)
+* [Getting started 'standalone' guide](http://logstash.net/docs/%VERSION%/tutorials/getting-started-simple)
+* [Getting started 'centralized' guide](http://logstash.net/docs/%VERSION%/tutorials/getting-started-centralized) - 
+  learn how to build out your logstash infrastructure and centralize your logs.
+* [Dive into the docs](http://logstash.net/docs/%VERSION%/)
diff --git a/docs/tutorials/10-minute-walkthrough/step-5-output.txt b/docs/tutorials/10-minute-walkthrough/step-5-output.txt
new file mode 100644
index 00000000000..ec87c0cebda
--- /dev/null
+++ b/docs/tutorials/10-minute-walkthrough/step-5-output.txt
@@ -0,0 +1,17 @@
+{
+  "type"        => "apache",
+  "clientip"          => "129.92.249.70",
+  "ident"             => "-",
+  "auth"              => "-",
+  "timestamp"         => "18/Aug/2011:06:00:14 -0700",
+  "verb"              => "GET",
+  "request"           => "/style2.css",
+  "httpversion"       => "1.1",
+  "response"          => "200",
+  "bytes"             => "1820",
+  "referrer"          => "http://www.semicomplete.com/blog/geekery/bypassing-captive-portals.html",
+  "agent"             => "\"Mozilla/5.0 (iPad; U; CPU OS 4_3_5 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8L1 Safari/6533.18.5\"",
+  "@timestamp"   => "2011-08-18T13:00:14.000Z",
+  "host" => "127.0.0.1",
+  "message"     => "129.92.249.70 - - [18/Aug/2011:06:00:14 -0700] \"GET /style2.css HTTP/1.1\" 200 1820 \"http://www.semicomplete.com/blog/geekery/bypassing-captive-portals.html\" \"Mozilla/5.0 (iPad; U; CPU OS 4_3_5 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8L1 Safari/6533.18.5\"\n"
+}
diff --git a/docs/tutorials/getting-started-with-logstash.asciidoc b/docs/tutorials/getting-started-with-logstash.asciidoc
new file mode 100644
index 00000000000..43376152fef
--- /dev/null
+++ b/docs/tutorials/getting-started-with-logstash.asciidoc
@@ -0,0 +1,426 @@
+= Getting Started with Logstash
+
+== Introduction
+Logstash is a tool for receiving, processing and outputting logs. All kinds of logs. System logs, webserver logs, error logs, application logs and just about anything you can throw at it. Sounds great, eh?
+
+Using Elasticsearch as a backend datastore, and kibana as a frontend reporting tool, Logstash acts as the workhorse, creating a powerful pipeline for storing, querying and analyzing your logs. With an arsenal of built-in inputs, filters, codecs and outputs, you can harness some powerful functionality with a small amount of effort. So, let's get started!
+
+=== Prerequisite: Java
+The only prerequisite required by Logstash is a Java runtime. You can check that you have it installed by running the  command `java -version` in your shell. Here's something similar to what you might see:
+----
+> java -version
+java version "1.7.0_45"
+Java(TM) SE Runtime Environment (build 1.7.0_45-b18)
+Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode)
+----
+It is recommended to run a recent version of Java in order to ensure the greatest success in running Logstash.
+
+It's fine to run an open-source version such as OpenJDK: +
+http://openjdk.java.net/
+
+Or you can use the official Oracle version: +
+http://www.oracle.com/technetwork/java/index.html
+
+Once you have verified the existence of Java on your system, we can move on!
+
+== Up and Running!
+
+=== Logstash in two commands
+First, we're going to download the 'logstash' binary and run it with a very simple configuration.
+----
+curl -O https://download.elasticsearch.org/logstash/logstash/logstash-%VERSION%.tar.gz
+----
+Now you should have the file named 'logstash-%VERSION%.tar.gz' on your local filesystem. Let's unpack it:
+----
+tar zxvf logstash-%VERSION%.tar.gz
+cd logstash-%VERSION%
+----
+Now let's run it:
+----
+bin/logstash -e 'input { stdin { } } output { stdout {} }'
+----
+
+Now type something into your command prompt, and you will see it output by Logstash:
+----
+hello world
+2013-11-21T01:22:14.405+0000 0.0.0.0 hello world
+----
+
+OK, that's interesting... We ran Logstash with an input called "stdin", and an output named "stdout", and Logstash basically echoed back whatever we typed in some sort of structured format. Note that specifying the *-e* command line flag allows Logstash to accept a configuration directly from the command line. This is especially useful for quickly testing configurations without having to edit a file between iterations.
+
+Let's try a slightly fancier example. First, you should exit Logstash by issuing a 'CTRL-D' command (or 'CTRL-C Enter') in the shell in which it is running. Now run Logstash again with the following command:
+----
+bin/logstash -e 'input { stdin { } } output { stdout { codec => rubydebug } }'
+----
+
+And then try another test input, typing the text "goodnight moon":
+----
+goodnight moon
+{
+  "message" => "goodnight moon",
+  "@timestamp" => "2013-11-20T23:48:05.335Z",
+  "@version" => "1",
+  "host" => "my-laptop"
+}
+----
+
+So, by re-configuring the "stdout" output (adding a "codec"), we can change the output of Logstash. By adding inputs, outputs and filters to your configuration, it's possible to massage the log data in many ways, in order to maximize flexibility of the stored data when you are querying it.
+
+== Storing logs with Elasticsearch
+Now, you're probably saying, "that's all fine and dandy, but typing all my logs into Logstash isn't really an option, and merely seeing them spit to STDOUT isn't very useful." Good point. First, let's set up Elasticsearch to store the messages we send into Logstash. If you don't have Elasticsearch already installed, you can http://www.elasticsearch.org/download/[download the RPM or DEB package], or install manually by downloading the current release tarball, by issuing the following four commands:
+----
+curl -O https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-%ELASTICSEARCH_VERSION%.tar.gz
+tar zxvf elasticsearch-%ELASTICSEARCH_VERSION%.tar.gz
+cd elasticsearch-%ELASTICSEARCH_VERSION%/
+./bin/elasticsearch
+----
+
+NOTE: This tutorial specifies running Logstash %VERSION% with Elasticsearch %ELASTICSEARCH_VERSION%. Each release of Logstash has a *recommended* version of Elasticsearch to pair with. Make sure the versions match based on the http://logstash.net/docs/latest[Logstash version] you're running!
+
+More detailed information on installing and configuring Elasticsearch can be found on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index.html[The Elasticsearch reference pages]. However, for the purposes of Getting Started with Logstash, the default installation and configuration of Elasticsearch should be sufficient.
+
+Now that we have Elasticsearch running on port 9200 (we do, right?), Logstash can be simply configured to use Elasticsearch as its backend. The defaults for both Logstash and Elasticsearch are fairly sane and well thought out, so we can omit the optional configurations within the elasticsearch output:
+----
+bin/logstash -e 'input { stdin { } } output { elasticsearch { host => localhost } }'
+----
+
+Type something, and Logstash will process it as before (this time you won't see any output, since we don't have the stdout output configured)
+----
+you know, for logs
+----
+
+You can confirm that ES actually received the data by making a curl request and inspecting the return:
+----
+curl 'http://localhost:9200/_search?pretty'
+----
+
+which should return something like this:
+----
+{
+  "took" : 2,
+  "timed_out" : false,
+  "_shards" : {
+    "total" : 5,
+    "successful" : 5,
+    "failed" : 0
+  },
+  "hits" : {
+    "total" : 1,
+    "max_score" : 1.0,
+    "hits" : [ {
+      "_index" : "logstash-2013.11.21",
+      "_type" : "logs",
+      "_id" : "2ijaoKqARqGvbMgP3BspJA",
+      "_score" : 1.0, "_source" : {"message":"you know, for logs","@timestamp":"2013-11-21T18:45:09.862Z","@version":"1","host":"my-laptop"}
+    } ]
+  }
+}
+----
+
+Congratulations! You've successfully stashed logs in Elasticsearch via Logstash.
+
+=== Elasticsearch Plugins (an aside)
+Another very useful tool for querying your Logstash data (and Elasticsearch in general) is the Elasticsearch-kopf plugin. Here is more information on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html[Elasticsearch plugins]. To install elasticsearch-kopf, simply issue the following command in your Elasticsearch directory (the same one in which you ran Elasticsearch earlier):
+----
+bin/plugin -install lmenezes/elasticsearch-kopf
+----
+Now you can browse to http://localhost:9200/_plugin/kopf[http://localhost:9200/_plugin/kopf] to browse your Elasticsearch data, settings and mappings!
+
+=== Multiple Outputs
+As a quick exercise in configuring multiple Logstash outputs, let's invoke Logstash again, using both the 'stdout' as well as the 'elasticsearch' output:
+----
+bin/logstash -e 'input { stdin { } } output { elasticsearch { host => localhost } stdout { } }'
+----
+Typing a phrase will now echo back to your terminal, as well as save in Elasticsearch! (Feel free to verify this using curl or elasticsearch-kopf).
+
+=== Default - Daily Indices
+You might notice that Logstash was smart enough to create a new index in Elasticsearch... The default index name is in the form of 'logstash-YYYY.MM.DD', which essentially creates one index per day. At midnight (GMT?), Logstash will automagically rotate the index to a fresh new one, with the new current day's timestamp. This allows you to keep windows of data, based on how far retroactively you'd like to query your log data. Of course, you can always archive (or re-index) your data to an alternate location, where you are able to query further into the past. If you'd like to simply delete old indices after a certain time period, you can use the https://github.com/elasticsearch/curator[Elasticsearch Curator tool].
+
+== Moving On
+Now you're ready for more advanced configurations. At this point, it makes sense for a quick discussion of some of the core features of Logstash, and how they interact with the Logstash engine.
+
+=== The Life of an Event
+
+Inputs, Outputs, Codecs and Filters are at the heart of the Logstash configuration. By creating a pipeline of event processing, Logstash is able to extract the relevant data from your logs and make it available to elasticsearch, in order to efficiently query your data. To get you thinking about the various options available in Logstash, let's discuss some of the more common configurations currently in use. For more details, read about http://logstash.net/docs/latest/life-of-an-event[the Logstash event pipeline].
+
+==== Inputs
+Inputs are the mechanism for passing log data to Logstash. Some of the more useful, commonly-used ones are:
+
+* *file*: reads from a file on the filesystem, much like the UNIX command "tail -0F"
+* *syslog*: listens on the well-known port 514 for syslog messages and parses according to RFC3164 format
+* *redis*: reads from a redis server, using both redis channels and also redis lists. Redis is often used as a "broker" in a centralized Logstash installation, which queues Logstash events from remote Logstash "shippers".
+* *lumberjack*: processes events sent in the lumberjack protocol. Now called https://github.com/elasticsearch/logstash-forwarder[logstash-forwarder].
+
+==== Filters
+Filters are used as intermediary processing devices in the Logstash chain. They are often combined with conditionals in order to perform a certain action on an event, if it matches particular criteria. Some useful filters:
+
+* *grok*: parses arbitrary text and structure it. Grok is currently the best way in Logstash to parse unstructured log data into something structured and queryable. With 120 patterns shipped built-in to Logstash, it's more than likely you'll find one that meets your needs!
+* *mutate*: The mutate filter allows you to do general mutations to fields. You can rename, remove, replace, and modify fields in your events.
+* *drop*: drop an event completely, for example, 'debug' events.
+* *clone*: make a copy of an event, possibly adding or removing fields.
+* *geoip*: adds information about geographical location of IP addresses (and displays amazing charts in kibana)
+
+==== Outputs
+Outputs are the final phase of the Logstash pipeline. An event may pass through multiple outputs during processing, but once all outputs are complete, the event has finished its execution. Some commonly used outputs include:
+
+* *elasticsearch*: If you're planning to save your data in an efficient, convenient and easily queryable format... Elasticsearch is the way to go. Period. Yes, we're biased :)
+* *file*: writes event data to a file on disk.
+* *graphite*: sends event data to graphite, a popular open source tool for storing and graphing metrics. http://graphite.wikidot.com/
+* *statsd*: a service which "listens for statistics, like counters and timers, sent over UDP and sends aggregates to one or more pluggable backend services". If you're already using statsd, this could be useful for you!
+
+==== Codecs
+Codecs are basically stream filters which can operate as part of an input, or an output. Codecs allow you to easily separate the transport of your messages from the serialization process. Popular codecs include 'json', 'msgpack' and 'plain' (text).
+
+* *json*: encode / decode data in JSON format
+* *multiline*: Takes multiple-line text events and merge them into a single event, e.g. java exception and stacktrace messages
+
+For the complete list of (current) configurations, visit the Logstash "plugin configuration" section of the http://logstash.net/docs/latest/[Logstash documentation page].
+
+
+== More fun with Logstash
+=== Persistent Configuration files
+
+Specifying configurations on the command line using '-e' is only so helpful, and more advanced setups will require more lengthy, long-lived configurations. First, let's create a simple configuration file, and invoke Logstash using it. Create a file named "logstash-simple.conf" and save it in the same directory as Logstash.
+
+----
+input { stdin { } }
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----
+
+Then, run this command:
+
+----
+bin/logstash -f logstash-simple.conf
+----
+
+Et voil√†! Logstash will read in the configuration file you just created and run as in the example we saw earlier. Note that we used the '-f' to read in the file, rather than the '-e' to read the configuration from the command line. This is a very simple case, of course, so let's move on to some more complex examples.
+
+=== Filters
+Filters are an in-line processing mechanism which provide the flexibility to slice and dice your data to fit your needs. Let's see one in action, namely the *grok filter*.
+
+----
+input { stdin { } }
+
+filter {
+  grok {
+    match => { "message" => "%{COMBINEDAPACHELOG}" }
+  }
+  date {
+    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
+  }
+}
+
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----
+Run Logstash with this configuration:
+
+----
+bin/logstash -f logstash-filter.conf
+----
+
+Now paste this line into the terminal (so it will be processed by the stdin input):
+----
+127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] "GET /xampp/status.php HTTP/1.1" 200 3891 "http://cadenza/xampp/navi.php" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0"
+----
+You should see something returned to STDOUT which looks like this:
+----
+{
+        "message" => "127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] \"GET /xampp/status.php HTTP/1.1\" 200 3891 \"http://cadenza/xampp/navi.php\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\"",
+     "@timestamp" => "2013-12-11T08:01:45.000Z",
+       "@version" => "1",
+           "host" => "cadenza",
+       "clientip" => "127.0.0.1",
+          "ident" => "-",
+           "auth" => "-",
+      "timestamp" => "11/Dec/2013:00:01:45 -0800",
+           "verb" => "GET",
+        "request" => "/xampp/status.php",
+    "httpversion" => "1.1",
+       "response" => "200",
+          "bytes" => "3891",
+       "referrer" => "\"http://cadenza/xampp/navi.php\"",
+          "agent" => "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\""
+}
+----
+As you can see, Logstash (with help from the *grok* filter) was able to parse the log line (which happens to be in Apache "combined log" format) and break it up into many different discrete bits of information. This will be extremely useful later when we start querying and analyzing our log data... for example, we'll be able to run reports on HTTP response codes, IP addresses, referrers, etc. very easily. There are quite a few grok patterns included with Logstash out-of-the-box, so it's quite likely if you're attempting to parse a fairly common log format, someone has already done the work for you. For more details, see the list of https://github.com/logstash/logstash/blob/master/patterns/grok-patterns[logstash grok patterns] on github.
+
+The other filter used in this example is the *date* filter. This filter parses out a timestamp and uses it as the timestamp for the event (regardless of when you're ingesting the log data). You'll notice that the @timestamp field in this example is set to December 11, 2013, even though Logstash is ingesting the event at some point afterwards. This is handy when backfilling logs, for example... the ability to tell Logstash "use this value as the timestamp for this event".
+
+== Useful Examples
+
+=== Apache logs (from files)
+Now, let's configure something actually *useful*... apache2 access log files! We are going to read the input from a file on the localhost, and use a *conditional* to process the event according to our needs. First, create a file called something like 'logstash-apache.conf' with the following contents (you'll need to change the log's file path to suit your needs):
+
+----
+input {
+  file {
+    path => "/tmp/access_log"
+    start_position => "beginning"
+  }
+}
+
+filter {
+  if [path] =~ "access" {
+    mutate { replace => { "type" => "apache_access" } }
+    grok {
+      match => { "message" => "%{COMBINEDAPACHELOG}" }
+    }
+  }
+  date {
+    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
+  }
+}
+
+output {
+  elasticsearch {
+    host => localhost
+  }
+  stdout { codec => rubydebug }
+}
+
+----
+Then, create the file you configured above (in this example, "/tmp/access_log") with the following log lines as contents (or use some from your own webserver):
+
+----
+71.141.244.242 - kurt [18/May/2011:01:48:10 -0700] "GET /admin HTTP/1.1" 301 566 "-" "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3"
+134.39.72.245 - - [18/May/2011:12:40:18 -0700] "GET /favicon.ico HTTP/1.1" 200 1189 "-" "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; InfoPath.2; .NET4.0C; .NET4.0E)"
+98.83.179.51 - - [18/May/2011:19:35:08 -0700] "GET /css/main.css HTTP/1.1" 200 1837 "http://www.safesand.com/information.htm" "Mozilla/5.0 (Windows NT 6.0; WOW64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1"
+----
+
+Now run it with the -f flag as in the last example:
+----
+bin/logstash -f logstash-apache.conf
+----
+You should be able to see your apache log data in Elasticsearch now! You'll notice that Logstash opened the file you configured, and read through it, processing any events it encountered. Any additional lines logged to this file will also be captured, processed by Logstash as events and stored in Elasticsearch. As an added bonus, they will be stashed with the field "type" set to "apache_access" (this is done by the type => "apache_access" line in the input configuration).
+
+In this configuration, Logstash is only watching the apache access_log, but it's easy enough to watch both the access_log and the error_log (actually, any file matching '*log'), by changing one line in the above configuration, like this:
+
+----
+input {
+  file {
+    path => "/tmp/*_log"
+...
+----
+Now, rerun Logstash, and you will see both the error and access logs processed via Logstash. However, if you inspect your data (using elasticsearch-kopf, perhaps), you will see that the access_log was broken up into discrete fields, but not the error_log. That's because we used a "grok" filter to match the standard combined apache log format and automatically split the data into separate fields. Wouldn't it be nice *if* we could control how a line was parsed, based on its format? Well, we can...
+
+Also, you might have noticed that Logstash did not reprocess the events which were already seen in the access_log file. Logstash is able to save its position in files, only processing new lines as they are added to the file. Neat!
+
+=== Conditionals
+Now we can build on the previous example, where we introduced the concept of a *conditional*. A conditional should be familiar to most Logstash users, in the general sense. You may use 'if', 'else if' and 'else' statements, as in many other programming languages. Let's label each event according to which file it appeared in (access_log, error_log and other random files which end with "log").
+
+----
+input {
+  file {
+    path => "/tmp/*_log"
+  }
+}
+
+filter {
+  if [path] =~ "access" {
+    mutate { replace => { type => "apache_access" } }
+    grok {
+      match => { "message" => "%{COMBINEDAPACHELOG}" }
+    }
+    date {
+      match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
+    }
+  } else if [path] =~ "error" {
+    mutate { replace => { type => "apache_error" } }
+  } else {
+    mutate { replace => { type => "random_logs" } }
+  }
+}
+
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----
+
+You'll notice we've labeled all events using the "type" field, but we didn't actually parse the "error" or "random" files... There are so many types of error logs that it's better left as an exercise for you, depending on the logs you're seeing.
+
+=== Syslog
+OK, now we can move on to another incredibly useful example: *syslog*. Syslog is one of the most common use cases for Logstash, and one it handles exceedingly well (as long as the log lines conform roughly to RFC3164 :). Syslog is the de facto UNIX networked logging standard, sending messages from client machines to a local file, or to a centralized log server via rsyslog. For this example, you won't need a functioning syslog instance; we'll fake it from the command line, so you can get a feel for what happens.
+
+First, let's make a simple configuration file for Logstash + syslog, called 'logstash-syslog.conf'.
+
+----
+input {
+  tcp {
+    port => 5000
+    type => syslog
+  }
+  udp {
+    port => 5000
+    type => syslog
+  }
+}
+
+filter {
+  if [type] == "syslog" {
+    grok {
+      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
+      add_field => [ "received_at", "%{@timestamp}" ]
+      add_field => [ "received_from", "%{host}" ]
+    }
+    syslog_pri { }
+    date {
+      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
+    }
+  }
+}
+
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----
+Run it as normal:
+----
+bin/logstash -f logstash-syslog.conf
+----
+Normally, a client machine would connect to the Logstash instance on port 5000 and send its message. In this simplified case, we're simply going to telnet to Logstash and enter a log line (similar to how we entered log lines into STDIN earlier). First, open another shell window to interact with the Logstash syslog input and type the following command:
+
+----
+telnet localhost 5000
+----
+
+You can copy and paste the following lines as samples (feel free to try some of your own, but keep in mind they might not parse if the grok filter is not correct for your data):
+
+----
+Dec 23 12:11:43 louis postfix/smtpd[31499]: connect from unknown[95.75.93.154]
+Dec 23 14:42:56 louis named[16000]: client 199.48.164.7#64817: query (cache) 'amsterdamboothuren.com/MX/IN' denied
+Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)
+Dec 22 18:28:06 louis rsyslogd: [origin software="rsyslogd" swVersion="4.2.0" x-pid="2253" x-info="http://www.rsyslog.com"] rsyslogd was HUPed, type 'lightweight'.
+----
+
+Now you should see the output of Logstash in your original shell as it processes and parses messages!
+
+----
+{
+                 "message" => "Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)",
+              "@timestamp" => "2013-12-23T22:30:01.000Z",
+                "@version" => "1",
+                    "type" => "syslog",
+                    "host" => "0:0:0:0:0:0:0:1:52617",
+        "syslog_timestamp" => "Dec 23 14:30:01",
+         "syslog_hostname" => "louis",
+          "syslog_program" => "CRON",
+              "syslog_pid" => "619",
+          "syslog_message" => "(www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)",
+             "received_at" => "2013-12-23 22:49:22 UTC",
+           "received_from" => "0:0:0:0:0:0:0:1:52617",
+    "syslog_severity_code" => 5,
+    "syslog_facility_code" => 1,
+         "syslog_facility" => "user-level",
+         "syslog_severity" => "notice"
+}
+----
+
+Congratulations! You're well on your way to being a real Logstash power user. You should be comfortable configuring, running and sending events to Logstash, but there's much more to explore.
diff --git a/docs/tutorials/just-enough-rabbitmq-for-logstash.md b/docs/tutorials/just-enough-rabbitmq-for-logstash.md
new file mode 100644
index 00000000000..060fa6f0ac2
--- /dev/null
+++ b/docs/tutorials/just-enough-rabbitmq-for-logstash.md
@@ -0,0 +1,201 @@
+---
+title: Just Enough RabbitMQ - logstash
+layout: content_right
+---
+
+While configuring your RabbitMQ broker is out of scope for logstash, it's important
+to understand how logstash uses RabbitMQ. To do that, we need to understand a
+little about AMQP.
+
+You should also consider reading
+[this](http://www.rabbitmq.com/tutorials/amqp-concepts.html) at the RabbitMQ
+website.
+
+# Exchanges, queues and bindings; OH MY!
+
+You can get a long way by understanding a few key terms.
+
+## Exchanges
+
+Exchanges are for message **producers**. In Logstash, we map these to
+**outputs**.  Logstash puts messages on exchanges.  There are many types of
+exchanges and they are discussed below.
+
+## Queues
+
+Queues are for message **consumers**. In Logstash, we map these to inputs.
+Logstash reads messages from queues.  Optionally, queues can consume only a
+subset of messages. This is done with "routing keys".
+
+## Bindings
+
+Just having a producer and a consumer is not enough. We must `bind` a queue to
+an exchange.  When we bind a queue to an exchange, we can optionally provide a
+routing key.  Routing keys are discussed below.
+
+## Broker
+
+A broker is simply the AMQP server software. There are several brokers, but this
+tutorial will cover the most common (and arguably popular), [RabbitMQ](http://www.rabbitmq.com).
+
+# Routing Keys
+
+Simply put, routing keys are somewhat like tags for messages. In practice, they
+are hierarchical in nature with the each level separated by a dot:
+
+- `messages.servers.production`
+- `sports.atlanta.baseball`
+- `company.myorg.mydepartment`
+
+Routing keys are really handy with a tool like logstash where you
+can programatically define the routing key for a given event using the metadata that logstash provides:
+
+- `logs.servers.production.host1`
+- `logs.servers.development.host1.syslog`
+- `logs.servers.application_foo.critical`
+
+From a consumer/queue perspective, routing keys also support two types wildcards - `#` and `*`.
+
+- `*` (asterisk) matches any single word.
+- `#` (hash) matches any number of words and behaves like a traditional wildcard.
+
+Using the above examples, if you wanted to bind to an exchange and see messages
+for just production, you would use the routing key `logs.servers.production.*`.
+If you wanted to see messages for host1, regardless of environment you could
+use `logs.servers.%.host1.#`.
+
+Wildcards can be a bit confusing but a good general rule to follow is to use
+`*` in places where you need wildcards for a known element.  Use `#` when you
+need to match any remaining placeholders. Note that wildcards in routing keys
+only make sense on the consumer/queue binding, not in the publishing/exchange
+side.
+
+We'll get into some of that neat stuff below. For now, it's enough to
+understand the general idea behind routing keys.
+
+# Exchange types
+
+There are three primary types of exchanges that you'll see.
+
+## Direct
+
+A direct exchange is one that is probably most familiar to people. Message
+comes in and, assuming there is a queue bound, the message is picked up.  You
+can have multiple queues bound to the same direct exchange. The best way to
+understand this pattern is pool of workers (queues) that read from a direct
+exchange to get units of work. Only one consumer will see a given message in a
+direct exchange.
+
+You can set routing keys on messages published to a direct exchange. This
+allows you do have workers that do different tasks read from the same global
+pool of messages yet consume only the ones they know how to handle.
+
+The RabbitMQ concepts guide (linked below) does a good job of describing this
+visually
+[here](http://www.rabbitmq.com/img/tutorials/intro/exchange-direct.png)
+
+## Fanout
+
+Fanouts are another type of exchange. Unlike direct exchanges, every queue
+bound to a fanout exchange will see the same messages.  This is best described
+as a PUB/SUB pattern. This is helpful when you need broadcast messages to
+multiple interested parties.
+
+Fanout exchanges do NOT support routing keys. All bound queues see all
+messages.
+
+## Topic
+
+Topic exchanges are special type of fanout exchange. Fanout exchanges don't
+support routing keys. Topic exchanges do support them.  Just like a fanout
+exchange, all bound queues see all messages with the additional filter of the
+routing key.
+
+# RabbitMQ in logstash
+
+As stated earlier, in Logstash, Outputs publish to Exchanges. Inputs read from
+Queues that are bound to Exchanges.  Logstash uses the `bunny` RabbitMQ library for
+interaction with a broker. Logstash endeavors to expose as much of the
+configuration for both exchanges and queues.  There are many different tunables
+that you might be concerned with setting - including things like message
+durability or persistence of declared queues/exchanges.  See the relevant input
+and output documentation for RabbitMQ for a full list of tunables.
+
+# Sample configurations, tips, tricks and gotchas
+
+There are several examples in the logstash source directory of RabbitMQ usage,
+however a few general rules might help eliminate any issues.
+
+## Check your bindings
+
+If logstash is publishing the messages and logstash is consuming the messages,
+the `exchange` value for the input should match the `name` in the output.
+
+sender agent
+
+    input { stdin { type = "test" } }
+    output {
+      rabbitmq {
+        exchange => "test_exchange"
+        host => "my_rabbitmq_server"
+        exchange_type => "fanout"
+      }
+    }
+
+receiver agent
+
+    input {
+      rabbitmq {
+        queue => "test_queue"
+        host => "my_rabbitmq_server"
+        exchange => "test_exchange" # This matches the exchange declared above
+      }
+    }
+    output { stdout { debug => true }}
+
+## Message persistence
+
+By default, logstash will attempt to ensure that you don't lose any messages.
+This is reflected in the RabbitMQ default settings as well.  However there are
+cases where you might not want this. A good example is where RabbitMQ is not your
+primary method of shipping.
+
+In the following example, we use RabbitMQ as a sniffing interface. Our primary
+destination is the embedded ElasticSearch instance. We have a secondary RabbitMQ
+output that we use for duplicating messages. However we disable persistence and
+durability on this interface so that messages don't pile up waiting for
+delivery. We only use RabbitMQ when we want to watch messages in realtime.
+Additionally, we're going to leverage routing keys so that we can optionally
+filter incoming messages to subsets of hosts. The exercise of getting messages
+to this logstash agent are left up to the user.
+
+    input { 
+      # some input definition here
+    }
+
+    output {
+      elasticsearch { embedded => true }
+      rabbitmq {
+        exchange => "logtail"
+        host => "my_rabbitmq_server"
+        exchange_type => "topic" # We use topic here to enable pub/sub with routing keys
+        key => "logs.%{host}"
+        durable => false # If rabbitmq restarts, the exchange disappears.
+        auto_delete => true # If logstash disconnects, the exchange goes away
+        persistent => false # Messages are not persisted to disk
+      }
+    }
+
+Now if you want to stream logs in realtime, you can use the programming
+language of your choice to bind a queue to the `logtail` exchange.  If you do
+not specify a routing key, you will see every message that comes in to
+logstash. However, you can specify a routing key like `logs.apache1` and see
+only messages from host `apache1`.
+
+Note that any logstash variable is valid in the key definition. This allows you
+to create really complex routing key hierarchies for advanced filtering.
+
+Note that RabbitMQ has specific rules about durability and persistence matching
+on both the queue and exchange. You should read the RabbitMQ documentation to
+make sure you don't crash your RabbitMQ server with messages awaiting someone
+to pick them up.
diff --git a/docs/tutorials/media/frontend-response-codes.png b/docs/tutorials/media/frontend-response-codes.png
new file mode 100644
index 00000000000..e5b0ed47ee9
Binary files /dev/null and b/docs/tutorials/media/frontend-response-codes.png differ
diff --git a/docs/tutorials/metrics-from-logs.md b/docs/tutorials/metrics-from-logs.md
new file mode 100644
index 00000000000..a044fef0fb9
--- /dev/null
+++ b/docs/tutorials/metrics-from-logs.md
@@ -0,0 +1,84 @@
+---
+title: Metrics from Logs - logstash
+layout: content_right
+---
+# Pull metrics from logs
+
+Logs are more than just text. How many customers signed up today? How many HTTP
+errors happened this week? When was your last puppet run?
+
+Apache logs give you the http response code and bytes sent - that's useful in a
+graph. Metrics occur in logs so frequently there are piles of tools available to
+help process them.
+
+Logstash can help (and even replace some tools you might already be using).
+
+## Example: Replacing Etsy's Logster
+
+[Etsy](https://github.com/etsy) has some excellent open source tools. One of
+them, [logster](https://github.com/etsy/logster), is meant to help you pull
+metrics from logs and ship them to [graphite](http://graphite.wikidot.com/) so
+you can make pretty graphs of those metrics.
+
+One sample logster parser is one that pulls http response codes out of your
+apache logs: [SampleLogster.py](https://github.com/etsy/logster/blob/master/logster/parsers/SampleLogster.py)
+
+The above code is roughly 50 lines of python and only solves one specific
+problem in only apache logs: count http response codes by major number (1xx,
+2xx, 3xx, etc). To be completely fair, you could shrink the code required for
+a Logster parser, but size is not strictly the point, here.
+
+## Keep it simple
+
+Logstash can do more than the above, simpler, and without much coding skill:
+
+    input {
+      file { 
+        path => "/var/log/apache/access.log" 
+        type => "apache-access"
+      }
+    }
+
+    filter {
+      grok { 
+        type => "apache-access"
+        pattern => "%{COMBINEDAPACHELOG}" 
+      }
+    }
+
+    output {
+      statsd { 
+        # Count one hit every event by response
+        increment => "apache.response.%{response}" 
+      }
+    }
+
+The above uses grok to parse fields out of apache logs and using the statsd
+output to increment counters based on the response code. Of course, now that we
+are parsing apache logs fully, we can trivially add additional metrics:
+
+    output {
+      statsd {
+        # Count one hit every event by response
+        increment => "apache.response.%{response}"
+
+        # Use the 'bytes' field from the apache log as the count value.
+        count => [ "apache.bytes", "%{bytes}" ]
+      }
+    }
+
+Now adding additional metrics is just one more line in your logstash config
+file. BTW, the 'statsd' output writes to another Etsy tool,
+[statsd](https://github.com/etsy/statsd), which helps build counters/latency
+data and ship it to graphite for graphing.
+
+Using the logstash config above and a bunch of apache access requests, you might end up
+with a graph that looks like this:
+
+![apache response codes graphed with graphite, fed data with logstash](media/frontend-response-codes.png)
+
+The point made above is not "logstash is better than Logster" - the point is
+that logstash is a general-purpose log management and pipelining tool and that
+while you can centralize logs with logstash, you can read, modify, and write
+them to and from just about anywhere.
+
diff --git a/docs/tutorials/zeromq.md b/docs/tutorials/zeromq.md
new file mode 100644
index 00000000000..796ec0ea3ae
--- /dev/null
+++ b/docs/tutorials/zeromq.md
@@ -0,0 +1,118 @@
+---
+title: ZeroMQ - logstash
+layout: content_right
+---
+
+*ZeroMQ support in Logstash is currently in an experimental phase. As such, parts of this document are subject to change.*
+
+# ZeroMQ
+Simply put ZeroMQ (0mq) is a socket on steroids. This makes it a perfect compliment to Logstash - a pipe on steroids.
+
+ZeroMQ allows you to easily create sockets of various types for moving data around. These sockets are refered to in ZeroMQ by the behavior of each side of the socket pair:
+
+* PUSH/PULL
+* REQ/REP
+* PUB/SUB
+* ROUTER/DEALER
+
+There is also a `PAIR` socket type as well.
+
+Additionally, the socket type is independent of the connection method. A PUB/SUB socket pair could have the SUB side of the socket be a listener and the PUB side a connecting client. This makes it very easy to fit ZeroMQ into various firewalled architectures.
+
+Note that this is not a full-fledged tutorial on ZeroMQ. It is a tutorial on how Logstash uses ZeroMQ.
+
+# ZeroMQ and logstash
+In the spirit of ZeroMQ, Logstash takes these socket type pairs and uses them to create topologies with some very simply rules that make usage very easy to understand:
+
+* The receiving end of a socket pair is always a logstash input
+* The sending end of a socket pair is always a logstash output
+* By default, inputs `bind`/listen and outputs `connect`
+* Logstash refers to the socket pairs as topologies and mirrors the naming scheme from ZeroMQ
+* By default, ZeroMQ inputs listen on all interfaces on port 2120, ZeroMQ outputs connect to `localhost` on port 2120
+
+The currently understood Logstash topologies for ZeroMQ inputs and outputs are:
+
+* `pushpull`
+* `pubsub`
+* `pair`
+
+We have found from various discussions that these three topologies will cover most of user's needs. We hope to expose the full span of ZeroMQ socket types as time goes on.
+
+By keeping the options simple, this allows you to get started VERY easily with what are normally complex message flows. No more confusion over `exchanges` and `queues` and `brokers`. If you need to add fanout capability to your flow, you can simply use the following configs:
+
+* _node agent lives at 192.168.1.2_
+* _indexer agent lives at 192.168.1.1_
+
+    # Node agent config
+    input { stdin { type => "test-stdin-input" } }
+    output { zeromq { topology => "pubsub" address => "tcp://192.168.1.1.:2120" } }
+
+    # Indexer agent config
+    input { zeromq { topology => "pubsub" } }
+    output { stdout { debug => true }}
+
+If for some reason you need connections to initiate from the indexer because of firewall rules:
+
+    # Node agent config - now listening on all interfaces port 2120
+    input { stdin { type => "test-stdin-input" } }
+    output { zeromq { topology => "pubsub" address => "tcp://*.:2120" mode => "server" } }
+
+    # Indexer agent config
+    input { zeromq { topology => "pubsub" address => "tcp://192.168.1.2" mode => "client" } }
+    output { stdout { debug => true }}
+
+As stated above, by default `inputs` always start as listeners and `outputs` always start as initiators. Please don't confuse what happens once the socket is connect with the direction of the connection. ZeroMQ separates connection from topology. In the second case of the above configs, once the two sockets are connected, regardless of who initiated the connection, the message flow itself is absolute. The indexer is reading events from the node.
+
+# Which topology to use
+The choice of topology can be broken down very easily based on need
+
+## one to one
+Use `pair` topology. On the output side, specify the ipaddress and port of the input side.
+
+## broadcast
+Use `pubsub`
+If you need to broadcast ALL messages to multiple hosts that each need to see all events, use `pubsub`. Note that all events are broadcast to all subscribers. When using `pubsub` you might also want to investigate the `topic` configuration option which allows subscribers to see only a subset of messages.
+
+## Filter workers
+Use `pushpull`
+In `pushpull`, ZeroMQ automatically load balances to all connected peers. This means that no peer sees the same message as any other peer.
+
+# What's with the address format?
+ZeroMQ supports multiple types of transports:
+
+* inproc:// (unsupported by logstash due to threading)
+* tcp:// (exactly what it sounds like)
+* ipc:// (probably useless in logstash)
+* pgm:// and epgm:// (a multicast format - only usable with PUB and SUB socket types)
+
+For pretty much all cases, you'll be using `tcp://` transports with Logstash.
+
+## Topic - applies to `pubsub`
+This opt mimics the routing keys functionality in AMQP. Imagine you have a network of receivers but only a subset of the messages need to be seen by a subset of the hosts. You can use this option as a routing key to facilite that:
+
+    # This output is a PUB
+    output {
+    zeromq { topology => "pubsub" topic => "logs.production.%{host}" }
+    }
+
+    # This input is a SUB
+    # I only care about db1 logs
+    input { zeromq { type => "db1logs" address => "tcp://<ipaddress>:2120" topic => "logs.production.db1"}}
+
+One thing important to note about 0mq PUBSUB and topics is that all filtering is done on the subscriber side. The subscriber will get ALL messages but discard any that don't match the topic.
+
+Also important to note is that 0mq doesn't do topic in the same sense as an AMQP broker might. When a SUB socket gets a message, it compares the first bytes of the message against the topic. However, this isn't always flexible depending on the format of your message. The common practice then, is to send a 0mq multipart message and make the first part the topic. The next parts become the actual message body.
+
+This is approach is how logstash handles this. When using PUBSUB, Logstash will send a multipart message where the first part is the name of the topic and the second part is the event. This is important to know if you are sending to a SUB input from sources other than Logstash.
+
+# sockopts
+Sockopts is not you choosing between blue or black socks. ZeroMQ supports setting various flags or options on sockets. In the interest of minimizing configuration syntax, these are _hidden_ behind a logstash configuration element called `sockopts`. You probably won't need to tune these for most cases. If you do need to tune them, you'll probably set the following:
+
+## ZMQ::HWM - sets the high water mark
+The high water mark is the maximum number of messages a given socket pair can have in its internal queue. Use this to throttle essentially.
+
+## ZMQ::SWAP_SIZE
+TODO
+
+## ZMQ::IDENTITY
+TODO
diff --git a/dripmain.rb b/dripmain.rb
new file mode 100644
index 00000000000..23426a5b063
--- /dev/null
+++ b/dripmain.rb
@@ -0,0 +1,29 @@
+# dripmain.rb is called by org.jruby.main.DripMain to further warm the JVM with any preloading
+# that we can do to speedup future startup using drip.
+
+# we are out of the application context here so setup the load path and gem paths
+lib_path = File.expand_path(File.join(File.dirname(__FILE__), "./lib"))
+$:.unshift(lib_path)
+
+require "logstash/environment"
+LogStash::Environment.set_gem_paths!
+
+# typical required gems and libs
+require "i18n"
+I18n.enforce_available_locales = true
+I18n.load_path << LogStash::Environment.locales_path("en.yml")
+require "cabin"
+require "stud/trap"
+require "stud/task"
+require "clamp"
+require "rspec"
+require "rspec/core/runner"
+
+require "logstash/namespace"
+require "logstash/program"
+require "logstash/agent"
+require "logstash/kibana"
+require "logstash/util"
+require "logstash/errors"
+require "logstash/pipeline"
+require "logstash/plugin"
diff --git a/etc/agent.conf.example b/etc/agent.conf.example
deleted file mode 100644
index e42ac37fc44..00000000000
--- a/etc/agent.conf.example
+++ /dev/null
@@ -1,29 +0,0 @@
-input {
-  file {
-    path => [ "/var/log/messages", "/var/log/*.log" ]
-    type => "linux-syslog"
-  }
-}
-
-filter {
-  grok {
-    type => "linux-syslog"
-    pattern => "%{SYSLOGLINE}"
-  }
-
-  date {
-    type => "linux-syslog"
-    timestamp => "MMM dd HH:mm:ss"
-    timestamp8601 => ISO8601
-  }
-}
-
-output {
-  stdout {
-  }
-
-  elasticsearch { 
-    index => "logstash"
-    type => "%{@type}"
-  }
-}
diff --git a/etc/agent.lgtm.conf b/etc/agent.lgtm.conf
deleted file mode 100644
index c0604ec3c5f..00000000000
--- a/etc/agent.lgtm.conf
+++ /dev/null
@@ -1,59 +0,0 @@
-input {
-  file {
-    path => [ "/var/log/messages", "/var/log/kern.log" ]
-    type => "linux-syslog"
-  }
-
-  file {
-    path => "/var/log/apache2/access.log"
-    type => "apache-access"
-  }
-
-  file {
-    path => "/var/log/apache2/error.log"
-    type => "apache-error"
-  }
-}
-
-output {
-  amqp {
-    host => "myamqpserver"
-    exchange_type => "fanout"
-    name => "rawlogs"
-  }
-  amqp {
-    host => "127.0.0.1"
-    exchange_type => "topic"
-    name => "logsniff"
-  }
-  stdout { }
-}
-
-# Filters are applied in the order the appear.
-filter {
-  multiline {
-    type => "supervisorlogs"
-    pattern => "^\s"
-    what => previous
-  } 
-
-  multiline { 
-    type => "testing"
-    pattern => "^\s"
-    what => previous
-  }
-
-  grok {
-    type => "linux-syslog"
-    pattern => ["%{SYSLOG_SUDO}", "%{SYSLOG_KERNEL}", "%{SYSLOGLINE}" ]
-  }
-
-  grok {
-    type => "nagios"
-    pattern => "%{NAGIOSLOGLINE}"
-  }
-
-  #date {
-    #" testing" => fizzle
-  #}
-}
diff --git a/etc/examples/agent-nagios.conf b/etc/examples/agent-nagios.conf
deleted file mode 100644
index 65149065c70..00000000000
--- a/etc/examples/agent-nagios.conf
+++ /dev/null
@@ -1,22 +0,0 @@
-input {
-  stdin { type => "foo" }
-}
-
-filter {
-  grep {
-    type => "foo"
-    match => [ "@message", ".*" ]
-    add_fields => [ "nagios_host", "%{@source_host}" ]
-    add_fields => [ "nagios_service", "example service" ]
-    add_fields => [ "nagios_annotation", "my annotation" ]
-  }
-}
-
-output {
-  stdout {
-  }
-
-  nagios { 
-    commandfile => "/tmp/cmdfile"
-  }
-}
diff --git a/etc/examples/agent-stomp.conf b/etc/examples/agent-stomp.conf
deleted file mode 100644
index 901d738a0fa..00000000000
--- a/etc/examples/agent-stomp.conf
+++ /dev/null
@@ -1,18 +0,0 @@
-input {
-  stdin { }
-  stomp {
-    host => "localhost"
-    destination => "/topic/foo"
-  }
-}
-
-output {
-  stdout {
-    debug => true
-  }
-
-  stomp {
-    host => "localhost"
-    destination => "/topic/foo"
-  }
-}
diff --git a/etc/examples/agent-twitter.conf b/etc/examples/agent-twitter.conf
deleted file mode 100644
index 39c5fcdd159..00000000000
--- a/etc/examples/agent-twitter.conf
+++ /dev/null
@@ -1,16 +0,0 @@
-input {
-  twitter {
-    type => "twitter"
-    user => "USER"
-    password => "PASSWORD"
-    keywords => ["python", "ruby", "perl", "sysadmiN"]
-  }
-  stdin { 
-    type => "testingstdin"
-  }
-}
-
-output {
-  stdout { }
-  elasticsearch { }
-}
diff --git a/etc/examples/indexer.conf b/etc/examples/indexer.conf
deleted file mode 100644
index 1437a49c354..00000000000
--- a/etc/examples/indexer.conf
+++ /dev/null
@@ -1,30 +0,0 @@
-input {
-  amqp {
-    host => "127.0.0.1"
-    user => "guest"
-    pass => "guest"
-    exchange_type => "topic"
-    name => "testing"
-    type => "all"
-  }
-
-  tcp {
-    port => 1234
-    type => "linux-syslog"
-  }
-}
-
-filter {
-  grok {
-    type => "linux-syslog"
-    pattern => ["%{SYSLOG_SUDO}", "%{SYSLOG_KERNEL}", "%{SYSLOGLINE}"]
-    add_tag => "grok"
-    add_field => ["test_key", "the pid is %{pid}"]
-  }
-}
-
-output {
-  stdout {
-    debug => true
-  }
-}
diff --git a/etc/logstash-elasticsearch-rabbitmq-river.yaml b/etc/logstash-elasticsearch-rabbitmq-river.yaml
deleted file mode 100644
index 24c1758de44..00000000000
--- a/etc/logstash-elasticsearch-rabbitmq-river.yaml
+++ /dev/null
@@ -1,41 +0,0 @@
---- 
-# this is a sample logstash config (code is still highly in change, so
-# this could change later)
-#
-#
-inputs:
-# Give a list of inputs. Tag them for easy query/filter later.
-  linux-syslog: # this is the 'linux-syslog' type
-  - /var/log/messages # watch /var/log/messages (uses eventmachine-tail)
-  - /var/log/kern.log
-  - /var/log/auth.log
-  - /var/log/user.log
-  apache-access: # similar, different type.
-  - /var/log/apache2/access.log
-  - /b/access
-  apache-error:
-  - /var/log/apache2/error.log
-filters:
-- grok:
-    linux-syslog: # for logs of type 'linux-syslog'
-      patterns:
-      - %{SYSLOGLINE}
-    apache-access: # for logs of type 'apache-error'
-      patterns:
-      - %{COMBINEDAPACHELOG}
-- date:
-    linux-syslog:  # for logs of type 'linux-syslog'
-      # Look for a field 'timestamp' with this format, parse and it for the timestamp
-      # This field comes from the SYSLOGLINE pattern
-      timestamp: "%b %e %H:%M:%S"
-    apache-access:
-      timestamp: "%d/%b/%Y:%H:%M:%S %Z"
-outputs:
-- stdout:///
-- "elasticsearch://localhost:9200/logs/all?method=river&type=rabbitmq&host=127.0.0.1&user=guest&pass=guest&vhost=/&queue=es"
-# But we could write to mongodb, too.
-#  - mongodb://localhost/parsedlogs
-# And also write to an AMQP topic
-#  - amqp://localhost/topic/parsedlogs
-# Write to stdout ... etc.
-# - stdout:///
diff --git a/etc/logstash-grep.yaml b/etc/logstash-grep.yaml
deleted file mode 100644
index 4574a55afcd..00000000000
--- a/etc/logstash-grep.yaml
+++ /dev/null
@@ -1,31 +0,0 @@
----
-inputs:
-  linux-syslog:
-  - /var/log/messages
-filters:
-- grok:
-    linux-syslog: # for logs of type 'linux-syslog'
-      patterns:
-      - %{SYSLOGLINE}
-- date:
-    linux-syslog:
-      timestamp: "%b %e %H:%M:%S"
-      timestamp8601: ISO8601
-- grep:
-    linux-syslog:
-      - match:
-          message: test
-        add_fields:
-          nagios_alert: test_alert
-        add_tags:
-          - nagios
-          - test
-      - match:
-          message: (?i)foo.*bar
-          program: test
-        add_fields:
-          nagios_alert: foo_alert
-        add_tags:
-          - nagios
-outputs:
-- stdout:///
diff --git a/etc/logstash-jruby-test.yaml b/etc/logstash-jruby-test.yaml
deleted file mode 100644
index 19e9e5d6245..00000000000
--- a/etc/logstash-jruby-test.yaml
+++ /dev/null
@@ -1,17 +0,0 @@
-# Example config that reads parsed logs from AMQP and prints to stdout
-inputs:
-  linux-syslog:
-  - file:///var/log/messages
-filters:
-- grok:
-    linux-syslog:
-      patterns:
-      - %{SYSLOGLINE}
-- grep:
-    linux-syslog:
-      - match:
-          @message: test
-        add_fields:
-          filter_test: hello world
-outputs:
-  - stdout:///debug
diff --git a/etc/logstash-mongodb-storage.yaml b/etc/logstash-mongodb-storage.yaml
deleted file mode 100644
index 312c78663ef..00000000000
--- a/etc/logstash-mongodb-storage.yaml
+++ /dev/null
@@ -1,5 +0,0 @@
-# Example config that reads parsed logs from AMQP and dumps results into mongodb
-inputs:
-  - amqp://localhost/topic/parsedlogs
-outputs:
-  - mongodb://localhost/parsedlogs
diff --git a/etc/logstash-nagios.yaml b/etc/logstash-nagios.yaml
deleted file mode 100644
index 204eda57dfc..00000000000
--- a/etc/logstash-nagios.yaml
+++ /dev/null
@@ -1,19 +0,0 @@
---- 
-configname: nagios
-# Example config that filters already-parsed logs (grok filter at least) for
-# certain patterns and sends the results to Nagios.
-inputs:
-  all:
-  - amqp:///topic/parsedlogs
-filters:
-- grep:
-    java:
-      - match:
-          JAVASTACKTRACEPART: .*
-        add_fields:
-          nagios_host: localhost
-          nagios_service: Java Exceptions
-          nagios_annotation: "Java exception"
-outputs:
-- stdout:///
-- nagios:///var/lib/nagios3/rw/nagios.cmd
diff --git a/etc/logstash-parser.yaml b/etc/logstash-parser.yaml
deleted file mode 100644
index f7675bde533..00000000000
--- a/etc/logstash-parser.yaml
+++ /dev/null
@@ -1,20 +0,0 @@
-# Example config that parses rawlogs with grok and puts them on another AMQP topic
-inputs:
-  all:
-  - amqp://localhost/topic/rawlogs
-outputs:
-  - stdout:///
-filters:
-- grok:
-    linux-syslog: # for logs tagged 'linux-syslog'
-      timestamp:
-        key: date
-        format: %b %e %H:%M:%S
-      patterns:
-      - %{SYSLOGLINE}
-    apache-access: # for logs tagged 'apache-error'
-      timestamp:
-        key: timestamp
-        format: %d/%b/%Y:%H:%M:%S %Z
-      patterns:
-      - %{COMBINEDAPACHELOG}
diff --git a/etc/logstash-reader.yaml b/etc/logstash-reader.yaml
deleted file mode 100644
index 0965774fa20..00000000000
--- a/etc/logstash-reader.yaml
+++ /dev/null
@@ -1,13 +0,0 @@
-# Example config that reads parsed logs from AMQP and prints to stdout
-inputs:
-  linux-syslog:
-  - file:///var/log/messages
-filters:
-- grep:
-    linux-syslog:
-      - match:
-          @message: test
-        add_fields:
-          filter_test: hello world
-outputs:
-  - stdout:///
diff --git a/etc/logstash-shipper.yaml b/etc/logstash-shipper.yaml
deleted file mode 100644
index 0789ba0b63b..00000000000
--- a/etc/logstash-shipper.yaml
+++ /dev/null
@@ -1,18 +0,0 @@
---- 
-configname: shipper
-# Example config that only ships log data from files to an AMQP topic
-inputs:
-  linux-syslog:
-  - /var/log/messages
-  - /var/log/kern.log
-  - /var/log/auth.log
-  - /var/log/user.log
-  apache-access:
-  - /var/log/apache2/access.log
-  - /b/access
-  apache-error:
-  - /var/log/apache2/access.log
-  unknown:
-  - /b/randomdata
-outputs:
-- amqp://localhost/topic/rawlogs
diff --git a/etc/logstash-standalone.yaml b/etc/logstash-standalone.yaml
deleted file mode 100644
index 99d67483660..00000000000
--- a/etc/logstash-standalone.yaml
+++ /dev/null
@@ -1,42 +0,0 @@
---- 
-# this is a sample logstash config (code is still highly in change, so
-# this could change later)
-#
-#
-inputs:
-# Give a list of inputs. Tag them for easy query/filter later.
-  linux-syslog: # this is the 'linux-syslog' type
-  - /var/log/messages # watch /var/log/messages (uses eventmachine-tail)
-  - /var/log/kern.log
-  - /var/log/auth.log
-  - /var/log/user.log
-  apache-access: # similar, different type.
-  - /var/log/apache2/access.log
-  - /b/access
-  apache-error:
-  - /var/log/apache2/error.log
-filters:
-- grok:
-    linux-syslog: # for logs of type 'linux-syslog'
-      patterns:
-      - %{SYSLOGLINE}
-    apache-access: # for logs of type 'apache-error'
-      patterns:
-      - %{COMBINEDAPACHELOG}
-- date:
-    linux-syslog:  # for logs of type 'linux-syslog'
-      # Look for a field 'timestamp' with this format, parse and it for the timestamp
-      # This field comes from the SYSLOGLINE pattern
-      timestamp: "%b %e %H:%M:%S"
-      timestamp8601: ISO8601
-    apache-access:
-      timestamp: "%d/%b/%Y:%H:%M:%S %Z"
-outputs:
-- stdout:///
-- elasticsearch://localhost:9200/logs/all
-# But we could write to mongodb, too.
-#  - mongodb://localhost/parsedlogs
-# And also write to an AMQP topic
-#  - amqp://localhost/topic/parsedlogs
-# Write to stdout ... etc.
-# - stdout:///
diff --git a/etc/logstash-stomp-input.yaml b/etc/logstash-stomp-input.yaml
deleted file mode 100644
index b45132076e3..00000000000
--- a/etc/logstash-stomp-input.yaml
+++ /dev/null
@@ -1,7 +0,0 @@
---- 
-inputs:
-  stomp:
-    - stomp://logs:password@localhost:6163/topic/logs
-outputs:
-- stdout:///
-
diff --git a/etc/logstash-stomp.yaml b/etc/logstash-stomp.yaml
deleted file mode 100644
index f34dd65002c..00000000000
--- a/etc/logstash-stomp.yaml
+++ /dev/null
@@ -1,7 +0,0 @@
---- 
-inputs:
-  tail-syslog:
-  - /var/log/syslog
-outputs:
-- stomp://logs:password@localhost:6163/topic/logs
-
diff --git a/etc/prod.yaml b/etc/prod.yaml
deleted file mode 100644
index a27109a46fc..00000000000
--- a/etc/prod.yaml
+++ /dev/null
@@ -1,53 +0,0 @@
---- 
-inputs:
-  all:
-  - amqp://activemq/topic/logstash-events
-  linux-syslog:
-  - /var/log/messages
-  - /var/log/kern.log
-  - /var/log/auth.log
-  - /var/log/user.log
-  apache-access:
-  - /var/log/apache2/access.log
-  apache-error:
-  - /var/log/apache2/error.log
-  testing:
-  - /tmp/logstashtest.log
-filters:
-- grok:
-    linux-syslog: # for logs of type 'linux-syslog'
-      patterns:
-      - %{SYSLOGLINE}
-    apache-access: # for logs of type 'apache-error'
-      patterns:
-      - %{COMBINEDAPACHELOG}
-    nagios:
-      patterns:
-      - %{NAGIOSLOGLINE}
-    loggly:
-      patterns:
-      - %{JAVASTACKTRACEPART}
-    testing:
-      patterns:
-      - %{JAVASTACKTRACEPART}
-- date:
-    linux-syslog:  # for logs of type 'linux-syslog'
-      # Look for a field 'timestamp' with this format, parse and it for the timestamp
-      # This field comes from the SYSLOGLINE pattern
-      timestamp: "%b %e %H:%M:%S"
-      timestamp8601: ISO8601
-    apache-access:
-      timestamp: "%d/%b/%Y:%H:%M:%S %Z"
-    nagios:
-      epochtime: %s
-- multiline:
-    supervisorlogs:
-      pattern: ^\s
-      what: previous
-    testing:
-      pattern: ^\s
-      what: previous
-outputs:
-- stdout:///
-#- elasticsearch://localhost:9200/logstash/all
-- "elasticsearch://localhost:9200/logstash/all_river?method=river&type=rabbitmq&host=activemq&user=guest&pass=guest&vhost=/&queue=es"
diff --git a/etc/tograylog.yaml b/etc/tograylog.yaml
deleted file mode 100644
index 5f5e9f3b017..00000000000
--- a/etc/tograylog.yaml
+++ /dev/null
@@ -1,37 +0,0 @@
---- 
-inputs:
-  linux-syslog:
-  - /var/log/messages
-  - /var/log/kern.log
-  - /var/log/auth.log
-  - /var/log/user.log
-  apache-access:
-  - /var/log/apache2/access.log
-  - /home/jls/logs/access_log
-  apache-error:
-  - /var/log/apache2/error.log
-  - /home/jls/logs/error_log
-filters:
-- grok:
-    linux-syslog: # for logs of type 'linux-syslog'
-      patterns:
-      - %{SYSLOGLINE}
-    apache-access: # for logs of type 'apache-error'
-      patterns:
-      - %{COMBINEDAPACHELOG}
-    nagios:
-      patterns:
-      - %{NAGIOSLOGLINE}
-- date:
-    linux-syslog:  # for logs of type 'linux-syslog'
-      # Look for a field 'timestamp' with this format, parse and it for the timestamp
-      # This field comes from the SYSLOGLINE pattern
-      timestamp: "%b %e %H:%M:%S"
-      timestamp8601: ISO8601
-    apache-access:
-      timestamp: "%d/%b/%Y:%H:%M:%S %Z"
-    nagios:
-      epochtime: %s
-outputs:
-- stdout:///
-- gelf://localhost/
diff --git a/examples/sample-agent-in-ruby.rb b/examples/sample-agent-in-ruby.rb
deleted file mode 100644
index 6f19a46fe43..00000000000
--- a/examples/sample-agent-in-ruby.rb
+++ /dev/null
@@ -1,56 +0,0 @@
-#!/usr/bin/env ruby
-$: << "lib"
-
-require "rubygems"
-require "logstash/agent"
-require "yaml"
-
-collector_config = YAML.load <<"YAML"
----
-inputs:
-  foo:
-  - internal:///
-outputs:
-- amqp://localhost/topic/logstash/testing
-YAML
-
-receiver_config = YAML.load <<"YAML"
----
-inputs:
-  foo:
-  - amqp://localhost/topic/logstash/testing
-outputs:
-- internal:///
-YAML
-
-collector_agent = LogStash::Agent.new(collector_config)
-receiver_agent = LogStash::Agent.new(receiver_config)
-
-data = ["hello world", "foo", "bar"]
-
-EventMachine::run do
-  receiver_agent.register
-  collector_agent.register
-
-  EM::next_tick do
-    # Register output callback on the receiver
-    receiver_agent.outputs\
-        .find { |o| o.is_a?(LogStash::Outputs::Internal) }\
-        .callback do |event|
-      puts event
-      #puts expect.first == event.message
-      #expect.shift
-      #agent.stop
-    end
-
-    EM::next_tick do
-      # Send input to the collector
-      expect = data.clone
-      input = collector_agent.inputs\
-        .find { |i| i.is_a?(LogStash::Inputs::Internal) }
-      channel = input.channel
-      data.each { |message| channel.push(message) }
-    end
-  end
-end
-
diff --git a/examples/test.rb b/examples/test.rb
deleted file mode 100644
index 2c73cb3c719..00000000000
--- a/examples/test.rb
+++ /dev/null
@@ -1,38 +0,0 @@
-#!/usr/bin/env ruby
-#
-# How to trigger the 'evil ip' message:
-# % logger -t "pantscon" "naughty host 14.33.24.55 $RANDOM"
-
-require "rubygems"
-require "logstash/agent"
-
-class MyAgent < LogStash::Agent
-  def receive(event)
-    filter(event)  # Invoke any filters
-
-    return unless event["progname"][0] == "pantscon"
-    return unless event.message =~ /naughty host/
-    event["IP"].each do |ip|
-      next unless ip.length > 0
-      puts "Evil IP: #{ip}"
-    end
-  end # def receive
-end # class MyAgent
-
-# Read a local file, parse it, and react accordingly (see MyAgent#receive)
-agent = MyAgent.new({
-  "input" => [
-    "/var/log/messages",
-  ],
-  "filter" => [ "grok" ],
-})
-agent.run
-
-# Read messages that we expect to be parsed by another agent. Reads
-# a particular AMQP topic for messages
-#agent = MyAgent.new({
-  #"input" => [
-    #"amqp://localhost/topic/parsed",
-  #]
-#})
-#agent.run
diff --git a/gembag.rb b/gembag.rb
new file mode 100644
index 00000000000..093c1aee510
--- /dev/null
+++ b/gembag.rb
@@ -0,0 +1,74 @@
+#!/usr/bin/env ruby
+
+require "logstash/environment"
+
+# set gem paths here to help find the required gems below
+ENV["GEM_PATH"] = LogStash::Environment.gem_home
+ENV["GEM_HOME"] = LogStash::Environment.gem_home
+
+require "rubygems/specification"
+require "rubygems/commands/install_command"
+require "logstash/JRUBY-PR1448" if RUBY_PLATFORM == "java" && Gem.win_platform?
+
+
+def install_gem(name, requirement, target)
+  puts "Fetching and installing gem: #{name} (#{requirement})"
+
+  installer = Gem::Commands::InstallCommand.new
+  installer.options[:generate_rdoc] = false
+  installer.options[:generate_ri] = false
+  installer.options[:version] = requirement
+  installer.options[:args] = [name]
+  installer.options[:install_dir] = target
+
+  # ruby 2.0.0 / rubygems 2.x; disable documentation generation
+  installer.options[:document] = []
+  begin
+    installer.execute
+  rescue Gem::SystemExitException => e
+    if e.exit_code != 0
+      puts "Installation of #{name} failed"
+      raise
+    end
+  end
+end # def install_gem
+
+# Ensure bundler is available.
+begin
+  gem("bundler", ">=1.7.3")
+rescue Gem::LoadError => e
+  install_gem("bundler", ">= 1.7.3", LogStash::Environment.gem_home)
+end
+
+require "bundler/cli"
+
+# Monkeypatch bundler to write a .lock file specific to the version of ruby.
+# This keeps MRI/JRuby/RBX from conflicting over the Gemfile.lock updates
+module Bundler
+  module SharedHelpers
+    def default_lockfile
+      ruby = "#{LogStash::Environment.ruby_engine}-#{LogStash::Environment.gem_ruby_version}"
+      Pathname.new("#{default_gemfile}.#{ruby}.lock")
+    end
+  end
+end
+
+if LogStash::Environment.ruby_engine == "rbx"
+  begin
+    gem("rubysl")
+  rescue Gem::LoadError => e
+    install_gem("rubysl", ">= 0", LogStash::Environment.gem_home)
+  end
+end
+
+# Try installing a few times in case we hit the "bad_record_mac" ssl error during installation.
+10.times do
+  begin
+    Bundler::CLI.start(["install", "--gemfile=#{LogStash::Environment::GEMFILE_PATH}", "--path", LogStash::Environment::BUNDLE_DIR, "--standalone", "--clean", "--without", "development"])
+    break
+  rescue Gem::RemoteFetcher::FetchError => e
+    puts e.message
+    puts e.backtrace.inspect
+    sleep 5 #slow down a bit before retry
+  end
+end
diff --git a/lib/logstash-event.rb b/lib/logstash-event.rb
new file mode 100644
index 00000000000..0f44322944b
--- /dev/null
+++ b/lib/logstash-event.rb
@@ -0,0 +1,2 @@
+# encoding: utf-8
+require "logstash/event"
diff --git a/lib/logstash.rb b/lib/logstash.rb
index 60a4467c7eb..4837cb3e181 100644
--- a/lib/logstash.rb
+++ b/lib/logstash.rb
@@ -1,3 +1,4 @@
+# encoding: utf-8
 require "logstash/agent"
 require "logstash/event"
 require "logstash/namespace"
diff --git a/lib/logstash/JRUBY-PR1448.rb b/lib/logstash/JRUBY-PR1448.rb
new file mode 100644
index 00000000000..282862a5c15
--- /dev/null
+++ b/lib/logstash/JRUBY-PR1448.rb
@@ -0,0 +1,32 @@
+# This patch fixes a problem that exists in JRuby prior to 1.7.11 where the
+# ruby binary path used by rubygems is malformed on Windows, causing
+# dependencies to not install cleanly when using `.\bin\logstash.bat deps`.
+# This monkeypatch can probably be removed once it's unlikely that people
+# are still using JRuby older than 1.7.11.
+  class << Gem
+    def ruby
+      ruby_path = original_ruby
+      ruby_path = "java -jar #{jar_path(ruby_path)}" if jarred_path?(ruby_path)
+      ruby_path
+    end
+
+    def jarred_path?(p)
+      p =~ /^file:/
+    end
+
+    # A jar path looks like this on non-Windows platforms:
+    #   file:/path/to/file.jar!/path/within/jar/to/file.txt
+    # and like this on Windows:
+    #   file:/C:/path/to/file.jar!/path/within/jar/to/file.txt
+    #
+    # This method returns:
+    #   /path/to/file.jar
+    # or
+    #   C:/path/to/file.jar
+    # as appropriate.
+    def jar_path(p)
+      path = p.sub(/^file:/, "").sub(/!.*/, "")
+      path = path.sub(/^\//, "") if win_platform? && path =~ /^\/[A-Za-z]:/
+      path
+    end
+  end
diff --git a/lib/logstash/agent.rb b/lib/logstash/agent.rb
index 917e805fb81..0a6ce1da6bd 100644
--- a/lib/logstash/agent.rb
+++ b/lib/logstash/agent.rb
@@ -1,416 +1,308 @@
-#TODO(sissel): Maybe this will help jruby jar issues?
-#$: << File.join(File.dirname(__FILE__), "../"
-
-require "java"
-require "logstash/config/file"
-require "logstash/filters"
-require "logstash/inputs"
-require "logstash/logging"
-require "logstash/multiqueue"
-require "logstash/namespace"
-require "logstash/outputs"
-require "logstash/util"
-require "optparse"
-require "uri"
-
-# TODO(sissel): only enable this if we are in debug mode.
-# JRuby.objectspace=true
-
-# Collect logs, ship them out.
-class LogStash::Agent
-  attr_reader :config
-  attr_reader :inputs
-  attr_reader :outputs
-  attr_reader :filters
-  attr_accessor :logger
-
-  # flags
-  attr_reader :config_file
-  attr_reader :daemonize
-  attr_reader :logfile
-  attr_reader :verbose
-
-  public
-  def initialize
-    log_to(STDERR)
-
-    # flag/config defaults
-    @verbose = 0
-    @daemonize = false
-
-    @threads = {}
-    @outputs = []
-    @inputs = []
-    @filters = []
-
-    @plugin_paths = []
-
-    # Add logstash's plugin path (plugin paths must contain inputs, outputs, filters)
-    @plugin_paths << File.dirname(__FILE__)
-
-    # TODO(sissel): Other default plugin paths?
-
-    Thread::abort_on_exception = true
-  end # def initialize
-
-  public
-  def log_to(target)
-    @logger = LogStash::Logger.new(target)
-  end # def log_to
-
-  public
-  def argv=(argv)
-    @argv = argv
+# encoding: utf-8
+require "clamp" # gem 'clamp'
+require "logstash/environment"
+require "logstash/errors"
+require "logstash/environment"
+LogStash::Environment.load_locale!
+
+class LogStash::Agent < Clamp::Command
+  option ["-f", "--config"], "CONFIG_PATH",
+    I18n.t("logstash.agent.flag.config"),
+    :attribute_name => :config_path
+
+  option "-e", "CONFIG_STRING",
+    I18n.t("logstash.agent.flag.config-string"),
+    :default => "", :attribute_name => :config_string
+
+  option ["-w", "--filterworkers"], "COUNT",
+    I18n.t("logstash.agent.flag.filterworkers"),
+    :attribute_name => :filter_workers, :default => 1, &:to_i
+
+  option "--watchdog-timeout", "SECONDS",
+    I18n.t("logstash.agent.flag.watchdog-timeout"),
+    :default => 10, &:to_f
+
+  option ["-l", "--log"], "FILE",
+    I18n.t("logstash.agent.flag.log"),
+    :attribute_name => :log_file
+
+  # Old support for the '-v' flag'
+  option "-v", :flag,
+    I18n.t("logstash.agent.flag.verbosity"),
+    :attribute_name => :verbosity, :multivalued => true
+
+  option "--quiet", :flag, I18n.t("logstash.agent.flag.quiet")
+  option "--verbose", :flag, I18n.t("logstash.agent.flag.verbose")
+  option "--debug", :flag, I18n.t("logstash.agent.flag.debug")
+
+  option ["-V", "--version"], :flag,
+    I18n.t("logstash.agent.flag.version")
+
+  option ["-p", "--pluginpath"] , "PATH",
+    I18n.t("logstash.agent.flag.pluginpath"),
+    :multivalued => true,
+    :attribute_name => :plugin_paths
+
+  option ["-t", "--configtest"], :flag,
+    I18n.t("logstash.agent.flag.configtest"),
+    :attribute_name => :config_test
+
+  # Emit a warning message.
+  def warn(message)
+    # For now, all warnings are fatal.
+    raise LogStash::ConfigurationError, message
+  end # def warn
+
+  # Emit a failure message and abort.
+  def fail(message)
+    raise LogStash::ConfigurationError, message
+  end # def fail
+
+  def report(message)
+    # Print to stdout just in case we're logging to a file
+    puts message
+    @logger.log(message) if log_file
   end
 
-  private
-  def options(opts)
-    opts.on("-f CONFIGFILE", "--config CONFIGFILE",
-            "Load the logstash config from a specific file") do |arg|
-      @config_file = arg
+  # Run the agent. This method is invoked after clamp parses the
+  # flags given to this program.
+  def execute
+    require "logstash/pipeline"
+    require "cabin" # gem 'cabin'
+    require "logstash/plugin"
+    @logger = Cabin::Channel.get(LogStash)
+
+    if version?
+      show_version
+      return 0
     end
 
-    opts.on("-d", "--daemonize", "Daemonize (default is run in foreground)") do 
-      @daemonize = true
-    end
+    # temporarily send logs to stdout as well if a --log is specified
+    # and stdout appears to be a tty
+    show_startup_errors = log_file && STDOUT.tty?
 
-    opts.on("-l", "--log FILE", "Log to a given path. Default is stdout.") do |path|
-      @logfile = path
+    if show_startup_errors
+      stdout_logs = @logger.subscribe(STDOUT)
     end
+    configure
 
-    opts.on("-v", "Increase verbosity") do
-      @verbose += 1
+    # You must specify a config_string or config_path
+    if @config_string.nil? && @config_path.nil?
+      fail(help + "\n" + I18n.t("logstash.agent.missing-configuration"))
     end
 
-    opts.on("-p PLUGIN_PATH", "--pluginpath PLUGIN_PATH",
-            "A colon-delimited path to find plugins in.") do |path|
-      path.split(":").each do |p|
-        @plugin_paths << p unless @plugin_paths.include?(p)
+    @config_string = @config_string.to_s
+
+    if @config_path
+      # Append the config string.
+      # This allows users to provide both -f and -e flags. The combination
+      # is rare, but useful for debugging.
+      @config_string = @config_string + load_config(@config_path)
+    else
+      # include a default stdin input if no inputs given
+      if @config_string !~ /input *{/
+        @config_string += "input { stdin { type => stdin } }"
       end
-    end
-  end # def options
-
-  # Parse options.
-  private
-  def parse_options
-    @opts = OptionParser.new
-
-    # Step one is to add agent flags.
-    options(@opts)
-
-    # TODO(sissel): Check for plugin_path flags, add them to @plugin_paths.
-    @argv.each_with_index do |arg, index|
-      next unless arg =~ /^(?:-p|--pluginpath)(?:=(.*))?$/
-      path = $1
-      if path.nil?
-        path = @argv[index + 1]
+      # include a default stdout output if no outputs given
+      if @config_string !~ /output *{/
+        @config_string += "output { stdout { codec => rubydebug } }"
       end
-
-      @plugin_paths += path.split(":")
-    end # @argv.each
-
-    # At this point, we should load any plugin-specific flags.
-    # These are 'unknown' flags that begin --<plugin>-flag
-    # Put any plugin paths into the ruby library path for requiring later.
-    @plugin_paths.each do |p|
-      @logger.info "Adding #{p.inspect} to ruby load path"
-      $:.unshift p
     end
 
-    # TODO(sissel): Go through all inputs, filters, and outputs to get the flags.
-    # Add plugin flags to @opts
-
-    # Load any plugins that we have flags for.
-    # TODO(sissel): The --<plugin> flag support currently will load
-    # any matching plugins input, output, or filter. This means, for example,
-    # that the 'amqp' input *and* output plugin will be loaded if you pass
-    # --amqp-foo flag. This might cause confusion, but it seems reasonable for
-    # now that any same-named component will have the same flags.
-    plugins = []
-    @argv.each do |arg|
-      # skip things that don't look like plugin flags
-      next unless arg =~ /^--[A-z0-9]+-/ 
-      name = arg.split("-")[2]  # pull the plugin name out
-
-      # Try to load any plugin by that name
-      %w{inputs outputs filters}.each do |component|
-        @plugin_paths.each do |path|
-          plugin = File.join(path, component, name) + ".rb"
-          @logger.debug("Flag #{arg} found; trying to load #{plugin}")
-          if File.file?(plugin)
-            @logger.info("Loading plugin #{plugin}")
-            require plugin
-            [LogStash::Inputs, LogStash::Filters, LogStash::Outputs].each do |c|
-              # If we get flag --foo-bar, check for LogStash::Inputs::Foo
-              # and add any options to our option parser.
-              klass_name = name.capitalize
-              if c.const_defined?(klass_name)
-                @logger.info("Found plugin class #{c}::#{klass_name})")
-                klass = c.const_get(klass_name)
-                # See LogStash::Config::Mixin::DSL#options
-                klass.options(@opts)
-                plugins << klass
-              end # c.const_defined?
-            end # each component type (input/filter/outputs)
-          end # if File.file?(plugin)
-        end # @plugin_paths.each
-      end # %{inputs outputs filters}.each
-
-      #if !found
-        #@logger.fatal("Flag #{arg.inspect} requires plugin #{name}, but no plugin found.")
-        #return false
-      #end
-    end # @remaining_args.each 
-   
     begin
-      @opts.parse!(@argv)
-    rescue OptionParser::InvalidOption => e
-      @logger.info e
-      raise e
+      pipeline = LogStash::Pipeline.new(@config_string)
+    rescue LoadError => e
+      fail("Configuration problem.")
     end
- 
-    return true
-  end # def parse_options
 
-  private
-  def configure
-    if @config_file.nil? || @config_file.empty?
-      @logger.fatal "No config file given. (missing -f or --config flag?)"
-      @logger.fatal @opts.help
-      raise "Configuration problem"
+    # Make SIGINT shutdown the pipeline.
+    trap_id = Stud::trap("INT") do
+      @logger.warn(I18n.t("logstash.agent.interrupted"))
+      pipeline.shutdown
     end
 
-    if !File.exist?(@config_file)
-      @logger.fatal "Config file '#{@config_file}' does not exist."
-      raise "Configuration problem"
+    Stud::trap("HUP") do
+      @logger.info(I18n.t("logstash.agent.sighup"))
+      configure_logging(log_file)
     end
 
-    if @daemonize
-      @logger.fatal "Can't daemonize, no support yet in JRuby."
-      raise "Can't daemonize, no fork in JRuby."
-    end
+    pipeline.configure("filter-workers", filter_workers)
 
-    if @logfile
-      logfile = File.open(@logfile, "w")
-      STDOUT.reopen(logfile)
-      STDERR.reopen(logfile)
-    elsif @daemonize
-      devnull = File.open("/dev/null", "w")
-      STDOUT.reopen(devnull)
-      STDERR.reopen(devnull)
+    # Stop now if we are only asking for a config test.
+    if config_test?
+      report "Configuration OK"
+      return
     end
 
-    if @verbose >= 3  # Uber debugging.
-      @logger.level = Logger::DEBUG
-      $DEBUG = true
-    elsif @verbose == 2 # logstash debug logs
-      @logger.level = Logger::DEBUG
-    elsif @verbose == 1 # logstash info logs
-      @logger.level = Logger::INFO
-    else # Default log level
-      @logger.level = Logger::WARN
-    end
-  end # def configure
+    @logger.unsubscribe(stdout_logs) if show_startup_errors
 
-  public
-  def run(&block)
-    LogStash::Util::set_thread_name(self.class.name)
+    # TODO(sissel): Get pipeline completion status.
+    pipeline.run
+    return 0
+  rescue LogStash::ConfigurationError => e
+    @logger.unsubscribe(stdout_logs) if show_startup_errors
+    report I18n.t("logstash.agent.error", :error => e)
+    if !config_test?
+      report I18n.t("logstash.agent.configtest-flag-information")
+    end
+    return 1
+  rescue => e
+    @logger.unsubscribe(stdout_logs) if show_startup_errors
+    report I18n.t("oops", :error => e)
+    report e.backtrace if @logger.debug? || $DEBUGLIST.include?("stacktrace")
+    return 1
+  ensure
+    @log_fd.close if @log_fd
+    Stud::untrap("INT", trap_id) unless trap_id.nil?
+  end # def execute
+
+  def show_version
+    show_version_logstash
+
+    if [:info, :debug].include?(verbosity?) || debug? || verbose?
+      show_version_ruby
+
+      if RUBY_PLATFORM == "java"
+        show_version_java
+        show_version_elasticsearch
+      end
 
-    ok = parse_options
-    if !ok
-      raise "Option parsing failed. See error log."
+      if [:debug].include?(verbosity?) || debug?
+        show_gems
+      end
     end
+  end # def show_version
+
+  def show_version_logstash
+    require "logstash/version"
+    puts "logstash #{LOGSTASH_VERSION}"
+  end # def show_version_logstash
+
+  def show_version_ruby
+    puts RUBY_DESCRIPTION
+  end # def show_version_ruby
+
+  def show_version_elasticsearch
+    LogStash::Environment.load_elasticsearch_jars!
+
+    $stdout.write("Elasticsearch: ");
+    org.elasticsearch.Version::main([])
+  end # def show_version_elasticsearch
+
+  def show_version_java
+    properties = java.lang.System.getProperties
+    puts "java #{properties["java.version"]} (#{properties["java.vendor"]})"
+    puts "jvm #{properties["java.vm.name"]} / #{properties["java.vm.version"]}"
+  end # def show_version_java
+
+  def show_gems
+    require "rubygems"
+    Gem::Specification.each do |spec|
+      puts "gem #{spec.name} #{spec.version}"
+    end
+  end # def show_gems
 
-    configure
+  # Do any start-time configuration.
+  #
+  # Log file stuff, plugin path checking, etc.
+  def configure
+    configure_logging(log_file)
+    configure_plugin_path(plugin_paths) if !plugin_paths.nil?
+  end # def configure
 
-    # Load the config file
-    config = LogStash::Config::File.new(@config_file)
-
-    run_with_config(config, &block)
-  end # def run
-
-  def run_with_config(config)
-    config.parse do |plugin|
-      # 'plugin' is a has containing:
-      #   :type => the base class of the plugin (LogStash::Inputs::Base, etc)
-      #   :plugin => the class of the plugin (LogStash::Inputs::File, etc)
-      #   :parameters => hash of key-value parameters from the config.
-      type = plugin[:type].config_name  # "input" or "filter" etc...
-      klass = plugin[:plugin]
-
-      # Create a new instance of a plugin, called like:
-      # -> LogStash::Inputs::File.new( params )
-      instance = klass.new(plugin[:parameters])
-      instance.logger = @logger
-
-      case type
-        when "input"
-          @inputs << instance
-        when "filter"
-          @filters << instance
-        when "output"
-          @outputs << instance
+  # Point logging at a specific path.
+  def configure_logging(path)
+    # Set with the -v (or -vv...) flag
+    if quiet?
+      @logger.level = :error
+    elsif verbose?
+      @logger.level = :info
+    elsif debug?
+      @logger.level = :debug
+    else
+      # Old support for the -v and -vv stuff.
+      if verbosity? && verbosity?.any?
+        # this is an array with length of how many times the flag is given
+        if verbosity?.length == 1
+          @logger.warn("The -v flag is deprecated and will be removed in a future release. You should use --verbose instead.")
+          @logger.level = :info
         else
-          @logger.error("Unknown config type '#{type}'")
-          exit 1
-      end # case type
-    end # config.parse
+          @logger.warn("The -vv flag is deprecated and will be removed in a future release. You should use --debug instead.")
+          @logger.level = :debug
+        end
+      else
+        @logger.level = :warn
+      end
 
-    if @inputs.length == 0 or @outputs.length == 0
-      raise "Must have both inputs and outputs configured."
     end
 
-    # NOTE(petef) we should use a SizedQueue here (w/config params for size)
-    #filter_queue = Queue.new
-    filter_queue = SizedQueue.new(10)
-    output_queue = LogStash::MultiQueue.new
-
-    ready_queue = Queue.new
-
-    input_target = @filters.length > 0 ? filter_queue : output_queue
-    # Start inputs
-    @inputs.each do |input|
-      @logger.info(["Starting input", input])
-      @threads[input] = Thread.new(input_target) do |input_target|
-        LogStash::Util::set_thread_name("input|#{input.inspect}")
-        input.logger = @logger
-        input.register
-        ready_queue << input
-        input.run(input_target)
-      end # new thread for thsi input
-    end # @inputs.each
-
-    # Create N filter-worker threads
-    if @filters.length > 0
-      1.times do |n|
-        @logger.info("Starting filter worker thread #{n}")
-        @threads["filter|worker|#{n}"] = Thread.new do
-          LogStash::Util::set_thread_name("filter|worker|#{n}")
-          @filters.each do |filter|
-            filter.logger = @logger
-            filter.register
-          end
-
-          while event = filter_queue.pop
-            filters.each do |filter|
-              filter.filter(event)
-              if event.cancelled?
-                @logger.debug({:message => "Event cancelled",
-                               :event => event,
-                               :filter => filter.class,
-                })
-                break
-              end
-            end # filters.each
-
-            @logger.debug(["Event finished filtering", event])
-            output_queue.push(event) unless event.cancelled?
-          end # event pop
-        end # Thread.new
-      end # N.times
-    end # if @filters.length > 0
-
-
-    # Create output threads
-    @outputs.each do |output|
-      queue = SizedQueue.new(10)
-      output_queue.add_queue(queue)
-      @threads["outputs/#{output.to_s}"] = Thread.new(queue) do |queue|
-        output.register
-        ready_queue << output
-        begin
-          LogStash::Util::set_thread_name("output/#{output.to_s}")
-          output.logger = @logger
-
-          while event = queue.pop do
-            @logger.debug("Sending event to #{output.to_s}")
-            output.receive(event)
-          end
-        rescue Exception => e
-          @logger.warn(["Output #{output.to_s} thread exception", e])
-          @logger.debug(["Output #{output.to_s} thread exception backtrace",
-                         e.backtrace])
-          # TODO(sissel): should we abort after too many failures?
-          retry
-        end # begin/rescue
-      end # Thread.new
-    end # @outputs.each
-
-    # Wait for all inputs and outputs to be registered.
-    wait_count = outputs.size + inputs.size
-    while wait_count > 0 and ready_queue.pop 
-      wait_count -= 1
+    if log_file
+      # TODO(sissel): Implement file output/rotation in Cabin.
+      # TODO(sissel): Catch exceptions, report sane errors.
+      begin
+        @log_fd.close if @log_fd
+        @log_fd = File.new(path, "a")
+      rescue => e
+        fail(I18n.t("logstash.agent.configuration.log_file_failed",
+                    :path => path, :error => e))
+      end
+
+      puts "Sending logstash logs to #{path}."
+      @logger.unsubscribe(@logger_subscription) if @logger_subscription
+      @logger_subscription = @logger.subscribe(@log_fd)
+    else
+      @logger.subscribe(STDOUT)
     end
 
-    # yield to a block in case someone's waiting for us to be done setting up
-    # like tests, etc.
-    yield if block_given?
+    # TODO(sissel): redirect stdout/stderr to the log as well
+    # http://jira.codehaus.org/browse/JRUBY-7003
+  end # def configure_logging
 
-    # TODO(sissel): Monitor what's going on? Sleep forever? what?
-    while sleep 5
+  # Validate and add any paths to the list of locations
+  # logstash will look to find plugins.
+  def configure_plugin_path(paths)
+    # Append any plugin paths to the ruby search path
+    paths.each do |path|
+      # Verify the path exists
+      if !Dir.exists?(path)
+        warn(I18n.t("logstash.agent.configuration.plugin_path_missing",
+                    :path => path))
+
+      end
+
+      # TODO(sissel): Verify the path looks like the correct form.
+      # aka, there must be file in path/logstash/{inputs,codecs,filters,outputs}/*.rb
+      plugin_glob = File.join(path, "logstash", "{inputs,codecs,filters,outputs}", "*.rb")
+      if Dir.glob(plugin_glob).empty?
+        @logger.warn(I18n.t("logstash.agent.configuration.no_plugins_found",
+                    :path => path, :plugin_glob => plugin_glob))
+      end
+
+      # We push plugin paths to the front of the LOAD_PATH so that folks
+      # can override any core logstash plugins if they need to.
+      @logger.debug("Adding plugin path", :path => path)
+      $LOAD_PATH.unshift(path)
     end
-  end # def run_with_config
-
-  public
-  def stop
-    # TODO(petef): Stop inputs, fluch outputs, wait for finish,
-    # then stop the event loop
-  end # def stop
-
-  protected
-  def filter(event)
-    @filters.each do |f|
-      f.filter(event)
-      break if event.cancelled?
+  end # def configure_plugin_path
+
+  def load_config(path)
+    path = File.join(path, "*") if File.directory?(path)
+
+    if Dir.glob(path).length == 0
+      fail(I18n.t("logstash.agent.configuration.file-not-found", :path => path))
     end
-  end # def filter
-
-  protected
-  def output(event)
-    # TODO(sissel): write to a multiqueue and do 1 thread per output?
-    @outputs.each do |o|
-      o.receive(event)
-    end # each output
-  end # def output
-
-  protected
-  # Process a message
-  def receive(event)
-    filter(event)
-
-    if !event.cancelled?
-      output(event)
+
+    config = ""
+    Dir.glob(path).sort.each do |file|
+      next unless File.file?(file)
+      if file.match(/~$/)
+        @logger.debug("NOT reading config file because it is a temp file", :file => file)
+        next
+      end
+      @logger.debug("Reading config file", :file => file)
+      config << File.read(file) + "\n"
     end
-  end # def input
-
-  public
-  def register_signal_handler
-    # TODO(sissel): This doesn't work well in jruby since ObjectSpace is disabled
-    # by default.
-    Signal.trap("USR2") do
-      # TODO(sissel): Make this a function.
-      #counts = Hash.new { |h,k| h[k] = 0 }
-      #ObjectSpace.each_object do |obj|
-        #counts[obj.class] += 1
-      #end
-
-      @logger.info("SIGUSR1 received. Dumping state")
-      @logger.info("#{self.class.name} config")
-      @logger.info(["  Inputs:", @inputs])
-      @logger.info(["  Filters:", @filters])
-      @logger.info(["  Outputs:", @outputs])
-
-      #@logger.info("Dumping counts of objects by class")
-      #counts.sort { |a,b| a[1] <=> b[1] or a[0] <=> b[0] }.each do |key, value|
-        #@logger.info("Class: [#{value}] #{key}")
-      #end
-    end # SIGUSR1
-  end # def register_signal_handler
-end # class LogStash::Agent
+    return config
+  end # def load_config
 
-if __FILE__ == $0
-  $: << "net"
-  agent = LogStash::Agent.new
-  agent.argv = ARGV
-  agent.run
-end
+end # class LogStash::Agent
diff --git a/lib/logstash/certs/cacert.pem b/lib/logstash/certs/cacert.pem
new file mode 100644
index 00000000000..99b310bce91
--- /dev/null
+++ b/lib/logstash/certs/cacert.pem
@@ -0,0 +1,3895 @@
+##
+## ca-bundle.crt -- Bundle of CA Root Certificates
+##
+## Certificate data from Mozilla as of: Sat Dec 29 20:03:40 2012
+##
+## This is a bundle of X.509 certificates of public Certificate Authorities
+## (CA). These were automatically extracted from Mozilla's root certificates
+## file (certdata.txt).  This file can be found in the mozilla source tree:
+## http://mxr.mozilla.org/mozilla/source/security/nss/lib/ckfw/builtins/certdata.txt?raw=1
+##
+## It contains the certificates in PEM format and therefore
+## can be directly used with curl / libcurl / php_curl, or with
+## an Apache+mod_ssl webserver for SSL client authentication.
+## Just configure this file as the SSLCACertificateFile.
+##
+
+# @(#) $RCSfile: certdata.txt,v $ $Revision: 1.87 $ $Date: 2012/12/29 16:32:45 $
+
+GTE CyberTrust Global Root
+==========================
+-----BEGIN CERTIFICATE-----
+MIICWjCCAcMCAgGlMA0GCSqGSIb3DQEBBAUAMHUxCzAJBgNVBAYTAlVTMRgwFgYDVQQKEw9HVEUg
+Q29ycG9yYXRpb24xJzAlBgNVBAsTHkdURSBDeWJlclRydXN0IFNvbHV0aW9ucywgSW5jLjEjMCEG
+A1UEAxMaR1RFIEN5YmVyVHJ1c3QgR2xvYmFsIFJvb3QwHhcNOTgwODEzMDAyOTAwWhcNMTgwODEz
+MjM1OTAwWjB1MQswCQYDVQQGEwJVUzEYMBYGA1UEChMPR1RFIENvcnBvcmF0aW9uMScwJQYDVQQL
+Ex5HVEUgQ3liZXJUcnVzdCBTb2x1dGlvbnMsIEluYy4xIzAhBgNVBAMTGkdURSBDeWJlclRydXN0
+IEdsb2JhbCBSb290MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCVD6C28FCc6HrHiM3dFw4u
+sJTQGz0O9pTAipTHBsiQl8i4ZBp6fmw8U+E3KHNgf7KXUwefU/ltWJTSr41tiGeA5u2ylc9yMcql
+HHK6XALnZELn+aks1joNrI1CqiQBOeacPwGFVw1Yh0X404Wqk2kmhXBIgD8SFcd5tB8FLztimQID
+AQABMA0GCSqGSIb3DQEBBAUAA4GBAG3rGwnpXtlR22ciYaQqPEh346B8pt5zohQDhT37qw4wxYMW
+M4ETCJ57NE7fQMh017l93PR2VX2bY1QY6fDq81yx2YtCHrnAlU66+tXifPVoYb+O7AWXX1uw16OF
+NMQkpw0PlZPvy5TYnh+dXIVtx6quTx8itc2VrbqnzPmrC3p/
+-----END CERTIFICATE-----
+
+Thawte Server CA
+================
+-----BEGIN CERTIFICATE-----
+MIIDEzCCAnygAwIBAgIBATANBgkqhkiG9w0BAQQFADCBxDELMAkGA1UEBhMCWkExFTATBgNVBAgT
+DFdlc3Rlcm4gQ2FwZTESMBAGA1UEBxMJQ2FwZSBUb3duMR0wGwYDVQQKExRUaGF3dGUgQ29uc3Vs
+dGluZyBjYzEoMCYGA1UECxMfQ2VydGlmaWNhdGlvbiBTZXJ2aWNlcyBEaXZpc2lvbjEZMBcGA1UE
+AxMQVGhhd3RlIFNlcnZlciBDQTEmMCQGCSqGSIb3DQEJARYXc2VydmVyLWNlcnRzQHRoYXd0ZS5j
+b20wHhcNOTYwODAxMDAwMDAwWhcNMjAxMjMxMjM1OTU5WjCBxDELMAkGA1UEBhMCWkExFTATBgNV
+BAgTDFdlc3Rlcm4gQ2FwZTESMBAGA1UEBxMJQ2FwZSBUb3duMR0wGwYDVQQKExRUaGF3dGUgQ29u
+c3VsdGluZyBjYzEoMCYGA1UECxMfQ2VydGlmaWNhdGlvbiBTZXJ2aWNlcyBEaXZpc2lvbjEZMBcG
+A1UEAxMQVGhhd3RlIFNlcnZlciBDQTEmMCQGCSqGSIb3DQEJARYXc2VydmVyLWNlcnRzQHRoYXd0
+ZS5jb20wgZ8wDQYJKoZIhvcNAQEBBQADgY0AMIGJAoGBANOkUG7I/1Zr5s9dtuoMaHVHoqrC2oQl
+/Kj0R1HahbUgdJSGHg91yekIYfUGbTBuFRkC6VLAYttNmZ7iagxEOM3+vuNkCXDF/rFrKbYvScg7
+1CcEJRCXL+eQbcAoQpnXTEPew/UhbVSfXcNY4cDk2VuwuNy0e982OsK1ZiIS1ocNAgMBAAGjEzAR
+MA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZIhvcNAQEEBQADgYEAB/pMaVz7lcxG7oWDTSEwjsrZqG9J
+GubaUeNgcGyEYRGhGshIPllDfU+VPaGLtwtimHp1it2ITk6eQNuozDJ0uW8NxuOzRAvZim+aKZuZ
+GCg70eNAKJpaPNW15yAbi8qkq43pUdniTCxZqdq5snUb9kLy78fyGPmJvKP/iiMucEc=
+-----END CERTIFICATE-----
+
+Thawte Premium Server CA
+========================
+-----BEGIN CERTIFICATE-----
+MIIDJzCCApCgAwIBAgIBATANBgkqhkiG9w0BAQQFADCBzjELMAkGA1UEBhMCWkExFTATBgNVBAgT
+DFdlc3Rlcm4gQ2FwZTESMBAGA1UEBxMJQ2FwZSBUb3duMR0wGwYDVQQKExRUaGF3dGUgQ29uc3Vs
+dGluZyBjYzEoMCYGA1UECxMfQ2VydGlmaWNhdGlvbiBTZXJ2aWNlcyBEaXZpc2lvbjEhMB8GA1UE
+AxMYVGhhd3RlIFByZW1pdW0gU2VydmVyIENBMSgwJgYJKoZIhvcNAQkBFhlwcmVtaXVtLXNlcnZl
+ckB0aGF3dGUuY29tMB4XDTk2MDgwMTAwMDAwMFoXDTIwMTIzMTIzNTk1OVowgc4xCzAJBgNVBAYT
+AlpBMRUwEwYDVQQIEwxXZXN0ZXJuIENhcGUxEjAQBgNVBAcTCUNhcGUgVG93bjEdMBsGA1UEChMU
+VGhhd3RlIENvbnN1bHRpbmcgY2MxKDAmBgNVBAsTH0NlcnRpZmljYXRpb24gU2VydmljZXMgRGl2
+aXNpb24xITAfBgNVBAMTGFRoYXd0ZSBQcmVtaXVtIFNlcnZlciBDQTEoMCYGCSqGSIb3DQEJARYZ
+cHJlbWl1bS1zZXJ2ZXJAdGhhd3RlLmNvbTCBnzANBgkqhkiG9w0BAQEFAAOBjQAwgYkCgYEA0jY2
+aovXwlue2oFBYo847kkEVdbQ7xwblRZH7xhINTpS9CtqBo87L+pW46+GjZ4X9560ZXUCTe/LCaIh
+Udib0GfQug2SBhRz1JPLlyoAnFxODLz6FVL88kRu2hFKbgifLy3j+ao6hnO2RlNYyIkFvYMRuHM/
+qgeN9EJN50CdHDcCAwEAAaMTMBEwDwYDVR0TAQH/BAUwAwEB/zANBgkqhkiG9w0BAQQFAAOBgQAm
+SCwWwlj66BZ0DKqqX1Q/8tfJeGBeXm43YyJ3Nn6yF8Q0ufUIhfzJATj/Tb7yFkJD57taRvvBxhEf
+8UqwKEbJw8RCfbz6q1lu1bdRiBHjpIUZa4JMpAwSremkrj/xw0llmozFyD4lt5SZu5IycQfwhl7t
+UCemDaYj+bvLpgcUQg==
+-----END CERTIFICATE-----
+
+Equifax Secure CA
+=================
+-----BEGIN CERTIFICATE-----
+MIIDIDCCAomgAwIBAgIENd70zzANBgkqhkiG9w0BAQUFADBOMQswCQYDVQQGEwJVUzEQMA4GA1UE
+ChMHRXF1aWZheDEtMCsGA1UECxMkRXF1aWZheCBTZWN1cmUgQ2VydGlmaWNhdGUgQXV0aG9yaXR5
+MB4XDTk4MDgyMjE2NDE1MVoXDTE4MDgyMjE2NDE1MVowTjELMAkGA1UEBhMCVVMxEDAOBgNVBAoT
+B0VxdWlmYXgxLTArBgNVBAsTJEVxdWlmYXggU2VjdXJlIENlcnRpZmljYXRlIEF1dGhvcml0eTCB
+nzANBgkqhkiG9w0BAQEFAAOBjQAwgYkCgYEAwV2xWGcIYu6gmi0fCG2RFGiYCh7+2gRvE4RiIcPR
+fM6fBeC4AfBONOziipUEZKzxa1NfBbPLZ4C/QgKO/t0BCezhABRP/PvwDN1Dulsr4R+AcJkVV5MW
+8Q+XarfCaCMczE1ZMKxRHjuvK9buY0V7xdlfUNLjUA86iOe/FP3gx7kCAwEAAaOCAQkwggEFMHAG
+A1UdHwRpMGcwZaBjoGGkXzBdMQswCQYDVQQGEwJVUzEQMA4GA1UEChMHRXF1aWZheDEtMCsGA1UE
+CxMkRXF1aWZheCBTZWN1cmUgQ2VydGlmaWNhdGUgQXV0aG9yaXR5MQ0wCwYDVQQDEwRDUkwxMBoG
+A1UdEAQTMBGBDzIwMTgwODIyMTY0MTUxWjALBgNVHQ8EBAMCAQYwHwYDVR0jBBgwFoAUSOZo+SvS
+spXXR9gjIBBPM5iQn9QwHQYDVR0OBBYEFEjmaPkr0rKV10fYIyAQTzOYkJ/UMAwGA1UdEwQFMAMB
+Af8wGgYJKoZIhvZ9B0EABA0wCxsFVjMuMGMDAgbAMA0GCSqGSIb3DQEBBQUAA4GBAFjOKer89961
+zgK5F7WF0bnj4JXMJTENAKaSbn+2kmOeUJXRmm/kEd5jhW6Y7qj/WsjTVbJmcVfewCHrPSqnI0kB
+BIZCe/zuf6IWUrVnZ9NA2zsmWLIodz2uFHdh1voqZiegDfqnc1zqcPGUIWVEX/r87yloqaKHee95
+70+sB3c4
+-----END CERTIFICATE-----
+
+Digital Signature Trust Co. Global CA 1
+=======================================
+-----BEGIN CERTIFICATE-----
+MIIDKTCCApKgAwIBAgIENnAVljANBgkqhkiG9w0BAQUFADBGMQswCQYDVQQGEwJVUzEkMCIGA1UE
+ChMbRGlnaXRhbCBTaWduYXR1cmUgVHJ1c3QgQ28uMREwDwYDVQQLEwhEU1RDQSBFMTAeFw05ODEy
+MTAxODEwMjNaFw0xODEyMTAxODQwMjNaMEYxCzAJBgNVBAYTAlVTMSQwIgYDVQQKExtEaWdpdGFs
+IFNpZ25hdHVyZSBUcnVzdCBDby4xETAPBgNVBAsTCERTVENBIEUxMIGdMA0GCSqGSIb3DQEBAQUA
+A4GLADCBhwKBgQCgbIGpzzQeJN3+hijM3oMv+V7UQtLodGBmE5gGHKlREmlvMVW5SXIACH7TpWJE
+NySZj9mDSI+ZbZUTu0M7LklOiDfBu1h//uG9+LthzfNHwJmm8fOR6Hh8AMthyUQncWlVSn5JTe2i
+o74CTADKAqjuAQIxZA9SLRN0dja1erQtcQIBA6OCASQwggEgMBEGCWCGSAGG+EIBAQQEAwIABzBo
+BgNVHR8EYTBfMF2gW6BZpFcwVTELMAkGA1UEBhMCVVMxJDAiBgNVBAoTG0RpZ2l0YWwgU2lnbmF0
+dXJlIFRydXN0IENvLjERMA8GA1UECxMIRFNUQ0EgRTExDTALBgNVBAMTBENSTDEwKwYDVR0QBCQw
+IoAPMTk5ODEyMTAxODEwMjNagQ8yMDE4MTIxMDE4MTAyM1owCwYDVR0PBAQDAgEGMB8GA1UdIwQY
+MBaAFGp5fpFpRhgTCgJ3pVlbYJglDqL4MB0GA1UdDgQWBBRqeX6RaUYYEwoCd6VZW2CYJQ6i+DAM
+BgNVHRMEBTADAQH/MBkGCSqGSIb2fQdBAAQMMAobBFY0LjADAgSQMA0GCSqGSIb3DQEBBQUAA4GB
+ACIS2Hod3IEGtgllsofIH160L+nEHvI8wbsEkBFKg05+k7lNQseSJqBcNJo4cvj9axY+IO6CizEq
+kzaFI4iKPANo08kJD038bKTaKHKTDomAsH3+gG9lbRgzl4vCa4nuYD3Im+9/KzJic5PLPON74nZ4
+RbyhkwS7hp86W0N6w4pl
+-----END CERTIFICATE-----
+
+Digital Signature Trust Co. Global CA 3
+=======================================
+-----BEGIN CERTIFICATE-----
+MIIDKTCCApKgAwIBAgIENm7TzjANBgkqhkiG9w0BAQUFADBGMQswCQYDVQQGEwJVUzEkMCIGA1UE
+ChMbRGlnaXRhbCBTaWduYXR1cmUgVHJ1c3QgQ28uMREwDwYDVQQLEwhEU1RDQSBFMjAeFw05ODEy
+MDkxOTE3MjZaFw0xODEyMDkxOTQ3MjZaMEYxCzAJBgNVBAYTAlVTMSQwIgYDVQQKExtEaWdpdGFs
+IFNpZ25hdHVyZSBUcnVzdCBDby4xETAPBgNVBAsTCERTVENBIEUyMIGdMA0GCSqGSIb3DQEBAQUA
+A4GLADCBhwKBgQC/k48Xku8zExjrEH9OFr//Bo8qhbxe+SSmJIi2A7fBw18DW9Fvrn5C6mYjuGOD
+VvsoLeE4i7TuqAHhzhy2iCoiRoX7n6dwqUcUP87eZfCocfdPJmyMvMa1795JJ/9IKn3oTQPMx7JS
+xhcxEzu1TdvIxPbDDyQq2gyd55FbgM2UnQIBA6OCASQwggEgMBEGCWCGSAGG+EIBAQQEAwIABzBo
+BgNVHR8EYTBfMF2gW6BZpFcwVTELMAkGA1UEBhMCVVMxJDAiBgNVBAoTG0RpZ2l0YWwgU2lnbmF0
+dXJlIFRydXN0IENvLjERMA8GA1UECxMIRFNUQ0EgRTIxDTALBgNVBAMTBENSTDEwKwYDVR0QBCQw
+IoAPMTk5ODEyMDkxOTE3MjZagQ8yMDE4MTIwOTE5MTcyNlowCwYDVR0PBAQDAgEGMB8GA1UdIwQY
+MBaAFB6CTShlgDzJQW6sNS5ay97u+DlbMB0GA1UdDgQWBBQegk0oZYA8yUFurDUuWsve7vg5WzAM
+BgNVHRMEBTADAQH/MBkGCSqGSIb2fQdBAAQMMAobBFY0LjADAgSQMA0GCSqGSIb3DQEBBQUAA4GB
+AEeNg61i8tuwnkUiBbmi1gMOOHLnnvx75pO2mqWilMg0HZHRxdf0CiUPPXiBng+xZ8SQTGPdXqfi
+up/1902lMXucKS1M/mQ+7LZT/uqb7YLbdHVLB3luHtgZg3Pe9T7Qtd7nS2h9Qy4qIOF+oHhEngj1
+mPnHfxsb1gYgAlihw6ID
+-----END CERTIFICATE-----
+
+Verisign Class 3 Public Primary Certification Authority
+=======================================================
+-----BEGIN CERTIFICATE-----
+MIICPDCCAaUCEHC65B0Q2Sk0tjjKewPMur8wDQYJKoZIhvcNAQECBQAwXzELMAkGA1UEBhMCVVMx
+FzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMTcwNQYDVQQLEy5DbGFzcyAzIFB1YmxpYyBQcmltYXJ5
+IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MB4XDTk2MDEyOTAwMDAwMFoXDTI4MDgwMTIzNTk1OVow
+XzELMAkGA1UEBhMCVVMxFzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMTcwNQYDVQQLEy5DbGFzcyAz
+IFB1YmxpYyBQcmltYXJ5IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MIGfMA0GCSqGSIb3DQEBAQUA
+A4GNADCBiQKBgQDJXFme8huKARS0EN8EQNvjV69qRUCPhAwL0TPZ2RHP7gJYHyX3KqhEBarsAx94
+f56TuZoAqiN91qyFomNFx3InzPRMxnVx0jnvT0Lwdd8KkMaOIG+YD/isI19wKTakyYbnsZogy1Ol
+hec9vn2a/iRFM9x2Fe0PonFkTGUugWhFpwIDAQABMA0GCSqGSIb3DQEBAgUAA4GBALtMEivPLCYA
+TxQT3ab7/AoRhIzzKBxnki98tsX63/Dolbwdj2wsqFHMc9ikwFPwTtYmwHYBV4GSXiHx0bH/59Ah
+WM1pF+NEHJwZRDmJXNycAA9WjQKZ7aKQRUzkuxCkPfAyAw7xzvjoyVGM5mKf5p/AfbdynMk2Omuf
+Tqj/ZA1k
+-----END CERTIFICATE-----
+
+Verisign Class 1 Public Primary Certification Authority - G2
+============================================================
+-----BEGIN CERTIFICATE-----
+MIIDAjCCAmsCEEzH6qqYPnHTkxD4PTqJkZIwDQYJKoZIhvcNAQEFBQAwgcExCzAJBgNVBAYTAlVT
+MRcwFQYDVQQKEw5WZXJpU2lnbiwgSW5jLjE8MDoGA1UECxMzQ2xhc3MgMSBQdWJsaWMgUHJpbWFy
+eSBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eSAtIEcyMTowOAYDVQQLEzEoYykgMTk5OCBWZXJpU2ln
+biwgSW5jLiAtIEZvciBhdXRob3JpemVkIHVzZSBvbmx5MR8wHQYDVQQLExZWZXJpU2lnbiBUcnVz
+dCBOZXR3b3JrMB4XDTk4MDUxODAwMDAwMFoXDTI4MDgwMTIzNTk1OVowgcExCzAJBgNVBAYTAlVT
+MRcwFQYDVQQKEw5WZXJpU2lnbiwgSW5jLjE8MDoGA1UECxMzQ2xhc3MgMSBQdWJsaWMgUHJpbWFy
+eSBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eSAtIEcyMTowOAYDVQQLEzEoYykgMTk5OCBWZXJpU2ln
+biwgSW5jLiAtIEZvciBhdXRob3JpemVkIHVzZSBvbmx5MR8wHQYDVQQLExZWZXJpU2lnbiBUcnVz
+dCBOZXR3b3JrMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCq0Lq+Fi24g9TK0g+8djHKlNgd
+k4xWArzZbxpvUjZudVYKVdPfQ4chEWWKfo+9Id5rMj8bhDSVBZ1BNeuS65bdqlk/AVNtmU/t5eIq
+WpDBucSmFc/IReumXY6cPvBkJHalzasab7bYe1FhbqZ/h8jit+U03EGI6glAvnOSPWvndQIDAQAB
+MA0GCSqGSIb3DQEBBQUAA4GBAKlPww3HZ74sy9mozS11534Vnjty637rXC0Jh9ZrbWB85a7FkCMM
+XErQr7Fd88e2CtvgFZMN3QO8x3aKtd1Pw5sTdbgBwObJW2uluIncrKTdcu1OofdPvAbT6shkdHvC
+lUGcZXNY8ZCaPGqxmMnEh7zPRW1F4m4iP/68DzFc6PLZ
+-----END CERTIFICATE-----
+
+Verisign Class 2 Public Primary Certification Authority - G2
+============================================================
+-----BEGIN CERTIFICATE-----
+MIIDAzCCAmwCEQC5L2DMiJ+hekYJuFtwbIqvMA0GCSqGSIb3DQEBBQUAMIHBMQswCQYDVQQGEwJV
+UzEXMBUGA1UEChMOVmVyaVNpZ24sIEluYy4xPDA6BgNVBAsTM0NsYXNzIDIgUHVibGljIFByaW1h
+cnkgQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkgLSBHMjE6MDgGA1UECxMxKGMpIDE5OTggVmVyaVNp
+Z24sIEluYy4gLSBGb3IgYXV0aG9yaXplZCB1c2Ugb25seTEfMB0GA1UECxMWVmVyaVNpZ24gVHJ1
+c3QgTmV0d29yazAeFw05ODA1MTgwMDAwMDBaFw0yODA4MDEyMzU5NTlaMIHBMQswCQYDVQQGEwJV
+UzEXMBUGA1UEChMOVmVyaVNpZ24sIEluYy4xPDA6BgNVBAsTM0NsYXNzIDIgUHVibGljIFByaW1h
+cnkgQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkgLSBHMjE6MDgGA1UECxMxKGMpIDE5OTggVmVyaVNp
+Z24sIEluYy4gLSBGb3IgYXV0aG9yaXplZCB1c2Ugb25seTEfMB0GA1UECxMWVmVyaVNpZ24gVHJ1
+c3QgTmV0d29yazCBnzANBgkqhkiG9w0BAQEFAAOBjQAwgYkCgYEAp4gBIXQs5xoD8JjhlzwPIQjx
+nNuX6Zr8wgQGE75fUsjMHiwSViy4AWkszJkfrbCWrnkE8hM5wXuYuggs6MKEEyyqaekJ9MepAqRC
+wiNPStjwDqL7MWzJ5m+ZJwf15vRMeJ5t60aG+rmGyVTyssSv1EYcWskVMP8NbPUtDm3Of3cCAwEA
+ATANBgkqhkiG9w0BAQUFAAOBgQByLvl/0fFx+8Se9sVeUYpAmLho+Jscg9jinb3/7aHmZuovCfTK
+1+qlK5X2JGCGTUQug6XELaDTrnhpb3LabK4I8GOSN+a7xDAXrXfMSTWqz9iP0b63GJZHc2pUIjRk
+LbYWm1lbtFFZOrMLFPQS32eg9K0yZF6xRnInjBJ7xUS0rg==
+-----END CERTIFICATE-----
+
+Verisign Class 3 Public Primary Certification Authority - G2
+============================================================
+-----BEGIN CERTIFICATE-----
+MIIDAjCCAmsCEH3Z/gfPqB63EHln+6eJNMYwDQYJKoZIhvcNAQEFBQAwgcExCzAJBgNVBAYTAlVT
+MRcwFQYDVQQKEw5WZXJpU2lnbiwgSW5jLjE8MDoGA1UECxMzQ2xhc3MgMyBQdWJsaWMgUHJpbWFy
+eSBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eSAtIEcyMTowOAYDVQQLEzEoYykgMTk5OCBWZXJpU2ln
+biwgSW5jLiAtIEZvciBhdXRob3JpemVkIHVzZSBvbmx5MR8wHQYDVQQLExZWZXJpU2lnbiBUcnVz
+dCBOZXR3b3JrMB4XDTk4MDUxODAwMDAwMFoXDTI4MDgwMTIzNTk1OVowgcExCzAJBgNVBAYTAlVT
+MRcwFQYDVQQKEw5WZXJpU2lnbiwgSW5jLjE8MDoGA1UECxMzQ2xhc3MgMyBQdWJsaWMgUHJpbWFy
+eSBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eSAtIEcyMTowOAYDVQQLEzEoYykgMTk5OCBWZXJpU2ln
+biwgSW5jLiAtIEZvciBhdXRob3JpemVkIHVzZSBvbmx5MR8wHQYDVQQLExZWZXJpU2lnbiBUcnVz
+dCBOZXR3b3JrMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDMXtERXVxp0KvTuWpMmR9ZmDCO
+FoUgRm1HP9SFIIThbbP4pO0M8RcPO/mn+SXXwc+EY/J8Y8+iR/LGWzOOZEAEaMGAuWQcRXfH2G71
+lSk8UOg013gfqLptQ5GVj0VXXn7F+8qkBOvqlzdUMG+7AUcyM83cV5tkaWH4mx0ciU9cZwIDAQAB
+MA0GCSqGSIb3DQEBBQUAA4GBAFFNzb5cy5gZnBWyATl4Lk0PZ3BwmcYQWpSkU01UbSuvDV1Ai2TT
+1+7eVmGSX6bEHRBhNtMsJzzoKQm5EWR0zLVznxxIqbxhAe7iF6YM40AIOw7n60RzKprxaZLvcRTD
+Oaxxp5EJb+RxBrO6WVcmeQD2+A2iMzAo1KpYoJ2daZH9
+-----END CERTIFICATE-----
+
+GlobalSign Root CA
+==================
+-----BEGIN CERTIFICATE-----
+MIIDdTCCAl2gAwIBAgILBAAAAAABFUtaw5QwDQYJKoZIhvcNAQEFBQAwVzELMAkGA1UEBhMCQkUx
+GTAXBgNVBAoTEEdsb2JhbFNpZ24gbnYtc2ExEDAOBgNVBAsTB1Jvb3QgQ0ExGzAZBgNVBAMTEkds
+b2JhbFNpZ24gUm9vdCBDQTAeFw05ODA5MDExMjAwMDBaFw0yODAxMjgxMjAwMDBaMFcxCzAJBgNV
+BAYTAkJFMRkwFwYDVQQKExBHbG9iYWxTaWduIG52LXNhMRAwDgYDVQQLEwdSb290IENBMRswGQYD
+VQQDExJHbG9iYWxTaWduIFJvb3QgQ0EwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDa
+DuaZjc6j40+Kfvvxi4Mla+pIH/EqsLmVEQS98GPR4mdmzxzdzxtIK+6NiY6arymAZavpxy0Sy6sc
+THAHoT0KMM0VjU/43dSMUBUc71DuxC73/OlS8pF94G3VNTCOXkNz8kHp1Wrjsok6Vjk4bwY8iGlb
+Kk3Fp1S4bInMm/k8yuX9ifUSPJJ4ltbcdG6TRGHRjcdGsnUOhugZitVtbNV4FpWi6cgKOOvyJBNP
+c1STE4U6G7weNLWLBYy5d4ux2x8gkasJU26Qzns3dLlwR5EiUWMWea6xrkEmCMgZK9FGqkjWZCrX
+gzT/LCrBbBlDSgeF59N89iFo7+ryUp9/k5DPAgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNV
+HRMBAf8EBTADAQH/MB0GA1UdDgQWBBRge2YaRQ2XyolQL30EzTSo//z9SzANBgkqhkiG9w0BAQUF
+AAOCAQEA1nPnfE920I2/7LqivjTFKDK1fPxsnCwrvQmeU79rXqoRSLblCKOzyj1hTdNGCbM+w6Dj
+Y1Ub8rrvrTnhQ7k4o+YviiY776BQVvnGCv04zcQLcFGUl5gE38NflNUVyRRBnMRddWQVDf9VMOyG
+j/8N7yy5Y0b2qvzfvGn9LhJIZJrglfCm7ymPAbEVtQwdpf5pLGkkeB6zpxxxYu7KyJesF12KwvhH
+hm4qxFYxldBniYUr+WymXUadDKqC5JlR3XC321Y9YeRq4VzW9v493kHMB65jUr9TU/Qr6cf9tveC
+X4XSQRjbgbMEHMUfpIBvFSDJ3gyICh3WZlXi/EjJKSZp4A==
+-----END CERTIFICATE-----
+
+GlobalSign Root CA - R2
+=======================
+-----BEGIN CERTIFICATE-----
+MIIDujCCAqKgAwIBAgILBAAAAAABD4Ym5g0wDQYJKoZIhvcNAQEFBQAwTDEgMB4GA1UECxMXR2xv
+YmFsU2lnbiBSb290IENBIC0gUjIxEzARBgNVBAoTCkdsb2JhbFNpZ24xEzARBgNVBAMTCkdsb2Jh
+bFNpZ24wHhcNMDYxMjE1MDgwMDAwWhcNMjExMjE1MDgwMDAwWjBMMSAwHgYDVQQLExdHbG9iYWxT
+aWduIFJvb3QgQ0EgLSBSMjETMBEGA1UEChMKR2xvYmFsU2lnbjETMBEGA1UEAxMKR2xvYmFsU2ln
+bjCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKbPJA6+Lm8omUVCxKs+IVSbC9N/hHD6
+ErPLv4dfxn+G07IwXNb9rfF73OX4YJYJkhD10FPe+3t+c4isUoh7SqbKSaZeqKeMWhG8eoLrvozp
+s6yWJQeXSpkqBy+0Hne/ig+1AnwblrjFuTosvNYSuetZfeLQBoZfXklqtTleiDTsvHgMCJiEbKjN
+S7SgfQx5TfC4LcshytVsW33hoCmEofnTlEnLJGKRILzdC9XZzPnqJworc5HGnRusyMvo4KD0L5CL
+TfuwNhv2GXqF4G3yYROIXJ/gkwpRl4pazq+r1feqCapgvdzZX99yqWATXgAByUr6P6TqBwMhAo6C
+ygPCm48CAwEAAaOBnDCBmTAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4E
+FgQUm+IHV2ccHsBqBt5ZtJot39wZhi4wNgYDVR0fBC8wLTAroCmgJ4YlaHR0cDovL2NybC5nbG9i
+YWxzaWduLm5ldC9yb290LXIyLmNybDAfBgNVHSMEGDAWgBSb4gdXZxwewGoG3lm0mi3f3BmGLjAN
+BgkqhkiG9w0BAQUFAAOCAQEAmYFThxxol4aR7OBKuEQLq4GsJ0/WwbgcQ3izDJr86iw8bmEbTUsp
+9Z8FHSbBuOmDAGJFtqkIk7mpM0sYmsL4h4hO291xNBrBVNpGP+DTKqttVCL1OmLNIG+6KYnX3ZHu
+01yiPqFbQfXf5WRDLenVOavSot+3i9DAgBkcRcAtjOj4LaR0VknFBbVPFd5uRHg5h6h+u/N5GJG7
+9G+dwfCMNYxdAfvDbbnvRG15RjF+Cv6pgsH/76tuIMRQyV+dTZsXjAzlAcmgQWpzU/qlULRuJQ/7
+TBj0/VLZjmmx6BEP3ojY+x1J96relc8geMJgEtslQIxq/H5COEBkEveegeGTLg==
+-----END CERTIFICATE-----
+
+ValiCert Class 1 VA
+===================
+-----BEGIN CERTIFICATE-----
+MIIC5zCCAlACAQEwDQYJKoZIhvcNAQEFBQAwgbsxJDAiBgNVBAcTG1ZhbGlDZXJ0IFZhbGlkYXRp
+b24gTmV0d29yazEXMBUGA1UEChMOVmFsaUNlcnQsIEluYy4xNTAzBgNVBAsTLFZhbGlDZXJ0IENs
+YXNzIDEgUG9saWN5IFZhbGlkYXRpb24gQXV0aG9yaXR5MSEwHwYDVQQDExhodHRwOi8vd3d3LnZh
+bGljZXJ0LmNvbS8xIDAeBgkqhkiG9w0BCQEWEWluZm9AdmFsaWNlcnQuY29tMB4XDTk5MDYyNTIy
+MjM0OFoXDTE5MDYyNTIyMjM0OFowgbsxJDAiBgNVBAcTG1ZhbGlDZXJ0IFZhbGlkYXRpb24gTmV0
+d29yazEXMBUGA1UEChMOVmFsaUNlcnQsIEluYy4xNTAzBgNVBAsTLFZhbGlDZXJ0IENsYXNzIDEg
+UG9saWN5IFZhbGlkYXRpb24gQXV0aG9yaXR5MSEwHwYDVQQDExhodHRwOi8vd3d3LnZhbGljZXJ0
+LmNvbS8xIDAeBgkqhkiG9w0BCQEWEWluZm9AdmFsaWNlcnQuY29tMIGfMA0GCSqGSIb3DQEBAQUA
+A4GNADCBiQKBgQDYWYJ6ibiWuqYvaG9YLqdUHAZu9OqNSLwxlBfw8068srg1knaw0KWlAdcAAxIi
+GQj4/xEjm84H9b9pGib+TunRf50sQB1ZaG6m+FiwnRqP0z/x3BkGgagO4DrdyFNFCQbmD3DD+kCm
+DuJWBQ8YTfwggtFzVXSNdnKgHZ0dwN0/cQIDAQABMA0GCSqGSIb3DQEBBQUAA4GBAFBoPUn0LBwG
+lN+VYH+Wexf+T3GtZMjdd9LvWVXoP+iOBSoh8gfStadS/pyxtuJbdxdA6nLWI8sogTLDAHkY7FkX
+icnGah5xyf23dKUlRWnFSKsZ4UWKJWsZ7uW7EvV/96aNUcPwnXS3qT6gpf+2SQMT2iLM7XGCK5nP
+Orf1LXLI
+-----END CERTIFICATE-----
+
+ValiCert Class 2 VA
+===================
+-----BEGIN CERTIFICATE-----
+MIIC5zCCAlACAQEwDQYJKoZIhvcNAQEFBQAwgbsxJDAiBgNVBAcTG1ZhbGlDZXJ0IFZhbGlkYXRp
+b24gTmV0d29yazEXMBUGA1UEChMOVmFsaUNlcnQsIEluYy4xNTAzBgNVBAsTLFZhbGlDZXJ0IENs
+YXNzIDIgUG9saWN5IFZhbGlkYXRpb24gQXV0aG9yaXR5MSEwHwYDVQQDExhodHRwOi8vd3d3LnZh
+bGljZXJ0LmNvbS8xIDAeBgkqhkiG9w0BCQEWEWluZm9AdmFsaWNlcnQuY29tMB4XDTk5MDYyNjAw
+MTk1NFoXDTE5MDYyNjAwMTk1NFowgbsxJDAiBgNVBAcTG1ZhbGlDZXJ0IFZhbGlkYXRpb24gTmV0
+d29yazEXMBUGA1UEChMOVmFsaUNlcnQsIEluYy4xNTAzBgNVBAsTLFZhbGlDZXJ0IENsYXNzIDIg
+UG9saWN5IFZhbGlkYXRpb24gQXV0aG9yaXR5MSEwHwYDVQQDExhodHRwOi8vd3d3LnZhbGljZXJ0
+LmNvbS8xIDAeBgkqhkiG9w0BCQEWEWluZm9AdmFsaWNlcnQuY29tMIGfMA0GCSqGSIb3DQEBAQUA
+A4GNADCBiQKBgQDOOnHK5avIWZJV16vYdA757tn2VUdZZUcOBVXc65g2PFxTXdMwzzjsvUGJ7SVC
+CSRrCl6zfN1SLUzm1NZ9WlmpZdRJEy0kTRxQb7XBhVQ7/nHk01xC+YDgkRoKWzk2Z/M/VXwbP7Rf
+ZHM047QSv4dk+NoS/zcnwbNDu+97bi5p9wIDAQABMA0GCSqGSIb3DQEBBQUAA4GBADt/UG9vUJSZ
+SWI4OB9L+KXIPqeCgfYrx+jFzug6EILLGACOTb2oWH+heQC1u+mNr0HZDzTuIYEZoDJJKPTEjlbV
+UjP9UNV+mWwD5MlM/Mtsq2azSiGM5bUMMj4QssxsodyamEwCW/POuZ6lcg5Ktz885hZo+L7tdEy8
+W9ViH0Pd
+-----END CERTIFICATE-----
+
+RSA Root Certificate 1
+======================
+-----BEGIN CERTIFICATE-----
+MIIC5zCCAlACAQEwDQYJKoZIhvcNAQEFBQAwgbsxJDAiBgNVBAcTG1ZhbGlDZXJ0IFZhbGlkYXRp
+b24gTmV0d29yazEXMBUGA1UEChMOVmFsaUNlcnQsIEluYy4xNTAzBgNVBAsTLFZhbGlDZXJ0IENs
+YXNzIDMgUG9saWN5IFZhbGlkYXRpb24gQXV0aG9yaXR5MSEwHwYDVQQDExhodHRwOi8vd3d3LnZh
+bGljZXJ0LmNvbS8xIDAeBgkqhkiG9w0BCQEWEWluZm9AdmFsaWNlcnQuY29tMB4XDTk5MDYyNjAw
+MjIzM1oXDTE5MDYyNjAwMjIzM1owgbsxJDAiBgNVBAcTG1ZhbGlDZXJ0IFZhbGlkYXRpb24gTmV0
+d29yazEXMBUGA1UEChMOVmFsaUNlcnQsIEluYy4xNTAzBgNVBAsTLFZhbGlDZXJ0IENsYXNzIDMg
+UG9saWN5IFZhbGlkYXRpb24gQXV0aG9yaXR5MSEwHwYDVQQDExhodHRwOi8vd3d3LnZhbGljZXJ0
+LmNvbS8xIDAeBgkqhkiG9w0BCQEWEWluZm9AdmFsaWNlcnQuY29tMIGfMA0GCSqGSIb3DQEBAQUA
+A4GNADCBiQKBgQDjmFGWHOjVsQaBalfDcnWTq8+epvzzFlLWLU2fNUSoLgRNB0mKOCn1dzfnt6td
+3zZxFJmP3MKS8edgkpfs2Ejcv8ECIMYkpChMMFp2bbFc893enhBxoYjHW5tBbcqwuI4V7q0zK89H
+BFx1cQqYJJgpp0lZpd34t0NiYfPT4tBVPwIDAQABMA0GCSqGSIb3DQEBBQUAA4GBAFa7AliEZwgs
+3x/be0kz9dNnnfS0ChCzycUs4pJqcXgn8nCDQtM+z6lU9PHYkhaM0QTLS6vJn0WuPIqpsHEzXcjF
+V9+vqDWzf4mH6eglkrh/hXqu1rweN1gqZ8mRzyqBPu3GOd/APhmcGcwTTYJBtYze4D1gCCAPRX5r
+on+jjBXu
+-----END CERTIFICATE-----
+
+Verisign Class 1 Public Primary Certification Authority - G3
+============================================================
+-----BEGIN CERTIFICATE-----
+MIIEGjCCAwICEQCLW3VWhFSFCwDPrzhIzrGkMA0GCSqGSIb3DQEBBQUAMIHKMQswCQYDVQQGEwJV
+UzEXMBUGA1UEChMOVmVyaVNpZ24sIEluYy4xHzAdBgNVBAsTFlZlcmlTaWduIFRydXN0IE5ldHdv
+cmsxOjA4BgNVBAsTMShjKSAxOTk5IFZlcmlTaWduLCBJbmMuIC0gRm9yIGF1dGhvcml6ZWQgdXNl
+IG9ubHkxRTBDBgNVBAMTPFZlcmlTaWduIENsYXNzIDEgUHVibGljIFByaW1hcnkgQ2VydGlmaWNh
+dGlvbiBBdXRob3JpdHkgLSBHMzAeFw05OTEwMDEwMDAwMDBaFw0zNjA3MTYyMzU5NTlaMIHKMQsw
+CQYDVQQGEwJVUzEXMBUGA1UEChMOVmVyaVNpZ24sIEluYy4xHzAdBgNVBAsTFlZlcmlTaWduIFRy
+dXN0IE5ldHdvcmsxOjA4BgNVBAsTMShjKSAxOTk5IFZlcmlTaWduLCBJbmMuIC0gRm9yIGF1dGhv
+cml6ZWQgdXNlIG9ubHkxRTBDBgNVBAMTPFZlcmlTaWduIENsYXNzIDEgUHVibGljIFByaW1hcnkg
+Q2VydGlmaWNhdGlvbiBBdXRob3JpdHkgLSBHMzCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoC
+ggEBAN2E1Lm0+afY8wR4nN493GwTFtl63SRRZsDHJlkNrAYIwpTRMx/wgzUfbhvI3qpuFU5UJ+/E
+bRrsC+MO8ESlV8dAWB6jRx9x7GD2bZTIGDnt/kIYVt/kTEkQeE4BdjVjEjbdZrwBBDajVWjVojYJ
+rKshJlQGrT/KFOCsyq0GHZXi+J3x4GD/wn91K0zM2v6HmSHquv4+VNfSWXjbPG7PoBMAGrgnoeS+
+Z5bKoMWznN3JdZ7rMJpfo83ZrngZPyPpXNspva1VyBtUjGP26KbqxzcSXKMpHgLZ2x87tNcPVkeB
+FQRKr4Mn0cVYiMHd9qqnoxjaaKptEVHhv2Vrn5Z20T0CAwEAATANBgkqhkiG9w0BAQUFAAOCAQEA
+q2aN17O6x5q25lXQBfGfMY1aqtmqRiYPce2lrVNWYgFHKkTp/j90CxObufRNG7LRX7K20ohcs5/N
+y9Sn2WCVhDr4wTcdYcrnsMXlkdpUpqwxga6X3s0IrLjAl4B/bnKk52kTlWUfxJM8/XmPBNQ+T+r3
+ns7NZ3xPZQL/kYVUc8f/NveGLezQXk//EZ9yBta4GvFMDSZl4kSAHsef493oCtrspSCAaWihT37h
+a88HQfqDjrw43bAuEbFrskLMmrz5SCJ5ShkPshw+IHTZasO+8ih4E1Z5T21Q6huwtVexN2ZYI/Pc
+D98Kh8TvhgXVOBRgmaNL3gaWcSzy27YfpO8/7g==
+-----END CERTIFICATE-----
+
+Verisign Class 2 Public Primary Certification Authority - G3
+============================================================
+-----BEGIN CERTIFICATE-----
+MIIEGTCCAwECEGFwy0mMX5hFKeewptlQW3owDQYJKoZIhvcNAQEFBQAwgcoxCzAJBgNVBAYTAlVT
+MRcwFQYDVQQKEw5WZXJpU2lnbiwgSW5jLjEfMB0GA1UECxMWVmVyaVNpZ24gVHJ1c3QgTmV0d29y
+azE6MDgGA1UECxMxKGMpIDE5OTkgVmVyaVNpZ24sIEluYy4gLSBGb3IgYXV0aG9yaXplZCB1c2Ug
+b25seTFFMEMGA1UEAxM8VmVyaVNpZ24gQ2xhc3MgMiBQdWJsaWMgUHJpbWFyeSBDZXJ0aWZpY2F0
+aW9uIEF1dGhvcml0eSAtIEczMB4XDTk5MTAwMTAwMDAwMFoXDTM2MDcxNjIzNTk1OVowgcoxCzAJ
+BgNVBAYTAlVTMRcwFQYDVQQKEw5WZXJpU2lnbiwgSW5jLjEfMB0GA1UECxMWVmVyaVNpZ24gVHJ1
+c3QgTmV0d29yazE6MDgGA1UECxMxKGMpIDE5OTkgVmVyaVNpZ24sIEluYy4gLSBGb3IgYXV0aG9y
+aXplZCB1c2Ugb25seTFFMEMGA1UEAxM8VmVyaVNpZ24gQ2xhc3MgMiBQdWJsaWMgUHJpbWFyeSBD
+ZXJ0aWZpY2F0aW9uIEF1dGhvcml0eSAtIEczMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC
+AQEArwoNwtUs22e5LeWUJ92lvuCwTY+zYVY81nzD9M0+hsuiiOLh2KRpxbXiv8GmR1BeRjmL1Za6
+tW8UvxDOJxOeBUebMXoT2B/Z0wI3i60sR/COgQanDTAM6/c8DyAd3HJG7qUCyFvDyVZpTMUYwZF7
+C9UTAJu878NIPkZgIIUq1ZC2zYugzDLdt/1AVbJQHFauzI13TccgTacxdu9okoqQHgiBVrKtaaNS
+0MscxCM9H5n+TOgWY47GCI72MfbS+uV23bUckqNJzc0BzWjNqWm6o+sdDZykIKbBoMXRRkwXbdKs
+Zj+WjOCE1Db/IlnF+RFgqF8EffIa9iVCYQ/ESrg+iQIDAQABMA0GCSqGSIb3DQEBBQUAA4IBAQA0
+JhU8wI1NQ0kdvekhktdmnLfexbjQ5F1fdiLAJvmEOjr5jLX77GDx6M4EsMjdpwOPMPOY36TmpDHf
+0xwLRtxyID+u7gU8pDM/CzmscHhzS5kr3zDCVLCoO1Wh/hYozUK9dG6A2ydEp85EXdQbkJgNHkKU
+sQAsBNB0owIFImNjzYO1+8FtYmtpdf1dcEG59b98377BMnMiIYtYgXsVkXq642RIsH/7NiXaldDx
+JBQX3RiAa0YjOVT1jmIJBB2UkKab5iXiQkWquJCtvgiPqQtCGJTPcjnhsUPgKM+351psE2tJs//j
+GHyJizNdrDPXp/naOlXJWBD5qu9ats9LS98q
+-----END CERTIFICATE-----
+
+Verisign Class 3 Public Primary Certification Authority - G3
+============================================================
+-----BEGIN CERTIFICATE-----
+MIIEGjCCAwICEQCbfgZJoz5iudXukEhxKe9XMA0GCSqGSIb3DQEBBQUAMIHKMQswCQYDVQQGEwJV
+UzEXMBUGA1UEChMOVmVyaVNpZ24sIEluYy4xHzAdBgNVBAsTFlZlcmlTaWduIFRydXN0IE5ldHdv
+cmsxOjA4BgNVBAsTMShjKSAxOTk5IFZlcmlTaWduLCBJbmMuIC0gRm9yIGF1dGhvcml6ZWQgdXNl
+IG9ubHkxRTBDBgNVBAMTPFZlcmlTaWduIENsYXNzIDMgUHVibGljIFByaW1hcnkgQ2VydGlmaWNh
+dGlvbiBBdXRob3JpdHkgLSBHMzAeFw05OTEwMDEwMDAwMDBaFw0zNjA3MTYyMzU5NTlaMIHKMQsw
+CQYDVQQGEwJVUzEXMBUGA1UEChMOVmVyaVNpZ24sIEluYy4xHzAdBgNVBAsTFlZlcmlTaWduIFRy
+dXN0IE5ldHdvcmsxOjA4BgNVBAsTMShjKSAxOTk5IFZlcmlTaWduLCBJbmMuIC0gRm9yIGF1dGhv
+cml6ZWQgdXNlIG9ubHkxRTBDBgNVBAMTPFZlcmlTaWduIENsYXNzIDMgUHVibGljIFByaW1hcnkg
+Q2VydGlmaWNhdGlvbiBBdXRob3JpdHkgLSBHMzCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoC
+ggEBAMu6nFL8eB8aHm8bN3O9+MlrlBIwT/A2R/XQkQr1F8ilYcEWQE37imGQ5XYgwREGfassbqb1
+EUGO+i2tKmFZpGcmTNDovFJbcCAEWNF6yaRpvIMXZK0Fi7zQWM6NjPXr8EJJC52XJ2cybuGukxUc
+cLwgTS8Y3pKI6GyFVxEa6X7jJhFUokWWVYPKMIno3Nij7SqAP395ZVc+FSBmCC+Vk7+qRy+oRpfw
+EuL+wgorUeZ25rdGt+INpsyow0xZVYnm6FNcHOqd8GIWC6fJXwzw3sJ2zq/3avL6QaaiMxTJ5Xpj
+055iN9WFZZ4O5lMkdBteHRJTW8cs54NJOxWuimi5V5cCAwEAATANBgkqhkiG9w0BAQUFAAOCAQEA
+ERSWwauSCPc/L8my/uRan2Te2yFPhpk0djZX3dAVL8WtfxUfN2JzPtTnX84XA9s1+ivbrmAJXx5f
+j267Cz3qWhMeDGBvtcC1IyIuBwvLqXTLR7sdwdela8wv0kL9Sd2nic9TutoAWii/gt/4uhMdUIaC
+/Y4wjylGsB49Ndo4YhYYSq3mtlFs3q9i6wHQHiT+eo8SGhJouPtmmRQURVyu565pF4ErWjfJXir0
+xuKhXFSbplQAz/DxwceYMBo7Nhbbo27q/a2ywtrvAkcTisDxszGtTxzhT5yvDwyd93gN2PQ1VoDa
+t20Xj50egWTh/sVFuq1ruQp6Tk9LhO5L8X3dEQ==
+-----END CERTIFICATE-----
+
+Verisign Class 4 Public Primary Certification Authority - G3
+============================================================
+-----BEGIN CERTIFICATE-----
+MIIEGjCCAwICEQDsoKeLbnVqAc/EfMwvlF7XMA0GCSqGSIb3DQEBBQUAMIHKMQswCQYDVQQGEwJV
+UzEXMBUGA1UEChMOVmVyaVNpZ24sIEluYy4xHzAdBgNVBAsTFlZlcmlTaWduIFRydXN0IE5ldHdv
+cmsxOjA4BgNVBAsTMShjKSAxOTk5IFZlcmlTaWduLCBJbmMuIC0gRm9yIGF1dGhvcml6ZWQgdXNl
+IG9ubHkxRTBDBgNVBAMTPFZlcmlTaWduIENsYXNzIDQgUHVibGljIFByaW1hcnkgQ2VydGlmaWNh
+dGlvbiBBdXRob3JpdHkgLSBHMzAeFw05OTEwMDEwMDAwMDBaFw0zNjA3MTYyMzU5NTlaMIHKMQsw
+CQYDVQQGEwJVUzEXMBUGA1UEChMOVmVyaVNpZ24sIEluYy4xHzAdBgNVBAsTFlZlcmlTaWduIFRy
+dXN0IE5ldHdvcmsxOjA4BgNVBAsTMShjKSAxOTk5IFZlcmlTaWduLCBJbmMuIC0gRm9yIGF1dGhv
+cml6ZWQgdXNlIG9ubHkxRTBDBgNVBAMTPFZlcmlTaWduIENsYXNzIDQgUHVibGljIFByaW1hcnkg
+Q2VydGlmaWNhdGlvbiBBdXRob3JpdHkgLSBHMzCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoC
+ggEBAK3LpRFpxlmr8Y+1GQ9Wzsy1HyDkniYlS+BzZYlZ3tCD5PUPtbut8XzoIfzk6AzufEUiGXaS
+tBO3IFsJ+mGuqPKljYXCKtbeZjbSmwL0qJJgfJxptI8kHtCGUvYynEFYHiK9zUVilQhu0GbdU6LM
+8BDcVHOLBKFGMzNcF0C5nk3T875Vg+ixiY5afJqWIpA7iCXy0lOIAgwLePLmNxdLMEYH5IBtptiW
+Lugs+BGzOA1mppvqySNb247i8xOOGlktqgLw7KSHZtzBP/XYufTsgsbSPZUd5cBPhMnZo0QoBmrX
+Razwa2rvTl/4EYIeOGM0ZlDUPpNz+jDDZq3/ky2X7wMCAwEAATANBgkqhkiG9w0BAQUFAAOCAQEA
+j/ola09b5KROJ1WrIhVZPMq1CtRK26vdoV9TxaBXOcLORyu+OshWv8LZJxA6sQU8wHcxuzrTBXtt
+mhwwjIDLk5Mqg6sFUYICABFna/OIYUdfA5PVWw3g8dShMjWFsjrbsIKr0csKvE+MW8VLADsfKoKm
+fjaF3H48ZwC15DtS4KjrXRX5xm3wrR0OhbepmnMUWluPQSjA1egtTaRezarZ7c7c2NU8Qh0XwRJd
+RTjDOPP8hS6DRkiy1yBfkjaP53kPmF6Z6PDQpLv1U70qzlmwr25/bLvSHgCwIe34QWKCudiyxLtG
+UPMxxY8BqHTr9Xgn2uf3ZkPznoM+IKrDNWCRzg==
+-----END CERTIFICATE-----
+
+Entrust.net Secure Server CA
+============================
+-----BEGIN CERTIFICATE-----
+MIIE2DCCBEGgAwIBAgIEN0rSQzANBgkqhkiG9w0BAQUFADCBwzELMAkGA1UEBhMCVVMxFDASBgNV
+BAoTC0VudHJ1c3QubmV0MTswOQYDVQQLEzJ3d3cuZW50cnVzdC5uZXQvQ1BTIGluY29ycC4gYnkg
+cmVmLiAobGltaXRzIGxpYWIuKTElMCMGA1UECxMcKGMpIDE5OTkgRW50cnVzdC5uZXQgTGltaXRl
+ZDE6MDgGA1UEAxMxRW50cnVzdC5uZXQgU2VjdXJlIFNlcnZlciBDZXJ0aWZpY2F0aW9uIEF1dGhv
+cml0eTAeFw05OTA1MjUxNjA5NDBaFw0xOTA1MjUxNjM5NDBaMIHDMQswCQYDVQQGEwJVUzEUMBIG
+A1UEChMLRW50cnVzdC5uZXQxOzA5BgNVBAsTMnd3dy5lbnRydXN0Lm5ldC9DUFMgaW5jb3JwLiBi
+eSByZWYuIChsaW1pdHMgbGlhYi4pMSUwIwYDVQQLExwoYykgMTk5OSBFbnRydXN0Lm5ldCBMaW1p
+dGVkMTowOAYDVQQDEzFFbnRydXN0Lm5ldCBTZWN1cmUgU2VydmVyIENlcnRpZmljYXRpb24gQXV0
+aG9yaXR5MIGdMA0GCSqGSIb3DQEBAQUAA4GLADCBhwKBgQDNKIM0VBuJ8w+vN5Ex/68xYMmo6LIQ
+aO2f55M28Qpku0f1BBc/I0dNxScZgSYMVHINiC3ZH5oSn7yzcdOAGT9HZnuMNSjSuQrfJNqc1lB5
+gXpa0zf3wkrYKZImZNHkmGw6AIr1NJtl+O3jEP/9uElY3KDegjlrgbEWGWG5VLbmQwIBA6OCAdcw
+ggHTMBEGCWCGSAGG+EIBAQQEAwIABzCCARkGA1UdHwSCARAwggEMMIHeoIHboIHYpIHVMIHSMQsw
+CQYDVQQGEwJVUzEUMBIGA1UEChMLRW50cnVzdC5uZXQxOzA5BgNVBAsTMnd3dy5lbnRydXN0Lm5l
+dC9DUFMgaW5jb3JwLiBieSByZWYuIChsaW1pdHMgbGlhYi4pMSUwIwYDVQQLExwoYykgMTk5OSBF
+bnRydXN0Lm5ldCBMaW1pdGVkMTowOAYDVQQDEzFFbnRydXN0Lm5ldCBTZWN1cmUgU2VydmVyIENl
+cnRpZmljYXRpb24gQXV0aG9yaXR5MQ0wCwYDVQQDEwRDUkwxMCmgJ6AlhiNodHRwOi8vd3d3LmVu
+dHJ1c3QubmV0L0NSTC9uZXQxLmNybDArBgNVHRAEJDAigA8xOTk5MDUyNTE2MDk0MFqBDzIwMTkw
+NTI1MTYwOTQwWjALBgNVHQ8EBAMCAQYwHwYDVR0jBBgwFoAU8BdiE1U9s/8KAGv7UISX8+1i0Bow
+HQYDVR0OBBYEFPAXYhNVPbP/CgBr+1CEl/PtYtAaMAwGA1UdEwQFMAMBAf8wGQYJKoZIhvZ9B0EA
+BAwwChsEVjQuMAMCBJAwDQYJKoZIhvcNAQEFBQADgYEAkNwwAvpkdMKnCqV8IY00F6j7Rw7/JXyN
+Ewr75Ji174z4xRAN95K+8cPV1ZVqBLssziY2ZcgxxufuP+NXdYR6Ee9GTxj005i7qIcyunL2POI9
+n9cd2cNgQ4xYDiKWL2KjLB+6rQXvqzJ4h6BUcxm1XAX5Uj5tLUUL9wqT6u0G+bI=
+-----END CERTIFICATE-----
+
+Entrust.net Premium 2048 Secure Server CA
+=========================================
+-----BEGIN CERTIFICATE-----
+MIIEXDCCA0SgAwIBAgIEOGO5ZjANBgkqhkiG9w0BAQUFADCBtDEUMBIGA1UEChMLRW50cnVzdC5u
+ZXQxQDA+BgNVBAsUN3d3dy5lbnRydXN0Lm5ldC9DUFNfMjA0OCBpbmNvcnAuIGJ5IHJlZi4gKGxp
+bWl0cyBsaWFiLikxJTAjBgNVBAsTHChjKSAxOTk5IEVudHJ1c3QubmV0IExpbWl0ZWQxMzAxBgNV
+BAMTKkVudHJ1c3QubmV0IENlcnRpZmljYXRpb24gQXV0aG9yaXR5ICgyMDQ4KTAeFw05OTEyMjQx
+NzUwNTFaFw0xOTEyMjQxODIwNTFaMIG0MRQwEgYDVQQKEwtFbnRydXN0Lm5ldDFAMD4GA1UECxQ3
+d3d3LmVudHJ1c3QubmV0L0NQU18yMDQ4IGluY29ycC4gYnkgcmVmLiAobGltaXRzIGxpYWIuKTEl
+MCMGA1UECxMcKGMpIDE5OTkgRW50cnVzdC5uZXQgTGltaXRlZDEzMDEGA1UEAxMqRW50cnVzdC5u
+ZXQgQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkgKDIwNDgpMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8A
+MIIBCgKCAQEArU1LqRKGsuqjIAcVFmQqK0vRvwtKTY7tgHalZ7d4QMBzQshowNtTK91euHaYNZOL
+Gp18EzoOH1u3Hs/lJBQesYGpjX24zGtLA/ECDNyrpUAkAH90lKGdCCmziAv1h3edVc3kw37XamSr
+hRSGlVuXMlBvPci6Zgzj/L24ScF2iUkZ/cCovYmjZy/Gn7xxGWC4LeksyZB2ZnuU4q941mVTXTzW
+nLLPKQP5L6RQstRIzgUyVYr9smRMDuSYB3Xbf9+5CFVghTAp+XtIpGmG4zU/HoZdenoVve8AjhUi
+VBcAkCaTvA5JaJG/+EfTnZVCwQ5N328mz8MYIWJmQ3DW1cAH4QIDAQABo3QwcjARBglghkgBhvhC
+AQEEBAMCAAcwHwYDVR0jBBgwFoAUVeSB0RGAvtiJuQijMfmhJAkWuXAwHQYDVR0OBBYEFFXkgdER
+gL7YibkIozH5oSQJFrlwMB0GCSqGSIb2fQdBAAQQMA4bCFY1LjA6NC4wAwIEkDANBgkqhkiG9w0B
+AQUFAAOCAQEAWUesIYSKF8mciVMeuoCFGsY8Tj6xnLZ8xpJdGGQC49MGCBFhfGPjK50xA3B20qMo
+oPS7mmNz7W3lKtvtFKkrxjYR0CvrB4ul2p5cGZ1WEvVUKcgF7bISKo30Axv/55IQh7A6tcOdBTcS
+o8f0FbnVpDkWm1M6I5HxqIKiaohowXkCIryqptau37AUX7iH0N18f3v/rxzP5tsHrV7bhZ3QKw0z
+2wTR5klAEyt2+z7pnIkPFc4YsIV4IU9rTw76NmfNB/L/CNDi3tm/Kq+4h4YhPATKt5Rof8886ZjX
+OP/swNlQ8C5LWK5Gb9Auw2DaclVyvUxFnmG6v4SBkgPR0ml8xQ==
+-----END CERTIFICATE-----
+
+Baltimore CyberTrust Root
+=========================
+-----BEGIN CERTIFICATE-----
+MIIDdzCCAl+gAwIBAgIEAgAAuTANBgkqhkiG9w0BAQUFADBaMQswCQYDVQQGEwJJRTESMBAGA1UE
+ChMJQmFsdGltb3JlMRMwEQYDVQQLEwpDeWJlclRydXN0MSIwIAYDVQQDExlCYWx0aW1vcmUgQ3li
+ZXJUcnVzdCBSb290MB4XDTAwMDUxMjE4NDYwMFoXDTI1MDUxMjIzNTkwMFowWjELMAkGA1UEBhMC
+SUUxEjAQBgNVBAoTCUJhbHRpbW9yZTETMBEGA1UECxMKQ3liZXJUcnVzdDEiMCAGA1UEAxMZQmFs
+dGltb3JlIEN5YmVyVHJ1c3QgUm9vdDCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKME
+uyKrmD1X6CZymrV51Cni4eiVgLGw41uOKymaZN+hXe2wCQVt2yguzmKiYv60iNoS6zjrIZ3AQSsB
+UnuId9Mcj8e6uYi1agnnc+gRQKfRzMpijS3ljwumUNKoUMMo6vWrJYeKmpYcqWe4PwzV9/lSEy/C
+G9VwcPCPwBLKBsua4dnKM3p31vjsufFoREJIE9LAwqSuXmD+tqYF/LTdB1kC1FkYmGP1pWPgkAx9
+XbIGevOF6uvUA65ehD5f/xXtabz5OTZydc93Uk3zyZAsuT3lySNTPx8kmCFcB5kpvcY67Oduhjpr
+l3RjM71oGDHweI12v/yejl0qhqdNkNwnGjkCAwEAAaNFMEMwHQYDVR0OBBYEFOWdWTCCR1jMrPoI
+VDaGezq1BE3wMBIGA1UdEwEB/wQIMAYBAf8CAQMwDgYDVR0PAQH/BAQDAgEGMA0GCSqGSIb3DQEB
+BQUAA4IBAQCFDF2O5G9RaEIFoN27TyclhAO992T9Ldcw46QQF+vaKSm2eT929hkTI7gQCvlYpNRh
+cL0EYWoSihfVCr3FvDB81ukMJY2GQE/szKN+OMY3EU/t3WgxjkzSswF07r51XgdIGn9w/xZchMB5
+hbgF/X++ZRGjD8ACtPhSNzkE1akxehi/oCr0Epn3o0WC4zxe9Z2etciefC7IpJ5OCBRLbf1wbWsa
+Y71k5h+3zvDyny67G7fyUIhzksLi4xaNmjICq44Y3ekQEe5+NauQrz4wlHrQMz2nZQ/1/I6eYs9H
+RCwBXbsdtTLSR9I4LtD+gdwyah617jzV/OeBHRnDJELqYzmp
+-----END CERTIFICATE-----
+
+Equifax Secure Global eBusiness CA
+==================================
+-----BEGIN CERTIFICATE-----
+MIICkDCCAfmgAwIBAgIBATANBgkqhkiG9w0BAQQFADBaMQswCQYDVQQGEwJVUzEcMBoGA1UEChMT
+RXF1aWZheCBTZWN1cmUgSW5jLjEtMCsGA1UEAxMkRXF1aWZheCBTZWN1cmUgR2xvYmFsIGVCdXNp
+bmVzcyBDQS0xMB4XDTk5MDYyMTA0MDAwMFoXDTIwMDYyMTA0MDAwMFowWjELMAkGA1UEBhMCVVMx
+HDAaBgNVBAoTE0VxdWlmYXggU2VjdXJlIEluYy4xLTArBgNVBAMTJEVxdWlmYXggU2VjdXJlIEds
+b2JhbCBlQnVzaW5lc3MgQ0EtMTCBnzANBgkqhkiG9w0BAQEFAAOBjQAwgYkCgYEAuucXkAJlsTRV
+PEnCUdXfp9E3j9HngXNBUmCbnaEXJnitx7HoJpQytd4zjTov2/KaelpzmKNc6fuKcxtc58O/gGzN
+qfTWK8D3+ZmqY6KxRwIP1ORROhI8bIpaVIRw28HFkM9yRcuoWcDNM50/o5brhTMhHD4ePmBudpxn
+hcXIw2ECAwEAAaNmMGQwEQYJYIZIAYb4QgEBBAQDAgAHMA8GA1UdEwEB/wQFMAMBAf8wHwYDVR0j
+BBgwFoAUvqigdHJQa0S3ySPY+6j/s1draGwwHQYDVR0OBBYEFL6ooHRyUGtEt8kj2Puo/7NXa2hs
+MA0GCSqGSIb3DQEBBAUAA4GBADDiAVGqx+pf2rnQZQ8w1j7aDRRJbpGTJxQx78T3LUX47Me/okEN
+I7SS+RkAZ70Br83gcfxaz2TE4JaY0KNA4gGK7ycH8WUBikQtBmV1UsCGECAhX2xrD2yuCRyv8qIY
+NMR1pHMc8Y3c7635s3a0kr/clRAevsvIO1qEYBlWlKlV
+-----END CERTIFICATE-----
+
+Equifax Secure eBusiness CA 1
+=============================
+-----BEGIN CERTIFICATE-----
+MIICgjCCAeugAwIBAgIBBDANBgkqhkiG9w0BAQQFADBTMQswCQYDVQQGEwJVUzEcMBoGA1UEChMT
+RXF1aWZheCBTZWN1cmUgSW5jLjEmMCQGA1UEAxMdRXF1aWZheCBTZWN1cmUgZUJ1c2luZXNzIENB
+LTEwHhcNOTkwNjIxMDQwMDAwWhcNMjAwNjIxMDQwMDAwWjBTMQswCQYDVQQGEwJVUzEcMBoGA1UE
+ChMTRXF1aWZheCBTZWN1cmUgSW5jLjEmMCQGA1UEAxMdRXF1aWZheCBTZWN1cmUgZUJ1c2luZXNz
+IENBLTEwgZ8wDQYJKoZIhvcNAQEBBQADgY0AMIGJAoGBAM4vGbwXt3fek6lfWg0XTzQaDJj0ItlZ
+1MRoRvC0NcWFAyDGr0WlIVFFQesWWDYyb+JQYmT5/VGcqiTZ9J2DKocKIdMSODRsjQBuWqDZQu4a
+IZX5UkxVWsUPOE9G+m34LjXWHXzr4vCwdYDIqROsvojvOm6rXyo4YgKwEnv+j6YDAgMBAAGjZjBk
+MBEGCWCGSAGG+EIBAQQEAwIABzAPBgNVHRMBAf8EBTADAQH/MB8GA1UdIwQYMBaAFEp4MlIR21kW
+Nl7fwRQ2QGpHfEyhMB0GA1UdDgQWBBRKeDJSEdtZFjZe38EUNkBqR3xMoTANBgkqhkiG9w0BAQQF
+AAOBgQB1W6ibAxHm6VZMzfmpTMANmvPMZWnmJXbMWbfWVMMdzZmsGd20hdXgPfxiIKeES1hl8eL5
+lSE/9dR+WB5Hh1Q+WKG1tfgq73HnvMP2sUlG4tega+VWeponmHxGYhTnyfxuAxJ5gDgdSIKN/Bf+
+KpYrtWKmpj29f5JZzVoqgrI3eQ==
+-----END CERTIFICATE-----
+
+Equifax Secure eBusiness CA 2
+=============================
+-----BEGIN CERTIFICATE-----
+MIIDIDCCAomgAwIBAgIEN3DPtTANBgkqhkiG9w0BAQUFADBOMQswCQYDVQQGEwJVUzEXMBUGA1UE
+ChMORXF1aWZheCBTZWN1cmUxJjAkBgNVBAsTHUVxdWlmYXggU2VjdXJlIGVCdXNpbmVzcyBDQS0y
+MB4XDTk5MDYyMzEyMTQ0NVoXDTE5MDYyMzEyMTQ0NVowTjELMAkGA1UEBhMCVVMxFzAVBgNVBAoT
+DkVxdWlmYXggU2VjdXJlMSYwJAYDVQQLEx1FcXVpZmF4IFNlY3VyZSBlQnVzaW5lc3MgQ0EtMjCB
+nzANBgkqhkiG9w0BAQEFAAOBjQAwgYkCgYEA5Dk5kx5SBhsoNviyoynF7Y6yEb3+6+e0dMKP/wXn
+2Z0GvxLIPw7y1tEkshHe0XMJitSxLJgJDR5QRrKDpkWNYmi7hRsgcDKqQM2mll/EcTc/BPO3QSQ5
+BxoeLmFYoBIL5aXfxavqN3HMHMg3OrmXUqesxWoklE6ce8/AatbfIb0CAwEAAaOCAQkwggEFMHAG
+A1UdHwRpMGcwZaBjoGGkXzBdMQswCQYDVQQGEwJVUzEXMBUGA1UEChMORXF1aWZheCBTZWN1cmUx
+JjAkBgNVBAsTHUVxdWlmYXggU2VjdXJlIGVCdXNpbmVzcyBDQS0yMQ0wCwYDVQQDEwRDUkwxMBoG
+A1UdEAQTMBGBDzIwMTkwNjIzMTIxNDQ1WjALBgNVHQ8EBAMCAQYwHwYDVR0jBBgwFoAUUJ4L6q9e
+uSBIplBqy/3YIHqngnYwHQYDVR0OBBYEFFCeC+qvXrkgSKZQasv92CB6p4J2MAwGA1UdEwQFMAMB
+Af8wGgYJKoZIhvZ9B0EABA0wCxsFVjMuMGMDAgbAMA0GCSqGSIb3DQEBBQUAA4GBAAyGgq3oThr1
+jokn4jVYPSm0B482UJW/bsGe68SQsoWou7dC4A8HOd/7npCy0cE+U58DRLB+S/Rv5Hwf5+Kx5Lia
+78O9zt4LMjTZ3ijtM2vE1Nc9ElirfQkty3D1E4qUoSek1nDFbZS1yX2doNLGCEnZZpum0/QL3MUm
+V+GRMOrN
+-----END CERTIFICATE-----
+
+AddTrust Low-Value Services Root
+================================
+-----BEGIN CERTIFICATE-----
+MIIEGDCCAwCgAwIBAgIBATANBgkqhkiG9w0BAQUFADBlMQswCQYDVQQGEwJTRTEUMBIGA1UEChML
+QWRkVHJ1c3QgQUIxHTAbBgNVBAsTFEFkZFRydXN0IFRUUCBOZXR3b3JrMSEwHwYDVQQDExhBZGRU
+cnVzdCBDbGFzcyAxIENBIFJvb3QwHhcNMDAwNTMwMTAzODMxWhcNMjAwNTMwMTAzODMxWjBlMQsw
+CQYDVQQGEwJTRTEUMBIGA1UEChMLQWRkVHJ1c3QgQUIxHTAbBgNVBAsTFEFkZFRydXN0IFRUUCBO
+ZXR3b3JrMSEwHwYDVQQDExhBZGRUcnVzdCBDbGFzcyAxIENBIFJvb3QwggEiMA0GCSqGSIb3DQEB
+AQUAA4IBDwAwggEKAoIBAQCWltQhSWDia+hBBwzexODcEyPNwTXH+9ZOEQpnXvUGW2ulCDtbKRY6
+54eyNAbFvAWlA3yCyykQruGIgb3WntP+LVbBFc7jJp0VLhD7Bo8wBN6ntGO0/7Gcrjyvd7ZWxbWr
+oulpOj0OM3kyP3CCkplhbY0wCI9xP6ZIVxn4JdxLZlyldI+Yrsj5wAYi56xz36Uu+1LcsRVlIPo1
+Zmne3yzxbrww2ywkEtvrNTVokMsAsJchPXQhI2U0K7t4WaPW4XY5mqRJjox0r26kmqPZm9I4XJui
+GMx1I4S+6+JNM3GOGvDC+Mcdoq0Dlyz4zyXG9rgkMbFjXZJ/Y/AlyVMuH79NAgMBAAGjgdIwgc8w
+HQYDVR0OBBYEFJWxtPCUtr3H2tERCSG+wa9J/RB7MAsGA1UdDwQEAwIBBjAPBgNVHRMBAf8EBTAD
+AQH/MIGPBgNVHSMEgYcwgYSAFJWxtPCUtr3H2tERCSG+wa9J/RB7oWmkZzBlMQswCQYDVQQGEwJT
+RTEUMBIGA1UEChMLQWRkVHJ1c3QgQUIxHTAbBgNVBAsTFEFkZFRydXN0IFRUUCBOZXR3b3JrMSEw
+HwYDVQQDExhBZGRUcnVzdCBDbGFzcyAxIENBIFJvb3SCAQEwDQYJKoZIhvcNAQEFBQADggEBACxt
+ZBsfzQ3duQH6lmM0MkhHma6X7f1yFqZzR1r0693p9db7RcwpiURdv0Y5PejuvE1Uhh4dbOMXJ0Ph
+iVYrqW9yTkkz43J8KiOavD7/KCrto/8cI7pDVwlnTUtiBi34/2ydYB7YHEt9tTEv2dB8Xfjea4MY
+eDdXL+gzB2ffHsdrKpV2ro9Xo/D0UrSpUwjP4E/TelOL/bscVjby/rK25Xa71SJlpz/+0WatC7xr
+mYbvP33zGDLKe8bjq2RGlfgmadlVg3sslgf/WSxEo8bl6ancoWOAWiFeIc9TVPC6b4nbqKqVz4vj
+ccweGyBECMB6tkD9xOQ14R0WHNC8K47Wcdk=
+-----END CERTIFICATE-----
+
+AddTrust External Root
+======================
+-----BEGIN CERTIFICATE-----
+MIIENjCCAx6gAwIBAgIBATANBgkqhkiG9w0BAQUFADBvMQswCQYDVQQGEwJTRTEUMBIGA1UEChML
+QWRkVHJ1c3QgQUIxJjAkBgNVBAsTHUFkZFRydXN0IEV4dGVybmFsIFRUUCBOZXR3b3JrMSIwIAYD
+VQQDExlBZGRUcnVzdCBFeHRlcm5hbCBDQSBSb290MB4XDTAwMDUzMDEwNDgzOFoXDTIwMDUzMDEw
+NDgzOFowbzELMAkGA1UEBhMCU0UxFDASBgNVBAoTC0FkZFRydXN0IEFCMSYwJAYDVQQLEx1BZGRU
+cnVzdCBFeHRlcm5hbCBUVFAgTmV0d29yazEiMCAGA1UEAxMZQWRkVHJ1c3QgRXh0ZXJuYWwgQ0Eg
+Um9vdDCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALf3GjPm8gAELTngTlvtH7xsD821
++iO2zt6bETOXpClMfZOfvUq8k+0DGuOPz+VtUFrWlymUWoCwSXrbLpX9uMq/NzgtHj6RQa1wVsfw
+Tz/oMp50ysiQVOnGXw94nZpAPA6sYapeFI+eh6FqUNzXmk6vBbOmcZSccbNQYArHE504B4YCqOmo
+aSYYkKtMsE8jqzpPhNjfzp/haW+710LXa0Tkx63ubUFfclpxCDezeWWkWaCUN/cALw3CknLa0Dhy
+2xSoRcRdKn23tNbE7qzNE0S3ySvdQwAl+mG5aWpYIxG3pzOPVnVZ9c0p10a3CitlttNCbxWyuHv7
+7+ldU9U0WicCAwEAAaOB3DCB2TAdBgNVHQ4EFgQUrb2YejS0Jvf6xCZU7wO94CTLVBowCwYDVR0P
+BAQDAgEGMA8GA1UdEwEB/wQFMAMBAf8wgZkGA1UdIwSBkTCBjoAUrb2YejS0Jvf6xCZU7wO94CTL
+VBqhc6RxMG8xCzAJBgNVBAYTAlNFMRQwEgYDVQQKEwtBZGRUcnVzdCBBQjEmMCQGA1UECxMdQWRk
+VHJ1c3QgRXh0ZXJuYWwgVFRQIE5ldHdvcmsxIjAgBgNVBAMTGUFkZFRydXN0IEV4dGVybmFsIENB
+IFJvb3SCAQEwDQYJKoZIhvcNAQEFBQADggEBALCb4IUlwtYj4g+WBpKdQZic2YR5gdkeWxQHIzZl
+j7DYd7usQWxHYINRsPkyPef89iYTx4AWpb9a/IfPeHmJIZriTAcKhjW88t5RxNKWt9x+Tu5w/Rw5
+6wwCURQtjr0W4MHfRnXnJK3s9EK0hZNwEGe6nQY1ShjTK3rMUUKhemPR5ruhxSvCNr4TDea9Y355
+e6cJDUCrat2PisP29owaQgVR1EX1n6diIWgVIEM8med8vSTYqZEXc4g/VhsxOBi0cQ+azcgOno4u
+G+GMmIPLHzHxREzGBHNJdmAPx/i9F4BrLunMTA5amnkPIAou1Z5jJh5VkpTYghdae9C8x49OhgQ=
+-----END CERTIFICATE-----
+
+AddTrust Public Services Root
+=============================
+-----BEGIN CERTIFICATE-----
+MIIEFTCCAv2gAwIBAgIBATANBgkqhkiG9w0BAQUFADBkMQswCQYDVQQGEwJTRTEUMBIGA1UEChML
+QWRkVHJ1c3QgQUIxHTAbBgNVBAsTFEFkZFRydXN0IFRUUCBOZXR3b3JrMSAwHgYDVQQDExdBZGRU
+cnVzdCBQdWJsaWMgQ0EgUm9vdDAeFw0wMDA1MzAxMDQxNTBaFw0yMDA1MzAxMDQxNTBaMGQxCzAJ
+BgNVBAYTAlNFMRQwEgYDVQQKEwtBZGRUcnVzdCBBQjEdMBsGA1UECxMUQWRkVHJ1c3QgVFRQIE5l
+dHdvcmsxIDAeBgNVBAMTF0FkZFRydXN0IFB1YmxpYyBDQSBSb290MIIBIjANBgkqhkiG9w0BAQEF
+AAOCAQ8AMIIBCgKCAQEA6Rowj4OIFMEg2Dybjxt+A3S72mnTRqX4jsIMEZBRpS9mVEBV6tsfSlbu
+nyNu9DnLoblv8n75XYcmYZ4c+OLspoH4IcUkzBEMP9smcnrHAZcHF/nXGCwwfQ56HmIexkvA/X1i
+d9NEHif2P0tEs7c42TkfYNVRknMDtABp4/MUTu7R3AnPdzRGULD4EfL+OHn3Bzn+UZKXC1sIXzSG
+Aa2Il+tmzV7R/9x98oTaunet3IAIx6eH1lWfl2royBFkuucZKT8Rs3iQhCBSWxHveNCD9tVIkNAw
+HM+A+WD+eeSI8t0A65RF62WUaUC6wNW0uLp9BBGo6zEFlpROWCGOn9Bg/QIDAQABo4HRMIHOMB0G
+A1UdDgQWBBSBPjfYkrAfd59ctKtzquf2NGAv+jALBgNVHQ8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB
+/zCBjgYDVR0jBIGGMIGDgBSBPjfYkrAfd59ctKtzquf2NGAv+qFopGYwZDELMAkGA1UEBhMCU0Ux
+FDASBgNVBAoTC0FkZFRydXN0IEFCMR0wGwYDVQQLExRBZGRUcnVzdCBUVFAgTmV0d29yazEgMB4G
+A1UEAxMXQWRkVHJ1c3QgUHVibGljIENBIFJvb3SCAQEwDQYJKoZIhvcNAQEFBQADggEBAAP3FUr4
+JNojVhaTdt02KLmuG7jD8WS6IBh4lSknVwW8fCr0uVFV2ocC3g8WFzH4qnkuCRO7r7IgGRLlk/lL
++YPoRNWyQSW/iHVv/xD8SlTQX/D67zZzfRs2RcYhbbQVuE7PnFylPVoAjgbjPGsye/Kf8Lb93/Ao
+GEjwxrzQvzSAlsJKsW2Ox5BF3i9nrEUEo3rcVZLJR2bYGozH7ZxOmuASu7VqTITh4SINhwBk/ox9
+Yjllpu9CtoAlEmEBqCQTcAARJl/6NVDFSMwGR+gn2HCNX2TmoUQmXiLsks3/QppEIW1cxeMiHV9H
+EufOX1362KqxMy3ZdvJOOjMMK7MtkAY=
+-----END CERTIFICATE-----
+
+AddTrust Qualified Certificates Root
+====================================
+-----BEGIN CERTIFICATE-----
+MIIEHjCCAwagAwIBAgIBATANBgkqhkiG9w0BAQUFADBnMQswCQYDVQQGEwJTRTEUMBIGA1UEChML
+QWRkVHJ1c3QgQUIxHTAbBgNVBAsTFEFkZFRydXN0IFRUUCBOZXR3b3JrMSMwIQYDVQQDExpBZGRU
+cnVzdCBRdWFsaWZpZWQgQ0EgUm9vdDAeFw0wMDA1MzAxMDQ0NTBaFw0yMDA1MzAxMDQ0NTBaMGcx
+CzAJBgNVBAYTAlNFMRQwEgYDVQQKEwtBZGRUcnVzdCBBQjEdMBsGA1UECxMUQWRkVHJ1c3QgVFRQ
+IE5ldHdvcmsxIzAhBgNVBAMTGkFkZFRydXN0IFF1YWxpZmllZCBDQSBSb290MIIBIjANBgkqhkiG
+9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5B6a/twJWoekn0e+EV+vhDTbYjx5eLfpMLXsDBwqxBb/4Oxx
+64r1EW7tTw2R0hIYLUkVAcKkIhPHEWT/IhKauY5cLwjPcWqzZwFZ8V1G87B4pfYOQnrjfxvM0PC3
+KP0q6p6zsLkEqv32x7SxuCqg+1jxGaBvcCV+PmlKfw8i2O+tCBGaKZnhqkRFmhJePp1tUvznoD1o
+L/BLcHwTOK28FSXx1s6rosAx1i+f4P8UWfyEk9mHfExUE+uf0S0R+Bg6Ot4l2ffTQO2kBhLEO+GR
+wVY18BTcZTYJbqukB8c10cIDMzZbdSZtQvESa0NvS3GU+jQd7RNuyoB/mC9suWXY6QIDAQABo4HU
+MIHRMB0GA1UdDgQWBBQ5lYtii1zJ1IC6WA+XPxUIQ8yYpzALBgNVHQ8EBAMCAQYwDwYDVR0TAQH/
+BAUwAwEB/zCBkQYDVR0jBIGJMIGGgBQ5lYtii1zJ1IC6WA+XPxUIQ8yYp6FrpGkwZzELMAkGA1UE
+BhMCU0UxFDASBgNVBAoTC0FkZFRydXN0IEFCMR0wGwYDVQQLExRBZGRUcnVzdCBUVFAgTmV0d29y
+azEjMCEGA1UEAxMaQWRkVHJ1c3QgUXVhbGlmaWVkIENBIFJvb3SCAQEwDQYJKoZIhvcNAQEFBQAD
+ggEBABmrder4i2VhlRO6aQTvhsoToMeqT2QbPxj2qC0sVY8FtzDqQmodwCVRLae/DLPt7wh/bDxG
+GuoYQ992zPlmhpwsaPXpF/gxsxjE1kh9I0xowX67ARRvxdlu3rsEQmr49lx95dr6h+sNNVJn0J6X
+dgWTP5XHAeZpVTh/EGGZyeNfpso+gmNIquIISD6q8rKFYqa0p9m9N5xotS1WfbC3P6CxB9bpT9ze
+RXEwMn8bLgn5v1Kh7sKAPgZcLlVAwRv1cEWw3F369nJad9Jjzc9YiQBCYz95OdBEsIJuQRno3eDB
+iFrRHnGTHyQwdOUeqN48Jzd/g66ed8/wMLH/S5noxqE=
+-----END CERTIFICATE-----
+
+Entrust Root Certification Authority
+====================================
+-----BEGIN CERTIFICATE-----
+MIIEkTCCA3mgAwIBAgIERWtQVDANBgkqhkiG9w0BAQUFADCBsDELMAkGA1UEBhMCVVMxFjAUBgNV
+BAoTDUVudHJ1c3QsIEluYy4xOTA3BgNVBAsTMHd3dy5lbnRydXN0Lm5ldC9DUFMgaXMgaW5jb3Jw
+b3JhdGVkIGJ5IHJlZmVyZW5jZTEfMB0GA1UECxMWKGMpIDIwMDYgRW50cnVzdCwgSW5jLjEtMCsG
+A1UEAxMkRW50cnVzdCBSb290IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MB4XDTA2MTEyNzIwMjM0
+MloXDTI2MTEyNzIwNTM0MlowgbAxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1FbnRydXN0LCBJbmMu
+MTkwNwYDVQQLEzB3d3cuZW50cnVzdC5uZXQvQ1BTIGlzIGluY29ycG9yYXRlZCBieSByZWZlcmVu
+Y2UxHzAdBgNVBAsTFihjKSAyMDA2IEVudHJ1c3QsIEluYy4xLTArBgNVBAMTJEVudHJ1c3QgUm9v
+dCBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEB
+ALaVtkNC+sZtKm9I35RMOVcF7sN5EUFoNu3s/poBj6E4KPz3EEZmLk0eGrEaTsbRwJWIsMn/MYsz
+A9u3g3s+IIRe7bJWKKf44LlAcTfFy0cOlypowCKVYhXbR9n10Cv/gkvJrT7eTNuQgFA/CYqEAOww
+Cj0Yzfv9KlmaI5UXLEWeH25DeW0MXJj+SKfFI0dcXv1u5x609mhF0YaDW6KKjbHjKYD+JXGIrb68
+j6xSlkuqUY3kEzEZ6E5Nn9uss2rVvDlUccp6en+Q3X0dgNmBu1kmwhH+5pPi94DkZfs0Nw4pgHBN
+rziGLp5/V6+eF67rHMsoIV+2HNjnogQi+dPa2MsCAwEAAaOBsDCBrTAOBgNVHQ8BAf8EBAMCAQYw
+DwYDVR0TAQH/BAUwAwEB/zArBgNVHRAEJDAigA8yMDA2MTEyNzIwMjM0MlqBDzIwMjYxMTI3MjA1
+MzQyWjAfBgNVHSMEGDAWgBRokORnpKZTgMeGZqTx90tD+4S9bTAdBgNVHQ4EFgQUaJDkZ6SmU4DH
+hmak8fdLQ/uEvW0wHQYJKoZIhvZ9B0EABBAwDhsIVjcuMTo0LjADAgSQMA0GCSqGSIb3DQEBBQUA
+A4IBAQCT1DCw1wMgKtD5Y+iRDAUgqV8ZyntyTtSx29CW+1RaGSwMCPeyvIWonX9tO1KzKtvn1ISM
+Y/YPyyYBkVBs9F8U4pN0wBOeMDpQ47RgxRzwIkSNcUesyBrJ6ZuaAGAT/3B+XxFNSRuzFVJ7yVTa
+v52Vr2ua2J7p8eRDjeIRRDq/r72DQnNSi6q7pynP9WQcCk3RvKqsnyrQ/39/2n3qse0wJcGE2jTS
+W3iDVuycNsMm4hH2Z0kdkquM++v/eu6FSqdQgPCnXEqULl8FmTxSQeDNtGPPAUO6nIPcj2A781q0
+tHuu2guQOHXvgR1m0vdXcDazv/wor3ElhVsT/h5/WrQ8
+-----END CERTIFICATE-----
+
+RSA Security 2048 v3
+====================
+-----BEGIN CERTIFICATE-----
+MIIDYTCCAkmgAwIBAgIQCgEBAQAAAnwAAAAKAAAAAjANBgkqhkiG9w0BAQUFADA6MRkwFwYDVQQK
+ExBSU0EgU2VjdXJpdHkgSW5jMR0wGwYDVQQLExRSU0EgU2VjdXJpdHkgMjA0OCBWMzAeFw0wMTAy
+MjIyMDM5MjNaFw0yNjAyMjIyMDM5MjNaMDoxGTAXBgNVBAoTEFJTQSBTZWN1cml0eSBJbmMxHTAb
+BgNVBAsTFFJTQSBTZWN1cml0eSAyMDQ4IFYzMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC
+AQEAt49VcdKA3XtpeafwGFAyPGJn9gqVB93mG/Oe2dJBVGutn3y+Gc37RqtBaB4Y6lXIL5F4iSj7
+Jylg/9+PjDvJSZu1pJTOAeo+tWN7fyb9Gd3AIb2E0S1PRsNO3Ng3OTsor8udGuorryGlwSMiuLgb
+WhOHV4PR8CDn6E8jQrAApX2J6elhc5SYcSa8LWrg903w8bYqODGBDSnhAMFRD0xS+ARaqn1y07iH
+KrtjEAMqs6FPDVpeRrc9DvV07Jmf+T0kgYim3WBU6JU2PcYJk5qjEoAAVZkZR73QpXzDuvsf9/UP
++Ky5tfQ3mBMY3oVbtwyCO4dvlTlYMNpuAWgXIszACwIDAQABo2MwYTAPBgNVHRMBAf8EBTADAQH/
+MA4GA1UdDwEB/wQEAwIBBjAfBgNVHSMEGDAWgBQHw1EwpKrpRa41JPr/JCwz0LGdjDAdBgNVHQ4E
+FgQUB8NRMKSq6UWuNST6/yQsM9CxnYwwDQYJKoZIhvcNAQEFBQADggEBAF8+hnZuuDU8TjYcHnmY
+v/3VEhF5Ug7uMYm83X/50cYVIeiKAVQNOvtUudZj1LGqlk2iQk3UUx+LEN5/Zb5gEydxiKRz44Rj
+0aRV4VCT5hsOedBnvEbIvz8XDZXmxpBp3ue0L96VfdASPz0+f00/FGj1EVDVwfSQpQgdMWD/YIwj
+VAqv/qFuxdF6Kmh4zx6CCiC0H63lhbJqaHVOrSU3lIW+vaHU6rcMSzyd6BIA8F+sDeGscGNz9395
+nzIlQnQFgCi/vcEkllgVsRch6YlL2weIZ/QVrXA+L02FO8K32/6YaCOJ4XQP3vTFhGMpG8zLB8kA
+pKnXwiJPZ9d37CAFYd4=
+-----END CERTIFICATE-----
+
+GeoTrust Global CA
+==================
+-----BEGIN CERTIFICATE-----
+MIIDVDCCAjygAwIBAgIDAjRWMA0GCSqGSIb3DQEBBQUAMEIxCzAJBgNVBAYTAlVTMRYwFAYDVQQK
+Ew1HZW9UcnVzdCBJbmMuMRswGQYDVQQDExJHZW9UcnVzdCBHbG9iYWwgQ0EwHhcNMDIwNTIxMDQw
+MDAwWhcNMjIwNTIxMDQwMDAwWjBCMQswCQYDVQQGEwJVUzEWMBQGA1UEChMNR2VvVHJ1c3QgSW5j
+LjEbMBkGA1UEAxMSR2VvVHJ1c3QgR2xvYmFsIENBMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIB
+CgKCAQEA2swYYzD99BcjGlZ+W988bDjkcbd4kdS8odhM+KhDtgPpTSEHCIjaWC9mOSm9BXiLnTjo
+BbdqfnGk5sRgprDvgOSJKA+eJdbtg/OtppHHmMlCGDUUna2YRpIuT8rxh0PBFpVXLVDviS2Aelet
+8u5fa9IAjbkU+BQVNdnARqN7csiRv8lVK83Qlz6cJmTM386DGXHKTubU1XupGc1V3sjs0l44U+Vc
+T4wt/lAjNvxm5suOpDkZALeVAjmRCw7+OC7RHQWa9k0+bw8HHa8sHo9gOeL6NlMTOdReJivbPagU
+vTLrGAMoUgRx5aszPeE4uwc2hGKceeoWMPRfwCvocWvk+QIDAQABo1MwUTAPBgNVHRMBAf8EBTAD
+AQH/MB0GA1UdDgQWBBTAephojYn7qwVkDBF9qn1luMrMTjAfBgNVHSMEGDAWgBTAephojYn7qwVk
+DBF9qn1luMrMTjANBgkqhkiG9w0BAQUFAAOCAQEANeMpauUvXVSOKVCUn5kaFOSPeCpilKInZ57Q
+zxpeR+nBsqTP3UEaBU6bS+5Kb1VSsyShNwrrZHYqLizz/Tt1kL/6cdjHPTfStQWVYrmm3ok9Nns4
+d0iXrKYgjy6myQzCsplFAMfOEVEiIuCl6rYVSAlk6l5PdPcFPseKUgzbFbS9bZvlxrFUaKnjaZC2
+mqUPuLk/IH2uSrW4nOQdtqvmlKXBx4Ot2/Unhw4EbNX/3aBd7YdStysVAq45pmp06drE57xNNB6p
+XE0zX5IJL4hmXXeXxx12E6nV5fEWCRE11azbJHFwLJhWC9kXtNHjUStedejV0NxPNO3CBWaAocvm
+Mw==
+-----END CERTIFICATE-----
+
+GeoTrust Global CA 2
+====================
+-----BEGIN CERTIFICATE-----
+MIIDZjCCAk6gAwIBAgIBATANBgkqhkiG9w0BAQUFADBEMQswCQYDVQQGEwJVUzEWMBQGA1UEChMN
+R2VvVHJ1c3QgSW5jLjEdMBsGA1UEAxMUR2VvVHJ1c3QgR2xvYmFsIENBIDIwHhcNMDQwMzA0MDUw
+MDAwWhcNMTkwMzA0MDUwMDAwWjBEMQswCQYDVQQGEwJVUzEWMBQGA1UEChMNR2VvVHJ1c3QgSW5j
+LjEdMBsGA1UEAxMUR2VvVHJ1c3QgR2xvYmFsIENBIDIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAw
+ggEKAoIBAQDvPE1APRDfO1MA4Wf+lGAVPoWI8YkNkMgoI5kF6CsgncbzYEbYwbLVjDHZ3CB5JIG/
+NTL8Y2nbsSpr7iFY8gjpeMtvy/wWUsiRxP89c96xPqfCfWbB9X5SJBri1WeR0IIQ13hLTytCOb1k
+LUCgsBDTOEhGiKEMuzozKmKY+wCdE1l/bztyqu6mD4b5BWHqZ38MN5aL5mkWRxHCJ1kDs6ZgwiFA
+Vvqgx306E+PsV8ez1q6diYD3Aecs9pYrEw15LNnA5IZ7S4wMcoKK+xfNAGw6EzywhIdLFnopsk/b
+HdQL82Y3vdj2V7teJHq4PIu5+pIaGoSe2HSPqht/XvT+RSIhAgMBAAGjYzBhMA8GA1UdEwEB/wQF
+MAMBAf8wHQYDVR0OBBYEFHE4NvICMVNHK266ZUapEBVYIAUJMB8GA1UdIwQYMBaAFHE4NvICMVNH
+K266ZUapEBVYIAUJMA4GA1UdDwEB/wQEAwIBhjANBgkqhkiG9w0BAQUFAAOCAQEAA/e1K6tdEPx7
+srJerJsOflN4WT5CBP51o62sgU7XAotexC3IUnbHLB/8gTKY0UvGkpMzNTEv/NgdRN3ggX+d6Yvh
+ZJFiCzkIjKx0nVnZellSlxG5FntvRdOW2TF9AjYPnDtuzywNA0ZF66D0f0hExghAzN4bcLUprbqL
+OzRldRtxIR0sFAqwlpW41uryZfspuk/qkZN0abby/+Ea0AzRdoXLiiW9l14sbxWZJue2Kf8i7MkC
+x1YAzUm5s2x7UwQa4qjJqhIFI8LO57sEAszAR6LkxCkvW0VXiVHuPOtSCP8HNR6fNWpHSlaY0VqF
+H4z1Ir+rzoPz4iIprn2DQKi6bA==
+-----END CERTIFICATE-----
+
+GeoTrust Universal CA
+=====================
+-----BEGIN CERTIFICATE-----
+MIIFaDCCA1CgAwIBAgIBATANBgkqhkiG9w0BAQUFADBFMQswCQYDVQQGEwJVUzEWMBQGA1UEChMN
+R2VvVHJ1c3QgSW5jLjEeMBwGA1UEAxMVR2VvVHJ1c3QgVW5pdmVyc2FsIENBMB4XDTA0MDMwNDA1
+MDAwMFoXDTI5MDMwNDA1MDAwMFowRTELMAkGA1UEBhMCVVMxFjAUBgNVBAoTDUdlb1RydXN0IElu
+Yy4xHjAcBgNVBAMTFUdlb1RydXN0IFVuaXZlcnNhbCBDQTCCAiIwDQYJKoZIhvcNAQEBBQADggIP
+ADCCAgoCggIBAKYVVaCjxuAfjJ0hUNfBvitbtaSeodlyWL0AG0y/YckUHUWCq8YdgNY96xCcOq9t
+JPi8cQGeBvV8Xx7BDlXKg5pZMK4ZyzBIle0iN430SppyZj6tlcDgFgDgEB8rMQ7XlFTTQjOgNB0e
+RXbdT8oYN+yFFXoZCPzVx5zw8qkuEKmS5j1YPakWaDwvdSEYfyh3peFhF7em6fgemdtzbvQKoiFs
+7tqqhZJmr/Z6a4LauiIINQ/PQvE1+mrufislzDoR5G2vc7J2Ha3QsnhnGqQ5HFELZ1aD/ThdDc7d
+8Lsrlh/eezJS/R27tQahsiFepdaVaH/wmZ7cRQg+59IJDTWU3YBOU5fXtQlEIGQWFwMCTFMNaN7V
+qnJNk22CDtucvc+081xdVHppCZbW2xHBjXWotM85yM48vCR85mLK4b19p71XZQvk/iXttmkQ3Cga
+Rr0BHdCXteGYO8A3ZNY9lO4L4fUorgtWv3GLIylBjobFS1J72HGrH4oVpjuDWtdYAVHGTEHZf9hB
+Z3KiKN9gg6meyHv8U3NyWfWTehd2Ds735VzZC1U0oqpbtWpU5xPKV+yXbfReBi9Fi1jUIxaS5BZu
+KGNZMN9QAZxjiRqf2xeUgnA3wySemkfWWspOqGmJch+RbNt+nhutxx9z3SxPGWX9f5NAEC7S8O08
+ni4oPmkmM8V7AgMBAAGjYzBhMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFNq7LqqwDLiIJlF0
+XG0D08DYj3rWMB8GA1UdIwQYMBaAFNq7LqqwDLiIJlF0XG0D08DYj3rWMA4GA1UdDwEB/wQEAwIB
+hjANBgkqhkiG9w0BAQUFAAOCAgEAMXjmx7XfuJRAyXHEqDXsRh3ChfMoWIawC/yOsjmPRFWrZIRc
+aanQmjg8+uUfNeVE44B5lGiku8SfPeE0zTBGi1QrlaXv9z+ZhP015s8xxtxqv6fXIwjhmF7DWgh2
+qaavdy+3YL1ERmrvl/9zlcGO6JP7/TG37FcREUWbMPEaiDnBTzynANXH/KttgCJwpQzgXQQpAvvL
+oJHRfNbDflDVnVi+QTjruXU8FdmbyUqDWcDaU/0zuzYYm4UPFd3uLax2k7nZAY1IEKj79TiG8dsK
+xr2EoyNB3tZ3b4XUhRxQ4K5RirqNPnbiucon8l+f725ZDQbYKxek0nxru18UGkiPGkzns0ccjkxF
+KyDuSN/n3QmOGKjaQI2SJhFTYXNd673nxE0pN2HrrDktZy4W1vUAg4WhzH92xH3kt0tm7wNFYGm2
+DFKWkoRepqO1pD4r2czYG0eq8kTaT/kD6PAUyz/zg97QwVTjt+gKN02LIFkDMBmhLMi9ER/frslK
+xfMnZmaGrGiR/9nmUxwPi1xpZQomyB40w11Re9epnAahNt3ViZS82eQtDF4JbAiXfKM9fJP/P6EU
+p8+1Xevb2xzEdt+Iub1FBZUbrvxGakyvSOPOrg/SfuvmbJxPgWp6ZKy7PtXny3YuxadIwVyQD8vI
+P/rmMuGNG2+k5o7Y+SlIis5z/iw=
+-----END CERTIFICATE-----
+
+GeoTrust Universal CA 2
+=======================
+-----BEGIN CERTIFICATE-----
+MIIFbDCCA1SgAwIBAgIBATANBgkqhkiG9w0BAQUFADBHMQswCQYDVQQGEwJVUzEWMBQGA1UEChMN
+R2VvVHJ1c3QgSW5jLjEgMB4GA1UEAxMXR2VvVHJ1c3QgVW5pdmVyc2FsIENBIDIwHhcNMDQwMzA0
+MDUwMDAwWhcNMjkwMzA0MDUwMDAwWjBHMQswCQYDVQQGEwJVUzEWMBQGA1UEChMNR2VvVHJ1c3Qg
+SW5jLjEgMB4GA1UEAxMXR2VvVHJ1c3QgVW5pdmVyc2FsIENBIDIwggIiMA0GCSqGSIb3DQEBAQUA
+A4ICDwAwggIKAoICAQCzVFLByT7y2dyxUxpZKeexw0Uo5dfR7cXFS6GqdHtXr0om/Nj1XqduGdt0
+DE81WzILAePb63p3NeqqWuDW6KFXlPCQo3RWlEQwAx5cTiuFJnSCegx2oG9NzkEtoBUGFF+3Qs17
+j1hhNNwqCPkuwwGmIkQcTAeC5lvO0Ep8BNMZcyfwqph/Lq9O64ceJHdqXbboW0W63MOhBW9Wjo8Q
+JqVJwy7XQYci4E+GymC16qFjwAGXEHm9ADwSbSsVsaxLse4YuU6W3Nx2/zu+z18DwPw76L5GG//a
+QMJS9/7jOvdqdzXQ2o3rXhhqMcceujwbKNZrVMaqW9eiLBsZzKIC9ptZvTdrhrVtgrrY6slWvKk2
+WP0+GfPtDCapkzj4T8FdIgbQl+rhrcZV4IErKIM6+vR7IVEAvlI4zs1meaj0gVbi0IMJR1FbUGrP
+20gaXT73y/Zl92zxlfgCOzJWgjl6W70viRu/obTo/3+NjN8D8WBOWBFM66M/ECuDmgFz2ZRthAAn
+ZqzwcEAJQpKtT5MNYQlRJNiS1QuUYbKHsu3/mjX/hVTK7URDrBs8FmtISgocQIgfksILAAX/8sgC
+SqSqqcyZlpwvWOB94b67B9xfBHJcMTTD7F8t4D1kkCLm0ey4Lt1ZrtmhN79UNdxzMk+MBB4zsslG
+8dhcyFVQyWi9qLo2CQIDAQABo2MwYTAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBR281Xh+qQ2
++/CfXGJx7Tz0RzgQKzAfBgNVHSMEGDAWgBR281Xh+qQ2+/CfXGJx7Tz0RzgQKzAOBgNVHQ8BAf8E
+BAMCAYYwDQYJKoZIhvcNAQEFBQADggIBAGbBxiPz2eAubl/oz66wsCVNK/g7WJtAJDday6sWSf+z
+dXkzoS9tcBc0kf5nfo/sm+VegqlVHy/c1FEHEv6sFj4sNcZj/NwQ6w2jqtB8zNHQL1EuxBRa3ugZ
+4T7GzKQp5y6EqgYweHZUcyiYWTjgAA1i00J9IZ+uPTqM1fp3DRgrFg5fNuH8KrUwJM/gYwx7WBr+
+mbpCErGR9Hxo4sjoryzqyX6uuyo9DRXcNJW2GHSoag/HtPQTxORb7QrSpJdMKu0vbBKJPfEncKpq
+A1Ihn0CoZ1Dy81of398j9tx4TuaYT1U6U+Pv8vSfx3zYWK8pIpe44L2RLrB27FcRz+8pRPPphXpg
+Y+RdM4kX2TGq2tbzGDVyz4crL2MjhF2EjD9XoIj8mZEoJmmZ1I+XRL6O1UixpCgp8RW04eWe3fiP
+pm8m1wk8OhwRDqZsN/etRIcsKMfYdIKz0G9KV7s1KSegi+ghp4dkNl3M2Basx7InQJJVOCiNUW7d
+FGdTbHFcJoRNdVq2fmBWqU2t+5sel/MN2dKXVHfaPRK34B7vCAas+YWH6aLcr34YEoP9VhdBLtUp
+gn2Z9DH2canPLAEnpQW5qrJITirvn5NSUZU8UnOOVkwXQMAJKOSLakhT2+zNVVXxxvjpoixMptEm
+X36vWkzaH6byHCx+rgIW0lbQL1dTR+iS
+-----END CERTIFICATE-----
+
+UTN-USER First-Network Applications
+===================================
+-----BEGIN CERTIFICATE-----
+MIIEZDCCA0ygAwIBAgIQRL4Mi1AAJLQR0zYwS8AzdzANBgkqhkiG9w0BAQUFADCBozELMAkGA1UE
+BhMCVVMxCzAJBgNVBAgTAlVUMRcwFQYDVQQHEw5TYWx0IExha2UgQ2l0eTEeMBwGA1UEChMVVGhl
+IFVTRVJUUlVTVCBOZXR3b3JrMSEwHwYDVQQLExhodHRwOi8vd3d3LnVzZXJ0cnVzdC5jb20xKzAp
+BgNVBAMTIlVUTi1VU0VSRmlyc3QtTmV0d29yayBBcHBsaWNhdGlvbnMwHhcNOTkwNzA5MTg0ODM5
+WhcNMTkwNzA5MTg1NzQ5WjCBozELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAlVUMRcwFQYDVQQHEw5T
+YWx0IExha2UgQ2l0eTEeMBwGA1UEChMVVGhlIFVTRVJUUlVTVCBOZXR3b3JrMSEwHwYDVQQLExho
+dHRwOi8vd3d3LnVzZXJ0cnVzdC5jb20xKzApBgNVBAMTIlVUTi1VU0VSRmlyc3QtTmV0d29yayBB
+cHBsaWNhdGlvbnMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCz+5Gh5DZVhawGNFug
+mliy+LUPBXeDrjKxdpJo7CNKyXY/45y2N3kDuatpjQclthln5LAbGHNhSuh+zdMvZOOmfAz6F4Cj
+DUeJT1FxL+78P/m4FoCHiZMlIJpDgmkkdihZNaEdwH+DBmQWICzTSaSFtMBhf1EI+GgVkYDLpdXu
+Ozr0hAReYFmnjDRy7rh4xdE7EkpvfmUnuaRVxblvQ6TFHSyZwFKkeEwVs0CYCGtDxgGwenv1axwi
+P8vv/6jQOkt2FZ7S0cYu49tXGzKiuG/ohqY/cKvlcJKrRB5AUPuco2LkbG6gyN7igEL66S/ozjIE
+j3yNtxyjNTwV3Z7DrpelAgMBAAGjgZEwgY4wCwYDVR0PBAQDAgHGMA8GA1UdEwEB/wQFMAMBAf8w
+HQYDVR0OBBYEFPqGydvguul49Uuo1hXf8NPhahQ8ME8GA1UdHwRIMEYwRKBCoECGPmh0dHA6Ly9j
+cmwudXNlcnRydXN0LmNvbS9VVE4tVVNFUkZpcnN0LU5ldHdvcmtBcHBsaWNhdGlvbnMuY3JsMA0G
+CSqGSIb3DQEBBQUAA4IBAQCk8yXM0dSRgyLQzDKrm5ZONJFUICU0YV8qAhXhi6r/fWRRzwr/vH3Y
+IWp4yy9Rb/hCHTO967V7lMPDqaAt39EpHx3+jz+7qEUqf9FuVSTiuwL7MT++6LzsQCv4AdRWOOTK
+RIK1YSAhZ2X28AvnNPilwpyjXEAfhZOVBt5P1CeptqX8Fs1zMT+4ZSfP1FMa8Kxun08FDAOBp4Qp
+xFq9ZFdyrTvPNximmMatBrTcCKME1SmklpoSZ0qMYEWd8SOasACcaLWYUNPvji6SZbFIPiG+FTAq
+DbUMo2s/rn9X9R+WfN9v3YIwLGUbQErNaLly7HF27FSOH4UMAWr6pjisH8SE
+-----END CERTIFICATE-----
+
+America Online Root Certification Authority 1
+=============================================
+-----BEGIN CERTIFICATE-----
+MIIDpDCCAoygAwIBAgIBATANBgkqhkiG9w0BAQUFADBjMQswCQYDVQQGEwJVUzEcMBoGA1UEChMT
+QW1lcmljYSBPbmxpbmUgSW5jLjE2MDQGA1UEAxMtQW1lcmljYSBPbmxpbmUgUm9vdCBDZXJ0aWZp
+Y2F0aW9uIEF1dGhvcml0eSAxMB4XDTAyMDUyODA2MDAwMFoXDTM3MTExOTIwNDMwMFowYzELMAkG
+A1UEBhMCVVMxHDAaBgNVBAoTE0FtZXJpY2EgT25saW5lIEluYy4xNjA0BgNVBAMTLUFtZXJpY2Eg
+T25saW5lIFJvb3QgQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkgMTCCASIwDQYJKoZIhvcNAQEBBQAD
+ggEPADCCAQoCggEBAKgv6KRpBgNHw+kqmP8ZonCaxlCyfqXfaE0bfA+2l2h9LaaLl+lkhsmj76CG
+v2BlnEtUiMJIxUo5vxTjWVXlGbR0yLQFOVwWpeKVBeASrlmLojNoWBym1BW32J/X3HGrfpq/m44z
+DyL9Hy7nBzbvYjnF3cu6JRQj3gzGPTzOggjmZj7aUTsWOqMFf6Dch9Wc/HKpoH145LcxVR5lu9Rh
+sCFg7RAycsWSJR74kEoYeEfffjA3PlAb2xzTa5qGUwew76wGePiEmf4hjUyAtgyC9mZweRrTT6PP
+8c9GsEsPPt2IYriMqQkoO3rHl+Ee5fSfwMCuJKDIodkP1nsmgmkyPacCAwEAAaNjMGEwDwYDVR0T
+AQH/BAUwAwEB/zAdBgNVHQ4EFgQUAK3Zo/Z59m50qX8zPYEX10zPM94wHwYDVR0jBBgwFoAUAK3Z
+o/Z59m50qX8zPYEX10zPM94wDgYDVR0PAQH/BAQDAgGGMA0GCSqGSIb3DQEBBQUAA4IBAQB8itEf
+GDeC4Liwo+1WlchiYZwFos3CYiZhzRAW18y0ZTTQEYqtqKkFZu90821fnZmv9ov761KyBZiibyrF
+VL0lvV+uyIbqRizBs73B6UlwGBaXCBOMIOAbLjpHyx7kADCVW/RFo8AasAFOq73AI25jP4BKxQft
+3OJvx8Fi8eNy1gTIdGcL+oiroQHIb/AUr9KZzVGTfu0uOMe9zkZQPXLjeSWdm4grECDdpbgyn43g
+Kd8hdIaC2y+CMMbHNYaz+ZZfRtsMRf3zUMNvxsNIrUam4SdHCh0Om7bCd39j8uB9Gr784N/Xx6ds
+sPmuujz9dLQR6FgNgLzTqIA6me11zEZ7
+-----END CERTIFICATE-----
+
+America Online Root Certification Authority 2
+=============================================
+-----BEGIN CERTIFICATE-----
+MIIFpDCCA4ygAwIBAgIBATANBgkqhkiG9w0BAQUFADBjMQswCQYDVQQGEwJVUzEcMBoGA1UEChMT
+QW1lcmljYSBPbmxpbmUgSW5jLjE2MDQGA1UEAxMtQW1lcmljYSBPbmxpbmUgUm9vdCBDZXJ0aWZp
+Y2F0aW9uIEF1dGhvcml0eSAyMB4XDTAyMDUyODA2MDAwMFoXDTM3MDkyOTE0MDgwMFowYzELMAkG
+A1UEBhMCVVMxHDAaBgNVBAoTE0FtZXJpY2EgT25saW5lIEluYy4xNjA0BgNVBAMTLUFtZXJpY2Eg
+T25saW5lIFJvb3QgQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkgMjCCAiIwDQYJKoZIhvcNAQEBBQAD
+ggIPADCCAgoCggIBAMxBRR3pPU0Q9oyxQcngXssNt79Hc9PwVU3dxgz6sWYFas14tNwC206B89en
+fHG8dWOgXeMHDEjsJcQDIPT/DjsS/5uN4cbVG7RtIuOx238hZK+GvFciKtZHgVdEglZTvYYUAQv8
+f3SkWq7xuhG1m1hagLQ3eAkzfDJHA1zEpYNI9FdWboE2JxhP7JsowtS013wMPgwr38oE18aO6lhO
+qKSlGBxsRZijQdEt0sdtjRnxrXm3gT+9BoInLRBYBbV4Bbkv2wxrkJB+FFk4u5QkE+XRnRTf04JN
+RvCAOVIyD+OEsnpD8l7eXz8d3eOyG6ChKiMDbi4BFYdcpnV1x5dhvt6G3NRI270qv0pV2uh9UPu0
+gBe4lL8BPeraunzgWGcXuVjgiIZGZ2ydEEdYMtA1fHkqkKJaEBEjNa0vzORKW6fIJ/KD3l67Xnfn
+6KVuY8INXWHQjNJsWiEOyiijzirplcdIz5ZvHZIlyMbGwcEMBawmxNJ10uEqZ8A9W6Wa6897Gqid
+FEXlD6CaZd4vKL3Ob5Rmg0gp2OpljK+T2WSfVVcmv2/LNzGZo2C7HK2JNDJiuEMhBnIMoVxtRsX6
+Kc8w3onccVvdtjc+31D1uAclJuW8tf48ArO3+L5DwYcRlJ4jbBeKuIonDFRH8KmzwICMoCfrHRnj
+B453cMor9H124HhnAgMBAAGjYzBhMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFE1FwWg4u3Op
+aaEg5+31IqEjFNeeMB8GA1UdIwQYMBaAFE1FwWg4u3OpaaEg5+31IqEjFNeeMA4GA1UdDwEB/wQE
+AwIBhjANBgkqhkiG9w0BAQUFAAOCAgEAZ2sGuV9FOypLM7PmG2tZTiLMubekJcmnxPBUlgtk87FY
+T15R/LKXeydlwuXK5w0MJXti4/qftIe3RUavg6WXSIylvfEWK5t2LHo1YGwRgJfMqZJS5ivmae2p
++DYtLHe/YUjRYwu5W1LtGLBDQiKmsXeu3mnFzcccobGlHBD7GL4acN3Bkku+KVqdPzW+5X1R+FXg
+JXUjhx5c3LqdsKyzadsXg8n33gy8CNyRnqjQ1xU3c6U1uPx+xURABsPr+CKAXEfOAuMRn0T//Zoy
+zH1kUQ7rVyZ2OuMeIjzCpjbdGe+n/BLzJsBZMYVMnNjP36TMzCmT/5RtdlwTCJfy7aULTd3oyWgO
+ZtMADjMSW7yV5TKQqLPGbIOtd+6Lfn6xqavT4fG2wLHqiMDn05DpKJKUe2h7lyoKZy2FAjgQ5ANh
+1NolNscIWC2hp1GvMApJ9aZphwctREZ2jirlmjvXGKL8nDgQzMY70rUXOm/9riW99XJZZLF0Kjhf
+GEzfz3EEWjbUvy+ZnOjZurGV5gJLIaFb1cFPj65pbVPbAZO1XB4Y3WRayhgoPmMEEf0cjQAPuDff
+Z4qdZqkCapH/E8ovXYO8h5Ns3CRRFgQlZvqz2cK6Kb6aSDiCmfS/O0oxGfm/jiEzFMpPVF/7zvuP
+cX/9XhmgD0uRuMRUvAawRY8mkaKO/qk=
+-----END CERTIFICATE-----
+
+Visa eCommerce Root
+===================
+-----BEGIN CERTIFICATE-----
+MIIDojCCAoqgAwIBAgIQE4Y1TR0/BvLB+WUF1ZAcYjANBgkqhkiG9w0BAQUFADBrMQswCQYDVQQG
+EwJVUzENMAsGA1UEChMEVklTQTEvMC0GA1UECxMmVmlzYSBJbnRlcm5hdGlvbmFsIFNlcnZpY2Ug
+QXNzb2NpYXRpb24xHDAaBgNVBAMTE1Zpc2EgZUNvbW1lcmNlIFJvb3QwHhcNMDIwNjI2MDIxODM2
+WhcNMjIwNjI0MDAxNjEyWjBrMQswCQYDVQQGEwJVUzENMAsGA1UEChMEVklTQTEvMC0GA1UECxMm
+VmlzYSBJbnRlcm5hdGlvbmFsIFNlcnZpY2UgQXNzb2NpYXRpb24xHDAaBgNVBAMTE1Zpc2EgZUNv
+bW1lcmNlIFJvb3QwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCvV95WHm6h2mCxlCfL
+F9sHP4CFT8icttD0b0/Pmdjh28JIXDqsOTPHH2qLJj0rNfVIsZHBAk4ElpF7sDPwsRROEW+1QK8b
+RaVK7362rPKgH1g/EkZgPI2h4H3PVz4zHvtH8aoVlwdVZqW1LS7YgFmypw23RuwhY/81q6UCzyr0
+TP579ZRdhE2o8mCP2w4lPJ9zcc+U30rq299yOIzzlr3xF7zSujtFWsan9sYXiwGd/BmoKoMWuDpI
+/k4+oKsGGelT84ATB+0tvz8KPFUgOSwsAGl0lUq8ILKpeeUYiZGo3BxN77t+Nwtd/jmliFKMAGzs
+GHxBvfaLdXe6YJ2E5/4tAgMBAAGjQjBAMA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/BAQDAgEG
+MB0GA1UdDgQWBBQVOIMPPyw/cDMezUb+B4wg4NfDtzANBgkqhkiG9w0BAQUFAAOCAQEAX/FBfXxc
+CLkr4NWSR/pnXKUTwwMhmytMiUbPWU3J/qVAtmPN3XEolWcRzCSs00Rsca4BIGsDoo8Ytyk6feUW
+YFN4PMCvFYP3j1IzJL1kk5fui/fbGKhtcbP3LBfQdCVp9/5rPJS+TUtBjE7ic9DjkCJzQ83z7+pz
+zkWKsKZJ/0x9nXGIxHYdkFsd7v3M9+79YKWxehZx0RbQfBI8bGmX265fOZpwLwU8GUYEmSA20GBu
+YQa7FkKMcPcw++DbZqMAAb3mLNqRX6BGi01qnD093QVG/na/oAo85ADmJ7f/hC3euiInlhBx6yLt
+398znM/jra6O1I7mT1GvFpLgXPYHDw==
+-----END CERTIFICATE-----
+
+Certum Root CA
+==============
+-----BEGIN CERTIFICATE-----
+MIIDDDCCAfSgAwIBAgIDAQAgMA0GCSqGSIb3DQEBBQUAMD4xCzAJBgNVBAYTAlBMMRswGQYDVQQK
+ExJVbml6ZXRvIFNwLiB6IG8uby4xEjAQBgNVBAMTCUNlcnR1bSBDQTAeFw0wMjA2MTExMDQ2Mzla
+Fw0yNzA2MTExMDQ2MzlaMD4xCzAJBgNVBAYTAlBMMRswGQYDVQQKExJVbml6ZXRvIFNwLiB6IG8u
+by4xEjAQBgNVBAMTCUNlcnR1bSBDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAM6x
+wS7TT3zNJc4YPk/EjG+AanPIW1H4m9LcuwBcsaD8dQPugfCI7iNS6eYVM42sLQnFdvkrOYCJ5JdL
+kKWoePhzQ3ukYbDYWMzhbGZ+nPMJXlVjhNWo7/OxLjBos8Q82KxujZlakE403Daaj4GIULdtlkIJ
+89eVgw1BS7Bqa/j8D35in2fE7SZfECYPCE/wpFcozo+47UX2bu4lXapuOb7kky/ZR6By6/qmW6/K
+Uz/iDsaWVhFu9+lmqSbYf5VT7QqFiLpPKaVCjF62/IUgAKpoC6EahQGcxEZjgoi2IrHu/qpGWX7P
+NSzVttpd90gzFFS269lvzs2I1qsb2pY7HVkCAwEAAaMTMBEwDwYDVR0TAQH/BAUwAwEB/zANBgkq
+hkiG9w0BAQUFAAOCAQEAuI3O7+cUus/usESSbLQ5PqKEbq24IXfS1HeCh+YgQYHu4vgRt2PRFze+
+GXYkHAQaTOs9qmdvLdTN/mUxcMUbpgIKumB7bVjCmkn+YzILa+M6wKyrO7Do0wlRjBCDxjTgxSvg
+GrZgFCdsMneMvLJymM/NzD+5yCRCFNZX/OYmQ6kd5YCQzgNUKD73P9P4Te1qCjqTE5s7FCMTY5w/
+0YcneeVMUeMBrYVdGjux1XMQpNPyvG5k9VpWkKjHDkx0Dy5xO/fIR/RpbxXyEV6DHpx8Uq79AtoS
+qFlnGNu8cN2bsWntgM6JQEhqDjXKKWYVIZQs6GAqm4VKQPNriiTsBhYscw==
+-----END CERTIFICATE-----
+
+Comodo AAA Services root
+========================
+-----BEGIN CERTIFICATE-----
+MIIEMjCCAxqgAwIBAgIBATANBgkqhkiG9w0BAQUFADB7MQswCQYDVQQGEwJHQjEbMBkGA1UECAwS
+R3JlYXRlciBNYW5jaGVzdGVyMRAwDgYDVQQHDAdTYWxmb3JkMRowGAYDVQQKDBFDb21vZG8gQ0Eg
+TGltaXRlZDEhMB8GA1UEAwwYQUFBIENlcnRpZmljYXRlIFNlcnZpY2VzMB4XDTA0MDEwMTAwMDAw
+MFoXDTI4MTIzMTIzNTk1OVowezELMAkGA1UEBhMCR0IxGzAZBgNVBAgMEkdyZWF0ZXIgTWFuY2hl
+c3RlcjEQMA4GA1UEBwwHU2FsZm9yZDEaMBgGA1UECgwRQ29tb2RvIENBIExpbWl0ZWQxITAfBgNV
+BAMMGEFBQSBDZXJ0aWZpY2F0ZSBTZXJ2aWNlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoC
+ggEBAL5AnfRu4ep2hxxNRUSOvkbIgwadwSr+GB+O5AL686tdUIoWMQuaBtDFcCLNSS1UY8y2bmhG
+C1Pqy0wkwLxyTurxFa70VJoSCsN6sjNg4tqJVfMiWPPe3M/vg4aijJRPn2jymJBGhCfHdr/jzDUs
+i14HZGWCwEiwqJH5YZ92IFCokcdmtet4YgNW8IoaE+oxox6gmf049vYnMlhvB/VruPsUK6+3qszW
+Y19zjNoFmag4qMsXeDZRrOme9Hg6jc8P2ULimAyrL58OAd7vn5lJ8S3frHRNG5i1R8XlKdH5kBjH
+Ypy+g8cmez6KJcfA3Z3mNWgQIJ2P2N7Sw4ScDV7oL8kCAwEAAaOBwDCBvTAdBgNVHQ4EFgQUoBEK
+Iz6W8Qfs4q8p74Klf9AwpLQwDgYDVR0PAQH/BAQDAgEGMA8GA1UdEwEB/wQFMAMBAf8wewYDVR0f
+BHQwcjA4oDagNIYyaHR0cDovL2NybC5jb21vZG9jYS5jb20vQUFBQ2VydGlmaWNhdGVTZXJ2aWNl
+cy5jcmwwNqA0oDKGMGh0dHA6Ly9jcmwuY29tb2RvLm5ldC9BQUFDZXJ0aWZpY2F0ZVNlcnZpY2Vz
+LmNybDANBgkqhkiG9w0BAQUFAAOCAQEACFb8AvCb6P+k+tZ7xkSAzk/ExfYAWMymtrwUSWgEdujm
+7l3sAg9g1o1QGE8mTgHj5rCl7r+8dFRBv/38ErjHT1r0iWAFf2C3BUrz9vHCv8S5dIa2LX1rzNLz
+Rt0vxuBqw8M0Ayx9lt1awg6nCpnBBYurDC/zXDrPbDdVCYfeU0BsWO/8tqtlbgT2G9w84FoVxp7Z
+8VlIMCFlA2zs6SFz7JsDoeA3raAVGI/6ugLOpyypEBMs1OUIJqsil2D4kF501KKaU73yqWjgom7C
+12yxow+ev+to51byrvLjKzg6CYG1a4XXvi3tPxq3smPi9WIsgtRqAEFQ8TmDn5XpNpaYbg==
+-----END CERTIFICATE-----
+
+Comodo Secure Services root
+===========================
+-----BEGIN CERTIFICATE-----
+MIIEPzCCAyegAwIBAgIBATANBgkqhkiG9w0BAQUFADB+MQswCQYDVQQGEwJHQjEbMBkGA1UECAwS
+R3JlYXRlciBNYW5jaGVzdGVyMRAwDgYDVQQHDAdTYWxmb3JkMRowGAYDVQQKDBFDb21vZG8gQ0Eg
+TGltaXRlZDEkMCIGA1UEAwwbU2VjdXJlIENlcnRpZmljYXRlIFNlcnZpY2VzMB4XDTA0MDEwMTAw
+MDAwMFoXDTI4MTIzMTIzNTk1OVowfjELMAkGA1UEBhMCR0IxGzAZBgNVBAgMEkdyZWF0ZXIgTWFu
+Y2hlc3RlcjEQMA4GA1UEBwwHU2FsZm9yZDEaMBgGA1UECgwRQ29tb2RvIENBIExpbWl0ZWQxJDAi
+BgNVBAMMG1NlY3VyZSBDZXJ0aWZpY2F0ZSBTZXJ2aWNlczCCASIwDQYJKoZIhvcNAQEBBQADggEP
+ADCCAQoCggEBAMBxM4KK0HDrc4eCQNUd5MvJDkKQ+d40uaG6EfQlhfPMcm3ye5drswfxdySRXyWP
+9nQ95IDC+DwN879A6vfIUtFyb+/Iq0G4bi4XKpVpDM3SHpR7LZQdqnXXs5jLrLxkU0C8j6ysNstc
+rbvd4JQX7NFc0L/vpZXJkMWwrPsbQ996CF23uPJAGysnnlDOXmWCiIxe004MeuoIkbY2qitC++rC
+oznl2yY4rYsK7hljxxwk3wN42ubqwUcaCwtGCd0C/N7Lh1/XMGNooa7cMqG6vv5Eq2i2pRcV/b3V
+p6ea5EQz6YiO/O1R65NxTq0B50SOqy3LqP4BSUjwwN3HaNiS/j0CAwEAAaOBxzCBxDAdBgNVHQ4E
+FgQUPNiTiMLAggnMAZkGkyDpnnAJY08wDgYDVR0PAQH/BAQDAgEGMA8GA1UdEwEB/wQFMAMBAf8w
+gYEGA1UdHwR6MHgwO6A5oDeGNWh0dHA6Ly9jcmwuY29tb2RvY2EuY29tL1NlY3VyZUNlcnRpZmlj
+YXRlU2VydmljZXMuY3JsMDmgN6A1hjNodHRwOi8vY3JsLmNvbW9kby5uZXQvU2VjdXJlQ2VydGlm
+aWNhdGVTZXJ2aWNlcy5jcmwwDQYJKoZIhvcNAQEFBQADggEBAIcBbSMdflsXfcFhMs+P5/OKlFlm
+4J4oqF7Tt/Q05qo5spcWxYJvMqTpjOev/e/C6LlLqqP05tqNZSH7uoDrJiiFGv45jN5bBAS0VPmj
+Z55B+glSzAVIqMk/IQQezkhr/IXownuvf7fM+F86/TXGDe+X3EyrEeFryzHRbPtIgKvcnDe4IRRL
+DXE97IMzbtFuMhbsmMcWi1mmNKsFVy2T96oTy9IT4rcuO81rUBcJaD61JlfutuC23bkpgHl9j6Pw
+pCikFcSF9CfUa7/lXORlAnZUtOM3ZiTTGWHIUhDlizeauan5Hb/qmZJhlv8BzaFfDbxxvA6sCx1H
+RR3B7Hzs/Sk=
+-----END CERTIFICATE-----
+
+Comodo Trusted Services root
+============================
+-----BEGIN CERTIFICATE-----
+MIIEQzCCAyugAwIBAgIBATANBgkqhkiG9w0BAQUFADB/MQswCQYDVQQGEwJHQjEbMBkGA1UECAwS
+R3JlYXRlciBNYW5jaGVzdGVyMRAwDgYDVQQHDAdTYWxmb3JkMRowGAYDVQQKDBFDb21vZG8gQ0Eg
+TGltaXRlZDElMCMGA1UEAwwcVHJ1c3RlZCBDZXJ0aWZpY2F0ZSBTZXJ2aWNlczAeFw0wNDAxMDEw
+MDAwMDBaFw0yODEyMzEyMzU5NTlaMH8xCzAJBgNVBAYTAkdCMRswGQYDVQQIDBJHcmVhdGVyIE1h
+bmNoZXN0ZXIxEDAOBgNVBAcMB1NhbGZvcmQxGjAYBgNVBAoMEUNvbW9kbyBDQSBMaW1pdGVkMSUw
+IwYDVQQDDBxUcnVzdGVkIENlcnRpZmljYXRlIFNlcnZpY2VzMIIBIjANBgkqhkiG9w0BAQEFAAOC
+AQ8AMIIBCgKCAQEA33FvNlhTWvI2VFeAxHQIIO0Yfyod5jWaHiWsnOWWfnJSoBVC21ndZHoa0Lh7
+3TkVvFVIxO06AOoxEbrycXQaZ7jPM8yoMa+j49d/vzMtTGo87IvDktJTdyR0nAducPy9C1t2ul/y
+/9c3S0pgePfw+spwtOpZqqPOSC+pw7ILfhdyFgymBwwbOM/JYrc/oJOlh0Hyt3BAd9i+FHzjqMB6
+juljatEPmsbS9Is6FARW1O24zG71++IsWL1/T2sr92AkWCTOJu80kTrV44HQsvAEAtdbtz6SrGsS
+ivnkBbA7kUlcsutT6vifR4buv5XAwAaf0lteERv0xwQ1KdJVXOTt6wIDAQABo4HJMIHGMB0GA1Ud
+DgQWBBTFe1i97doladL3WRaoszLAeydb9DAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB
+/zCBgwYDVR0fBHwwejA8oDqgOIY2aHR0cDovL2NybC5jb21vZG9jYS5jb20vVHJ1c3RlZENlcnRp
+ZmljYXRlU2VydmljZXMuY3JsMDqgOKA2hjRodHRwOi8vY3JsLmNvbW9kby5uZXQvVHJ1c3RlZENl
+cnRpZmljYXRlU2VydmljZXMuY3JsMA0GCSqGSIb3DQEBBQUAA4IBAQDIk4E7ibSvuIQSTI3S8Ntw
+uleGFTQQuS9/HrCoiWChisJ3DFBKmwCL2Iv0QeLQg4pKHBQGsKNoBXAxMKdTmw7pSqBYaWcOrp32
+pSxBvzwGa+RZzG0Q8ZZvH9/0BAKkn0U+yNj6NkZEUD+Cl5EfKNsYEYwq5GWDVxISjBc/lDb+XbDA
+BHcTuPQV1T84zJQ6VdCsmPW6AF/ghhmBeC8owH7TzEIK9a5QoNE+xqFx7D+gIIxmOom0jtTYsU0l
+R+4viMi14QVFwL4Ucd56/Y57fU0IlqUSc/AtyjcndBInTMu2l+nZrghtWjlA3QVHdWpaIbOjGM9O
+9y5Xt5hwXsjEeLBi
+-----END CERTIFICATE-----
+
+QuoVadis Root CA
+================
+-----BEGIN CERTIFICATE-----
+MIIF0DCCBLigAwIBAgIEOrZQizANBgkqhkiG9w0BAQUFADB/MQswCQYDVQQGEwJCTTEZMBcGA1UE
+ChMQUXVvVmFkaXMgTGltaXRlZDElMCMGA1UECxMcUm9vdCBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0
+eTEuMCwGA1UEAxMlUXVvVmFkaXMgUm9vdCBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eTAeFw0wMTAz
+MTkxODMzMzNaFw0yMTAzMTcxODMzMzNaMH8xCzAJBgNVBAYTAkJNMRkwFwYDVQQKExBRdW9WYWRp
+cyBMaW1pdGVkMSUwIwYDVQQLExxSb290IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MS4wLAYDVQQD
+EyVRdW9WYWRpcyBSb290IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MIIBIjANBgkqhkiG9w0BAQEF
+AAOCAQ8AMIIBCgKCAQEAv2G1lVO6V/z68mcLOhrfEYBklbTRvM16z/Ypli4kVEAkOPcahdxYTMuk
+J0KX0J+DisPkBgNbAKVRHnAEdOLB1Dqr1607BxgFjv2DrOpm2RgbaIr1VxqYuvXtdj182d6UajtL
+F8HVj71lODqV0D1VNk7feVcxKh7YWWVJWCCYfqtffp/p1k3sg3Spx2zY7ilKhSoGFPlU5tPaZQeL
+YzcS19Dsw3sgQUSj7cugF+FxZc4dZjH3dgEZyH0DWLaVSR2mEiboxgx24ONmy+pdpibu5cxfvWen
+AScOospUxbF6lR1xHkopigPcakXBpBlebzbNw6Kwt/5cOOJSvPhEQ+aQuwIDAQABo4ICUjCCAk4w
+PQYIKwYBBQUHAQEEMTAvMC0GCCsGAQUFBzABhiFodHRwczovL29jc3AucXVvdmFkaXNvZmZzaG9y
+ZS5jb20wDwYDVR0TAQH/BAUwAwEB/zCCARoGA1UdIASCAREwggENMIIBCQYJKwYBBAG+WAABMIH7
+MIHUBggrBgEFBQcCAjCBxxqBxFJlbGlhbmNlIG9uIHRoZSBRdW9WYWRpcyBSb290IENlcnRpZmlj
+YXRlIGJ5IGFueSBwYXJ0eSBhc3N1bWVzIGFjY2VwdGFuY2Ugb2YgdGhlIHRoZW4gYXBwbGljYWJs
+ZSBzdGFuZGFyZCB0ZXJtcyBhbmQgY29uZGl0aW9ucyBvZiB1c2UsIGNlcnRpZmljYXRpb24gcHJh
+Y3RpY2VzLCBhbmQgdGhlIFF1b1ZhZGlzIENlcnRpZmljYXRlIFBvbGljeS4wIgYIKwYBBQUHAgEW
+Fmh0dHA6Ly93d3cucXVvdmFkaXMuYm0wHQYDVR0OBBYEFItLbe3TKbkGGew5Oanwl4Rqy+/fMIGu
+BgNVHSMEgaYwgaOAFItLbe3TKbkGGew5Oanwl4Rqy+/foYGEpIGBMH8xCzAJBgNVBAYTAkJNMRkw
+FwYDVQQKExBRdW9WYWRpcyBMaW1pdGVkMSUwIwYDVQQLExxSb290IENlcnRpZmljYXRpb24gQXV0
+aG9yaXR5MS4wLAYDVQQDEyVRdW9WYWRpcyBSb290IENlcnRpZmljYXRpb24gQXV0aG9yaXR5ggQ6
+tlCLMA4GA1UdDwEB/wQEAwIBBjANBgkqhkiG9w0BAQUFAAOCAQEAitQUtf70mpKnGdSkfnIYj9lo
+fFIk3WdvOXrEql494liwTXCYhGHoG+NpGA7O+0dQoE7/8CQfvbLO9Sf87C9TqnN7Az10buYWnuul
+LsS/VidQK2K6vkscPFVcQR0kvoIgR13VRH56FmjffU1RcHhXHTMe/QKZnAzNCgVPx7uOpHX6Sm2x
+gI4JVrmcGmD+XcHXetwReNDWXcG31a0ymQM6isxUJTkxgXsTIlG6Rmyhu576BGxJJnSP0nPrzDCi
+5upZIof4l/UO/erMkqQWxFIY6iHOsfHmhIHluqmGKPJDWl0Snawe2ajlCmqnf6CHKc/yiU3U7MXi
+5nrQNiOKSnQ2+Q==
+-----END CERTIFICATE-----
+
+QuoVadis Root CA 2
+==================
+-----BEGIN CERTIFICATE-----
+MIIFtzCCA5+gAwIBAgICBQkwDQYJKoZIhvcNAQEFBQAwRTELMAkGA1UEBhMCQk0xGTAXBgNVBAoT
+EFF1b1ZhZGlzIExpbWl0ZWQxGzAZBgNVBAMTElF1b1ZhZGlzIFJvb3QgQ0EgMjAeFw0wNjExMjQx
+ODI3MDBaFw0zMTExMjQxODIzMzNaMEUxCzAJBgNVBAYTAkJNMRkwFwYDVQQKExBRdW9WYWRpcyBM
+aW1pdGVkMRswGQYDVQQDExJRdW9WYWRpcyBSb290IENBIDIwggIiMA0GCSqGSIb3DQEBAQUAA4IC
+DwAwggIKAoICAQCaGMpLlA0ALa8DKYrwD4HIrkwZhR0In6spRIXzL4GtMh6QRr+jhiYaHv5+HBg6
+XJxgFyo6dIMzMH1hVBHL7avg5tKifvVrbxi3Cgst/ek+7wrGsxDp3MJGF/hd/aTa/55JWpzmM+Yk
+lvc/ulsrHHo1wtZn/qtmUIttKGAr79dgw8eTvI02kfN/+NsRE8Scd3bBrrcCaoF6qUWD4gXmuVbB
+lDePSHFjIuwXZQeVikvfj8ZaCuWw419eaxGrDPmF60Tp+ARz8un+XJiM9XOva7R+zdRcAitMOeGy
+lZUtQofX1bOQQ7dsE/He3fbE+Ik/0XX1ksOR1YqI0JDs3G3eicJlcZaLDQP9nL9bFqyS2+r+eXyt
+66/3FsvbzSUr5R/7mp/iUcw6UwxI5g69ybR2BlLmEROFcmMDBOAENisgGQLodKcftslWZvB1Jdxn
+wQ5hYIizPtGo/KPaHbDRsSNU30R2be1B2MGyIrZTHN81Hdyhdyox5C315eXbyOD/5YDXC2Og/zOh
+D7osFRXql7PSorW+8oyWHhqPHWykYTe5hnMz15eWniN9gqRMgeKh0bpnX5UHoycR7hYQe7xFSkyy
+BNKr79X9DFHOUGoIMfmR2gyPZFwDwzqLID9ujWc9Otb+fVuIyV77zGHcizN300QyNQliBJIWENie
+J0f7OyHj+OsdWwIDAQABo4GwMIGtMA8GA1UdEwEB/wQFMAMBAf8wCwYDVR0PBAQDAgEGMB0GA1Ud
+DgQWBBQahGK8SEwzJQTU7tD2A8QZRtGUazBuBgNVHSMEZzBlgBQahGK8SEwzJQTU7tD2A8QZRtGU
+a6FJpEcwRTELMAkGA1UEBhMCQk0xGTAXBgNVBAoTEFF1b1ZhZGlzIExpbWl0ZWQxGzAZBgNVBAMT
+ElF1b1ZhZGlzIFJvb3QgQ0EgMoICBQkwDQYJKoZIhvcNAQEFBQADggIBAD4KFk2fBluornFdLwUv
+Z+YTRYPENvbzwCYMDbVHZF34tHLJRqUDGCdViXh9duqWNIAXINzng/iN/Ae42l9NLmeyhP3ZRPx3
+UIHmfLTJDQtyU/h2BwdBR5YM++CCJpNVjP4iH2BlfF/nJrP3MpCYUNQ3cVX2kiF495V5+vgtJodm
+VjB3pjd4M1IQWK4/YY7yarHvGH5KWWPKjaJW1acvvFYfzznB4vsKqBUsfU16Y8Zsl0Q80m/DShcK
++JDSV6IZUaUtl0HaB0+pUNqQjZRG4T7wlP0QADj1O+hA4bRuVhogzG9Yje0uRY/W6ZM/57Es3zrW
+IozchLsib9D45MY56QSIPMO661V6bYCZJPVsAfv4l7CUW+v90m/xd2gNNWQjrLhVoQPRTUIZ3Ph1
+WVaj+ahJefivDrkRoHy3au000LYmYjgahwz46P0u05B/B5EqHdZ+XIWDmbA4CD/pXvk1B+TJYm5X
+f6dQlfe6yJvmjqIBxdZmv3lh8zwc4bmCXF2gw+nYSL0ZohEUGW6yhhtoPkg3Goi3XZZenMfvJ2II
+4pEZXNLxId26F0KCl3GBUzGpn/Z9Yr9y4aOTHcyKJloJONDO1w2AFrR4pTqHTI2KpdVGl/IsELm8
+VCLAAVBpQ570su9t+Oza8eOx79+Rj1QqCyXBJhnEUhAFZdWCEOrCMc0u
+-----END CERTIFICATE-----
+
+QuoVadis Root CA 3
+==================
+-----BEGIN CERTIFICATE-----
+MIIGnTCCBIWgAwIBAgICBcYwDQYJKoZIhvcNAQEFBQAwRTELMAkGA1UEBhMCQk0xGTAXBgNVBAoT
+EFF1b1ZhZGlzIExpbWl0ZWQxGzAZBgNVBAMTElF1b1ZhZGlzIFJvb3QgQ0EgMzAeFw0wNjExMjQx
+OTExMjNaFw0zMTExMjQxOTA2NDRaMEUxCzAJBgNVBAYTAkJNMRkwFwYDVQQKExBRdW9WYWRpcyBM
+aW1pdGVkMRswGQYDVQQDExJRdW9WYWRpcyBSb290IENBIDMwggIiMA0GCSqGSIb3DQEBAQUAA4IC
+DwAwggIKAoICAQDMV0IWVJzmmNPTTe7+7cefQzlKZbPoFog02w1ZkXTPkrgEQK0CSzGrvI2RaNgg
+DhoB4hp7Thdd4oq3P5kazethq8Jlph+3t723j/z9cI8LoGe+AaJZz3HmDyl2/7FWeUUrH556VOij
+KTVopAFPD6QuN+8bv+OPEKhyq1hX51SGyMnzW9os2l2ObjyjPtr7guXd8lyyBTNvijbO0BNO/79K
+DDRMpsMhvVAEVeuxu537RR5kFd5VAYwCdrXLoT9CabwvvWhDFlaJKjdhkf2mrk7AyxRllDdLkgbv
+BNDInIjbC3uBr7E9KsRlOni27tyAsdLTmZw67mtaa7ONt9XOnMK+pUsvFrGeaDsGb659n/je7Mwp
+p5ijJUMv7/FfJuGITfhebtfZFG4ZM2mnO4SJk8RTVROhUXhA+LjJou57ulJCg54U7QVSWllWp5f8
+nT8KKdjcT5EOE7zelaTfi5m+rJsziO+1ga8bxiJTyPbH7pcUsMV8eFLI8M5ud2CEpukqdiDtWAEX
+MJPpGovgc2PZapKUSU60rUqFxKMiMPwJ7Wgic6aIDFUhWMXhOp8q3crhkODZc6tsgLjoC2SToJyM
+Gf+z0gzskSaHirOi4XCPLArlzW1oUevaPwV/izLmE1xr/l9A4iLItLRkT9a6fUg+qGkM17uGcclz
+uD87nSVL2v9A6wIDAQABo4IBlTCCAZEwDwYDVR0TAQH/BAUwAwEB/zCB4QYDVR0gBIHZMIHWMIHT
+BgkrBgEEAb5YAAMwgcUwgZMGCCsGAQUFBwICMIGGGoGDQW55IHVzZSBvZiB0aGlzIENlcnRpZmlj
+YXRlIGNvbnN0aXR1dGVzIGFjY2VwdGFuY2Ugb2YgdGhlIFF1b1ZhZGlzIFJvb3QgQ0EgMyBDZXJ0
+aWZpY2F0ZSBQb2xpY3kgLyBDZXJ0aWZpY2F0aW9uIFByYWN0aWNlIFN0YXRlbWVudC4wLQYIKwYB
+BQUHAgEWIWh0dHA6Ly93d3cucXVvdmFkaXNnbG9iYWwuY29tL2NwczALBgNVHQ8EBAMCAQYwHQYD
+VR0OBBYEFPLAE+CCQz777i9nMpY1XNu4ywLQMG4GA1UdIwRnMGWAFPLAE+CCQz777i9nMpY1XNu4
+ywLQoUmkRzBFMQswCQYDVQQGEwJCTTEZMBcGA1UEChMQUXVvVmFkaXMgTGltaXRlZDEbMBkGA1UE
+AxMSUXVvVmFkaXMgUm9vdCBDQSAzggIFxjANBgkqhkiG9w0BAQUFAAOCAgEAT62gLEz6wPJv92ZV
+qyM07ucp2sNbtrCD2dDQ4iH782CnO11gUyeim/YIIirnv6By5ZwkajGxkHon24QRiSemd1o417+s
+hvzuXYO8BsbRd2sPbSQvS3pspweWyuOEn62Iix2rFo1bZhfZFvSLgNLd+LJ2w/w4E6oM3kJpK27z
+POuAJ9v1pkQNn1pVWQvVDVJIxa6f8i+AxeoyUDUSly7B4f/xI4hROJ/yZlZ25w9Rl6VSDE1JUZU2
+Pb+iSwwQHYaZTKrzchGT5Or2m9qoXadNt54CrnMAyNojA+j56hl0YgCUyyIgvpSnWbWCar6ZeXqp
+8kokUvd0/bpO5qgdAm6xDYBEwa7TIzdfu4V8K5Iu6H6li92Z4b8nby1dqnuH/grdS/yO9SbkbnBC
+bjPsMZ57k8HkyWkaPcBrTiJt7qtYTcbQQcEr6k8Sh17rRdhs9ZgC06DYVYoGmRmioHfRMJ6szHXu
+g/WwYjnPbFfiTNKRCw51KBuav/0aQ/HKd/s7j2G4aSgWQgRecCocIdiP4b0jWy10QJLZYxkNc91p
+vGJHvOB0K7Lrfb5BG7XARsWhIstfTsEokt4YutUqKLsRixeTmJlglFwjz1onl14LBQaTNx47aTbr
+qZ5hHY8y2o4M1nQ+ewkk2gF3R8Q7zTSMmfXK4SVhM7JZG+Ju1zdXtg2pEto=
+-----END CERTIFICATE-----
+
+Security Communication Root CA
+==============================
+-----BEGIN CERTIFICATE-----
+MIIDWjCCAkKgAwIBAgIBADANBgkqhkiG9w0BAQUFADBQMQswCQYDVQQGEwJKUDEYMBYGA1UEChMP
+U0VDT00gVHJ1c3QubmV0MScwJQYDVQQLEx5TZWN1cml0eSBDb21tdW5pY2F0aW9uIFJvb3RDQTEw
+HhcNMDMwOTMwMDQyMDQ5WhcNMjMwOTMwMDQyMDQ5WjBQMQswCQYDVQQGEwJKUDEYMBYGA1UEChMP
+U0VDT00gVHJ1c3QubmV0MScwJQYDVQQLEx5TZWN1cml0eSBDb21tdW5pY2F0aW9uIFJvb3RDQTEw
+ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCzs/5/022x7xZ8V6UMbXaKL0u/ZPtM7orw
+8yl89f/uKuDp6bpbZCKamm8sOiZpUQWZJtzVHGpxxpp9Hp3dfGzGjGdnSj74cbAZJ6kJDKaVv0uM
+DPpVmDvY6CKhS3E4eayXkmmziX7qIWgGmBSWh9JhNrxtJ1aeV+7AwFb9Ms+k2Y7CI9eNqPPYJayX
+5HA49LY6tJ07lyZDo6G8SVlyTCMwhwFY9k6+HGhWZq/NQV3Is00qVUarH9oe4kA92819uZKAnDfd
+DJZkndwi92SL32HeFZRSFaB9UslLqCHJxrHty8OVYNEP8Ktw+N/LTX7s1vqr2b1/VPKl6Xn62dZ2
+JChzAgMBAAGjPzA9MB0GA1UdDgQWBBSgc0mZaNyFW2XjmygvV5+9M7wHSDALBgNVHQ8EBAMCAQYw
+DwYDVR0TAQH/BAUwAwEB/zANBgkqhkiG9w0BAQUFAAOCAQEAaECpqLvkT115swW1F7NgE+vGkl3g
+0dNq/vu+m22/xwVtWSDEHPC32oRYAmP6SBbvT6UL90qY8j+eG61Ha2POCEfrUj94nK9NrvjVT8+a
+mCoQQTlSxN3Zmw7vkwGusi7KaEIkQmywszo+zenaSMQVy+n5Bw+SUEmK3TGXX8npN6o7WWWXlDLJ
+s58+OmJYxUmtYg5xpTKqL8aJdkNAExNnPaJUJRDL8Try2frbSVa7pv6nQTXD4IhhyYjH3zYQIphZ
+6rBK+1YWc26sTfcioU+tHXotRSflMMFe8toTyyVCUZVHA4xsIcx0Qu1T/zOLjw9XARYvz6buyXAi
+FL39vmwLAw==
+-----END CERTIFICATE-----
+
+Sonera Class 1 Root CA
+======================
+-----BEGIN CERTIFICATE-----
+MIIDIDCCAgigAwIBAgIBJDANBgkqhkiG9w0BAQUFADA5MQswCQYDVQQGEwJGSTEPMA0GA1UEChMG
+U29uZXJhMRkwFwYDVQQDExBTb25lcmEgQ2xhc3MxIENBMB4XDTAxMDQwNjEwNDkxM1oXDTIxMDQw
+NjEwNDkxM1owOTELMAkGA1UEBhMCRkkxDzANBgNVBAoTBlNvbmVyYTEZMBcGA1UEAxMQU29uZXJh
+IENsYXNzMSBDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALWJHytPZwp5/8Ue+H88
+7dF+2rDNbS82rDTG29lkFwhjMDMiikzujrsPDUJVyZ0upe/3p4zDq7mXy47vPxVnqIJyY1MPQYx9
+EJUkoVqlBvqSV536pQHydekfvFYmUk54GWVYVQNYwBSujHxVX3BbdyMGNpfzJLWaRpXk3w0LBUXl
+0fIdgrvGE+D+qnr9aTCU89JFhfzyMlsy3uhsXR/LpCJ0sICOXZT3BgBLqdReLjVQCfOAl/QMF645
+2F/NM8EcyonCIvdFEu1eEpOdY6uCLrnrQkFEy0oaAIINnvmLVz5MxxftLItyM19yejhW1ebZrgUa
+HXVFsculJRwSVzb9IjcCAwEAAaMzMDEwDwYDVR0TAQH/BAUwAwEB/zARBgNVHQ4ECgQIR+IMi/ZT
+iFIwCwYDVR0PBAQDAgEGMA0GCSqGSIb3DQEBBQUAA4IBAQCLGrLJXWG04bkruVPRsoWdd44W7hE9
+28Jj2VuXZfsSZ9gqXLar5V7DtxYvyOirHYr9qxp81V9jz9yw3Xe5qObSIjiHBxTZ/75Wtf0HDjxV
+yhbMp6Z3N/vbXB9OWQaHowND9Rart4S9Tu+fMTfwRvFAttEMpWT4Y14h21VOTzF2nBBhjrZTOqMR
+vq9tfB69ri3iDGnHhVNoomG6xT60eVR4ngrHAr5i0RGCS2UvkVrCqIexVmiUefkl98HVrhq4uz2P
+qYo4Ffdz0Fpg0YCw8NzVUM1O7pJIae2yIx4wzMiUyLb1O4Z/P6Yun/Y+LLWSlj7fLJOK/4GMDw9Z
+IRlXvVWa
+-----END CERTIFICATE-----
+
+Sonera Class 2 Root CA
+======================
+-----BEGIN CERTIFICATE-----
+MIIDIDCCAgigAwIBAgIBHTANBgkqhkiG9w0BAQUFADA5MQswCQYDVQQGEwJGSTEPMA0GA1UEChMG
+U29uZXJhMRkwFwYDVQQDExBTb25lcmEgQ2xhc3MyIENBMB4XDTAxMDQwNjA3Mjk0MFoXDTIxMDQw
+NjA3Mjk0MFowOTELMAkGA1UEBhMCRkkxDzANBgNVBAoTBlNvbmVyYTEZMBcGA1UEAxMQU29uZXJh
+IENsYXNzMiBDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAJAXSjWdyvANlsdE+hY3
+/Ei9vX+ALTU74W+oZ6m/AxxNjG8yR9VBaKQTBME1DJqEQ/xcHf+Js+gXGM2RX/uJ4+q/Tl18GybT
+dXnt5oTjV+WtKcT0OijnpXuENmmz/V52vaMtmdOQTiMofRhj8VQ7Jp12W5dCsv+u8E7s3TmVToMG
+f+dJQMjFAbJUWmYdPfz56TwKnoG4cPABi+QjVHzIrviQHgCWctRUz2EjvOr7nQKV0ba5cTppCD8P
+tOFCx4j1P5iop7oc4HFx71hXgVB6XGt0Rg6DA5jDjqhu8nYybieDwnPz3BjotJPqdURrBGAgcVeH
+nfO+oJAjPYok4doh28MCAwEAAaMzMDEwDwYDVR0TAQH/BAUwAwEB/zARBgNVHQ4ECgQISqCqWITT
+XjwwCwYDVR0PBAQDAgEGMA0GCSqGSIb3DQEBBQUAA4IBAQBazof5FnIVV0sd2ZvnoiYw7JNn39Yt
+0jSv9zilzqsWuasvfDXLrNAPtEwr/IDva4yRXzZ299uzGxnq9LIR/WFxRL8oszodv7ND6J+/3DEI
+cbCdjdY0RzKQxmUk96BKfARzjzlvF4xytb1LyHr4e4PDKE6cCepnP7JnBBvDFNr450kkkdAdavph
+Oe9r5yF1BgfYErQhIHBCcYHaPJo2vqZbDWpsmh+Re/n570K6Tk6ezAyNlNzZRZxe7EJQY670XcSx
+EtzKO6gunRRaBXW37Ndj4ro1tgQIkejanZz2ZrUYrAqmVCY0M9IbwdR/GjqOC6oybtv8TyWf2TLH
+llpwrN9M
+-----END CERTIFICATE-----
+
+Staat der Nederlanden Root CA
+=============================
+-----BEGIN CERTIFICATE-----
+MIIDujCCAqKgAwIBAgIEAJiWijANBgkqhkiG9w0BAQUFADBVMQswCQYDVQQGEwJOTDEeMBwGA1UE
+ChMVU3RhYXQgZGVyIE5lZGVybGFuZGVuMSYwJAYDVQQDEx1TdGFhdCBkZXIgTmVkZXJsYW5kZW4g
+Um9vdCBDQTAeFw0wMjEyMTcwOTIzNDlaFw0xNTEyMTYwOTE1MzhaMFUxCzAJBgNVBAYTAk5MMR4w
+HAYDVQQKExVTdGFhdCBkZXIgTmVkZXJsYW5kZW4xJjAkBgNVBAMTHVN0YWF0IGRlciBOZWRlcmxh
+bmRlbiBSb290IENBMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAmNK1URF6gaYUmHFt
+vsznExvWJw56s2oYHLZhWtVhCb/ekBPHZ+7d89rFDBKeNVU+LCeIQGv33N0iYfXCxw719tV2U02P
+jLwYdjeFnejKScfST5gTCaI+Ioicf9byEGW07l8Y1Rfj+MX94p2i71MOhXeiD+EwR+4A5zN9RGca
+C1Hoi6CeUJhoNFIfLm0B8mBF8jHrqTFoKbt6QZ7GGX+UtFE5A3+y3qcym7RHjm+0Sq7lr7HcsBth
+vJly3uSJt3omXdozSVtSnA71iq3DuD3oBmrC1SoLbHuEvVYFy4ZlkuxEK7COudxwC0barbxjiDn6
+22r+I/q85Ej0ZytqERAhSQIDAQABo4GRMIGOMAwGA1UdEwQFMAMBAf8wTwYDVR0gBEgwRjBEBgRV
+HSAAMDwwOgYIKwYBBQUHAgEWLmh0dHA6Ly93d3cucGtpb3ZlcmhlaWQubmwvcG9saWNpZXMvcm9v
+dC1wb2xpY3kwDgYDVR0PAQH/BAQDAgEGMB0GA1UdDgQWBBSofeu8Y6R0E3QA7Jbg0zTBLL9s+DAN
+BgkqhkiG9w0BAQUFAAOCAQEABYSHVXQ2YcG70dTGFagTtJ+k/rvuFbQvBgwp8qiSpGEN/KtcCFtR
+EytNwiphyPgJWPwtArI5fZlmgb9uXJVFIGzmeafR2Bwp/MIgJ1HI8XxdNGdphREwxgDS1/PTfLbw
+MVcoEoJz6TMvplW0C5GUR5z6u3pCMuiufi3IvKwUv9kP2Vv8wfl6leF9fpb8cbDCTMjfRTTJzg3y
+nGQI0DvDKcWy7ZAEwbEpkcUwb8GpcjPM/l0WFywRaed+/sWDCN+83CI6LiBpIzlWYGeQiy52OfsR
+iJf2fL1LuCAWZwWN4jvBcj+UlTfHXbme2JOhF4//DGYVwSR8MnwDHTuhWEUykw==
+-----END CERTIFICATE-----
+
+TDC Internet Root CA
+====================
+-----BEGIN CERTIFICATE-----
+MIIEKzCCAxOgAwIBAgIEOsylTDANBgkqhkiG9w0BAQUFADBDMQswCQYDVQQGEwJESzEVMBMGA1UE
+ChMMVERDIEludGVybmV0MR0wGwYDVQQLExRUREMgSW50ZXJuZXQgUm9vdCBDQTAeFw0wMTA0MDUx
+NjMzMTdaFw0yMTA0MDUxNzAzMTdaMEMxCzAJBgNVBAYTAkRLMRUwEwYDVQQKEwxUREMgSW50ZXJu
+ZXQxHTAbBgNVBAsTFFREQyBJbnRlcm5ldCBSb290IENBMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8A
+MIIBCgKCAQEAxLhAvJHVYx/XmaCLDEAedLdInUaMArLgJF/wGROnN4NrXceO+YQwzho7+vvOi20j
+xsNuZp+Jpd/gQlBn+h9sHvTQBda/ytZO5GhgbEaqHF1j4QeGDmUApy6mcca8uYGoOn0a0vnRrEvL
+znWv3Hv6gXPU/Lq9QYjUdLP5Xjg6PEOo0pVOd20TDJ2PeAG3WiAfAzc14izbSysseLlJ28TQx5yc
+5IogCSEWVmb/Bexb4/DPqyQkXsN/cHoSxNK1EKC2IeGNeGlVRGn1ypYcNIUXJXfi9i8nmHj9eQY6
+otZaQ8H/7AQ77hPv01ha/5Lr7K7a8jcDR0G2l8ktCkEiu7vmpwIDAQABo4IBJTCCASEwEQYJYIZI
+AYb4QgEBBAQDAgAHMGUGA1UdHwReMFwwWqBYoFakVDBSMQswCQYDVQQGEwJESzEVMBMGA1UEChMM
+VERDIEludGVybmV0MR0wGwYDVQQLExRUREMgSW50ZXJuZXQgUm9vdCBDQTENMAsGA1UEAxMEQ1JM
+MTArBgNVHRAEJDAigA8yMDAxMDQwNTE2MzMxN1qBDzIwMjEwNDA1MTcwMzE3WjALBgNVHQ8EBAMC
+AQYwHwYDVR0jBBgwFoAUbGQBx/2FbazI2p5QCIUItTxWqFAwHQYDVR0OBBYEFGxkAcf9hW2syNqe
+UAiFCLU8VqhQMAwGA1UdEwQFMAMBAf8wHQYJKoZIhvZ9B0EABBAwDhsIVjUuMDo0LjADAgSQMA0G
+CSqGSIb3DQEBBQUAA4IBAQBOQ8zR3R0QGwZ/t6T609lN+yOfI1Rb5osvBCiLtSdtiaHsmGnc540m
+gwV5dOy0uaOXwTUA/RXaOYE6lTGQ3pfphqiZdwzlWqCE/xIWrG64jcN7ksKsLtB9KOy282A4aW8+
+2ARVPp7MVdK6/rtHBNcK2RYKNCn1WBPVT8+PVkuzHu7TmHnaCB4Mb7j4Fifvwm899qNLPg7kbWzb
+O0ESm70NRyN/PErQr8Cv9u8btRXE64PECV90i9kR+8JWsTz4cMo0jUNAE4z9mQNUecYu6oah9jrU
+Cbz0vGbMPVjQV0kK7iXiQe4T+Zs4NNEA9X7nlB38aQNiuJkFBT1reBK9sG9l
+-----END CERTIFICATE-----
+
+TDC OCES Root CA
+================
+-----BEGIN CERTIFICATE-----
+MIIFGTCCBAGgAwIBAgIEPki9xDANBgkqhkiG9w0BAQUFADAxMQswCQYDVQQGEwJESzEMMAoGA1UE
+ChMDVERDMRQwEgYDVQQDEwtUREMgT0NFUyBDQTAeFw0wMzAyMTEwODM5MzBaFw0zNzAyMTEwOTA5
+MzBaMDExCzAJBgNVBAYTAkRLMQwwCgYDVQQKEwNUREMxFDASBgNVBAMTC1REQyBPQ0VTIENBMIIB
+IjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEArGL2YSCyz8DGhdfjeebM7fI5kqSXLmSjhFuH
+nEz9pPPEXyG9VhDr2y5h7JNp46PMvZnDBfwGuMo2HP6QjklMxFaaL1a8z3sM8W9Hpg1DTeLpHTk0
+zY0s2RKY+ePhwUp8hjjEqcRhiNJerxomTdXkoCJHhNlktxmW/OwZ5LKXJk5KTMuPJItUGBxIYXvV
+iGjaXbXqzRowwYCDdlCqT9HU3Tjw7xb04QxQBr/q+3pJoSgrHPb8FTKjdGqPqcNiKXEx5TukYBde
+dObaE+3pHx8b0bJoc8YQNHVGEBDjkAB2QMuLt0MJIf+rTpPGWOmlgtt3xDqZsXKVSQTwtyv6e1mO
+3QIDAQABo4ICNzCCAjMwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwgewGA1UdIASB
+5DCB4TCB3gYIKoFQgSkBAQEwgdEwLwYIKwYBBQUHAgEWI2h0dHA6Ly93d3cuY2VydGlmaWthdC5k
+ay9yZXBvc2l0b3J5MIGdBggrBgEFBQcCAjCBkDAKFgNUREMwAwIBARqBgUNlcnRpZmlrYXRlciBm
+cmEgZGVubmUgQ0EgdWRzdGVkZXMgdW5kZXIgT0lEIDEuMi4yMDguMTY5LjEuMS4xLiBDZXJ0aWZp
+Y2F0ZXMgZnJvbSB0aGlzIENBIGFyZSBpc3N1ZWQgdW5kZXIgT0lEIDEuMi4yMDguMTY5LjEuMS4x
+LjARBglghkgBhvhCAQEEBAMCAAcwgYEGA1UdHwR6MHgwSKBGoESkQjBAMQswCQYDVQQGEwJESzEM
+MAoGA1UEChMDVERDMRQwEgYDVQQDEwtUREMgT0NFUyBDQTENMAsGA1UEAxMEQ1JMMTAsoCqgKIYm
+aHR0cDovL2NybC5vY2VzLmNlcnRpZmlrYXQuZGsvb2Nlcy5jcmwwKwYDVR0QBCQwIoAPMjAwMzAy
+MTEwODM5MzBagQ8yMDM3MDIxMTA5MDkzMFowHwYDVR0jBBgwFoAUYLWF7FZkfhIZJ2cdUBVLc647
++RIwHQYDVR0OBBYEFGC1hexWZH4SGSdnHVAVS3OuO/kSMB0GCSqGSIb2fQdBAAQQMA4bCFY2LjA6
+NC4wAwIEkDANBgkqhkiG9w0BAQUFAAOCAQEACromJkbTc6gJ82sLMJn9iuFXehHTuJTXCRBuo7E4
+A9G28kNBKWKnctj7fAXmMXAnVBhOinxO5dHKjHiIzxvTkIvmI/gLDjNDfZziChmPyQE+dF10yYsc
+A+UYyAFMP8uXBV2YcaaYb7Z8vTd/vuGTJW1v8AqtFxjhA7wHKcitJuj4YfD9IQl+mo6paH1IYnK9
+AOoBmbgGglGBTvH1tJFUuSN6AJqfXY3gPGS5GhKSKseCRHI53OI8xthV9RVOyAUO28bQYqbsFbS1
+AoLbrIyigfCbmTH1ICCoiGEKB5+U/NDXG8wuF/MEJ3Zn61SD/aSQfgY9BKNDLdr8C2LqL19iUw==
+-----END CERTIFICATE-----
+
+UTN DATACorp SGC Root CA
+========================
+-----BEGIN CERTIFICATE-----
+MIIEXjCCA0agAwIBAgIQRL4Mi1AAIbQR0ypoBqmtaTANBgkqhkiG9w0BAQUFADCBkzELMAkGA1UE
+BhMCVVMxCzAJBgNVBAgTAlVUMRcwFQYDVQQHEw5TYWx0IExha2UgQ2l0eTEeMBwGA1UEChMVVGhl
+IFVTRVJUUlVTVCBOZXR3b3JrMSEwHwYDVQQLExhodHRwOi8vd3d3LnVzZXJ0cnVzdC5jb20xGzAZ
+BgNVBAMTElVUTiAtIERBVEFDb3JwIFNHQzAeFw05OTA2MjQxODU3MjFaFw0xOTA2MjQxOTA2MzBa
+MIGTMQswCQYDVQQGEwJVUzELMAkGA1UECBMCVVQxFzAVBgNVBAcTDlNhbHQgTGFrZSBDaXR5MR4w
+HAYDVQQKExVUaGUgVVNFUlRSVVNUIE5ldHdvcmsxITAfBgNVBAsTGGh0dHA6Ly93d3cudXNlcnRy
+dXN0LmNvbTEbMBkGA1UEAxMSVVROIC0gREFUQUNvcnAgU0dDMIIBIjANBgkqhkiG9w0BAQEFAAOC
+AQ8AMIIBCgKCAQEA3+5YEKIrblXEjr8uRgnn4AgPLit6E5Qbvfa2gI5lBZMAHryv4g+OGQ0SR+ys
+raP6LnD43m77VkIVni5c7yPeIbkFdicZD0/Ww5y0vpQZY/KmEQrrU0icvvIpOxboGqBMpsn0GFlo
+wHDyUwDAXlCCpVZvNvlK4ESGoE1O1kduSUrLZ9emxAW5jh70/P/N5zbgnAVssjMiFdC04MwXwLLA
+9P4yPykqlXvY8qdOD1R8oQ2AswkDwf9c3V6aPryuvEeKaq5xyh+xKrhfQgUL7EYw0XILyulWbfXv
+33i+Ybqypa4ETLyorGkVl73v67SMvzX41MPRKA5cOp9wGDMgd8SirwIDAQABo4GrMIGoMAsGA1Ud
+DwQEAwIBxjAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBRTMtGzz3/64PGgXYVOktKeRR20TzA9
+BgNVHR8ENjA0MDKgMKAuhixodHRwOi8vY3JsLnVzZXJ0cnVzdC5jb20vVVROLURBVEFDb3JwU0dD
+LmNybDAqBgNVHSUEIzAhBggrBgEFBQcDAQYKKwYBBAGCNwoDAwYJYIZIAYb4QgQBMA0GCSqGSIb3
+DQEBBQUAA4IBAQAnNZcAiosovcYzMB4p/OL31ZjUQLtgyr+rFywJNn9Q+kHcrpY6CiM+iVnJowft
+Gzet/Hy+UUla3joKVAgWRcKZsYfNjGjgaQPpxE6YsjuMFrMOoAyYUJuTqXAJyCyjj98C5OBxOvG0
+I3KgqgHf35g+FFCgMSa9KOlaMCZ1+XtgHI3zzVAmbQQnmt/VDUVHKWss5nbZqSl9Mt3JNjy9rjXx
+EZ4du5A/EkdOjtd+D2JzHVImOBwYSf0wdJrE5SIv2MCN7ZF6TACPcn9d2t0bi0Vr591pl6jFVkwP
+DPafepE39peC4N1xaf92P2BNPM/3mfnGV/TJVTl4uix5yaaIK/QI
+-----END CERTIFICATE-----
+
+UTN USERFirst Email Root CA
+===========================
+-----BEGIN CERTIFICATE-----
+MIIEojCCA4qgAwIBAgIQRL4Mi1AAJLQR0zYlJWfJiTANBgkqhkiG9w0BAQUFADCBrjELMAkGA1UE
+BhMCVVMxCzAJBgNVBAgTAlVUMRcwFQYDVQQHEw5TYWx0IExha2UgQ2l0eTEeMBwGA1UEChMVVGhl
+IFVTRVJUUlVTVCBOZXR3b3JrMSEwHwYDVQQLExhodHRwOi8vd3d3LnVzZXJ0cnVzdC5jb20xNjA0
+BgNVBAMTLVVUTi1VU0VSRmlyc3QtQ2xpZW50IEF1dGhlbnRpY2F0aW9uIGFuZCBFbWFpbDAeFw05
+OTA3MDkxNzI4NTBaFw0xOTA3MDkxNzM2NThaMIGuMQswCQYDVQQGEwJVUzELMAkGA1UECBMCVVQx
+FzAVBgNVBAcTDlNhbHQgTGFrZSBDaXR5MR4wHAYDVQQKExVUaGUgVVNFUlRSVVNUIE5ldHdvcmsx
+ITAfBgNVBAsTGGh0dHA6Ly93d3cudXNlcnRydXN0LmNvbTE2MDQGA1UEAxMtVVROLVVTRVJGaXJz
+dC1DbGllbnQgQXV0aGVudGljYXRpb24gYW5kIEVtYWlsMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8A
+MIIBCgKCAQEAsjmFpPJ9q0E7YkY3rs3BYHW8OWX5ShpHornMSMxqmNVNNRm5pELlzkniii8efNIx
+B8dOtINknS4p1aJkxIW9hVE1eaROaJB7HHqkkqgX8pgV8pPMyaQylbsMTzC9mKALi+VuG6JG+ni8
+om+rWV6lL8/K2m2qL+usobNqqrcuZzWLeeEeaYji5kbNoKXqvgvOdjp6Dpvq/NonWz1zHyLmSGHG
+TPNpsaguG7bUMSAsvIKKjqQOpdeJQ/wWWq8dcdcRWdq6hw2v+vPhwvCkxWeM1tZUOt4KpLoDd7Nl
+yP0e03RiqhjKaJMeoYV+9Udly/hNVyh00jT/MLbu9mIwFIws6wIDAQABo4G5MIG2MAsGA1UdDwQE
+AwIBxjAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBSJgmd9xJ0mcABLtFBIfN49rgRufTBYBgNV
+HR8EUTBPME2gS6BJhkdodHRwOi8vY3JsLnVzZXJ0cnVzdC5jb20vVVROLVVTRVJGaXJzdC1DbGll
+bnRBdXRoZW50aWNhdGlvbmFuZEVtYWlsLmNybDAdBgNVHSUEFjAUBggrBgEFBQcDAgYIKwYBBQUH
+AwQwDQYJKoZIhvcNAQEFBQADggEBALFtYV2mGn98q0rkMPxTbyUkxsrt4jFcKw7u7mFVbwQ+zzne
+xRtJlOTrIEy05p5QLnLZjfWqo7NK2lYcYJeA3IKirUq9iiv/Cwm0xtcgBEXkzYABurorbs6q15L+
+5K/r9CYdFip/bDCVNy8zEqx/3cfREYxRmLLQo5HQrfafnoOTHh1CuEava2bwm3/q4wMC5QJRwarV
+NZ1yQAOJujEdxRBoUp7fooXFXAimeOZTT7Hot9MUnpOmw2TjrH5xzbyf6QMbzPvprDHBr3wVdAKZ
+w7JHpsIyYdfHb0gkUSeh1YdV8nuPmD0Wnu51tvjQjvLzxq4oW6fw8zYX/MMF08oDSlQ=
+-----END CERTIFICATE-----
+
+UTN USERFirst Hardware Root CA
+==============================
+-----BEGIN CERTIFICATE-----
+MIIEdDCCA1ygAwIBAgIQRL4Mi1AAJLQR0zYq/mUK/TANBgkqhkiG9w0BAQUFADCBlzELMAkGA1UE
+BhMCVVMxCzAJBgNVBAgTAlVUMRcwFQYDVQQHEw5TYWx0IExha2UgQ2l0eTEeMBwGA1UEChMVVGhl
+IFVTRVJUUlVTVCBOZXR3b3JrMSEwHwYDVQQLExhodHRwOi8vd3d3LnVzZXJ0cnVzdC5jb20xHzAd
+BgNVBAMTFlVUTi1VU0VSRmlyc3QtSGFyZHdhcmUwHhcNOTkwNzA5MTgxMDQyWhcNMTkwNzA5MTgx
+OTIyWjCBlzELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAlVUMRcwFQYDVQQHEw5TYWx0IExha2UgQ2l0
+eTEeMBwGA1UEChMVVGhlIFVTRVJUUlVTVCBOZXR3b3JrMSEwHwYDVQQLExhodHRwOi8vd3d3LnVz
+ZXJ0cnVzdC5jb20xHzAdBgNVBAMTFlVUTi1VU0VSRmlyc3QtSGFyZHdhcmUwggEiMA0GCSqGSIb3
+DQEBAQUAA4IBDwAwggEKAoIBAQCx98M4P7Sof885glFn0G2f0v9Y8+efK+wNiVSZuTiZFvfgIXlI
+wrthdBKWHTxqctU8EGc6Oe0rE81m65UJM6Rsl7HoxuzBdXmcRl6Nq9Bq/bkqVRcQVLMZ8Jr28bFd
+tqdt++BxF2uiiPsA3/4aMXcMmgF6sTLjKwEHOG7DpV4jvEWbe1DByTCP2+UretNb+zNAHqDVmBe8
+i4fDidNdoI6yqqr2jmmIBsX6iSHzCJ1pLgkzmykNRg+MzEk0sGlRvfkGzWitZky8PqxhvQqIDsjf
+Pe58BEydCl5rkdbux+0ojatNh4lz0G6k0B4WixThdkQDf2Os5M1JnMWS9KsyoUhbAgMBAAGjgbkw
+gbYwCwYDVR0PBAQDAgHGMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFKFyXyYbKJhDlV0HN9WF
+lp1L0sNFMEQGA1UdHwQ9MDswOaA3oDWGM2h0dHA6Ly9jcmwudXNlcnRydXN0LmNvbS9VVE4tVVNF
+UkZpcnN0LUhhcmR3YXJlLmNybDAxBgNVHSUEKjAoBggrBgEFBQcDAQYIKwYBBQUHAwUGCCsGAQUF
+BwMGBggrBgEFBQcDBzANBgkqhkiG9w0BAQUFAAOCAQEARxkP3nTGmZev/K0oXnWO6y1n7k57K9cM
+//bey1WiCuFMVGWTYGufEpytXoMs61quwOQt9ABjHbjAbPLPSbtNk28GpgoiskliCE7/yMgUsogW
+XecB5BKV5UU0s4tpvc+0hY91UZ59Ojg6FEgSxvunOxqNDYJAB+gECJChicsZUN/KHAG8HQQZexB2
+lzvukJDKxA4fFm517zP4029bHpbj4HR3dHuKom4t3XbWOTCC8KucUvIqx69JXn7HaOWCgchqJ/kn
+iCrVWFCVH/A7HFe7fRQ5YiuayZSSKqMiDP+JJn1fIytH1xUdqWqeUQ0qUZ6B+dQ7XnASfxAynB67
+nfhmqA==
+-----END CERTIFICATE-----
+
+UTN USERFirst Object Root CA
+============================
+-----BEGIN CERTIFICATE-----
+MIIEZjCCA06gAwIBAgIQRL4Mi1AAJLQR0zYt4LNfGzANBgkqhkiG9w0BAQUFADCBlTELMAkGA1UE
+BhMCVVMxCzAJBgNVBAgTAlVUMRcwFQYDVQQHEw5TYWx0IExha2UgQ2l0eTEeMBwGA1UEChMVVGhl
+IFVTRVJUUlVTVCBOZXR3b3JrMSEwHwYDVQQLExhodHRwOi8vd3d3LnVzZXJ0cnVzdC5jb20xHTAb
+BgNVBAMTFFVUTi1VU0VSRmlyc3QtT2JqZWN0MB4XDTk5MDcwOTE4MzEyMFoXDTE5MDcwOTE4NDAz
+NlowgZUxCzAJBgNVBAYTAlVTMQswCQYDVQQIEwJVVDEXMBUGA1UEBxMOU2FsdCBMYWtlIENpdHkx
+HjAcBgNVBAoTFVRoZSBVU0VSVFJVU1QgTmV0d29yazEhMB8GA1UECxMYaHR0cDovL3d3dy51c2Vy
+dHJ1c3QuY29tMR0wGwYDVQQDExRVVE4tVVNFUkZpcnN0LU9iamVjdDCCASIwDQYJKoZIhvcNAQEB
+BQADggEPADCCAQoCggEBAM6qgT+jo2F4qjEAVZURnicPHxzfOpuCaDDASmEd8S8O+r5596Uj71VR
+loTN2+O5bj4x2AogZ8f02b+U60cEPgLOKqJdhwQJ9jCdGIqXsqoc/EHSoTbL+z2RuufZcDX65OeQ
+w5ujm9M89RKZd7G3CeBo5hy485RjiGpq/gt2yb70IuRnuasaXnfBhQfdDWy/7gbHd2pBnqcP1/vu
+lBe3/IW+pKvEHDHd17bR5PDv3xaPslKT16HUiaEHLr/hARJCHhrh2JU022R5KP+6LhHC5ehbkkj7
+RwvCbNqtMoNB86XlQXD9ZZBt+vpRxPm9lisZBCzTbafc8H9vg2XiaquHhnUCAwEAAaOBrzCBrDAL
+BgNVHQ8EBAMCAcYwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQU2u1kdBScFDyr3ZmpvVsoTYs8
+ydgwQgYDVR0fBDswOTA3oDWgM4YxaHR0cDovL2NybC51c2VydHJ1c3QuY29tL1VUTi1VU0VSRmly
+c3QtT2JqZWN0LmNybDApBgNVHSUEIjAgBggrBgEFBQcDAwYIKwYBBQUHAwgGCisGAQQBgjcKAwQw
+DQYJKoZIhvcNAQEFBQADggEBAAgfUrE3RHjb/c652pWWmKpVZIC1WkDdIaXFwfNfLEzIR1pp6ujw
+NTX00CXzyKakh0q9G7FzCL3Uw8q2NbtZhncxzaeAFK4T7/yxSPlrJSUtUbYsbUXBmMiKVl0+7kNO
+PmsnjtA6S4ULX9Ptaqd1y9Fahy85dRNacrACgZ++8A+EVCBibGnU4U3GDZlDAQ0Slox4nb9QorFE
+qmrPF3rPbw/U+CRVX/A0FklmPlBGyWNxODFiuGK581OtbLUrohKqGU8J2l7nk8aOFAj+8DCAGKCG
+hU3IfdeLA/5u1fedFqySLKAj5ZyRUh+U3xeUc8OzwcFxBSAAeL0TUh2oPs0AH8g=
+-----END CERTIFICATE-----
+
+Camerfirma Chambers of Commerce Root
+====================================
+-----BEGIN CERTIFICATE-----
+MIIEvTCCA6WgAwIBAgIBADANBgkqhkiG9w0BAQUFADB/MQswCQYDVQQGEwJFVTEnMCUGA1UEChMe
+QUMgQ2FtZXJmaXJtYSBTQSBDSUYgQTgyNzQzMjg3MSMwIQYDVQQLExpodHRwOi8vd3d3LmNoYW1i
+ZXJzaWduLm9yZzEiMCAGA1UEAxMZQ2hhbWJlcnMgb2YgQ29tbWVyY2UgUm9vdDAeFw0wMzA5MzAx
+NjEzNDNaFw0zNzA5MzAxNjEzNDRaMH8xCzAJBgNVBAYTAkVVMScwJQYDVQQKEx5BQyBDYW1lcmZp
+cm1hIFNBIENJRiBBODI3NDMyODcxIzAhBgNVBAsTGmh0dHA6Ly93d3cuY2hhbWJlcnNpZ24ub3Jn
+MSIwIAYDVQQDExlDaGFtYmVycyBvZiBDb21tZXJjZSBSb290MIIBIDANBgkqhkiG9w0BAQEFAAOC
+AQ0AMIIBCAKCAQEAtzZV5aVdGDDg2olUkfzIx1L4L1DZ77F1c2VHfRtbunXF/KGIJPov7coISjlU
+xFF6tdpg6jg8gbLL8bvZkSM/SAFwdakFKq0fcfPJVD0dBmpAPrMMhe5cG3nCYsS4No41XQEMIwRH
+NaqbYE6gZj3LJgqcQKH0XZi/caulAGgq7YN6D6IUtdQis4CwPAxaUWktWBiP7Zme8a7ileb2R6jW
+DA+wWFjbw2Y3npuRVDM30pQcakjJyfKl2qUMI/cjDpwyVV5xnIQFUZot/eZOKjRa3spAN2cMVCFV
+d9oKDMyXroDclDZK9D7ONhMeU+SsTjoF7Nuucpw4i9A5O4kKPnf+dQIBA6OCAUQwggFAMBIGA1Ud
+EwEB/wQIMAYBAf8CAQwwPAYDVR0fBDUwMzAxoC+gLYYraHR0cDovL2NybC5jaGFtYmVyc2lnbi5v
+cmcvY2hhbWJlcnNyb290LmNybDAdBgNVHQ4EFgQU45T1sU3p26EpW1eLTXYGduHRooowDgYDVR0P
+AQH/BAQDAgEGMBEGCWCGSAGG+EIBAQQEAwIABzAnBgNVHREEIDAegRxjaGFtYmVyc3Jvb3RAY2hh
+bWJlcnNpZ24ub3JnMCcGA1UdEgQgMB6BHGNoYW1iZXJzcm9vdEBjaGFtYmVyc2lnbi5vcmcwWAYD
+VR0gBFEwTzBNBgsrBgEEAYGHLgoDATA+MDwGCCsGAQUFBwIBFjBodHRwOi8vY3BzLmNoYW1iZXJz
+aWduLm9yZy9jcHMvY2hhbWJlcnNyb290Lmh0bWwwDQYJKoZIhvcNAQEFBQADggEBAAxBl8IahsAi
+fJ/7kPMa0QOx7xP5IV8EnNrJpY0nbJaHkb5BkAFyk+cefV/2icZdp0AJPaxJRUXcLo0waLIJuvvD
+L8y6C98/d3tGfToSJI6WjzwFCm/SlCgdbQzALogi1djPHRPH8EjX1wWnz8dHnjs8NMiAT9QUu/wN
+UPf6s+xCX6ndbcj0dc97wXImsQEcXCz9ek60AcUFV7nnPKoF2YjpB0ZBzu9Bga5Y34OirsrXdx/n
+ADydb47kMgkdTXg0eDQ8lJsm7U9xxhl6vSAiSFr+S30Dt+dYvsYyTnQeaN2oaFuzPu5ifdmA6Ap1
+erfutGWaIZDgqtCYvDi1czyL+Nw=
+-----END CERTIFICATE-----
+
+Camerfirma Global Chambersign Root
+==================================
+-----BEGIN CERTIFICATE-----
+MIIExTCCA62gAwIBAgIBADANBgkqhkiG9w0BAQUFADB9MQswCQYDVQQGEwJFVTEnMCUGA1UEChMe
+QUMgQ2FtZXJmaXJtYSBTQSBDSUYgQTgyNzQzMjg3MSMwIQYDVQQLExpodHRwOi8vd3d3LmNoYW1i
+ZXJzaWduLm9yZzEgMB4GA1UEAxMXR2xvYmFsIENoYW1iZXJzaWduIFJvb3QwHhcNMDMwOTMwMTYx
+NDE4WhcNMzcwOTMwMTYxNDE4WjB9MQswCQYDVQQGEwJFVTEnMCUGA1UEChMeQUMgQ2FtZXJmaXJt
+YSBTQSBDSUYgQTgyNzQzMjg3MSMwIQYDVQQLExpodHRwOi8vd3d3LmNoYW1iZXJzaWduLm9yZzEg
+MB4GA1UEAxMXR2xvYmFsIENoYW1iZXJzaWduIFJvb3QwggEgMA0GCSqGSIb3DQEBAQUAA4IBDQAw
+ggEIAoIBAQCicKLQn0KuWxfH2H3PFIP8T8mhtxOviteePgQKkotgVvq0Mi+ITaFgCPS3CU6gSS9J
+1tPfnZdan5QEcOw/Wdm3zGaLmFIoCQLfxS+EjXqXd7/sQJ0lcqu1PzKY+7e3/HKE5TWH+VX6ox8O
+by4o3Wmg2UIQxvi1RMLQQ3/bvOSiPGpVeAp3qdjqGTK3L/5cPxvusZjsyq16aUXjlg9V9ubtdepl
+6DJWk0aJqCWKZQbua795B9Dxt6/tLE2Su8CoX6dnfQTyFQhwrJLWfQTSM/tMtgsL+xrJxI0DqX5c
+8lCrEqWhz0hQpe/SyBoT+rB/sYIcd2oPX9wLlY/vQ37mRQklAgEDo4IBUDCCAUwwEgYDVR0TAQH/
+BAgwBgEB/wIBDDA/BgNVHR8EODA2MDSgMqAwhi5odHRwOi8vY3JsLmNoYW1iZXJzaWduLm9yZy9j
+aGFtYmVyc2lnbnJvb3QuY3JsMB0GA1UdDgQWBBRDnDafsJ4wTcbOX60Qq+UDpfqpFDAOBgNVHQ8B
+Af8EBAMCAQYwEQYJYIZIAYb4QgEBBAQDAgAHMCoGA1UdEQQjMCGBH2NoYW1iZXJzaWducm9vdEBj
+aGFtYmVyc2lnbi5vcmcwKgYDVR0SBCMwIYEfY2hhbWJlcnNpZ25yb290QGNoYW1iZXJzaWduLm9y
+ZzBbBgNVHSAEVDBSMFAGCysGAQQBgYcuCgEBMEEwPwYIKwYBBQUHAgEWM2h0dHA6Ly9jcHMuY2hh
+bWJlcnNpZ24ub3JnL2Nwcy9jaGFtYmVyc2lnbnJvb3QuaHRtbDANBgkqhkiG9w0BAQUFAAOCAQEA
+PDtwkfkEVCeR4e3t/mh/YV3lQWVPMvEYBZRqHN4fcNs+ezICNLUMbKGKfKX0j//U2K0X1S0E0T9Y
+gOKBWYi+wONGkyT+kL0mojAt6JcmVzWJdJYY9hXiryQZVgICsroPFOrGimbBhkVVi76SvpykBMdJ
+PJ7oKXqJ1/6v/2j1pReQvayZzKWGVwlnRtvWFsJG8eSpUPWP0ZIV018+xgBJOm5YstHRJw0lyDL4
+IBHNfTIzSJRUTN3cecQwn+uOuFW114hcxWokPbLTBQNRxgfvzBRydD1ucs4YKIxKoHflCStFREes
+t2d/AYoFWpO+ocH/+OcOZ6RHSXZddZAa9SaP8A==
+-----END CERTIFICATE-----
+
+NetLock Qualified (Class QA) Root
+=================================
+-----BEGIN CERTIFICATE-----
+MIIG0TCCBbmgAwIBAgIBezANBgkqhkiG9w0BAQUFADCByTELMAkGA1UEBhMCSFUxETAPBgNVBAcT
+CEJ1ZGFwZXN0MScwJQYDVQQKEx5OZXRMb2NrIEhhbG96YXRiaXp0b25zYWdpIEtmdC4xGjAYBgNV
+BAsTEVRhbnVzaXR2YW55a2lhZG9rMUIwQAYDVQQDEzlOZXRMb2NrIE1pbm9zaXRldHQgS296amVn
+eXpvaSAoQ2xhc3MgUUEpIFRhbnVzaXR2YW55a2lhZG8xHjAcBgkqhkiG9w0BCQEWD2luZm9AbmV0
+bG9jay5odTAeFw0wMzAzMzAwMTQ3MTFaFw0yMjEyMTUwMTQ3MTFaMIHJMQswCQYDVQQGEwJIVTER
+MA8GA1UEBxMIQnVkYXBlc3QxJzAlBgNVBAoTHk5ldExvY2sgSGFsb3phdGJpenRvbnNhZ2kgS2Z0
+LjEaMBgGA1UECxMRVGFudXNpdHZhbnlraWFkb2sxQjBABgNVBAMTOU5ldExvY2sgTWlub3NpdGV0
+dCBLb3pqZWd5em9pIChDbGFzcyBRQSkgVGFudXNpdHZhbnlraWFkbzEeMBwGCSqGSIb3DQEJARYP
+aW5mb0BuZXRsb2NrLmh1MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAx1Ilstg91IRV
+CacbvWy5FPSKAtt2/GoqeKvld/Bu4IwjZ9ulZJm53QE+b+8tmjwi8F3JV6BVQX/yQ15YglMxZc4e
+8ia6AFQer7C8HORSjKAyr7c3sVNnaHRnUPYtLmTeriZ539+Zhqurf4XsoPuAzPS4DB6TRWO53Lhb
+m+1bOdRfYrCnjnxmOCyqsQhjF2d9zL2z8cM/z1A57dEZgxXbhxInlrfa6uWdvLrqOU+L73Sa58XQ
+0uqGURzk/mQIKAR5BevKxXEOC++r6uwSEaEYBTJp0QwsGj0lmT+1fMptsK6ZmfoIYOcZwvK9UdPM
+0wKswREMgM6r3JSda6M5UzrWhQIDAMV9o4ICwDCCArwwEgYDVR0TAQH/BAgwBgEB/wIBBDAOBgNV
+HQ8BAf8EBAMCAQYwggJ1BglghkgBhvhCAQ0EggJmFoICYkZJR1lFTEVNISBFemVuIHRhbnVzaXR2
+YW55IGEgTmV0TG9jayBLZnQuIE1pbm9zaXRldHQgU3pvbGdhbHRhdGFzaSBTemFiYWx5emF0YWJh
+biBsZWlydCBlbGphcmFzb2sgYWxhcGphbiBrZXN6dWx0LiBBIG1pbm9zaXRldHQgZWxla3Ryb25p
+a3VzIGFsYWlyYXMgam9naGF0YXMgZXJ2ZW55ZXN1bGVzZW5laywgdmFsYW1pbnQgZWxmb2dhZGFz
+YW5hayBmZWx0ZXRlbGUgYSBNaW5vc2l0ZXR0IFN6b2xnYWx0YXRhc2kgU3phYmFseXphdGJhbiwg
+YXogQWx0YWxhbm9zIFN6ZXJ6b2Rlc2kgRmVsdGV0ZWxla2JlbiBlbG9pcnQgZWxsZW5vcnplc2kg
+ZWxqYXJhcyBtZWd0ZXRlbGUuIEEgZG9rdW1lbnR1bW9rIG1lZ3RhbGFsaGF0b2sgYSBodHRwczov
+L3d3dy5uZXRsb2NrLmh1L2RvY3MvIGNpbWVuIHZhZ3kga2VyaGV0b2sgYXogaW5mb0BuZXRsb2Nr
+Lm5ldCBlLW1haWwgY2ltZW4uIFdBUk5JTkchIFRoZSBpc3N1YW5jZSBhbmQgdGhlIHVzZSBvZiB0
+aGlzIGNlcnRpZmljYXRlIGFyZSBzdWJqZWN0IHRvIHRoZSBOZXRMb2NrIFF1YWxpZmllZCBDUFMg
+YXZhaWxhYmxlIGF0IGh0dHBzOi8vd3d3Lm5ldGxvY2suaHUvZG9jcy8gb3IgYnkgZS1tYWlsIGF0
+IGluZm9AbmV0bG9jay5uZXQwHQYDVR0OBBYEFAlqYhaSsFq7VQ7LdTI6MuWyIckoMA0GCSqGSIb3
+DQEBBQUAA4IBAQCRalCc23iBmz+LQuM7/KbD7kPgz/PigDVJRXYC4uMvBcXxKufAQTPGtpvQMznN
+wNuhrWw3AkxYQTvyl5LGSKjN5Yo5iWH5Upfpvfb5lHTocQ68d4bDBsxafEp+NFAwLvt/MpqNPfMg
+W/hqyobzMUwsWYACff44yTB1HLdV47yfuqhthCgFdbOLDcCRVCHnpgu0mfVRQdzNo0ci2ccBgcTc
+R08m6h/t280NmPSjnLRzMkqWmf68f8glWPhY83ZmiVSkpj7EUFy6iRiCdUgh0k8T6GB+B3bbELVR
+5qq5aKrN9p2QdRLqOBrKROi3macqaJVmlaut74nLYKkGEsaUR+ko
+-----END CERTIFICATE-----
+
+NetLock Notary (Class A) Root
+=============================
+-----BEGIN CERTIFICATE-----
+MIIGfTCCBWWgAwIBAgICAQMwDQYJKoZIhvcNAQEEBQAwga8xCzAJBgNVBAYTAkhVMRAwDgYDVQQI
+EwdIdW5nYXJ5MREwDwYDVQQHEwhCdWRhcGVzdDEnMCUGA1UEChMeTmV0TG9jayBIYWxvemF0Yml6
+dG9uc2FnaSBLZnQuMRowGAYDVQQLExFUYW51c2l0dmFueWtpYWRvazE2MDQGA1UEAxMtTmV0TG9j
+ayBLb3pqZWd5em9pIChDbGFzcyBBKSBUYW51c2l0dmFueWtpYWRvMB4XDTk5MDIyNDIzMTQ0N1oX
+DTE5MDIxOTIzMTQ0N1owga8xCzAJBgNVBAYTAkhVMRAwDgYDVQQIEwdIdW5nYXJ5MREwDwYDVQQH
+EwhCdWRhcGVzdDEnMCUGA1UEChMeTmV0TG9jayBIYWxvemF0Yml6dG9uc2FnaSBLZnQuMRowGAYD
+VQQLExFUYW51c2l0dmFueWtpYWRvazE2MDQGA1UEAxMtTmV0TG9jayBLb3pqZWd5em9pIChDbGFz
+cyBBKSBUYW51c2l0dmFueWtpYWRvMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvHSM
+D7tM9DceqQWC2ObhbHDqeLVu0ThEDaiDzl3S1tWBxdRL51uUcCbbO51qTGL3cfNk1mE7PetzozfZ
+z+qMkjvN9wfcZnSX9EUi3fRc4L9t875lM+QVOr/bmJBVOMTtplVjC7B4BPTjbsE/jvxReB+SnoPC
+/tmwqcm8WgD/qaiYdPv2LD4VOQ22BFWoDpggQrOxJa1+mm9dU7GrDPzr4PN6s6iz/0b2Y6LYOph7
+tqyF/7AlT3Rj5xMHpQqPBffAZG9+pyeAlt7ULoZgx2srXnN7F+eRP2QM2EsiNCubMvJIH5+hCoR6
+4sKtlz2O1cH5VqNQ6ca0+pii7pXmKgOM3wIDAQABo4ICnzCCApswDgYDVR0PAQH/BAQDAgAGMBIG
+A1UdEwEB/wQIMAYBAf8CAQQwEQYJYIZIAYb4QgEBBAQDAgAHMIICYAYJYIZIAYb4QgENBIICURaC
+Ak1GSUdZRUxFTSEgRXplbiB0YW51c2l0dmFueSBhIE5ldExvY2sgS2Z0LiBBbHRhbGFub3MgU3pv
+bGdhbHRhdGFzaSBGZWx0ZXRlbGVpYmVuIGxlaXJ0IGVsamFyYXNvayBhbGFwamFuIGtlc3p1bHQu
+IEEgaGl0ZWxlc2l0ZXMgZm9seWFtYXRhdCBhIE5ldExvY2sgS2Z0LiB0ZXJtZWtmZWxlbG9zc2Vn
+LWJpenRvc2l0YXNhIHZlZGkuIEEgZGlnaXRhbGlzIGFsYWlyYXMgZWxmb2dhZGFzYW5hayBmZWx0
+ZXRlbGUgYXogZWxvaXJ0IGVsbGVub3J6ZXNpIGVsamFyYXMgbWVndGV0ZWxlLiBBeiBlbGphcmFz
+IGxlaXJhc2EgbWVndGFsYWxoYXRvIGEgTmV0TG9jayBLZnQuIEludGVybmV0IGhvbmxhcGphbiBh
+IGh0dHBzOi8vd3d3Lm5ldGxvY2submV0L2RvY3MgY2ltZW4gdmFneSBrZXJoZXRvIGF6IGVsbGVu
+b3J6ZXNAbmV0bG9jay5uZXQgZS1tYWlsIGNpbWVuLiBJTVBPUlRBTlQhIFRoZSBpc3N1YW5jZSBh
+bmQgdGhlIHVzZSBvZiB0aGlzIGNlcnRpZmljYXRlIGlzIHN1YmplY3QgdG8gdGhlIE5ldExvY2sg
+Q1BTIGF2YWlsYWJsZSBhdCBodHRwczovL3d3dy5uZXRsb2NrLm5ldC9kb2NzIG9yIGJ5IGUtbWFp
+bCBhdCBjcHNAbmV0bG9jay5uZXQuMA0GCSqGSIb3DQEBBAUAA4IBAQBIJEb3ulZv+sgoA0BO5TE5
+ayZrU3/b39/zcT0mwBQOxmd7I6gMc90Bu8bKbjc5VdXHjFYgDigKDtIqpLBJUsY4B/6+CgmM0ZjP
+ytoUMaFP0jn8DxEsQ8Pdq5PHVT5HfBgaANzze9jyf1JsIPQLX2lS9O74silg6+NJMSEN1rUQQeJB
+CWziGppWS3cC9qCbmieH6FUpccKQn0V4GuEVZD3QDtigdp+uxdAu6tYPVuxkf1qbFFgBJ34TUMdr
+KuZoPL9coAob4Q566eKAw+np9v1sEZ7Q5SgnK1QyQhSCdeZK8CtmdWOMovsEPoMOmzbwGOQmIMOM
+8CgHrTwXZoi1/baI
+-----END CERTIFICATE-----
+
+NetLock Business (Class B) Root
+===============================
+-----BEGIN CERTIFICATE-----
+MIIFSzCCBLSgAwIBAgIBaTANBgkqhkiG9w0BAQQFADCBmTELMAkGA1UEBhMCSFUxETAPBgNVBAcT
+CEJ1ZGFwZXN0MScwJQYDVQQKEx5OZXRMb2NrIEhhbG96YXRiaXp0b25zYWdpIEtmdC4xGjAYBgNV
+BAsTEVRhbnVzaXR2YW55a2lhZG9rMTIwMAYDVQQDEylOZXRMb2NrIFV6bGV0aSAoQ2xhc3MgQikg
+VGFudXNpdHZhbnlraWFkbzAeFw05OTAyMjUxNDEwMjJaFw0xOTAyMjAxNDEwMjJaMIGZMQswCQYD
+VQQGEwJIVTERMA8GA1UEBxMIQnVkYXBlc3QxJzAlBgNVBAoTHk5ldExvY2sgSGFsb3phdGJpenRv
+bnNhZ2kgS2Z0LjEaMBgGA1UECxMRVGFudXNpdHZhbnlraWFkb2sxMjAwBgNVBAMTKU5ldExvY2sg
+VXpsZXRpIChDbGFzcyBCKSBUYW51c2l0dmFueWtpYWRvMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCB
+iQKBgQCx6gTsIKAjwo84YM/HRrPVG/77uZmeBNwcf4xKgZjupNTKihe5In+DCnVMm8Bp2GQ5o+2S
+o/1bXHQawEfKOml2mrriRBf8TKPV/riXiK+IA4kfpPIEPsgHC+b5sy96YhQJRhTKZPWLgLViqNhr
+1nGTLbO/CVRY7QbrqHvcQ7GhaQIDAQABo4ICnzCCApswEgYDVR0TAQH/BAgwBgEB/wIBBDAOBgNV
+HQ8BAf8EBAMCAAYwEQYJYIZIAYb4QgEBBAQDAgAHMIICYAYJYIZIAYb4QgENBIICURaCAk1GSUdZ
+RUxFTSEgRXplbiB0YW51c2l0dmFueSBhIE5ldExvY2sgS2Z0LiBBbHRhbGFub3MgU3pvbGdhbHRh
+dGFzaSBGZWx0ZXRlbGVpYmVuIGxlaXJ0IGVsamFyYXNvayBhbGFwamFuIGtlc3p1bHQuIEEgaGl0
+ZWxlc2l0ZXMgZm9seWFtYXRhdCBhIE5ldExvY2sgS2Z0LiB0ZXJtZWtmZWxlbG9zc2VnLWJpenRv
+c2l0YXNhIHZlZGkuIEEgZGlnaXRhbGlzIGFsYWlyYXMgZWxmb2dhZGFzYW5hayBmZWx0ZXRlbGUg
+YXogZWxvaXJ0IGVsbGVub3J6ZXNpIGVsamFyYXMgbWVndGV0ZWxlLiBBeiBlbGphcmFzIGxlaXJh
+c2EgbWVndGFsYWxoYXRvIGEgTmV0TG9jayBLZnQuIEludGVybmV0IGhvbmxhcGphbiBhIGh0dHBz
+Oi8vd3d3Lm5ldGxvY2submV0L2RvY3MgY2ltZW4gdmFneSBrZXJoZXRvIGF6IGVsbGVub3J6ZXNA
+bmV0bG9jay5uZXQgZS1tYWlsIGNpbWVuLiBJTVBPUlRBTlQhIFRoZSBpc3N1YW5jZSBhbmQgdGhl
+IHVzZSBvZiB0aGlzIGNlcnRpZmljYXRlIGlzIHN1YmplY3QgdG8gdGhlIE5ldExvY2sgQ1BTIGF2
+YWlsYWJsZSBhdCBodHRwczovL3d3dy5uZXRsb2NrLm5ldC9kb2NzIG9yIGJ5IGUtbWFpbCBhdCBj
+cHNAbmV0bG9jay5uZXQuMA0GCSqGSIb3DQEBBAUAA4GBAATbrowXr/gOkDFOzT4JwG06sPgzTEdM
+43WIEJessDgVkcYplswhwG08pXTP2IKlOcNl40JwuyKQ433bNXbhoLXan3BukxowOR0w2y7jfLKR
+stE3Kfq51hdcR0/jHTjrn9V7lagonhVK0dHQKwCXoOKSNitjrFgBazMpUIaD8QFI
+-----END CERTIFICATE-----
+
+NetLock Express (Class C) Root
+==============================
+-----BEGIN CERTIFICATE-----
+MIIFTzCCBLigAwIBAgIBaDANBgkqhkiG9w0BAQQFADCBmzELMAkGA1UEBhMCSFUxETAPBgNVBAcT
+CEJ1ZGFwZXN0MScwJQYDVQQKEx5OZXRMb2NrIEhhbG96YXRiaXp0b25zYWdpIEtmdC4xGjAYBgNV
+BAsTEVRhbnVzaXR2YW55a2lhZG9rMTQwMgYDVQQDEytOZXRMb2NrIEV4cHJlc3N6IChDbGFzcyBD
+KSBUYW51c2l0dmFueWtpYWRvMB4XDTk5MDIyNTE0MDgxMVoXDTE5MDIyMDE0MDgxMVowgZsxCzAJ
+BgNVBAYTAkhVMREwDwYDVQQHEwhCdWRhcGVzdDEnMCUGA1UEChMeTmV0TG9jayBIYWxvemF0Yml6
+dG9uc2FnaSBLZnQuMRowGAYDVQQLExFUYW51c2l0dmFueWtpYWRvazE0MDIGA1UEAxMrTmV0TG9j
+ayBFeHByZXNzeiAoQ2xhc3MgQykgVGFudXNpdHZhbnlraWFkbzCBnzANBgkqhkiG9w0BAQEFAAOB
+jQAwgYkCgYEA6+ywbGGKIyWvYCDj2Z/8kwvbXY2wobNAOoLO/XXgeDIDhlqGlZHtU/qdQPzm6N3Z
+W3oDvV3zOwzDUXmbrVWg6dADEK8KuhRC2VImESLH0iDMgqSaqf64gXadarfSNnU+sYYJ9m5tfk63
+euyucYT2BDMIJTLrdKwWRMbkQJMdf60CAwEAAaOCAp8wggKbMBIGA1UdEwEB/wQIMAYBAf8CAQQw
+DgYDVR0PAQH/BAQDAgAGMBEGCWCGSAGG+EIBAQQEAwIABzCCAmAGCWCGSAGG+EIBDQSCAlEWggJN
+RklHWUVMRU0hIEV6ZW4gdGFudXNpdHZhbnkgYSBOZXRMb2NrIEtmdC4gQWx0YWxhbm9zIFN6b2xn
+YWx0YXRhc2kgRmVsdGV0ZWxlaWJlbiBsZWlydCBlbGphcmFzb2sgYWxhcGphbiBrZXN6dWx0LiBB
+IGhpdGVsZXNpdGVzIGZvbHlhbWF0YXQgYSBOZXRMb2NrIEtmdC4gdGVybWVrZmVsZWxvc3NlZy1i
+aXp0b3NpdGFzYSB2ZWRpLiBBIGRpZ2l0YWxpcyBhbGFpcmFzIGVsZm9nYWRhc2FuYWsgZmVsdGV0
+ZWxlIGF6IGVsb2lydCBlbGxlbm9yemVzaSBlbGphcmFzIG1lZ3RldGVsZS4gQXogZWxqYXJhcyBs
+ZWlyYXNhIG1lZ3RhbGFsaGF0byBhIE5ldExvY2sgS2Z0LiBJbnRlcm5ldCBob25sYXBqYW4gYSBo
+dHRwczovL3d3dy5uZXRsb2NrLm5ldC9kb2NzIGNpbWVuIHZhZ3kga2VyaGV0byBheiBlbGxlbm9y
+emVzQG5ldGxvY2submV0IGUtbWFpbCBjaW1lbi4gSU1QT1JUQU5UISBUaGUgaXNzdWFuY2UgYW5k
+IHRoZSB1c2Ugb2YgdGhpcyBjZXJ0aWZpY2F0ZSBpcyBzdWJqZWN0IHRvIHRoZSBOZXRMb2NrIENQ
+UyBhdmFpbGFibGUgYXQgaHR0cHM6Ly93d3cubmV0bG9jay5uZXQvZG9jcyBvciBieSBlLW1haWwg
+YXQgY3BzQG5ldGxvY2submV0LjANBgkqhkiG9w0BAQQFAAOBgQAQrX/XDDKACtiG8XmYta3UzbM2
+xJZIwVzNmtkFLp++UOv0JhQQLdRmF/iewSf98e3ke0ugbLWrmldwpu2gpO0u9f38vf5NNwgMvOOW
+gyL1SRt/Syu0VMGAfJlOHdCM7tCs5ZL6dVb+ZKATj7i4Fp1hBWeAyNDYpQcCNJgEjTME1A==
+-----END CERTIFICATE-----
+
+XRamp Global CA Root
+====================
+-----BEGIN CERTIFICATE-----
+MIIEMDCCAxigAwIBAgIQUJRs7Bjq1ZxN1ZfvdY+grTANBgkqhkiG9w0BAQUFADCBgjELMAkGA1UE
+BhMCVVMxHjAcBgNVBAsTFXd3dy54cmFtcHNlY3VyaXR5LmNvbTEkMCIGA1UEChMbWFJhbXAgU2Vj
+dXJpdHkgU2VydmljZXMgSW5jMS0wKwYDVQQDEyRYUmFtcCBHbG9iYWwgQ2VydGlmaWNhdGlvbiBB
+dXRob3JpdHkwHhcNMDQxMTAxMTcxNDA0WhcNMzUwMTAxMDUzNzE5WjCBgjELMAkGA1UEBhMCVVMx
+HjAcBgNVBAsTFXd3dy54cmFtcHNlY3VyaXR5LmNvbTEkMCIGA1UEChMbWFJhbXAgU2VjdXJpdHkg
+U2VydmljZXMgSW5jMS0wKwYDVQQDEyRYUmFtcCBHbG9iYWwgQ2VydGlmaWNhdGlvbiBBdXRob3Jp
+dHkwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCYJB69FbS638eMpSe2OAtp87ZOqCwu
+IR1cRN8hXX4jdP5efrRKt6atH67gBhbim1vZZ3RrXYCPKZ2GG9mcDZhtdhAoWORlsH9KmHmf4MMx
+foArtYzAQDsRhtDLooY2YKTVMIJt2W7QDxIEM5dfT2Fa8OT5kavnHTu86M/0ay00fOJIYRyO82FE
+zG+gSqmUsE3a56k0enI4qEHMPJQRfevIpoy3hsvKMzvZPTeL+3o+hiznc9cKV6xkmxnr9A8ECIqs
+AxcZZPRaJSKNNCyy9mgdEm3Tih4U2sSPpuIjhdV6Db1q4Ons7Be7QhtnqiXtRYMh/MHJfNViPvry
+xS3T/dRlAgMBAAGjgZ8wgZwwEwYJKwYBBAGCNxQCBAYeBABDAEEwCwYDVR0PBAQDAgGGMA8GA1Ud
+EwEB/wQFMAMBAf8wHQYDVR0OBBYEFMZPoj0GY4QJnM5i5ASsjVy16bYbMDYGA1UdHwQvMC0wK6Ap
+oCeGJWh0dHA6Ly9jcmwueHJhbXBzZWN1cml0eS5jb20vWEdDQS5jcmwwEAYJKwYBBAGCNxUBBAMC
+AQEwDQYJKoZIhvcNAQEFBQADggEBAJEVOQMBG2f7Shz5CmBbodpNl2L5JFMn14JkTpAuw0kbK5rc
+/Kh4ZzXxHfARvbdI4xD2Dd8/0sm2qlWkSLoC295ZLhVbO50WfUfXN+pfTXYSNrsf16GBBEYgoyxt
+qZ4Bfj8pzgCT3/3JknOJiWSe5yvkHJEs0rnOfc5vMZnT5r7SHpDwCRR5XCOrTdLaIR9NmXmd4c8n
+nxCbHIgNsIpkQTG4DmyQJKSbXHGPurt+HBvbaoAPIbzp26a3QPSyi6mx5O+aGtA9aZnuqCij4Tyz
+8LIRnM98QObd50N9otg6tamN8jSZxNQQ4Qb9CYQQO+7ETPTsJ3xCwnR8gooJybQDJbw=
+-----END CERTIFICATE-----
+
+Go Daddy Class 2 CA
+===================
+-----BEGIN CERTIFICATE-----
+MIIEADCCAuigAwIBAgIBADANBgkqhkiG9w0BAQUFADBjMQswCQYDVQQGEwJVUzEhMB8GA1UEChMY
+VGhlIEdvIERhZGR5IEdyb3VwLCBJbmMuMTEwLwYDVQQLEyhHbyBEYWRkeSBDbGFzcyAyIENlcnRp
+ZmljYXRpb24gQXV0aG9yaXR5MB4XDTA0MDYyOTE3MDYyMFoXDTM0MDYyOTE3MDYyMFowYzELMAkG
+A1UEBhMCVVMxITAfBgNVBAoTGFRoZSBHbyBEYWRkeSBHcm91cCwgSW5jLjExMC8GA1UECxMoR28g
+RGFkZHkgQ2xhc3MgMiBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eTCCASAwDQYJKoZIhvcNAQEBBQAD
+ggENADCCAQgCggEBAN6d1+pXGEmhW+vXX0iG6r7d/+TvZxz0ZWizV3GgXne77ZtJ6XCAPVYYYwhv
+2vLM0D9/AlQiVBDYsoHUwHU9S3/Hd8M+eKsaA7Ugay9qK7HFiH7Eux6wwdhFJ2+qN1j3hybX2C32
+qRe3H3I2TqYXP2WYktsqbl2i/ojgC95/5Y0V4evLOtXiEqITLdiOr18SPaAIBQi2XKVlOARFmR6j
+YGB0xUGlcmIbYsUfb18aQr4CUWWoriMYavx4A6lNf4DD+qta/KFApMoZFv6yyO9ecw3ud72a9nmY
+vLEHZ6IVDd2gWMZEewo+YihfukEHU1jPEX44dMX4/7VpkI+EdOqXG68CAQOjgcAwgb0wHQYDVR0O
+BBYEFNLEsNKR1EwRcbNhyz2h/t2oatTjMIGNBgNVHSMEgYUwgYKAFNLEsNKR1EwRcbNhyz2h/t2o
+atTjoWekZTBjMQswCQYDVQQGEwJVUzEhMB8GA1UEChMYVGhlIEdvIERhZGR5IEdyb3VwLCBJbmMu
+MTEwLwYDVQQLEyhHbyBEYWRkeSBDbGFzcyAyIENlcnRpZmljYXRpb24gQXV0aG9yaXR5ggEAMAwG
+A1UdEwQFMAMBAf8wDQYJKoZIhvcNAQEFBQADggEBADJL87LKPpH8EsahB4yOd6AzBhRckB4Y9wim
+PQoZ+YeAEW5p5JYXMP80kWNyOO7MHAGjHZQopDH2esRU1/blMVgDoszOYtuURXO1v0XJJLXVggKt
+I3lpjbi2Tc7PTMozI+gciKqdi0FuFskg5YmezTvacPd+mSYgFFQlq25zheabIZ0KbIIOqPjCDPoQ
+HmyW74cNxA9hi63ugyuV+I6ShHI56yDqg+2DzZduCLzrTia2cyvk0/ZM/iZx4mERdEr/VxqHD3VI
+Ls9RaRegAhJhldXRQLIQTO7ErBBDpqWeCtWVYpoNz4iCxTIM5CufReYNnyicsbkqWletNw+vHX/b
+vZ8=
+-----END CERTIFICATE-----
+
+Starfield Class 2 CA
+====================
+-----BEGIN CERTIFICATE-----
+MIIEDzCCAvegAwIBAgIBADANBgkqhkiG9w0BAQUFADBoMQswCQYDVQQGEwJVUzElMCMGA1UEChMc
+U3RhcmZpZWxkIFRlY2hub2xvZ2llcywgSW5jLjEyMDAGA1UECxMpU3RhcmZpZWxkIENsYXNzIDIg
+Q2VydGlmaWNhdGlvbiBBdXRob3JpdHkwHhcNMDQwNjI5MTczOTE2WhcNMzQwNjI5MTczOTE2WjBo
+MQswCQYDVQQGEwJVUzElMCMGA1UEChMcU3RhcmZpZWxkIFRlY2hub2xvZ2llcywgSW5jLjEyMDAG
+A1UECxMpU3RhcmZpZWxkIENsYXNzIDIgQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkwggEgMA0GCSqG
+SIb3DQEBAQUAA4IBDQAwggEIAoIBAQC3Msj+6XGmBIWtDBFk385N78gDGIc/oav7PKaf8MOh2tTY
+bitTkPskpD6E8J7oX+zlJ0T1KKY/e97gKvDIr1MvnsoFAZMej2YcOadN+lq2cwQlZut3f+dZxkqZ
+JRRU6ybH838Z1TBwj6+wRir/resp7defqgSHo9T5iaU0X9tDkYI22WY8sbi5gv2cOj4QyDvvBmVm
+epsZGD3/cVE8MC5fvj13c7JdBmzDI1aaK4UmkhynArPkPw2vCHmCuDY96pzTNbO8acr1zJ3o/WSN
+F4Azbl5KXZnJHoe0nRrA1W4TNSNe35tfPe/W93bC6j67eA0cQmdrBNj41tpvi/JEoAGrAgEDo4HF
+MIHCMB0GA1UdDgQWBBS/X7fRzt0fhvRbVazc1xDCDqmI5zCBkgYDVR0jBIGKMIGHgBS/X7fRzt0f
+hvRbVazc1xDCDqmI56FspGowaDELMAkGA1UEBhMCVVMxJTAjBgNVBAoTHFN0YXJmaWVsZCBUZWNo
+bm9sb2dpZXMsIEluYy4xMjAwBgNVBAsTKVN0YXJmaWVsZCBDbGFzcyAyIENlcnRpZmljYXRpb24g
+QXV0aG9yaXR5ggEAMAwGA1UdEwQFMAMBAf8wDQYJKoZIhvcNAQEFBQADggEBAAWdP4id0ckaVaGs
+afPzWdqbAYcaT1epoXkJKtv3L7IezMdeatiDh6GX70k1PncGQVhiv45YuApnP+yz3SFmH8lU+nLM
+PUxA2IGvd56Deruix/U0F47ZEUD0/CwqTRV/p2JdLiXTAAsgGh1o+Re49L2L7ShZ3U0WixeDyLJl
+xy16paq8U4Zt3VekyvggQQto8PT7dL5WXXp59fkdheMtlb71cZBDzI0fmgAKhynpVSJYACPq4xJD
+KVtHCN2MQWplBqjlIapBtJUhlbl90TSrE9atvNziPTnNvT51cKEYWQPJIrSPnNVeKtelttQKbfi3
+QBFGmh95DmK/D5fs4C8fF5Q=
+-----END CERTIFICATE-----
+
+StartCom Certification Authority
+================================
+-----BEGIN CERTIFICATE-----
+MIIHyTCCBbGgAwIBAgIBATANBgkqhkiG9w0BAQUFADB9MQswCQYDVQQGEwJJTDEWMBQGA1UEChMN
+U3RhcnRDb20gTHRkLjErMCkGA1UECxMiU2VjdXJlIERpZ2l0YWwgQ2VydGlmaWNhdGUgU2lnbmlu
+ZzEpMCcGA1UEAxMgU3RhcnRDb20gQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkwHhcNMDYwOTE3MTk0
+NjM2WhcNMzYwOTE3MTk0NjM2WjB9MQswCQYDVQQGEwJJTDEWMBQGA1UEChMNU3RhcnRDb20gTHRk
+LjErMCkGA1UECxMiU2VjdXJlIERpZ2l0YWwgQ2VydGlmaWNhdGUgU2lnbmluZzEpMCcGA1UEAxMg
+U3RhcnRDb20gQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkwggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAw
+ggIKAoICAQDBiNsJvGxGfHiflXu1M5DycmLWwTYgIiRezul38kMKogZkpMyONvg45iPwbm2xPN1y
+o4UcodM9tDMr0y+v/uqwQVlntsQGfQqedIXWeUyAN3rfOQVSWff0G0ZDpNKFhdLDcfN1YjS6LIp/
+Ho/u7TTQEceWzVI9ujPW3U3eCztKS5/CJi/6tRYccjV3yjxd5srhJosaNnZcAdt0FCX+7bWgiA/d
+eMotHweXMAEtcnn6RtYTKqi5pquDSR3l8u/d5AGOGAqPY1MWhWKpDhk6zLVmpsJrdAfkK+F2PrRt
+2PZE4XNiHzvEvqBTViVsUQn3qqvKv3b9bZvzndu/PWa8DFaqr5hIlTpL36dYUNk4dalb6kMMAv+Z
+6+hsTXBbKWWc3apdzK8BMewM69KN6Oqce+Zu9ydmDBpI125C4z/eIT574Q1w+2OqqGwaVLRcJXrJ
+osmLFqa7LH4XXgVNWG4SHQHuEhANxjJ/GP/89PrNbpHoNkm+Gkhpi8KWTRoSsmkXwQqQ1vp5Iki/
+untp+HDH+no32NgN0nZPV/+Qt+OR0t3vwmC3Zzrd/qqc8NSLf3Iizsafl7b4r4qgEKjZ+xjGtrVc
+UjyJthkqcwEKDwOzEmDyei+B26Nu/yYwl/WL3YlXtq09s68rxbd2AvCl1iuahhQqcvbjM4xdCUsT
+37uMdBNSSwIDAQABo4ICUjCCAk4wDAYDVR0TBAUwAwEB/zALBgNVHQ8EBAMCAa4wHQYDVR0OBBYE
+FE4L7xqkQFulF2mHMMo0aEPQQa7yMGQGA1UdHwRdMFswLKAqoCiGJmh0dHA6Ly9jZXJ0LnN0YXJ0
+Y29tLm9yZy9zZnNjYS1jcmwuY3JsMCugKaAnhiVodHRwOi8vY3JsLnN0YXJ0Y29tLm9yZy9zZnNj
+YS1jcmwuY3JsMIIBXQYDVR0gBIIBVDCCAVAwggFMBgsrBgEEAYG1NwEBATCCATswLwYIKwYBBQUH
+AgEWI2h0dHA6Ly9jZXJ0LnN0YXJ0Y29tLm9yZy9wb2xpY3kucGRmMDUGCCsGAQUFBwIBFilodHRw
+Oi8vY2VydC5zdGFydGNvbS5vcmcvaW50ZXJtZWRpYXRlLnBkZjCB0AYIKwYBBQUHAgIwgcMwJxYg
+U3RhcnQgQ29tbWVyY2lhbCAoU3RhcnRDb20pIEx0ZC4wAwIBARqBl0xpbWl0ZWQgTGlhYmlsaXR5
+LCByZWFkIHRoZSBzZWN0aW9uICpMZWdhbCBMaW1pdGF0aW9ucyogb2YgdGhlIFN0YXJ0Q29tIENl
+cnRpZmljYXRpb24gQXV0aG9yaXR5IFBvbGljeSBhdmFpbGFibGUgYXQgaHR0cDovL2NlcnQuc3Rh
+cnRjb20ub3JnL3BvbGljeS5wZGYwEQYJYIZIAYb4QgEBBAQDAgAHMDgGCWCGSAGG+EIBDQQrFilT
+dGFydENvbSBGcmVlIFNTTCBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eTANBgkqhkiG9w0BAQUFAAOC
+AgEAFmyZ9GYMNPXQhV59CuzaEE44HF7fpiUFS5Eyweg78T3dRAlbB0mKKctmArexmvclmAk8jhvh
+3TaHK0u7aNM5Zj2gJsfyOZEdUauCe37Vzlrk4gNXcGmXCPleWKYK34wGmkUWFjgKXlf2Ysd6AgXm
+vB618p70qSmD+LIU424oh0TDkBreOKk8rENNZEXO3SipXPJzewT4F+irsfMuXGRuczE6Eri8sxHk
+fY+BUZo7jYn0TZNmezwD7dOaHZrzZVD1oNB1ny+v8OqCQ5j4aZyJecRDjkZy42Q2Eq/3JR44iZB3
+fsNrarnDy0RLrHiQi+fHLB5LEUTINFInzQpdn4XBidUaePKVEFMy3YCEZnXZtWgo+2EuvoSoOMCZ
+EoalHmdkrQYuL6lwhceWD3yJZfWOQ1QOq92lgDmUYMA0yZZwLKMS9R9Ie70cfmu3nZD0Ijuu+Pwq
+yvqCUqDvr0tVk+vBtfAii6w0TiYiBKGHLHVKt+V9E9e4DGTANtLJL4YSjCMJwRuCO3NJo2pXh5Tl
+1njFmUNj403gdy3hZZlyaQQaRwnmDwFWJPsfvw55qVguucQJAX6Vum0ABj6y6koQOdjQK/W/7HW/
+lwLFCRsI3FU34oH7N4RDYiDK51ZLZer+bMEkkyShNOsF/5oirpt9P/FlUQqmMGqz9IgcgA38coro
+g14=
+-----END CERTIFICATE-----
+
+Taiwan GRCA
+===========
+-----BEGIN CERTIFICATE-----
+MIIFcjCCA1qgAwIBAgIQH51ZWtcvwgZEpYAIaeNe9jANBgkqhkiG9w0BAQUFADA/MQswCQYDVQQG
+EwJUVzEwMC4GA1UECgwnR292ZXJubWVudCBSb290IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MB4X
+DTAyMTIwNTEzMjMzM1oXDTMyMTIwNTEzMjMzM1owPzELMAkGA1UEBhMCVFcxMDAuBgNVBAoMJ0dv
+dmVybm1lbnQgUm9vdCBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eTCCAiIwDQYJKoZIhvcNAQEBBQAD
+ggIPADCCAgoCggIBAJoluOzMonWoe/fOW1mKydGGEghU7Jzy50b2iPN86aXfTEc2pBsBHH8eV4qN
+w8XRIePaJD9IK/ufLqGU5ywck9G/GwGHU5nOp/UKIXZ3/6m3xnOUT0b3EEk3+qhZSV1qgQdW8or5
+BtD3cCJNtLdBuTK4sfCxw5w/cP1T3YGq2GN49thTbqGsaoQkclSGxtKyyhwOeYHWtXBiCAEuTk8O
+1RGvqa/lmr/czIdtJuTJV6L7lvnM4T9TjGxMfptTCAtsF/tnyMKtsc2AtJfcdgEWFelq16TheEfO
+htX7MfP6Mb40qij7cEwdScevLJ1tZqa2jWR+tSBqnTuBto9AAGdLiYa4zGX+FVPpBMHWXx1E1wov
+J5pGfaENda1UhhXcSTvxls4Pm6Dso3pdvtUqdULle96ltqqvKKyskKw4t9VoNSZ63Pc78/1Fm9G7
+Q3hub/FCVGqY8A2tl+lSXunVanLeavcbYBT0peS2cWeqH+riTcFCQP5nRhc4L0c/cZyu5SHKYS1t
+B6iEfC3uUSXxY5Ce/eFXiGvviiNtsea9P63RPZYLhY3Naye7twWb7LuRqQoHEgKXTiCQ8P8NHuJB
+O9NAOueNXdpm5AKwB1KYXA6OM5zCppX7VRluTI6uSw+9wThNXo+EHWbNxWCWtFJaBYmOlXqYwZE8
+lSOyDvR5tMl8wUohAgMBAAGjajBoMB0GA1UdDgQWBBTMzO/MKWCkO7GStjz6MmKPrCUVOzAMBgNV
+HRMEBTADAQH/MDkGBGcqBwAEMTAvMC0CAQAwCQYFKw4DAhoFADAHBgVnKgMAAAQUA5vwIhP/lSg2
+09yewDL7MTqKUWUwDQYJKoZIhvcNAQEFBQADggIBAECASvomyc5eMN1PhnR2WPWus4MzeKR6dBcZ
+TulStbngCnRiqmjKeKBMmo4sIy7VahIkv9Ro04rQ2JyftB8M3jh+Vzj8jeJPXgyfqzvS/3WXy6Tj
+Zwj/5cAWtUgBfen5Cv8b5Wppv3ghqMKnI6mGq3ZW6A4M9hPdKmaKZEk9GhiHkASfQlK3T8v+R0F2
+Ne//AHY2RTKbxkaFXeIksB7jSJaYV0eUVXoPQbFEJPPB/hprv4j9wabak2BegUqZIJxIZhm1AHlU
+D7gsL0u8qV1bYH+Mh6XgUmMqvtg7hUAV/h62ZT/FS9p+tXo1KaMuephgIqP0fSdOLeq0dDzpD6Qz
+DxARvBMB1uUO07+1EqLhRSPAzAhuYbeJq4PjJB7mXQfnHyA+z2fI56wwbSdLaG5LKlwCCDTb+Hbk
+Z6MmnD+iMsJKxYEYMRBWqoTvLQr/uB930r+lWKBi5NdLkXWNiYCYfm3LU05er/ayl4WXudpVBrkk
+7tfGOB5jGxI7leFYrPLfhNVfmS8NVVvmONsuP3LpSIXLuykTjx44VbnzssQwmSNOXfJIoRIM3BKQ
+CZBUkQM8R+XVyWXgt0t97EfTsws+rZ7QdAAO671RrcDeLMDDav7v3Aun+kbfYNucpllQdSNpc5Oy
++fwC00fmcc4QAu4njIT/rEUNE1yDMuAlpYYsfPQS
+-----END CERTIFICATE-----
+
+Firmaprofesional Root CA
+========================
+-----BEGIN CERTIFICATE-----
+MIIEVzCCAz+gAwIBAgIBATANBgkqhkiG9w0BAQUFADCBnTELMAkGA1UEBhMCRVMxIjAgBgNVBAcT
+GUMvIE11bnRhbmVyIDI0NCBCYXJjZWxvbmExQjBABgNVBAMTOUF1dG9yaWRhZCBkZSBDZXJ0aWZp
+Y2FjaW9uIEZpcm1hcHJvZmVzaW9uYWwgQ0lGIEE2MjYzNDA2ODEmMCQGCSqGSIb3DQEJARYXY2FA
+ZmlybWFwcm9mZXNpb25hbC5jb20wHhcNMDExMDI0MjIwMDAwWhcNMTMxMDI0MjIwMDAwWjCBnTEL
+MAkGA1UEBhMCRVMxIjAgBgNVBAcTGUMvIE11bnRhbmVyIDI0NCBCYXJjZWxvbmExQjBABgNVBAMT
+OUF1dG9yaWRhZCBkZSBDZXJ0aWZpY2FjaW9uIEZpcm1hcHJvZmVzaW9uYWwgQ0lGIEE2MjYzNDA2
+ODEmMCQGCSqGSIb3DQEJARYXY2FAZmlybWFwcm9mZXNpb25hbC5jb20wggEiMA0GCSqGSIb3DQEB
+AQUAA4IBDwAwggEKAoIBAQDnIwNvbyOlXnjOlSztlB5uCp4Bx+ow0Syd3Tfom5h5VtP8c9/Qit5V
+j1H5WuretXDE7aTt/6MNbg9kUDGvASdYrv5sp0ovFy3Tc9UTHI9ZpTQsHVQERc1ouKDAA6XPhUJH
+lShbz++AbOCQl4oBPB3zhxAwJkh91/zpnZFx/0GaqUC1N5wpIE8fUuOgfRNtVLcK3ulqTgesrBlf
+3H5idPayBQC6haD9HThuy1q7hryUZzM1gywfI834yJFxzJeL764P3CkDG8A563DtwW4O2GcLiam8
+NeTvtjS0pbbELaW+0MOUJEjb35bTALVmGotmBQ/dPz/LP6pemkr4tErvlTcbAgMBAAGjgZ8wgZww
+KgYDVR0RBCMwIYYfaHR0cDovL3d3dy5maXJtYXByb2Zlc2lvbmFsLmNvbTASBgNVHRMBAf8ECDAG
+AQH/AgEBMCsGA1UdEAQkMCKADzIwMDExMDI0MjIwMDAwWoEPMjAxMzEwMjQyMjAwMDBaMA4GA1Ud
+DwEB/wQEAwIBBjAdBgNVHQ4EFgQUMwugZtHq2s7eYpMEKFK1FH84aLcwDQYJKoZIhvcNAQEFBQAD
+ggEBAEdz/o0nVPD11HecJ3lXV7cVVuzH2Fi3AQL0M+2TUIiefEaxvT8Ub/GzR0iLjJcG1+p+o1wq
+u00vR+L4OQbJnC4xGgN49Lw4xiKLMzHwFgQEffl25EvXwOaD7FnMP97/T2u3Z36mhoEyIwOdyPdf
+wUpgpZKpsaSgYMN4h7Mi8yrrW6ntBas3D7Hi05V2Y1Z0jFhyGzflZKG+TQyTmAyX9odtsz/ny4Cm
+7YjHX1BiAuiZdBbQ5rQ58SfLyEDW44YQqSMSkuBpQWOnryULwMWSyx6Yo1q6xTMPoJcB3X/ge9YG
+VM+h4k0460tQtcsm9MracEpqoeJ5quGnM/b9Sh/22WA=
+-----END CERTIFICATE-----
+
+Wells Fargo Root CA
+===================
+-----BEGIN CERTIFICATE-----
+MIID5TCCAs2gAwIBAgIEOeSXnjANBgkqhkiG9w0BAQUFADCBgjELMAkGA1UEBhMCVVMxFDASBgNV
+BAoTC1dlbGxzIEZhcmdvMSwwKgYDVQQLEyNXZWxscyBGYXJnbyBDZXJ0aWZpY2F0aW9uIEF1dGhv
+cml0eTEvMC0GA1UEAxMmV2VsbHMgRmFyZ28gUm9vdCBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkwHhcN
+MDAxMDExMTY0MTI4WhcNMjEwMTE0MTY0MTI4WjCBgjELMAkGA1UEBhMCVVMxFDASBgNVBAoTC1dl
+bGxzIEZhcmdvMSwwKgYDVQQLEyNXZWxscyBGYXJnbyBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eTEv
+MC0GA1UEAxMmV2VsbHMgRmFyZ28gUm9vdCBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkwggEiMA0GCSqG
+SIb3DQEBAQUAA4IBDwAwggEKAoIBAQDVqDM7Jvk0/82bfuUER84A4n135zHCLielTWi5MbqNQ1mX
+x3Oqfz1cQJ4F5aHiidlMuD+b+Qy0yGIZLEWukR5zcUHESxP9cMIlrCL1dQu3U+SlK93OvRw6esP3
+E48mVJwWa2uv+9iWsWCaSOAlIiR5NM4OJgALTqv9i86C1y8IcGjBqAr5dE8Hq6T54oN+J3N0Prj5
+OEL8pahbSCOz6+MlsoCultQKnMJ4msZoGK43YjdeUXWoWGPAUe5AeH6orxqg4bB4nVCMe+ez/I4j
+sNtlAHCEAQgAFG5Uhpq6zPk3EPbg3oQtnaSFN9OH4xXQwReQfhkhahKpdv0SAulPIV4XAgMBAAGj
+YTBfMA8GA1UdEwEB/wQFMAMBAf8wTAYDVR0gBEUwQzBBBgtghkgBhvt7hwcBCzAyMDAGCCsGAQUF
+BwIBFiRodHRwOi8vd3d3LndlbGxzZmFyZ28uY29tL2NlcnRwb2xpY3kwDQYJKoZIhvcNAQEFBQAD
+ggEBANIn3ZwKdyu7IvICtUpKkfnRLb7kuxpo7w6kAOnu5+/u9vnldKTC2FJYxHT7zmu1Oyl5GFrv
+m+0fazbuSCUlFLZWohDo7qd/0D+j0MNdJu4HzMPBJCGHHt8qElNvQRbn7a6U+oxy+hNH8Dx+rn0R
+OhPs7fpvcmR7nX1/Jv16+yWt6j4pf0zjAFcysLPp7VMX2YuyFA4w6OXVE8Zkr8QA1dhYJPz1j+zx
+x32l2w8n0cbyQIjmH/ZhqPRCyLk306m+LFZ4wnKbWV01QIroTmMatukgalHizqSQ33ZwmVxwQ023
+tqcZZE6St8WRPH9IFmV7Fv3L/PvZ1dZPIWU7Sn9Ho/s=
+-----END CERTIFICATE-----
+
+Swisscom Root CA 1
+==================
+-----BEGIN CERTIFICATE-----
+MIIF2TCCA8GgAwIBAgIQXAuFXAvnWUHfV8w/f52oNjANBgkqhkiG9w0BAQUFADBkMQswCQYDVQQG
+EwJjaDERMA8GA1UEChMIU3dpc3Njb20xJTAjBgNVBAsTHERpZ2l0YWwgQ2VydGlmaWNhdGUgU2Vy
+dmljZXMxGzAZBgNVBAMTElN3aXNzY29tIFJvb3QgQ0EgMTAeFw0wNTA4MTgxMjA2MjBaFw0yNTA4
+MTgyMjA2MjBaMGQxCzAJBgNVBAYTAmNoMREwDwYDVQQKEwhTd2lzc2NvbTElMCMGA1UECxMcRGln
+aXRhbCBDZXJ0aWZpY2F0ZSBTZXJ2aWNlczEbMBkGA1UEAxMSU3dpc3Njb20gUm9vdCBDQSAxMIIC
+IjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEA0LmwqAzZuz8h+BvVM5OAFmUgdbI9m2BtRsiM
+MW8Xw/qabFbtPMWRV8PNq5ZJkCoZSx6jbVfd8StiKHVFXqrWW/oLJdihFvkcxC7mlSpnzNApbjyF
+NDhhSbEAn9Y6cV9Nbc5fuankiX9qUvrKm/LcqfmdmUc/TilftKaNXXsLmREDA/7n29uj/x2lzZAe
+AR81sH8A25Bvxn570e56eqeqDFdvpG3FEzuwpdntMhy0XmeLVNxzh+XTF3xmUHJd1BpYwdnP2IkC
+b6dJtDZd0KTeByy2dbcokdaXvij1mB7qWybJvbCXc9qukSbraMH5ORXWZ0sKbU/Lz7DkQnGMU3nn
+7uHbHaBuHYwadzVcFh4rUx80i9Fs/PJnB3r1re3WmquhsUvhzDdf/X/NTa64H5xD+SpYVUNFvJbN
+cA78yeNmuk6NO4HLFWR7uZToXTNShXEuT46iBhFRyePLoW4xCGQMwtI89Tbo19AOeCMgkckkKmUp
+WyL3Ic6DXqTz3kvTaI9GdVyDCW4pa8RwjPWd1yAv/0bSKzjCL3UcPX7ape8eYIVpQtPM+GP+HkM5
+haa2Y0EQs3MevNP6yn0WR+Kn1dCjigoIlmJWbjTb2QK5MHXjBNLnj8KwEUAKrNVxAmKLMb7dxiNY
+MUJDLXT5xp6mig/p/r+D5kNXJLrvRjSq1xIBOO0CAwEAAaOBhjCBgzAOBgNVHQ8BAf8EBAMCAYYw
+HQYDVR0hBBYwFDASBgdghXQBUwABBgdghXQBUwABMBIGA1UdEwEB/wQIMAYBAf8CAQcwHwYDVR0j
+BBgwFoAUAyUv3m+CATpcLNwroWm1Z9SM0/0wHQYDVR0OBBYEFAMlL95vggE6XCzcK6FptWfUjNP9
+MA0GCSqGSIb3DQEBBQUAA4ICAQA1EMvspgQNDQ/NwNurqPKIlwzfky9NfEBWMXrrpA9gzXrzvsMn
+jgM+pN0S734edAY8PzHyHHuRMSG08NBsl9Tpl7IkVh5WwzW9iAUPWxAaZOHHgjD5Mq2eUCzneAXQ
+MbFamIp1TpBcahQq4FJHgmDmHtqBsfsUC1rxn9KVuj7QG9YVHaO+htXbD8BJZLsuUBlL0iT43R4H
+VtA4oJVwIHaM190e3p9xxCPvgxNcoyQVTSlAPGrEqdi3pkSlDfTgnXceQHAm/NrZNuR55LU/vJtl
+vrsRls/bxig5OgjOR1tTWsWZ/l2p3e9M1MalrQLmjAcSHm8D0W+go/MpvRLHUKKwf4ipmXeascCl
+OS5cfGniLLDqN2qk4Vrh9VDlg++luyqI54zb/W1elxmofmZ1a3Hqv7HHb6D0jqTsNFFbjCYDcKF3
+1QESVwA12yPeDooomf2xEG9L/zgtYE4snOtnta1J7ksfrK/7DZBaZmBwXarNeNQk7shBoJMBkpxq
+nvy5JMWzFYJ+vq6VK+uxwNrjAWALXmmshFZhvnEX/h0TD/7Gh0Xp/jKgGg0TpJRVcaUWi7rKibCy
+x/yP2FS1k2Kdzs9Z+z0YzirLNRWCXf9UIltxUvu3yf5gmwBBZPCqKuy2QkPOiWaByIufOVQDJdMW
+NY6E0F/6MBr1mmz0DlP5OlvRHA==
+-----END CERTIFICATE-----
+
+DigiCert Assured ID Root CA
+===========================
+-----BEGIN CERTIFICATE-----
+MIIDtzCCAp+gAwIBAgIQDOfg5RfYRv6P5WD8G/AwOTANBgkqhkiG9w0BAQUFADBlMQswCQYDVQQG
+EwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3d3cuZGlnaWNlcnQuY29tMSQw
+IgYDVQQDExtEaWdpQ2VydCBBc3N1cmVkIElEIFJvb3QgQ0EwHhcNMDYxMTEwMDAwMDAwWhcNMzEx
+MTEwMDAwMDAwWjBlMQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQL
+ExB3d3cuZGlnaWNlcnQuY29tMSQwIgYDVQQDExtEaWdpQ2VydCBBc3N1cmVkIElEIFJvb3QgQ0Ew
+ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCtDhXO5EOAXLGH87dg+XESpa7cJpSIqvTO
+9SA5KFhgDPiA2qkVlTJhPLWxKISKityfCgyDF3qPkKyK53lTXDGEKvYPmDI2dsze3Tyoou9q+yHy
+UmHfnyDXH+Kx2f4YZNISW1/5WBg1vEfNoTb5a3/UsDg+wRvDjDPZ2C8Y/igPs6eD1sNuRMBhNZYW
+/lmci3Zt1/GiSw0r/wty2p5g0I6QNcZ4VYcgoc/lbQrISXwxmDNsIumH0DJaoroTghHtORedmTpy
+oeb6pNnVFzF1roV9Iq4/AUaG9ih5yLHa5FcXxH4cDrC0kqZWs72yl+2qp/C3xag/lRbQ/6GW6whf
+GHdPAgMBAAGjYzBhMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBRF
+66Kv9JLLgjEtUYunpyGd823IDzAfBgNVHSMEGDAWgBRF66Kv9JLLgjEtUYunpyGd823IDzANBgkq
+hkiG9w0BAQUFAAOCAQEAog683+Lt8ONyc3pklL/3cmbYMuRCdWKuh+vy1dneVrOfzM4UKLkNl2Bc
+EkxY5NM9g0lFWJc1aRqoR+pWxnmrEthngYTffwk8lOa4JiwgvT2zKIn3X/8i4peEH+ll74fg38Fn
+SbNd67IJKusm7Xi+fT8r87cmNW1fiQG2SVufAQWbqz0lwcy2f8Lxb4bG+mRo64EtlOtCt/qMHt1i
+8b5QZ7dsvfPxH2sMNgcWfzd8qVttevESRmCD1ycEvkvOl77DZypoEd+A5wwzZr8TDRRu838fYxAe
++o0bJW1sj6W3YQGx0qMmoRBxna3iw/nDmVG3KwcIzi7mULKn+gpFL6Lw8g==
+-----END CERTIFICATE-----
+
+DigiCert Global Root CA
+=======================
+-----BEGIN CERTIFICATE-----
+MIIDrzCCApegAwIBAgIQCDvgVpBCRrGhdWrJWZHHSjANBgkqhkiG9w0BAQUFADBhMQswCQYDVQQG
+EwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3d3cuZGlnaWNlcnQuY29tMSAw
+HgYDVQQDExdEaWdpQ2VydCBHbG9iYWwgUm9vdCBDQTAeFw0wNjExMTAwMDAwMDBaFw0zMTExMTAw
+MDAwMDBaMGExCzAJBgNVBAYTAlVTMRUwEwYDVQQKEwxEaWdpQ2VydCBJbmMxGTAXBgNVBAsTEHd3
+dy5kaWdpY2VydC5jb20xIDAeBgNVBAMTF0RpZ2lDZXJ0IEdsb2JhbCBSb290IENBMIIBIjANBgkq
+hkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4jvhEXLeqKTTo1eqUKKPC3eQyaKl7hLOllsBCSDMAZOn
+TjC3U/dDxGkAV53ijSLdhwZAAIEJzs4bg7/fzTtxRuLWZscFs3YnFo97nh6Vfe63SKMI2tavegw5
+BmV/Sl0fvBf4q77uKNd0f3p4mVmFaG5cIzJLv07A6Fpt43C/dxC//AH2hdmoRBBYMql1GNXRor5H
+4idq9Joz+EkIYIvUX7Q6hL+hqkpMfT7PT19sdl6gSzeRntwi5m3OFBqOasv+zbMUZBfHWymeMr/y
+7vrTC0LUq7dBMtoM1O/4gdW7jVg/tRvoSSiicNoxBN33shbyTApOB6jtSj1etX+jkMOvJwIDAQAB
+o2MwYTAOBgNVHQ8BAf8EBAMCAYYwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUA95QNVbRTLtm
+8KPiGxvDl7I90VUwHwYDVR0jBBgwFoAUA95QNVbRTLtm8KPiGxvDl7I90VUwDQYJKoZIhvcNAQEF
+BQADggEBAMucN6pIExIK+t1EnE9SsPTfrgT1eXkIoyQY/EsrhMAtudXH/vTBH1jLuG2cenTnmCmr
+EbXjcKChzUyImZOMkXDiqw8cvpOp/2PV5Adg06O/nVsJ8dWO41P0jmP6P6fbtGbfYmbW0W5BjfIt
+tep3Sp+dWOIrWcBAI+0tKIJFPnlUkiaY4IBIqDfv8NZ5YBberOgOzW6sRBc4L0na4UU+Krk2U886
+UAb3LujEV0lsYSEY1QSteDwsOoBrp+uvFRTp2InBuThs4pFsiv9kuXclVzDAGySj4dzp30d8tbQk
+CAUw7C29C79Fv1C5qfPrmAESrciIxpg0X40KPMbp1ZWVbd4=
+-----END CERTIFICATE-----
+
+DigiCert High Assurance EV Root CA
+==================================
+-----BEGIN CERTIFICATE-----
+MIIDxTCCAq2gAwIBAgIQAqxcJmoLQJuPC3nyrkYldzANBgkqhkiG9w0BAQUFADBsMQswCQYDVQQG
+EwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3d3cuZGlnaWNlcnQuY29tMSsw
+KQYDVQQDEyJEaWdpQ2VydCBIaWdoIEFzc3VyYW5jZSBFViBSb290IENBMB4XDTA2MTExMDAwMDAw
+MFoXDTMxMTExMDAwMDAwMFowbDELMAkGA1UEBhMCVVMxFTATBgNVBAoTDERpZ2lDZXJ0IEluYzEZ
+MBcGA1UECxMQd3d3LmRpZ2ljZXJ0LmNvbTErMCkGA1UEAxMiRGlnaUNlcnQgSGlnaCBBc3N1cmFu
+Y2UgRVYgUm9vdCBDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMbM5XPm+9S75S0t
+Mqbf5YE/yc0lSbZxKsPVlDRnogocsF9ppkCxxLeyj9CYpKlBWTrT3JTWPNt0OKRKzE0lgvdKpVMS
+OO7zSW1xkX5jtqumX8OkhPhPYlG++MXs2ziS4wblCJEMxChBVfvLWokVfnHoNb9Ncgk9vjo4UFt3
+MRuNs8ckRZqnrG0AFFoEt7oT61EKmEFBIk5lYYeBQVCmeVyJ3hlKV9Uu5l0cUyx+mM0aBhakaHPQ
+NAQTXKFx01p8VdteZOE3hzBWBOURtCmAEvF5OYiiAhF8J2a3iLd48soKqDirCmTCv2ZdlYTBoSUe
+h10aUAsgEsxBu24LUTi4S8sCAwEAAaNjMGEwDgYDVR0PAQH/BAQDAgGGMA8GA1UdEwEB/wQFMAMB
+Af8wHQYDVR0OBBYEFLE+w2kD+L9HAdSYJhoIAu9jZCvDMB8GA1UdIwQYMBaAFLE+w2kD+L9HAdSY
+JhoIAu9jZCvDMA0GCSqGSIb3DQEBBQUAA4IBAQAcGgaX3NecnzyIZgYIVyHbIUf4KmeqvxgydkAQ
+V8GK83rZEWWONfqe/EW1ntlMMUu4kehDLI6zeM7b41N5cdblIZQB2lWHmiRk9opmzN6cN82oNLFp
+myPInngiK3BD41VHMWEZ71jFhS9OMPagMRYjyOfiZRYzy78aG6A9+MpeizGLYAiJLQwGXFK3xPkK
+mNEVX58Svnw2Yzi9RKR/5CYrCsSXaQ3pjOLAEFe4yHYSkVXySGnYvCoCWw9E1CAx2/S6cCZdkGCe
+vEsXCS+0yx5DaMkHJ8HSXPfqIbloEpw8nL+e/IBcm2PN7EeqJSdnoDfzAIJ9VNep+OkuE6N36B9K
+-----END CERTIFICATE-----
+
+Certplus Class 2 Primary CA
+===========================
+-----BEGIN CERTIFICATE-----
+MIIDkjCCAnqgAwIBAgIRAIW9S/PY2uNp9pTXX8OlRCMwDQYJKoZIhvcNAQEFBQAwPTELMAkGA1UE
+BhMCRlIxETAPBgNVBAoTCENlcnRwbHVzMRswGQYDVQQDExJDbGFzcyAyIFByaW1hcnkgQ0EwHhcN
+OTkwNzA3MTcwNTAwWhcNMTkwNzA2MjM1OTU5WjA9MQswCQYDVQQGEwJGUjERMA8GA1UEChMIQ2Vy
+dHBsdXMxGzAZBgNVBAMTEkNsYXNzIDIgUHJpbWFyeSBDQTCCASIwDQYJKoZIhvcNAQEBBQADggEP
+ADCCAQoCggEBANxQltAS+DXSCHh6tlJw/W/uz7kRy1134ezpfgSN1sxvc0NXYKwzCkTsA18cgCSR
+5aiRVhKC9+Ar9NuuYS6JEI1rbLqzAr3VNsVINyPi8Fo3UjMXEuLRYE2+L0ER4/YXJQyLkcAbmXuZ
+Vg2v7tK8R1fjeUl7NIknJITesezpWE7+Tt9avkGtrAjFGA7v0lPubNCdEgETjdyAYveVqUSISnFO
+YFWe2yMZeVYHDD9jC1yw4r5+FfyUM1hBOHTE4Y+L3yasH7WLO7dDWWuwJKZtkIvEcupdM5i3y95e
+e++U8Rs+yskhwcWYAqqi9lt3m/V+llU0HGdpwPFC40es/CgcZlUCAwEAAaOBjDCBiTAPBgNVHRME
+CDAGAQH/AgEKMAsGA1UdDwQEAwIBBjAdBgNVHQ4EFgQU43Mt38sOKAze3bOkynm4jrvoMIkwEQYJ
+YIZIAYb4QgEBBAQDAgEGMDcGA1UdHwQwMC4wLKAqoCiGJmh0dHA6Ly93d3cuY2VydHBsdXMuY29t
+L0NSTC9jbGFzczIuY3JsMA0GCSqGSIb3DQEBBQUAA4IBAQCnVM+IRBnL39R/AN9WM2K191EBkOvD
+P9GIROkkXe/nFL0gt5o8AP5tn9uQ3Nf0YtaLcF3n5QRIqWh8yfFC82x/xXp8HVGIutIKPidd3i1R
+TtMTZGnkLuPT55sJmabglZvOGtd/vjzOUrMRFcEPF80Du5wlFbqidon8BvEY0JNLDnyCt6X09l/+
+7UCmnYR0ObncHoUW2ikbhiMAybuJfm6AiB4vFLQDJKgybwOaRywwvlbGp0ICcBvqQNi6BQNwB6SW
+//1IMwrh3KWBkJtN3X3n57LNXMhqlfil9o3EXXgIvnsG1knPGTZQIy4I5p4FTUcY1Rbpsda2ENW7
+l7+ijrRU
+-----END CERTIFICATE-----
+
+DST Root CA X3
+==============
+-----BEGIN CERTIFICATE-----
+MIIDSjCCAjKgAwIBAgIQRK+wgNajJ7qJMDmGLvhAazANBgkqhkiG9w0BAQUFADA/MSQwIgYDVQQK
+ExtEaWdpdGFsIFNpZ25hdHVyZSBUcnVzdCBDby4xFzAVBgNVBAMTDkRTVCBSb290IENBIFgzMB4X
+DTAwMDkzMDIxMTIxOVoXDTIxMDkzMDE0MDExNVowPzEkMCIGA1UEChMbRGlnaXRhbCBTaWduYXR1
+cmUgVHJ1c3QgQ28uMRcwFQYDVQQDEw5EU1QgUm9vdCBDQSBYMzCCASIwDQYJKoZIhvcNAQEBBQAD
+ggEPADCCAQoCggEBAN+v6ZdQCINXtMxiZfaQguzH0yxrMMpb7NnDfcdAwRgUi+DoM3ZJKuM/IUmT
+rE4Orz5Iy2Xu/NMhD2XSKtkyj4zl93ewEnu1lcCJo6m67XMuegwGMoOifooUMM0RoOEqOLl5CjH9
+UL2AZd+3UWODyOKIYepLYYHsUmu5ouJLGiifSKOeDNoJjj4XLh7dIN9bxiqKqy69cK3FCxolkHRy
+xXtqqzTWMIn/5WgTe1QLyNau7Fqckh49ZLOMxt+/yUFw7BZy1SbsOFU5Q9D8/RhcQPGX69Wam40d
+utolucbY38EVAjqr2m7xPi71XAicPNaDaeQQmxkqtilX4+U9m5/wAl0CAwEAAaNCMEAwDwYDVR0T
+AQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwHQYDVR0OBBYEFMSnsaR7LHH62+FLkHX/xBVghYkQ
+MA0GCSqGSIb3DQEBBQUAA4IBAQCjGiybFwBcqR7uKGY3Or+Dxz9LwwmglSBd49lZRNI+DT69ikug
+dB/OEIKcdBodfpga3csTS7MgROSR6cz8faXbauX+5v3gTt23ADq1cEmv8uXrAvHRAosZy5Q6XkjE
+GB5YGV8eAlrwDPGxrancWYaLbumR9YbK+rlmM6pZW87ipxZzR8srzJmwN0jP41ZL9c8PDHIyh8bw
+RLtTcm1D9SZImlJnt1ir/md2cXjbDaJWFBM5JDGFoqgCWjBH4d1QB7wCCZAA62RjYJsWvIjJEubS
+fZGL+T0yjWW06XyxV3bqxbYoOb8VZRzI9neWagqNdwvYkQsEjgfbKbYK7p2CNTUQ
+-----END CERTIFICATE-----
+
+DST ACES CA X6
+==============
+-----BEGIN CERTIFICATE-----
+MIIECTCCAvGgAwIBAgIQDV6ZCtadt3js2AdWO4YV2TANBgkqhkiG9w0BAQUFADBbMQswCQYDVQQG
+EwJVUzEgMB4GA1UEChMXRGlnaXRhbCBTaWduYXR1cmUgVHJ1c3QxETAPBgNVBAsTCERTVCBBQ0VT
+MRcwFQYDVQQDEw5EU1QgQUNFUyBDQSBYNjAeFw0wMzExMjAyMTE5NThaFw0xNzExMjAyMTE5NTha
+MFsxCzAJBgNVBAYTAlVTMSAwHgYDVQQKExdEaWdpdGFsIFNpZ25hdHVyZSBUcnVzdDERMA8GA1UE
+CxMIRFNUIEFDRVMxFzAVBgNVBAMTDkRTVCBBQ0VTIENBIFg2MIIBIjANBgkqhkiG9w0BAQEFAAOC
+AQ8AMIIBCgKCAQEAuT31LMmU3HWKlV1j6IR3dma5WZFcRt2SPp/5DgO0PWGSvSMmtWPuktKe1jzI
+DZBfZIGxqAgNTNj50wUoUrQBJcWVHAx+PhCEdc/BGZFjz+iokYi5Q1K7gLFViYsx+tC3dr5BPTCa
+pCIlF3PoHuLTrCq9Wzgh1SpL11V94zpVvddtawJXa+ZHfAjIgrrep4c9oW24MFbCswKBXy314pow
+GCi4ZtPLAZZv6opFVdbgnf9nKxcCpk4aahELfrd755jWjHZvwTvbUJN+5dCOHze4vbrGn2zpfDPy
+MjwmR/onJALJfh1biEITajV8fTXpLmaRcpPVMibEdPVTo7NdmvYJywIDAQABo4HIMIHFMA8GA1Ud
+EwEB/wQFMAMBAf8wDgYDVR0PAQH/BAQDAgHGMB8GA1UdEQQYMBaBFHBraS1vcHNAdHJ1c3Rkc3Qu
+Y29tMGIGA1UdIARbMFkwVwYKYIZIAWUDAgEBATBJMEcGCCsGAQUFBwIBFjtodHRwOi8vd3d3LnRy
+dXN0ZHN0LmNvbS9jZXJ0aWZpY2F0ZXMvcG9saWN5L0FDRVMtaW5kZXguaHRtbDAdBgNVHQ4EFgQU
+CXIGThhDD+XWzMNqizF7eI+og7gwDQYJKoZIhvcNAQEFBQADggEBAKPYjtay284F5zLNAdMEA+V2
+5FYrnJmQ6AgwbN99Pe7lv7UkQIRJ4dEorsTCOlMwiPH1d25Ryvr/ma8kXxug/fKshMrfqfBfBC6t
+Fr8hlxCBPeP/h40y3JTlR4peahPJlJU90u7INJXQgNStMgiAVDzgvVJT11J8smk/f3rPanTK+gQq
+nExaBqXpIK1FZg9p8d2/6eMyi/rgwYZNcjwu2JN4Cir42NInPRmJX1p7ijvMDNpRrscL9yuwNwXs
+vFcj4jjSm2jzVhKIT0J8uDHEtdvkyCE06UgRNe76x5JXxZ805Mf29w4LTJxoeHtxMcfrHuBnQfO3
+oKfN5XozNmr6mis=
+-----END CERTIFICATE-----
+
+TURKTRUST Certificate Services Provider Root 1
+==============================================
+-----BEGIN CERTIFICATE-----
+MIID+zCCAuOgAwIBAgIBATANBgkqhkiG9w0BAQUFADCBtzE/MD0GA1UEAww2VMOcUktUUlVTVCBF
+bGVrdHJvbmlrIFNlcnRpZmlrYSBIaXptZXQgU2HEn2xhecSxY8Sxc8SxMQswCQYDVQQGDAJUUjEP
+MA0GA1UEBwwGQU5LQVJBMVYwVAYDVQQKDE0oYykgMjAwNSBUw5xSS1RSVVNUIEJpbGdpIMSwbGV0
+acWfaW0gdmUgQmlsacWfaW0gR8O8dmVubGnEn2kgSGl6bWV0bGVyaSBBLsWeLjAeFw0wNTA1MTMx
+MDI3MTdaFw0xNTAzMjIxMDI3MTdaMIG3MT8wPQYDVQQDDDZUw5xSS1RSVVNUIEVsZWt0cm9uaWsg
+U2VydGlmaWthIEhpem1ldCBTYcSfbGF5xLFjxLFzxLExCzAJBgNVBAYMAlRSMQ8wDQYDVQQHDAZB
+TktBUkExVjBUBgNVBAoMTShjKSAyMDA1IFTDnFJLVFJVU1QgQmlsZ2kgxLBsZXRpxZ9pbSB2ZSBC
+aWxpxZ9pbSBHw7x2ZW5sacSfaSBIaXptZXRsZXJpIEEuxZ4uMIIBIjANBgkqhkiG9w0BAQEFAAOC
+AQ8AMIIBCgKCAQEAylIF1mMD2Bxf3dJ7XfIMYGFbazt0K3gNfUW9InTojAPBxhEqPZW8qZSwu5GX
+yGl8hMW0kWxsE2qkVa2kheiVfrMArwDCBRj1cJ02i67L5BuBf5OI+2pVu32Fks66WJ/bMsW9Xe8i
+Si9BB35JYbOG7E6mQW6EvAPs9TscyB/C7qju6hJKjRTP8wrgUDn5CDX4EVmt5yLqS8oUBt5CurKZ
+8y1UiBAG6uEaPj1nH/vO+3yC6BFdSsG5FOpU2WabfIl9BJpiyelSPJ6c79L1JuTm5Rh8i27fbMx4
+W09ysstcP4wFjdFMjK2Sx+F4f2VsSQZQLJ4ywtdKxnWKWU51b0dewQIDAQABoxAwDjAMBgNVHRME
+BTADAQH/MA0GCSqGSIb3DQEBBQUAA4IBAQAV9VX/N5aAWSGk/KEVTCD21F/aAyT8z5Aa9CEKmu46
+sWrv7/hg0Uw2ZkUd82YCdAR7kjCo3gp2D++Vbr3JN+YaDayJSFvMgzbC9UZcWYJWtNX+I7TYVBxE
+q8Sn5RTOPEFhfEPmzcSBCYsk+1Ql1haolgxnB2+zUEfjHCQo3SqYpGH+2+oSN7wBGjSFvW5P55Fy
+B0SFHljKVETd96y5y4khctuPwGkplyqjrhgjlxxBKot8KsF8kOipKMDTkcatKIdAaLX/7KfS0zgY
+nNN9aV3wxqUeJBujR/xpB2jn5Jq07Q+hh4cCzofSSE7hvP/L8XKSRGQDJereW26fyfJOrN3H
+-----END CERTIFICATE-----
+
+TURKTRUST Certificate Services Provider Root 2
+==============================================
+-----BEGIN CERTIFICATE-----
+MIIEPDCCAySgAwIBAgIBATANBgkqhkiG9w0BAQUFADCBvjE/MD0GA1UEAww2VMOcUktUUlVTVCBF
+bGVrdHJvbmlrIFNlcnRpZmlrYSBIaXptZXQgU2HEn2xhecSxY8Sxc8SxMQswCQYDVQQGEwJUUjEP
+MA0GA1UEBwwGQW5rYXJhMV0wWwYDVQQKDFRUw5xSS1RSVVNUIEJpbGdpIMSwbGV0acWfaW0gdmUg
+QmlsacWfaW0gR8O8dmVubGnEn2kgSGl6bWV0bGVyaSBBLsWeLiAoYykgS2FzxLFtIDIwMDUwHhcN
+MDUxMTA3MTAwNzU3WhcNMTUwOTE2MTAwNzU3WjCBvjE/MD0GA1UEAww2VMOcUktUUlVTVCBFbGVr
+dHJvbmlrIFNlcnRpZmlrYSBIaXptZXQgU2HEn2xhecSxY8Sxc8SxMQswCQYDVQQGEwJUUjEPMA0G
+A1UEBwwGQW5rYXJhMV0wWwYDVQQKDFRUw5xSS1RSVVNUIEJpbGdpIMSwbGV0acWfaW0gdmUgQmls
+acWfaW0gR8O8dmVubGnEn2kgSGl6bWV0bGVyaSBBLsWeLiAoYykgS2FzxLFtIDIwMDUwggEiMA0G
+CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCpNn7DkUNMwxmYCMjHWHtPFoylzkkBH3MOrHUTpvqe
+LCDe2JAOCtFp0if7qnefJ1Il4std2NiDUBd9irWCPwSOtNXwSadktx4uXyCcUHVPr+G1QRT0mJKI
+x+XlZEdhR3n9wFHxwZnn3M5q+6+1ATDcRhzviuyV79z/rxAc653YsKpqhRgNF8k+v/Gb0AmJQv2g
+QrSdiVFVKc8bcLyEVK3BEx+Y9C52YItdP5qtygy/p1Zbj3e41Z55SZI/4PGXJHpsmxcPbe9TmJEr
+5A++WXkHeLuXlfSfadRYhwqp48y2WBmfJiGxxFmNskF1wK1pzpwACPI2/z7woQ8arBT9pmAPAgMB
+AAGjQzBBMB0GA1UdDgQWBBTZN7NOBf3Zz58SFq62iS/rJTqIHDAPBgNVHQ8BAf8EBQMDBwYAMA8G
+A1UdEwEB/wQFMAMBAf8wDQYJKoZIhvcNAQEFBQADggEBAHJglrfJ3NgpXiOFX7KzLXb7iNcX/ntt
+Rbj2hWyfIvwqECLsqrkw9qtY1jkQMZkpAL2JZkH7dN6RwRgLn7Vhy506vvWolKMiVW4XSf/SKfE4
+Jl3vpao6+XF75tpYHdN0wgH6PmlYX63LaL4ULptswLbcoCb6dxriJNoaN+BnrdFzgw2lGh1uEpJ+
+hGIAF728JRhX8tepb1mIvDS3LoV4nZbcFMMsilKbloxSZj2GFotHuFEJjOp9zYhys2AzsfAKRO8P
+9Qk3iCQOLGsgOqL6EfJANZxEaGM7rDNvY7wsu/LSy3Z9fYjYHcgFHW68lKlmjHdxx/qR+i9Rnuk5
+UrbnBEI=
+-----END CERTIFICATE-----
+
+SwissSign Platinum CA - G2
+==========================
+-----BEGIN CERTIFICATE-----
+MIIFwTCCA6mgAwIBAgIITrIAZwwDXU8wDQYJKoZIhvcNAQEFBQAwSTELMAkGA1UEBhMCQ0gxFTAT
+BgNVBAoTDFN3aXNzU2lnbiBBRzEjMCEGA1UEAxMaU3dpc3NTaWduIFBsYXRpbnVtIENBIC0gRzIw
+HhcNMDYxMDI1MDgzNjAwWhcNMzYxMDI1MDgzNjAwWjBJMQswCQYDVQQGEwJDSDEVMBMGA1UEChMM
+U3dpc3NTaWduIEFHMSMwIQYDVQQDExpTd2lzc1NpZ24gUGxhdGludW0gQ0EgLSBHMjCCAiIwDQYJ
+KoZIhvcNAQEBBQADggIPADCCAgoCggIBAMrfogLi2vj8Bxax3mCq3pZcZB/HL37PZ/pEQtZ2Y5Wu
+669yIIpFR4ZieIbWIDkm9K6j/SPnpZy1IiEZtzeTIsBQnIJ71NUERFzLtMKfkr4k2HtnIuJpX+UF
+eNSH2XFwMyVTtIc7KZAoNppVRDBopIOXfw0enHb/FZ1glwCNioUD7IC+6ixuEFGSzH7VozPY1kne
+WCqv9hbrS3uQMpe5up1Y8fhXSQQeol0GcN1x2/ndi5objM89o03Oy3z2u5yg+gnOI2Ky6Q0f4nIo
+j5+saCB9bzuohTEJfwvH6GXp43gOCWcwizSC+13gzJ2BbWLuCB4ELE6b7P6pT1/9aXjvCR+htL/6
+8++QHkwFix7qepF6w9fl+zC8bBsQWJj3Gl/QKTIDE0ZNYWqFTFJ0LwYfexHihJfGmfNtf9dng34T
+aNhxKFrYzt3oEBSa/m0jh26OWnA81Y0JAKeqvLAxN23IhBQeW71FYyBrS3SMvds6DsHPWhaPpZjy
+domyExI7C3d3rLvlPClKknLKYRorXkzig3R3+jVIeoVNjZpTxN94ypeRSCtFKwH3HBqi7Ri6Cr2D
++m+8jVeTO9TUps4e8aCxzqv9KyiaTxvXw3LbpMS/XUz13XuWae5ogObnmLo2t/5u7Su9IPhlGdpV
+CX4l3P5hYnL5fhgC72O00Puv5TtjjGePAgMBAAGjgawwgakwDgYDVR0PAQH/BAQDAgEGMA8GA1Ud
+EwEB/wQFMAMBAf8wHQYDVR0OBBYEFFCvzAeHFUdvOMW0ZdHelarp35zMMB8GA1UdIwQYMBaAFFCv
+zAeHFUdvOMW0ZdHelarp35zMMEYGA1UdIAQ/MD0wOwYJYIV0AVkBAQEBMC4wLAYIKwYBBQUHAgEW
+IGh0dHA6Ly9yZXBvc2l0b3J5LnN3aXNzc2lnbi5jb20vMA0GCSqGSIb3DQEBBQUAA4ICAQAIhab1
+Fgz8RBrBY+D5VUYI/HAcQiiWjrfFwUF1TglxeeVtlspLpYhg0DB0uMoI3LQwnkAHFmtllXcBrqS3
+NQuB2nEVqXQXOHtYyvkv+8Bldo1bAbl93oI9ZLi+FHSjClTTLJUYFzX1UWs/j6KWYTl4a0vlpqD4
+U99REJNi54Av4tHgvI42Rncz7Lj7jposiU0xEQ8mngS7twSNC/K5/FqdOxa3L8iYq/6KUFkuozv8
+KV2LwUvJ4ooTHbG/u0IdUt1O2BReEMYxB+9xJ/cbOQncguqLs5WGXv312l0xpuAxtpTmREl0xRbl
+9x8DYSjFyMsSoEJL+WuICI20MhjzdZ/EfwBPBZWcoxcCw7NTm6ogOSkrZvqdr16zktK1puEa+S1B
+aYEUtLS17Yk9zvupnTVCRLEcFHOBzyoBNZox1S2PbYTfgE1X4z/FhHXaicYwu+uPyyIIoK6q8QNs
+OktNCaUOcsZWayFCTiMlFGiudgp8DAdwZPmaL/YFOSbGDI8Zf0NebvRbFS/bYV3mZy8/CJT5YLSY
+Mdp08YSTcU1f+2BY0fvEwW2JorsgH51xkcsymxM9Pn2SUjWskpSi0xjCfMfqr3YFFt1nJ8J+HAci
+IfNAChs0B0QTwoRqjt8ZWr9/6x3iGjjRXK9HkmuAtTClyY3YqzGBH9/CZjfTk6mFhnll0g==
+-----END CERTIFICATE-----
+
+SwissSign Gold CA - G2
+======================
+-----BEGIN CERTIFICATE-----
+MIIFujCCA6KgAwIBAgIJALtAHEP1Xk+wMA0GCSqGSIb3DQEBBQUAMEUxCzAJBgNVBAYTAkNIMRUw
+EwYDVQQKEwxTd2lzc1NpZ24gQUcxHzAdBgNVBAMTFlN3aXNzU2lnbiBHb2xkIENBIC0gRzIwHhcN
+MDYxMDI1MDgzMDM1WhcNMzYxMDI1MDgzMDM1WjBFMQswCQYDVQQGEwJDSDEVMBMGA1UEChMMU3dp
+c3NTaWduIEFHMR8wHQYDVQQDExZTd2lzc1NpZ24gR29sZCBDQSAtIEcyMIICIjANBgkqhkiG9w0B
+AQEFAAOCAg8AMIICCgKCAgEAr+TufoskDhJuqVAtFkQ7kpJcyrhdhJJCEyq8ZVeCQD5XJM1QiyUq
+t2/876LQwB8CJEoTlo8jE+YoWACjR8cGp4QjK7u9lit/VcyLwVcfDmJlD909Vopz2q5+bbqBHH5C
+jCA12UNNhPqE21Is8w4ndwtrvxEvcnifLtg+5hg3Wipy+dpikJKVyh+c6bM8K8vzARO/Ws/BtQpg
+vd21mWRTuKCWs2/iJneRjOBiEAKfNA+k1ZIzUd6+jbqEemA8atufK+ze3gE/bk3lUIbLtK/tREDF
+ylqM2tIrfKjuvqblCqoOpd8FUrdVxyJdMmqXl2MT28nbeTZ7hTpKxVKJ+STnnXepgv9VHKVxaSvR
+AiTysybUa9oEVeXBCsdtMDeQKuSeFDNeFhdVxVu1yzSJkvGdJo+hB9TGsnhQ2wwMC3wLjEHXuend
+jIj3o02yMszYF9rNt85mndT9Xv+9lz4pded+p2JYryU0pUHHPbwNUMoDAw8IWh+Vc3hiv69yFGkO
+peUDDniOJihC8AcLYiAQZzlG+qkDzAQ4embvIIO1jEpWjpEA/I5cgt6IoMPiaG59je883WX0XaxR
+7ySArqpWl2/5rX3aYT+YdzylkbYcjCbaZaIJbcHiVOO5ykxMgI93e2CaHt+28kgeDrpOVG2Y4OGi
+GqJ3UM/EY5LsRxmd6+ZrzsECAwEAAaOBrDCBqTAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUw
+AwEB/zAdBgNVHQ4EFgQUWyV7lqRlUX64OfPAeGZe6Drn8O4wHwYDVR0jBBgwFoAUWyV7lqRlUX64
+OfPAeGZe6Drn8O4wRgYDVR0gBD8wPTA7BglghXQBWQECAQEwLjAsBggrBgEFBQcCARYgaHR0cDov
+L3JlcG9zaXRvcnkuc3dpc3NzaWduLmNvbS8wDQYJKoZIhvcNAQEFBQADggIBACe645R88a7A3hfm
+5djV9VSwg/S7zV4Fe0+fdWavPOhWfvxyeDgD2StiGwC5+OlgzczOUYrHUDFu4Up+GC9pWbY9ZIEr
+44OE5iKHjn3g7gKZYbge9LgriBIWhMIxkziWMaa5O1M/wySTVltpkuzFwbs4AOPsF6m43Md8AYOf
+Mke6UiI0HTJ6CVanfCU2qT1L2sCCbwq7EsiHSycR+R4tx5M/nttfJmtS2S6K8RTGRI0Vqbe/vd6m
+Gu6uLftIdxf+u+yvGPUqUfA5hJeVbG4bwyvEdGB5JbAKJ9/fXtI5z0V9QkvfsywexcZdylU6oJxp
+mo/a77KwPJ+HbBIrZXAVUjEaJM9vMSNQH4xPjyPDdEFjHFWoFN0+4FFQz/EbMFYOkrCChdiDyyJk
+vC24JdVUorgG6q2SpCSgwYa1ShNqR88uC1aVVMvOmttqtKay20EIhid392qgQmwLOM7XdVAyksLf
+KzAiSNDVQTglXaTpXZ/GlHXQRf0wl0OPkKsKx4ZzYEppLd6leNcG2mqeSz53OiATIgHQv2ieY2Br
+NU0LbbqhPcCT4H8js1WtciVORvnSFu+wZMEBnunKoGqYDs/YYPIvSbjkQuE4NRb0yG5P94FW6Lqj
+viOvrv1vA+ACOzB2+httQc8Bsem4yWb02ybzOqR08kkkW8mw0FfB+j564ZfJ
+-----END CERTIFICATE-----
+
+SwissSign Silver CA - G2
+========================
+-----BEGIN CERTIFICATE-----
+MIIFvTCCA6WgAwIBAgIITxvUL1S7L0swDQYJKoZIhvcNAQEFBQAwRzELMAkGA1UEBhMCQ0gxFTAT
+BgNVBAoTDFN3aXNzU2lnbiBBRzEhMB8GA1UEAxMYU3dpc3NTaWduIFNpbHZlciBDQSAtIEcyMB4X
+DTA2MTAyNTA4MzI0NloXDTM2MTAyNTA4MzI0NlowRzELMAkGA1UEBhMCQ0gxFTATBgNVBAoTDFN3
+aXNzU2lnbiBBRzEhMB8GA1UEAxMYU3dpc3NTaWduIFNpbHZlciBDQSAtIEcyMIICIjANBgkqhkiG
+9w0BAQEFAAOCAg8AMIICCgKCAgEAxPGHf9N4Mfc4yfjDmUO8x/e8N+dOcbpLj6VzHVxumK4DV644
+N0MvFz0fyM5oEMF4rhkDKxD6LHmD9ui5aLlV8gREpzn5/ASLHvGiTSf5YXu6t+WiE7brYT7QbNHm
++/pe7R20nqA1W6GSy/BJkv6FCgU+5tkL4k+73JU3/JHpMjUi0R86TieFnbAVlDLaYQ1HTWBCrpJH
+6INaUFjpiou5XaHc3ZlKHzZnu0jkg7Y360g6rw9njxcH6ATK72oxh9TAtvmUcXtnZLi2kUpCe2Uu
+MGoM9ZDulebyzYLs2aFK7PayS+VFheZteJMELpyCbTapxDFkH4aDCyr0NQp4yVXPQbBH6TCfmb5h
+qAaEuSh6XzjZG6k4sIN/c8HDO0gqgg8hm7jMqDXDhBuDsz6+pJVpATqJAHgE2cn0mRmrVn5bi4Y5
+FZGkECwJMoBgs5PAKrYYC51+jUnyEEp/+dVGLxmSo5mnJqy7jDzmDrxHB9xzUfFwZC8I+bRHHTBs
+ROopN4WSaGa8gzj+ezku01DwH/teYLappvonQfGbGHLy9YR0SslnxFSuSGTfjNFusB3hB48IHpmc
+celM2KX3RxIfdNFRnobzwqIjQAtz20um53MGjMGg6cFZrEb65i/4z3GcRm25xBWNOHkDRUjvxF3X
+CO6HOSKGsg0PWEP3calILv3q1h8CAwEAAaOBrDCBqTAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/
+BAUwAwEB/zAdBgNVHQ4EFgQUF6DNweRBtjpbO8tFnb0cwpj6hlgwHwYDVR0jBBgwFoAUF6DNweRB
+tjpbO8tFnb0cwpj6hlgwRgYDVR0gBD8wPTA7BglghXQBWQEDAQEwLjAsBggrBgEFBQcCARYgaHR0
+cDovL3JlcG9zaXRvcnkuc3dpc3NzaWduLmNvbS8wDQYJKoZIhvcNAQEFBQADggIBAHPGgeAn0i0P
+4JUw4ppBf1AsX19iYamGamkYDHRJ1l2E6kFSGG9YrVBWIGrGvShpWJHckRE1qTodvBqlYJ7YH39F
+kWnZfrt4csEGDyrOj4VwYaygzQu4OSlWhDJOhrs9xCrZ1x9y7v5RoSJBsXECYxqCsGKrXlcSH9/L
+3XWgwF15kIwb4FDm3jH+mHtwX6WQ2K34ArZv02DdQEsixT2tOnqfGhpHkXkzuoLcMmkDlm4fS/Bx
+/uNncqCxv1yL5PqZIseEuRuNI5c/7SXgz2W79WEE790eslpBIlqhn10s6FvJbakMDHiqYMZWjwFa
+DGi8aRl5xB9+lwW/xekkUV7U1UtT7dkjWjYDZaPBA61BMPNGG4WQr2W11bHkFlt4dR2Xem1ZqSqP
+e97Dh4kQmUlzeMg9vVE1dCrV8X5pGyq7O70luJpaPXJhkGaH7gzWTdQRdAtq/gsD/KNVV4n+Ssuu
+WxcFyPKNIzFTONItaj+CuY0IavdeQXRuwxF+B6wpYJE/OMpXEA29MC/HpeZBoNquBYeaoKRlbEwJ
+DIm6uNO5wJOKMPqN5ZprFQFOZ6raYlY+hAhm0sQ2fac+EPyI4NSA5QC9qvNOBqN6avlicuMJT+ub
+DgEj8Z+7fNzcbBGXJbLytGMU0gYqZ4yD9c7qB9iaah7s5Aq7KkzrCWA5zspi2C5u
+-----END CERTIFICATE-----
+
+GeoTrust Primary Certification Authority
+========================================
+-----BEGIN CERTIFICATE-----
+MIIDfDCCAmSgAwIBAgIQGKy1av1pthU6Y2yv2vrEoTANBgkqhkiG9w0BAQUFADBYMQswCQYDVQQG
+EwJVUzEWMBQGA1UEChMNR2VvVHJ1c3QgSW5jLjExMC8GA1UEAxMoR2VvVHJ1c3QgUHJpbWFyeSBD
+ZXJ0aWZpY2F0aW9uIEF1dGhvcml0eTAeFw0wNjExMjcwMDAwMDBaFw0zNjA3MTYyMzU5NTlaMFgx
+CzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1HZW9UcnVzdCBJbmMuMTEwLwYDVQQDEyhHZW9UcnVzdCBQ
+cmltYXJ5IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIB
+CgKCAQEAvrgVe//UfH1nrYNke8hCUy3f9oQIIGHWAVlqnEQRr+92/ZV+zmEwu3qDXwK9AWbK7hWN
+b6EwnL2hhZ6UOvNWiAAxz9juapYC2e0DjPt1befquFUWBRaa9OBesYjAZIVcFU2Ix7e64HXprQU9
+nceJSOC7KMgD4TCTZF5SwFlwIjVXiIrxlQqD17wxcwE07e9GceBrAqg1cmuXm2bgyxx5X9gaBGge
+RwLmnWDiNpcB3841kt++Z8dtd1k7j53WkBWUvEI0EME5+bEnPn7WinXFsq+W06Lem+SYvn3h6YGt
+tm/81w7a4DSwDRp35+MImO9Y+pyEtzavwt+s0vQQBnBxNQIDAQABo0IwQDAPBgNVHRMBAf8EBTAD
+AQH/MA4GA1UdDwEB/wQEAwIBBjAdBgNVHQ4EFgQULNVQQZcVi/CPNmFbSvtr2ZnJM5IwDQYJKoZI
+hvcNAQEFBQADggEBAFpwfyzdtzRP9YZRqSa+S7iq8XEN3GHHoOo0Hnp3DwQ16CePbJC/kRYkRj5K
+Ts4rFtULUh38H2eiAkUxT87z+gOneZ1TatnaYzr4gNfTmeGl4b7UVXGYNTq+k+qurUKykG/g/CFN
+NWMziUnWm07Kx+dOCQD32sfvmWKZd7aVIl6KoKv0uHiYyjgZmclynnjNS6yvGaBzEi38wkG6gZHa
+Floxt/m0cYASSJlyc1pZU8FjUjPtp8nSOQJw+uCxQmYpqptR7TBUIhRf2asdweSU8Pj1K/fqynhG
+1riR/aYNKxoUAT6A8EKglQdebc3MS6RFjasS6LPeWuWgfOgPIh1a6Vk=
+-----END CERTIFICATE-----
+
+thawte Primary Root CA
+======================
+-----BEGIN CERTIFICATE-----
+MIIEIDCCAwigAwIBAgIQNE7VVyDV7exJ9C/ON9srbTANBgkqhkiG9w0BAQUFADCBqTELMAkGA1UE
+BhMCVVMxFTATBgNVBAoTDHRoYXd0ZSwgSW5jLjEoMCYGA1UECxMfQ2VydGlmaWNhdGlvbiBTZXJ2
+aWNlcyBEaXZpc2lvbjE4MDYGA1UECxMvKGMpIDIwMDYgdGhhd3RlLCBJbmMuIC0gRm9yIGF1dGhv
+cml6ZWQgdXNlIG9ubHkxHzAdBgNVBAMTFnRoYXd0ZSBQcmltYXJ5IFJvb3QgQ0EwHhcNMDYxMTE3
+MDAwMDAwWhcNMzYwNzE2MjM1OTU5WjCBqTELMAkGA1UEBhMCVVMxFTATBgNVBAoTDHRoYXd0ZSwg
+SW5jLjEoMCYGA1UECxMfQ2VydGlmaWNhdGlvbiBTZXJ2aWNlcyBEaXZpc2lvbjE4MDYGA1UECxMv
+KGMpIDIwMDYgdGhhd3RlLCBJbmMuIC0gRm9yIGF1dGhvcml6ZWQgdXNlIG9ubHkxHzAdBgNVBAMT
+FnRoYXd0ZSBQcmltYXJ5IFJvb3QgQ0EwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCs
+oPD7gFnUnMekz52hWXMJEEUMDSxuaPFsW0hoSVk3/AszGcJ3f8wQLZU0HObrTQmnHNK4yZc2AreJ
+1CRfBsDMRJSUjQJib+ta3RGNKJpchJAQeg29dGYvajig4tVUROsdB58Hum/u6f1OCyn1PoSgAfGc
+q/gcfomk6KHYcWUNo1F77rzSImANuVud37r8UVsLr5iy6S7pBOhih94ryNdOwUxkHt3Ph1i6Sk/K
+aAcdHJ1KxtUvkcx8cXIcxcBn6zL9yZJclNqFwJu/U30rCfSMnZEfl2pSy94JNqR32HuHUETVPm4p
+afs5SSYeCaWAe0At6+gnhcn+Yf1+5nyXHdWdAgMBAAGjQjBAMA8GA1UdEwEB/wQFMAMBAf8wDgYD
+VR0PAQH/BAQDAgEGMB0GA1UdDgQWBBR7W0XPr87Lev0xkhpqtvNG61dIUDANBgkqhkiG9w0BAQUF
+AAOCAQEAeRHAS7ORtvzw6WfUDW5FvlXok9LOAz/t2iWwHVfLHjp2oEzsUHboZHIMpKnxuIvW1oeE
+uzLlQRHAd9mzYJ3rG9XRbkREqaYB7FViHXe4XI5ISXycO1cRrK1zN44veFyQaEfZYGDm/Ac9IiAX
+xPcW6cTYcvnIc3zfFi8VqT79aie2oetaupgf1eNNZAqdE8hhuvU5HIe6uL17In/2/qxAeeWsEG89
+jxt5dovEN7MhGITlNgDrYyCZuen+MwS7QcjBAvlEYyCegc5C09Y/LHbTY5xZ3Y+m4Q6gLkH3LpVH
+z7z9M/P2C2F+fpErgUfCJzDupxBdN49cOSvkBPB7jVaMaA==
+-----END CERTIFICATE-----
+
+VeriSign Class 3 Public Primary Certification Authority - G5
+============================================================
+-----BEGIN CERTIFICATE-----
+MIIE0zCCA7ugAwIBAgIQGNrRniZ96LtKIVjNzGs7SjANBgkqhkiG9w0BAQUFADCByjELMAkGA1UE
+BhMCVVMxFzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMR8wHQYDVQQLExZWZXJpU2lnbiBUcnVzdCBO
+ZXR3b3JrMTowOAYDVQQLEzEoYykgMjAwNiBWZXJpU2lnbiwgSW5jLiAtIEZvciBhdXRob3JpemVk
+IHVzZSBvbmx5MUUwQwYDVQQDEzxWZXJpU2lnbiBDbGFzcyAzIFB1YmxpYyBQcmltYXJ5IENlcnRp
+ZmljYXRpb24gQXV0aG9yaXR5IC0gRzUwHhcNMDYxMTA4MDAwMDAwWhcNMzYwNzE2MjM1OTU5WjCB
+yjELMAkGA1UEBhMCVVMxFzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMR8wHQYDVQQLExZWZXJpU2ln
+biBUcnVzdCBOZXR3b3JrMTowOAYDVQQLEzEoYykgMjAwNiBWZXJpU2lnbiwgSW5jLiAtIEZvciBh
+dXRob3JpemVkIHVzZSBvbmx5MUUwQwYDVQQDEzxWZXJpU2lnbiBDbGFzcyAzIFB1YmxpYyBQcmlt
+YXJ5IENlcnRpZmljYXRpb24gQXV0aG9yaXR5IC0gRzUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAw
+ggEKAoIBAQCvJAgIKXo1nmAMqudLO07cfLw8RRy7K+D+KQL5VwijZIUVJ/XxrcgxiV0i6CqqpkKz
+j/i5Vbext0uz/o9+B1fs70PbZmIVYc9gDaTY3vjgw2IIPVQT60nKWVSFJuUrjxuf6/WhkcIzSdhD
+Y2pSS9KP6HBRTdGJaXvHcPaz3BJ023tdS1bTlr8Vd6Gw9KIl8q8ckmcY5fQGBO+QueQA5N06tRn/
+Arr0PO7gi+s3i+z016zy9vA9r911kTMZHRxAy3QkGSGT2RT+rCpSx4/VBEnkjWNHiDxpg8v+R70r
+fk/Fla4OndTRQ8Bnc+MUCH7lP59zuDMKz10/NIeWiu5T6CUVAgMBAAGjgbIwga8wDwYDVR0TAQH/
+BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwbQYIKwYBBQUHAQwEYTBfoV2gWzBZMFcwVRYJaW1hZ2Uv
+Z2lmMCEwHzAHBgUrDgMCGgQUj+XTGoasjY5rw8+AatRIGCx7GS4wJRYjaHR0cDovL2xvZ28udmVy
+aXNpZ24uY29tL3ZzbG9nby5naWYwHQYDVR0OBBYEFH/TZafC3ey78DAJ80M5+gKvMzEzMA0GCSqG
+SIb3DQEBBQUAA4IBAQCTJEowX2LP2BqYLz3q3JktvXf2pXkiOOzEp6B4Eq1iDkVwZMXnl2YtmAl+
+X6/WzChl8gGqCBpH3vn5fJJaCGkgDdk+bW48DW7Y5gaRQBi5+MHt39tBquCWIMnNZBU4gcmU7qKE
+KQsTb47bDN0lAtukixlE0kF6BWlKWE9gyn6CagsCqiUXObXbf+eEZSqVir2G3l6BFoMtEMze/aiC
+Km0oHw0LxOXnGiYZ4fQRbxC1lfznQgUy286dUV4otp6F01vvpX1FQHKOtw5rDgb7MzVIcbidJ4vE
+ZV8NhnacRHr2lVz2XTIIM6RUthg/aFzyQkqFOFSDX9HoLPKsEdao7WNq
+-----END CERTIFICATE-----
+
+SecureTrust CA
+==============
+-----BEGIN CERTIFICATE-----
+MIIDuDCCAqCgAwIBAgIQDPCOXAgWpa1Cf/DrJxhZ0DANBgkqhkiG9w0BAQUFADBIMQswCQYDVQQG
+EwJVUzEgMB4GA1UEChMXU2VjdXJlVHJ1c3QgQ29ycG9yYXRpb24xFzAVBgNVBAMTDlNlY3VyZVRy
+dXN0IENBMB4XDTA2MTEwNzE5MzExOFoXDTI5MTIzMTE5NDA1NVowSDELMAkGA1UEBhMCVVMxIDAe
+BgNVBAoTF1NlY3VyZVRydXN0IENvcnBvcmF0aW9uMRcwFQYDVQQDEw5TZWN1cmVUcnVzdCBDQTCC
+ASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKukgeWVzfX2FI7CT8rU4niVWJxB4Q2ZQCQX
+OZEzZum+4YOvYlyJ0fwkW2Gz4BERQRwdbvC4u/jep4G6pkjGnx29vo6pQT64lO0pGtSO0gMdA+9t
+DWccV9cGrcrI9f4Or2YlSASWC12juhbDCE/RRvgUXPLIXgGZbf2IzIaowW8xQmxSPmjL8xk037uH
+GFaAJsTQ3MBv396gwpEWoGQRS0S8Hvbn+mPeZqx2pHGj7DaUaHp3pLHnDi+BeuK1cobvomuL8A/b
+01k/unK8RCSc43Oz969XL0Imnal0ugBS8kvNU3xHCzaFDmapCJcWNFfBZveA4+1wVMeT4C4oFVmH
+ursCAwEAAaOBnTCBmjATBgkrBgEEAYI3FAIEBh4EAEMAQTALBgNVHQ8EBAMCAYYwDwYDVR0TAQH/
+BAUwAwEB/zAdBgNVHQ4EFgQUQjK2FvoE/f5dS3rD/fdMQB1aQ68wNAYDVR0fBC0wKzApoCegJYYj
+aHR0cDovL2NybC5zZWN1cmV0cnVzdC5jb20vU1RDQS5jcmwwEAYJKwYBBAGCNxUBBAMCAQAwDQYJ
+KoZIhvcNAQEFBQADggEBADDtT0rhWDpSclu1pqNlGKa7UTt36Z3q059c4EVlew3KW+JwULKUBRSu
+SceNQQcSc5R+DCMh/bwQf2AQWnL1mA6s7Ll/3XpvXdMc9P+IBWlCqQVxyLesJugutIxq/3HcuLHf
+mbx8IVQr5Fiiu1cprp6poxkmD5kuCLDv/WnPmRoJjeOnnyvJNjR7JLN4TJUXpAYmHrZkUjZfYGfZ
+nMUFdAvnZyPSCPyI6a6Lf+Ew9Dd+/cYy2i2eRDAwbO4H3tI0/NL/QPZL9GZGBlSm8jIKYyYwa5vR
+3ItHuuG51WLQoqD0ZwV4KWMabwTW+MZMo5qxN7SN5ShLHZ4swrhovO0C7jE=
+-----END CERTIFICATE-----
+
+Secure Global CA
+================
+-----BEGIN CERTIFICATE-----
+MIIDvDCCAqSgAwIBAgIQB1YipOjUiolN9BPI8PjqpTANBgkqhkiG9w0BAQUFADBKMQswCQYDVQQG
+EwJVUzEgMB4GA1UEChMXU2VjdXJlVHJ1c3QgQ29ycG9yYXRpb24xGTAXBgNVBAMTEFNlY3VyZSBH
+bG9iYWwgQ0EwHhcNMDYxMTA3MTk0MjI4WhcNMjkxMjMxMTk1MjA2WjBKMQswCQYDVQQGEwJVUzEg
+MB4GA1UEChMXU2VjdXJlVHJ1c3QgQ29ycG9yYXRpb24xGTAXBgNVBAMTEFNlY3VyZSBHbG9iYWwg
+Q0EwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCvNS7YrGxVaQZx5RNoJLNP2MwhR/jx
+YDiJiQPpvepeRlMJ3Fz1Wuj3RSoC6zFh1ykzTM7HfAo3fg+6MpjhHZevj8fcyTiW89sa/FHtaMbQ
+bqR8JNGuQsiWUGMu4P51/pinX0kuleM5M2SOHqRfkNJnPLLZ/kG5VacJjnIFHovdRIWCQtBJwB1g
+8NEXLJXr9qXBkqPFwqcIYA1gBBCWeZ4WNOaptvolRTnIHmX5k/Wq8VLcmZg9pYYaDDUz+kulBAYV
+HDGA76oYa8J719rO+TMg1fW9ajMtgQT7sFzUnKPiXB3jqUJ1XnvUd+85VLrJChgbEplJL4hL/VBi
+0XPnj3pDAgMBAAGjgZ0wgZowEwYJKwYBBAGCNxQCBAYeBABDAEEwCwYDVR0PBAQDAgGGMA8GA1Ud
+EwEB/wQFMAMBAf8wHQYDVR0OBBYEFK9EBMJBfkiD2045AuzshHrmzsmkMDQGA1UdHwQtMCswKaAn
+oCWGI2h0dHA6Ly9jcmwuc2VjdXJldHJ1c3QuY29tL1NHQ0EuY3JsMBAGCSsGAQQBgjcVAQQDAgEA
+MA0GCSqGSIb3DQEBBQUAA4IBAQBjGghAfaReUw132HquHw0LURYD7xh8yOOvaliTFGCRsoTciE6+
+OYo68+aCiV0BN7OrJKQVDpI1WkpEXk5X+nXOH0jOZvQ8QCaSmGwb7iRGDBezUqXbpZGRzzfTb+cn
+CDpOGR86p1hcF895P4vkp9MmI50mD1hp/Ed+stCNi5O/KU9DaXR2Z0vPB4zmAve14bRDtUstFJ/5
+3CYNv6ZHdAbYiNE6KTCEztI5gGIbqMdXSbxqVVFnFUq+NQfk1XWYN3kwFNspnWzFacxHVaIw98xc
+f8LDmBxrThaA63p4ZUWiABqvDA1VZDRIuJK58bRQKfJPIx/abKwfROHdI3hRW8cW
+-----END CERTIFICATE-----
+
+COMODO Certification Authority
+==============================
+-----BEGIN CERTIFICATE-----
+MIIEHTCCAwWgAwIBAgIQToEtioJl4AsC7j41AkblPTANBgkqhkiG9w0BAQUFADCBgTELMAkGA1UE
+BhMCR0IxGzAZBgNVBAgTEkdyZWF0ZXIgTWFuY2hlc3RlcjEQMA4GA1UEBxMHU2FsZm9yZDEaMBgG
+A1UEChMRQ09NT0RPIENBIExpbWl0ZWQxJzAlBgNVBAMTHkNPTU9ETyBDZXJ0aWZpY2F0aW9uIEF1
+dGhvcml0eTAeFw0wNjEyMDEwMDAwMDBaFw0yOTEyMzEyMzU5NTlaMIGBMQswCQYDVQQGEwJHQjEb
+MBkGA1UECBMSR3JlYXRlciBNYW5jaGVzdGVyMRAwDgYDVQQHEwdTYWxmb3JkMRowGAYDVQQKExFD
+T01PRE8gQ0EgTGltaXRlZDEnMCUGA1UEAxMeQ09NT0RPIENlcnRpZmljYXRpb24gQXV0aG9yaXR5
+MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA0ECLi3LjkRv3UcEbVASY06m/weaKXTuH
++7uIzg3jLz8GlvCiKVCZrts7oVewdFFxze1CkU1B/qnI2GqGd0S7WWaXUF601CxwRM/aN5VCaTww
+xHGzUvAhTaHYujl8HJ6jJJ3ygxaYqhZ8Q5sVW7euNJH+1GImGEaaP+vB+fGQV+useg2L23IwambV
+4EajcNxo2f8ESIl33rXp+2dtQem8Ob0y2WIC8bGoPW43nOIv4tOiJovGuFVDiOEjPqXSJDlqR6sA
+1KGzqSX+DT+nHbrTUcELpNqsOO9VUCQFZUaTNE8tja3G1CEZ0o7KBWFxB3NH5YoZEr0ETc5OnKVI
+rLsm9wIDAQABo4GOMIGLMB0GA1UdDgQWBBQLWOWLxkwVN6RAqTCpIb5HNlpW/zAOBgNVHQ8BAf8E
+BAMCAQYwDwYDVR0TAQH/BAUwAwEB/zBJBgNVHR8EQjBAMD6gPKA6hjhodHRwOi8vY3JsLmNvbW9k
+b2NhLmNvbS9DT01PRE9DZXJ0aWZpY2F0aW9uQXV0aG9yaXR5LmNybDANBgkqhkiG9w0BAQUFAAOC
+AQEAPpiem/Yb6dc5t3iuHXIYSdOH5EOC6z/JqvWote9VfCFSZfnVDeFs9D6Mk3ORLgLETgdxb8CP
+OGEIqB6BCsAvIC9Bi5HcSEW88cbeunZrM8gALTFGTO3nnc+IlP8zwFboJIYmuNg4ON8qa90SzMc/
+RxdMosIGlgnW2/4/PEZB31jiVg88O8EckzXZOFKs7sjsLjBOlDW0JB9LeGna8gI4zJVSk/BwJVmc
+IGfE7vmLV2H0knZ9P4SNVbfo5azV8fUZVqZa+5Acr5Pr5RzUZ5ddBA6+C4OmF4O5MBKgxTMVBbkN
++8cFduPYSo38NBejxiEovjBFMR7HeL5YYTisO+IBZQ==
+-----END CERTIFICATE-----
+
+Network Solutions Certificate Authority
+=======================================
+-----BEGIN CERTIFICATE-----
+MIID5jCCAs6gAwIBAgIQV8szb8JcFuZHFhfjkDFo4DANBgkqhkiG9w0BAQUFADBiMQswCQYDVQQG
+EwJVUzEhMB8GA1UEChMYTmV0d29yayBTb2x1dGlvbnMgTC5MLkMuMTAwLgYDVQQDEydOZXR3b3Jr
+IFNvbHV0aW9ucyBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkwHhcNMDYxMjAxMDAwMDAwWhcNMjkxMjMx
+MjM1OTU5WjBiMQswCQYDVQQGEwJVUzEhMB8GA1UEChMYTmV0d29yayBTb2x1dGlvbnMgTC5MLkMu
+MTAwLgYDVQQDEydOZXR3b3JrIFNvbHV0aW9ucyBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkwggEiMA0G
+CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDkvH6SMG3G2I4rC7xGzuAnlt7e+foS0zwzc7MEL7xx
+jOWftiJgPl9dzgn/ggwbmlFQGiaJ3dVhXRncEg8tCqJDXRfQNJIg6nPPOCwGJgl6cvf6UDL4wpPT
+aaIjzkGxzOTVHzbRijr4jGPiFFlp7Q3Tf2vouAPlT2rlmGNpSAW+Lv8ztumXWWn4Zxmuk2GWRBXT
+crA/vGp97Eh/jcOrqnErU2lBUzS1sLnFBgrEsEX1QV1uiUV7PTsmjHTC5dLRfbIR1PtYMiKagMnc
+/Qzpf14Dl847ABSHJ3A4qY5usyd2mFHgBeMhqxrVhSI8KbWaFsWAqPS7azCPL0YCorEMIuDTAgMB
+AAGjgZcwgZQwHQYDVR0OBBYEFCEwyfsA106Y2oeqKtCnLrFAMadMMA4GA1UdDwEB/wQEAwIBBjAP
+BgNVHRMBAf8EBTADAQH/MFIGA1UdHwRLMEkwR6BFoEOGQWh0dHA6Ly9jcmwubmV0c29sc3NsLmNv
+bS9OZXR3b3JrU29sdXRpb25zQ2VydGlmaWNhdGVBdXRob3JpdHkuY3JsMA0GCSqGSIb3DQEBBQUA
+A4IBAQC7rkvnt1frf6ott3NHhWrB5KUd5Oc86fRZZXe1eltajSU24HqXLjjAV2CDmAaDn7l2em5Q
+4LqILPxFzBiwmZVRDuwduIj/h1AcgsLj4DKAv6ALR8jDMe+ZZzKATxcheQxpXN5eNK4CtSbqUN9/
+GGUsyfJj4akH/nxxH2szJGoeBfcFaMBqEssuXmHLrijTfsK0ZpEmXzwuJF/LWA/rKOyvEZbz3Htv
+wKeI8lN3s2Berq4o2jUsbzRF0ybh3uxbTydrFny9RAQYgrOJeRcQcT16ohZO9QHNpGxlaKFJdlxD
+ydi8NmdspZS11My5vWo1ViHe2MPr+8ukYEywVaCge1ey
+-----END CERTIFICATE-----
+
+WellsSecure Public Root Certificate Authority
+=============================================
+-----BEGIN CERTIFICATE-----
+MIIEvTCCA6WgAwIBAgIBATANBgkqhkiG9w0BAQUFADCBhTELMAkGA1UEBhMCVVMxIDAeBgNVBAoM
+F1dlbGxzIEZhcmdvIFdlbGxzU2VjdXJlMRwwGgYDVQQLDBNXZWxscyBGYXJnbyBCYW5rIE5BMTYw
+NAYDVQQDDC1XZWxsc1NlY3VyZSBQdWJsaWMgUm9vdCBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkwHhcN
+MDcxMjEzMTcwNzU0WhcNMjIxMjE0MDAwNzU0WjCBhTELMAkGA1UEBhMCVVMxIDAeBgNVBAoMF1dl
+bGxzIEZhcmdvIFdlbGxzU2VjdXJlMRwwGgYDVQQLDBNXZWxscyBGYXJnbyBCYW5rIE5BMTYwNAYD
+VQQDDC1XZWxsc1NlY3VyZSBQdWJsaWMgUm9vdCBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkwggEiMA0G
+CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDub7S9eeKPCCGeOARBJe+rWxxTkqxtnt3CxC5FlAM1
+iGd0V+PfjLindo8796jE2yljDpFoNoqXjopxaAkH5OjUDk/41itMpBb570OYj7OeUt9tkTmPOL13
+i0Nj67eT/DBMHAGTthP796EfvyXhdDcsHqRePGj4S78NuR4uNuip5Kf4D8uCdXw1LSLWwr8L87T8
+bJVhHlfXBIEyg1J55oNjz7fLY4sR4r1e6/aN7ZVyKLSsEmLpSjPmgzKuBXWVvYSV2ypcm44uDLiB
+K0HmOFafSZtsdvqKXfcBeYF8wYNABf5x/Qw/zE5gCQ5lRxAvAcAFP4/4s0HvWkJ+We/SlwxlAgMB
+AAGjggE0MIIBMDAPBgNVHRMBAf8EBTADAQH/MDkGA1UdHwQyMDAwLqAsoCqGKGh0dHA6Ly9jcmwu
+cGtpLndlbGxzZmFyZ28uY29tL3dzcHJjYS5jcmwwDgYDVR0PAQH/BAQDAgHGMB0GA1UdDgQWBBQm
+lRkQ2eihl5H/3BnZtQQ+0nMKajCBsgYDVR0jBIGqMIGngBQmlRkQ2eihl5H/3BnZtQQ+0nMKaqGB
+i6SBiDCBhTELMAkGA1UEBhMCVVMxIDAeBgNVBAoMF1dlbGxzIEZhcmdvIFdlbGxzU2VjdXJlMRww
+GgYDVQQLDBNXZWxscyBGYXJnbyBCYW5rIE5BMTYwNAYDVQQDDC1XZWxsc1NlY3VyZSBQdWJsaWMg
+Um9vdCBDZXJ0aWZpY2F0ZSBBdXRob3JpdHmCAQEwDQYJKoZIhvcNAQEFBQADggEBALkVsUSRzCPI
+K0134/iaeycNzXK7mQDKfGYZUMbVmO2rvwNa5U3lHshPcZeG1eMd/ZDJPHV3V3p9+N701NX3leZ0
+bh08rnyd2wIDBSxxSyU+B+NemvVmFymIGjifz6pBA4SXa5M4esowRBskRDPQ5NHcKDj0E0M1NSlj
+qHyita04pO2t/caaH/+Xc/77szWnk4bGdpEA5qxRFsQnMlzbc9qlk1eOPm01JghZ1edE13YgY+es
+E2fDbbFwRnzVlhE9iW9dqKHrjQrawx0zbKPqZxmamX9LPYNRKh3KL4YMon4QLSvUFpULB6ouFJJJ
+tylv2G0xffX8oRAHh84vWdw+WNs=
+-----END CERTIFICATE-----
+
+COMODO ECC Certification Authority
+==================================
+-----BEGIN CERTIFICATE-----
+MIICiTCCAg+gAwIBAgIQH0evqmIAcFBUTAGem2OZKjAKBggqhkjOPQQDAzCBhTELMAkGA1UEBhMC
+R0IxGzAZBgNVBAgTEkdyZWF0ZXIgTWFuY2hlc3RlcjEQMA4GA1UEBxMHU2FsZm9yZDEaMBgGA1UE
+ChMRQ09NT0RPIENBIExpbWl0ZWQxKzApBgNVBAMTIkNPTU9ETyBFQ0MgQ2VydGlmaWNhdGlvbiBB
+dXRob3JpdHkwHhcNMDgwMzA2MDAwMDAwWhcNMzgwMTE4MjM1OTU5WjCBhTELMAkGA1UEBhMCR0Ix
+GzAZBgNVBAgTEkdyZWF0ZXIgTWFuY2hlc3RlcjEQMA4GA1UEBxMHU2FsZm9yZDEaMBgGA1UEChMR
+Q09NT0RPIENBIExpbWl0ZWQxKzApBgNVBAMTIkNPTU9ETyBFQ0MgQ2VydGlmaWNhdGlvbiBBdXRo
+b3JpdHkwdjAQBgcqhkjOPQIBBgUrgQQAIgNiAAQDR3svdcmCFYX7deSRFtSrYpn1PlILBs5BAH+X
+4QokPB0BBO490o0JlwzgdeT6+3eKKvUDYEs2ixYjFq0JcfRK9ChQtP6IHG4/bC8vCVlbpVsLM5ni
+wz2J+Wos77LTBumjQjBAMB0GA1UdDgQWBBR1cacZSBm8nZ3qQUfflMRId5nTeTAOBgNVHQ8BAf8E
+BAMCAQYwDwYDVR0TAQH/BAUwAwEB/zAKBggqhkjOPQQDAwNoADBlAjEA7wNbeqy3eApyt4jf/7VG
+FAkK+qDmfQjGGoe9GKhzvSbKYAydzpmfz1wPMOG+FDHqAjAU9JM8SaczepBGR7NjfRObTrdvGDeA
+U/7dIOA1mjbRxwG55tzd8/8dLDoWV9mSOdY=
+-----END CERTIFICATE-----
+
+IGC/A
+=====
+-----BEGIN CERTIFICATE-----
+MIIEAjCCAuqgAwIBAgIFORFFEJQwDQYJKoZIhvcNAQEFBQAwgYUxCzAJBgNVBAYTAkZSMQ8wDQYD
+VQQIEwZGcmFuY2UxDjAMBgNVBAcTBVBhcmlzMRAwDgYDVQQKEwdQTS9TR0ROMQ4wDAYDVQQLEwVE
+Q1NTSTEOMAwGA1UEAxMFSUdDL0ExIzAhBgkqhkiG9w0BCQEWFGlnY2FAc2dkbi5wbS5nb3V2LmZy
+MB4XDTAyMTIxMzE0MjkyM1oXDTIwMTAxNzE0MjkyMlowgYUxCzAJBgNVBAYTAkZSMQ8wDQYDVQQI
+EwZGcmFuY2UxDjAMBgNVBAcTBVBhcmlzMRAwDgYDVQQKEwdQTS9TR0ROMQ4wDAYDVQQLEwVEQ1NT
+STEOMAwGA1UEAxMFSUdDL0ExIzAhBgkqhkiG9w0BCQEWFGlnY2FAc2dkbi5wbS5nb3V2LmZyMIIB
+IjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAsh/R0GLFMzvABIaIs9z4iPf930Pfeo2aSVz2
+TqrMHLmh6yeJ8kbpO0px1R2OLc/mratjUMdUC24SyZA2xtgv2pGqaMVy/hcKshd+ebUyiHDKcMCW
+So7kVc0dJ5S/znIq7Fz5cyD+vfcuiWe4u0dzEvfRNWk68gq5rv9GQkaiv6GFGvm/5P9JhfejcIYy
+HF2fYPepraX/z9E0+X1bF8bc1g4oa8Ld8fUzaJ1O/Id8NhLWo4DoQw1VYZTqZDdH6nfK0LJYBcNd
+frGoRpAxVs5wKpayMLh35nnAvSk7/ZR3TL0gzUEl4C7HG7vupARB0l2tEmqKm0f7yd1GQOGdPDPQ
+tQIDAQABo3cwdTAPBgNVHRMBAf8EBTADAQH/MAsGA1UdDwQEAwIBRjAVBgNVHSAEDjAMMAoGCCqB
+egF5AQEBMB0GA1UdDgQWBBSjBS8YYFDCiQrdKyFP/45OqDAxNjAfBgNVHSMEGDAWgBSjBS8YYFDC
+iQrdKyFP/45OqDAxNjANBgkqhkiG9w0BAQUFAAOCAQEABdwm2Pp3FURo/C9mOnTgXeQp/wYHE4RK
+q89toB9RlPhJy3Q2FLwV3duJL92PoF189RLrn544pEfMs5bZvpwlqwN+Mw+VgQ39FuCIvjfwbF3Q
+MZsyK10XZZOYYLxuj7GoPB7ZHPOpJkL5ZB3C55L29B5aqhlSXa/oovdgoPaN8In1buAKBQGVyYsg
+Crpa/JosPL3Dt8ldeCUFP1YUmwza+zpI/pdpXsoQhvdOlgQITeywvl3cO45Pwf2aNjSaTFR+FwNI
+lQgRHAdvhQh+XU3Endv7rs6y0bO4g2wdsrN58dhwmX7wEwLOXt1R0982gaEbeC9xs/FZTEYYKKuF
+0mBWWg==
+-----END CERTIFICATE-----
+
+Security Communication EV RootCA1
+=================================
+-----BEGIN CERTIFICATE-----
+MIIDfTCCAmWgAwIBAgIBADANBgkqhkiG9w0BAQUFADBgMQswCQYDVQQGEwJKUDElMCMGA1UEChMc
+U0VDT00gVHJ1c3QgU3lzdGVtcyBDTy4sTFRELjEqMCgGA1UECxMhU2VjdXJpdHkgQ29tbXVuaWNh
+dGlvbiBFViBSb290Q0ExMB4XDTA3MDYwNjAyMTIzMloXDTM3MDYwNjAyMTIzMlowYDELMAkGA1UE
+BhMCSlAxJTAjBgNVBAoTHFNFQ09NIFRydXN0IFN5c3RlbXMgQ08uLExURC4xKjAoBgNVBAsTIVNl
+Y3VyaXR5IENvbW11bmljYXRpb24gRVYgUm9vdENBMTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCC
+AQoCggEBALx/7FebJOD+nLpCeamIivqA4PUHKUPqjgo0No0c+qe1OXj/l3X3L+SqawSERMqm4miO
+/VVQYg+kcQ7OBzgtQoVQrTyWb4vVog7P3kmJPdZkLjjlHmy1V4qe70gOzXppFodEtZDkBp2uoQSX
+WHnvIEqCa4wiv+wfD+mEce3xDuS4GBPMVjZd0ZoeUWs5bmB2iDQL87PRsJ3KYeJkHcFGB7hj3R4z
+ZbOOCVVSPbW9/wfrrWFVGCypaZhKqkDFMxRldAD5kd6vA0jFQFTcD4SQaCDFkpbcLuUCRarAX1T4
+bepJz11sS6/vmsJWXMY1VkJqMF/Cq/biPT+zyRGPMUzXn0kCAwEAAaNCMEAwHQYDVR0OBBYEFDVK
+9U2vP9eCOKyrcWUXdYydVZPmMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMBAf8EBTADAQH/MA0GCSqG
+SIb3DQEBBQUAA4IBAQCoh+ns+EBnXcPBZsdAS5f8hxOQWsTvoMpfi7ent/HWtWS3irO4G8za+6xm
+iEHO6Pzk2x6Ipu0nUBsCMCRGef4Eh3CXQHPRwMFXGZpppSeZq51ihPZRwSzJIxXYKLerJRO1RuGG
+Av8mjMSIkh1W/hln8lXkgKNrnKt34VFxDSDbEJrbvXZ5B3eZKK2aXtqxT0QsNY6llsf9g/BYxnnW
+mHyojf6GPgcWkuF75x3sM3Z+Qi5KhfmRiWiEA4Glm5q+4zfFVKtWOxgtQaQM+ELbmaDgcm+7XeEW
+T1MKZPlO9L9OVL14bIjqv5wTJMJwaaJ/D8g8rQjJsJhAoyrniIPtd490
+-----END CERTIFICATE-----
+
+OISTE WISeKey Global Root GA CA
+===============================
+-----BEGIN CERTIFICATE-----
+MIID8TCCAtmgAwIBAgIQQT1yx/RrH4FDffHSKFTfmjANBgkqhkiG9w0BAQUFADCBijELMAkGA1UE
+BhMCQ0gxEDAOBgNVBAoTB1dJU2VLZXkxGzAZBgNVBAsTEkNvcHlyaWdodCAoYykgMjAwNTEiMCAG
+A1UECxMZT0lTVEUgRm91bmRhdGlvbiBFbmRvcnNlZDEoMCYGA1UEAxMfT0lTVEUgV0lTZUtleSBH
+bG9iYWwgUm9vdCBHQSBDQTAeFw0wNTEyMTExNjAzNDRaFw0zNzEyMTExNjA5NTFaMIGKMQswCQYD
+VQQGEwJDSDEQMA4GA1UEChMHV0lTZUtleTEbMBkGA1UECxMSQ29weXJpZ2h0IChjKSAyMDA1MSIw
+IAYDVQQLExlPSVNURSBGb3VuZGF0aW9uIEVuZG9yc2VkMSgwJgYDVQQDEx9PSVNURSBXSVNlS2V5
+IEdsb2JhbCBSb290IEdBIENBMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAy0+zAJs9
+Nt350UlqaxBJH+zYK7LG+DKBKUOVTJoZIyEVRd7jyBxRVVuuk+g3/ytr6dTqvirdqFEr12bDYVxg
+Asj1znJ7O7jyTmUIms2kahnBAbtzptf2w93NvKSLtZlhuAGio9RN1AU9ka34tAhxZK9w8RxrfvbD
+d50kc3vkDIzh2TbhmYsFmQvtRTEJysIA2/dyoJaqlYfQjse2YXMNdmaM3Bu0Y6Kff5MTMPGhJ9vZ
+/yxViJGg4E8HsChWjBgbl0SOid3gF27nKu+POQoxhILYQBRJLnpB5Kf+42TMwVlxSywhp1t94B3R
+LoGbw9ho972WG6xwsRYUC9tguSYBBQIDAQABo1EwTzALBgNVHQ8EBAMCAYYwDwYDVR0TAQH/BAUw
+AwEB/zAdBgNVHQ4EFgQUswN+rja8sHnR3JQmthG+IbJphpQwEAYJKwYBBAGCNxUBBAMCAQAwDQYJ
+KoZIhvcNAQEFBQADggEBAEuh/wuHbrP5wUOxSPMowB0uyQlB+pQAHKSkq0lPjz0e701vvbyk9vIm
+MMkQyh2I+3QZH4VFvbBsUfk2ftv1TDI6QU9bR8/oCy22xBmddMVHxjtqD6wU2zz0c5ypBd8A3HR4
++vg1YFkCExh8vPtNsCBtQ7tgMHpnM1zFmdH4LTlSc/uMqpclXHLZCB6rTjzjgTGfA6b7wP4piFXa
+hNVQA7bihKOmNqoROgHhGEvWRGizPflTdISzRpFGlgC3gCy24eMQ4tui5yiPAZZiFj4A4xylNoEY
+okxSdsARo27mHbrjWr42U8U+dY+GaSlYU7Wcu2+fXMUY7N0v4ZjJ/L7fCg0=
+-----END CERTIFICATE-----
+
+S-TRUST Authentication and Encryption Root CA 2005 PN
+=====================================================
+-----BEGIN CERTIFICATE-----
+MIIEezCCA2OgAwIBAgIQNxkY5lNUfBq1uMtZWts1tzANBgkqhkiG9w0BAQUFADCBrjELMAkGA1UE
+BhMCREUxIDAeBgNVBAgTF0JhZGVuLVd1ZXJ0dGVtYmVyZyAoQlcpMRIwEAYDVQQHEwlTdHV0dGdh
+cnQxKTAnBgNVBAoTIERldXRzY2hlciBTcGFya2Fzc2VuIFZlcmxhZyBHbWJIMT4wPAYDVQQDEzVT
+LVRSVVNUIEF1dGhlbnRpY2F0aW9uIGFuZCBFbmNyeXB0aW9uIFJvb3QgQ0EgMjAwNTpQTjAeFw0w
+NTA2MjIwMDAwMDBaFw0zMDA2MjEyMzU5NTlaMIGuMQswCQYDVQQGEwJERTEgMB4GA1UECBMXQmFk
+ZW4tV3VlcnR0ZW1iZXJnIChCVykxEjAQBgNVBAcTCVN0dXR0Z2FydDEpMCcGA1UEChMgRGV1dHNj
+aGVyIFNwYXJrYXNzZW4gVmVybGFnIEdtYkgxPjA8BgNVBAMTNVMtVFJVU1QgQXV0aGVudGljYXRp
+b24gYW5kIEVuY3J5cHRpb24gUm9vdCBDQSAyMDA1OlBOMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8A
+MIIBCgKCAQEA2bVKwdMz6tNGs9HiTNL1toPQb9UY6ZOvJ44TzbUlNlA0EmQpoVXhOmCTnijJ4/Ob
+4QSwI7+Vio5bG0F/WsPoTUzVJBY+h0jUJ67m91MduwwA7z5hca2/OnpYH5Q9XIHV1W/fuJvS9eXL
+g3KSwlOyggLrra1fFi2SU3bxibYs9cEv4KdKb6AwajLrmnQDaHgTncovmwsdvs91DSaXm8f1Xgqf
+eN+zvOyauu9VjxuapgdjKRdZYgkqeQd3peDRF2npW932kKvimAoA0SVtnteFhy+S8dF2g08LOlk3
+KC8zpxdQ1iALCvQm+Z845y2kuJuJja2tyWp9iRe79n+Ag3rm7QIDAQABo4GSMIGPMBIGA1UdEwEB
+/wQIMAYBAf8CAQAwDgYDVR0PAQH/BAQDAgEGMCkGA1UdEQQiMCCkHjAcMRowGAYDVQQDExFTVFJv
+bmxpbmUxLTIwNDgtNTAdBgNVHQ4EFgQUD8oeXHngovMpttKFswtKtWXsa1IwHwYDVR0jBBgwFoAU
+D8oeXHngovMpttKFswtKtWXsa1IwDQYJKoZIhvcNAQEFBQADggEBAK8B8O0ZPCjoTVy7pWMciDMD
+pwCHpB8gq9Yc4wYfl35UvbfRssnV2oDsF9eK9XvCAPbpEW+EoFolMeKJ+aQAPzFoLtU96G7m1R08
+P7K9n3frndOMusDXtk3sU5wPBG7qNWdX4wple5A64U8+wwCSersFiXOMy6ZNwPv2AtawB6MDwidA
+nwzkhYItr5pCHdDHjfhA7p0GVxzZotiAFP7hYy0yh9WUUpY6RsZxlj33mA6ykaqP2vROJAA5Veit
+F7nTNCtKqUDMFypVZUF0Qn71wK/Ik63yGFs9iQzbRzkk+OBM8h+wPQrKBU6JIRrjKpms/H+h8Q8b
+Hz2eBIPdltkdOpQ=
+-----END CERTIFICATE-----
+
+Microsec e-Szigno Root CA
+=========================
+-----BEGIN CERTIFICATE-----
+MIIHqDCCBpCgAwIBAgIRAMy4579OKRr9otxmpRwsDxEwDQYJKoZIhvcNAQEFBQAwcjELMAkGA1UE
+BhMCSFUxETAPBgNVBAcTCEJ1ZGFwZXN0MRYwFAYDVQQKEw1NaWNyb3NlYyBMdGQuMRQwEgYDVQQL
+EwtlLVN6aWdubyBDQTEiMCAGA1UEAxMZTWljcm9zZWMgZS1Temlnbm8gUm9vdCBDQTAeFw0wNTA0
+MDYxMjI4NDRaFw0xNzA0MDYxMjI4NDRaMHIxCzAJBgNVBAYTAkhVMREwDwYDVQQHEwhCdWRhcGVz
+dDEWMBQGA1UEChMNTWljcm9zZWMgTHRkLjEUMBIGA1UECxMLZS1Temlnbm8gQ0ExIjAgBgNVBAMT
+GU1pY3Jvc2VjIGUtU3ppZ25vIFJvb3QgQ0EwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIB
+AQDtyADVgXvNOABHzNuEwSFpLHSQDCHZU4ftPkNEU6+r+ICbPHiN1I2uuO/TEdyB5s87lozWbxXG
+d36hL+BfkrYn13aaHUM86tnsL+4582pnS4uCzyL4ZVX+LMsvfUh6PXX5qqAnu3jCBspRwn5mS6/N
+oqdNAoI/gqyFxuEPkEeZlApxcpMqyabAvjxWTHOSJ/FrtfX9/DAFYJLG65Z+AZHCabEeHXtTRbjc
+QR/Ji3HWVBTji1R4P770Yjtb9aPs1ZJ04nQw7wHb4dSrmZsqa/i9phyGI0Jf7Enemotb9HI6QMVJ
+PqW+jqpx62z69Rrkav17fVVA71hu5tnVvCSrwe+3AgMBAAGjggQ3MIIEMzBnBggrBgEFBQcBAQRb
+MFkwKAYIKwYBBQUHMAGGHGh0dHBzOi8vcmNhLmUtc3ppZ25vLmh1L29jc3AwLQYIKwYBBQUHMAKG
+IWh0dHA6Ly93d3cuZS1zemlnbm8uaHUvUm9vdENBLmNydDAPBgNVHRMBAf8EBTADAQH/MIIBcwYD
+VR0gBIIBajCCAWYwggFiBgwrBgEEAYGoGAIBAQEwggFQMCgGCCsGAQUFBwIBFhxodHRwOi8vd3d3
+LmUtc3ppZ25vLmh1L1NaU1ovMIIBIgYIKwYBBQUHAgIwggEUHoIBEABBACAAdABhAG4A+gBzAO0A
+dAB2AOEAbgB5ACAA6QByAHQAZQBsAG0AZQB6AOkAcwDpAGgAZQB6ACAA6QBzACAAZQBsAGYAbwBn
+AGEAZADhAHMA4QBoAG8AegAgAGEAIABTAHoAbwBsAGcA4QBsAHQAYQB0APMAIABTAHoAbwBsAGcA
+4QBsAHQAYQB0AOEAcwBpACAAUwB6AGEAYgDhAGwAeQB6AGEAdABhACAAcwB6AGUAcgBpAG4AdAAg
+AGsAZQBsAGwAIABlAGwAagDhAHIAbgBpADoAIABoAHQAdABwADoALwAvAHcAdwB3AC4AZQAtAHMA
+egBpAGcAbgBvAC4AaAB1AC8AUwBaAFMAWgAvMIHIBgNVHR8EgcAwgb0wgbqggbeggbSGIWh0dHA6
+Ly93d3cuZS1zemlnbm8uaHUvUm9vdENBLmNybIaBjmxkYXA6Ly9sZGFwLmUtc3ppZ25vLmh1L0NO
+PU1pY3Jvc2VjJTIwZS1Temlnbm8lMjBSb290JTIwQ0EsT1U9ZS1Temlnbm8lMjBDQSxPPU1pY3Jv
+c2VjJTIwTHRkLixMPUJ1ZGFwZXN0LEM9SFU/Y2VydGlmaWNhdGVSZXZvY2F0aW9uTGlzdDtiaW5h
+cnkwDgYDVR0PAQH/BAQDAgEGMIGWBgNVHREEgY4wgYuBEGluZm9AZS1zemlnbm8uaHWkdzB1MSMw
+IQYDVQQDDBpNaWNyb3NlYyBlLVN6aWduw7MgUm9vdCBDQTEWMBQGA1UECwwNZS1TemlnbsOzIEhT
+WjEWMBQGA1UEChMNTWljcm9zZWMgS2Z0LjERMA8GA1UEBxMIQnVkYXBlc3QxCzAJBgNVBAYTAkhV
+MIGsBgNVHSMEgaQwgaGAFMegSXUWYYTbMUuE0vE3QJDvTtz3oXakdDByMQswCQYDVQQGEwJIVTER
+MA8GA1UEBxMIQnVkYXBlc3QxFjAUBgNVBAoTDU1pY3Jvc2VjIEx0ZC4xFDASBgNVBAsTC2UtU3pp
+Z25vIENBMSIwIAYDVQQDExlNaWNyb3NlYyBlLVN6aWdubyBSb290IENBghEAzLjnv04pGv2i3Gal
+HCwPETAdBgNVHQ4EFgQUx6BJdRZhhNsxS4TS8TdAkO9O3PcwDQYJKoZIhvcNAQEFBQADggEBANMT
+nGZjWS7KXHAM/IO8VbH0jgdsZifOwTsgqRy7RlRw7lrMoHfqaEQn6/Ip3Xep1fvj1KcExJW4C+FE
+aGAHQzAxQmHl7tnlJNUb3+FKG6qfx1/4ehHqE5MAyopYse7tDk2016g2JnzgOsHVV4Lxdbb9iV/a
+86g4nzUGCM4ilb7N1fy+W955a9x6qWVmvrElWl/tftOsRm1M9DKHtCAE4Gx4sHfRhUZLphK3dehK
+yVZs15KrnfVJONJPU+NVkBHbmJbGSfI+9J8b4PeI3CVimUTYc78/MPMMNz7UwiiAc7EBt51alhQB
+S6kRnSlqLtBdgcDPsiBDxwPgN05dCtxZICU=
+-----END CERTIFICATE-----
+
+Certigna
+========
+-----BEGIN CERTIFICATE-----
+MIIDqDCCApCgAwIBAgIJAP7c4wEPyUj/MA0GCSqGSIb3DQEBBQUAMDQxCzAJBgNVBAYTAkZSMRIw
+EAYDVQQKDAlEaGlteW90aXMxETAPBgNVBAMMCENlcnRpZ25hMB4XDTA3MDYyOTE1MTMwNVoXDTI3
+MDYyOTE1MTMwNVowNDELMAkGA1UEBhMCRlIxEjAQBgNVBAoMCURoaW15b3RpczERMA8GA1UEAwwI
+Q2VydGlnbmEwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDIaPHJ1tazNHUmgh7stL7q
+XOEm7RFHYeGifBZ4QCHkYJ5ayGPhxLGWkv8YbWkj4Sti993iNi+RB7lIzw7sebYs5zRLcAglozyH
+GxnygQcPOJAZ0xH+hrTy0V4eHpbNgGzOOzGTtvKg0KmVEn2lmsxryIRWijOp5yIVUxbwzBfsV1/p
+ogqYCd7jX5xv3EjjhQsVWqa6n6xI4wmy9/Qy3l40vhx4XUJbzg4ij02Q130yGLMLLGq/jj8UEYkg
+DncUtT2UCIf3JR7VsmAA7G8qKCVuKj4YYxclPz5EIBb2JsglrgVKtOdjLPOMFlN+XPsRGgjBRmKf
+Irjxwo1p3Po6WAbfAgMBAAGjgbwwgbkwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUGu3+QTmQ
+tCRZvgHyUtVF9lo53BEwZAYDVR0jBF0wW4AUGu3+QTmQtCRZvgHyUtVF9lo53BGhOKQ2MDQxCzAJ
+BgNVBAYTAkZSMRIwEAYDVQQKDAlEaGlteW90aXMxETAPBgNVBAMMCENlcnRpZ25hggkA/tzjAQ/J
+SP8wDgYDVR0PAQH/BAQDAgEGMBEGCWCGSAGG+EIBAQQEAwIABzANBgkqhkiG9w0BAQUFAAOCAQEA
+hQMeknH2Qq/ho2Ge6/PAD/Kl1NqV5ta+aDY9fm4fTIrv0Q8hbV6lUmPOEvjvKtpv6zf+EwLHyzs+
+ImvaYS5/1HI93TDhHkxAGYwP15zRgzB7mFncfca5DClMoTOi62c6ZYTTluLtdkVwj7Ur3vkj1klu
+PBS1xp81HlDQwY9qcEQCYsuuHWhBp6pX6FOqB9IG9tUUBguRA3UsbHK1YZWaDYu5Def131TN3ubY
+1gkIl2PlwS6wt0QmwCbAr1UwnjvVNioZBPRcHv/PLLf/0P2HQBHVESO7SMAhqaQoLf0V+LBOK/Qw
+WyH8EZE0vkHve52Xdf+XlcCWWC/qu0bXu+TZLg==
+-----END CERTIFICATE-----
+
+AC Ra\xC3\xADz Certic\xC3\xA1mara S.A.
+======================================
+-----BEGIN CERTIFICATE-----
+MIIGZjCCBE6gAwIBAgIPB35Sk3vgFeNX8GmMy+wMMA0GCSqGSIb3DQEBBQUAMHsxCzAJBgNVBAYT
+AkNPMUcwRQYDVQQKDD5Tb2NpZWRhZCBDYW1lcmFsIGRlIENlcnRpZmljYWNpw7NuIERpZ2l0YWwg
+LSBDZXJ0aWPDoW1hcmEgUy5BLjEjMCEGA1UEAwwaQUMgUmHDrXogQ2VydGljw6FtYXJhIFMuQS4w
+HhcNMDYxMTI3MjA0NjI5WhcNMzAwNDAyMjE0MjAyWjB7MQswCQYDVQQGEwJDTzFHMEUGA1UECgw+
+U29jaWVkYWQgQ2FtZXJhbCBkZSBDZXJ0aWZpY2FjacOzbiBEaWdpdGFsIC0gQ2VydGljw6FtYXJh
+IFMuQS4xIzAhBgNVBAMMGkFDIFJhw616IENlcnRpY8OhbWFyYSBTLkEuMIICIjANBgkqhkiG9w0B
+AQEFAAOCAg8AMIICCgKCAgEAq2uJo1PMSCMI+8PPUZYILrgIem08kBeGqentLhM0R7LQcNzJPNCN
+yu5LF6vQhbCnIwTLqKL85XXbQMpiiY9QngE9JlsYhBzLfDe3fezTf3MZsGqy2IiKLUV0qPezuMDU
+2s0iiXRNWhU5cxh0T7XrmafBHoi0wpOQY5fzp6cSsgkiBzPZkc0OnB8OIMfuuzONj8LSWKdf/WU3
+4ojC2I+GdV75LaeHM/J4Ny+LvB2GNzmxlPLYvEqcgxhaBvzz1NS6jBUJJfD5to0EfhcSM2tXSExP
+2yYe68yQ54v5aHxwD6Mq0Do43zeX4lvegGHTgNiRg0JaTASJaBE8rF9ogEHMYELODVoqDA+bMMCm
+8Ibbq0nXl21Ii/kDwFJnmxL3wvIumGVC2daa49AZMQyth9VXAnow6IYm+48jilSH5L887uvDdUhf
+HjlvgWJsxS3EF1QZtzeNnDeRyPYL1epjb4OsOMLzP96a++EjYfDIJss2yKHzMI+ko6Kh3VOz3vCa
+Mh+DkXkwwakfU5tTohVTP92dsxA7SH2JD/ztA/X7JWR1DhcZDY8AFmd5ekD8LVkH2ZD6mq093ICK
+5lw1omdMEWux+IBkAC1vImHFrEsm5VoQgpukg3s0956JkSCXjrdCx2bD0Omk1vUgjcTDlaxECp1b
+czwmPS9KvqfJpxAe+59QafMCAwEAAaOB5jCB4zAPBgNVHRMBAf8EBTADAQH/MA4GA1UdDwEB/wQE
+AwIBBjAdBgNVHQ4EFgQU0QnQ6dfOeXRU+Tows/RtLAMDG2gwgaAGA1UdIASBmDCBlTCBkgYEVR0g
+ADCBiTArBggrBgEFBQcCARYfaHR0cDovL3d3dy5jZXJ0aWNhbWFyYS5jb20vZHBjLzBaBggrBgEF
+BQcCAjBOGkxMaW1pdGFjaW9uZXMgZGUgZ2FyYW507WFzIGRlIGVzdGUgY2VydGlmaWNhZG8gc2Ug
+cHVlZGVuIGVuY29udHJhciBlbiBsYSBEUEMuMA0GCSqGSIb3DQEBBQUAA4ICAQBclLW4RZFNjmEf
+AygPU3zmpFmps4p6xbD/CHwso3EcIRNnoZUSQDWDg4902zNc8El2CoFS3UnUmjIz75uny3XlesuX
+EpBcunvFm9+7OSPI/5jOCk0iAUgHforA1SBClETvv3eiiWdIG0ADBaGJ7M9i4z0ldma/Jre7Ir5v
+/zlXdLp6yQGVwZVR6Kss+LGGIOk/yzVb0hfpKv6DExdA7ohiZVvVO2Dpezy4ydV/NgIlqmjCMRW3
+MGXrfx1IebHPOeJCgBbT9ZMj/EyXyVo3bHwi2ErN0o42gzmRkBDI8ck1fj+404HGIGQatlDCIaR4
+3NAvO2STdPCWkPHv+wlaNECW8DYSwaN0jJN+Qd53i+yG2dIPPy3RzECiiWZIHiCznCNZc6lEc7wk
+eZBWN7PGKX6jD/EpOe9+XCgycDWs2rjIdWb8m0w5R44bb5tNAlQiM+9hup4phO9OSzNHdpdqy35f
+/RWmnkJDW2ZaiogN9xa5P1FlK2Zqi9E4UqLWRhH6/JocdJ6PlwsCT2TG9WjTSy3/pDceiz+/RL5h
+RqGEPQgnTIEgd4kI6mdAXmwIUV80WoyWaM3X94nCHNMyAK9Sy9NgWyo6R35rMDOhYil/SrnhLecU
+Iw4OGEfhefwVVdCx/CVxY3UzHCMrr1zZ7Ud3YA47Dx7SwNxkBYn8eNZcLCZDqQ==
+-----END CERTIFICATE-----
+
+TC TrustCenter Class 2 CA II
+============================
+-----BEGIN CERTIFICATE-----
+MIIEqjCCA5KgAwIBAgIOLmoAAQACH9dSISwRXDswDQYJKoZIhvcNAQEFBQAwdjELMAkGA1UEBhMC
+REUxHDAaBgNVBAoTE1RDIFRydXN0Q2VudGVyIEdtYkgxIjAgBgNVBAsTGVRDIFRydXN0Q2VudGVy
+IENsYXNzIDIgQ0ExJTAjBgNVBAMTHFRDIFRydXN0Q2VudGVyIENsYXNzIDIgQ0EgSUkwHhcNMDYw
+MTEyMTQzODQzWhcNMjUxMjMxMjI1OTU5WjB2MQswCQYDVQQGEwJERTEcMBoGA1UEChMTVEMgVHJ1
+c3RDZW50ZXIgR21iSDEiMCAGA1UECxMZVEMgVHJ1c3RDZW50ZXIgQ2xhc3MgMiBDQTElMCMGA1UE
+AxMcVEMgVHJ1c3RDZW50ZXIgQ2xhc3MgMiBDQSBJSTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCC
+AQoCggEBAKuAh5uO8MN8h9foJIIRszzdQ2Lu+MNF2ujhoF/RKrLqk2jftMjWQ+nEdVl//OEd+DFw
+IxuInie5e/060smp6RQvkL4DUsFJzfb95AhmC1eKokKguNV/aVyQMrKXDcpK3EY+AlWJU+MaWss2
+xgdW94zPEfRMuzBwBJWl9jmM/XOBCH2JXjIeIqkiRUuwZi4wzJ9l/fzLganx4Duvo4bRierERXlQ
+Xa7pIXSSTYtZgo+U4+lK8edJsBTj9WLL1XK9H7nSn6DNqPoByNkN39r8R52zyFTfSUrxIan+GE7u
+SNQZu+995OKdy1u2bv/jzVrndIIFuoAlOMvkaZ6vQaoahPUCAwEAAaOCATQwggEwMA8GA1UdEwEB
+/wQFMAMBAf8wDgYDVR0PAQH/BAQDAgEGMB0GA1UdDgQWBBTjq1RMgKHbVkO3kUrL84J6E1wIqzCB
+7QYDVR0fBIHlMIHiMIHfoIHcoIHZhjVodHRwOi8vd3d3LnRydXN0Y2VudGVyLmRlL2NybC92Mi90
+Y19jbGFzc18yX2NhX0lJLmNybIaBn2xkYXA6Ly93d3cudHJ1c3RjZW50ZXIuZGUvQ049VEMlMjBU
+cnVzdENlbnRlciUyMENsYXNzJTIwMiUyMENBJTIwSUksTz1UQyUyMFRydXN0Q2VudGVyJTIwR21i
+SCxPVT1yb290Y2VydHMsREM9dHJ1c3RjZW50ZXIsREM9ZGU/Y2VydGlmaWNhdGVSZXZvY2F0aW9u
+TGlzdD9iYXNlPzANBgkqhkiG9w0BAQUFAAOCAQEAjNfffu4bgBCzg/XbEeprS6iSGNn3Bzn1LL4G
+dXpoUxUc6krtXvwjshOg0wn/9vYua0Fxec3ibf2uWWuFHbhOIprtZjluS5TmVfwLG4t3wVMTZonZ
+KNaL80VKY7f9ewthXbhtvsPcW3nS7Yblok2+XnR8au0WOB9/WIFaGusyiC2y8zl3gK9etmF1Kdsj
+TYjKUCjLhdLTEKJZbtOTVAB6okaVhgWcqRmY5TFyDADiZ9lA4CQze28suVyrZZ0srHbqNZn1l7kP
+JOzHdiEoZa5X6AeIdUpWoNIFOqTmjZKILPPy4cHGYdtBxceb9w4aUUXCYWvcZCcXjFq32nQozZfk
+vQ==
+-----END CERTIFICATE-----
+
+TC TrustCenter Class 3 CA II
+============================
+-----BEGIN CERTIFICATE-----
+MIIEqjCCA5KgAwIBAgIOSkcAAQAC5aBd1j8AUb8wDQYJKoZIhvcNAQEFBQAwdjELMAkGA1UEBhMC
+REUxHDAaBgNVBAoTE1RDIFRydXN0Q2VudGVyIEdtYkgxIjAgBgNVBAsTGVRDIFRydXN0Q2VudGVy
+IENsYXNzIDMgQ0ExJTAjBgNVBAMTHFRDIFRydXN0Q2VudGVyIENsYXNzIDMgQ0EgSUkwHhcNMDYw
+MTEyMTQ0MTU3WhcNMjUxMjMxMjI1OTU5WjB2MQswCQYDVQQGEwJERTEcMBoGA1UEChMTVEMgVHJ1
+c3RDZW50ZXIgR21iSDEiMCAGA1UECxMZVEMgVHJ1c3RDZW50ZXIgQ2xhc3MgMyBDQTElMCMGA1UE
+AxMcVEMgVHJ1c3RDZW50ZXIgQ2xhc3MgMyBDQSBJSTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCC
+AQoCggEBALTgu1G7OVyLBMVMeRwjhjEQY0NVJz/GRcekPewJDRoeIMJWHt4bNwcwIi9v8Qbxq63W
+yKthoy9DxLCyLfzDlml7forkzMA5EpBCYMnMNWju2l+QVl/NHE1bWEnrDgFPZPosPIlY2C8u4rBo
+6SI7dYnWRBpl8huXJh0obazovVkdKyT21oQDZogkAHhg8fir/gKya/si+zXmFtGt9i4S5Po1auUZ
+uV3bOx4a+9P/FRQI2AlqukWdFHlgfa9Aigdzs5OW03Q0jTo3Kd5c7PXuLjHCINy+8U9/I1LZW+Jk
+2ZyqBwi1Rb3R0DHBq1SfqdLDYmAD8bs5SpJKPQq5ncWg/jcCAwEAAaOCATQwggEwMA8GA1UdEwEB
+/wQFMAMBAf8wDgYDVR0PAQH/BAQDAgEGMB0GA1UdDgQWBBTUovyfs8PYA9NXXAek0CSnwPIA1DCB
+7QYDVR0fBIHlMIHiMIHfoIHcoIHZhjVodHRwOi8vd3d3LnRydXN0Y2VudGVyLmRlL2NybC92Mi90
+Y19jbGFzc18zX2NhX0lJLmNybIaBn2xkYXA6Ly93d3cudHJ1c3RjZW50ZXIuZGUvQ049VEMlMjBU
+cnVzdENlbnRlciUyMENsYXNzJTIwMyUyMENBJTIwSUksTz1UQyUyMFRydXN0Q2VudGVyJTIwR21i
+SCxPVT1yb290Y2VydHMsREM9dHJ1c3RjZW50ZXIsREM9ZGU/Y2VydGlmaWNhdGVSZXZvY2F0aW9u
+TGlzdD9iYXNlPzANBgkqhkiG9w0BAQUFAAOCAQEANmDkcPcGIEPZIxpC8vijsrlNirTzwppVMXzE
+O2eatN9NDoqTSheLG43KieHPOh6sHfGcMrSOWXaiQYUlN6AT0PV8TtXqluJucsG7Kv5sbviRmEb8
+yRtXW+rIGjs/sFGYPAfaLFkB2otE6OF0/ado3VS6g0bsyEa1+K+XwDsJHI/OcpY9M1ZwvJbL2NV9
+IJqDnxrcOfHFcqMRA/07QlIp2+gB95tejNaNhk4Z+rwcvsUhpYeeeC422wlxo3I0+GzjBgnyXlal
+092Y+tTmBvTwtiBjS+opvaqCZh77gaqnN60TGOaSw4HBM7uIHqHn4rS9MWwOUT1v+5ZWgOI2F9Hc
+5A==
+-----END CERTIFICATE-----
+
+TC TrustCenter Universal CA I
+=============================
+-----BEGIN CERTIFICATE-----
+MIID3TCCAsWgAwIBAgIOHaIAAQAC7LdggHiNtgYwDQYJKoZIhvcNAQEFBQAweTELMAkGA1UEBhMC
+REUxHDAaBgNVBAoTE1RDIFRydXN0Q2VudGVyIEdtYkgxJDAiBgNVBAsTG1RDIFRydXN0Q2VudGVy
+IFVuaXZlcnNhbCBDQTEmMCQGA1UEAxMdVEMgVHJ1c3RDZW50ZXIgVW5pdmVyc2FsIENBIEkwHhcN
+MDYwMzIyMTU1NDI4WhcNMjUxMjMxMjI1OTU5WjB5MQswCQYDVQQGEwJERTEcMBoGA1UEChMTVEMg
+VHJ1c3RDZW50ZXIgR21iSDEkMCIGA1UECxMbVEMgVHJ1c3RDZW50ZXIgVW5pdmVyc2FsIENBMSYw
+JAYDVQQDEx1UQyBUcnVzdENlbnRlciBVbml2ZXJzYWwgQ0EgSTCCASIwDQYJKoZIhvcNAQEBBQAD
+ggEPADCCAQoCggEBAKR3I5ZEr5D0MacQ9CaHnPM42Q9e3s9B6DGtxnSRJJZ4Hgmgm5qVSkr1YnwC
+qMqs+1oEdjneX/H5s7/zA1hV0qq34wQi0fiU2iIIAI3TfCZdzHd55yx4Oagmcw6iXSVphU9VDprv
+xrlE4Vc93x9UIuVvZaozhDrzznq+VZeujRIPFDPiUHDDSYcTvFHe15gSWu86gzOSBnWLknwSaHtw
+ag+1m7Z3W0hZneTvWq3zwZ7U10VOylY0Ibw+F1tvdwxIAUMpsN0/lm7mlaoMwCC2/T42J5zjXM9O
+gdwZu5GQfezmlwQek8wiSdeXhrYTCjxDI3d+8NzmzSQfO4ObNDqDNOMCAwEAAaNjMGEwHwYDVR0j
+BBgwFoAUkqR1LKSevoFE63n8isWVpesQdXMwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMC
+AYYwHQYDVR0OBBYEFJKkdSyknr6BROt5/IrFlaXrEHVzMA0GCSqGSIb3DQEBBQUAA4IBAQAo0uCG
+1eb4e/CX3CJrO5UUVg8RMKWaTzqwOuAGy2X17caXJ/4l8lfmXpWMPmRgFVp/Lw0BxbFg/UU1z/Cy
+vwbZ71q+s2IhtNerNXxTPqYn8aEt2hojnczd7Dwtnic0XQ/CNnm8yUpiLe1r2X1BQ3y2qsrtYbE3
+ghUJGooWMNjsydZHcnhLEEYUjl8Or+zHL6sQ17bxbuyGssLoDZJz3KL0Dzq/YSMQiZxIQG5wALPT
+ujdEWBF6AmqI8Dc08BnprNRlc/ZpjGSUOnmFKbAWKwyCPwacx/0QK54PLLae4xW/2TYcuiUaUj0a
+7CIMHOCkoj3w6DnPgcB77V0fb8XQC9eY
+-----END CERTIFICATE-----
+
+Deutsche Telekom Root CA 2
+==========================
+-----BEGIN CERTIFICATE-----
+MIIDnzCCAoegAwIBAgIBJjANBgkqhkiG9w0BAQUFADBxMQswCQYDVQQGEwJERTEcMBoGA1UEChMT
+RGV1dHNjaGUgVGVsZWtvbSBBRzEfMB0GA1UECxMWVC1UZWxlU2VjIFRydXN0IENlbnRlcjEjMCEG
+A1UEAxMaRGV1dHNjaGUgVGVsZWtvbSBSb290IENBIDIwHhcNOTkwNzA5MTIxMTAwWhcNMTkwNzA5
+MjM1OTAwWjBxMQswCQYDVQQGEwJERTEcMBoGA1UEChMTRGV1dHNjaGUgVGVsZWtvbSBBRzEfMB0G
+A1UECxMWVC1UZWxlU2VjIFRydXN0IENlbnRlcjEjMCEGA1UEAxMaRGV1dHNjaGUgVGVsZWtvbSBS
+b290IENBIDIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCrC6M14IspFLEUha88EOQ5
+bzVdSq7d6mGNlUn0b2SjGmBmpKlAIoTZ1KXleJMOaAGtuU1cOs7TuKhCQN/Po7qCWWqSG6wcmtoI
+KyUn+WkjR/Hg6yx6m/UTAtB+NHzCnjwAWav12gz1MjwrrFDa1sPeg5TKqAyZMg4ISFZbavva4VhY
+AUlfckE8FQYBjl2tqriTtM2e66foai1SNNs671x1Udrb8zH57nGYMsRUFUQM+ZtV7a3fGAigo4aK
+Se5TBY8ZTNXeWHmb0mocQqvF1afPaA+W5OFhmHZhyJF81j4A4pFQh+GdCuatl9Idxjp9y7zaAzTV
+jlsB9WoHtxa2bkp/AgMBAAGjQjBAMB0GA1UdDgQWBBQxw3kbuvVT1xfgiXotF2wKsyudMzAPBgNV
+HRMECDAGAQH/AgEFMA4GA1UdDwEB/wQEAwIBBjANBgkqhkiG9w0BAQUFAAOCAQEAlGRZrTlk5ynr
+E/5aw4sTV8gEJPB0d8Bg42f76Ymmg7+Wgnxu1MM9756AbrsptJh6sTtU6zkXR34ajgv8HzFZMQSy
+zhfzLMdiNlXiItiJVbSYSKpk+tYcNthEeFpaIzpXl/V6ME+un2pMSyuOoAPjPuCp1NJ70rOo4nI8
+rZ7/gFnkm0W09juwzTkZmDLl6iFhkOQxIY40sfcvNUqFENrnijchvllj4PKFiDFT1FQUhXB59C4G
+dyd1Lx+4ivn+xbrYNuSD7Odlt79jWvNGr4GUN9RBjNYj1h7P9WgbRGOiWrqnNVmh5XAFmw4jV5mU
+Cm26OWMohpLzGITY+9HPBVZkVw==
+-----END CERTIFICATE-----
+
+ComSign CA
+==========
+-----BEGIN CERTIFICATE-----
+MIIDkzCCAnugAwIBAgIQFBOWgxRVjOp7Y+X8NId3RDANBgkqhkiG9w0BAQUFADA0MRMwEQYDVQQD
+EwpDb21TaWduIENBMRAwDgYDVQQKEwdDb21TaWduMQswCQYDVQQGEwJJTDAeFw0wNDAzMjQxMTMy
+MThaFw0yOTAzMTkxNTAyMThaMDQxEzARBgNVBAMTCkNvbVNpZ24gQ0ExEDAOBgNVBAoTB0NvbVNp
+Z24xCzAJBgNVBAYTAklMMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA8ORUaSvTx49q
+ROR+WCf4C9DklBKK8Rs4OC8fMZwG1Cyn3gsqrhqg455qv588x26i+YtkbDqthVVRVKU4VbirgwTy
+P2Q298CNQ0NqZtH3FyrV7zb6MBBC11PN+fozc0yz6YQgitZBJzXkOPqUm7h65HkfM/sb2CEJKHxN
+GGleZIp6GZPKfuzzcuc3B1hZKKxC+cX/zT/npfo4sdAMx9lSGlPWgcxCejVb7Us6eva1jsz/D3zk
+YDaHL63woSV9/9JLEYhwVKZBqGdTUkJe5DSe5L6j7KpiXd3DTKaCQeQzC6zJMw9kglcq/QytNuEM
+rkvF7zuZ2SOzW120V+x0cAwqTwIDAQABo4GgMIGdMAwGA1UdEwQFMAMBAf8wPQYDVR0fBDYwNDAy
+oDCgLoYsaHR0cDovL2ZlZGlyLmNvbXNpZ24uY28uaWwvY3JsL0NvbVNpZ25DQS5jcmwwDgYDVR0P
+AQH/BAQDAgGGMB8GA1UdIwQYMBaAFEsBmz5WGmU2dst7l6qSBe4y5ygxMB0GA1UdDgQWBBRLAZs+
+VhplNnbLe5eqkgXuMucoMTANBgkqhkiG9w0BAQUFAAOCAQEA0Nmlfv4pYEWdfoPPbrxHbvUanlR2
+QnG0PFg/LUAlQvaBnPGJEMgOqnhPOAlXsDzACPw1jvFIUY0McXS6hMTXcpuEfDhOZAYnKuGntewI
+mbQKDdSFc8gS4TXt8QUxHXOZDOuWyt3T5oWq8Ir7dcHyCTxlZWTzTNity4hp8+SDtwy9F1qWF8pb
+/627HOkthIDYIb6FUtnUdLlphbpN7Sgy6/lhSuTENh4Z3G+EER+V9YMoGKgzkkMn3V0TBEVPh9VG
+zT2ouvDzuFYkRes3x+F2T3I5GN9+dHLHcy056mDmrRGiVod7w2ia/viMcKjfZTL0pECMocJEAw6U
+AGegcQCCSA==
+-----END CERTIFICATE-----
+
+ComSign Secured CA
+==================
+-----BEGIN CERTIFICATE-----
+MIIDqzCCApOgAwIBAgIRAMcoRwmzuGxFjB36JPU2TukwDQYJKoZIhvcNAQEFBQAwPDEbMBkGA1UE
+AxMSQ29tU2lnbiBTZWN1cmVkIENBMRAwDgYDVQQKEwdDb21TaWduMQswCQYDVQQGEwJJTDAeFw0w
+NDAzMjQxMTM3MjBaFw0yOTAzMTYxNTA0NTZaMDwxGzAZBgNVBAMTEkNvbVNpZ24gU2VjdXJlZCBD
+QTEQMA4GA1UEChMHQ29tU2lnbjELMAkGA1UEBhMCSUwwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAw
+ggEKAoIBAQDGtWhfHZQVw6QIVS3joFd67+l0Kru5fFdJGhFeTymHDEjWaueP1H5XJLkGieQcPOqs
+49ohgHMhCu95mGwfCP+hUH3ymBvJVG8+pSjsIQQPRbsHPaHA+iqYHU4Gk/v1iDurX8sWv+bznkqH
+7Rnqwp9D5PGBpX8QTz7RSmKtUxvLg/8HZaWSLWapW7ha9B20IZFKF3ueMv5WJDmyVIRD9YTC2LxB
+kMyd1mja6YJQqTtoz7VdApRgFrFD2UNd3V2Hbuq7s8lr9gOUCXDeFhF6K+h2j0kQmHe5Y1yLM5d1
+9guMsqtb3nQgJT/j8xH5h2iGNXHDHYwt6+UarA9z1YJZQIDTAgMBAAGjgacwgaQwDAYDVR0TBAUw
+AwEB/zBEBgNVHR8EPTA7MDmgN6A1hjNodHRwOi8vZmVkaXIuY29tc2lnbi5jby5pbC9jcmwvQ29t
+U2lnblNlY3VyZWRDQS5jcmwwDgYDVR0PAQH/BAQDAgGGMB8GA1UdIwQYMBaAFMFL7XC29z58ADsA
+j8c+DkWfHl3sMB0GA1UdDgQWBBTBS+1wtvc+fAA7AI/HPg5Fnx5d7DANBgkqhkiG9w0BAQUFAAOC
+AQEAFs/ukhNQq3sUnjO2QiBq1BW9Cav8cujvR3qQrFHBZE7piL1DRYHjZiM/EoZNGeQFsOY3wo3a
+BijJD4mkU6l1P7CW+6tMM1X5eCZGbxs2mPtCdsGCuY7e+0X5YxtiOzkGynd6qDwJz2w2PQ8KRUtp
+FhpFfTMDZflScZAmlaxMDPWLkz/MdXSFmLr/YnpNH4n+rr2UAJm/EaXc4HnFFgt9AmEd6oX5AhVP
+51qJThRv4zdLhfXBPGHg/QVBspJ/wx2g0K5SZGBrGMYmnNj1ZOQ2GmKfig8+/21OGVZOIJFsnzQz
+OjRXUDpvgV4GxvU+fE6OK85lBi5d0ipTdF7Tbieejw==
+-----END CERTIFICATE-----
+
+Cybertrust Global Root
+======================
+-----BEGIN CERTIFICATE-----
+MIIDoTCCAomgAwIBAgILBAAAAAABD4WqLUgwDQYJKoZIhvcNAQEFBQAwOzEYMBYGA1UEChMPQ3li
+ZXJ0cnVzdCwgSW5jMR8wHQYDVQQDExZDeWJlcnRydXN0IEdsb2JhbCBSb290MB4XDTA2MTIxNTA4
+MDAwMFoXDTIxMTIxNTA4MDAwMFowOzEYMBYGA1UEChMPQ3liZXJ0cnVzdCwgSW5jMR8wHQYDVQQD
+ExZDeWJlcnRydXN0IEdsb2JhbCBSb290MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA
++Mi8vRRQZhP/8NN57CPytxrHjoXxEnOmGaoQ25yiZXRadz5RfVb23CO21O1fWLE3TdVJDm71aofW
+0ozSJ8bi/zafmGWgE07GKmSb1ZASzxQG9Dvj1Ci+6A74q05IlG2OlTEQXO2iLb3VOm2yHLtgwEZL
+AfVJrn5GitB0jaEMAs7u/OePuGtm839EAL9mJRQr3RAwHQeWP032a7iPt3sMpTjr3kfb1V05/Iin
+89cqdPHoWqI7n1C6poxFNcJQZZXcY4Lv3b93TZxiyWNzFtApD0mpSPCzqrdsxacwOUBdrsTiXSZT
+8M4cIwhhqJQZugRiQOwfOHB3EgZxpzAYXSUnpQIDAQABo4GlMIGiMA4GA1UdDwEB/wQEAwIBBjAP
+BgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBS2CHsNesysIEyGVjJez6tuhS1wVzA/BgNVHR8EODA2
+MDSgMqAwhi5odHRwOi8vd3d3Mi5wdWJsaWMtdHJ1c3QuY29tL2NybC9jdC9jdHJvb3QuY3JsMB8G
+A1UdIwQYMBaAFLYIew16zKwgTIZWMl7Pq26FLXBXMA0GCSqGSIb3DQEBBQUAA4IBAQBW7wojoFRO
+lZfJ+InaRcHUowAl9B8Tq7ejhVhpwjCt2BWKLePJzYFa+HMjWqd8BfP9IjsO0QbE2zZMcwSO5bAi
+5MXzLqXZI+O4Tkogp24CJJ8iYGd7ix1yCcUxXOl5n4BHPa2hCwcUPUf/A2kaDAtE52Mlp3+yybh2
+hO0j9n0Hq0V+09+zv+mKts2oomcrUtW3ZfA5TGOgkXmTUg9U3YO7n9GPp1Nzw8v/MOx8BLjYRB+T
+X3EJIrduPuocA06dGiBh+4E37F78CkWr1+cXVdCg6mCbpvbjjFspwgZgFJ0tl0ypkxWdYcQBX0jW
+WL1WMRJOEcgh4LMRkWXbtKaIOM5V
+-----END CERTIFICATE-----
+
+ePKI Root Certification Authority
+=================================
+-----BEGIN CERTIFICATE-----
+MIIFsDCCA5igAwIBAgIQFci9ZUdcr7iXAF7kBtK8nTANBgkqhkiG9w0BAQUFADBeMQswCQYDVQQG
+EwJUVzEjMCEGA1UECgwaQ2h1bmdod2EgVGVsZWNvbSBDby4sIEx0ZC4xKjAoBgNVBAsMIWVQS0kg
+Um9vdCBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eTAeFw0wNDEyMjAwMjMxMjdaFw0zNDEyMjAwMjMx
+MjdaMF4xCzAJBgNVBAYTAlRXMSMwIQYDVQQKDBpDaHVuZ2h3YSBUZWxlY29tIENvLiwgTHRkLjEq
+MCgGA1UECwwhZVBLSSBSb290IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MIICIjANBgkqhkiG9w0B
+AQEFAAOCAg8AMIICCgKCAgEA4SUP7o3biDN1Z82tH306Tm2d0y8U82N0ywEhajfqhFAHSyZbCUNs
+IZ5qyNUD9WBpj8zwIuQf5/dqIjG3LBXy4P4AakP/h2XGtRrBp0xtInAhijHyl3SJCRImHJ7K2RKi
+lTza6We/CKBk49ZCt0Xvl/T29de1ShUCWH2YWEtgvM3XDZoTM1PRYfl61dd4s5oz9wCGzh1NlDiv
+qOx4UXCKXBCDUSH3ET00hl7lSM2XgYI1TBnsZfZrxQWh7kcT1rMhJ5QQCtkkO7q+RBNGMD+XPNjX
+12ruOzjjK9SXDrkb5wdJfzcq+Xd4z1TtW0ado4AOkUPB1ltfFLqfpo0kR0BZv3I4sjZsN/+Z0V0O
+WQqraffAsgRFelQArr5T9rXn4fg8ozHSqf4hUmTFpmfwdQcGlBSBVcYn5AGPF8Fqcde+S/uUWH1+
+ETOxQvdibBjWzwloPn9s9h6PYq2lY9sJpx8iQkEeb5mKPtf5P0B6ebClAZLSnT0IFaUQAS2zMnao
+lQ2zepr7BxB4EW/hj8e6DyUadCrlHJhBmd8hh+iVBmoKs2pHdmX2Os+PYhcZewoozRrSgx4hxyy/
+vv9haLdnG7t4TY3OZ+XkwY63I2binZB1NJipNiuKmpS5nezMirH4JYlcWrYvjB9teSSnUmjDhDXi
+Zo1jDiVN1Rmy5nk3pyKdVDECAwEAAaNqMGgwHQYDVR0OBBYEFB4M97Zn8uGSJglFwFU5Lnc/Qkqi
+MAwGA1UdEwQFMAMBAf8wOQYEZyoHAAQxMC8wLQIBADAJBgUrDgMCGgUAMAcGBWcqAwAABBRFsMLH
+ClZ87lt4DJX5GFPBphzYEDANBgkqhkiG9w0BAQUFAAOCAgEACbODU1kBPpVJufGBuvl2ICO1J2B0
+1GqZNF5sAFPZn/KmsSQHRGoqxqWOeBLoR9lYGxMqXnmbnwoqZ6YlPwZpVnPDimZI+ymBV3QGypzq
+KOg4ZyYr8dW1P2WT+DZdjo2NQCCHGervJ8A9tDkPJXtoUHRVnAxZfVo9QZQlUgjgRywVMRnVvwdV
+xrsStZf0X4OFunHB2WyBEXYKCrC/gpf36j36+uwtqSiUO1bd0lEursC9CBWMd1I0ltabrNMdjmEP
+NXubrjlpC2JgQCA2j6/7Nu4tCEoduL+bXPjqpRugc6bY+G7gMwRfaKonh+3ZwZCc7b3jajWvY9+r
+GNm65ulK6lCKD2GTHuItGeIwlDWSXQ62B68ZgI9HkFFLLk3dheLSClIKF5r8GrBQAuUBo2M3IUxE
+xJtRmREOc5wGj1QupyheRDmHVi03vYVElOEMSyycw5KFNGHLD7ibSkNS/jQ6fbjpKdx2qcgw+BRx
+gMYeNkh0IkFch4LoGHGLQYlE535YW6i4jRPpp2zDR+2zGp1iro2C6pSe3VkQw63d4k3jMdXH7Ojy
+sP6SHhYKGvzZ8/gntsm+HbRsZJB/9OTEW9c3rkIO3aQab3yIVMUWbuF6aC74Or8NpDyJO3inTmOD
+BCEIZ43ygknQW/2xzQ+DhNQ+IIX3Sj0rnP0qCglN6oH4EZw=
+-----END CERTIFICATE-----
+
+T\xc3\x9c\x42\xC4\xB0TAK UEKAE K\xC3\xB6k Sertifika Hizmet Sa\xC4\x9Flay\xc4\xb1\x63\xc4\xb1s\xc4\xb1 - S\xC3\xBCr\xC3\xBCm 3
+=============================================================================================================================
+-----BEGIN CERTIFICATE-----
+MIIFFzCCA/+gAwIBAgIBETANBgkqhkiG9w0BAQUFADCCASsxCzAJBgNVBAYTAlRSMRgwFgYDVQQH
+DA9HZWJ6ZSAtIEtvY2FlbGkxRzBFBgNVBAoMPlTDvHJraXllIEJpbGltc2VsIHZlIFRla25vbG9q
+aWsgQXJhxZ90xLFybWEgS3VydW11IC0gVMOcQsSwVEFLMUgwRgYDVQQLDD9VbHVzYWwgRWxla3Ry
+b25payB2ZSBLcmlwdG9sb2ppIEFyYcWfdMSxcm1hIEVuc3RpdMO8c8O8IC0gVUVLQUUxIzAhBgNV
+BAsMGkthbXUgU2VydGlmaWthc3lvbiBNZXJrZXppMUowSAYDVQQDDEFUw5xCxLBUQUsgVUVLQUUg
+S8O2ayBTZXJ0aWZpa2EgSGl6bWV0IFNhxJ9sYXnEsWPEsXPEsSAtIFPDvHLDvG0gMzAeFw0wNzA4
+MjQxMTM3MDdaFw0xNzA4MjExMTM3MDdaMIIBKzELMAkGA1UEBhMCVFIxGDAWBgNVBAcMD0dlYnpl
+IC0gS29jYWVsaTFHMEUGA1UECgw+VMO8cmtpeWUgQmlsaW1zZWwgdmUgVGVrbm9sb2ppayBBcmHF
+n3TEsXJtYSBLdXJ1bXUgLSBUw5xCxLBUQUsxSDBGBgNVBAsMP1VsdXNhbCBFbGVrdHJvbmlrIHZl
+IEtyaXB0b2xvamkgQXJhxZ90xLFybWEgRW5zdGl0w7xzw7wgLSBVRUtBRTEjMCEGA1UECwwaS2Ft
+dSBTZXJ0aWZpa2FzeW9uIE1lcmtlemkxSjBIBgNVBAMMQVTDnELEsFRBSyBVRUtBRSBLw7ZrIFNl
+cnRpZmlrYSBIaXptZXQgU2HEn2xhecSxY8Sxc8SxIC0gU8O8csO8bSAzMIIBIjANBgkqhkiG9w0B
+AQEFAAOCAQ8AMIIBCgKCAQEAim1L/xCIOsP2fpTo6iBkcK4hgb46ezzb8R1Sf1n68yJMlaCQvEhO
+Eav7t7WNeoMojCZG2E6VQIdhn8WebYGHV2yKO7Rm6sxA/OOqbLLLAdsyv9Lrhc+hDVXDWzhXcLh1
+xnnRFDDtG1hba+818qEhTsXOfJlfbLm4IpNQp81McGq+agV/E5wrHur+R84EpW+sky58K5+eeROR
+6Oqeyjh1jmKwlZMq5d/pXpduIF9fhHpEORlAHLpVK/swsoHvhOPc7Jg4OQOFCKlUAwUp8MmPi+oL
+hmUZEdPpCSPeaJMDyTYcIW7OjGbxmTDY17PDHfiBLqi9ggtm/oLL4eAagsNAgQIDAQABo0IwQDAd
+BgNVHQ4EFgQUvYiHyY/2pAoLquvF/pEjnatKijIwDgYDVR0PAQH/BAQDAgEGMA8GA1UdEwEB/wQF
+MAMBAf8wDQYJKoZIhvcNAQEFBQADggEBAB18+kmPNOm3JpIWmgV050vQbTlswyb2zrgxvMTfvCr4
+N5EY3ATIZJkrGG2AA1nJrvhY0D7twyOfaTyGOBye79oneNGEN3GKPEs5z35FBtYt2IpNeBLWrcLT
+y9LQQfMmNkqblWwM7uXRQydmwYj3erMgbOqwaSvHIOgMA8RBBZniP+Rr+KCGgceExh/VS4ESshYh
+LBOhgLJeDEoTniDYYkCrkOpkSi+sDQESeUWoL4cZaMjihccwsnX5OD+ywJO0a+IDRM5noN+J1q2M
+dqMTw5RhK2vZbMEHCiIHhWyFJEapvj+LeISCfiQMnf2BN+MlqO02TpUsyZyQ2uypQjyttgI=
+-----END CERTIFICATE-----
+
+Buypass Class 2 CA 1
+====================
+-----BEGIN CERTIFICATE-----
+MIIDUzCCAjugAwIBAgIBATANBgkqhkiG9w0BAQUFADBLMQswCQYDVQQGEwJOTzEdMBsGA1UECgwU
+QnV5cGFzcyBBUy05ODMxNjMzMjcxHTAbBgNVBAMMFEJ1eXBhc3MgQ2xhc3MgMiBDQSAxMB4XDTA2
+MTAxMzEwMjUwOVoXDTE2MTAxMzEwMjUwOVowSzELMAkGA1UEBhMCTk8xHTAbBgNVBAoMFEJ1eXBh
+c3MgQVMtOTgzMTYzMzI3MR0wGwYDVQQDDBRCdXlwYXNzIENsYXNzIDIgQ0EgMTCCASIwDQYJKoZI
+hvcNAQEBBQADggEPADCCAQoCggEBAIs8B0XY9t/mx8q6jUPFR42wWsE425KEHK8T1A9vNkYgxC7M
+cXA0ojTTNy7Y3Tp3L8DrKehc0rWpkTSHIln+zNvnma+WwajHQN2lFYxuyHyXA8vmIPLXl18xoS83
+0r7uvqmtqEyeIWZDO6i88wmjONVZJMHCR3axiFyCO7srpgTXjAePzdVBHfCuuCkslFJgNJQ72uA4
+0Z0zPhX0kzLFANq1KWYOOngPIVJfAuWSeyXTkh4vFZ2B5J2O6O+JzhRMVB0cgRJNcKi+EAUXfh/R
+uFdV7c27UsKwHnjCTTZoy1YmwVLBvXb3WNVyfh9EdrsAiR0WnVE1703CVu9r4Iw7DekCAwEAAaNC
+MEAwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUP42aWYv8e3uco684sDntkHGA1sgwDgYDVR0P
+AQH/BAQDAgEGMA0GCSqGSIb3DQEBBQUAA4IBAQAVGn4TirnoB6NLJzKyQJHyIdFkhb5jatLPgcIV
+1Xp+DCmsNx4cfHZSldq1fyOhKXdlyTKdqC5Wq2B2zha0jX94wNWZUYN/Xtm+DKhQ7SLHrQVMdvvt
+7h5HZPb3J31cKA9FxVxiXqaakZG3Uxcu3K1gnZZkOb1naLKuBctN518fV4bVIJwo+28TOPX2EZL2
+fZleHwzoq0QkKXJAPTZSr4xYkHPB7GEseaHsh7U/2k3ZIQAw3pDaDtMaSKk+hQsUi4y8QZ5q9w5w
+wDX3OaJdZtB7WZ+oRxKaJyOkLY4ng5IgodcVf/EuGO70SH8vf/GhGLWhC5SgYiAynB321O+/TIho
+-----END CERTIFICATE-----
+
+Buypass Class 3 CA 1
+====================
+-----BEGIN CERTIFICATE-----
+MIIDUzCCAjugAwIBAgIBAjANBgkqhkiG9w0BAQUFADBLMQswCQYDVQQGEwJOTzEdMBsGA1UECgwU
+QnV5cGFzcyBBUy05ODMxNjMzMjcxHTAbBgNVBAMMFEJ1eXBhc3MgQ2xhc3MgMyBDQSAxMB4XDTA1
+MDUwOTE0MTMwM1oXDTE1MDUwOTE0MTMwM1owSzELMAkGA1UEBhMCTk8xHTAbBgNVBAoMFEJ1eXBh
+c3MgQVMtOTgzMTYzMzI3MR0wGwYDVQQDDBRCdXlwYXNzIENsYXNzIDMgQ0EgMTCCASIwDQYJKoZI
+hvcNAQEBBQADggEPADCCAQoCggEBAKSO13TZKWTeXx+HgJHqTjnmGcZEC4DVC69TB4sSveZn8AKx
+ifZgisRbsELRwCGoy+Gb72RRtqfPFfV0gGgEkKBYouZ0plNTVUhjP5JW3SROjvi6K//zNIqeKNc0
+n6wv1g/xpC+9UrJJhW05NfBEMJNGJPO251P7vGGvqaMU+8IXF4Rs4HyI+MkcVyzwPX6UvCWThOia
+AJpFBUJXgPROztmuOfbIUxAMZTpHe2DC1vqRycZxbL2RhzyRhkmr8w+gbCZ2Xhysm3HljbybIR6c
+1jh+JIAVMYKWsUnTYjdbiAwKYjT+p0h+mbEwi5A3lRyoH6UsjfRVyNvdWQrCrXig9IsCAwEAAaNC
+MEAwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUOBTmyPCppAP0Tj4io1vy1uCtQHQwDgYDVR0P
+AQH/BAQDAgEGMA0GCSqGSIb3DQEBBQUAA4IBAQABZ6OMySU9E2NdFm/soT4JXJEVKirZgCFPBdy7
+pYmrEzMqnji3jG8CcmPHc3ceCQa6Oyh7pEfJYWsICCD8igWKH7y6xsL+z27sEzNxZy5p+qksP2bA
+EllNC1QCkoS72xLvg3BweMhT+t/Gxv/ciC8HwEmdMldg0/L2mSlf56oBzKwzqBwKu5HEA6BvtjT5
+htOzdlSY9EqBs1OdTUDs5XcTRa9bqh/YL0yCe/4qxFi7T/ye/QNlGioOw6UgFpRreaaiErS7GqQj
+el/wroQk5PMr+4okoyeYZdowdXb8GZHo2+ubPzK/QJcHJrrM85SFSnonk8+QQtS4Wxam58tAA915
+-----END CERTIFICATE-----
+
+EBG Elektronik Sertifika Hizmet Sa\xC4\x9Flay\xc4\xb1\x63\xc4\xb1s\xc4\xb1
+==========================================================================
+-----BEGIN CERTIFICATE-----
+MIIF5zCCA8+gAwIBAgIITK9zQhyOdAIwDQYJKoZIhvcNAQEFBQAwgYAxODA2BgNVBAMML0VCRyBF
+bGVrdHJvbmlrIFNlcnRpZmlrYSBIaXptZXQgU2HEn2xhecSxY8Sxc8SxMTcwNQYDVQQKDC5FQkcg
+QmlsacWfaW0gVGVrbm9sb2ppbGVyaSB2ZSBIaXptZXRsZXJpIEEuxZ4uMQswCQYDVQQGEwJUUjAe
+Fw0wNjA4MTcwMDIxMDlaFw0xNjA4MTQwMDMxMDlaMIGAMTgwNgYDVQQDDC9FQkcgRWxla3Ryb25p
+ayBTZXJ0aWZpa2EgSGl6bWV0IFNhxJ9sYXnEsWPEsXPEsTE3MDUGA1UECgwuRUJHIEJpbGnFn2lt
+IFRla25vbG9qaWxlcmkgdmUgSGl6bWV0bGVyaSBBLsWeLjELMAkGA1UEBhMCVFIwggIiMA0GCSqG
+SIb3DQEBAQUAA4ICDwAwggIKAoICAQDuoIRh0DpqZhAy2DE4f6en5f2h4fuXd7hxlugTlkaDT7by
+X3JWbhNgpQGR4lvFzVcfd2NR/y8927k/qqk153nQ9dAktiHq6yOU/im/+4mRDGSaBUorzAzu8T2b
+gmmkTPiab+ci2hC6X5L8GCcKqKpE+i4stPtGmggDg3KriORqcsnlZR9uKg+ds+g75AxuetpX/dfr
+eYteIAbTdgtsApWjluTLdlHRKJ2hGvxEok3MenaoDT2/F08iiFD9rrbskFBKW5+VQarKD7JK/oCZ
+TqNGFav4c0JqwmZ2sQomFd2TkuzbqV9UIlKRcF0T6kjsbgNs2d1s/OsNA/+mgxKb8amTD8UmTDGy
+Y5lhcucqZJnSuOl14nypqZoaqsNW2xCaPINStnuWt6yHd6i58mcLlEOzrz5z+kI2sSXFCjEmN1Zn
+uqMLfdb3ic1nobc6HmZP9qBVFCVMLDMNpkGMvQQxahByCp0OLna9XvNRiYuoP1Vzv9s6xiQFlpJI
+qkuNKgPlV5EQ9GooFW5Hd4RcUXSfGenmHmMWOeMRFeNYGkS9y8RsZteEBt8w9DeiQyJ50hBs37vm
+ExH8nYQKE3vwO9D8owrXieqWfo1IhR5kX9tUoqzVegJ5a9KK8GfaZXINFHDk6Y54jzJ0fFfy1tb0
+Nokb+Clsi7n2l9GkLqq+CxnCRelwXQIDAJ3Zo2MwYTAPBgNVHRMBAf8EBTADAQH/MA4GA1UdDwEB
+/wQEAwIBBjAdBgNVHQ4EFgQU587GT/wWZ5b6SqMHwQSny2re2kcwHwYDVR0jBBgwFoAU587GT/wW
+Z5b6SqMHwQSny2re2kcwDQYJKoZIhvcNAQEFBQADggIBAJuYml2+8ygjdsZs93/mQJ7ANtyVDR2t
+FcU22NU57/IeIl6zgrRdu0waypIN30ckHrMk2pGI6YNw3ZPX6bqz3xZaPt7gyPvT/Wwp+BVGoGgm
+zJNSroIBk5DKd8pNSe/iWtkqvTDOTLKBtjDOWU/aWR1qeqRFsIImgYZ29fUQALjuswnoT4cCB64k
+XPBfrAowzIpAoHMEwfuJJPaaHFy3PApnNgUIMbOv2AFoKuB4j3TeuFGkjGwgPaL7s9QJ/XvCgKqT
+bCmYIai7FvOpEl90tYeY8pUm3zTvilORiF0alKM/fCL414i6poyWqD1SNGKfAB5UVUJnxk1Gj7sU
+RT0KlhaOEKGXmdXTMIXM3rRyt7yKPBgpaP3ccQfuJDlq+u2lrDgv+R4QDgZxGhBM/nV+/x5XOULK
+1+EVoVZVWRvRo68R2E7DpSvvkL/A7IITW43WciyTTo9qKd+FPNMN4KIYEsxVL0e3p5sC/kH2iExt
+2qkBR4NkJ2IQgtYSe14DHzSpyZH+r11thie3I6p1GMog57AP14kOpmciY/SDQSsGS7tY1dHXt7kQ
+Y9iJSrSq3RZj9W6+YKH47ejWkE8axsWgKdOnIaj1Wjz3x0miIZpKlVIglnKaZsv30oZDfCK+lvm9
+AahH3eU7QPl1K5srRmSGjR70j/sHd9DqSaIcjVIUpgqT
+-----END CERTIFICATE-----
+
+certSIGN ROOT CA
+================
+-----BEGIN CERTIFICATE-----
+MIIDODCCAiCgAwIBAgIGIAYFFnACMA0GCSqGSIb3DQEBBQUAMDsxCzAJBgNVBAYTAlJPMREwDwYD
+VQQKEwhjZXJ0U0lHTjEZMBcGA1UECxMQY2VydFNJR04gUk9PVCBDQTAeFw0wNjA3MDQxNzIwMDRa
+Fw0zMTA3MDQxNzIwMDRaMDsxCzAJBgNVBAYTAlJPMREwDwYDVQQKEwhjZXJ0U0lHTjEZMBcGA1UE
+CxMQY2VydFNJR04gUk9PVCBDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALczuX7I
+JUqOtdu0KBuqV5Do0SLTZLrTk+jUrIZhQGpgV2hUhE28alQCBf/fm5oqrl0Hj0rDKH/v+yv6efHH
+rfAQUySQi2bJqIirr1qjAOm+ukbuW3N7LBeCgV5iLKECZbO9xSsAfsT8AzNXDe3i+s5dRdY4zTW2
+ssHQnIFKquSyAVwdj1+ZxLGt24gh65AIgoDzMKND5pCCrlUoSe1b16kQOA7+j0xbm0bqQfWwCHTD
+0IgztnzXdN/chNFDDnU5oSVAKOp4yw4sLjmdjItuFhwvJoIQ4uNllAoEwF73XVv4EOLQunpL+943
+AAAaWyjj0pxzPjKHmKHJUS/X3qwzs08CAwEAAaNCMEAwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8B
+Af8EBAMCAcYwHQYDVR0OBBYEFOCMm9slSbPxfIbWskKHC9BroNnkMA0GCSqGSIb3DQEBBQUAA4IB
+AQA+0hyJLjX8+HXd5n9liPRyTMks1zJO890ZeUe9jjtbkw9QSSQTaxQGcu8J06Gh40CEyecYMnQ8
+SG4Pn0vU9x7Tk4ZkVJdjclDVVc/6IJMCopvDI5NOFlV2oHB5bc0hH88vLbwZ44gx+FkagQnIl6Z0
+x2DEW8xXjrJ1/RsCCdtZb3KTafcxQdaIOL+Hsr0Wefmq5L6IJd1hJyMctTEHBDa0GpC9oHRxUIlt
+vBTjD4au8as+x6AJzKNI0eDbZOeStc+vckNwi/nDhDwTqn6Sm1dTk/pwwpEOMfmbZ13pljheX7Nz
+TogVZ96edhBiIL5VaZVDADlN9u6wWk5JRFRYX0KD
+-----END CERTIFICATE-----
+
+CNNIC ROOT
+==========
+-----BEGIN CERTIFICATE-----
+MIIDVTCCAj2gAwIBAgIESTMAATANBgkqhkiG9w0BAQUFADAyMQswCQYDVQQGEwJDTjEOMAwGA1UE
+ChMFQ05OSUMxEzARBgNVBAMTCkNOTklDIFJPT1QwHhcNMDcwNDE2MDcwOTE0WhcNMjcwNDE2MDcw
+OTE0WjAyMQswCQYDVQQGEwJDTjEOMAwGA1UEChMFQ05OSUMxEzARBgNVBAMTCkNOTklDIFJPT1Qw
+ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDTNfc/c3et6FtzF8LRb+1VvG7q6KR5smzD
+o+/hn7E7SIX1mlwhIhAsxYLO2uOabjfhhyzcuQxauohV3/2q2x8x6gHx3zkBwRP9SFIhxFXf2tiz
+VHa6dLG3fdfA6PZZxU3Iva0fFNrfWEQlMhkqx35+jq44sDB7R3IJMfAw28Mbdim7aXZOV/kbZKKT
+VrdvmW7bCgScEeOAH8tjlBAKqeFkgjH5jCftppkA9nCTGPihNIaj3XrCGHn2emU1z5DrvTOTn1Or
+czvmmzQgLx3vqR1jGqCA2wMv+SYahtKNu6m+UjqHZ0gNv7Sg2Ca+I19zN38m5pIEo3/PIKe38zrK
+y5nLAgMBAAGjczBxMBEGCWCGSAGG+EIBAQQEAwIABzAfBgNVHSMEGDAWgBRl8jGtKvf33VKWCscC
+wQ7vptU7ETAPBgNVHRMBAf8EBTADAQH/MAsGA1UdDwQEAwIB/jAdBgNVHQ4EFgQUZfIxrSr3991S
+lgrHAsEO76bVOxEwDQYJKoZIhvcNAQEFBQADggEBAEs17szkrr/Dbq2flTtLP1se31cpolnKOOK5
+Gv+e5m4y3R6u6jW39ZORTtpC4cMXYFDy0VwmuYK36m3knITnA3kXr5g9lNvHugDnuL8BV8F3RTIM
+O/G0HAiw/VGgod2aHRM2mm23xzy54cXZF/qD1T0VoDy7HgviyJA/qIYM/PmLXoXLT1tLYhFHxUV8
+BS9BsZ4QaRuZluBVeftOhpm4lNqGOGqTo+fLbuXf6iFViZx9fX+Y9QCJ7uOEwFyWtcVG6kbghVW2
+G8kS1sHNzYDzAgE8yGnLRUhj2JTQ7IUOO04RZfSCjKY9ri4ilAnIXOo8gV0WKgOXFlUJ24pBgp5m
+mxE=
+-----END CERTIFICATE-----
+
+ApplicationCA - Japanese Government
+===================================
+-----BEGIN CERTIFICATE-----
+MIIDoDCCAoigAwIBAgIBMTANBgkqhkiG9w0BAQUFADBDMQswCQYDVQQGEwJKUDEcMBoGA1UEChMT
+SmFwYW5lc2UgR292ZXJubWVudDEWMBQGA1UECxMNQXBwbGljYXRpb25DQTAeFw0wNzEyMTIxNTAw
+MDBaFw0xNzEyMTIxNTAwMDBaMEMxCzAJBgNVBAYTAkpQMRwwGgYDVQQKExNKYXBhbmVzZSBHb3Zl
+cm5tZW50MRYwFAYDVQQLEw1BcHBsaWNhdGlvbkNBMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIB
+CgKCAQEAp23gdE6Hj6UG3mii24aZS2QNcfAKBZuOquHMLtJqO8F6tJdhjYq+xpqcBrSGUeQ3DnR4
+fl+Kf5Sk10cI/VBaVuRorChzoHvpfxiSQE8tnfWuREhzNgaeZCw7NCPbXCbkcXmP1G55IrmTwcrN
+wVbtiGrXoDkhBFcsovW8R0FPXjQilbUfKW1eSvNNcr5BViCH/OlQR9cwFO5cjFW6WY2H/CPek9AE
+jP3vbb3QesmlOmpyM8ZKDQUXKi17safY1vC+9D/qDihtQWEjdnjDuGWk81quzMKq2edY3rZ+nYVu
+nyoKb58DKTCXKB28t89UKU5RMfkntigm/qJj5kEW8DOYRwIDAQABo4GeMIGbMB0GA1UdDgQWBBRU
+WssmP3HMlEYNllPqa0jQk/5CdTAOBgNVHQ8BAf8EBAMCAQYwWQYDVR0RBFIwUKROMEwxCzAJBgNV
+BAYTAkpQMRgwFgYDVQQKDA/ml6XmnKzlm73mlL/lupwxIzAhBgNVBAsMGuOCouODl+ODquOCseOD
+vOOCt+ODp+ODs0NBMA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZIhvcNAQEFBQADggEBADlqRHZ3ODrs
+o2dGD/mLBqj7apAxzn7s2tGJfHrrLgy9mTLnsCTWw//1sogJhyzjVOGjprIIC8CFqMjSnHH2HZ9g
+/DgzE+Ge3Atf2hZQKXsvcJEPmbo0NI2VdMV+eKlmXb3KIXdCEKxmJj3ekav9FfBv7WxfEPjzFvYD
+io+nEhEMy/0/ecGc/WLuo89UDNErXxc+4z6/wCs+CZv+iKZ+tJIX/COUgb1up8WMwusRRdv4QcmW
+dupwX3kSa+SjB1oF7ydJzyGfikwJcGapJsErEU4z0g781mzSDjJkaP+tBXhfAx2o45CsJOAPQKdL
+rosot4LKGAfmt1t06SAZf7IbiVQ=
+-----END CERTIFICATE-----
+
+GeoTrust Primary Certification Authority - G3
+=============================================
+-----BEGIN CERTIFICATE-----
+MIID/jCCAuagAwIBAgIQFaxulBmyeUtB9iepwxgPHzANBgkqhkiG9w0BAQsFADCBmDELMAkGA1UE
+BhMCVVMxFjAUBgNVBAoTDUdlb1RydXN0IEluYy4xOTA3BgNVBAsTMChjKSAyMDA4IEdlb1RydXN0
+IEluYy4gLSBGb3IgYXV0aG9yaXplZCB1c2Ugb25seTE2MDQGA1UEAxMtR2VvVHJ1c3QgUHJpbWFy
+eSBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eSAtIEczMB4XDTA4MDQwMjAwMDAwMFoXDTM3MTIwMTIz
+NTk1OVowgZgxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1HZW9UcnVzdCBJbmMuMTkwNwYDVQQLEzAo
+YykgMjAwOCBHZW9UcnVzdCBJbmMuIC0gRm9yIGF1dGhvcml6ZWQgdXNlIG9ubHkxNjA0BgNVBAMT
+LUdlb1RydXN0IFByaW1hcnkgQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkgLSBHMzCCASIwDQYJKoZI
+hvcNAQEBBQADggEPADCCAQoCggEBANziXmJYHTNXOTIz+uvLh4yn1ErdBojqZI4xmKU4kB6Yzy5j
+K/BGvESyiaHAKAxJcCGVn2TAppMSAmUmhsalifD614SgcK9PGpc/BkTVyetyEH3kMSj7HGHmKAdE
+c5IiaacDiGydY8hS2pgn5whMcD60yRLBxWeDXTPzAxHsatBT4tG6NmCUgLthY2xbF37fQJQeqw3C
+IShwiP/WJmxsYAQlTlV+fe+/lEjetx3dcI0FX4ilm/LC7urRQEFtYjgdVgbFA0dRIBn8exALDmKu
+dlW/X3e+PkkBUz2YJQN2JFodtNuJ6nnltrM7P7pMKEF/BqxqjsHQ9gUdfeZChuOl1UcCAwEAAaNC
+MEAwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwHQYDVR0OBBYEFMR5yo6hTgMdHNxr
+2zFblD4/MH8tMA0GCSqGSIb3DQEBCwUAA4IBAQAtxRPPVoB7eni9n64smefv2t+UXglpp+duaIy9
+cr5HqQ6XErhK8WTTOd8lNNTBzU6B8A8ExCSzNJbGpqow32hhc9f5joWJ7w5elShKKiePEI4ufIbE
+Ap7aDHdlDkQNkv39sxY2+hENHYwOB4lqKVb3cvTdFZx3NWZXqxNT2I7BQMXXExZacse3aQHEerGD
+AWh9jUGhlBjBJVz88P6DAod8DQ3PLghcSkANPuyBYeYk28rgDi0Hsj5W3I31QYUHSJsMC8tJP33s
+t/3LjWeJGqvtux6jAAgIFyqCXDFdRootD4abdNlF+9RAsXqqaC2Gspki4cErx5z481+oghLrGREt
+-----END CERTIFICATE-----
+
+thawte Primary Root CA - G2
+===========================
+-----BEGIN CERTIFICATE-----
+MIICiDCCAg2gAwIBAgIQNfwmXNmET8k9Jj1Xm67XVjAKBggqhkjOPQQDAzCBhDELMAkGA1UEBhMC
+VVMxFTATBgNVBAoTDHRoYXd0ZSwgSW5jLjE4MDYGA1UECxMvKGMpIDIwMDcgdGhhd3RlLCBJbmMu
+IC0gRm9yIGF1dGhvcml6ZWQgdXNlIG9ubHkxJDAiBgNVBAMTG3RoYXd0ZSBQcmltYXJ5IFJvb3Qg
+Q0EgLSBHMjAeFw0wNzExMDUwMDAwMDBaFw0zODAxMTgyMzU5NTlaMIGEMQswCQYDVQQGEwJVUzEV
+MBMGA1UEChMMdGhhd3RlLCBJbmMuMTgwNgYDVQQLEy8oYykgMjAwNyB0aGF3dGUsIEluYy4gLSBG
+b3IgYXV0aG9yaXplZCB1c2Ugb25seTEkMCIGA1UEAxMbdGhhd3RlIFByaW1hcnkgUm9vdCBDQSAt
+IEcyMHYwEAYHKoZIzj0CAQYFK4EEACIDYgAEotWcgnuVnfFSeIf+iha/BebfowJPDQfGAFG6DAJS
+LSKkQjnE/o/qycG+1E3/n3qe4rF8mq2nhglzh9HnmuN6papu+7qzcMBniKI11KOasf2twu8x+qi5
+8/sIxpHR+ymVo0IwQDAPBgNVHRMBAf8EBTADAQH/MA4GA1UdDwEB/wQEAwIBBjAdBgNVHQ4EFgQU
+mtgAMADna3+FGO6Lts6KDPgR4bswCgYIKoZIzj0EAwMDaQAwZgIxAN344FdHW6fmCsO99YCKlzUN
+G4k8VIZ3KMqh9HneteY4sPBlcIx/AlTCv//YoT7ZzwIxAMSNlPzcU9LcnXgWHxUzI1NS41oxXZ3K
+rr0TKUQNJ1uo52icEvdYPy5yAlejj6EULg==
+-----END CERTIFICATE-----
+
+thawte Primary Root CA - G3
+===========================
+-----BEGIN CERTIFICATE-----
+MIIEKjCCAxKgAwIBAgIQYAGXt0an6rS0mtZLL/eQ+zANBgkqhkiG9w0BAQsFADCBrjELMAkGA1UE
+BhMCVVMxFTATBgNVBAoTDHRoYXd0ZSwgSW5jLjEoMCYGA1UECxMfQ2VydGlmaWNhdGlvbiBTZXJ2
+aWNlcyBEaXZpc2lvbjE4MDYGA1UECxMvKGMpIDIwMDggdGhhd3RlLCBJbmMuIC0gRm9yIGF1dGhv
+cml6ZWQgdXNlIG9ubHkxJDAiBgNVBAMTG3RoYXd0ZSBQcmltYXJ5IFJvb3QgQ0EgLSBHMzAeFw0w
+ODA0MDIwMDAwMDBaFw0zNzEyMDEyMzU5NTlaMIGuMQswCQYDVQQGEwJVUzEVMBMGA1UEChMMdGhh
+d3RlLCBJbmMuMSgwJgYDVQQLEx9DZXJ0aWZpY2F0aW9uIFNlcnZpY2VzIERpdmlzaW9uMTgwNgYD
+VQQLEy8oYykgMjAwOCB0aGF3dGUsIEluYy4gLSBGb3IgYXV0aG9yaXplZCB1c2Ugb25seTEkMCIG
+A1UEAxMbdGhhd3RlIFByaW1hcnkgUm9vdCBDQSAtIEczMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8A
+MIIBCgKCAQEAsr8nLPvb2FvdeHsbnndmgcs+vHyu86YnmjSjaDFxODNi5PNxZnmxqWWjpYvVj2At
+P0LMqmsywCPLLEHd5N/8YZzic7IilRFDGF/Eth9XbAoFWCLINkw6fKXRz4aviKdEAhN0cXMKQlkC
++BsUa0Lfb1+6a4KinVvnSr0eAXLbS3ToO39/fR8EtCab4LRarEc9VbjXsCZSKAExQGbY2SS99irY
+7CFJXJv2eul/VTV+lmuNk5Mny5K76qxAwJ/C+IDPXfRa3M50hqY+bAtTyr2SzhkGcuYMXDhpxwTW
+vGzOW/b3aJzcJRVIiKHpqfiYnODz1TEoYRFsZ5aNOZnLwkUkOQIDAQABo0IwQDAPBgNVHRMBAf8E
+BTADAQH/MA4GA1UdDwEB/wQEAwIBBjAdBgNVHQ4EFgQUrWyqlGCc7eT/+j4KdCtjA/e2Wb8wDQYJ
+KoZIhvcNAQELBQADggEBABpA2JVlrAmSicY59BDlqQ5mU1143vokkbvnRFHfxhY0Cu9qRFHqKweK
+A3rD6z8KLFIWoCtDuSWQP3CpMyVtRRooOyfPqsMpQhvfO0zAMzRbQYi/aytlryjvsvXDqmbOe1bu
+t8jLZ8HJnBoYuMTDSQPxYA5QzUbF83d597YV4Djbxy8ooAw/dyZ02SUS2jHaGh7cKUGRIjxpp7sC
+8rZcJwOJ9Abqm+RyguOhCcHpABnTPtRwa7pxpqpYrvS76Wy274fMm7v/OeZWYdMKp8RcTGB7BXcm
+er/YB1IsYvdwY9k5vG8cwnncdimvzsUsZAReiDZuMdRAGmI0Nj81Aa6sY6A=
+-----END CERTIFICATE-----
+
+GeoTrust Primary Certification Authority - G2
+=============================================
+-----BEGIN CERTIFICATE-----
+MIICrjCCAjWgAwIBAgIQPLL0SAoA4v7rJDteYD7DazAKBggqhkjOPQQDAzCBmDELMAkGA1UEBhMC
+VVMxFjAUBgNVBAoTDUdlb1RydXN0IEluYy4xOTA3BgNVBAsTMChjKSAyMDA3IEdlb1RydXN0IElu
+Yy4gLSBGb3IgYXV0aG9yaXplZCB1c2Ugb25seTE2MDQGA1UEAxMtR2VvVHJ1c3QgUHJpbWFyeSBD
+ZXJ0aWZpY2F0aW9uIEF1dGhvcml0eSAtIEcyMB4XDTA3MTEwNTAwMDAwMFoXDTM4MDExODIzNTk1
+OVowgZgxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1HZW9UcnVzdCBJbmMuMTkwNwYDVQQLEzAoYykg
+MjAwNyBHZW9UcnVzdCBJbmMuIC0gRm9yIGF1dGhvcml6ZWQgdXNlIG9ubHkxNjA0BgNVBAMTLUdl
+b1RydXN0IFByaW1hcnkgQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkgLSBHMjB2MBAGByqGSM49AgEG
+BSuBBAAiA2IABBWx6P0DFUPlrOuHNxFi79KDNlJ9RVcLSo17VDs6bl8VAsBQps8lL33KSLjHUGMc
+KiEIfJo22Av+0SbFWDEwKCXzXV2juLaltJLtbCyf691DiaI8S0iRHVDsJt/WYC69IaNCMEAwDwYD
+VR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwHQYDVR0OBBYEFBVfNVdRVfslsq0DafwBo/q+
+EVXVMAoGCCqGSM49BAMDA2cAMGQCMGSWWaboCd6LuvpaiIjwH5HTRqjySkwCY/tsXzjbLkGTqQ7m
+ndwxHLKgpxgceeHHNgIwOlavmnRs9vuD4DPTCF+hnMJbn0bWtsuRBmOiBuczrD6ogRLQy7rQkgu2
+npaqBA+K
+-----END CERTIFICATE-----
+
+VeriSign Universal Root Certification Authority
+===============================================
+-----BEGIN CERTIFICATE-----
+MIIEuTCCA6GgAwIBAgIQQBrEZCGzEyEDDrvkEhrFHTANBgkqhkiG9w0BAQsFADCBvTELMAkGA1UE
+BhMCVVMxFzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMR8wHQYDVQQLExZWZXJpU2lnbiBUcnVzdCBO
+ZXR3b3JrMTowOAYDVQQLEzEoYykgMjAwOCBWZXJpU2lnbiwgSW5jLiAtIEZvciBhdXRob3JpemVk
+IHVzZSBvbmx5MTgwNgYDVQQDEy9WZXJpU2lnbiBVbml2ZXJzYWwgUm9vdCBDZXJ0aWZpY2F0aW9u
+IEF1dGhvcml0eTAeFw0wODA0MDIwMDAwMDBaFw0zNzEyMDEyMzU5NTlaMIG9MQswCQYDVQQGEwJV
+UzEXMBUGA1UEChMOVmVyaVNpZ24sIEluYy4xHzAdBgNVBAsTFlZlcmlTaWduIFRydXN0IE5ldHdv
+cmsxOjA4BgNVBAsTMShjKSAyMDA4IFZlcmlTaWduLCBJbmMuIC0gRm9yIGF1dGhvcml6ZWQgdXNl
+IG9ubHkxODA2BgNVBAMTL1ZlcmlTaWduIFVuaXZlcnNhbCBSb290IENlcnRpZmljYXRpb24gQXV0
+aG9yaXR5MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAx2E3XrEBNNti1xWb/1hajCMj
+1mCOkdeQmIN65lgZOIzF9uVkhbSicfvtvbnazU0AtMgtc6XHaXGVHzk8skQHnOgO+k1KxCHfKWGP
+MiJhgsWHH26MfF8WIFFE0XBPV+rjHOPMee5Y2A7Cs0WTwCznmhcrewA3ekEzeOEz4vMQGn+HLL72
+9fdC4uW/h2KJXwBL38Xd5HVEMkE6HnFuacsLdUYI0crSK5XQz/u5QGtkjFdN/BMReYTtXlT2NJ8I
+AfMQJQYXStrxHXpma5hgZqTZ79IugvHw7wnqRMkVauIDbjPTrJ9VAMf2CGqUuV/c4DPxhGD5WycR
+tPwW8rtWaoAljQIDAQABo4GyMIGvMA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/BAQDAgEGMG0G
+CCsGAQUFBwEMBGEwX6FdoFswWTBXMFUWCWltYWdlL2dpZjAhMB8wBwYFKw4DAhoEFI/l0xqGrI2O
+a8PPgGrUSBgsexkuMCUWI2h0dHA6Ly9sb2dvLnZlcmlzaWduLmNvbS92c2xvZ28uZ2lmMB0GA1Ud
+DgQWBBS2d/ppSEefUxLVwuoHMnYH0ZcHGTANBgkqhkiG9w0BAQsFAAOCAQEASvj4sAPmLGd75JR3
+Y8xuTPl9Dg3cyLk1uXBPY/ok+myDjEedO2Pzmvl2MpWRsXe8rJq+seQxIcaBlVZaDrHC1LGmWazx
+Y8u4TB1ZkErvkBYoH1quEPuBUDgMbMzxPcP1Y+Oz4yHJJDnp/RVmRvQbEdBNc6N9Rvk97ahfYtTx
+P/jgdFcrGJ2BtMQo2pSXpXDrrB2+BxHw1dvd5Yzw1TKwg+ZX4o+/vqGqvz0dtdQ46tewXDpPaj+P
+wGZsY6rp2aQW9IHRlRQOfc2VNNnSj3BzgXucfr2YYdhFh5iQxeuGMMY1v/D/w1WIg0vvBZIGcfK4
+mJO37M2CYfE45k+XmCpajQ==
+-----END CERTIFICATE-----
+
+VeriSign Class 3 Public Primary Certification Authority - G4
+============================================================
+-----BEGIN CERTIFICATE-----
+MIIDhDCCAwqgAwIBAgIQL4D+I4wOIg9IZxIokYesszAKBggqhkjOPQQDAzCByjELMAkGA1UEBhMC
+VVMxFzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMR8wHQYDVQQLExZWZXJpU2lnbiBUcnVzdCBOZXR3
+b3JrMTowOAYDVQQLEzEoYykgMjAwNyBWZXJpU2lnbiwgSW5jLiAtIEZvciBhdXRob3JpemVkIHVz
+ZSBvbmx5MUUwQwYDVQQDEzxWZXJpU2lnbiBDbGFzcyAzIFB1YmxpYyBQcmltYXJ5IENlcnRpZmlj
+YXRpb24gQXV0aG9yaXR5IC0gRzQwHhcNMDcxMTA1MDAwMDAwWhcNMzgwMTE4MjM1OTU5WjCByjEL
+MAkGA1UEBhMCVVMxFzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMR8wHQYDVQQLExZWZXJpU2lnbiBU
+cnVzdCBOZXR3b3JrMTowOAYDVQQLEzEoYykgMjAwNyBWZXJpU2lnbiwgSW5jLiAtIEZvciBhdXRo
+b3JpemVkIHVzZSBvbmx5MUUwQwYDVQQDEzxWZXJpU2lnbiBDbGFzcyAzIFB1YmxpYyBQcmltYXJ5
+IENlcnRpZmljYXRpb24gQXV0aG9yaXR5IC0gRzQwdjAQBgcqhkjOPQIBBgUrgQQAIgNiAASnVnp8
+Utpkmw4tXNherJI9/gHmGUo9FANL+mAnINmDiWn6VMaaGF5VKmTeBvaNSjutEDxlPZCIBIngMGGz
+rl0Bp3vefLK+ymVhAIau2o970ImtTR1ZmkGxvEeA3J5iw/mjgbIwga8wDwYDVR0TAQH/BAUwAwEB
+/zAOBgNVHQ8BAf8EBAMCAQYwbQYIKwYBBQUHAQwEYTBfoV2gWzBZMFcwVRYJaW1hZ2UvZ2lmMCEw
+HzAHBgUrDgMCGgQUj+XTGoasjY5rw8+AatRIGCx7GS4wJRYjaHR0cDovL2xvZ28udmVyaXNpZ24u
+Y29tL3ZzbG9nby5naWYwHQYDVR0OBBYEFLMWkf3upm7ktS5Jj4d4gYDs5bG1MAoGCCqGSM49BAMD
+A2gAMGUCMGYhDBgmYFo4e1ZC4Kf8NoRRkSAsdk1DPcQdhCPQrNZ8NQbOzWm9kA3bbEhCHQ6qQgIx
+AJw9SDkjOVgaFRJZap7v1VmyHVIsmXHNxynfGyphe3HR3vPA5Q06Sqotp9iGKt0uEA==
+-----END CERTIFICATE-----
+
+NetLock Arany (Class Gold) F≈ëtan√∫s√≠tv√°ny
+============================================
+-----BEGIN CERTIFICATE-----
+MIIEFTCCAv2gAwIBAgIGSUEs5AAQMA0GCSqGSIb3DQEBCwUAMIGnMQswCQYDVQQGEwJIVTERMA8G
+A1UEBwwIQnVkYXBlc3QxFTATBgNVBAoMDE5ldExvY2sgS2Z0LjE3MDUGA1UECwwuVGFuw7pzw610
+dsOhbnlraWFkw7NrIChDZXJ0aWZpY2F0aW9uIFNlcnZpY2VzKTE1MDMGA1UEAwwsTmV0TG9jayBB
+cmFueSAoQ2xhc3MgR29sZCkgRsWRdGFuw7pzw610dsOhbnkwHhcNMDgxMjExMTUwODIxWhcNMjgx
+MjA2MTUwODIxWjCBpzELMAkGA1UEBhMCSFUxETAPBgNVBAcMCEJ1ZGFwZXN0MRUwEwYDVQQKDAxO
+ZXRMb2NrIEtmdC4xNzA1BgNVBAsMLlRhbsO6c8OtdHbDoW55a2lhZMOzayAoQ2VydGlmaWNhdGlv
+biBTZXJ2aWNlcykxNTAzBgNVBAMMLE5ldExvY2sgQXJhbnkgKENsYXNzIEdvbGQpIEbFkXRhbsO6
+c8OtdHbDoW55MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxCRec75LbRTDofTjl5Bu
+0jBFHjzuZ9lk4BqKf8owyoPjIMHj9DrTlF8afFttvzBPhCf2nx9JvMaZCpDyD/V/Q4Q3Y1GLeqVw
+/HpYzY6b7cNGbIRwXdrzAZAj/E4wqX7hJ2Pn7WQ8oLjJM2P+FpD/sLj916jAwJRDC7bVWaaeVtAk
+H3B5r9s5VA1lddkVQZQBr17s9o3x/61k/iCa11zr/qYfCGSji3ZVrR47KGAuhyXoqq8fxmRGILdw
+fzzeSNuWU7c5d+Qa4scWhHaXWy+7GRWF+GmF9ZmnqfI0p6m2pgP8b4Y9VHx2BJtr+UBdADTHLpl1
+neWIA6pN+APSQnbAGwIDAKiLo0UwQzASBgNVHRMBAf8ECDAGAQH/AgEEMA4GA1UdDwEB/wQEAwIB
+BjAdBgNVHQ4EFgQUzPpnk/C2uNClwB7zU/2MU9+D15YwDQYJKoZIhvcNAQELBQADggEBAKt/7hwW
+qZw8UQCgwBEIBaeZ5m8BiFRhbvG5GK1Krf6BQCOUL/t1fC8oS2IkgYIL9WHxHG64YTjrgfpioTta
+YtOUZcTh5m2C+C8lcLIhJsFyUR+MLMOEkMNaj7rP9KdlpeuY0fsFskZ1FSNqb4VjMIDw1Z4fKRzC
+bLBQWV2QWzuoDTDPv31/zvGdg73JRm4gpvlhUbohL3u+pRVjodSVh/GeufOJ8z2FuLjbvrW5Kfna
+NwUASZQDhETnv0Mxz3WLJdH0pmT1kvarBes96aULNmLazAZfNou2XjG4Kvte9nHfRCaexOYNkbQu
+dZWAUWpLMKawYqGT8ZvYzsRjdT9ZR7E=
+-----END CERTIFICATE-----
+
+Staat der Nederlanden Root CA - G2
+==================================
+-----BEGIN CERTIFICATE-----
+MIIFyjCCA7KgAwIBAgIEAJiWjDANBgkqhkiG9w0BAQsFADBaMQswCQYDVQQGEwJOTDEeMBwGA1UE
+CgwVU3RhYXQgZGVyIE5lZGVybGFuZGVuMSswKQYDVQQDDCJTdGFhdCBkZXIgTmVkZXJsYW5kZW4g
+Um9vdCBDQSAtIEcyMB4XDTA4MDMyNjExMTgxN1oXDTIwMDMyNTExMDMxMFowWjELMAkGA1UEBhMC
+TkwxHjAcBgNVBAoMFVN0YWF0IGRlciBOZWRlcmxhbmRlbjErMCkGA1UEAwwiU3RhYXQgZGVyIE5l
+ZGVybGFuZGVuIFJvb3QgQ0EgLSBHMjCCAiIwDQYJKoZIhvcNAQEBBQADggIPADCCAgoCggIBAMVZ
+5291qj5LnLW4rJ4L5PnZyqtdj7U5EILXr1HgO+EASGrP2uEGQxGZqhQlEq0i6ABtQ8SpuOUfiUtn
+vWFI7/3S4GCI5bkYYCjDdyutsDeqN95kWSpGV+RLufg3fNU254DBtvPUZ5uW6M7XxgpT0GtJlvOj
+CwV3SPcl5XCsMBQgJeN/dVrlSPhOewMHBPqCYYdu8DvEpMfQ9XQ+pV0aCPKbJdL2rAQmPlU6Yiil
+e7Iwr/g3wtG61jj99O9JMDeZJiFIhQGp5Rbn3JBV3w/oOM2ZNyFPXfUib2rFEhZgF1XyZWampzCR
+OME4HYYEhLoaJXhena/MUGDWE4dS7WMfbWV9whUYdMrhfmQpjHLYFhN9C0lK8SgbIHRrxT3dsKpI
+CT0ugpTNGmXZK4iambwYfp/ufWZ8Pr2UuIHOzZgweMFvZ9C+X+Bo7d7iscksWXiSqt8rYGPy5V65
+48r6f1CGPqI0GAwJaCgRHOThuVw+R7oyPxjMW4T182t0xHJ04eOLoEq9jWYv6q012iDTiIJh8BIi
+trzQ1aTsr1SIJSQ8p22xcik/Plemf1WvbibG/ufMQFxRRIEKeN5KzlW/HdXZt1bv8Hb/C3m1r737
+qWmRRpdogBQ2HbN/uymYNqUg+oJgYjOk7Na6B6duxc8UpufWkjTYgfX8HV2qXB72o007uPc5AgMB
+AAGjgZcwgZQwDwYDVR0TAQH/BAUwAwEB/zBSBgNVHSAESzBJMEcGBFUdIAAwPzA9BggrBgEFBQcC
+ARYxaHR0cDovL3d3dy5wa2lvdmVyaGVpZC5ubC9wb2xpY2llcy9yb290LXBvbGljeS1HMjAOBgNV
+HQ8BAf8EBAMCAQYwHQYDVR0OBBYEFJFoMocVHYnitfGsNig0jQt8YojrMA0GCSqGSIb3DQEBCwUA
+A4ICAQCoQUpnKpKBglBu4dfYszk78wIVCVBR7y29JHuIhjv5tLySCZa59sCrI2AGeYwRTlHSeYAz
++51IvuxBQ4EffkdAHOV6CMqqi3WtFMTC6GY8ggen5ieCWxjmD27ZUD6KQhgpxrRW/FYQoAUXvQwj
+f/ST7ZwaUb7dRUG/kSS0H4zpX897IZmflZ85OkYcbPnNe5yQzSipx6lVu6xiNGI1E0sUOlWDuYaN
+kqbG9AclVMwWVxJKgnjIFNkXgiYtXSAfea7+1HAWFpWD2DU5/1JddRwWxRNVz0fMdWVSSt7wsKfk
+CpYL+63C4iWEst3kvX5ZbJvw8NjnyvLplzh+ib7M+zkXYT9y2zqR2GUBGR2tUKRXCnxLvJxxcypF
+URmFzI79R6d0lR2o0a9OF7FpJsKqeFdbxU2n5Z4FF5TKsl+gSRiNNOkmbEgeqmiSBeGCc1qb3Adb
+CG19ndeNIdn8FCCqwkXfP+cAslHkwvgFuXkajDTznlvkN1trSt8sV4pAWja63XVECDdCcAz+3F4h
+oKOKwJCcaNpQ5kUQR3i2TtJlycM33+FCY7BXN0Ute4qcvwXqZVUz9zkQxSgqIXobisQk+T8VyJoV
+IPVVYpbtbZNQvOSqeK3Zywplh6ZmwcSBo3c6WB4L7oOLnR7SUqTMHW+wmG2UMbX4cQrcufx9MmDm
+66+KAQ==
+-----END CERTIFICATE-----
+
+CA Disig
+========
+-----BEGIN CERTIFICATE-----
+MIIEDzCCAvegAwIBAgIBATANBgkqhkiG9w0BAQUFADBKMQswCQYDVQQGEwJTSzETMBEGA1UEBxMK
+QnJhdGlzbGF2YTETMBEGA1UEChMKRGlzaWcgYS5zLjERMA8GA1UEAxMIQ0EgRGlzaWcwHhcNMDYw
+MzIyMDEzOTM0WhcNMTYwMzIyMDEzOTM0WjBKMQswCQYDVQQGEwJTSzETMBEGA1UEBxMKQnJhdGlz
+bGF2YTETMBEGA1UEChMKRGlzaWcgYS5zLjERMA8GA1UEAxMIQ0EgRGlzaWcwggEiMA0GCSqGSIb3
+DQEBAQUAA4IBDwAwggEKAoIBAQCS9jHBfYj9mQGp2HvycXXxMcbzdWb6UShGhJd4NLxs/LxFWYgm
+GErENx+hSkS943EE9UQX4j/8SFhvXJ56CbpRNyIjZkMhsDxkovhqFQ4/61HhVKndBpnXmjxUizkD
+Pw/Fzsbrg3ICqB9x8y34dQjbYkzo+s7552oftms1grrijxaSfQUMbEYDXcDtab86wYqg6I7ZuUUo
+hwjstMoVvoLdtUSLLa2GDGhibYVW8qwUYzrG0ZmsNHhWS8+2rT+MitcE5eN4TPWGqvWP+j1scaMt
+ymfraHtuM6kMgiioTGohQBUgDCZbg8KpFhXAJIJdKxatymP2dACw30PEEGBWZ2NFAgMBAAGjgf8w
+gfwwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUjbJJaJ1yCCW5wCf1UJNWSEZx+Y8wDgYDVR0P
+AQH/BAQDAgEGMDYGA1UdEQQvMC2BE2Nhb3BlcmF0b3JAZGlzaWcuc2uGFmh0dHA6Ly93d3cuZGlz
+aWcuc2svY2EwZgYDVR0fBF8wXTAtoCugKYYnaHR0cDovL3d3dy5kaXNpZy5zay9jYS9jcmwvY2Ff
+ZGlzaWcuY3JsMCygKqAohiZodHRwOi8vY2EuZGlzaWcuc2svY2EvY3JsL2NhX2Rpc2lnLmNybDAa
+BgNVHSAEEzARMA8GDSuBHpGT5goAAAABAQEwDQYJKoZIhvcNAQEFBQADggEBAF00dGFMrzvY/59t
+WDYcPQuBDRIrRhCA/ec8J9B6yKm2fnQwM6M6int0wHl5QpNt/7EpFIKrIYwvF/k/Ji/1WcbvgAa3
+mkkp7M5+cTxqEEHA9tOasnxakZzArFvITV734VP/Q3f8nktnbNfzg9Gg4H8l37iYC5oyOGwwoPP/
+CBUz91BKez6jPiCp3C9WgArtQVCwyfTssuMmRAAOb54GvCKWU3BlxFAKRmukLyeBEicTXxChds6K
+ezfqwzlhA5WYOudsiCUI/HloDYd9Yvi0X/vF2Ey9WLw/Q1vUHgFNPGO+I++MzVpQuGhU+QqZMxEA
+4Z7CRneC9VkGjCFMhwnN5ag=
+-----END CERTIFICATE-----
+
+Juur-SK
+=======
+-----BEGIN CERTIFICATE-----
+MIIE5jCCA86gAwIBAgIEO45L/DANBgkqhkiG9w0BAQUFADBdMRgwFgYJKoZIhvcNAQkBFglwa2lA
+c2suZWUxCzAJBgNVBAYTAkVFMSIwIAYDVQQKExlBUyBTZXJ0aWZpdHNlZXJpbWlza2Vza3VzMRAw
+DgYDVQQDEwdKdXVyLVNLMB4XDTAxMDgzMDE0MjMwMVoXDTE2MDgyNjE0MjMwMVowXTEYMBYGCSqG
+SIb3DQEJARYJcGtpQHNrLmVlMQswCQYDVQQGEwJFRTEiMCAGA1UEChMZQVMgU2VydGlmaXRzZWVy
+aW1pc2tlc2t1czEQMA4GA1UEAxMHSnV1ci1TSzCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoC
+ggEBAIFxNj4zB9bjMI0TfncyRsvPGbJgMUaXhvSYRqTCZUXP00B841oiqBB4M8yIsdOBSvZiF3tf
+TQou0M+LI+5PAk676w7KvRhj6IAcjeEcjT3g/1tf6mTll+g/mX8MCgkzABpTpyHhOEvWgxutr2TC
++Rx6jGZITWYfGAriPrsfB2WThbkasLnE+w0R9vXW+RvHLCu3GFH+4Hv2qEivbDtPL+/40UceJlfw
+UR0zlv/vWT3aTdEVNMfqPxZIe5EcgEMPPbgFPtGzlc3Yyg/CQ2fbt5PgIoIuvvVoKIO5wTtpeyDa
+Tpxt4brNj3pssAki14sL2xzVWiZbDcDq5WDQn/413z8CAwEAAaOCAawwggGoMA8GA1UdEwEB/wQF
+MAMBAf8wggEWBgNVHSAEggENMIIBCTCCAQUGCisGAQQBzh8BAQEwgfYwgdAGCCsGAQUFBwICMIHD
+HoHAAFMAZQBlACAAcwBlAHIAdABpAGYAaQBrAGEAYQB0ACAAbwBuACAAdgDkAGwAagBhAHMAdABh
+AHQAdQBkACAAQQBTAC0AaQBzACAAUwBlAHIAdABpAGYAaQB0AHMAZQBlAHIAaQBtAGkAcwBrAGUA
+cwBrAHUAcwAgAGEAbABhAG0ALQBTAEsAIABzAGUAcgB0AGkAZgBpAGsAYQBhAHQAaQBkAGUAIABr
+AGkAbgBuAGkAdABhAG0AaQBzAGUAawBzMCEGCCsGAQUFBwIBFhVodHRwOi8vd3d3LnNrLmVlL2Nw
+cy8wKwYDVR0fBCQwIjAgoB6gHIYaaHR0cDovL3d3dy5zay5lZS9qdXVyL2NybC8wHQYDVR0OBBYE
+FASqekej5ImvGs8KQKcYP2/v6X2+MB8GA1UdIwQYMBaAFASqekej5ImvGs8KQKcYP2/v6X2+MA4G
+A1UdDwEB/wQEAwIB5jANBgkqhkiG9w0BAQUFAAOCAQEAe8EYlFOiCfP+JmeaUOTDBS8rNXiRTHyo
+ERF5TElZrMj3hWVcRrs7EKACr81Ptcw2Kuxd/u+gkcm2k298gFTsxwhwDY77guwqYHhpNjbRxZyL
+abVAyJRld/JXIWY7zoVAtjNjGr95HvxcHdMdkxuLDF2FvZkwMhgJkVLpfKG6/2SSmuz+Ne6ML678
+IIbsSt4beDI3poHSna9aEhbKmVv8b20OxaAehsmR0FyYgl9jDIpaq9iVpszLita/ZEuOyoqysOkh
+Mp6qqIWYNIE5ITuoOlIyPfZrN4YGWhWY3PARZv40ILcD9EEQfTmEeZZyY7aWAuVrua0ZTbvGRNs2
+yyqcjg==
+-----END CERTIFICATE-----
+
+Hongkong Post Root CA 1
+=======================
+-----BEGIN CERTIFICATE-----
+MIIDMDCCAhigAwIBAgICA+gwDQYJKoZIhvcNAQEFBQAwRzELMAkGA1UEBhMCSEsxFjAUBgNVBAoT
+DUhvbmdrb25nIFBvc3QxIDAeBgNVBAMTF0hvbmdrb25nIFBvc3QgUm9vdCBDQSAxMB4XDTAzMDUx
+NTA1MTMxNFoXDTIzMDUxNTA0NTIyOVowRzELMAkGA1UEBhMCSEsxFjAUBgNVBAoTDUhvbmdrb25n
+IFBvc3QxIDAeBgNVBAMTF0hvbmdrb25nIFBvc3QgUm9vdCBDQSAxMIIBIjANBgkqhkiG9w0BAQEF
+AAOCAQ8AMIIBCgKCAQEArP84tulmAknjorThkPlAj3n54r15/gK97iSSHSL22oVyaf7XPwnU3ZG1
+ApzQjVrhVcNQhrkpJsLj2aDxaQMoIIBFIi1WpztUlVYiWR8o3x8gPW2iNr4joLFutbEnPzlTCeqr
+auh0ssJlXI6/fMN4hM2eFvz1Lk8gKgifd/PFHsSaUmYeSF7jEAaPIpjhZY4bXSNmO7ilMlHIhqqh
+qZ5/dpTCpmy3QfDVyAY45tQM4vM7TG1QjMSDJ8EThFk9nnV0ttgCXjqQesBCNnLsak3c78QA3xMY
+V18meMjWCnl3v/evt3a5pQuEF10Q6m/hq5URX208o1xNg1vysxmKgIsLhwIDAQABoyYwJDASBgNV
+HRMBAf8ECDAGAQH/AgEDMA4GA1UdDwEB/wQEAwIBxjANBgkqhkiG9w0BAQUFAAOCAQEADkbVPK7i
+h9legYsCmEEIjEy82tvuJxuC52pF7BaLT4Wg87JwvVqWuspube5Gi27nKi6Wsxkz67SfqLI37pio
+l7Yutmcn1KZJ/RyTZXaeQi/cImyaT/JaFTmxcdcrUehtHJjA2Sr0oYJ71clBoiMBdDhViw+5Lmei
+IAQ32pwL0xch4I+XeTRvhEgCIDMb5jREn5Fw9IBehEPCKdJsEhTkYY2sEJCehFC78JZvRZ+K88ps
+T/oROhUVRsPNH4NbLUES7VBnQRM9IauUiqpOfMGx+6fWtScvl6tu4B3i0RwsH0Ti/L6RoZz71ilT
+c4afU9hDDl3WY4JxHYB0yvbiAmvZWg==
+-----END CERTIFICATE-----
+
+SecureSign RootCA11
+===================
+-----BEGIN CERTIFICATE-----
+MIIDbTCCAlWgAwIBAgIBATANBgkqhkiG9w0BAQUFADBYMQswCQYDVQQGEwJKUDErMCkGA1UEChMi
+SmFwYW4gQ2VydGlmaWNhdGlvbiBTZXJ2aWNlcywgSW5jLjEcMBoGA1UEAxMTU2VjdXJlU2lnbiBS
+b290Q0ExMTAeFw0wOTA0MDgwNDU2NDdaFw0yOTA0MDgwNDU2NDdaMFgxCzAJBgNVBAYTAkpQMSsw
+KQYDVQQKEyJKYXBhbiBDZXJ0aWZpY2F0aW9uIFNlcnZpY2VzLCBJbmMuMRwwGgYDVQQDExNTZWN1
+cmVTaWduIFJvb3RDQTExMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA/XeqpRyQBTvL
+TJszi1oURaTnkBbR31fSIRCkF/3frNYfp+TbfPfs37gD2pRY/V1yfIw/XwFndBWW4wI8h9uuywGO
+wvNmxoVF9ALGOrVisq/6nL+k5tSAMJjzDbaTj6nU2DbysPyKyiyhFTOVMdrAG/LuYpmGYz+/3ZMq
+g6h2uRMft85OQoWPIucuGvKVCbIFtUROd6EgvanyTgp9UK31BQ1FT0Zx/Sg+U/sE2C3XZR1KG/rP
+O7AxmjVuyIsG0wCR8pQIZUyxNAYAeoni8McDWc/V1uinMrPmmECGxc0nEovMe863ETxiYAcjPitA
+bpSACW22s293bzUIUPsCh8U+iQIDAQABo0IwQDAdBgNVHQ4EFgQUW/hNT7KlhtQ60vFjmqC+CfZX
+t94wDgYDVR0PAQH/BAQDAgEGMA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZIhvcNAQEFBQADggEBAKCh
+OBZmLqdWHyGcBvod7bkixTgm2E5P7KN/ed5GIaGHd48HCJqypMWvDzKYC3xmKbabfSVSSUOrTC4r
+bnpwrxYO4wJs+0LmGJ1F2FXI6Dvd5+H0LgscNFxsWEr7jIhQX5Ucv+2rIrVls4W6ng+4reV6G4pQ
+Oh29Dbx7VFALuUKvVaAYga1lme++5Jy/xIWrQbJUb9wlze144o4MjQlJ3WN7WmmWAiGovVJZ6X01
+y8hSyn+B/tlr0/cR7SXf+Of5pPpyl4RTDaXQMhhRdlkUbA/r7F+AjHVDg8OFmP9Mni0N5HeDk061
+lgeLKBObjBmNQSdJQO7e5iNEOdyhIta6A/I=
+-----END CERTIFICATE-----
+
+ACEDICOM Root
+=============
+-----BEGIN CERTIFICATE-----
+MIIFtTCCA52gAwIBAgIIYY3HhjsBggUwDQYJKoZIhvcNAQEFBQAwRDEWMBQGA1UEAwwNQUNFRElD
+T00gUm9vdDEMMAoGA1UECwwDUEtJMQ8wDQYDVQQKDAZFRElDT00xCzAJBgNVBAYTAkVTMB4XDTA4
+MDQxODE2MjQyMloXDTI4MDQxMzE2MjQyMlowRDEWMBQGA1UEAwwNQUNFRElDT00gUm9vdDEMMAoG
+A1UECwwDUEtJMQ8wDQYDVQQKDAZFRElDT00xCzAJBgNVBAYTAkVTMIICIjANBgkqhkiG9w0BAQEF
+AAOCAg8AMIICCgKCAgEA/5KV4WgGdrQsyFhIyv2AVClVYyT/kGWbEHV7w2rbYgIB8hiGtXxaOLHk
+WLn709gtn70yN78sFW2+tfQh0hOR2QetAQXW8713zl9CgQr5auODAKgrLlUTY4HKRxx7XBZXehuD
+YAQ6PmXDzQHe3qTWDLqO3tkE7hdWIpuPY/1NFgu3e3eM+SW10W2ZEi5PGrjm6gSSrj0RuVFCPYew
+MYWveVqc/udOXpJPQ/yrOq2lEiZmueIM15jO1FillUAKt0SdE3QrwqXrIhWYENiLxQSfHY9g5QYb
+m8+5eaA9oiM/Qj9r+hwDezCNzmzAv+YbX79nuIQZ1RXve8uQNjFiybwCq0Zfm/4aaJQ0PZCOrfbk
+HQl/Sog4P75n/TSW9R28MHTLOO7VbKvU/PQAtwBbhTIWdjPp2KOZnQUAqhbm84F9b32qhm2tFXTT
+xKJxqvQUfecyuB+81fFOvW8XAjnXDpVCOscAPukmYxHqC9FK/xidstd7LzrZlvvoHpKuE1XI2Sf2
+3EgbsCTBheN3nZqk8wwRHQ3ItBTutYJXCb8gWH8vIiPYcMt5bMlL8qkqyPyHK9caUPgn6C9D4zq9
+2Fdx/c6mUlv53U3t5fZvie27k5x2IXXwkkwp9y+cAS7+UEaeZAwUswdbxcJzbPEHXEUkFDWug/Fq
+TYl6+rPYLWbwNof1K1MCAwEAAaOBqjCBpzAPBgNVHRMBAf8EBTADAQH/MB8GA1UdIwQYMBaAFKaz
+4SsrSbbXc6GqlPUB53NlTKxQMA4GA1UdDwEB/wQEAwIBhjAdBgNVHQ4EFgQUprPhKytJttdzoaqU
+9QHnc2VMrFAwRAYDVR0gBD0wOzA5BgRVHSAAMDEwLwYIKwYBBQUHAgEWI2h0dHA6Ly9hY2VkaWNv
+bS5lZGljb21ncm91cC5jb20vZG9jMA0GCSqGSIb3DQEBBQUAA4ICAQDOLAtSUWImfQwng4/F9tqg
+aHtPkl7qpHMyEVNEskTLnewPeUKzEKbHDZ3Ltvo/Onzqv4hTGzz3gvoFNTPhNahXwOf9jU8/kzJP
+eGYDdwdY6ZXIfj7QeQCM8htRM5u8lOk6e25SLTKeI6RF+7YuE7CLGLHdztUdp0J/Vb77W7tH1Pwk
+zQSulgUV1qzOMPPKC8W64iLgpq0i5ALudBF/TP94HTXa5gI06xgSYXcGCRZj6hitoocf8seACQl1
+ThCojz2GuHURwCRiipZ7SkXp7FnFvmuD5uHorLUwHv4FB4D54SMNUI8FmP8sX+g7tq3PgbUhh8oI
+KiMnMCArz+2UW6yyetLHKKGKC5tNSixthT8Jcjxn4tncB7rrZXtaAWPWkFtPF2Y9fwsZo5NjEFIq
+nxQWWOLcpfShFosOkYuByptZ+thrkQdlVV9SH686+5DdaaVbnG0OLLb6zqylfDJKZ0DcMDQj3dcE
+I2bw/FWAp/tmGYI1Z2JwOV5vx+qQQEQIHriy1tvuWacNGHk0vFQYXlPKNFHtRQrmjseCNj6nOGOp
+MCwXEGCSn1WHElkQwg9naRHMTh5+Spqtr0CodaxWkHS4oJyleW/c6RrIaQXpuvoDs3zk4E7Czp3o
+tkYNbn5XOmeUwssfnHdKZ05phkOTOPu220+DkdRgfks+KzgHVZhepA==
+-----END CERTIFICATE-----
+
+Verisign Class 1 Public Primary Certification Authority
+=======================================================
+-----BEGIN CERTIFICATE-----
+MIICPDCCAaUCED9pHoGc8JpK83P/uUii5N0wDQYJKoZIhvcNAQEFBQAwXzELMAkGA1UEBhMCVVMx
+FzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMTcwNQYDVQQLEy5DbGFzcyAxIFB1YmxpYyBQcmltYXJ5
+IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MB4XDTk2MDEyOTAwMDAwMFoXDTI4MDgwMjIzNTk1OVow
+XzELMAkGA1UEBhMCVVMxFzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMTcwNQYDVQQLEy5DbGFzcyAx
+IFB1YmxpYyBQcmltYXJ5IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MIGfMA0GCSqGSIb3DQEBAQUA
+A4GNADCBiQKBgQDlGb9to1ZhLZlIcfZn3rmN67eehoAKkQ76OCWvRoiC5XOooJskXQ0fzGVuDLDQ
+VoQYh5oGmxChc9+0WDlrbsH2FdWoqD+qEgaNMax/sDTXjzRniAnNFBHiTkVWaR94AoDa3EeRKbs2
+yWNcxeDXLYd7obcysHswuiovMaruo2fa2wIDAQABMA0GCSqGSIb3DQEBBQUAA4GBAFgVKTk8d6Pa
+XCUDfGD67gmZPCcQcMgMCeazh88K4hiWNWLMv5sneYlfycQJ9M61Hd8qveXbhpxoJeUwfLaJFf5n
+0a3hUKw8fGJLj7qE1xIVGx/KXQ/BUpQqEZnae88MNhPVNdwQGVnqlMEAv3WP2fr9dgTbYruQagPZ
+RjXZ+Hxb
+-----END CERTIFICATE-----
+
+Verisign Class 3 Public Primary Certification Authority
+=======================================================
+-----BEGIN CERTIFICATE-----
+MIICPDCCAaUCEDyRMcsf9tAbDpq40ES/Er4wDQYJKoZIhvcNAQEFBQAwXzELMAkGA1UEBhMCVVMx
+FzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMTcwNQYDVQQLEy5DbGFzcyAzIFB1YmxpYyBQcmltYXJ5
+IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MB4XDTk2MDEyOTAwMDAwMFoXDTI4MDgwMjIzNTk1OVow
+XzELMAkGA1UEBhMCVVMxFzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMTcwNQYDVQQLEy5DbGFzcyAz
+IFB1YmxpYyBQcmltYXJ5IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MIGfMA0GCSqGSIb3DQEBAQUA
+A4GNADCBiQKBgQDJXFme8huKARS0EN8EQNvjV69qRUCPhAwL0TPZ2RHP7gJYHyX3KqhEBarsAx94
+f56TuZoAqiN91qyFomNFx3InzPRMxnVx0jnvT0Lwdd8KkMaOIG+YD/isI19wKTakyYbnsZogy1Ol
+hec9vn2a/iRFM9x2Fe0PonFkTGUugWhFpwIDAQABMA0GCSqGSIb3DQEBBQUAA4GBABByUqkFFBky
+CEHwxWsKzH4PIRnN5GfcX6kb5sroc50i2JhucwNhkcV8sEVAbkSdjbCxlnRhLQ2pRdKkkirWmnWX
+bj9T/UWZYB2oK0z5XqcJ2HUw19JlYD1n1khVdWk/kfVIC0dpImmClr7JyDiGSnoscxlIaU5rfGW/
+D/xwzoiQ
+-----END CERTIFICATE-----
+
+Microsec e-Szigno Root CA 2009
+==============================
+-----BEGIN CERTIFICATE-----
+MIIECjCCAvKgAwIBAgIJAMJ+QwRORz8ZMA0GCSqGSIb3DQEBCwUAMIGCMQswCQYDVQQGEwJIVTER
+MA8GA1UEBwwIQnVkYXBlc3QxFjAUBgNVBAoMDU1pY3Jvc2VjIEx0ZC4xJzAlBgNVBAMMHk1pY3Jv
+c2VjIGUtU3ppZ25vIFJvb3QgQ0EgMjAwOTEfMB0GCSqGSIb3DQEJARYQaW5mb0BlLXN6aWduby5o
+dTAeFw0wOTA2MTYxMTMwMThaFw0yOTEyMzAxMTMwMThaMIGCMQswCQYDVQQGEwJIVTERMA8GA1UE
+BwwIQnVkYXBlc3QxFjAUBgNVBAoMDU1pY3Jvc2VjIEx0ZC4xJzAlBgNVBAMMHk1pY3Jvc2VjIGUt
+U3ppZ25vIFJvb3QgQ0EgMjAwOTEfMB0GCSqGSIb3DQEJARYQaW5mb0BlLXN6aWduby5odTCCASIw
+DQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAOn4j/NjrdqG2KfgQvvPkd6mJviZpWNwrZuuyjNA
+fW2WbqEORO7hE52UQlKavXWFdCyoDh2Tthi3jCyoz/tccbna7P7ofo/kLx2yqHWH2Leh5TvPmUpG
+0IMZfcChEhyVbUr02MelTTMuhTlAdX4UfIASmFDHQWe4oIBhVKZsTh/gnQ4H6cm6M+f+wFUoLAKA
+pxn1ntxVUwOXewdI/5n7N4okxFnMUBBjjqqpGrCEGob5X7uxUG6k0QrM1XF+H6cbfPVTbiJfyyvm
+1HxdrtbCxkzlBQHZ7Vf8wSN5/PrIJIOV87VqUQHQd9bpEqH5GoP7ghu5sJf0dgYzQ0mg/wu1+rUC
+AwEAAaOBgDB+MA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/BAQDAgEGMB0GA1UdDgQWBBTLD8bf
+QkPMPcu1SCOhGnqmKrs0aDAfBgNVHSMEGDAWgBTLD8bfQkPMPcu1SCOhGnqmKrs0aDAbBgNVHREE
+FDASgRBpbmZvQGUtc3ppZ25vLmh1MA0GCSqGSIb3DQEBCwUAA4IBAQDJ0Q5eLtXMs3w+y/w9/w0o
+lZMEyL/azXm4Q5DwpL7v8u8hmLzU1F0G9u5C7DBsoKqpyvGvivo/C3NqPuouQH4frlRheesuCDfX
+I/OMn74dseGkddug4lQUsbocKaQY9hK6ohQU4zE1yED/t+AFdlfBHFny+L/k7SViXITwfn4fs775
+tyERzAMBVnCnEJIeGzSBHq2cGsMEPO0CYdYeBvNfOofyK/FFh+U9rNHHV4S9a67c2Pm2G2JwCz02
+yULyMtd6YebS2z3PyKnJm9zbWETXbzivf3jTo60adbocwTZ8jx5tHMN1Rq41Bab2XD0h7lbwyYIi
+LXpUq3DDfSJlgnCW
+-----END CERTIFICATE-----
+
+E-Guven Kok Elektronik Sertifika Hizmet Saglayicisi
+===================================================
+-----BEGIN CERTIFICATE-----
+MIIDtjCCAp6gAwIBAgIQRJmNPMADJ72cdpW56tustTANBgkqhkiG9w0BAQUFADB1MQswCQYDVQQG
+EwJUUjEoMCYGA1UEChMfRWxla3Ryb25payBCaWxnaSBHdXZlbmxpZ2kgQS5TLjE8MDoGA1UEAxMz
+ZS1HdXZlbiBLb2sgRWxla3Ryb25payBTZXJ0aWZpa2EgSGl6bWV0IFNhZ2xheWljaXNpMB4XDTA3
+MDEwNDExMzI0OFoXDTE3MDEwNDExMzI0OFowdTELMAkGA1UEBhMCVFIxKDAmBgNVBAoTH0VsZWt0
+cm9uaWsgQmlsZ2kgR3V2ZW5saWdpIEEuUy4xPDA6BgNVBAMTM2UtR3V2ZW4gS29rIEVsZWt0cm9u
+aWsgU2VydGlmaWthIEhpem1ldCBTYWdsYXlpY2lzaTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCC
+AQoCggEBAMMSIJ6wXgBljU5Gu4Bc6SwGl9XzcslwuedLZYDBS75+PNdUMZTe1RK6UxYC6lhj71vY
+8+0qGqpxSKPcEC1fX+tcS5yWCEIlKBHMilpiAVDV6wlTL/jDj/6z/P2douNffb7tC+Bg62nsM+3Y
+jfsSSYMAyYuXjDtzKjKzEve5TfL0TW3H5tYmNwjy2f1rXKPlSFxYvEK+A1qBuhw1DADT9SN+cTAI
+JjjcJRFHLfO6IxClv7wC90Nex/6wN1CZew+TzuZDLMN+DfIcQ2Zgy2ExR4ejT669VmxMvLz4Bcpk
+9Ok0oSy1c+HCPujIyTQlCFzz7abHlJ+tiEMl1+E5YP6sOVkCAwEAAaNCMEAwDgYDVR0PAQH/BAQD
+AgEGMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFJ/uRLOU1fqRTy7ZVZoEVtstxNulMA0GCSqG
+SIb3DQEBBQUAA4IBAQB/X7lTW2M9dTLn+sR0GstG30ZpHFLPqk/CaOv/gKlR6D1id4k9CnU58W5d
+F4dvaAXBlGzZXd/aslnLpRCKysw5zZ/rTt5S/wzw9JKp8mxTq5vSR6AfdPebmvEvFZ96ZDAYBzwq
+D2fK/A+JYZ1lpTzlvBNbCNvj/+27BrtqBrF6T2XGgv0enIu1De5Iu7i9qgi0+6N8y5/NkHZchpZ4
+Vwpm+Vganf2XKWDeEaaQHBkc7gGWIjQ0LpH5t8Qn0Xvmv/uARFoW5evg1Ao4vOSR49XrXMGs3xtq
+fJ7lddK2l4fbzIcrQzqECK+rPNv3PGYxhrCdU3nt+CPeQuMtgvEP5fqX
+-----END CERTIFICATE-----
+
+GlobalSign Root CA - R3
+=======================
+-----BEGIN CERTIFICATE-----
+MIIDXzCCAkegAwIBAgILBAAAAAABIVhTCKIwDQYJKoZIhvcNAQELBQAwTDEgMB4GA1UECxMXR2xv
+YmFsU2lnbiBSb290IENBIC0gUjMxEzARBgNVBAoTCkdsb2JhbFNpZ24xEzARBgNVBAMTCkdsb2Jh
+bFNpZ24wHhcNMDkwMzE4MTAwMDAwWhcNMjkwMzE4MTAwMDAwWjBMMSAwHgYDVQQLExdHbG9iYWxT
+aWduIFJvb3QgQ0EgLSBSMzETMBEGA1UEChMKR2xvYmFsU2lnbjETMBEGA1UEAxMKR2xvYmFsU2ln
+bjCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMwldpB5BngiFvXAg7aEyiie/QV2EcWt
+iHL8RgJDx7KKnQRfJMsuS+FggkbhUqsMgUdwbN1k0ev1LKMPgj0MK66X17YUhhB5uzsTgHeMCOFJ
+0mpiLx9e+pZo34knlTifBtc+ycsmWQ1z3rDI6SYOgxXG71uL0gRgykmmKPZpO/bLyCiR5Z2KYVc3
+rHQU3HTgOu5yLy6c+9C7v/U9AOEGM+iCK65TpjoWc4zdQQ4gOsC0p6Hpsk+QLjJg6VfLuQSSaGjl
+OCZgdbKfd/+RFO+uIEn8rUAVSNECMWEZXriX7613t2Saer9fwRPvm2L7DWzgVGkWqQPabumDk3F2
+xmmFghcCAwEAAaNCMEAwDgYDVR0PAQH/BAQDAgEGMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYE
+FI/wS3+oLkUkrk1Q+mOai97i3Ru8MA0GCSqGSIb3DQEBCwUAA4IBAQBLQNvAUKr+yAzv95ZURUm7
+lgAJQayzE4aGKAczymvmdLm6AC2upArT9fHxD4q/c2dKg8dEe3jgr25sbwMpjjM5RcOO5LlXbKr8
+EpbsU8Yt5CRsuZRj+9xTaGdWPoO4zzUhw8lo/s7awlOqzJCK6fBdRoyV3XpYKBovHd7NADdBj+1E
+bddTKJd+82cEHhXXipa0095MJ6RMG3NzdvQXmcIfeg7jLQitChws/zyrVQ4PkX4268NXSb7hLi18
+YIvDQVETI53O9zJrlAGomecsMx86OyXShkDOOyyGeMlhLxS67ttVb9+E7gUJTb0o2HLO02JQZR7r
+kpeDMdmztcpHWD9f
+-----END CERTIFICATE-----
+
+TC TrustCenter Universal CA III
+===============================
+-----BEGIN CERTIFICATE-----
+MIID4TCCAsmgAwIBAgIOYyUAAQACFI0zFQLkbPQwDQYJKoZIhvcNAQEFBQAwezELMAkGA1UEBhMC
+REUxHDAaBgNVBAoTE1RDIFRydXN0Q2VudGVyIEdtYkgxJDAiBgNVBAsTG1RDIFRydXN0Q2VudGVy
+IFVuaXZlcnNhbCBDQTEoMCYGA1UEAxMfVEMgVHJ1c3RDZW50ZXIgVW5pdmVyc2FsIENBIElJSTAe
+Fw0wOTA5MDkwODE1MjdaFw0yOTEyMzEyMzU5NTlaMHsxCzAJBgNVBAYTAkRFMRwwGgYDVQQKExNU
+QyBUcnVzdENlbnRlciBHbWJIMSQwIgYDVQQLExtUQyBUcnVzdENlbnRlciBVbml2ZXJzYWwgQ0Ex
+KDAmBgNVBAMTH1RDIFRydXN0Q2VudGVyIFVuaXZlcnNhbCBDQSBJSUkwggEiMA0GCSqGSIb3DQEB
+AQUAA4IBDwAwggEKAoIBAQDC2pxisLlxErALyBpXsq6DFJmzNEubkKLF5+cvAqBNLaT6hdqbJYUt
+QCggbergvbFIgyIpRJ9Og+41URNzdNW88jBmlFPAQDYvDIRlzg9uwliT6CwLOunBjvvya8o84pxO
+juT5fdMnnxvVZ3iHLX8LR7PH6MlIfK8vzArZQe+f/prhsq75U7Xl6UafYOPfjdN/+5Z+s7Vy+Eut
+CHnNaYlAJ/Uqwa1D7KRTyGG299J5KmcYdkhtWyUB0SbFt1dpIxVbYYqt8Bst2a9c8SaQaanVDED1
+M4BDj5yjdipFtK+/fz6HP3bFzSreIMUWWMv5G/UPyw0RUmS40nZid4PxWJ//AgMBAAGjYzBhMB8G
+A1UdIwQYMBaAFFbn4VslQ4Dg9ozhcbyO5YAvxEjiMA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/
+BAQDAgEGMB0GA1UdDgQWBBRW5+FbJUOA4PaM4XG8juWAL8RI4jANBgkqhkiG9w0BAQUFAAOCAQEA
+g8ev6n9NCjw5sWi+e22JLumzCecYV42FmhfzdkJQEw/HkG8zrcVJYCtsSVgZ1OK+t7+rSbyUyKu+
+KGwWaODIl0YgoGhnYIg5IFHYaAERzqf2EQf27OysGh+yZm5WZ2B6dF7AbZc2rrUNXWZzwCUyRdhK
+BgePxLcHsU0GDeGl6/R1yrqc0L2z0zIkTO5+4nYES0lT2PLpVDP85XEfPRRclkvxOvIAu2y0+pZV
+CIgJwcyRGSmwIC3/yzikQOEXvnlhgP8HA4ZMTnsGnxGGjYnuJ8Tb4rwZjgvDwxPHLQNjO9Po5KIq
+woIIlBZU8O8fJ5AluA0OKBtHd0e9HKgl8ZS0Zg==
+-----END CERTIFICATE-----
+
+Autoridad de Certificacion Firmaprofesional CIF A62634068
+=========================================================
+-----BEGIN CERTIFICATE-----
+MIIGFDCCA/ygAwIBAgIIU+w77vuySF8wDQYJKoZIhvcNAQEFBQAwUTELMAkGA1UEBhMCRVMxQjBA
+BgNVBAMMOUF1dG9yaWRhZCBkZSBDZXJ0aWZpY2FjaW9uIEZpcm1hcHJvZmVzaW9uYWwgQ0lGIEE2
+MjYzNDA2ODAeFw0wOTA1MjAwODM4MTVaFw0zMDEyMzEwODM4MTVaMFExCzAJBgNVBAYTAkVTMUIw
+QAYDVQQDDDlBdXRvcmlkYWQgZGUgQ2VydGlmaWNhY2lvbiBGaXJtYXByb2Zlc2lvbmFsIENJRiBB
+NjI2MzQwNjgwggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQDKlmuO6vj78aI14H9M2uDD
+Utd9thDIAl6zQyrET2qyyhxdKJp4ERppWVevtSBC5IsP5t9bpgOSL/UR5GLXMnE42QQMcas9UX4P
+B99jBVzpv5RvwSmCwLTaUbDBPLutN0pcyvFLNg4kq7/DhHf9qFD0sefGL9ItWY16Ck6WaVICqjaY
+7Pz6FIMMNx/Jkjd/14Et5cS54D40/mf0PmbR0/RAz15iNA9wBj4gGFrO93IbJWyTdBSTo3OxDqqH
+ECNZXyAFGUftaI6SEspd/NYrspI8IM/hX68gvqB2f3bl7BqGYTM+53u0P6APjqK5am+5hyZvQWyI
+plD9amML9ZMWGxmPsu2bm8mQ9QEM3xk9Dz44I8kvjwzRAv4bVdZO0I08r0+k8/6vKtMFnXkIoctX
+MbScyJCyZ/QYFpM6/EfY0XiWMR+6KwxfXZmtY4laJCB22N/9q06mIqqdXuYnin1oKaPnirjaEbsX
+LZmdEyRG98Xi2J+Of8ePdG1asuhy9azuJBCtLxTa/y2aRnFHvkLfuwHb9H/TKI8xWVvTyQKmtFLK
+bpf7Q8UIJm+K9Lv9nyiqDdVF8xM6HdjAeI9BZzwelGSuewvF6NkBiDkal4ZkQdU7hwxu+g/GvUgU
+vzlN1J5Bto+WHWOWk9mVBngxaJ43BjuAiUVhOSPHG0SjFeUc+JIwuwIDAQABo4HvMIHsMBIGA1Ud
+EwEB/wQIMAYBAf8CAQEwDgYDVR0PAQH/BAQDAgEGMB0GA1UdDgQWBBRlzeurNR4APn7VdMActHNH
+DhpkLzCBpgYDVR0gBIGeMIGbMIGYBgRVHSAAMIGPMC8GCCsGAQUFBwIBFiNodHRwOi8vd3d3LmZp
+cm1hcHJvZmVzaW9uYWwuY29tL2NwczBcBggrBgEFBQcCAjBQHk4AUABhAHMAZQBvACAAZABlACAA
+bABhACAAQgBvAG4AYQBuAG8AdgBhACAANAA3ACAAQgBhAHIAYwBlAGwAbwBuAGEAIAAwADgAMAAx
+ADcwDQYJKoZIhvcNAQEFBQADggIBABd9oPm03cXF661LJLWhAqvdpYhKsg9VSytXjDvlMd3+xDLx
+51tkljYyGOylMnfX40S2wBEqgLk9am58m9Ot/MPWo+ZkKXzR4Tgegiv/J2Wv+xYVxC5xhOW1//qk
+R71kMrv2JYSiJ0L1ILDCExARzRAVukKQKtJE4ZYm6zFIEv0q2skGz3QeqUvVhyj5eTSSPi5E6PaP
+T481PyWzOdxjKpBrIF/EUhJOlywqrJ2X3kjyo2bbwtKDlaZmp54lD+kLM5FlClrD2VQS3a/DTg4f
+Jl4N3LON7NWBcN7STyQF82xO9UxJZo3R/9ILJUFI/lGExkKvgATP0H5kSeTy36LssUzAKh3ntLFl
+osS88Zj0qnAHY7S42jtM+kAiMFsRpvAFDsYCA0irhpuF3dvd6qJ2gHN99ZwExEWN57kci57q13XR
+crHedUTnQn3iV2t93Jm8PYMo6oCTjcVMZcFwgbg4/EMxsvYDNEeyrPsiBsse3RdHHF9mudMaotoR
+saS8I8nkvof/uZS2+F0gStRf571oe2XyFR7SOqkt6dhrJKyXWERHrVkY8SFlcN7ONGCoQPHzPKTD
+KCOM/iczQ0CgFzzr6juwcqajuUpLXhZI9LK8yIySxZ2frHI2vDSANGupi5LAuBft7HZT9SQBjLMi
+6Et8Vcad+qMUu2WFbm5PEn4KPJ2V
+-----END CERTIFICATE-----
+
+Izenpe.com
+==========
+-----BEGIN CERTIFICATE-----
+MIIF8TCCA9mgAwIBAgIQALC3WhZIX7/hy/WL1xnmfTANBgkqhkiG9w0BAQsFADA4MQswCQYDVQQG
+EwJFUzEUMBIGA1UECgwLSVpFTlBFIFMuQS4xEzARBgNVBAMMCkl6ZW5wZS5jb20wHhcNMDcxMjEz
+MTMwODI4WhcNMzcxMjEzMDgyNzI1WjA4MQswCQYDVQQGEwJFUzEUMBIGA1UECgwLSVpFTlBFIFMu
+QS4xEzARBgNVBAMMCkl6ZW5wZS5jb20wggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQDJ
+03rKDx6sp4boFmVqscIbRTJxldn+EFvMr+eleQGPicPK8lVx93e+d5TzcqQsRNiekpsUOqHnJJAK
+ClaOxdgmlOHZSOEtPtoKct2jmRXagaKH9HtuJneJWK3W6wyyQXpzbm3benhB6QiIEn6HLmYRY2xU
++zydcsC8Lv/Ct90NduM61/e0aL6i9eOBbsFGb12N4E3GVFWJGjMxCrFXuaOKmMPsOzTFlUFpfnXC
+PCDFYbpRR6AgkJOhkEvzTnyFRVSa0QUmQbC1TR0zvsQDyCV8wXDbO/QJLVQnSKwv4cSsPsjLkkxT
+OTcj7NMB+eAJRE1NZMDhDVqHIrytG6P+JrUV86f8hBnp7KGItERphIPzidF0BqnMC9bC3ieFUCbK
+F7jJeodWLBoBHmy+E60QrLUk9TiRodZL2vG70t5HtfG8gfZZa88ZU+mNFctKy6lvROUbQc/hhqfK
+0GqfvEyNBjNaooXlkDWgYlwWTvDjovoDGrQscbNYLN57C9saD+veIR8GdwYDsMnvmfzAuU8Lhij+
+0rnq49qlw0dpEuDb8PYZi+17cNcC1u2HGCgsBCRMd+RIihrGO5rUD8r6ddIBQFqNeb+Lz0vPqhbB
+leStTIo+F5HUsWLlguWABKQDfo2/2n+iD5dPDNMN+9fR5XJ+HMh3/1uaD7euBUbl8agW7EekFwID
+AQABo4H2MIHzMIGwBgNVHREEgagwgaWBD2luZm9AaXplbnBlLmNvbaSBkTCBjjFHMEUGA1UECgw+
+SVpFTlBFIFMuQS4gLSBDSUYgQTAxMzM3MjYwLVJNZXJjLlZpdG9yaWEtR2FzdGVpeiBUMTA1NSBG
+NjIgUzgxQzBBBgNVBAkMOkF2ZGEgZGVsIE1lZGl0ZXJyYW5lbyBFdG9yYmlkZWEgMTQgLSAwMTAx
+MCBWaXRvcmlhLUdhc3RlaXowDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwHQYDVR0O
+BBYEFB0cZQ6o8iV7tJHP5LGx5r1VdGwFMA0GCSqGSIb3DQEBCwUAA4ICAQB4pgwWSp9MiDrAyw6l
+Fn2fuUhfGI8NYjb2zRlrrKvV9pF9rnHzP7MOeIWblaQnIUdCSnxIOvVFfLMMjlF4rJUT3sb9fbga
+kEyrkgPH7UIBzg/YsfqikuFgba56awmqxinuaElnMIAkejEWOVt+8Rwu3WwJrfIxwYJOubv5vr8q
+hT/AQKM6WfxZSzwoJNu0FXWuDYi6LnPAvViH5ULy617uHjAimcs30cQhbIHsvm0m5hzkQiCeR7Cs
+g1lwLDXWrzY0tM07+DKo7+N4ifuNRSzanLh+QBxh5z6ikixL8s36mLYp//Pye6kfLqCTVyvehQP5
+aTfLnnhqBbTFMXiJ7HqnheG5ezzevh55hM6fcA5ZwjUukCox2eRFekGkLhObNA5me0mrZJfQRsN5
+nXJQY6aYWwa9SG3YOYNw6DXwBdGqvOPbyALqfP2C2sJbUjWumDqtujWTI6cfSN01RpiyEGjkpTHC
+ClguGYEQyVB1/OpaFs4R1+7vUIgtYf8/QnMFlEPVjjxOAToZpR9GTnfQXeWBIiGH/pR9hNiTrdZo
+Q0iy2+tzJOeRf1SktoA+naM8THLCV8Sg1Mw4J87VBp6iSNnpn86CcDaTmjvfliHjWbcM2pE38P1Z
+WrOZyGlsQyYBNWNgVYkDOnXYukrZVP/u3oDYLdE41V4tC5h9Pmzb/CaIxw==
+-----END CERTIFICATE-----
+
+Chambers of Commerce Root - 2008
+================================
+-----BEGIN CERTIFICATE-----
+MIIHTzCCBTegAwIBAgIJAKPaQn6ksa7aMA0GCSqGSIb3DQEBBQUAMIGuMQswCQYDVQQGEwJFVTFD
+MEEGA1UEBxM6TWFkcmlkIChzZWUgY3VycmVudCBhZGRyZXNzIGF0IHd3dy5jYW1lcmZpcm1hLmNv
+bS9hZGRyZXNzKTESMBAGA1UEBRMJQTgyNzQzMjg3MRswGQYDVQQKExJBQyBDYW1lcmZpcm1hIFMu
+QS4xKTAnBgNVBAMTIENoYW1iZXJzIG9mIENvbW1lcmNlIFJvb3QgLSAyMDA4MB4XDTA4MDgwMTEy
+Mjk1MFoXDTM4MDczMTEyMjk1MFowga4xCzAJBgNVBAYTAkVVMUMwQQYDVQQHEzpNYWRyaWQgKHNl
+ZSBjdXJyZW50IGFkZHJlc3MgYXQgd3d3LmNhbWVyZmlybWEuY29tL2FkZHJlc3MpMRIwEAYDVQQF
+EwlBODI3NDMyODcxGzAZBgNVBAoTEkFDIENhbWVyZmlybWEgUy5BLjEpMCcGA1UEAxMgQ2hhbWJl
+cnMgb2YgQ29tbWVyY2UgUm9vdCAtIDIwMDgwggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoIC
+AQCvAMtwNyuAWko6bHiUfaN/Gh/2NdW928sNRHI+JrKQUrpjOyhYb6WzbZSm891kDFX29ufyIiKA
+XuFixrYp4YFs8r/lfTJqVKAyGVn+H4vXPWCGhSRv4xGzdz4gljUha7MI2XAuZPeEklPWDrCQiorj
+h40G072QDuKZoRuGDtqaCrsLYVAGUvGef3bsyw/QHg3PmTA9HMRFEFis1tPo1+XqxQEHd9ZR5gN/
+ikilTWh1uem8nk4ZcfUyS5xtYBkL+8ydddy/Js2Pk3g5eXNeJQ7KXOt3EgfLZEFHcpOrUMPrCXZk
+NNI5t3YRCQ12RcSprj1qr7V9ZS+UWBDsXHyvfuK2GNnQm05aSd+pZgvMPMZ4fKecHePOjlO+Bd5g
+D2vlGts/4+EhySnB8esHnFIbAURRPHsl18TlUlRdJQfKFiC4reRB7noI/plvg6aRArBsNlVq5331
+lubKgdaX8ZSD6e2wsWsSaR6s+12pxZjptFtYer49okQ6Y1nUCyXeG0+95QGezdIp1Z8XGQpvvwyQ
+0wlf2eOKNcx5Wk0ZN5K3xMGtr/R5JJqyAQuxr1yW84Ay+1w9mPGgP0revq+ULtlVmhduYJ1jbLhj
+ya6BXBg14JC7vjxPNyK5fuvPnnchpj04gftI2jE9K+OJ9dC1vX7gUMQSibMjmhAxhduub+84Mxh2
+EQIDAQABo4IBbDCCAWgwEgYDVR0TAQH/BAgwBgEB/wIBDDAdBgNVHQ4EFgQU+SSsD7K1+HnA+mCI
+G8TZTQKeFxkwgeMGA1UdIwSB2zCB2IAU+SSsD7K1+HnA+mCIG8TZTQKeFxmhgbSkgbEwga4xCzAJ
+BgNVBAYTAkVVMUMwQQYDVQQHEzpNYWRyaWQgKHNlZSBjdXJyZW50IGFkZHJlc3MgYXQgd3d3LmNh
+bWVyZmlybWEuY29tL2FkZHJlc3MpMRIwEAYDVQQFEwlBODI3NDMyODcxGzAZBgNVBAoTEkFDIENh
+bWVyZmlybWEgUy5BLjEpMCcGA1UEAxMgQ2hhbWJlcnMgb2YgQ29tbWVyY2UgUm9vdCAtIDIwMDiC
+CQCj2kJ+pLGu2jAOBgNVHQ8BAf8EBAMCAQYwPQYDVR0gBDYwNDAyBgRVHSAAMCowKAYIKwYBBQUH
+AgEWHGh0dHA6Ly9wb2xpY3kuY2FtZXJmaXJtYS5jb20wDQYJKoZIhvcNAQEFBQADggIBAJASryI1
+wqM58C7e6bXpeHxIvj99RZJe6dqxGfwWPJ+0W2aeaufDuV2I6A+tzyMP3iU6XsxPpcG1Lawk0lgH
+3qLPaYRgM+gQDROpI9CF5Y57pp49chNyM/WqfcZjHwj0/gF/JM8rLFQJ3uIrbZLGOU8W6jx+ekbU
+RWpGqOt1glanq6B8aBMz9p0w8G8nOSQjKpD9kCk18pPfNKXG9/jvjA9iSnyu0/VU+I22mlaHFoI6
+M6taIgj3grrqLuBHmrS1RaMFO9ncLkVAO+rcf+g769HsJtg1pDDFOqxXnrN2pSB7+R5KBWIBpih1
+YJeSDW4+TTdDDZIVnBgizVGZoCkaPF+KMjNbMMeJL0eYD6MDxvbxrN8y8NmBGuScvfaAFPDRLLmF
+9dijscilIeUcE5fuDr3fKanvNFNb0+RqE4QGtjICxFKuItLcsiFCGtpA8CnJ7AoMXOLQusxI0zcK
+zBIKinmwPQN/aUv0NCB9szTqjktk9T79syNnFQ0EuPAtwQlRPLJsFfClI9eDdOTlLsn+mCdCxqvG
+nrDQWzilm1DefhiYtUU79nm06PcaewaD+9CL2rvHvRirCG88gGtAPxkZumWK5r7VXNM21+9AUiRg
+OGcEMeyP84LG3rlV8zsxkVrctQgVrXYlCg17LofiDKYGvCYQbTed7N14jHyAxfDZd0jQ
+-----END CERTIFICATE-----
+
+Global Chambersign Root - 2008
+==============================
+-----BEGIN CERTIFICATE-----
+MIIHSTCCBTGgAwIBAgIJAMnN0+nVfSPOMA0GCSqGSIb3DQEBBQUAMIGsMQswCQYDVQQGEwJFVTFD
+MEEGA1UEBxM6TWFkcmlkIChzZWUgY3VycmVudCBhZGRyZXNzIGF0IHd3dy5jYW1lcmZpcm1hLmNv
+bS9hZGRyZXNzKTESMBAGA1UEBRMJQTgyNzQzMjg3MRswGQYDVQQKExJBQyBDYW1lcmZpcm1hIFMu
+QS4xJzAlBgNVBAMTHkdsb2JhbCBDaGFtYmVyc2lnbiBSb290IC0gMjAwODAeFw0wODA4MDExMjMx
+NDBaFw0zODA3MzExMjMxNDBaMIGsMQswCQYDVQQGEwJFVTFDMEEGA1UEBxM6TWFkcmlkIChzZWUg
+Y3VycmVudCBhZGRyZXNzIGF0IHd3dy5jYW1lcmZpcm1hLmNvbS9hZGRyZXNzKTESMBAGA1UEBRMJ
+QTgyNzQzMjg3MRswGQYDVQQKExJBQyBDYW1lcmZpcm1hIFMuQS4xJzAlBgNVBAMTHkdsb2JhbCBD
+aGFtYmVyc2lnbiBSb290IC0gMjAwODCCAiIwDQYJKoZIhvcNAQEBBQADggIPADCCAgoCggIBAMDf
+VtPkOpt2RbQT2//BthmLN0EYlVJH6xedKYiONWwGMi5HYvNJBL99RDaxccy9Wglz1dmFRP+RVyXf
+XjaOcNFccUMd2drvXNL7G706tcuto8xEpw2uIRU/uXpbknXYpBI4iRmKt4DS4jJvVpyR1ogQC7N0
+ZJJ0YPP2zxhPYLIj0Mc7zmFLmY/CDNBAspjcDahOo7kKrmCgrUVSY7pmvWjg+b4aqIG7HkF4ddPB
+/gBVsIdU6CeQNR1MM62X/JcumIS/LMmjv9GYERTtY/jKmIhYF5ntRQOXfjyGHoiMvvKRhI9lNNgA
+TH23MRdaKXoKGCQwoze1eqkBfSbW+Q6OWfH9GzO1KTsXO0G2Id3UwD2ln58fQ1DJu7xsepeY7s2M
+H/ucUa6LcL0nn3HAa6x9kGbo1106DbDVwo3VyJ2dwW3Q0L9R5OP4wzg2rtandeavhENdk5IMagfe
+Ox2YItaswTXbo6Al/3K1dh3ebeksZixShNBFks4c5eUzHdwHU1SjqoI7mjcv3N2gZOnm3b2u/GSF
+HTynyQbehP9r6GsaPMWis0L7iwk+XwhSx2LE1AVxv8Rk5Pihg+g+EpuoHtQ2TS9x9o0o9oOpE9Jh
+wZG7SMA0j0GMS0zbaRL/UJScIINZc+18ofLx/d33SdNDWKBWY8o9PeU1VlnpDsogzCtLkykPAgMB
+AAGjggFqMIIBZjASBgNVHRMBAf8ECDAGAQH/AgEMMB0GA1UdDgQWBBS5CcqcHtvTbDprru1U8VuT
+BjUuXjCB4QYDVR0jBIHZMIHWgBS5CcqcHtvTbDprru1U8VuTBjUuXqGBsqSBrzCBrDELMAkGA1UE
+BhMCRVUxQzBBBgNVBAcTOk1hZHJpZCAoc2VlIGN1cnJlbnQgYWRkcmVzcyBhdCB3d3cuY2FtZXJm
+aXJtYS5jb20vYWRkcmVzcykxEjAQBgNVBAUTCUE4Mjc0MzI4NzEbMBkGA1UEChMSQUMgQ2FtZXJm
+aXJtYSBTLkEuMScwJQYDVQQDEx5HbG9iYWwgQ2hhbWJlcnNpZ24gUm9vdCAtIDIwMDiCCQDJzdPp
+1X0jzjAOBgNVHQ8BAf8EBAMCAQYwPQYDVR0gBDYwNDAyBgRVHSAAMCowKAYIKwYBBQUHAgEWHGh0
+dHA6Ly9wb2xpY3kuY2FtZXJmaXJtYS5jb20wDQYJKoZIhvcNAQEFBQADggIBAICIf3DekijZBZRG
+/5BXqfEv3xoNa/p8DhxJJHkn2EaqbylZUohwEurdPfWbU1Rv4WCiqAm57OtZfMY18dwY6fFn5a+6
+ReAJ3spED8IXDneRRXozX1+WLGiLwUePmJs9wOzL9dWCkoQ10b42OFZyMVtHLaoXpGNR6woBrX/s
+dZ7LoR/xfxKxueRkf2fWIyr0uDldmOghp+G9PUIadJpwr2hsUF1Jz//7Dl3mLEfXgTpZALVza2Mg
+9jFFCDkO9HB+QHBaP9BrQql0PSgvAm11cpUJjUhjxsYjV5KTXjXBjfkK9yydYhz2rXzdpjEetrHH
+foUm+qRqtdpjMNHvkzeyZi99Bffnt0uYlDXA2TopwZ2yUDMdSqlapskD7+3056huirRXhOukP9Du
+qqqHW2Pok+JrqNS4cnhrG+055F3Lm6qH1U9OAP7Zap88MQ8oAgF9mOinsKJknnn4SPIVqczmyETr
+P3iZ8ntxPjzxmKfFGBI/5rsoM0LpRQp8bfKGeS/Fghl9CYl8slR2iK7ewfPM4W7bMdaTrpmg7yVq
+c5iJWzouE4gev8CSlDQb4ye3ix5vQv/n6TebUB0tovkC7stYWDpxvGjjqsGvHCgfotwjZT+B6q6Z
+09gwzxMNTxXJhLynSC34MCN32EZLeW32jO06f2ARePTpm67VVMB0gNELQp/B
+-----END CERTIFICATE-----
+
+Go Daddy Root Certificate Authority - G2
+========================================
+-----BEGIN CERTIFICATE-----
+MIIDxTCCAq2gAwIBAgIBADANBgkqhkiG9w0BAQsFADCBgzELMAkGA1UEBhMCVVMxEDAOBgNVBAgT
+B0FyaXpvbmExEzARBgNVBAcTClNjb3R0c2RhbGUxGjAYBgNVBAoTEUdvRGFkZHkuY29tLCBJbmMu
+MTEwLwYDVQQDEyhHbyBEYWRkeSBSb290IENlcnRpZmljYXRlIEF1dGhvcml0eSAtIEcyMB4XDTA5
+MDkwMTAwMDAwMFoXDTM3MTIzMTIzNTk1OVowgYMxCzAJBgNVBAYTAlVTMRAwDgYDVQQIEwdBcml6
+b25hMRMwEQYDVQQHEwpTY290dHNkYWxlMRowGAYDVQQKExFHb0RhZGR5LmNvbSwgSW5jLjExMC8G
+A1UEAxMoR28gRGFkZHkgUm9vdCBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkgLSBHMjCCASIwDQYJKoZI
+hvcNAQEBBQADggEPADCCAQoCggEBAL9xYgjx+lk09xvJGKP3gElY6SKDE6bFIEMBO4Tx5oVJnyfq
+9oQbTqC023CYxzIBsQU+B07u9PpPL1kwIuerGVZr4oAH/PMWdYA5UXvl+TW2dE6pjYIT5LY/qQOD
++qK+ihVqf94Lw7YZFAXK6sOoBJQ7RnwyDfMAZiLIjWltNowRGLfTshxgtDj6AozO091GB94KPutd
+fMh8+7ArU6SSYmlRJQVhGkSBjCypQ5Yj36w6gZoOKcUcqeldHraenjAKOc7xiID7S13MMuyFYkMl
+NAJWJwGRtDtwKj9useiciAF9n9T521NtYJ2/LOdYq7hfRvzOxBsDPAnrSTFcaUaz4EcCAwEAAaNC
+MEAwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwHQYDVR0OBBYEFDqahQcQZyi27/a9
+BUFuIMGU2g/eMA0GCSqGSIb3DQEBCwUAA4IBAQCZ21151fmXWWcDYfF+OwYxdS2hII5PZYe096ac
+vNjpL9DbWu7PdIxztDhC2gV7+AJ1uP2lsdeu9tfeE8tTEH6KRtGX+rcuKxGrkLAngPnon1rpN5+r
+5N9ss4UXnT3ZJE95kTXWXwTrgIOrmgIttRD02JDHBHNA7XIloKmf7J6raBKZV8aPEjoJpL1E/QYV
+N8Gb5DKj7Tjo2GTzLH4U/ALqn83/B2gX2yKQOC16jdFU8WnjXzPKej17CuPKf1855eJ1usV2GDPO
+LPAvTK33sefOT6jEm0pUBsV/fdUID+Ic/n4XuKxe9tQWskMJDE32p2u0mYRlynqI4uJEvlz36hz1
+-----END CERTIFICATE-----
+
+Starfield Root Certificate Authority - G2
+=========================================
+-----BEGIN CERTIFICATE-----
+MIID3TCCAsWgAwIBAgIBADANBgkqhkiG9w0BAQsFADCBjzELMAkGA1UEBhMCVVMxEDAOBgNVBAgT
+B0FyaXpvbmExEzARBgNVBAcTClNjb3R0c2RhbGUxJTAjBgNVBAoTHFN0YXJmaWVsZCBUZWNobm9s
+b2dpZXMsIEluYy4xMjAwBgNVBAMTKVN0YXJmaWVsZCBSb290IENlcnRpZmljYXRlIEF1dGhvcml0
+eSAtIEcyMB4XDTA5MDkwMTAwMDAwMFoXDTM3MTIzMTIzNTk1OVowgY8xCzAJBgNVBAYTAlVTMRAw
+DgYDVQQIEwdBcml6b25hMRMwEQYDVQQHEwpTY290dHNkYWxlMSUwIwYDVQQKExxTdGFyZmllbGQg
+VGVjaG5vbG9naWVzLCBJbmMuMTIwMAYDVQQDEylTdGFyZmllbGQgUm9vdCBDZXJ0aWZpY2F0ZSBB
+dXRob3JpdHkgLSBHMjCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAL3twQP89o/8ArFv
+W59I2Z154qK3A2FWGMNHttfKPTUuiUP3oWmb3ooa/RMgnLRJdzIpVv257IzdIvpy3Cdhl+72WoTs
+bhm5iSzchFvVdPtrX8WJpRBSiUZV9Lh1HOZ/5FSuS/hVclcCGfgXcVnrHigHdMWdSL5stPSksPNk
+N3mSwOxGXn/hbVNMYq/NHwtjuzqd+/x5AJhhdM8mgkBj87JyahkNmcrUDnXMN/uLicFZ8WJ/X7Nf
+ZTD4p7dNdloedl40wOiWVpmKs/B/pM293DIxfJHP4F8R+GuqSVzRmZTRouNjWwl2tVZi4Ut0HZbU
+JtQIBFnQmA4O5t78w+wfkPECAwEAAaNCMEAwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMC
+AQYwHQYDVR0OBBYEFHwMMh+n2TB/xH1oo2Kooc6rB1snMA0GCSqGSIb3DQEBCwUAA4IBAQARWfol
+TwNvlJk7mh+ChTnUdgWUXuEok21iXQnCoKjUsHU48TRqneSfioYmUeYs0cYtbpUgSpIB7LiKZ3sx
+4mcujJUDJi5DnUox9g61DLu34jd/IroAow57UvtruzvE03lRTs2Q9GcHGcg8RnoNAX3FWOdt5oUw
+F5okxBDgBPfg8n/Uqgr/Qh037ZTlZFkSIHc40zI+OIF1lnP6aI+xy84fxez6nH7PfrHxBy22/L/K
+pL/QlwVKvOoYKAKQvVR4CSFx09F9HdkWsKlhPdAKACL8x3vLCWRFCztAgfd9fDL1mMpYjn0q7pBZ
+c2T5NnReJaH1ZgUufzkVqSr7UIuOhWn0
+-----END CERTIFICATE-----
+
+Starfield Services Root Certificate Authority - G2
+==================================================
+-----BEGIN CERTIFICATE-----
+MIID7zCCAtegAwIBAgIBADANBgkqhkiG9w0BAQsFADCBmDELMAkGA1UEBhMCVVMxEDAOBgNVBAgT
+B0FyaXpvbmExEzARBgNVBAcTClNjb3R0c2RhbGUxJTAjBgNVBAoTHFN0YXJmaWVsZCBUZWNobm9s
+b2dpZXMsIEluYy4xOzA5BgNVBAMTMlN0YXJmaWVsZCBTZXJ2aWNlcyBSb290IENlcnRpZmljYXRl
+IEF1dGhvcml0eSAtIEcyMB4XDTA5MDkwMTAwMDAwMFoXDTM3MTIzMTIzNTk1OVowgZgxCzAJBgNV
+BAYTAlVTMRAwDgYDVQQIEwdBcml6b25hMRMwEQYDVQQHEwpTY290dHNkYWxlMSUwIwYDVQQKExxT
+dGFyZmllbGQgVGVjaG5vbG9naWVzLCBJbmMuMTswOQYDVQQDEzJTdGFyZmllbGQgU2VydmljZXMg
+Um9vdCBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkgLSBHMjCCASIwDQYJKoZIhvcNAQEBBQADggEPADCC
+AQoCggEBANUMOsQq+U7i9b4Zl1+OiFOxHz/Lz58gE20pOsgPfTz3a3Y4Y9k2YKibXlwAgLIvWX/2
+h/klQ4bnaRtSmpDhcePYLQ1Ob/bISdm28xpWriu2dBTrz/sm4xq6HZYuajtYlIlHVv8loJNwU4Pa
+hHQUw2eeBGg6345AWh1KTs9DkTvnVtYAcMtS7nt9rjrnvDH5RfbCYM8TWQIrgMw0R9+53pBlbQLP
+LJGmpufehRhJfGZOozptqbXuNC66DQO4M99H67FrjSXZm86B0UVGMpZwh94CDklDhbZsc7tk6mFB
+rMnUVN+HL8cisibMn1lUaJ/8viovxFUcdUBgF4UCVTmLfwUCAwEAAaNCMEAwDwYDVR0TAQH/BAUw
+AwEB/zAOBgNVHQ8BAf8EBAMCAQYwHQYDVR0OBBYEFJxfAN+qAdcwKziIorhtSpzyEZGDMA0GCSqG
+SIb3DQEBCwUAA4IBAQBLNqaEd2ndOxmfZyMIbw5hyf2E3F/YNoHN2BtBLZ9g3ccaaNnRbobhiCPP
+E95Dz+I0swSdHynVv/heyNXBve6SbzJ08pGCL72CQnqtKrcgfU28elUSwhXqvfdqlS5sdJ/PHLTy
+xQGjhdByPq1zqwubdQxtRbeOlKyWN7Wg0I8VRw7j6IPdj/3vQQF3zCepYoUz8jcI73HPdwbeyBkd
+iEDPfUYd/x7H4c7/I9vG+o1VTqkC50cRRj70/b17KSa7qWFiNyi2LSr2EIZkyXCn0q23KXB56jza
+YyWf/Wi3MOxw+3WKt21gZ7IeyLnp2KhvAotnDU0mV3HaIPzBSlCNsSi6
+-----END CERTIFICATE-----
+
+AffirmTrust Commercial
+======================
+-----BEGIN CERTIFICATE-----
+MIIDTDCCAjSgAwIBAgIId3cGJyapsXwwDQYJKoZIhvcNAQELBQAwRDELMAkGA1UEBhMCVVMxFDAS
+BgNVBAoMC0FmZmlybVRydXN0MR8wHQYDVQQDDBZBZmZpcm1UcnVzdCBDb21tZXJjaWFsMB4XDTEw
+MDEyOTE0MDYwNloXDTMwMTIzMTE0MDYwNlowRDELMAkGA1UEBhMCVVMxFDASBgNVBAoMC0FmZmly
+bVRydXN0MR8wHQYDVQQDDBZBZmZpcm1UcnVzdCBDb21tZXJjaWFsMIIBIjANBgkqhkiG9w0BAQEF
+AAOCAQ8AMIIBCgKCAQEA9htPZwcroRX1BiLLHwGy43NFBkRJLLtJJRTWzsO3qyxPxkEylFf6Eqdb
+DuKPHx6GGaeqtS25Xw2Kwq+FNXkyLbscYjfysVtKPcrNcV/pQr6U6Mje+SJIZMblq8Yrba0F8PrV
+C8+a5fBQpIs7R6UjW3p6+DM/uO+Zl+MgwdYoic+U+7lF7eNAFxHUdPALMeIrJmqbTFeurCA+ukV6
+BfO9m2kVrn1OIGPENXY6BwLJN/3HR+7o8XYdcxXyl6S1yHp52UKqK39c/s4mT6NmgTWvRLpUHhww
+MmWd5jyTXlBOeuM61G7MGvv50jeuJCqrVwMiKA1JdX+3KNp1v47j3A55MQIDAQABo0IwQDAdBgNV
+HQ4EFgQUnZPGU4teyq8/nx4P5ZmVvCT2lI8wDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMC
+AQYwDQYJKoZIhvcNAQELBQADggEBAFis9AQOzcAN/wr91LoWXym9e2iZWEnStB03TX8nfUYGXUPG
+hi4+c7ImfU+TqbbEKpqrIZcUsd6M06uJFdhrJNTxFq7YpFzUf1GO7RgBsZNjvbz4YYCanrHOQnDi
+qX0GJX0nof5v7LMeJNrjS1UaADs1tDvZ110w/YETifLCBivtZ8SOyUOyXGsViQK8YvxO8rUzqrJv
+0wqiUOP2O+guRMLbZjipM1ZI8W0bM40NjD9gN53Tym1+NH4Nn3J2ixufcv1SNUFFApYvHLKac0kh
+sUlHRUe072o0EclNmsxZt9YCnlpOZbWUrhvfKbAW8b8Angc6F2S1BLUjIZkKlTuXfO8=
+-----END CERTIFICATE-----
+
+AffirmTrust Networking
+======================
+-----BEGIN CERTIFICATE-----
+MIIDTDCCAjSgAwIBAgIIfE8EORzUmS0wDQYJKoZIhvcNAQEFBQAwRDELMAkGA1UEBhMCVVMxFDAS
+BgNVBAoMC0FmZmlybVRydXN0MR8wHQYDVQQDDBZBZmZpcm1UcnVzdCBOZXR3b3JraW5nMB4XDTEw
+MDEyOTE0MDgyNFoXDTMwMTIzMTE0MDgyNFowRDELMAkGA1UEBhMCVVMxFDASBgNVBAoMC0FmZmly
+bVRydXN0MR8wHQYDVQQDDBZBZmZpcm1UcnVzdCBOZXR3b3JraW5nMIIBIjANBgkqhkiG9w0BAQEF
+AAOCAQ8AMIIBCgKCAQEAtITMMxcua5Rsa2FSoOujz3mUTOWUgJnLVWREZY9nZOIG41w3SfYvm4SE
+Hi3yYJ0wTsyEheIszx6e/jarM3c1RNg1lho9Nuh6DtjVR6FqaYvZ/Ls6rnla1fTWcbuakCNrmreI
+dIcMHl+5ni36q1Mr3Lt2PpNMCAiMHqIjHNRqrSK6mQEubWXLviRmVSRLQESxG9fhwoXA3hA/Pe24
+/PHxI1Pcv2WXb9n5QHGNfb2V1M6+oF4nI979ptAmDgAp6zxG8D1gvz9Q0twmQVGeFDdCBKNwV6gb
+h+0t+nvujArjqWaJGctB+d1ENmHP4ndGyH329JKBNv3bNPFyfvMMFr20FQIDAQABo0IwQDAdBgNV
+HQ4EFgQUBx/S55zawm6iQLSwelAQUHTEyL0wDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMC
+AQYwDQYJKoZIhvcNAQEFBQADggEBAIlXshZ6qML91tmbmzTCnLQyFE2npN/svqe++EPbkTfOtDIu
+UFUaNU52Q3Eg75N3ThVwLofDwR1t3Mu1J9QsVtFSUzpE0nPIxBsFZVpikpzuQY0x2+c06lkh1QF6
+12S4ZDnNye2v7UsDSKegmQGA3GWjNq5lWUhPgkvIZfFXHeVZLgo/bNjR9eUJtGxUAArgFU2HdW23
+WJZa3W3SAKD0m0i+wzekujbgfIeFlxoVot4uolu9rxj5kFDNcFn4J2dHy8egBzp90SxdbBk6ZrV9
+/ZFvgrG+CJPbFEfxojfHRZ48x3evZKiT3/Zpg4Jg8klCNO1aAFSFHBY2kgxc+qatv9s=
+-----END CERTIFICATE-----
+
+AffirmTrust Premium
+===================
+-----BEGIN CERTIFICATE-----
+MIIFRjCCAy6gAwIBAgIIbYwURrGmCu4wDQYJKoZIhvcNAQEMBQAwQTELMAkGA1UEBhMCVVMxFDAS
+BgNVBAoMC0FmZmlybVRydXN0MRwwGgYDVQQDDBNBZmZpcm1UcnVzdCBQcmVtaXVtMB4XDTEwMDEy
+OTE0MTAzNloXDTQwMTIzMTE0MTAzNlowQTELMAkGA1UEBhMCVVMxFDASBgNVBAoMC0FmZmlybVRy
+dXN0MRwwGgYDVQQDDBNBZmZpcm1UcnVzdCBQcmVtaXVtMIICIjANBgkqhkiG9w0BAQEFAAOCAg8A
+MIICCgKCAgEAxBLfqV/+Qd3d9Z+K4/as4Tx4mrzY8H96oDMq3I0gW64tb+eT2TZwamjPjlGjhVtn
+BKAQJG9dKILBl1fYSCkTtuG+kU3fhQxTGJoeJKJPj/CihQvL9Cl/0qRY7iZNyaqoe5rZ+jjeRFcV
+5fiMyNlI4g0WJx0eyIOFJbe6qlVBzAMiSy2RjYvmia9mx+n/K+k8rNrSs8PhaJyJ+HoAVt70VZVs
++7pk3WKL3wt3MutizCaam7uqYoNMtAZ6MMgpv+0GTZe5HMQxK9VfvFMSF5yZVylmd2EhMQcuJUmd
+GPLu8ytxjLW6OQdJd/zvLpKQBY0tL3d770O/Nbua2Plzpyzy0FfuKE4mX4+QaAkvuPjcBukumj5R
+p9EixAqnOEhss/n/fauGV+O61oV4d7pD6kh/9ti+I20ev9E2bFhc8e6kGVQa9QPSdubhjL08s9NI
+S+LI+H+SqHZGnEJlPqQewQcDWkYtuJfzt9WyVSHvutxMAJf7FJUnM7/oQ0dG0giZFmA7mn7S5u04
+6uwBHjxIVkkJx0w3AJ6IDsBz4W9m6XJHMD4Q5QsDyZpCAGzFlH5hxIrff4IaC1nEWTJ3s7xgaVY5
+/bQGeyzWZDbZvUjthB9+pSKPKrhC9IK31FOQeE4tGv2Bb0TXOwF0lkLgAOIua+rF7nKsu7/+6qqo
++Nz2snmKtmcCAwEAAaNCMEAwHQYDVR0OBBYEFJ3AZ6YMItkm9UWrpmVSESfYRaxjMA8GA1UdEwEB
+/wQFMAMBAf8wDgYDVR0PAQH/BAQDAgEGMA0GCSqGSIb3DQEBDAUAA4ICAQCzV00QYk465KzquByv
+MiPIs0laUZx2KI15qldGF9X1Uva3ROgIRL8YhNILgM3FEv0AVQVhh0HctSSePMTYyPtwni94loMg
+Nt58D2kTiKV1NpgIpsbfrM7jWNa3Pt668+s0QNiigfV4Py/VpfzZotReBA4Xrf5B8OWycvpEgjNC
+6C1Y91aMYj+6QrCcDFx+LmUmXFNPALJ4fqENmS2NuB2OosSw/WDQMKSOyARiqcTtNd56l+0OOF6S
+L5Nwpamcb6d9Ex1+xghIsV5n61EIJenmJWtSKZGc0jlzCFfemQa0W50QBuHCAKi4HEoCChTQwUHK
++4w1IX2COPKpVJEZNZOUbWo6xbLQu4mGk+ibyQ86p3q4ofB4Rvr8Ny/lioTz3/4E2aFooC8k4gmV
+BtWVyuEklut89pMFu+1z6S3RdTnX5yTb2E5fQ4+e0BQ5v1VwSJlXMbSc7kqYA5YwH2AG7hsj/oFg
+IxpHYoWlzBk0gG+zrBrjn/B7SK3VAdlntqlyk+otZrWyuOQ9PLLvTIzq6we/qzWaVYa8GKa1qF60
+g2xraUDTn9zxw2lrueFtCfTxqlB2Cnp9ehehVZZCmTEJ3WARjQUwfuaORtGdFNrHF+QFlozEJLUb
+zxQHskD4o55BhrwE0GuWyCqANP2/7waj3VjFhT0+j/6eKeC2uAloGRwYQw==
+-----END CERTIFICATE-----
+
+AffirmTrust Premium ECC
+=======================
+-----BEGIN CERTIFICATE-----
+MIIB/jCCAYWgAwIBAgIIdJclisc/elQwCgYIKoZIzj0EAwMwRTELMAkGA1UEBhMCVVMxFDASBgNV
+BAoMC0FmZmlybVRydXN0MSAwHgYDVQQDDBdBZmZpcm1UcnVzdCBQcmVtaXVtIEVDQzAeFw0xMDAx
+MjkxNDIwMjRaFw00MDEyMzExNDIwMjRaMEUxCzAJBgNVBAYTAlVTMRQwEgYDVQQKDAtBZmZpcm1U
+cnVzdDEgMB4GA1UEAwwXQWZmaXJtVHJ1c3QgUHJlbWl1bSBFQ0MwdjAQBgcqhkjOPQIBBgUrgQQA
+IgNiAAQNMF4bFZ0D0KF5Nbc6PJJ6yhUczWLznCZcBz3lVPqj1swS6vQUX+iOGasvLkjmrBhDeKzQ
+N8O9ss0s5kfiGuZjuD0uL3jET9v0D6RoTFVya5UdThhClXjMNzyR4ptlKymjQjBAMB0GA1UdDgQW
+BBSaryl6wBE1NSZRMADDav5A1a7WPDAPBgNVHRMBAf8EBTADAQH/MA4GA1UdDwEB/wQEAwIBBjAK
+BggqhkjOPQQDAwNnADBkAjAXCfOHiFBar8jAQr9HX/VsaobgxCd05DhT1wV/GzTjxi+zygk8N53X
+57hG8f2h4nECMEJZh0PUUd+60wkyWs6Iflc9nF9Ca/UHLbXwgpP5WW+uZPpY5Yse42O+tYHNbwKM
+eQ==
+-----END CERTIFICATE-----
+
+Certum Trusted Network CA
+=========================
+-----BEGIN CERTIFICATE-----
+MIIDuzCCAqOgAwIBAgIDBETAMA0GCSqGSIb3DQEBBQUAMH4xCzAJBgNVBAYTAlBMMSIwIAYDVQQK
+ExlVbml6ZXRvIFRlY2hub2xvZ2llcyBTLkEuMScwJQYDVQQLEx5DZXJ0dW0gQ2VydGlmaWNhdGlv
+biBBdXRob3JpdHkxIjAgBgNVBAMTGUNlcnR1bSBUcnVzdGVkIE5ldHdvcmsgQ0EwHhcNMDgxMDIy
+MTIwNzM3WhcNMjkxMjMxMTIwNzM3WjB+MQswCQYDVQQGEwJQTDEiMCAGA1UEChMZVW5pemV0byBU
+ZWNobm9sb2dpZXMgUy5BLjEnMCUGA1UECxMeQ2VydHVtIENlcnRpZmljYXRpb24gQXV0aG9yaXR5
+MSIwIAYDVQQDExlDZXJ0dW0gVHJ1c3RlZCBOZXR3b3JrIENBMIIBIjANBgkqhkiG9w0BAQEFAAOC
+AQ8AMIIBCgKCAQEA4/t9o3K6wvDJFIf1awFO4W5AB7ptJ11/91sts1rHUV+rpDKmYYe2bg+G0jAC
+l/jXaVehGDldamR5xgFZrDwxSjh80gTSSyjoIF87B6LMTXPb865Px1bVWqeWifrzq2jUI4ZZJ88J
+J7ysbnKDHDBy3+Ci6dLhdHUZvSqeexVUBBvXQzmtVSjF4hq79MDkrjhJM8x2hZ85RdKknvISjFH4
+fOQtf/WsX+sWn7Et0brMkUJ3TCXJkDhv2/DM+44el1k+1WBO5gUo7Ul5E0u6SNsv+XLTOcr+H9g0
+cvW0QM8xAcPs3hEtF10fuFDRXhmnad4HMyjKUJX5p1TLVIZQRan5SQIDAQABo0IwQDAPBgNVHRMB
+Af8EBTADAQH/MB0GA1UdDgQWBBQIds3LB/8k9sXN7buQvOKEN0Z19zAOBgNVHQ8BAf8EBAMCAQYw
+DQYJKoZIhvcNAQEFBQADggEBAKaorSLOAT2mo/9i0Eidi15ysHhE49wcrwn9I0j6vSrEuVUEtRCj
+jSfeC4Jj0O7eDDd5QVsisrCaQVymcODU0HfLI9MA4GxWL+FpDQ3Zqr8hgVDZBqWo/5U30Kr+4rP1
+mS1FhIrlQgnXdAIv94nYmem8J9RHjboNRhx3zxSkHLmkMcScKHQDNP8zGSal6Q10tz6XxnboJ5aj
+Zt3hrvJBW8qYVoNzcOSGGtIxQbovvi0TWnZvTuhOgQ4/WwMioBK+ZlgRSssDxLQqKi2WF+A5VLxI
+03YnnZotBqbJ7DnSq9ufmgsnAjUpsUCV5/nonFWIGUbWtzT1fs45mtk48VH3Tyw=
+-----END CERTIFICATE-----
+
+Certinomis - Autorit√© Racine
+=============================
+-----BEGIN CERTIFICATE-----
+MIIFnDCCA4SgAwIBAgIBATANBgkqhkiG9w0BAQUFADBjMQswCQYDVQQGEwJGUjETMBEGA1UEChMK
+Q2VydGlub21pczEXMBUGA1UECxMOMDAwMiA0MzM5OTg5MDMxJjAkBgNVBAMMHUNlcnRpbm9taXMg
+LSBBdXRvcml0w6kgUmFjaW5lMB4XDTA4MDkxNzA4Mjg1OVoXDTI4MDkxNzA4Mjg1OVowYzELMAkG
+A1UEBhMCRlIxEzARBgNVBAoTCkNlcnRpbm9taXMxFzAVBgNVBAsTDjAwMDIgNDMzOTk4OTAzMSYw
+JAYDVQQDDB1DZXJ0aW5vbWlzIC0gQXV0b3JpdMOpIFJhY2luZTCCAiIwDQYJKoZIhvcNAQEBBQAD
+ggIPADCCAgoCggIBAJ2Fn4bT46/HsmtuM+Cet0I0VZ35gb5j2CN2DpdUzZlMGvE5x4jYF1AMnmHa
+wE5V3udauHpOd4cN5bjr+p5eex7Ezyh0x5P1FMYiKAT5kcOrJ3NqDi5N8y4oH3DfVS9O7cdxbwly
+Lu3VMpfQ8Vh30WC8Tl7bmoT2R2FFK/ZQpn9qcSdIhDWerP5pqZ56XjUl+rSnSTV3lqc2W+HN3yNw
+2F1MpQiD8aYkOBOo7C+ooWfHpi2GR+6K/OybDnT0K0kCe5B1jPyZOQE51kqJ5Z52qz6WKDgmi92N
+jMD2AR5vpTESOH2VwnHu7XSu5DaiQ3XV8QCb4uTXzEIDS3h65X27uK4uIJPT5GHfceF2Z5c/tt9q
+c1pkIuVC28+BA5PY9OMQ4HL2AHCs8MF6DwV/zzRpRbWT5BnbUhYjBYkOjUjkJW+zeL9i9Qf6lSTC
+lrLooyPCXQP8w9PlfMl1I9f09bze5N/NgL+RiH2nE7Q5uiy6vdFrzPOlKO1Enn1So2+WLhl+HPNb
+xxaOu2B9d2ZHVIIAEWBsMsGoOBvrbpgT1u449fCfDu/+MYHB0iSVL1N6aaLwD4ZFjliCK0wi1F6g
+530mJ0jfJUaNSih8hp75mxpZuWW/Bd22Ql095gBIgl4g9xGC3srYn+Y3RyYe63j3YcNBZFgCQfna
+4NH4+ej9Uji29YnfAgMBAAGjWzBZMA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/BAQDAgEGMB0G
+A1UdDgQWBBQNjLZh2kS40RR9w759XkjwzspqsDAXBgNVHSAEEDAOMAwGCiqBegFWAgIAAQEwDQYJ
+KoZIhvcNAQEFBQADggIBACQ+YAZ+He86PtvqrxyaLAEL9MW12Ukx9F1BjYkMTv9sov3/4gbIOZ/x
+WqndIlgVqIrTseYyCYIDbNc/CMf4uboAbbnW/FIyXaR/pDGUu7ZMOH8oMDX/nyNTt7buFHAAQCva
+R6s0fl6nVjBhK4tDrP22iCj1a7Y+YEq6QpA0Z43q619FVDsXrIvkxmUP7tCMXWY5zjKn2BCXwH40
+nJ+U8/aGH88bc62UeYdocMMzpXDn2NU4lG9jeeu/Cg4I58UvD0KgKxRA/yHgBcUn4YQRE7rWhh1B
+CxMjidPJC+iKunqjo3M3NYB9Ergzd0A4wPpeMNLytqOx1qKVl4GbUu1pTP+A5FPbVFsDbVRfsbjv
+JL1vnxHDx2TCDyhihWZeGnuyt++uNckZM6i4J9szVb9o4XVIRFb7zdNIu0eJOqxp9YDG5ERQL1TE
+qkPFMTFYvZbF6nVsmnWxTfj3l/+WFvKXTej28xH5On2KOG4Ey+HTRRWqpdEdnV1j6CTmNhTih60b
+WfVEm/vXd3wfAXBioSAaosUaKPQhA+4u2cGA6rnZgtZbdsLLO7XSAPCjDuGtbkD326C00EauFddE
+wk01+dIL8hf2rGbVJLJP0RyZwG71fet0BLj5TXcJ17TPBzAJ8bgAVtkXFhYKK4bfjwEZGuW7gmP/
+vgt2Fl43N+bYdJeimUV5
+-----END CERTIFICATE-----
+
+Root CA Generalitat Valenciana
+==============================
+-----BEGIN CERTIFICATE-----
+MIIGizCCBXOgAwIBAgIEO0XlaDANBgkqhkiG9w0BAQUFADBoMQswCQYDVQQGEwJFUzEfMB0GA1UE
+ChMWR2VuZXJhbGl0YXQgVmFsZW5jaWFuYTEPMA0GA1UECxMGUEtJR1ZBMScwJQYDVQQDEx5Sb290
+IENBIEdlbmVyYWxpdGF0IFZhbGVuY2lhbmEwHhcNMDEwNzA2MTYyMjQ3WhcNMjEwNzAxMTUyMjQ3
+WjBoMQswCQYDVQQGEwJFUzEfMB0GA1UEChMWR2VuZXJhbGl0YXQgVmFsZW5jaWFuYTEPMA0GA1UE
+CxMGUEtJR1ZBMScwJQYDVQQDEx5Sb290IENBIEdlbmVyYWxpdGF0IFZhbGVuY2lhbmEwggEiMA0G
+CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDGKqtXETcvIorKA3Qdyu0togu8M1JAJke+WmmmO3I2
+F0zo37i7L3bhQEZ0ZQKQUgi0/6iMweDHiVYQOTPvaLRfX9ptI6GJXiKjSgbwJ/BXufjpTjJ3Cj9B
+ZPPrZe52/lSqfR0grvPXdMIKX/UIKFIIzFVd0g/bmoGlu6GzwZTNVOAydTGRGmKy3nXiz0+J2ZGQ
+D0EbtFpKd71ng+CT516nDOeB0/RSrFOyA8dEJvt55cs0YFAQexvba9dHq198aMpunUEDEO5rmXte
+JajCq+TA81yc477OMUxkHl6AovWDfgzWyoxVjr7gvkkHD6MkQXpYHYTqWBLI4bft75PelAgxAgMB
+AAGjggM7MIIDNzAyBggrBgEFBQcBAQQmMCQwIgYIKwYBBQUHMAGGFmh0dHA6Ly9vY3NwLnBraS5n
+dmEuZXMwEgYDVR0TAQH/BAgwBgEB/wIBAjCCAjQGA1UdIASCAiswggInMIICIwYKKwYBBAG/VQIB
+ADCCAhMwggHoBggrBgEFBQcCAjCCAdoeggHWAEEAdQB0AG8AcgBpAGQAYQBkACAAZABlACAAQwBl
+AHIAdABpAGYAaQBjAGEAYwBpAPMAbgAgAFIAYQDtAHoAIABkAGUAIABsAGEAIABHAGUAbgBlAHIA
+YQBsAGkAdABhAHQAIABWAGEAbABlAG4AYwBpAGEAbgBhAC4ADQAKAEwAYQAgAEQAZQBjAGwAYQBy
+AGEAYwBpAPMAbgAgAGQAZQAgAFAAcgDhAGMAdABpAGMAYQBzACAAZABlACAAQwBlAHIAdABpAGYA
+aQBjAGEAYwBpAPMAbgAgAHEAdQBlACAAcgBpAGcAZQAgAGUAbAAgAGYAdQBuAGMAaQBvAG4AYQBt
+AGkAZQBuAHQAbwAgAGQAZQAgAGwAYQAgAHAAcgBlAHMAZQBuAHQAZQAgAEEAdQB0AG8AcgBpAGQA
+YQBkACAAZABlACAAQwBlAHIAdABpAGYAaQBjAGEAYwBpAPMAbgAgAHMAZQAgAGUAbgBjAHUAZQBu
+AHQAcgBhACAAZQBuACAAbABhACAAZABpAHIAZQBjAGMAaQDzAG4AIAB3AGUAYgAgAGgAdAB0AHAA
+OgAvAC8AdwB3AHcALgBwAGsAaQAuAGcAdgBhAC4AZQBzAC8AYwBwAHMwJQYIKwYBBQUHAgEWGWh0
+dHA6Ly93d3cucGtpLmd2YS5lcy9jcHMwHQYDVR0OBBYEFHs100DSHHgZZu90ECjcPk+yeAT8MIGV
+BgNVHSMEgY0wgYqAFHs100DSHHgZZu90ECjcPk+yeAT8oWykajBoMQswCQYDVQQGEwJFUzEfMB0G
+A1UEChMWR2VuZXJhbGl0YXQgVmFsZW5jaWFuYTEPMA0GA1UECxMGUEtJR1ZBMScwJQYDVQQDEx5S
+b290IENBIEdlbmVyYWxpdGF0IFZhbGVuY2lhbmGCBDtF5WgwDQYJKoZIhvcNAQEFBQADggEBACRh
+TvW1yEICKrNcda3FbcrnlD+laJWIwVTAEGmiEi8YPyVQqHxK6sYJ2fR1xkDar1CdPaUWu20xxsdz
+Ckj+IHLtb8zog2EWRpABlUt9jppSCS/2bxzkoXHPjCpaF3ODR00PNvsETUlR4hTJZGH71BTg9J63
+NI8KJr2XXPR5OkowGcytT6CYirQxlyric21+eLj4iIlPsSKRZEv1UN4D2+XFducTZnV+ZfsBn5OH
+iJ35Rld8TWCvmHMTI6QgkYH60GFmuH3Rr9ZvHmw96RH9qfmCIoaZM3Fa6hlXPZHNqcCjbgcTpsnt
++GijnsNacgmHKNHEc8RzGF9QdRYxn7fofMM=
+-----END CERTIFICATE-----
+
+A-Trust-nQual-03
+================
+-----BEGIN CERTIFICATE-----
+MIIDzzCCAregAwIBAgIDAWweMA0GCSqGSIb3DQEBBQUAMIGNMQswCQYDVQQGEwJBVDFIMEYGA1UE
+Cgw/QS1UcnVzdCBHZXMuIGYuIFNpY2hlcmhlaXRzc3lzdGVtZSBpbSBlbGVrdHIuIERhdGVudmVy
+a2VociBHbWJIMRkwFwYDVQQLDBBBLVRydXN0LW5RdWFsLTAzMRkwFwYDVQQDDBBBLVRydXN0LW5R
+dWFsLTAzMB4XDTA1MDgxNzIyMDAwMFoXDTE1MDgxNzIyMDAwMFowgY0xCzAJBgNVBAYTAkFUMUgw
+RgYDVQQKDD9BLVRydXN0IEdlcy4gZi4gU2ljaGVyaGVpdHNzeXN0ZW1lIGltIGVsZWt0ci4gRGF0
+ZW52ZXJrZWhyIEdtYkgxGTAXBgNVBAsMEEEtVHJ1c3QtblF1YWwtMDMxGTAXBgNVBAMMEEEtVHJ1
+c3QtblF1YWwtMDMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCtPWFuA/OQO8BBC4SA
+zewqo51ru27CQoT3URThoKgtUaNR8t4j8DRE/5TrzAUjlUC5B3ilJfYKvUWG6Nm9wASOhURh73+n
+yfrBJcyFLGM/BWBzSQXgYHiVEEvc+RFZznF/QJuKqiTfC0Li21a8StKlDJu3Qz7dg9MmEALP6iPE
+SU7l0+m0iKsMrmKS1GWH2WrX9IWf5DMiJaXlyDO6w8dB3F/GaswADm0yqLaHNgBid5seHzTLkDx4
+iHQF63n1k3Flyp3HaxgtPVxO59X4PzF9j4fsCiIvI+n+u33J4PTs63zEsMMtYrWacdaxaujs2e3V
+cuy+VwHOBVWf3tFgiBCzAgMBAAGjNjA0MA8GA1UdEwEB/wQFMAMBAf8wEQYDVR0OBAoECERqlWdV
+eRFPMA4GA1UdDwEB/wQEAwIBBjANBgkqhkiG9w0BAQUFAAOCAQEAVdRU0VlIXLOThaq/Yy/kgM40
+ozRiPvbY7meIMQQDbwvUB/tOdQ/TLtPAF8fGKOwGDREkDg6lXb+MshOWcdzUzg4NCmgybLlBMRmr
+sQd7TZjTXLDR8KdCoLXEjq/+8T/0709GAHbrAvv5ndJAlseIOrifEXnzgGWovR/TeIGgUUw3tKZd
+JXDRZslo+S4RFGjxVJgIrCaSD96JntT6s3kr0qN51OyLrIdTaEJMUVF0HhsnLuP1Hyl0Te2v9+GS
+mYHovjrHF1D2t8b8m7CKa9aIA5GPBnc6hQLdmNVDeD/GMBWsm2vLV7eJUYs66MmEDNuxUCAKGkq6
+ahq97BvIxYSazQ==
+-----END CERTIFICATE-----
+
+TWCA Root Certification Authority
+=================================
+-----BEGIN CERTIFICATE-----
+MIIDezCCAmOgAwIBAgIBATANBgkqhkiG9w0BAQUFADBfMQswCQYDVQQGEwJUVzESMBAGA1UECgwJ
+VEFJV0FOLUNBMRAwDgYDVQQLDAdSb290IENBMSowKAYDVQQDDCFUV0NBIFJvb3QgQ2VydGlmaWNh
+dGlvbiBBdXRob3JpdHkwHhcNMDgwODI4MDcyNDMzWhcNMzAxMjMxMTU1OTU5WjBfMQswCQYDVQQG
+EwJUVzESMBAGA1UECgwJVEFJV0FOLUNBMRAwDgYDVQQLDAdSb290IENBMSowKAYDVQQDDCFUV0NB
+IFJvb3QgQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEK
+AoIBAQCwfnK4pAOU5qfeCTiRShFAh6d8WWQUe7UREN3+v9XAu1bihSX0NXIP+FPQQeFEAcK0HMMx
+QhZHhTMidrIKbw/lJVBPhYa+v5guEGcevhEFhgWQxFnQfHgQsIBct+HHK3XLfJ+utdGdIzdjp9xC
+oi2SBBtQwXu4PhvJVgSLL1KbralW6cH/ralYhzC2gfeXRfwZVzsrb+RH9JlF/h3x+JejiB03HFyP
+4HYlmlD4oFT/RJB2I9IyxsOrBr/8+7/zrX2SYgJbKdM1o5OaQ2RgXbL6Mv87BK9NQGr5x+PvI/1r
+y+UPizgN7gr8/g+YnzAx3WxSZfmLgb4i4RxYA7qRG4kHAgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIB
+BjAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBRqOFsmjd6LWvJPelSDGRjjCDWmujANBgkqhkiG
+9w0BAQUFAAOCAQEAPNV3PdrfibqHDAhUaiBQkr6wQT25JmSDCi/oQMCXKCeCMErJk/9q56YAf4lC
+mtYR5VPOL8zy2gXE/uJQxDqGfczafhAJO5I1KlOy/usrBdlsXebQ79NqZp4VKIV66IIArB6nCWlW
+QtNoURi+VJq/REG6Sb4gumlc7rh3zc5sH62Dlhh9DrUUOYTxKOkto557HnpyWoOzeW/vtPzQCqVY
+T0bf+215WfKEIlKuD8z7fDvnaspHYcN6+NOSBB+4IIThNlQWx0DeO4pz3N/GCUzf7Nr/1FNCocny
+Yh0igzyXxfkZYiesZSLX0zzG5Y6yU8xJzrww/nsOM5D77dIUkR8Hrw==
+-----END CERTIFICATE-----
+
+Security Communication RootCA2
+==============================
+-----BEGIN CERTIFICATE-----
+MIIDdzCCAl+gAwIBAgIBADANBgkqhkiG9w0BAQsFADBdMQswCQYDVQQGEwJKUDElMCMGA1UEChMc
+U0VDT00gVHJ1c3QgU3lzdGVtcyBDTy4sTFRELjEnMCUGA1UECxMeU2VjdXJpdHkgQ29tbXVuaWNh
+dGlvbiBSb290Q0EyMB4XDTA5MDUyOTA1MDAzOVoXDTI5MDUyOTA1MDAzOVowXTELMAkGA1UEBhMC
+SlAxJTAjBgNVBAoTHFNFQ09NIFRydXN0IFN5c3RlbXMgQ08uLExURC4xJzAlBgNVBAsTHlNlY3Vy
+aXR5IENvbW11bmljYXRpb24gUm9vdENBMjCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEB
+ANAVOVKxUrO6xVmCxF1SrjpDZYBLx/KWvNs2l9amZIyoXvDjChz335c9S672XewhtUGrzbl+dp++
++T42NKA7wfYxEUV0kz1XgMX5iZnK5atq1LXaQZAQwdbWQonCv/Q4EpVMVAX3NuRFg3sUZdbcDE3R
+3n4MqzvEFb46VqZab3ZpUql6ucjrappdUtAtCms1FgkQhNBqyjoGADdH5H5XTz+L62e4iKrFvlNV
+spHEfbmwhRkGeC7bYRr6hfVKkaHnFtWOojnflLhwHyg/i/xAXmODPIMqGplrz95Zajv8bxbXH/1K
+EOtOghY6rCcMU/Gt1SSwawNQwS08Ft1ENCcadfsCAwEAAaNCMEAwHQYDVR0OBBYEFAqFqXdlBZh8
+QIH4D5csOPEK7DzPMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEB
+CwUAA4IBAQBMOqNErLlFsceTfsgLCkLfZOoc7llsCLqJX2rKSpWeeo8HxdpFcoJxDjrSzG+ntKEj
+u/Ykn8sX/oymzsLS28yN/HH8AynBbF0zX2S2ZTuJbxh2ePXcokgfGT+Ok+vx+hfuzU7jBBJV1uXk
+3fs+BXziHV7Gp7yXT2g69ekuCkO2r1dcYmh8t/2jioSgrGK+KwmHNPBqAbubKVY8/gA3zyNs8U6q
+tnRGEmyR7jTV7JqR50S+kDFy1UkC9gLl9B/rfNmWVan/7Ir5mUf/NVoCqgTLiluHcSmRvaS0eg29
+mvVXIwAHIRc/SjnRBUkLp7Y3gaVdjKozXoEofKd9J+sAro03
+-----END CERTIFICATE-----
+
+EC-ACC
+======
+-----BEGIN CERTIFICATE-----
+MIIFVjCCBD6gAwIBAgIQ7is969Qh3hSoYqwE893EATANBgkqhkiG9w0BAQUFADCB8zELMAkGA1UE
+BhMCRVMxOzA5BgNVBAoTMkFnZW5jaWEgQ2F0YWxhbmEgZGUgQ2VydGlmaWNhY2lvIChOSUYgUS0w
+ODAxMTc2LUkpMSgwJgYDVQQLEx9TZXJ2ZWlzIFB1YmxpY3MgZGUgQ2VydGlmaWNhY2lvMTUwMwYD
+VQQLEyxWZWdldSBodHRwczovL3d3dy5jYXRjZXJ0Lm5ldC92ZXJhcnJlbCAoYykwMzE1MDMGA1UE
+CxMsSmVyYXJxdWlhIEVudGl0YXRzIGRlIENlcnRpZmljYWNpbyBDYXRhbGFuZXMxDzANBgNVBAMT
+BkVDLUFDQzAeFw0wMzAxMDcyMzAwMDBaFw0zMTAxMDcyMjU5NTlaMIHzMQswCQYDVQQGEwJFUzE7
+MDkGA1UEChMyQWdlbmNpYSBDYXRhbGFuYSBkZSBDZXJ0aWZpY2FjaW8gKE5JRiBRLTA4MDExNzYt
+SSkxKDAmBgNVBAsTH1NlcnZlaXMgUHVibGljcyBkZSBDZXJ0aWZpY2FjaW8xNTAzBgNVBAsTLFZl
+Z2V1IGh0dHBzOi8vd3d3LmNhdGNlcnQubmV0L3ZlcmFycmVsIChjKTAzMTUwMwYDVQQLEyxKZXJh
+cnF1aWEgRW50aXRhdHMgZGUgQ2VydGlmaWNhY2lvIENhdGFsYW5lczEPMA0GA1UEAxMGRUMtQUND
+MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAsyLHT+KXQpWIR4NA9h0X84NzJB5R85iK
+w5K4/0CQBXCHYMkAqbWUZRkiFRfCQ2xmRJoNBD45b6VLeqpjt4pEndljkYRm4CgPukLjbo73FCeT
+ae6RDqNfDrHrZqJyTxIThmV6PttPB/SnCWDaOkKZx7J/sxaVHMf5NLWUhdWZXqBIoH7nF2W4onW4
+HvPlQn2v7fOKSGRdghST2MDk/7NQcvJ29rNdQlB50JQ+awwAvthrDk4q7D7SzIKiGGUzE3eeml0a
+E9jD2z3Il3rucO2n5nzbcc8tlGLfbdb1OL4/pYUKGbio2Al1QnDE6u/LDsg0qBIimAy4E5S2S+zw
+0JDnJwIDAQABo4HjMIHgMB0GA1UdEQQWMBSBEmVjX2FjY0BjYXRjZXJ0Lm5ldDAPBgNVHRMBAf8E
+BTADAQH/MA4GA1UdDwEB/wQEAwIBBjAdBgNVHQ4EFgQUoMOLRKo3pUW/l4Ba0fF4opvpXY0wfwYD
+VR0gBHgwdjB0BgsrBgEEAfV4AQMBCjBlMCwGCCsGAQUFBwIBFiBodHRwczovL3d3dy5jYXRjZXJ0
+Lm5ldC92ZXJhcnJlbDA1BggrBgEFBQcCAjApGidWZWdldSBodHRwczovL3d3dy5jYXRjZXJ0Lm5l
+dC92ZXJhcnJlbCAwDQYJKoZIhvcNAQEFBQADggEBAKBIW4IB9k1IuDlVNZyAelOZ1Vr/sXE7zDkJ
+lF7W2u++AVtd0x7Y/X1PzaBB4DSTv8vihpw3kpBWHNzrKQXlxJ7HNd+KDM3FIUPpqojlNcAZQmNa
+Al6kSBg6hW/cnbw/nZzBh7h6YQjpdwt/cKt63dmXLGQehb+8dJahw3oS7AwaboMMPOhyRp/7SNVe
+l+axofjk70YllJyJ22k4vuxcDlbHZVHlUIiIv0LVKz3l+bqeLrPK9HOSAgu+TGbrIP65y7WZf+a2
+E/rKS03Z7lNGBjvGTq2TWoF+bCpLagVFjPIhpDGQh2xlnJ2lYJU6Un/10asIbvPuW/mIPX64b24D
+5EI=
+-----END CERTIFICATE-----
+
+Hellenic Academic and Research Institutions RootCA 2011
+=======================================================
+-----BEGIN CERTIFICATE-----
+MIIEMTCCAxmgAwIBAgIBADANBgkqhkiG9w0BAQUFADCBlTELMAkGA1UEBhMCR1IxRDBCBgNVBAoT
+O0hlbGxlbmljIEFjYWRlbWljIGFuZCBSZXNlYXJjaCBJbnN0aXR1dGlvbnMgQ2VydC4gQXV0aG9y
+aXR5MUAwPgYDVQQDEzdIZWxsZW5pYyBBY2FkZW1pYyBhbmQgUmVzZWFyY2ggSW5zdGl0dXRpb25z
+IFJvb3RDQSAyMDExMB4XDTExMTIwNjEzNDk1MloXDTMxMTIwMTEzNDk1MlowgZUxCzAJBgNVBAYT
+AkdSMUQwQgYDVQQKEztIZWxsZW5pYyBBY2FkZW1pYyBhbmQgUmVzZWFyY2ggSW5zdGl0dXRpb25z
+IENlcnQuIEF1dGhvcml0eTFAMD4GA1UEAxM3SGVsbGVuaWMgQWNhZGVtaWMgYW5kIFJlc2VhcmNo
+IEluc3RpdHV0aW9ucyBSb290Q0EgMjAxMTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEB
+AKlTAOMupvaO+mDYLZU++CwqVE7NuYRhlFhPjz2L5EPzdYmNUeTDN9KKiE15HrcS3UN4SoqS5tdI
+1Q+kOilENbgH9mgdVc04UfCMJDGFr4PJfel3r+0ae50X+bOdOFAPplp5kYCvN66m0zH7tSYJnTxa
+71HFK9+WXesyHgLacEnsbgzImjeN9/E2YEsmLIKe0HjzDQ9jpFEw4fkrJxIH2Oq9GGKYsFk3fb7u
+8yBRQlqD75O6aRXxYp2fmTmCobd0LovUxQt7L/DICto9eQqakxylKHJzkUOap9FNhYS5qXSPFEDH
+3N6sQWRstBmbAmNtJGSPRLIl6s5ddAxjMlyNh+UCAwEAAaOBiTCBhjAPBgNVHRMBAf8EBTADAQH/
+MAsGA1UdDwQEAwIBBjAdBgNVHQ4EFgQUppFC/RNhSiOeCKQp5dgTBCPuQSUwRwYDVR0eBEAwPqA8
+MAWCAy5ncjAFggMuZXUwBoIELmVkdTAGggQub3JnMAWBAy5ncjAFgQMuZXUwBoEELmVkdTAGgQQu
+b3JnMA0GCSqGSIb3DQEBBQUAA4IBAQAf73lB4XtuP7KMhjdCSk4cNx6NZrokgclPEg8hwAOXhiVt
+XdMiKahsog2p6z0GW5k6x8zDmjR/qw7IThzh+uTczQ2+vyT+bOdrwg3IBp5OjWEopmr95fZi6hg8
+TqBTnbI6nOulnJEWtk2C4AwFSKls9cz4y51JtPACpf1wA+2KIaWuE4ZJwzNzvoc7dIsXRSZMFpGD
+/md9zU1jZ/rzAxKWeAaNsWftjj++n08C9bMJL/NMh98qy5V8AcysNnq/onN694/BtZqhFLKPM58N
+7yLcZnuEvUUXBj08yrl3NI/K6s8/MT7jiOOASSXIl7WdmplNsDz4SgCbZN2fOUvRJ9e4
+-----END CERTIFICATE-----
+
+Actalis Authentication Root CA
+==============================
+-----BEGIN CERTIFICATE-----
+MIIFuzCCA6OgAwIBAgIIVwoRl0LE48wwDQYJKoZIhvcNAQELBQAwazELMAkGA1UEBhMCSVQxDjAM
+BgNVBAcMBU1pbGFuMSMwIQYDVQQKDBpBY3RhbGlzIFMucC5BLi8wMzM1ODUyMDk2NzEnMCUGA1UE
+AwweQWN0YWxpcyBBdXRoZW50aWNhdGlvbiBSb290IENBMB4XDTExMDkyMjExMjIwMloXDTMwMDky
+MjExMjIwMlowazELMAkGA1UEBhMCSVQxDjAMBgNVBAcMBU1pbGFuMSMwIQYDVQQKDBpBY3RhbGlz
+IFMucC5BLi8wMzM1ODUyMDk2NzEnMCUGA1UEAwweQWN0YWxpcyBBdXRoZW50aWNhdGlvbiBSb290
+IENBMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAp8bEpSmkLO/lGMWwUKNvUTufClrJ
+wkg4CsIcoBh/kbWHuUA/3R1oHwiD1S0eiKD4j1aPbZkCkpAW1V8IbInX4ay8IMKx4INRimlNAJZa
+by/ARH6jDuSRzVju3PvHHkVH3Se5CAGfpiEd9UEtL0z9KK3giq0itFZljoZUj5NDKd45RnijMCO6
+zfB9E1fAXdKDa0hMxKufgFpbOr3JpyI/gCczWw63igxdBzcIy2zSekciRDXFzMwujt0q7bd9Zg1f
+YVEiVRvjRuPjPdA1YprbrxTIW6HMiRvhMCb8oJsfgadHHwTrozmSBp+Z07/T6k9QnBn+locePGX2
+oxgkg4YQ51Q+qDp2JE+BIcXjDwL4k5RHILv+1A7TaLndxHqEguNTVHnd25zS8gebLra8Pu2Fbe8l
+EfKXGkJh90qX6IuxEAf6ZYGyojnP9zz/GPvG8VqLWeICrHuS0E4UT1lF9gxeKF+w6D9Fz8+vm2/7
+hNN3WpVvrJSEnu68wEqPSpP4RCHiMUVhUE4Q2OM1fEwZtN4Fv6MGn8i1zeQf1xcGDXqVdFUNaBr8
+EBtiZJ1t4JWgw5QHVw0U5r0F+7if5t+L4sbnfpb2U8WANFAoWPASUHEXMLrmeGO89LKtmyuy/uE5
+jF66CyCU3nuDuP/jVo23Eek7jPKxwV2dpAtMK9myGPW1n0sCAwEAAaNjMGEwHQYDVR0OBBYEFFLY
+iDrIn3hm7YnzezhwlMkCAjbQMA8GA1UdEwEB/wQFMAMBAf8wHwYDVR0jBBgwFoAUUtiIOsifeGbt
+ifN7OHCUyQICNtAwDgYDVR0PAQH/BAQDAgEGMA0GCSqGSIb3DQEBCwUAA4ICAQALe3KHwGCmSUyI
+WOYdiPcUZEim2FgKDk8TNd81HdTtBjHIgT5q1d07GjLukD0R0i70jsNjLiNmsGe+b7bAEzlgqqI0
+JZN1Ut6nna0Oh4lScWoWPBkdg/iaKWW+9D+a2fDzWochcYBNy+A4mz+7+uAwTc+G02UQGRjRlwKx
+K3JCaKygvU5a2hi/a5iB0P2avl4VSM0RFbnAKVy06Ij3Pjaut2L9HmLecHgQHEhb2rykOLpn7VU+
+Xlff1ANATIGk0k9jpwlCCRT8AKnCgHNPLsBA2RF7SOp6AsDT6ygBJlh0wcBzIm2Tlf05fbsq4/aC
+4yyXX04fkZT6/iyj2HYauE2yOE+b+h1IYHkm4vP9qdCa6HCPSXrW5b0KDtst842/6+OkfcvHlXHo
+2qN8xcL4dJIEG4aspCJTQLas/kx2z/uUMsA1n3Y/buWQbqCmJqK4LL7RK4X9p2jIugErsWx0Hbhz
+lefut8cl8ABMALJ+tguLHPPAUJ4lueAI3jZm/zel0btUZCzJJ7VLkn5l/9Mt4blOvH+kQSGQQXem
+OR/qnuOf0GZvBeyqdn6/axag67XH/JJULysRJyU3eExRarDzzFhdFPFqSBX/wge2sY0PjlxQRrM9
+vwGYT7JZVEc+NHt4bVaTLnPqZih4zR0Uv6CPLy64Lo7yFIrM6bV8+2ydDKXhlg==
+-----END CERTIFICATE-----
+
+Trustis FPS Root CA
+===================
+-----BEGIN CERTIFICATE-----
+MIIDZzCCAk+gAwIBAgIQGx+ttiD5JNM2a/fH8YygWTANBgkqhkiG9w0BAQUFADBFMQswCQYDVQQG
+EwJHQjEYMBYGA1UEChMPVHJ1c3RpcyBMaW1pdGVkMRwwGgYDVQQLExNUcnVzdGlzIEZQUyBSb290
+IENBMB4XDTAzMTIyMzEyMTQwNloXDTI0MDEyMTExMzY1NFowRTELMAkGA1UEBhMCR0IxGDAWBgNV
+BAoTD1RydXN0aXMgTGltaXRlZDEcMBoGA1UECxMTVHJ1c3RpcyBGUFMgUm9vdCBDQTCCASIwDQYJ
+KoZIhvcNAQEBBQADggEPADCCAQoCggEBAMVQe547NdDfxIzNjpvto8A2mfRC6qc+gIMPpqdZh8mQ
+RUN+AOqGeSoDvT03mYlmt+WKVoaTnGhLaASMk5MCPjDSNzoiYYkchU59j9WvezX2fihHiTHcDnlk
+H5nSW7r+f2C/revnPDgpai/lkQtV/+xvWNUtyd5MZnGPDNcE2gfmHhjjvSkCqPoc4Vu5g6hBSLwa
+cY3nYuUtsuvffM/bq1rKMfFMIvMFE/eC+XN5DL7XSxzA0RU8k0Fk0ea+IxciAIleH2ulrG6nS4zt
+o3Lmr2NNL4XSFDWaLk6M6jKYKIahkQlBOrTh4/L68MkKokHdqeMDx4gVOxzUGpTXn2RZEm0CAwEA
+AaNTMFEwDwYDVR0TAQH/BAUwAwEB/zAfBgNVHSMEGDAWgBS6+nEleYtXQSUhhgtx67JkDoshZzAd
+BgNVHQ4EFgQUuvpxJXmLV0ElIYYLceuyZA6LIWcwDQYJKoZIhvcNAQEFBQADggEBAH5Y//01GX2c
+GE+esCu8jowU/yyg2kdbw++BLa8F6nRIW/M+TgfHbcWzk88iNVy2P3UnXwmWzaD+vkAMXBJV+JOC
+yinpXj9WV4s4NvdFGkwozZ5BuO1WTISkQMi4sKUraXAEasP41BIy+Q7DsdwyhEQsb8tGD+pmQQ9P
+8Vilpg0ND2HepZ5dfWWhPBfnqFVO76DH7cZEf1T1o+CP8HxVIo8ptoGj4W1OLBuAZ+ytIJ8MYmHV
+l/9D7S3B2l0pKoU/rGXuhg8FjZBf3+6f9L/uHfuY5H+QK4R4EA5sSVPvFVtlRkpdr7r7OnIdzfYl
+iB6XzCGcKQENZetX2fNXlrtIzYE=
+-----END CERTIFICATE-----
+
+StartCom Certification Authority
+================================
+-----BEGIN CERTIFICATE-----
+MIIHhzCCBW+gAwIBAgIBLTANBgkqhkiG9w0BAQsFADB9MQswCQYDVQQGEwJJTDEWMBQGA1UEChMN
+U3RhcnRDb20gTHRkLjErMCkGA1UECxMiU2VjdXJlIERpZ2l0YWwgQ2VydGlmaWNhdGUgU2lnbmlu
+ZzEpMCcGA1UEAxMgU3RhcnRDb20gQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkwHhcNMDYwOTE3MTk0
+NjM3WhcNMzYwOTE3MTk0NjM2WjB9MQswCQYDVQQGEwJJTDEWMBQGA1UEChMNU3RhcnRDb20gTHRk
+LjErMCkGA1UECxMiU2VjdXJlIERpZ2l0YWwgQ2VydGlmaWNhdGUgU2lnbmluZzEpMCcGA1UEAxMg
+U3RhcnRDb20gQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkwggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAw
+ggIKAoICAQDBiNsJvGxGfHiflXu1M5DycmLWwTYgIiRezul38kMKogZkpMyONvg45iPwbm2xPN1y
+o4UcodM9tDMr0y+v/uqwQVlntsQGfQqedIXWeUyAN3rfOQVSWff0G0ZDpNKFhdLDcfN1YjS6LIp/
+Ho/u7TTQEceWzVI9ujPW3U3eCztKS5/CJi/6tRYccjV3yjxd5srhJosaNnZcAdt0FCX+7bWgiA/d
+eMotHweXMAEtcnn6RtYTKqi5pquDSR3l8u/d5AGOGAqPY1MWhWKpDhk6zLVmpsJrdAfkK+F2PrRt
+2PZE4XNiHzvEvqBTViVsUQn3qqvKv3b9bZvzndu/PWa8DFaqr5hIlTpL36dYUNk4dalb6kMMAv+Z
+6+hsTXBbKWWc3apdzK8BMewM69KN6Oqce+Zu9ydmDBpI125C4z/eIT574Q1w+2OqqGwaVLRcJXrJ
+osmLFqa7LH4XXgVNWG4SHQHuEhANxjJ/GP/89PrNbpHoNkm+Gkhpi8KWTRoSsmkXwQqQ1vp5Iki/
+untp+HDH+no32NgN0nZPV/+Qt+OR0t3vwmC3Zzrd/qqc8NSLf3Iizsafl7b4r4qgEKjZ+xjGtrVc
+UjyJthkqcwEKDwOzEmDyei+B26Nu/yYwl/WL3YlXtq09s68rxbd2AvCl1iuahhQqcvbjM4xdCUsT
+37uMdBNSSwIDAQABo4ICEDCCAgwwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwHQYD
+VR0OBBYEFE4L7xqkQFulF2mHMMo0aEPQQa7yMB8GA1UdIwQYMBaAFE4L7xqkQFulF2mHMMo0aEPQ
+Qa7yMIIBWgYDVR0gBIIBUTCCAU0wggFJBgsrBgEEAYG1NwEBATCCATgwLgYIKwYBBQUHAgEWImh0
+dHA6Ly93d3cuc3RhcnRzc2wuY29tL3BvbGljeS5wZGYwNAYIKwYBBQUHAgEWKGh0dHA6Ly93d3cu
+c3RhcnRzc2wuY29tL2ludGVybWVkaWF0ZS5wZGYwgc8GCCsGAQUFBwICMIHCMCcWIFN0YXJ0IENv
+bW1lcmNpYWwgKFN0YXJ0Q29tKSBMdGQuMAMCAQEagZZMaW1pdGVkIExpYWJpbGl0eSwgcmVhZCB0
+aGUgc2VjdGlvbiAqTGVnYWwgTGltaXRhdGlvbnMqIG9mIHRoZSBTdGFydENvbSBDZXJ0aWZpY2F0
+aW9uIEF1dGhvcml0eSBQb2xpY3kgYXZhaWxhYmxlIGF0IGh0dHA6Ly93d3cuc3RhcnRzc2wuY29t
+L3BvbGljeS5wZGYwEQYJYIZIAYb4QgEBBAQDAgAHMDgGCWCGSAGG+EIBDQQrFilTdGFydENvbSBG
+cmVlIFNTTCBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eTANBgkqhkiG9w0BAQsFAAOCAgEAjo/n3JR5
+fPGFf59Jb2vKXfuM/gTFwWLRfUKKvFO3lANmMD+x5wqnUCBVJX92ehQN6wQOQOY+2IirByeDqXWm
+N3PH/UvSTa0XQMhGvjt/UfzDtgUx3M2FIk5xt/JxXrAaxrqTi3iSSoX4eA+D/i+tLPfkpLst0OcN
+Org+zvZ49q5HJMqjNTbOx8aHmNrs++myziebiMMEofYLWWivydsQD032ZGNcpRJvkrKTlMeIFw6T
+tn5ii5B/q06f/ON1FE8qMt9bDeD1e5MNq6HPh+GlBEXoPBKlCcWw0bdT82AUuoVpaiF8H3VhFyAX
+e2w7QSlc4axa0c2Mm+tgHRns9+Ww2vl5GKVFP0lDV9LdJNUso/2RjSe15esUBppMeyG7Oq0wBhjA
+2MFrLH9ZXF2RsXAiV+uKa0hK1Q8p7MZAwC+ITGgBF3f0JBlPvfrhsiAhS90a2Cl9qrjeVOwhVYBs
+HvUwyKMQ5bLmKhQxw4UtjJixhlpPiVktucf3HMiKf8CdBUrmQk9io20ppB+Fq9vlgcitKj1MXVuE
+JnHEhV5xJMqlG2zYYdMa4FTbzrqpMrUi9nNBCV24F10OD5mQ1kfabwo6YigUZ4LZ8dCAWZvLMdib
+D4x3TrVoivJs9iQOLWxwxXPR3hTQcY+203sC9uO41Alua551hDnmfyWl8kgAwKQB2j8=
+-----END CERTIFICATE-----
+
+StartCom Certification Authority G2
+===================================
+-----BEGIN CERTIFICATE-----
+MIIFYzCCA0ugAwIBAgIBOzANBgkqhkiG9w0BAQsFADBTMQswCQYDVQQGEwJJTDEWMBQGA1UEChMN
+U3RhcnRDb20gTHRkLjEsMCoGA1UEAxMjU3RhcnRDb20gQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkg
+RzIwHhcNMTAwMTAxMDEwMDAxWhcNMzkxMjMxMjM1OTAxWjBTMQswCQYDVQQGEwJJTDEWMBQGA1UE
+ChMNU3RhcnRDb20gTHRkLjEsMCoGA1UEAxMjU3RhcnRDb20gQ2VydGlmaWNhdGlvbiBBdXRob3Jp
+dHkgRzIwggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQC2iTZbB7cgNr2Cu+EWIAOVeq8O
+o1XJJZlKxdBWQYeQTSFgpBSHO839sj60ZwNq7eEPS8CRhXBF4EKe3ikj1AENoBB5uNsDvfOpL9HG
+4A/LnooUCri99lZi8cVytjIl2bLzvWXFDSxu1ZJvGIsAQRSCb0AgJnooD/Uefyf3lLE3PbfHkffi
+Aez9lInhzG7TNtYKGXmu1zSCZf98Qru23QumNK9LYP5/Q0kGi4xDuFby2X8hQxfqp0iVAXV16iul
+Q5XqFYSdCI0mblWbq9zSOdIxHWDirMxWRST1HFSr7obdljKF+ExP6JV2tgXdNiNnvP8V4so75qbs
+O+wmETRIjfaAKxojAuuKHDp2KntWFhxyKrOq42ClAJ8Em+JvHhRYW6Vsi1g8w7pOOlz34ZYrPu8H
+vKTlXcxNnw3h3Kq74W4a7I/htkxNeXJdFzULHdfBR9qWJODQcqhaX2YtENwvKhOuJv4KHBnM0D4L
+nMgJLvlblnpHnOl68wVQdJVznjAJ85eCXuaPOQgeWeU1FEIT/wCc976qUM/iUUjXuG+v+E5+M5iS
+FGI6dWPPe/regjupuznixL0sAA7IF6wT700ljtizkC+p2il9Ha90OrInwMEePnWjFqmveiJdnxMa
+z6eg6+OGCtP95paV1yPIN93EfKo2rJgaErHgTuixO/XWb/Ew1wIDAQABo0IwQDAPBgNVHRMBAf8E
+BTADAQH/MA4GA1UdDwEB/wQEAwIBBjAdBgNVHQ4EFgQUS8W0QGutHLOlHGVuRjaJhwUMDrYwDQYJ
+KoZIhvcNAQELBQADggIBAHNXPyzVlTJ+N9uWkusZXn5T50HsEbZH77Xe7XRcxfGOSeD8bpkTzZ+K
+2s06Ctg6Wgk/XzTQLwPSZh0avZyQN8gMjgdalEVGKua+etqhqaRpEpKwfTbURIfXUfEpY9Z1zRbk
+J4kd+MIySP3bmdCPX1R0zKxnNBFi2QwKN4fRoxdIjtIXHfbX/dtl6/2o1PXWT6RbdejF0mCy2wl+
+JYt7ulKSnj7oxXehPOBKc2thz4bcQ///If4jXSRK9dNtD2IEBVeC2m6kMyV5Sy5UGYvMLD0w6dEG
+/+gyRr61M3Z3qAFdlsHB1b6uJcDJHgoJIIihDsnzb02CVAAgp9KP5DlUFy6NHrgbuxu9mk47EDTc
+nIhT76IxW1hPkWLIwpqazRVdOKnWvvgTtZ8SafJQYqz7Fzf07rh1Z2AQ+4NQ+US1dZxAF7L+/Xld
+blhYXzD8AK6vM8EOTmy6p6ahfzLbOOCxchcKK5HsamMm7YnUeMx0HgX4a/6ManY5Ka5lIxKVCCIc
+l85bBu4M4ru8H0ST9tg4RQUh7eStqxK2A6RCLi3ECToDZ2mEmuFZkIoohdVddLHRDiBYmxOlsGOm
+7XtH/UVVMKTumtTm4ofvmMkyghEpIrwACjFeLQ/Ajulrso8uBtjRkcfGEvRM/TAXw8HaOFvjqerm
+obp573PYtlNXLfbQ4ddI
+-----END CERTIFICATE-----
+
+Buypass Class 2 Root CA
+=======================
+-----BEGIN CERTIFICATE-----
+MIIFWTCCA0GgAwIBAgIBAjANBgkqhkiG9w0BAQsFADBOMQswCQYDVQQGEwJOTzEdMBsGA1UECgwU
+QnV5cGFzcyBBUy05ODMxNjMzMjcxIDAeBgNVBAMMF0J1eXBhc3MgQ2xhc3MgMiBSb290IENBMB4X
+DTEwMTAyNjA4MzgwM1oXDTQwMTAyNjA4MzgwM1owTjELMAkGA1UEBhMCTk8xHTAbBgNVBAoMFEJ1
+eXBhc3MgQVMtOTgzMTYzMzI3MSAwHgYDVQQDDBdCdXlwYXNzIENsYXNzIDIgUm9vdCBDQTCCAiIw
+DQYJKoZIhvcNAQEBBQADggIPADCCAgoCggIBANfHXvfBB9R3+0Mh9PT1aeTuMgHbo4Yf5FkNuud1
+g1Lr6hxhFUi7HQfKjK6w3Jad6sNgkoaCKHOcVgb/S2TwDCo3SbXlzwx87vFKu3MwZfPVL4O2fuPn
+9Z6rYPnT8Z2SdIrkHJasW4DptfQxh6NR/Md+oW+OU3fUl8FVM5I+GC911K2GScuVr1QGbNgGE41b
+/+EmGVnAJLqBcXmQRFBoJJRfuLMR8SlBYaNByyM21cHxMlAQTn/0hpPshNOOvEu/XAFOBz3cFIqU
+CqTqc/sLUegTBxj6DvEr0VQVfTzh97QZQmdiXnfgolXsttlpF9U6r0TtSsWe5HonfOV116rLJeff
+awrbD02TTqigzXsu8lkBarcNuAeBfos4GzjmCleZPe4h6KP1DBbdi+w0jpwqHAAVF41og9JwnxgI
+zRFo1clrUs3ERo/ctfPYV3Me6ZQ5BL/T3jjetFPsaRyifsSP5BtwrfKi+fv3FmRmaZ9JUaLiFRhn
+Bkp/1Wy1TbMz4GHrXb7pmA8y1x1LPC5aAVKRCfLf6o3YBkBjqhHk/sM3nhRSP/TizPJhk9H9Z2vX
+Uq6/aKtAQ6BXNVN48FP4YUIHZMbXb5tMOA1jrGKvNouicwoN9SG9dKpN6nIDSdvHXx1iY8f93ZHs
+M+71bbRuMGjeyNYmsHVee7QHIJihdjK4TWxPAgMBAAGjQjBAMA8GA1UdEwEB/wQFMAMBAf8wHQYD
+VR0OBBYEFMmAd+BikoL1RpzzuvdMw964o605MA4GA1UdDwEB/wQEAwIBBjANBgkqhkiG9w0BAQsF
+AAOCAgEAU18h9bqwOlI5LJKwbADJ784g7wbylp7ppHR/ehb8t/W2+xUbP6umwHJdELFx7rxP462s
+A20ucS6vxOOto70MEae0/0qyexAQH6dXQbLArvQsWdZHEIjzIVEpMMpghq9Gqx3tOluwlN5E40EI
+osHsHdb9T7bWR9AUC8rmyrV7d35BH16Dx7aMOZawP5aBQW9gkOLo+fsicdl9sz1Gv7SEr5AcD48S
+aq/v7h56rgJKihcrdv6sVIkkLE8/trKnToyokZf7KcZ7XC25y2a2t6hbElGFtQl+Ynhw/qlqYLYd
+DnkM/crqJIByw5c/8nerQyIKx+u2DISCLIBrQYoIwOula9+ZEsuK1V6ADJHgJgg2SMX6OBE1/yWD
+LfJ6v9r9jv6ly0UsH8SIU653DtmadsWOLB2jutXsMq7Aqqz30XpN69QH4kj3Io6wpJ9qzo6ysmD0
+oyLQI+uUWnpp3Q+/QFesa1lQ2aOZ4W7+jQF5JyMV3pKdewlNWudLSDBaGOYKbeaP4NK75t98biGC
+wWg5TbSYWGZizEqQXsP6JwSxeRV0mcy+rSDeJmAc61ZRpqPq5KM/p/9h3PFaTWwyI0PurKju7koS
+CTxdccK+efrCh2gdC/1cacwG0Jp9VJkqyTkaGa9LKkPzY11aWOIv4x3kqdbQCtCev9eBCfHJxyYN
+rJgWVqA=
+-----END CERTIFICATE-----
+
+Buypass Class 3 Root CA
+=======================
+-----BEGIN CERTIFICATE-----
+MIIFWTCCA0GgAwIBAgIBAjANBgkqhkiG9w0BAQsFADBOMQswCQYDVQQGEwJOTzEdMBsGA1UECgwU
+QnV5cGFzcyBBUy05ODMxNjMzMjcxIDAeBgNVBAMMF0J1eXBhc3MgQ2xhc3MgMyBSb290IENBMB4X
+DTEwMTAyNjA4Mjg1OFoXDTQwMTAyNjA4Mjg1OFowTjELMAkGA1UEBhMCTk8xHTAbBgNVBAoMFEJ1
+eXBhc3MgQVMtOTgzMTYzMzI3MSAwHgYDVQQDDBdCdXlwYXNzIENsYXNzIDMgUm9vdCBDQTCCAiIw
+DQYJKoZIhvcNAQEBBQADggIPADCCAgoCggIBAKXaCpUWUOOV8l6ddjEGMnqb8RB2uACatVI2zSRH
+sJ8YZLya9vrVediQYkwiL944PdbgqOkcLNt4EemOaFEVcsfzM4fkoF0LXOBXByow9c3EN3coTRiR
+5r/VUv1xLXA+58bEiuPwKAv0dpihi4dVsjoT/Lc+JzeOIuOoTyrvYLs9tznDDgFHmV0ST9tD+leh
+7fmdvhFHJlsTmKtdFoqwNxxXnUX/iJY2v7vKB3tvh2PX0DJq1l1sDPGzbjniazEuOQAnFN44wOwZ
+ZoYS6J1yFhNkUsepNxz9gjDthBgd9K5c/3ATAOux9TN6S9ZV+AWNS2mw9bMoNlwUxFFzTWsL8TQH
+2xc519woe2v1n/MuwU8XKhDzzMro6/1rqy6any2CbgTUUgGTLT2G/H783+9CHaZr77kgxve9oKeV
+/afmiSTYzIw0bOIjL9kSGiG5VZFvC5F5GQytQIgLcOJ60g7YaEi7ghM5EFjp2CoHxhLbWNvSO1UQ
+RwUVZ2J+GGOmRj8JDlQyXr8NYnon74Do29lLBlo3WiXQCBJ31G8JUJc9yB3D34xFMFbG02SrZvPA
+Xpacw8Tvw3xrizp5f7NJzz3iiZ+gMEuFuZyUJHmPfWupRWgPK9Dx2hzLabjKSWJtyNBjYt1gD1iq
+j6G8BaVmos8bdrKEZLFMOVLAMLrwjEsCsLa3AgMBAAGjQjBAMA8GA1UdEwEB/wQFMAMBAf8wHQYD
+VR0OBBYEFEe4zf/lb+74suwvTg75JbCOPGvDMA4GA1UdDwEB/wQEAwIBBjANBgkqhkiG9w0BAQsF
+AAOCAgEAACAjQTUEkMJAYmDv4jVM1z+s4jSQuKFvdvoWFqRINyzpkMLyPPgKn9iB5btb2iUspKdV
+cSQy9sgL8rxq+JOssgfCX5/bzMiKqr5qb+FJEMwx14C7u8jYog5kV+qi9cKpMRXSIGrs/CIBKM+G
+uIAeqcwRpTzyFrNHnfzSgCHEy9BHcEGhyoMZCCxt8l13nIoUE9Q2HJLw5QY33KbmkJs4j1xrG0aG
+Q0JfPgEHU1RdZX33inOhmlRaHylDFCfChQ+1iHsaO5S3HWCntZznKWlXWpuTekMwGwPXYshApqr8
+ZORK15FTAaggiG6cX0S5y2CBNOxv033aSF/rtJC8LakcC6wc1aJoIIAE1vyxjy+7SjENSoYc6+I2
+KSb12tjE8nVhz36udmNKekBlk4f4HoCMhuWG1o8O/FMsYOgWYRqiPkN7zTlgVGr18okmAWiDSKIz
+6MkEkbIRNBE+6tBDGR8Dk5AM/1E9V/RBbuHLoL7ryWPNbczk+DaqaJ3tvV2XcEQNtg413OEMXbug
+UZTLfhbrES+jkkXITHHZvMmZUldGL1DPvTVp9D0VzgalLA8+9oG6lLvDu79leNKGef9JOxqDDPDe
+eOzI8k1MGt6CKfjBWtrt7uYnXuhF0J0cUahoq0Tj0Itq4/g7u9xN12TyUb7mqqta6THuBrxzvxNi
+Cp/HuZc=
+-----END CERTIFICATE-----
+
+T-TeleSec GlobalRoot Class 3
+============================
+-----BEGIN CERTIFICATE-----
+MIIDwzCCAqugAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgjELMAkGA1UEBhMCREUxKzApBgNVBAoM
+IlQtU3lzdGVtcyBFbnRlcnByaXNlIFNlcnZpY2VzIEdtYkgxHzAdBgNVBAsMFlQtU3lzdGVtcyBU
+cnVzdCBDZW50ZXIxJTAjBgNVBAMMHFQtVGVsZVNlYyBHbG9iYWxSb290IENsYXNzIDMwHhcNMDgx
+MDAxMTAyOTU2WhcNMzMxMDAxMjM1OTU5WjCBgjELMAkGA1UEBhMCREUxKzApBgNVBAoMIlQtU3lz
+dGVtcyBFbnRlcnByaXNlIFNlcnZpY2VzIEdtYkgxHzAdBgNVBAsMFlQtU3lzdGVtcyBUcnVzdCBD
+ZW50ZXIxJTAjBgNVBAMMHFQtVGVsZVNlYyBHbG9iYWxSb290IENsYXNzIDMwggEiMA0GCSqGSIb3
+DQEBAQUAA4IBDwAwggEKAoIBAQC9dZPwYiJvJK7genasfb3ZJNW4t/zN8ELg63iIVl6bmlQdTQyK
+9tPPcPRStdiTBONGhnFBSivwKixVA9ZIw+A5OO3yXDw/RLyTPWGrTs0NvvAgJ1gORH8EGoel15YU
+NpDQSXuhdfsaa3Ox+M6pCSzyU9XDFES4hqX2iys52qMzVNn6chr3IhUciJFrf2blw2qAsCTz34ZF
+iP0Zf3WHHx+xGwpzJFu5ZeAsVMhg02YXP+HMVDNzkQI6pn97djmiH5a2OK61yJN0HZ65tOVgnS9W
+0eDrXltMEnAMbEQgqxHY9Bn20pxSN+f6tsIxO0rUFJmtxxr1XV/6B7h8DR/Wgx6zAgMBAAGjQjBA
+MA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/BAQDAgEGMB0GA1UdDgQWBBS1A/d2O2GCahKqGFPr
+AyGUv/7OyjANBgkqhkiG9w0BAQsFAAOCAQEAVj3vlNW92nOyWL6ukK2YJ5f+AbGwUgC4TeQbIXQb
+fsDuXmkqJa9c1h3a0nnJ85cp4IaH3gRZD/FZ1GSFS5mvJQQeyUapl96Cshtwn5z2r3Ex3XsFpSzT
+ucpH9sry9uetuUg/vBa3wW306gmv7PO15wWeph6KU1HWk4HMdJP2udqmJQV0eVp+QD6CSyYRMG7h
+P0HHRwA11fXT91Q+gT3aSWqas+8QPebrb9HIIkfLzM8BMZLZGOMivgkeGj5asuRrDFR6fUNOuIml
+e9eiPZaGzPImNC1qkp2aGtAw4l1OBLBfiyB+d8E9lYLRRpo7PHi4b6HQDWSieB4pTpPDpFQUWw==
+-----END CERTIFICATE-----
+
+EE Certification Centre Root CA
+===============================
+-----BEGIN CERTIFICATE-----
+MIIEAzCCAuugAwIBAgIQVID5oHPtPwBMyonY43HmSjANBgkqhkiG9w0BAQUFADB1MQswCQYDVQQG
+EwJFRTEiMCAGA1UECgwZQVMgU2VydGlmaXRzZWVyaW1pc2tlc2t1czEoMCYGA1UEAwwfRUUgQ2Vy
+dGlmaWNhdGlvbiBDZW50cmUgUm9vdCBDQTEYMBYGCSqGSIb3DQEJARYJcGtpQHNrLmVlMCIYDzIw
+MTAxMDMwMTAxMDMwWhgPMjAzMDEyMTcyMzU5NTlaMHUxCzAJBgNVBAYTAkVFMSIwIAYDVQQKDBlB
+UyBTZXJ0aWZpdHNlZXJpbWlza2Vza3VzMSgwJgYDVQQDDB9FRSBDZXJ0aWZpY2F0aW9uIENlbnRy
+ZSBSb290IENBMRgwFgYJKoZIhvcNAQkBFglwa2lAc2suZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IB
+DwAwggEKAoIBAQDIIMDs4MVLqwd4lfNE7vsLDP90jmG7sWLqI9iroWUyeuuOF0+W2Ap7kaJjbMeM
+TC55v6kF/GlclY1i+blw7cNRfdCT5mzrMEvhvH2/UpvObntl8jixwKIy72KyaOBhU8E2lf/slLo2
+rpwcpzIP5Xy0xm90/XsY6KxX7QYgSzIwWFv9zajmofxwvI6Sc9uXp3whrj3B9UiHbCe9nyV0gVWw
+93X2PaRka9ZP585ArQ/dMtO8ihJTmMmJ+xAdTX7Nfh9WDSFwhfYggx/2uh8Ej+p3iDXE/+pOoYtN
+P2MbRMNE1CV2yreN1x5KZmTNXMWcg+HCCIia7E6j8T4cLNlsHaFLAgMBAAGjgYowgYcwDwYDVR0T
+AQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwHQYDVR0OBBYEFBLyWj7qVhy/zQas8fElyalL1BSZ
+MEUGA1UdJQQ+MDwGCCsGAQUFBwMCBggrBgEFBQcDAQYIKwYBBQUHAwMGCCsGAQUFBwMEBggrBgEF
+BQcDCAYIKwYBBQUHAwkwDQYJKoZIhvcNAQEFBQADggEBAHv25MANqhlHt01Xo/6tu7Fq1Q+e2+Rj
+xY6hUFaTlrg4wCQiZrxTFGGVv9DHKpY5P30osxBAIWrEr7BSdxjhlthWXePdNl4dp1BUoMUq5KqM
+lIpPnTX/dqQGE5Gion0ARD9V04I8GtVbvFZMIi5GQ4okQC3zErg7cBqklrkar4dBGmoYDQZPxz5u
+uSlNDUmJEYcyW+ZLBMjkXOZ0c5RdFpgTlf7727FE5TpwrDdr5rMzcijJs1eg9gIWiAYLtqZLICjU
+3j2LrTcFU3T+bsy8QxdxXvnFzBqpYe73dgzzcvRyrc9yAjYHR8/vGVCJYMzpJJUPwssd8m92kMfM
+dcGWxZ0=
+-----END CERTIFICATE-----
diff --git a/lib/logstash/codecs/base.rb b/lib/logstash/codecs/base.rb
new file mode 100644
index 00000000000..e5041704d52
--- /dev/null
+++ b/lib/logstash/codecs/base.rb
@@ -0,0 +1,49 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/event"
+require "logstash/plugin"
+require "logstash/logging"
+
+# This is the base class for logstash codecs.
+module LogStash::Codecs; class Base < LogStash::Plugin
+  include LogStash::Config::Mixin
+  config_name "codec"
+
+  def initialize(params={})
+    super
+    config_init(params)
+    register if respond_to?(:register)
+  end
+
+  public
+  def decode(data)
+    raise "#{self.class}#decode must be overidden"
+  end # def decode
+
+  alias_method :<<, :decode
+
+  public
+  def encode(data)
+    raise "#{self.class}#encode must be overidden"
+  end # def encode
+
+  public 
+  def teardown; end;
+
+  public
+  def on_event(&block)
+    @on_event = block
+  end
+
+  public
+  def flush(&block)
+    # does nothing by default.
+    # if your codec needs a flush method (like you are spooling things)
+    # you must implement this.
+  end
+
+  public
+  def clone
+    return self.class.new(params)
+  end
+end; end # class LogStash::Codecs::Base
diff --git a/lib/logstash/codecs/collectd.rb b/lib/logstash/codecs/collectd.rb
new file mode 100644
index 00000000000..770dc58a295
--- /dev/null
+++ b/lib/logstash/codecs/collectd.rb
@@ -0,0 +1,485 @@
+# encoding utf-8
+require "date"
+require "logstash/codecs/base"
+require "logstash/namespace"
+require "logstash/errors"
+require "tempfile"
+require "time"
+
+# Read events from the connectd binary protocol over the network via udp.
+# See https://collectd.org/wiki/index.php/Binary_protocol
+#
+# Configuration in your Logstash configuration file can be as simple as:
+#     input {
+#       udp {
+#         port => 28526
+#         buffer_size => 1452
+#         codec => collectd { }
+#       }
+#     }
+#
+# A sample collectd.conf to send to Logstash might be:
+#
+#     Hostname    "host.example.com"
+#     LoadPlugin interface
+#     LoadPlugin load
+#     LoadPlugin memory
+#     LoadPlugin network
+#     <Plugin interface>
+#         Interface "eth0"
+#         IgnoreSelected false
+#     </Plugin>
+#     <Plugin network>
+#         <Server "10.0.0.1" "25826">
+#         </Server>
+#     </Plugin>
+#
+# Be sure to replace "10.0.0.1" with the IP of your Logstash instance.
+#
+
+class ProtocolError < LogStash::Error; end
+class HeaderError < LogStash::Error; end
+class EncryptionError < LogStash::Error; end
+class NaNError < LogStash::Error; end
+
+class LogStash::Codecs::Collectd < LogStash::Codecs::Base
+  config_name "collectd"
+  milestone 1
+
+  AUTHFILEREGEX = /([^:]+): (.+)/
+
+  PLUGIN_TYPE = 2
+  COLLECTD_TYPE = 4
+  SIGNATURE_TYPE = 512
+  ENCRYPTION_TYPE = 528
+
+  TYPEMAP = {
+    0               => "host",
+    1               => "@timestamp",
+    PLUGIN_TYPE     => "plugin",
+    3               => "plugin_instance",
+    COLLECTD_TYPE   => "collectd_type",
+    5               => "type_instance",
+    6               => "values",
+    7               => "interval",
+    8               => "@timestamp",
+    9               => "interval",
+    256             => "message",
+    257             => "severity",
+    SIGNATURE_TYPE  => "signature",
+    ENCRYPTION_TYPE => "encryption"
+  }
+
+  PLUGIN_TYPE_FIELDS = {
+    'host' => true,
+    '@timestamp' => true,
+  }
+
+  COLLECTD_TYPE_FIELDS = {
+    'host' => true,
+    '@timestamp' => true,
+    'plugin' => true,
+    'plugin_instance' => true,
+    'type_instance' => true,
+  }
+
+  INTERVAL_VALUES_FIELDS = {
+    "interval" => true,
+    "values" => true,
+  }
+
+  INTERVAL_BASE_FIELDS = {
+    'host' => true,
+    'collectd_type' => true,
+    'plugin' => true,
+    'plugin_instance' => true,
+    '@timestamp' => true,
+    'type_instance' => true,
+  }
+
+  INTERVAL_TYPES = {
+    7 => true,
+    9 => true,
+  }
+
+  SECURITY_NONE = "None"
+  SECURITY_SIGN = "Sign"
+  SECURITY_ENCR = "Encrypt"
+
+  # File path(s) to collectd types.db to use.
+  # The last matching pattern wins if you have identical pattern names in multiple files.
+  # If no types.db is provided the included types.db will be used (currently 5.4.0).
+  config :typesdb, :validate => :array
+
+  # Prune interval records.  Defaults to true.
+  config :prune_intervals, :validate => :boolean, :default => true
+
+  # Security Level. Default is "None". This setting mirrors the setting from the
+  # collectd [Network plugin](https://collectd.org/wiki/index.php/Plugin:Network)
+  config :security_level, :validate => [SECURITY_NONE, SECURITY_SIGN, SECURITY_ENCR],
+    :default => "None"
+  
+  # What to do when a value in the event is NaN (Not a Number)
+  # - change_value (default): Change the NaN to the value of the nan_value option and add nan_tag as a tag
+  # - warn: Change the NaN to the value of the nan_value option, print a warning to the log and add nan_tag as a tag
+  # - drop: Drop the event containing the NaN (this only drops the single event, not the whole packet)
+  config :nan_handling, :validate => ['change_value','warn','drop'], :default => 'change_value'
+  
+  # Only relevant when nan_handeling is set to 'change_value'
+  # Change NaN to this configured value
+  config :nan_value, :validate => :number, :default => 0
+  
+  # The tag to add to the event if a NaN value was found
+  # Set this to an empty string ('') if you don't want to tag
+  config :nan_tag, :validate => :string, :default => '_collectdNaN'
+
+  # Path to the authentication file. This file should have the same format as
+  # the [AuthFile](http://collectd.org/documentation/manpages/collectd.conf.5.shtml#authfile_filename)
+  # in collectd. You only need to set this option if the security_level is set to
+  # "Sign" or "Encrypt"
+  config :authfile, :validate => :string
+
+  public
+  def register
+    @logger.info("Starting Collectd codec...")
+    init_lambdas!
+    if @typesdb.nil?
+      @typesdb = LogStash::Environment.vendor_path("collectd/types.db")
+      if !File.exists?(@typesdb)
+        raise "You must specify 'typesdb => ...' in your collectd input (I looked for '#{@typesdb}')"
+      end
+      @logger.info("Using types.db", :typesdb => @typesdb.to_s)
+    end
+    @types = get_types(@typesdb)
+
+    if ([SECURITY_SIGN, SECURITY_ENCR].include?(@security_level))
+      if @authfile.nil?
+        raise "Security level is set to #{@security_level}, but no authfile was configured"
+      else
+        # Load OpenSSL and instantiate Digest and Crypto functions
+        require 'openssl'
+        @sha256 = OpenSSL::Digest::Digest.new('sha256')
+        @sha1 = OpenSSL::Digest::Digest.new('sha1')
+        @cipher = OpenSSL::Cipher.new('AES-256-OFB')
+        @auth = {}
+        parse_authfile
+      end
+    end
+  end # def register
+
+  public
+  def get_types(paths)
+    types = {}
+    # Get the typesdb
+    paths = Array(paths) # Make sure a single path is still forced into an array type
+    paths.each do |path|
+      @logger.info("Getting Collectd typesdb info", :typesdb => path.to_s)
+      File.open(path, 'r').each_line do |line|
+        typename, *line = line.strip.split
+        @logger.debug("typename", :typename => typename.to_s)
+        next if typename.nil? || typename[0,1] == '#'
+        types[typename] = line.collect { |l| l.strip.split(":")[0] }
+      end
+    end
+    @logger.debug("Collectd Types", :types => types.to_s)
+    return types
+  end # def get_types
+
+  def init_lambdas!
+    # Lambdas for hash + closure methodology
+    # This replaces when statements for fixed values and is much faster
+    string_decoder  = lambda { |body| body.pack("C*")[0..-2] }
+    numeric_decoder = lambda { |body| body.slice!(0..7).pack("C*").unpack("E")[0] }
+    counter_decoder = lambda { |body| body.slice!(0..7).pack("C*").unpack("Q>")[0] }
+    gauge_decoder   = lambda { |body| body.slice!(0..7).pack("C*").unpack("E")[0] }
+    derive_decoder  = lambda { |body| body.slice!(0..7).pack("C*").unpack("q>")[0] }
+    # For Low-Resolution time
+    time_decoder = lambda do |body|
+      byte1, byte2 = body.pack("C*").unpack("NN")
+      Time.at(( ((byte1 << 32) + byte2))).utc
+    end
+    # Hi-Resolution time
+    hirestime_decoder = lambda do |body|
+      byte1, byte2 = body.pack("C*").unpack("NN")
+      Time.at(( ((byte1 << 32) + byte2) * (2**-30) )).utc
+    end
+    # Hi resolution intervals
+    hiresinterval_decoder = lambda do |body|
+      byte1, byte2 = body.pack("C*").unpack("NN")
+      Time.at(( ((byte1 << 32) + byte2) * (2**-30) )).to_i
+    end
+    # Value type decoder
+    value_type_decoder = lambda do |body|
+      body.slice!(0..1)       # Prune the header
+      if body.length % 9 == 0 # Should be 9 fields
+        count = 0
+        retval = []
+        # Iterate through and take a slice each time
+        types = body.slice!(0..((body.length/9)-1))
+        while body.length > 0
+          # Use another hash + closure here...
+          v = @values_decoder[types[count]].call(body)
+          if types[count] == 1 && v.nan?
+            case @nan_handling
+            when 'drop'; drop = true
+            else
+              v = @nan_value
+              add_nan_tag = true
+              @nan_handling == 'warn' && @logger.warn("NaN replaced by #{@nan_value}")
+            end
+          end
+          retval << v
+          count += 1
+        end
+      else
+        @logger.error("Incorrect number of data fields for collectd record", :body => body.to_s)
+      end
+      return retval, drop, add_nan_tag
+    end
+    # Signature
+    signature_decoder = lambda do |body|
+      if body.length < 32
+        @logger.warning("SHA256 signature too small (got #{body.length} bytes instead of 32)")
+      elsif body.length < 33
+        @logger.warning("Received signature without username")
+      else
+        retval = []
+        # Byte 32 till the end contains the username as chars (=unsigned ints)
+        retval << body[32..-1].pack('C*')
+        # Byte 0 till 31 contain the signature
+        retval << body[0..31].pack('C*')
+      end
+      return retval
+    end
+    # Encryption
+    encryption_decoder = lambda do |body|
+      retval = []
+      user_length = (body.slice!(0) << 8) + body.slice!(0)
+      retval << body.slice!(0..user_length-1).pack('C*') # Username
+      retval << body.slice!(0..15).pack('C*')            # IV
+      retval << body.pack('C*')
+      return retval
+    end
+    @id_decoder = {
+      0 => string_decoder,
+      1 => time_decoder,
+      2 => string_decoder,
+      3 => string_decoder,
+      4 => string_decoder,
+      5 => string_decoder,
+      6 => value_type_decoder,
+      7 => numeric_decoder,
+      8 => hirestime_decoder,
+      9 => hiresinterval_decoder,
+      256 => string_decoder,
+      257 => numeric_decoder,
+      512 => signature_decoder,
+      528 => encryption_decoder
+    }
+    # TYPE VALUES:
+    # 0: COUNTER
+    # 1: GAUGE
+    # 2: DERIVE
+    # 3: ABSOLUTE
+    @values_decoder = {
+      0 => counter_decoder,
+      1 => gauge_decoder,
+      2 => derive_decoder,
+      3 => counter_decoder
+    }
+  end # def init_lambdas!
+
+  public
+  def get_values(id, body)
+    drop = false
+    add_tag = false
+    if id == 6
+      retval, drop, add_nan_tag = @id_decoder[id].call(body)
+    # Use hash + closure/lambda to speed operations
+    else
+      retval = @id_decoder[id].call(body)
+    end
+    return retval, drop, add_nan_tag
+  end
+
+  private
+  def parse_authfile
+    # We keep the authfile parsed in memory so we don't have to open the file
+    # for every event.
+    @logger.debug("Parsing authfile #{@authfile}")
+    if !File.exist?(@authfile)
+      raise LogStash::ConfigurationError, "The file #{@authfile} was not found"
+    end
+    @auth.clear
+    @authmtime = File.stat(@authfile).mtime
+    File.readlines(@authfile).each do |line|
+      #line.chomp!
+      k,v = line.scan(AUTHFILEREGEX).flatten
+      if k && v
+        @logger.debug("Added authfile entry '#{k}' with key '#{v}'")
+        @auth[k] = v
+      else
+        @logger.info("Ignoring malformed authfile line '#{line.chomp}'")
+      end
+    end
+  end # def parse_authfile
+
+  private
+  def get_key(user)
+    return if @authmtime.nil? or @authfile.nil?
+    # Validate that our auth data is still up-to-date
+    parse_authfile if @authmtime < File.stat(@authfile).mtime
+    key = @auth[user]
+    @logger.warn("User #{user} is not found in the authfile #{@authfile}") if key.nil?
+    return key
+  end # def get_key
+
+  private
+  def verify_signature(user, signature, payload)
+    # The user doesn't care about the security
+    return true if @security_level == SECURITY_NONE
+
+    # We probably got and array of ints, pack it!
+    payload = payload.pack('C*') if payload.is_a?(Array)
+
+    key = get_key(user)
+    return false if key.nil?
+
+    return OpenSSL::HMAC.digest(@sha256, key, user+payload) == signature
+  end # def verify_signature
+
+  private
+  def decrypt_packet(user, iv, content)
+    # Content has to have at least a SHA1 hash (20 bytes), a header (4 bytes) and
+    # one byte of data
+    return [] if content.length < 26
+    content = content.pack('C*') if content.is_a?(Array)
+    key = get_key(user)
+    if key.nil?
+      @logger.debug("Key was nil")
+      return []
+    end
+
+    # Set the correct state of the cipher instance
+    @cipher.decrypt
+    @cipher.padding = 0
+    @cipher.iv = iv
+    @cipher.key = @sha256.digest(key);
+    # Decrypt the content
+    plaintext = @cipher.update(content) + @cipher.final
+    # Reset the state, as adding a new key to an already instantiated state
+    # results in an exception
+    @cipher.reset
+
+    # The plaintext contains a SHA1 hash as checksum in the first 160 bits
+    # (20 octets) of the rest of the data
+    hash = plaintext.slice!(0..19)
+
+    if @sha1.digest(plaintext) != hash
+      @logger.warn("Unable to decrypt packet, checksum mismatch")
+      return []
+    end
+    return plaintext.unpack('C*')
+  end # def decrypt_packet
+
+  public
+  def decode(payload)
+    payload = payload.bytes.to_a
+
+    collectd = {}
+    was_encrypted = false
+
+    while payload.length > 0 do
+      typenum = (payload.slice!(0) << 8) + payload.slice!(0)
+      # Get the length of the data in this part, but take into account that
+      # the header is 4 bytes
+      length  = ((payload.slice!(0) << 8) + payload.slice!(0)) - 4
+      # Validate that the part length is correct
+      raise(HeaderError) if length > payload.length
+      
+      body = payload.slice!(0..length-1)
+
+      field = TYPEMAP[typenum]
+      if field.nil?
+        @logger.warn("Unknown typenumber: #{typenum}")
+        next
+      end
+
+      values, drop, add_nan_tag = get_values(typenum, body)
+
+      case typenum
+      when SIGNATURE_TYPE
+        raise(EncryptionError) unless verify_signature(values[0], values[1], payload)
+        next
+      when ENCRYPTION_TYPE
+        payload = decrypt_packet(values[0], values[1], values[2])
+        raise(EncryptionError) if payload.empty?
+        was_encrypted = true
+        next
+      when PLUGIN_TYPE
+        # We've reached a new plugin, delete everything except for the the host
+        # field, because there's only one per packet and the timestamp field,
+        # because that one goes in front of the plugin
+        collectd.each_key do |k|
+          collectd.delete(k) unless PLUGIN_TYPE_FIELDS.has_key?(k)
+        end
+      when COLLECTD_TYPE
+        # We've reached a new type within the plugin section, delete all fields
+        # that could have something to do with the previous type (if any)
+        collectd.each_key do |k|
+          collectd.delete(k) unless COLLECTD_TYPE_FIELDS.has_key?(k)
+        end
+      end
+
+      raise(EncryptionError) if !was_encrypted and @security_level == SECURITY_ENCR
+
+      # Fill in the fields.
+      if values.is_a?(Array)
+        if values.length > 1              # Only do this iteration on multi-value arrays
+          values.each_with_index do |value, x|
+            begin
+              type = collectd['collectd_type']
+              key = @types[type]
+              key_x = key[x]
+              # assign
+              collectd[key_x] = value
+            rescue
+              @logger.error("Invalid value for type=#{type.inspect}, key=#{@types[type].inspect}, index=#{x}")
+            end
+          end
+        else                              # Otherwise it's a single value
+          collectd['value'] = values[0]      # So name it 'value' accordingly
+        end
+      elsif field != nil                  # Not an array, make sure it's non-empty
+        collectd[field] = values            # Append values to collectd under key field
+      end
+
+      if INTERVAL_VALUES_FIELDS.has_key?(field)
+        if ((@prune_intervals && !INTERVAL_TYPES.has_key?(typenum)) || !@prune_intervals)
+          # Prune these *specific* keys if they exist and are empty.
+          # This is better than looping over all keys every time.
+          collectd.delete('type_instance') if collectd['type_instance'] == ""
+          collectd.delete('plugin_instance') if collectd['plugin_instance'] == ""
+          if add_nan_tag
+            collectd['tags'] ||= []
+            collectd['tags'] << @nan_tag
+          end
+          # This ugly little shallow-copy hack keeps the new event from getting munged by the cleanup
+          # With pass-by-reference we get hosed (if we pass collectd, then clean it up rapidly, values can disappear)
+          if !drop # Drop the event if it's flagged true
+            yield LogStash::Event.new(collectd.dup)
+          else
+            raise(NaNError)
+          end
+        end
+        # Clean up the event
+        collectd.each_key do |k|
+          collectd.delete(k) if !INTERVAL_BASE_FIELDS.has_key?(k)
+        end
+      end
+    end # while payload.length > 0 do
+  rescue EncryptionError, ProtocolError, HeaderError, NaNError
+    # basically do nothing, we just want out
+  end # def decode
+
+end # class LogStash::Codecs::Collectd
\ No newline at end of file
diff --git a/lib/logstash/codecs/dots.rb b/lib/logstash/codecs/dots.rb
new file mode 100644
index 00000000000..471e60dfbea
--- /dev/null
+++ b/lib/logstash/codecs/dots.rb
@@ -0,0 +1,18 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+
+class LogStash::Codecs::Dots < LogStash::Codecs::Base
+  config_name "dots"
+  milestone 1
+
+  public
+  def decode(data)
+    raise "Not implemented"
+  end # def decode
+
+  public
+  def encode(data)
+    @on_event.call(".")
+  end # def encode
+
+end # class LogStash::Codecs::Dots
diff --git a/lib/logstash/codecs/edn.rb b/lib/logstash/codecs/edn.rb
new file mode 100644
index 00000000000..449b7cec40c
--- /dev/null
+++ b/lib/logstash/codecs/edn.rb
@@ -0,0 +1,34 @@
+require "logstash/codecs/base"
+require "logstash/codecs/line"
+require "logstash/util"
+
+class LogStash::Codecs::EDN < LogStash::Codecs::Base
+  config_name "edn"
+
+  milestone 1
+
+  def register
+    require "edn"
+  end
+
+  public
+  def decode(data)
+    begin
+      yield LogStash::Event.new(EDN.read(data))
+    rescue => e
+      @logger.warn("EDN parse failure. Falling back to plain-text", :error => e, :data => data)
+      yield LogStash::Event.new("message" => data)
+    end
+  end
+
+  public
+  def encode(event)
+    # use normalize to make sure returned Hash is pure Ruby
+    # #to_edn which relies on pure Ruby object recognition
+    data = LogStash::Util.normalize(event.to_hash)
+    # timestamp is serialized as a iso8601 string
+    # merge to avoid modifying data which could have side effects if multiple outputs
+    @on_event.call(data.merge(LogStash::Event::TIMESTAMP => event.timestamp.to_iso8601).to_edn)
+  end
+
+end
diff --git a/lib/logstash/codecs/edn_lines.rb b/lib/logstash/codecs/edn_lines.rb
new file mode 100644
index 00000000000..3c4a0a38b84
--- /dev/null
+++ b/lib/logstash/codecs/edn_lines.rb
@@ -0,0 +1,42 @@
+require "logstash/codecs/base"
+require "logstash/codecs/line"
+require "logstash/util"
+
+class LogStash::Codecs::EDNLines < LogStash::Codecs::Base
+  config_name "edn_lines"
+
+  milestone 1
+
+  def register
+    require "edn"
+  end
+
+  public
+  def initialize(params={})
+    super(params)
+    @lines = LogStash::Codecs::Line.new
+  end
+
+  public
+  def decode(data)
+    @lines.decode(data) do |event|
+      begin
+        yield LogStash::Event.new(EDN.read(event["message"]))
+      rescue => e
+        @logger.warn("EDN parse failure. Falling back to plain-text", :error => e, :data => data)
+        yield LogStash::Event.new("message" => data)
+      end
+    end
+  end
+
+  public
+  def encode(event)
+    # use normalize to make sure returned Hash is pure Ruby for
+    # #to_edn which relies on pure Ruby object recognition
+    data = LogStash::Util.normalize(event.to_hash)
+    # timestamp is serialized as a iso8601 string
+    # merge to avoid modifying data which could have side effects if multiple outputs
+    @on_event.call(data.merge(LogStash::Event::TIMESTAMP => event.timestamp.to_iso8601).to_edn + NL)
+  end
+
+end
diff --git a/lib/logstash/codecs/fluent.rb b/lib/logstash/codecs/fluent.rb
new file mode 100644
index 00000000000..cbcaf3ebe46
--- /dev/null
+++ b/lib/logstash/codecs/fluent.rb
@@ -0,0 +1,63 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/util/charset"
+require "logstash/timestamp"
+require "logstash/util"
+
+# This codec handles fluentd's msgpack schema.
+#
+# For example, you can receive logs from fluent-logger-ruby with:
+#
+#     input {
+#       tcp {
+#         codec => fluent
+#         port => 4000
+#       }
+#     }
+#
+# And from your ruby code in your own application:
+#
+#     logger = Fluent::Logger::FluentLogger.new(nil, :host => "example.log", :port => 4000)
+#     logger.post("some_tag", { "your" => "data", "here" => "yay!" })
+#
+# Notes:
+#
+# * the fluent uses a second-precision time for events, so you will never see
+#   subsecond precision on events processed by this codec.
+#
+class LogStash::Codecs::Fluent < LogStash::Codecs::Base
+  config_name "fluent"
+  milestone 1
+
+  public
+  def register
+    require "msgpack"
+    @decoder = MessagePack::Unpacker.new
+  end
+
+  public
+  def decode(data)
+    @decoder.feed(data)
+    @decoder.each do |tag, epochtime, map|
+      event = LogStash::Event.new(map.merge(
+        LogStash::Event::TIMESTAMP => LogStash::Timestamp.at(epochtime),
+        "tags" => tag
+      ))
+      yield event
+    end
+  end # def decode
+
+  public
+  def encode(event)
+    tag = event["tags"] || "log"
+    epochtime = event.timestamp.to_i
+
+    # use normalize to make sure returned Hash is pure Ruby for
+    # MessagePack#pack which relies on pure Ruby object recognition
+    data = LogStash::Util.normalize(event.to_hash)
+    # timestamp is serialized as a iso8601 string
+    # merge to avoid modifying data which could have side effects if multiple outputs
+    @on_event.call(MessagePack.pack([tag, epochtime, data.merge(LogStash::Event::TIMESTAMP => event.timestamp.to_iso8601)]))
+  end # def encode
+
+end # class LogStash::Codecs::Fluent
diff --git a/lib/logstash/codecs/graphite.rb b/lib/logstash/codecs/graphite.rb
new file mode 100644
index 00000000000..4471df9aa59
--- /dev/null
+++ b/lib/logstash/codecs/graphite.rb
@@ -0,0 +1,103 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/codecs/line"
+require "logstash/timestamp"
+
+# This codec will encode and decode Graphite formated lines.
+class LogStash::Codecs::Graphite < LogStash::Codecs::Base
+  config_name "graphite"
+
+  milestone 2
+
+  EXCLUDE_ALWAYS = [ "@timestamp", "@version" ]
+
+  DEFAULT_METRICS_FORMAT = "*"
+  METRIC_PLACEHOLDER = "*"
+
+  # The metric(s) to use. This supports dynamic strings like %{host}
+  # for metric names and also for values. This is a hash field with key
+  # of the metric name, value of the metric value. Example:
+  #
+  #     [ "%{host}/uptime", "%{uptime_1m}" ]
+  #
+  # The value will be coerced to a floating point value. Values which cannot be
+  # coerced will zero (0)
+  config :metrics, :validate => :hash, :default => {}
+
+  # Indicate that the event @fields should be treated as metrics and will be sent as is to graphite
+  config :fields_are_metrics, :validate => :boolean, :default => false
+
+  # Include only regex matched metric names
+  config :include_metrics, :validate => :array, :default => [ ".*" ]
+
+  # Exclude regex matched metric names, by default exclude unresolved %{field} strings
+  config :exclude_metrics, :validate => :array, :default => [ "%\{[^}]+\}" ]
+
+  # Defines format of the metric string. The placeholder '*' will be
+  # replaced with the name of the actual metric.
+  #
+  #     metrics_format => "foo.bar.*.sum"
+  #
+  # NOTE: If no metrics_format is defined the name of the metric will be used as fallback.
+  config :metrics_format, :validate => :string, :default => DEFAULT_METRICS_FORMAT
+
+
+  public
+  def initialize(params={})
+    super(params)
+    @lines = LogStash::Codecs::Line.new
+  end
+
+  public
+  def decode(data)
+    @lines.decode(data) do |event|
+      name, value, time = event["message"].split(" ")
+      yield LogStash::Event.new(name => value.to_f, LogStash::Event::TIMESTAMP => LogStash::Timestamp.at(time.to_i))
+    end # @lines.decode
+  end # def decode
+
+  private
+  def construct_metric_name(metric)
+    if @metrics_format
+      return @metrics_format.gsub(METRIC_PLACEHOLDER, metric)
+    end
+
+    return metric
+  end
+
+  public
+  def encode(event)
+    # Graphite message format: metric value timestamp\n
+
+    messages = []
+    timestamp = event.sprintf("%{+%s}")
+
+    if @fields_are_metrics
+      @logger.debug("got metrics event", :metrics => event.to_hash)
+      event.to_hash.each do |metric,value|
+        next if EXCLUDE_ALWAYS.include?(metric)
+        next unless @include_metrics.empty? || @include_metrics.any? { |regexp| metric.match(regexp) }
+        next if @exclude_metrics.any? {|regexp| metric.match(regexp)}
+        messages << "#{construct_metric_name(metric)} #{event.sprintf(value.to_s).to_f} #{timestamp}"
+      end # data.to_hash.each
+    else
+      @metrics.each do |metric, value|
+        @logger.debug("processing", :metric => metric, :value => value)
+        metric = event.sprintf(metric)
+        next unless @include_metrics.any? {|regexp| metric.match(regexp)}
+        next if @exclude_metrics.any? {|regexp| metric.match(regexp)}
+        messages << "#{construct_metric_name(event.sprintf(metric))} #{event.sprintf(value).to_f} #{timestamp}"
+      end # @metrics.each
+    end # if @fields_are_metrics
+
+    if messages.empty?
+      @logger.debug("Message is empty, not emiting anything.", :messages => messages)
+    else
+      message = messages.join(NL) + NL
+      @logger.debug("Emiting carbon messages", :messages => messages)
+
+      @on_event.call(message)
+    end # if messages.empty?
+  end # def encode
+
+end # class LogStash::Codecs::Graphite
diff --git a/lib/logstash/codecs/json.rb b/lib/logstash/codecs/json.rb
new file mode 100644
index 00000000000..1ba0163f8d8
--- /dev/null
+++ b/lib/logstash/codecs/json.rb
@@ -0,0 +1,48 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/util/charset"
+require "logstash/json"
+
+# This codec may be used to decode (via inputs) and encode (via outputs)
+# full JSON messages.  If you are streaming JSON messages delimited
+# by '\n' then see the `json_lines` codec.
+# Encoding will result in a single JSON string.
+class LogStash::Codecs::JSON < LogStash::Codecs::Base
+  config_name "json"
+
+  milestone 3
+
+  # The character encoding used in this codec. Examples include "UTF-8" and
+  # "CP1252".
+  #
+  # JSON requires valid UTF-8 strings, but in some cases, software that
+  # emits JSON does so in another encoding (nxlog, for example). In
+  # weird cases like this, you can set the `charset` setting to the
+  # actual encoding of the text and Logstash will convert it for you.
+  #
+  # For nxlog users, you'll want to set this to "CP1252".
+  config :charset, :validate => ::Encoding.name_list, :default => "UTF-8"
+
+  public
+  def register
+    @converter = LogStash::Util::Charset.new(@charset)
+    @converter.logger = @logger
+  end
+
+  public
+  def decode(data)
+    data = @converter.convert(data)
+    begin
+      yield LogStash::Event.new(LogStash::Json.load(data))
+    rescue LogStash::Json::ParserError => e
+      @logger.info("JSON parse failure. Falling back to plain-text", :error => e, :data => data)
+      yield LogStash::Event.new("message" => data)
+    end
+  end # def decode
+
+  public
+  def encode(event)
+    @on_event.call(event.to_json)
+  end # def encode
+
+end # class LogStash::Codecs::JSON
diff --git a/lib/logstash/codecs/json_lines.rb b/lib/logstash/codecs/json_lines.rb
new file mode 100644
index 00000000000..f319340f7f7
--- /dev/null
+++ b/lib/logstash/codecs/json_lines.rb
@@ -0,0 +1,53 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/codecs/line"
+require "logstash/json"
+
+# This codec will decode streamed JSON that is newline delimited.
+# For decoding line-oriented JSON payload in the redis or file inputs,
+# for example, use the json codec instead.
+# Encoding will emit a single JSON string ending in a '\n'
+class LogStash::Codecs::JSONLines < LogStash::Codecs::Base
+  config_name "json_lines"
+
+  milestone 3
+
+  # The character encoding used in this codec. Examples include "UTF-8" and
+  # "CP1252"
+  #
+  # JSON requires valid UTF-8 strings, but in some cases, software that
+  # emits JSON does so in another encoding (nxlog, for example). In
+  # weird cases like this, you can set the charset setting to the
+  # actual encoding of the text and logstash will convert it for you.
+  #
+  # For nxlog users, you'll want to set this to "CP1252"
+  config :charset, :validate => ::Encoding.name_list, :default => "UTF-8"
+
+  public
+  def initialize(params={})
+    super(params)
+    @lines = LogStash::Codecs::Line.new
+    @lines.charset = @charset
+  end
+
+  public
+  def decode(data)
+
+    @lines.decode(data) do |event|
+      begin
+        yield LogStash::Event.new(LogStash::Json.load(event["message"]))
+      rescue LogStash::Json::ParserError => e
+        @logger.info("JSON parse failure. Falling back to plain-text", :error => e, :data => data)
+        yield LogStash::Event.new("message" => event["message"])
+      end
+    end
+  end # def decode
+
+  public
+  def encode(event)
+    # Tack on a \n for now because previously most of logstash's JSON
+    # outputs emitted one per line, and whitespace is OK in json.
+    @on_event.call(event.to_json + NL)
+  end # def encode
+
+end # class LogStash::Codecs::JSON
diff --git a/lib/logstash/codecs/json_spooler.rb b/lib/logstash/codecs/json_spooler.rb
new file mode 100644
index 00000000000..2dc512b2fa5
--- /dev/null
+++ b/lib/logstash/codecs/json_spooler.rb
@@ -0,0 +1,28 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/codecs/spool"
+require "logstash/json"
+
+# This is the base class for logstash codecs.
+class LogStash::Codecs::JsonSpooler < LogStash::Codecs::Spool
+  config_name "json_spooler"
+  milestone 0
+
+  public
+  def register
+    @logger.error("the json_spooler codec is deprecated and will be removed in a future release")
+  end
+
+  public
+  def decode(data)
+    super(LogStash::Json.load(data.force_encoding(Encoding::UTF_8))) do |event|
+      yield event
+    end
+  end # def decode
+
+  public
+  def encode(event)
+    super(event)
+  end # def encode
+
+end # class LogStash::Codecs::Json
diff --git a/lib/logstash/codecs/line.rb b/lib/logstash/codecs/line.rb
new file mode 100644
index 00000000000..8bd77ff5805
--- /dev/null
+++ b/lib/logstash/codecs/line.rb
@@ -0,0 +1,58 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/util/charset"
+
+# Line-oriented text data.
+#
+# Decoding behavior: Only whole line events will be emitted.
+#
+# Encoding behavior: Each event will be emitted with a trailing newline.
+class LogStash::Codecs::Line < LogStash::Codecs::Base
+  config_name "line"
+  milestone 3
+
+  # Set the desired text format for encoding.
+  config :format, :validate => :string
+
+  # The character encoding used in this input. Examples include "UTF-8"
+  # and "cp1252"
+  #
+  # This setting is useful if your log files are in Latin-1 (aka cp1252)
+  # or in another character set other than UTF-8.
+  #
+  # This only affects "plain" format logs since json is UTF-8 already.
+  config :charset, :validate => ::Encoding.name_list, :default => "UTF-8"
+
+  public
+  def register
+    require "logstash/util/buftok"
+    @buffer = FileWatch::BufferedTokenizer.new
+    @converter = LogStash::Util::Charset.new(@charset)
+    @converter.logger = @logger
+  end
+
+  public
+  def decode(data)
+    @buffer.extract(data).each do |line|
+      yield LogStash::Event.new("message" => @converter.convert(line))
+    end
+  end # def decode
+
+  public
+  def flush(&block)
+    remainder = @buffer.flush
+    if !remainder.empty?
+      block.call(LogStash::Event.new("message" => @converter.convert(remainder)))
+    end
+  end
+
+  public
+  def encode(event)
+    if event.is_a? LogStash::Event and @format
+      @on_event.call(event.sprintf(@format) + NL)
+    else
+      @on_event.call(event.to_s + NL)
+    end
+  end # def encode
+
+end # class LogStash::Codecs::Plain
diff --git a/lib/logstash/codecs/msgpack.rb b/lib/logstash/codecs/msgpack.rb
new file mode 100644
index 00000000000..b2f45e28a7f
--- /dev/null
+++ b/lib/logstash/codecs/msgpack.rb
@@ -0,0 +1,48 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/timestamp"
+require "logstash/util"
+
+class LogStash::Codecs::Msgpack < LogStash::Codecs::Base
+  config_name "msgpack"
+
+  milestone 1
+
+  config :format, :validate => :string, :default => nil
+
+  public
+  def register
+    require "msgpack"
+  end
+
+  public
+  def decode(data)
+    begin
+      # Msgpack does not care about UTF-8
+      event = LogStash::Event.new(MessagePack.unpack(data))
+      event["tags"] ||= []
+      if @format
+        event["message"] ||= event.sprintf(@format)
+      end
+    rescue => e
+      # Treat as plain text and try to do the best we can with it?
+      @logger.warn("Trouble parsing msgpack input, falling back to plain text",
+                   :input => data, :exception => e)
+      event["message"] = data
+      event["tags"] ||= []
+      event["tags"] << "_msgpackparsefailure"
+    end
+    yield event
+  end # def decode
+
+  public
+  def encode(event)
+    # use normalize to make sure returned Hash is pure Ruby for
+    # MessagePack#pack which relies on pure Ruby object recognition
+    data = LogStash::Util.normalize(event.to_hash)
+    # timestamp is serialized as a iso8601 string
+    # merge to avoid modifying data which could have side effects if multiple outputs
+    @on_event.call(MessagePack.pack(data.merge(LogStash::Event::TIMESTAMP => event.timestamp.to_iso8601)))
+  end # def encode
+
+end # class LogStash::Codecs::Msgpack
diff --git a/lib/logstash/codecs/multiline.rb b/lib/logstash/codecs/multiline.rb
new file mode 100644
index 00000000000..4815ffa960f
--- /dev/null
+++ b/lib/logstash/codecs/multiline.rb
@@ -0,0 +1,194 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/util/charset"
+require "logstash/timestamp"
+
+# The multiline codec will collapse multiline messages and merge them into a
+# single event.
+#
+# The original goal of this codec was to allow joining of multiline messages
+# from files into a single event. For example, joining Java exception and
+# stacktrace messages into a single event.
+#
+# The config looks like this:
+#
+#     input {
+#       stdin {
+#         codec => multiline {
+#           pattern => "pattern, a regexp"
+#           negate => "true" or "false"
+#           what => "previous" or "next"
+#         }
+#       }
+#     }
+#
+# The `pattern` should match what you believe to be an indicator that the field
+# is part of a multi-line event.
+#
+# The `what` must be "previous" or "next" and indicates the relation
+# to the multi-line event.
+#
+# The `negate` can be "true" or "false" (defaults to "false"). If "true", a
+# message not matching the pattern will constitute a match of the multiline
+# filter and the `what` will be applied. (vice-versa is also true)
+#
+# For example, Java stack traces are multiline and usually have the message
+# starting at the far-left, with each subsequent line indented. Do this:
+#
+#     input {
+#       stdin {
+#         codec => multiline {
+#           pattern => "^\s"
+#           what => "previous"
+#         }
+#       }
+#     }
+#
+# This says that any line starting with whitespace belongs to the previous line.
+#
+# Another example is to merge lines not starting with a date up to the previous
+# line..
+#
+#     input {
+#       file {
+#         path => "/var/log/someapp.log"
+#         codec => multiline {
+#           # Grok pattern names are valid! :)
+#           pattern => "^%{TIMESTAMP_ISO8601} "
+#           negate => true
+#           what => previous
+#         }
+#       }
+#     }
+#
+# This says that any line not starting with a timestamp should be merged with the previous line.
+#
+# One more common example is C line continuations (backslash). Here's how to do that:
+#
+#     filter {
+#       multiline {
+#         type => "somefiletype"
+#         pattern => "\\$"
+#         what => "next"
+#       }
+#     }
+#
+# This says that any line ending with a backslash should be combined with the
+# following line.
+#
+class LogStash::Codecs::Multiline < LogStash::Codecs::Base
+  config_name "multiline"
+  milestone 3
+
+  # The regular expression to match.
+  config :pattern, :validate => :string, :required => true
+
+  # If the pattern matched, does event belong to the next or previous event?
+  config :what, :validate => ["previous", "next"], :required => true
+
+  # Negate the regexp pattern ('if not matched').
+  config :negate, :validate => :boolean, :default => false
+
+  # Logstash ships by default with a bunch of patterns, so you don't
+  # necessarily need to define this yourself unless you are adding additional
+  # patterns.
+  #
+  # Pattern files are plain text with format:
+  #
+  #     NAME PATTERN
+  #
+  # For example:
+  #
+  #     NUMBER \d+
+  config :patterns_dir, :validate => :array, :default => []
+
+  # The character encoding used in this input. Examples include "UTF-8"
+  # and "cp1252"
+  #
+  # This setting is useful if your log files are in Latin-1 (aka cp1252)
+  # or in another character set other than UTF-8.
+  #
+  # This only affects "plain" format logs since JSON is UTF-8 already.
+  config :charset, :validate => ::Encoding.name_list, :default => "UTF-8"
+
+  # Tag multiline events with a given tag. This tag will only be added
+  # to events that actually have multiple lines in them.
+  config :multiline_tag, :validate => :string, :default => "multiline"
+
+  public
+  def register
+    require "grok-pure" # rubygem 'jls-grok'
+    # Detect if we are running from a jarfile, pick the right path.
+    patterns_path = []
+    patterns_path += ["#{File.dirname(__FILE__)}/../../../patterns/*"]
+
+    @grok = Grok.new
+
+    @patterns_dir = patterns_path.to_a + @patterns_dir
+    @patterns_dir.each do |path|
+      if File.directory?(path)
+        path = File.join(path, "*")
+      end
+
+      Dir.glob(path).each do |file|
+        @logger.info("Grok loading patterns from file", :path => file)
+        @grok.add_patterns_from_file(file)
+      end
+    end
+
+    @grok.compile(@pattern)
+    @logger.debug("Registered multiline plugin", :type => @type, :config => @config)
+
+    @buffer = []
+    @handler = method("do_#{@what}".to_sym)
+
+    @converter = LogStash::Util::Charset.new(@charset)
+    @converter.logger = @logger
+  end # def register
+
+  public
+  def decode(text, &block)
+    text = @converter.convert(text)
+
+    match = @grok.match(text)
+    @logger.debug("Multiline", :pattern => @pattern, :text => text,
+                  :match => !match.nil?, :negate => @negate)
+
+    # Add negate option
+    match = (match and !@negate) || (!match and @negate)
+    @handler.call(text, match, &block)
+  end # def decode
+
+  def buffer(text)
+    @time = LogStash::Timestamp.now if @buffer.empty?
+    @buffer << text
+  end
+
+  def flush(&block)
+    if @buffer.any?
+      event = LogStash::Event.new(LogStash::Event::TIMESTAMP => @time, "message" => @buffer.join(NL))
+      # Tag multiline events
+      event.tag @multiline_tag if @multiline_tag && @buffer.size > 1
+
+      yield event
+      @buffer = []
+    end
+  end
+
+  def do_next(text, matched, &block)
+    buffer(text)
+    flush(&block) if !matched
+  end
+
+  def do_previous(text, matched, &block)
+    flush(&block) if !matched
+    buffer(text)
+  end
+
+  public
+  def encode(event)
+    # Nothing to do.
+    @on_event.call(event)
+  end # def encode
+
+end # class LogStash::Codecs::Plain
diff --git a/lib/logstash/codecs/netflow.rb b/lib/logstash/codecs/netflow.rb
new file mode 100644
index 00000000000..360de0fd87f
--- /dev/null
+++ b/lib/logstash/codecs/netflow.rb
@@ -0,0 +1,263 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+require "logstash/timestamp"
+
+# The "netflow" codec is for decoding Netflow v5/v9 flows.
+class LogStash::Codecs::Netflow < LogStash::Codecs::Base
+  config_name "netflow"
+  milestone 1
+
+  # Netflow v9 template cache TTL (minutes)
+  config :cache_ttl, :validate => :number, :default => 4000
+
+  # Specify into what field you want the Netflow data.
+  config :target, :validate => :string, :default => "netflow"
+
+  # Specify which Netflow versions you will accept.
+  config :versions, :validate => :array, :default => [5, 9]
+
+  # Override YAML file containing Netflow field definitions
+  #
+  # Each Netflow field is defined like so:
+  #
+  #    ---
+  #    id:
+  #    - default length in bytes
+  #    - :name
+  #    id:
+  #    - :uintN or :ip4_addr or :ip6_addr or :mac_addr or :string
+  #    - :name
+  #    id:
+  #    - :skip
+  #
+  # See <https://github.com/logstash/logstash/tree/v%VERSION%/lib/logstash/codecs/netflow/netflow.yaml> for the base set.
+  config :definitions, :validate => :path
+
+  public
+  def initialize(params={})
+    super(params)
+    @threadsafe = false
+  end
+
+  public
+  def register
+    require "logstash/codecs/netflow/util"
+    @templates = Vash.new()
+
+    # Path to default Netflow v9 field definitions
+    filename = LogStash::Environment.plugin_path("codecs/netflow/netflow.yaml")
+
+    begin
+      @fields = YAML.load_file(filename)
+    rescue Exception => e
+      raise "#{self.class.name}: Bad syntax in definitions file #{filename}"
+    end
+
+    # Allow the user to augment/override/rename the supported Netflow fields
+    if @definitions
+      raise "#{self.class.name}: definitions file #{@definitions} does not exists" unless File.exists?(@definitions)
+      begin
+        @fields.merge!(YAML.load_file(@definitions))
+      rescue Exception => e
+        raise "#{self.class.name}: Bad syntax in definitions file #{@definitions}"
+      end
+    end
+  end # def register
+
+  public
+  def decode(payload, &block)
+    header = Header.read(payload)
+
+    unless @versions.include?(header.version)
+      @logger.warn("Ignoring Netflow version v#{header.version}")
+      return
+    end
+
+    if header.version == 5
+      flowset = Netflow5PDU.read(payload)
+    elsif header.version == 9
+      flowset = Netflow9PDU.read(payload)
+    else
+      @logger.warn("Unsupported Netflow version v#{header.version}")
+      return
+    end
+
+    flowset.records.each do |record|
+      if flowset.version == 5
+        event = LogStash::Event.new
+
+        # FIXME Probably not doing this right WRT JRuby?
+        #
+        # The flowset header gives us the UTC epoch seconds along with
+        # residual nanoseconds so we can set @timestamp to that easily
+        event.timestamp = LogStash::Timestamp.at(flowset.unix_sec, flowset.unix_nsec / 1000)
+        event[@target] = {}
+
+        # Copy some of the pertinent fields in the header to the event
+        ['version', 'flow_seq_num', 'engine_type', 'engine_id', 'sampling_algorithm', 'sampling_interval', 'flow_records'].each do |f|
+          event[@target][f] = flowset[f]
+        end
+
+        # Create fields in the event from each field in the flow record
+        record.each_pair do |k,v|
+          case k.to_s
+          when /_switched$/
+            # The flow record sets the first and last times to the device
+            # uptime in milliseconds. Given the actual uptime is provided
+            # in the flowset header along with the epoch seconds we can
+            # convert these into absolute times
+            millis = flowset.uptime - v
+            seconds = flowset.unix_sec - (millis / 1000)
+            micros = (flowset.unix_nsec / 1000) - (millis % 1000)
+            if micros < 0
+              seconds--
+              micros += 1000000
+            end
+            # FIXME Again, probably doing this wrong WRT JRuby?
+            event[@target][k.to_s] = Time.at(seconds, micros).utc.strftime("%Y-%m-%dT%H:%M:%S.%3NZ")
+          else
+            event[@target][k.to_s] = v
+          end
+        end
+
+        yield event
+      elsif flowset.version == 9
+        case record.flowset_id
+        when 0
+          # Template flowset
+          record.flowset_data.templates.each do |template|
+            catch (:field) do
+              fields = []
+              template.fields.each do |field|
+                entry = netflow_field_for(field.field_type, field.field_length)
+                if ! entry
+                  throw :field
+                end
+                fields += entry
+              end
+              # We get this far, we have a list of fields
+              #key = "#{flowset.source_id}|#{event["source"]}|#{template.template_id}"
+              key = "#{flowset.source_id}|#{template.template_id}"
+              @templates[key, @cache_ttl] = BinData::Struct.new(:endian => :big, :fields => fields)
+              # Purge any expired templates
+              @templates.cleanup!
+            end
+          end
+        when 1
+          # Options template flowset
+          record.flowset_data.templates.each do |template|
+            catch (:field) do
+              fields = []
+              template.option_fields.each do |field|
+                entry = netflow_field_for(field.field_type, field.field_length)
+                if ! entry
+                  throw :field
+                end
+                fields += entry
+              end
+              # We get this far, we have a list of fields
+              #key = "#{flowset.source_id}|#{event["source"]}|#{template.template_id}"
+              key = "#{flowset.source_id}|#{template.template_id}"
+              @templates[key, @cache_ttl] = BinData::Struct.new(:endian => :big, :fields => fields)
+              # Purge any expired templates
+              @templates.cleanup!
+            end
+          end
+        when 256..65535
+          # Data flowset
+          #key = "#{flowset.source_id}|#{event["source"]}|#{record.flowset_id}"
+          key = "#{flowset.source_id}|#{record.flowset_id}"
+          template = @templates[key]
+
+          if ! template
+            #@logger.warn("No matching template for flow id #{record.flowset_id} from #{event["source"]}")
+            @logger.warn("No matching template for flow id #{record.flowset_id}")
+            next
+          end
+
+          length = record.flowset_length - 4
+
+          # Template shouldn't be longer than the record and there should
+          # be at most 3 padding bytes
+          if template.num_bytes > length or ! (length % template.num_bytes).between?(0, 3)
+            @logger.warn("Template length doesn't fit cleanly into flowset", :template_id => record.flowset_id, :template_length => template.num_bytes, :record_length => length)
+            next
+          end
+
+          array = BinData::Array.new(:type => template, :initial_length => length / template.num_bytes)
+
+          records = array.read(record.flowset_data)
+
+          records.each do |r|
+            event = LogStash::Event.new(
+              LogStash::Event::TIMESTAMP => LogStash::Timestamp.at(flowset.unix_sec),
+              @target => {}
+            )
+
+            # Fewer fields in the v9 header
+            ['version', 'flow_seq_num'].each do |f|
+              event[@target][f] = flowset[f]
+            end
+
+            event[@target]['flowset_id'] = record.flowset_id
+
+            r.each_pair do |k,v|
+              case k.to_s
+              when /_switched$/
+                millis = flowset.uptime - v
+                seconds = flowset.unix_sec - (millis / 1000)
+                # v9 did away with the nanosecs field
+                micros = 1000000 - (millis % 1000)
+                event[@target][k.to_s] = Time.at(seconds, micros).utc.strftime("%Y-%m-%dT%H:%M:%S.%3NZ")
+              else
+                event[@target][k.to_s] = v
+              end
+            end
+
+            yield event
+          end
+        else
+          @logger.warn("Unsupported flowset id #{record.flowset_id}")
+        end
+      end
+    end
+  end # def filter
+
+  private
+  def uint_field(length, default)
+    # If length is 4, return :uint32, etc. and use default if length is 0
+    ("uint" + (((length > 0) ? length : default) * 8).to_s).to_sym
+  end # def uint_field
+
+  private
+  def netflow_field_for(type, length)
+    if @fields.include?(type)
+      field = @fields[type]
+      if field.is_a?(Array)
+
+        if field[0].is_a?(Integer)
+          field[0] = uint_field(length, field[0])
+        end
+
+        # Small bit of fixup for skip or string field types where the length
+        # is dynamic
+        case field[0]
+        when :skip
+          field += [nil, {:length => length}]
+        when :string
+          field += [{:length => length, :trim_padding => true}]
+        end
+
+        @logger.debug("Definition complete", :field => field)
+        [field]
+      else
+        @logger.warn("Definition should be an array", :field => field)
+        nil
+      end
+    else
+      @logger.warn("Unsupported field", :type => type, :length => length)
+      nil
+    end
+  end # def netflow_field_for
+end # class LogStash::Filters::Netflow
diff --git a/lib/logstash/codecs/netflow/netflow.yaml b/lib/logstash/codecs/netflow/netflow.yaml
new file mode 100644
index 00000000000..9f823dcf449
--- /dev/null
+++ b/lib/logstash/codecs/netflow/netflow.yaml
@@ -0,0 +1,215 @@
+---
+1:
+- 4
+- :in_bytes
+2:
+- 4
+- :in_pkts
+3:
+- 4
+- :flows
+4:
+- :uint8
+- :protocol
+5:
+- :uint8
+- :src_tos
+6:
+- :uint8
+- :tcp_flags
+7:
+- :uint16
+- :l4_src_port
+8:
+- :ip4_addr
+- :ipv4_src_addr
+9:
+- :uint8
+- :src_mask
+10:
+- 2
+- :input_snmp
+11:
+- :uint16
+- :l4_dst_port
+12:
+- :ip4_addr
+- :ipv4_dst_addr
+13:
+- :uint8
+- :dst_mask
+14:
+- 2
+- :output_snmp
+15:
+- :ip4_addr
+- :ipv4_next_hop
+16:
+- 2
+- :src_as
+17:
+- 2
+- :dst_as
+18:
+- :ip4_addr
+- :bgp_ipv4_next_hop
+19:
+- 4
+- :mul_dst_pkts
+20:
+- 4
+- :mul_dst_bytes
+21:
+- :uint32
+- :last_switched
+22:
+- :uint32
+- :first_switched
+23:
+- 4
+- :out_bytes
+24:
+- 4
+- :out_pkts
+25:
+- :uint16
+- :min_pkt_length
+26:
+- :uint16
+- :max_pkt_length
+27:
+- :ip6_addr
+- :ipv6_src_addr
+28:
+- :ip6_addr
+- :ipv6_dst_addr
+29:
+- :uint8
+- :ipv6_src_mask
+30:
+- :uint8
+- :ipv6_dst_mask
+31:
+- :uint24
+- :ipv6_flow_label
+32:
+- :uint16
+- :icmp_type
+33:
+- :uint8
+- :mul_igmp_type
+34:
+- :uint32
+- :sampling_interval
+35:
+- :uint8
+- :sampling_algorithm
+36:
+- :uint16
+- :flow_active_timeout
+37:
+- :uint16
+- :flow_inactive_timeout
+38:
+- :uint8
+- :engine_type
+39:
+- :uint8
+- :engine_id
+40:
+- 4
+- :total_bytes_exp
+41:
+- 4
+- :total_pkts_exp
+42:
+- 4
+- :total_flows_exp
+43:
+- :skip
+44:
+- :ip4_addr
+- :ipv4_src_prefix
+45:
+- :ip4_addr
+- :ipv4_dst_prefix
+46:
+- :uint8
+- :mpls_top_label_type
+47:
+- :uint32
+- :mpls_top_label_ip_addr
+48:
+- 4
+- :flow_sampler_id
+49:
+- :uint8
+- :flow_sampler_mode
+50:
+- :uint32
+- :flow_sampler_random_interval
+51:
+- :skip
+52:
+- :uint8
+- :min_ttl
+53:
+- :uint8
+- :max_ttl
+54:
+- :uint16
+- :ipv4_ident
+55:
+- :uint8
+- :dst_tos
+56:
+- :mac_addr
+- :in_src_max
+57:
+- :mac_addr
+- :out_dst_max
+58:
+- :uint16
+- :src_vlan
+59:
+- :uint16
+- :dst_vlan
+60:
+- :uint8
+- :ip_protocol_version
+61:
+- :uint8
+- :direction
+62:
+- :ip6_addr
+- :ipv6_next_hop
+63:
+- :ip6_addr
+- :bgp_ipv6_next_hop
+64:
+- :uint32
+- :ipv6_option_headers
+64:
+- :skip
+65:
+- :skip
+66:
+- :skip
+67:
+- :skip
+68:
+- :skip
+69:
+- :skip
+80:
+- :mac_addr
+- :in_dst_mac
+81:
+- :mac_addr
+- :out_src_mac
+82:
+- :string
+- :if_name
+83:
+- :string
+- :if_desc
diff --git a/lib/logstash/codecs/netflow/util.rb b/lib/logstash/codecs/netflow/util.rb
new file mode 100644
index 00000000000..627fb1265f0
--- /dev/null
+++ b/lib/logstash/codecs/netflow/util.rb
@@ -0,0 +1,212 @@
+# encoding: utf-8
+require "bindata"
+require "ipaddr"
+
+class IP4Addr < BinData::Primitive
+  endian :big
+  uint32 :storage
+
+  def set(val)
+    ip = IPAddr.new(val)
+    if ! ip.ipv4?
+      raise ArgumentError, "invalid IPv4 address '#{val}'"
+    end
+    self.storage = ip.to_i
+  end
+
+  def get
+    IPAddr.new_ntoh([self.storage].pack('N')).to_s
+  end
+end
+
+class IP6Addr < BinData::Primitive
+  endian  :big
+  uint128 :storage
+
+  def set(val)
+    ip = IPAddr.new(val)
+    if ! ip.ipv6?
+      raise ArgumentError, "invalid IPv6 address `#{val}'"
+    end
+    self.storage = ip.to_i
+  end
+
+  def get
+    IPAddr.new_ntoh((0..7).map { |i|
+      (self.storage >> (112 - 16 * i)) & 0xffff
+    }.pack('n8')).to_s
+  end
+end
+
+class MacAddr < BinData::Primitive
+  array :bytes, :type => :uint8, :initial_length => 6
+
+  def set(val)
+    ints = val.split(/:/).collect { |int| int.to_i(16) }
+    self.bytes = ints
+  end
+
+  def get
+    self.bytes.collect { |byte| byte.to_s(16) }.join(":")
+  end
+end
+
+class Header < BinData::Record
+  endian :big
+  uint16 :version
+end
+
+class Netflow5PDU < BinData::Record
+  endian :big
+  uint16 :version
+  uint16 :flow_records
+  uint32 :uptime
+  uint32 :unix_sec
+  uint32 :unix_nsec
+  uint32 :flow_seq_num
+  uint8  :engine_type
+  uint8  :engine_id
+  bit2   :sampling_algorithm
+  bit14  :sampling_interval
+  array  :records, :initial_length => :flow_records do
+    ip4_addr :ipv4_src_addr
+    ip4_addr :ipv4_dst_addr
+    ip4_addr :ipv4_next_hop
+    uint16   :input_snmp
+    uint16   :output_snmp
+    uint32   :in_pkts
+    uint32   :in_bytes
+    uint32   :first_switched
+    uint32   :last_switched
+    uint16   :l4_src_port
+    uint16   :l4_dst_port
+    skip     :length => 1
+    uint8    :tcp_flags # Split up the TCP flags maybe?
+    uint8    :protocol
+    uint8    :src_tos
+    uint16   :src_as
+    uint16   :dst_as
+    uint8    :src_mask
+    uint8    :dst_mask
+    skip     :length => 2
+  end
+end
+
+class TemplateFlowset < BinData::Record
+  endian :big
+  array  :templates, :read_until => lambda { array.num_bytes == flowset_length - 4 } do
+    uint16 :template_id
+    uint16 :field_count
+    array  :fields, :initial_length => :field_count do
+      uint16 :field_type
+      uint16 :field_length
+    end
+  end
+end
+
+class OptionFlowset < BinData::Record
+  endian :big
+  array  :templates, :read_until => lambda { flowset_length - 4 - array.num_bytes <= 2 } do
+    uint16 :template_id
+    uint16 :scope_length
+    uint16 :option_length
+    array  :scope_fields, :initial_length => lambda { scope_length / 4 } do
+      uint16 :field_type
+      uint16 :field_length
+    end
+    array  :option_fields, :initial_length => lambda { option_length / 4 } do
+      uint16 :field_type
+      uint16 :field_length
+    end
+  end
+  skip   :length => lambda { templates.length.odd? ? 2 : 0 }
+end
+
+class Netflow9PDU < BinData::Record
+  endian :big
+  uint16 :version
+  uint16 :flow_records
+  uint32 :uptime
+  uint32 :unix_sec
+  uint32 :flow_seq_num
+  uint32 :source_id
+  array  :records, :read_until => :eof do
+    uint16 :flowset_id
+    uint16 :flowset_length
+    choice :flowset_data, :selection => :flowset_id do
+      template_flowset 0
+      option_flowset   1
+      string           :default, :read_length => lambda { flowset_length - 4 }
+    end
+  end
+end
+
+# https://gist.github.com/joshaven/184837
+class Vash < Hash
+  def initialize(constructor = {})
+    @register ||= {}
+    if constructor.is_a?(Hash)
+      super()
+      merge(constructor)
+    else
+      super(constructor)
+    end
+  end
+
+  alias_method :regular_writer, :[]= unless method_defined?(:regular_writer)
+  alias_method :regular_reader, :[] unless method_defined?(:regular_reader)
+
+  def [](key)
+    sterilize(key)
+    clear(key) if expired?(key)
+    regular_reader(key)
+  end
+
+  def []=(key, *args)
+    if args.length == 2
+      value, ttl = args[1], args[0]
+    elsif args.length == 1
+      value, ttl = args[0], 60
+    else
+      raise ArgumentError, "Wrong number of arguments, expected 2 or 3, received: #{args.length+1}\n"+
+                           "Example Usage:  volatile_hash[:key]=value OR volatile_hash[:key, ttl]=value"
+    end
+    sterilize(key)
+    ttl(key, ttl)
+    regular_writer(key, value)
+  end
+
+  def merge(hsh)
+    hsh.map {|key,value| self[sterile(key)] = hsh[key]}
+    self
+  end
+
+  def cleanup!
+    now = Time.now.to_i
+    @register.map {|k,v| clear(k) if v < now}
+  end
+
+  def clear(key)
+    sterilize(key)
+    @register.delete key
+    self.delete key
+  end
+
+  private
+  def expired?(key)
+    Time.now.to_i > @register[key].to_i
+  end
+
+  def ttl(key, secs=60)
+    @register[key] = Time.now.to_i + secs.to_i
+  end
+
+  def sterile(key)
+    String === key ? key.chomp('!').chomp('=') : key.to_s.chomp('!').chomp('=').to_sym
+  end
+
+  def sterilize(key)
+    key = sterile(key)
+  end
+end
+
diff --git a/lib/logstash/codecs/noop.rb b/lib/logstash/codecs/noop.rb
new file mode 100644
index 00000000000..ed74a685f2d
--- /dev/null
+++ b/lib/logstash/codecs/noop.rb
@@ -0,0 +1,19 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+
+class LogStash::Codecs::Noop < LogStash::Codecs::Base
+  config_name "noop"
+
+  milestone 1
+
+  public
+  def decode(data)
+    yield data
+  end # def decode
+
+  public
+  def encode(event)
+    @on_event.call event
+  end # def encode
+
+end # class LogStash::Codecs::Noop
diff --git a/lib/logstash/codecs/oldlogstashjson.rb b/lib/logstash/codecs/oldlogstashjson.rb
new file mode 100644
index 00000000000..0cd5ce74973
--- /dev/null
+++ b/lib/logstash/codecs/oldlogstashjson.rb
@@ -0,0 +1,57 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/json"
+
+class LogStash::Codecs::OldLogStashJSON < LogStash::Codecs::Base
+  config_name "oldlogstashjson"
+  milestone 2
+
+  # Map from v0 name to v1 name.
+  # Note: @source is gone and has no similar field.
+  V0_TO_V1 = {"@timestamp" => "@timestamp", "@message" => "message",
+              "@tags" => "tags", "@type" => "type",
+              "@source_host" => "host", "@source_path" => "path"}
+
+  public
+  def decode(data)
+    begin
+      obj = LogStash::Json.load(data.force_encoding(Encoding::UTF_8))
+    rescue LogStash::Json::ParserError => e
+      @logger.info("JSON parse failure. Falling back to plain-text", :error => e, :data => data)
+      yield LogStash::Event.new("message" => data)
+      return
+    end
+
+    h  = {}
+
+    # Convert the old logstash schema to the new one.
+    V0_TO_V1.each do |key, val|
+      h[val] = obj[key] if obj.include?(key)
+    end
+
+    h.merge!(obj["@fields"]) if obj["@fields"].is_a?(Hash)
+    yield LogStash::Event.new(h)
+  end # def decode
+
+  public
+  def encode(event)
+    h  = {}
+
+    # Convert the new logstash schema to the old one.
+    V0_TO_V1.each do |key, val|
+      h[key] = event[val] if event.include?(val)
+    end
+
+    event.to_hash.each do |field, val|
+      # TODO: might be better to V1_TO_V0 = V0_TO_V1.invert during
+      # initialization than V0_TO_V1.has_value? within loop
+      next if field == "@version" or V0_TO_V1.has_value?(field)
+      h["@fields"] ||= {}
+      h["@fields"][field] = val
+    end
+
+    # Tack on a \n because JSON outputs 1.1.x had them.
+    @on_event.call(LogStash::Json.dump(h) + NL)
+  end # def encode
+
+end # class LogStash::Codecs::OldLogStashJSON
diff --git a/lib/logstash/codecs/plain.rb b/lib/logstash/codecs/plain.rb
new file mode 100644
index 00000000000..5fe86aacd28
--- /dev/null
+++ b/lib/logstash/codecs/plain.rb
@@ -0,0 +1,48 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/util/charset"
+
+# The "plain" codec is for plain text with no delimiting between events.
+#
+# This is mainly useful on inputs and outputs that already have a defined
+# framing in their transport protocol (such as zeromq, rabbitmq, redis, etc)
+class LogStash::Codecs::Plain < LogStash::Codecs::Base
+  config_name "plain"
+  milestone 3
+
+  # Set the message you which to emit for each event. This supports sprintf
+  # strings.
+  #
+  # This setting only affects outputs (encoding of events).
+  config :format, :validate => :string
+
+  # The character encoding used in this input. Examples include "UTF-8"
+  # and "cp1252"
+  #
+  # This setting is useful if your log files are in Latin-1 (aka cp1252)
+  # or in another character set other than UTF-8.
+  #
+  # This only affects "plain" format logs since json is UTF-8 already.
+  config :charset, :validate => ::Encoding.name_list, :default => "UTF-8"
+
+  public
+  def register
+    @converter = LogStash::Util::Charset.new(@charset)
+    @converter.logger = @logger
+  end
+
+  public
+  def decode(data)
+    yield LogStash::Event.new("message" => @converter.convert(data))
+  end # def decode
+
+  public
+  def encode(event)
+    if event.is_a?(LogStash::Event) and @format
+      @on_event.call(event.sprintf(@format))
+    else
+      @on_event.call(event.to_s)
+    end
+  end # def encode
+
+end # class LogStash::Codecs::Plain
diff --git a/lib/logstash/codecs/rubydebug.rb b/lib/logstash/codecs/rubydebug.rb
new file mode 100644
index 00000000000..415344045c2
--- /dev/null
+++ b/lib/logstash/codecs/rubydebug.rb
@@ -0,0 +1,41 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+
+# The rubydebug codec will output your Logstash event data using
+# the Ruby Awesome Print library.
+#
+class LogStash::Codecs::RubyDebug < LogStash::Codecs::Base
+  config_name "rubydebug"
+  milestone 3
+
+  # Should the event's metadata be included?
+  config :metadata, :validate => :boolean, :default => false
+
+  def register
+    require "ap"
+    if @metadata
+      @encoder = method(:encode_with_metadata)
+    else
+      @encoder = method(:encode_default)
+    end
+  end
+
+  public
+  def decode(data)
+    raise "Not implemented"
+  end # def decode
+
+  public
+  def encode(event)
+    @encoder.call(event)
+  end
+
+  def encode_default(event)
+    @on_event.call(event.to_hash.awesome_inspect + NL)
+  end # def encode_default
+
+  def encode_with_metadata(event)
+    @on_event.call(event.to_hash_with_metadata.awesome_inspect + NL)
+  end # def encode_with_metadata
+
+end # class LogStash::Codecs::Dots
diff --git a/lib/logstash/codecs/spool.rb b/lib/logstash/codecs/spool.rb
new file mode 100644
index 00000000000..04fdd05bde2
--- /dev/null
+++ b/lib/logstash/codecs/spool.rb
@@ -0,0 +1,38 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+
+class LogStash::Codecs::Spool < LogStash::Codecs::Base
+  config_name 'spool'
+  milestone 1
+  config :spool_size, :validate => :number, :default => 50
+
+  attr_reader :buffer
+
+  public
+  def decode(data)
+    data.each do |event|
+      yield event
+    end
+  end # def decode
+
+  public
+  def encode(event)
+    @buffer ||= []
+    #buffer size is hard coded for now until a
+    #better way to pass args into codecs is implemented
+    if @buffer.length >= @spool_size
+      @on_event.call @buffer
+      @buffer = []
+    else
+      @buffer << event
+    end
+  end # def encode
+
+  public
+  def teardown
+    if !@buffer.nil? and @buffer.length > 0
+      @on_event.call @buffer
+    end
+    @buffer = []
+  end
+end # class LogStash::Codecs::Spool
diff --git a/lib/logstash/config/Makefile b/lib/logstash/config/Makefile
index 623557161a1..ffb1755fe0b 100644
--- a/lib/logstash/config/Makefile
+++ b/lib/logstash/config/Makefile
@@ -1,3 +1,4 @@
 
-grammar.rb: grammar.rl
-	ragel -R grammar.rl
+#ragel -R grammar.rl
+grammar.rb: grammar.treetop
+	tt grammar.treetop
diff --git a/lib/logstash/config/config_ast.rb b/lib/logstash/config/config_ast.rb
new file mode 100644
index 00000000000..3a31c04a3fb
--- /dev/null
+++ b/lib/logstash/config/config_ast.rb
@@ -0,0 +1,475 @@
+# encoding: utf-8
+require "treetop"
+class Treetop::Runtime::SyntaxNode
+  def compile
+    return "" if elements.nil?
+    return elements.collect(&:compile).reject(&:empty?).join("")
+  end
+
+  # Traverse the syntax tree recursively.
+  # The order should respect the order of the configuration file as it is read
+  # and written by humans (and the order in which it is parsed).
+  def recurse(e, depth=0, &block)
+    r = block.call(e, depth)
+    e.elements.each { |e| recurse(e, depth + 1, &block) } if r && e.elements
+    nil
+  end
+
+  def recursive_inject(results=[], &block)
+    if !elements.nil?
+      elements.each do |element|
+        if block.call(element)
+          results << element
+        else
+          element.recursive_inject(results, &block)
+        end
+      end
+    end
+    return results
+  end
+
+  def recursive_select(klass)
+    return recursive_inject { |e| e.is_a?(klass) }
+  end
+
+  def recursive_inject_parent(results=[], &block)
+    if !parent.nil?
+      if block.call(parent)
+        results << parent
+      else
+        parent.recursive_inject_parent(results, &block)
+      end
+    end
+    return results
+  end
+
+  def recursive_select_parent(results=[], klass)
+    return recursive_inject_parent(results) { |e| e.is_a?(klass) }
+  end
+end
+
+module LogStash; module Config; module AST
+  class Node < Treetop::Runtime::SyntaxNode; end
+  class Config < Node
+    def compile
+      code = []
+
+      code << <<-CODE
+        @inputs = []
+        @filters = []
+        @outputs = []
+        @periodic_flushers = []
+        @shutdown_flushers = []
+      CODE
+
+      sections = recursive_select(LogStash::Config::AST::PluginSection)
+      sections.each do |s|
+        code << s.compile_initializer
+      end
+
+      # start inputs
+      definitions = []
+
+      ["filter", "output"].each do |type|
+        # defines @filter_func and @output_func
+
+        definitions << "@#{type}_func = lambda do |event, &block|"
+        definitions << "  events = [event]"
+        definitions << "  @logger.debug? && @logger.debug(\"#{type} received\", :event => event.to_hash)"
+        sections.select { |s| s.plugin_type.text_value == type }.each do |s|
+          definitions << s.compile.split("\n", -1).map { |e| "  #{e}" }
+        end
+
+        if type == "filter"
+          definitions << "  events.flatten.each{|e| block.call(e) }"
+        end
+        definitions << "end"
+      end
+
+      code += definitions.join("\n").split("\n", -1).collect { |l| "  #{l}" }
+      return code.join("\n")
+    end
+  end
+
+  class Comment < Node; end
+  class Whitespace < Node; end
+  class PluginSection < Node
+    # Global plugin numbering for the janky instance variable naming we use
+    # like @filter_<name>_1
+    @@i = 0
+
+    # Generate ruby code to initialize all the plugins.
+    def compile_initializer
+      generate_variables
+      code = []
+      @variables.each do |plugin, name|
+
+
+        code << <<-CODE
+          #{name} = #{plugin.compile_initializer}
+          @#{plugin.plugin_type}s << #{name}
+        CODE
+
+        # The flush method for this filter.
+        if plugin.plugin_type == "filter"
+
+          code << <<-CODE
+            #{name}_flush = lambda do |options, &block|
+              @logger.debug? && @logger.debug(\"Flushing\", :plugin => #{name})
+
+              flushed_events = #{name}.flush(options)
+
+              return if flushed_events.nil? || flushed_events.empty?
+
+              flushed_events.each do |event|
+                @logger.debug? && @logger.debug(\"Flushing\", :plugin => #{name}, :event => event)
+
+                events = [event]
+                #{plugin.compile_starting_here.gsub(/^/, "  ")}
+
+                block.call(event)
+                events.flatten.each{|e| block.call(e) if e != event}
+              end
+
+            end
+
+            if #{name}.respond_to?(:flush)
+              @periodic_flushers << #{name}_flush if #{name}.periodic_flush
+              @shutdown_flushers << #{name}_flush
+            end
+          CODE
+
+        end
+      end
+      return code.join("\n")
+    end
+
+    def variable(object)
+      generate_variables
+      return @variables[object]
+    end
+
+    def generate_variables
+      return if !@variables.nil?
+      @variables = {}
+      plugins = recursive_select(Plugin)
+
+      plugins.each do |plugin|
+        # Unique number for every plugin.
+        @@i += 1
+        # store things as ivars, like @filter_grok_3
+        var = "@#{plugin.plugin_type}_#{plugin.plugin_name}_#{@@i}"
+        @variables[plugin] = var
+      end
+      return @variables
+    end
+
+  end
+
+  class Plugins < Node; end
+  class Plugin < Node
+    def plugin_type
+      if recursive_select_parent(Plugin).any?
+        return "codec"
+      else
+        return recursive_select_parent(PluginSection).first.plugin_type.text_value
+      end
+    end
+
+    def plugin_name
+      return name.text_value
+    end
+
+    def variable_name
+      return recursive_select_parent(PluginSection).first.variable(self)
+    end
+
+    def compile_initializer
+      # If any parent is a Plugin, this must be a codec.
+
+      if attributes.elements.nil?
+        return "plugin(#{plugin_type.inspect}, #{plugin_name.inspect})" << (plugin_type == "codec" ? "" : "\n")
+      else
+        settings = attributes.recursive_select(Attribute).collect(&:compile).reject(&:empty?)
+
+        attributes_code = "LogStash::Util.hash_merge_many(#{settings.map { |c| "{ #{c} }" }.join(", ")})"
+        return "plugin(#{plugin_type.inspect}, #{plugin_name.inspect}, #{attributes_code})" << (plugin_type == "codec" ? "" : "\n")
+      end
+    end
+
+    def compile
+      case plugin_type
+      when "input"
+        return "start_input(#{variable_name})"
+      when "filter"
+        return <<-CODE
+          events = events.flat_map do |event|
+            next [] if event.cancelled?
+
+            new_events = []
+            #{variable_name}.filter(event){|new_event| new_events << new_event}
+            event.cancelled? ? new_events : new_events.unshift(event)
+          end
+        CODE
+      when "output"
+        return "#{variable_name}.handle(event)\n"
+      when "codec"
+        settings = attributes.recursive_select(Attribute).collect(&:compile).reject(&:empty?)
+        attributes_code = "LogStash::Util.hash_merge_many(#{settings.map { |c| "{ #{c} }" }.join(", ")})"
+        return "plugin(#{plugin_type.inspect}, #{plugin_name.inspect}, #{attributes_code})"
+      end
+    end
+
+    def compile_starting_here
+      return unless plugin_type == "filter" # only filter supported.
+
+      expressions = [
+        LogStash::Config::AST::Branch,
+        LogStash::Config::AST::Plugin
+      ]
+      code = []
+
+      # Find the branch we are in, if any (the 'if' statement, etc)
+      self_branch = recursive_select_parent(LogStash::Config::AST::BranchEntry).first
+
+      # Find any siblings to our branch so we can skip them later.  For example,
+      # if we are in an 'else if' we want to skip any sibling 'else if' or
+      # 'else' blocks.
+      branch_siblings = []
+      if self_branch
+        branch_siblings = recursive_select_parent(LogStash::Config::AST::Branch).first \
+          .recursive_select(LogStash::Config::AST::BranchEntry) \
+          .reject { |b| b == self_branch }
+      end
+
+      #ast = recursive_select_parent(LogStash::Config::AST::PluginSection).first
+      ast = recursive_select_parent(LogStash::Config::AST::Config).first
+
+      found = false
+      recurse(ast) do |element, depth|
+        next false if element.is_a?(LogStash::Config::AST::PluginSection) && element.plugin_type.text_value != "filter"
+        if element == self
+          found = true
+          next false
+        end
+        if found && expressions.include?(element.class)
+          code << element.compile
+          next false
+        end
+        next false if branch_siblings.include?(element)
+        next true
+      end
+
+      return code.collect { |l| "#{l}\n" }.join("")
+    end # def compile_starting_here
+  end
+
+  class Name < Node
+    def compile
+      return text_value.inspect
+    end
+  end
+  class Attribute < Node
+    def compile
+      return %Q(#{name.compile} => #{value.compile})
+    end
+  end
+  class RValue < Node; end
+  class Value < RValue; end
+
+  module Unicode
+    def self.wrap(text)
+      return "(" + text.inspect + ".force_encoding(Encoding::UTF_8)" + ")"
+    end
+  end
+
+  class Bareword < Value
+    def compile
+      return Unicode.wrap(text_value)
+    end
+  end
+  class String < Value
+    def compile
+      return Unicode.wrap(text_value[1...-1])
+    end
+  end
+  class RegExp < Value
+    def compile
+      return "Regexp.new(" + Unicode.wrap(text_value[1...-1]) + ")"
+    end
+  end
+  class Number < Value
+    def compile
+      return text_value
+    end
+  end
+  class Array < Value
+    def compile
+      return "[" << recursive_select(Value).collect(&:compile).reject(&:empty?).join(", ") << "]"
+    end
+  end
+  class Hash < Value
+    def compile
+      return "{" << recursive_select(HashEntry).collect(&:compile).reject(&:empty?).join(", ") << "}"
+    end
+  end
+  class HashEntries < Node; end
+  class HashEntry < Node
+    def compile
+      return %Q(#{name.compile} => #{value.compile})
+    end
+  end
+
+  class BranchOrPlugin < Node; end
+
+  class Branch < Node
+    def compile
+
+      # this construct is non obvious. we need to loop through each event and apply the conditional.
+      # each branch of a conditional will contain a construct (a filter for example) that also loops through
+      # the events variable so we have to initialize it to [event] for the branch code.
+      # at the end, events is returned to handle the case where no branch match and no branch code is executed
+      # so we must make sure to return the current event.
+
+      return <<-CODE
+        events = events.flat_map do |event|
+          events = [event]
+          #{super}
+          end
+          events
+        end
+      CODE
+    end
+  end
+
+  class BranchEntry < Node; end
+  class If < BranchEntry
+    def compile
+      children = recursive_inject { |e| e.is_a?(Branch) || e.is_a?(Plugin) }
+      return "if #{condition.compile} # if #{condition.text_value}\n" \
+        << children.collect(&:compile).map { |s| s.split("\n", -1).map { |l| "  " + l }.join("\n") }.join("") << "\n"
+    end
+  end
+  class Elsif < BranchEntry
+    def compile
+      children = recursive_inject { |e| e.is_a?(Branch) || e.is_a?(Plugin) }
+      return "elsif #{condition.compile} # else if #{condition.text_value}\n" \
+        << children.collect(&:compile).map { |s| s.split("\n", -1).map { |l| "  " + l }.join("\n") }.join("") << "\n"
+    end
+  end
+  class Else < BranchEntry
+    def compile
+      children = recursive_inject { |e| e.is_a?(Branch) || e.is_a?(Plugin) }
+      return "else\n" \
+        << children.collect(&:compile).map { |s| s.split("\n", -1).map { |l| "  " + l }.join("\n") }.join("") << "\n"
+    end
+  end
+
+  class Condition < Node
+    def compile
+      return "(#{super})"
+    end
+  end
+
+  module Expression
+    def compile
+      return "(#{super})"
+    end
+  end
+
+  module NegativeExpression
+    def compile
+      return "!(#{super})"
+    end
+  end
+
+  module ComparisonExpression; end
+
+  module InExpression
+    def compile
+      item, list = recursive_select(LogStash::Config::AST::RValue)
+      return "(x = #{list.compile}; x.respond_to?(:include?) && x.include?(#{item.compile}))"
+    end
+  end
+
+  module NotInExpression
+    def compile
+      item, list = recursive_select(LogStash::Config::AST::RValue)
+      return "(x = #{list.compile}; !x.respond_to?(:include?) || !x.include?(#{item.compile}))"
+    end
+  end
+
+  class MethodCall < Node
+    def compile
+      arguments = recursive_inject { |e| [String, Number, Selector, Array, MethodCall].any? { |c| e.is_a?(c) } }
+      return "#{method.text_value}(" << arguments.collect(&:compile).join(", ") << ")"
+    end
+  end
+
+  class RegexpExpression < Node
+    def compile
+      operator = recursive_select(LogStash::Config::AST::RegExpOperator).first.text_value
+      item, regexp = recursive_select(LogStash::Config::AST::RValue)
+      # Compile strings to regexp's
+      if regexp.is_a?(LogStash::Config::AST::String)
+        regexp = "/#{regexp.text_value[1..-2]}/"
+      else
+        regexp = regexp.compile
+      end
+      return "(#{item.compile} #{operator} #{regexp})"
+    end
+  end
+
+  module ComparisonOperator
+    def compile
+      return " #{text_value} "
+    end
+  end
+  module RegExpOperator
+    def compile
+      return " #{text_value} "
+    end
+  end
+  module BooleanOperator
+    def compile
+      return " #{text_value} "
+    end
+  end
+  class Selector < RValue
+    def compile
+      return "event[#{text_value.inspect}]"
+    end
+  end
+  class SelectorElement < Node; end
+end; end; end
+
+
+# Monkeypatch Treetop::Runtime::SyntaxNode's inspect method to skip
+# any Whitespace or SyntaxNodes with no children.
+class Treetop::Runtime::SyntaxNode
+  def _inspect(indent="")
+    em = extension_modules
+    interesting_methods = methods-[em.last ? em.last.methods : nil]-self.class.instance_methods
+    im = interesting_methods.size > 0 ? " (#{interesting_methods.join(",")})" : ""
+    tv = text_value
+    tv = "...#{tv[-20..-1]}" if tv.size > 20
+
+    indent +
+    self.class.to_s.sub(/.*:/,'') +
+      em.map{|m| "+"+m.to_s.sub(/.*:/,'')}*"" +
+      " offset=#{interval.first}" +
+      ", #{tv.inspect}" +
+      im +
+      (elements && elements.size > 0 ?
+        ":" +
+          (elements.select { |e| !e.is_a?(LogStash::Config::AST::Whitespace) && e.elements && e.elements.size > 0 }||[]).map{|e|
+      begin
+        "\n"+e.inspect(indent+"  ")
+      rescue  # Defend against inspect not taking a parameter
+        "\n"+indent+" "+e.inspect
+      end
+          }.join("") :
+        ""
+      )
+  end
+end
diff --git a/lib/logstash/config/file.rb b/lib/logstash/config/file.rb
index 8c90abf0b23..fb0d939dfde 100644
--- a/lib/logstash/config/file.rb
+++ b/lib/logstash/config/file.rb
@@ -1,95 +1,39 @@
+# encoding: utf-8
 require "logstash/namespace"
 require "logstash/config/grammar"
+require "logstash/config/config_ast"
 require "logstash/config/registry"
-require "logstash/agent"
+require "logstash/errors"
+require "logger"
 
 class LogStash::Config::File
-  public
-  def initialize(path=nil, string=nil)
-    @path = path
-    @string = string
-
-    if (path.nil? and string.nil?) or (!path.nil? and !string.nil?)
-       raise "Must give path or string, not both or neither"
-    end
-  end # def initialize
+  include Enumerable
+  attr_accessor :logger
 
   public
-  def parse
-    grammar = LogStash::Config::Grammar.new
-
-    if @string.nil?
-      grammar.parse(File.new(@path).read)
-    else
-      grammar.parse(@string)
-    end
-
-    @config = grammar.config
-    
-    registry = LogStash::Config::Registry::registry
-    self.each do |o|
-      # Load the base class for the type given (like inputs/base, or filters/base)
-      # TODO(sissel): Error handling
-      tryload o[:type], :base
-      type = registry[o[:type]]
-
-      # Load the plugin itself (inputs/file, outputs/amqp, etc)
-      # TODO(sissel): Error handling
-      tryload o[:type], o[:plugin]
-      plugin = registry[o[:plugin]]
-
-      if type.nil?
-        puts "Unknown config #{o[:type]}/#{o[:plugin]}"
-      end
+  def initialize(text)
+    @logger = Cabin::Channel.get(LogStash)
+    @text = text
+    @config = parse(text)
+  end # def initialize
 
-      yield :type => type, :plugin => plugin, :parameters => o[:parameters]
+  def parse(text)
+    grammar = LogStashConfigParser.new
+    result = grammar.parse(text)
+    if result.nil?
+      raise LogStash::ConfigurationError, grammar.failure_reason
     end
+    return result
   end # def parse
 
-  public
-  def tryload(parent, child)
-    child = child.downcase if child.is_a? String
-    begin
-      loaded = (require "logstash/#{parent}s/#{child}")
-      #if loaded
-        #puts "Loading logstash/#{parent}s/#{child}"
-      #end
-    rescue => e
-      if child == :base
-        $stderr.puts "Failure loading base class '#{parent}': #{e.inspect}"
-      else
-        $stderr.puts "Failure loading plugin #{parent}s/#{child}: #{e.inspect}"
-      end
-      raise e
-    end
-  end # def tryload
-
-  public
-  def each(&block)
-    #ap @config
-
-    # First level is the components
-    # Like:
-    #   input {
-    #     ...
-    #   }
-    @config.each do |type, plugin_config_array|
-      # plugin_config_array has arrays of each component config:
-      # input {
-      #   amqp { ... }
-      #   file { ... }
-      #   file { ... }
-      # }
-      plugin_config_array.each do |plugin_config|
-        yield({
-          :type => type,
-          :plugin => plugin_config.keys.first,
-          :parameters => plugin_config.values.first
-        })
-      end
-    end # @config.each
-  end # def each
+  def plugin(plugin_type, name, *args)
+    klass = LogStash::Plugin.lookup(plugin_type, name)
+    return klass.new(*args)
+  end
 
+  def each
+    @config.recursive_select(LogStash::Config::AST::Plugin)
+  end
 end #  class LogStash::Config::Parser
 
 #agent.config(cfg)
diff --git a/lib/logstash/config/grammar.rb b/lib/logstash/config/grammar.rb
index ebd22dba577..1852c851a76 100644
--- a/lib/logstash/config/grammar.rb
+++ b/lib/logstash/config/grammar.rb
@@ -1,568 +1,3504 @@
+# encoding: utf-8
+# Autogenerated from a Treetop grammar. Edits may be lost.
 
-# line 1 "grammar.rl"
-require "logstash/namespace"
 
+require "treetop"
+require "logstash/config/config_ast"
 
-# line 145 "grammar.rl"
+module LogStashConfig
+  include Treetop::Runtime
 
+  def root
+    @root ||= :config
+  end
+
+  module Config0
+    def _
+      elements[0]
+    end
+
+    def plugin_section
+      elements[1]
+    end
+  end
+
+  module Config1
+    def _1
+      elements[0]
+    end
+
+    def plugin_section
+      elements[1]
+    end
+
+    def _2
+      elements[2]
+    end
+
+    def _3
+      elements[4]
+    end
+  end
+
+  def _nt_config
+    start_index = index
+    if node_cache[:config].has_key?(index)
+      cached = node_cache[:config][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    r1 = _nt__
+    s0 << r1
+    if r1
+      r2 = _nt_plugin_section
+      s0 << r2
+      if r2
+        r3 = _nt__
+        s0 << r3
+        if r3
+          s4, i4 = [], index
+          loop do
+            i5, s5 = index, []
+            r6 = _nt__
+            s5 << r6
+            if r6
+              r7 = _nt_plugin_section
+              s5 << r7
+            end
+            if s5.last
+              r5 = instantiate_node(SyntaxNode,input, i5...index, s5)
+              r5.extend(Config0)
+            else
+              @index = i5
+              r5 = nil
+            end
+            if r5
+              s4 << r5
+            else
+              break
+            end
+          end
+          r4 = instantiate_node(SyntaxNode,input, i4...index, s4)
+          s0 << r4
+          if r4
+            r8 = _nt__
+            s0 << r8
+          end
+        end
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::Config,input, i0...index, s0)
+      r0.extend(Config1)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:config][start_index] = r0
+
+    r0
+  end
+
+  module Comment0
+  end
+
+  def _nt_comment
+    start_index = index
+    if node_cache[:comment].has_key?(index)
+      cached = node_cache[:comment][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    s0, i0 = [], index
+    loop do
+      i1, s1 = index, []
+      r3 = _nt_whitespace
+      if r3
+        r2 = r3
+      else
+        r2 = instantiate_node(SyntaxNode,input, index...index)
+      end
+      s1 << r2
+      if r2
+        if has_terminal?("#", false, index)
+          r4 = instantiate_node(SyntaxNode,input, index...(index + 1))
+          @index += 1
+        else
+          terminal_parse_failure("#")
+          r4 = nil
+        end
+        s1 << r4
+        if r4
+          s5, i5 = [], index
+          loop do
+            if has_terminal?('\G[^\\r\\n]', true, index)
+              r6 = true
+              @index += 1
+            else
+              r6 = nil
+            end
+            if r6
+              s5 << r6
+            else
+              break
+            end
+          end
+          r5 = instantiate_node(SyntaxNode,input, i5...index, s5)
+          s1 << r5
+          if r5
+            if has_terminal?("\r", false, index)
+              r8 = instantiate_node(SyntaxNode,input, index...(index + 1))
+              @index += 1
+            else
+              terminal_parse_failure("\r")
+              r8 = nil
+            end
+            if r8
+              r7 = r8
+            else
+              r7 = instantiate_node(SyntaxNode,input, index...index)
+            end
+            s1 << r7
+            if r7
+              if has_terminal?("\n", false, index)
+                r9 = instantiate_node(SyntaxNode,input, index...(index + 1))
+                @index += 1
+              else
+                terminal_parse_failure("\n")
+                r9 = nil
+              end
+              s1 << r9
+            end
+          end
+        end
+      end
+      if s1.last
+        r1 = instantiate_node(SyntaxNode,input, i1...index, s1)
+        r1.extend(Comment0)
+      else
+        @index = i1
+        r1 = nil
+      end
+      if r1
+        s0 << r1
+      else
+        break
+      end
+    end
+    if s0.empty?
+      @index = i0
+      r0 = nil
+    else
+      r0 = instantiate_node(LogStash::Config::AST::Comment,input, i0...index, s0)
+    end
+
+    node_cache[:comment][start_index] = r0
+
+    r0
+  end
+
+  def _nt__
+    start_index = index
+    if node_cache[:_].has_key?(index)
+      cached = node_cache[:_][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    s0, i0 = [], index
+    loop do
+      i1 = index
+      r2 = _nt_comment
+      if r2
+        r1 = r2
+      else
+        r3 = _nt_whitespace
+        if r3
+          r1 = r3
+        else
+          @index = i1
+          r1 = nil
+        end
+      end
+      if r1
+        s0 << r1
+      else
+        break
+      end
+    end
+    r0 = instantiate_node(LogStash::Config::AST::Whitespace,input, i0...index, s0)
+
+    node_cache[:_][start_index] = r0
+
+    r0
+  end
+
+  def _nt_whitespace
+    start_index = index
+    if node_cache[:whitespace].has_key?(index)
+      cached = node_cache[:whitespace][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    s0, i0 = [], index
+    loop do
+      if has_terminal?('\G[ \\t\\r\\n]', true, index)
+        r1 = true
+        @index += 1
+      else
+        r1 = nil
+      end
+      if r1
+        s0 << r1
+      else
+        break
+      end
+    end
+    if s0.empty?
+      @index = i0
+      r0 = nil
+    else
+      r0 = instantiate_node(LogStash::Config::AST::Whitespace,input, i0...index, s0)
+    end
+
+    node_cache[:whitespace][start_index] = r0
+
+    r0
+  end
+
+  module PluginSection0
+    def branch_or_plugin
+      elements[0]
+    end
+
+    def _
+      elements[1]
+    end
+  end
+
+  module PluginSection1
+    def plugin_type
+      elements[0]
+    end
+
+    def _1
+      elements[1]
+    end
+
+    def _2
+      elements[3]
+    end
+
+  end
+
+  def _nt_plugin_section
+    start_index = index
+    if node_cache[:plugin_section].has_key?(index)
+      cached = node_cache[:plugin_section][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    r1 = _nt_plugin_type
+    s0 << r1
+    if r1
+      r2 = _nt__
+      s0 << r2
+      if r2
+        if has_terminal?("{", false, index)
+          r3 = instantiate_node(SyntaxNode,input, index...(index + 1))
+          @index += 1
+        else
+          terminal_parse_failure("{")
+          r3 = nil
+        end
+        s0 << r3
+        if r3
+          r4 = _nt__
+          s0 << r4
+          if r4
+            s5, i5 = [], index
+            loop do
+              i6, s6 = index, []
+              r7 = _nt_branch_or_plugin
+              s6 << r7
+              if r7
+                r8 = _nt__
+                s6 << r8
+              end
+              if s6.last
+                r6 = instantiate_node(SyntaxNode,input, i6...index, s6)
+                r6.extend(PluginSection0)
+              else
+                @index = i6
+                r6 = nil
+              end
+              if r6
+                s5 << r6
+              else
+                break
+              end
+            end
+            r5 = instantiate_node(SyntaxNode,input, i5...index, s5)
+            s0 << r5
+            if r5
+              if has_terminal?("}", false, index)
+                r9 = instantiate_node(SyntaxNode,input, index...(index + 1))
+                @index += 1
+              else
+                terminal_parse_failure("}")
+                r9 = nil
+              end
+              s0 << r9
+            end
+          end
+        end
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::PluginSection,input, i0...index, s0)
+      r0.extend(PluginSection1)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:plugin_section][start_index] = r0
+
+    r0
+  end
+
+  def _nt_branch_or_plugin
+    start_index = index
+    if node_cache[:branch_or_plugin].has_key?(index)
+      cached = node_cache[:branch_or_plugin][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0 = index
+    r1 = _nt_branch
+    if r1
+      r0 = r1
+    else
+      r2 = _nt_plugin
+      if r2
+        r0 = r2
+      else
+        @index = i0
+        r0 = nil
+      end
+    end
+
+    node_cache[:branch_or_plugin][start_index] = r0
+
+    r0
+  end
+
+  def _nt_plugin_type
+    start_index = index
+    if node_cache[:plugin_type].has_key?(index)
+      cached = node_cache[:plugin_type][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0 = index
+    if has_terminal?("input", false, index)
+      r1 = instantiate_node(SyntaxNode,input, index...(index + 5))
+      @index += 5
+    else
+      terminal_parse_failure("input")
+      r1 = nil
+    end
+    if r1
+      r0 = r1
+    else
+      if has_terminal?("filter", false, index)
+        r2 = instantiate_node(SyntaxNode,input, index...(index + 6))
+        @index += 6
+      else
+        terminal_parse_failure("filter")
+        r2 = nil
+      end
+      if r2
+        r0 = r2
+      else
+        if has_terminal?("output", false, index)
+          r3 = instantiate_node(SyntaxNode,input, index...(index + 6))
+          @index += 6
+        else
+          terminal_parse_failure("output")
+          r3 = nil
+        end
+        if r3
+          r0 = r3
+        else
+          @index = i0
+          r0 = nil
+        end
+      end
+    end
+
+    node_cache[:plugin_type][start_index] = r0
+
+    r0
+  end
+
+  module Plugins0
+    def _
+      elements[0]
+    end
+
+    def plugin
+      elements[1]
+    end
+  end
+
+  module Plugins1
+    def plugin
+      elements[0]
+    end
+
+  end
+
+  def _nt_plugins
+    start_index = index
+    if node_cache[:plugins].has_key?(index)
+      cached = node_cache[:plugins][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i1, s1 = index, []
+    r2 = _nt_plugin
+    s1 << r2
+    if r2
+      s3, i3 = [], index
+      loop do
+        i4, s4 = index, []
+        r5 = _nt__
+        s4 << r5
+        if r5
+          r6 = _nt_plugin
+          s4 << r6
+        end
+        if s4.last
+          r4 = instantiate_node(SyntaxNode,input, i4...index, s4)
+          r4.extend(Plugins0)
+        else
+          @index = i4
+          r4 = nil
+        end
+        if r4
+          s3 << r4
+        else
+          break
+        end
+      end
+      r3 = instantiate_node(SyntaxNode,input, i3...index, s3)
+      s1 << r3
+    end
+    if s1.last
+      r1 = instantiate_node(SyntaxNode,input, i1...index, s1)
+      r1.extend(Plugins1)
+    else
+      @index = i1
+      r1 = nil
+    end
+    if r1
+      r0 = r1
+    else
+      r0 = instantiate_node(SyntaxNode,input, index...index)
+    end
+
+    node_cache[:plugins][start_index] = r0
+
+    r0
+  end
+
+  module Plugin0
+    def whitespace
+      elements[0]
+    end
+
+    def _
+      elements[1]
+    end
+
+    def attribute
+      elements[2]
+    end
+  end
+
+  module Plugin1
+    def attribute
+      elements[0]
+    end
+
+  end
+
+  module Plugin2
+    def name
+      elements[0]
+    end
+
+    def _1
+      elements[1]
+    end
+
+    def _2
+      elements[3]
+    end
+
+    def attributes
+      elements[4]
+    end
+
+    def _3
+      elements[5]
+    end
+
+  end
+
+  def _nt_plugin
+    start_index = index
+    if node_cache[:plugin].has_key?(index)
+      cached = node_cache[:plugin][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    r1 = _nt_name
+    s0 << r1
+    if r1
+      r2 = _nt__
+      s0 << r2
+      if r2
+        if has_terminal?("{", false, index)
+          r3 = instantiate_node(SyntaxNode,input, index...(index + 1))
+          @index += 1
+        else
+          terminal_parse_failure("{")
+          r3 = nil
+        end
+        s0 << r3
+        if r3
+          r4 = _nt__
+          s0 << r4
+          if r4
+            i6, s6 = index, []
+            r7 = _nt_attribute
+            s6 << r7
+            if r7
+              s8, i8 = [], index
+              loop do
+                i9, s9 = index, []
+                r10 = _nt_whitespace
+                s9 << r10
+                if r10
+                  r11 = _nt__
+                  s9 << r11
+                  if r11
+                    r12 = _nt_attribute
+                    s9 << r12
+                  end
+                end
+                if s9.last
+                  r9 = instantiate_node(SyntaxNode,input, i9...index, s9)
+                  r9.extend(Plugin0)
+                else
+                  @index = i9
+                  r9 = nil
+                end
+                if r9
+                  s8 << r9
+                else
+                  break
+                end
+              end
+              r8 = instantiate_node(SyntaxNode,input, i8...index, s8)
+              s6 << r8
+            end
+            if s6.last
+              r6 = instantiate_node(SyntaxNode,input, i6...index, s6)
+              r6.extend(Plugin1)
+            else
+              @index = i6
+              r6 = nil
+            end
+            if r6
+              r5 = r6
+            else
+              r5 = instantiate_node(SyntaxNode,input, index...index)
+            end
+            s0 << r5
+            if r5
+              r13 = _nt__
+              s0 << r13
+              if r13
+                if has_terminal?("}", false, index)
+                  r14 = instantiate_node(SyntaxNode,input, index...(index + 1))
+                  @index += 1
+                else
+                  terminal_parse_failure("}")
+                  r14 = nil
+                end
+                s0 << r14
+              end
+            end
+          end
+        end
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::Plugin,input, i0...index, s0)
+      r0.extend(Plugin2)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:plugin][start_index] = r0
+
+    r0
+  end
+
+  def _nt_name
+    start_index = index
+    if node_cache[:name].has_key?(index)
+      cached = node_cache[:name][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0 = index
+    s1, i1 = [], index
+    loop do
+      if has_terminal?('\G[A-Za-z0-9_-]', true, index)
+        r2 = true
+        @index += 1
+      else
+        r2 = nil
+      end
+      if r2
+        s1 << r2
+      else
+        break
+      end
+    end
+    if s1.empty?
+      @index = i1
+      r1 = nil
+    else
+      r1 = instantiate_node(LogStash::Config::AST::Name,input, i1...index, s1)
+    end
+    if r1
+      r0 = r1
+    else
+      r3 = _nt_string
+      if r3
+        r0 = r3
+      else
+        @index = i0
+        r0 = nil
+      end
+    end
+
+    node_cache[:name][start_index] = r0
+
+    r0
+  end
+
+  module Attribute0
+    def name
+      elements[0]
+    end
+
+    def _1
+      elements[1]
+    end
+
+    def _2
+      elements[3]
+    end
+
+    def value
+      elements[4]
+    end
+  end
+
+  def _nt_attribute
+    start_index = index
+    if node_cache[:attribute].has_key?(index)
+      cached = node_cache[:attribute][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    r1 = _nt_name
+    s0 << r1
+    if r1
+      r2 = _nt__
+      s0 << r2
+      if r2
+        if has_terminal?("=>", false, index)
+          r3 = instantiate_node(SyntaxNode,input, index...(index + 2))
+          @index += 2
+        else
+          terminal_parse_failure("=>")
+          r3 = nil
+        end
+        s0 << r3
+        if r3
+          r4 = _nt__
+          s0 << r4
+          if r4
+            r5 = _nt_value
+            s0 << r5
+          end
+        end
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::Attribute,input, i0...index, s0)
+      r0.extend(Attribute0)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:attribute][start_index] = r0
+
+    r0
+  end
+
+  def _nt_value
+    start_index = index
+    if node_cache[:value].has_key?(index)
+      cached = node_cache[:value][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0 = index
+    r1 = _nt_plugin
+    if r1
+      r0 = r1
+    else
+      r2 = _nt_bareword
+      if r2
+        r0 = r2
+      else
+        r3 = _nt_string
+        if r3
+          r0 = r3
+        else
+          r4 = _nt_number
+          if r4
+            r0 = r4
+          else
+            r5 = _nt_array
+            if r5
+              r0 = r5
+            else
+              r6 = _nt_hash
+              if r6
+                r0 = r6
+              else
+                @index = i0
+                r0 = nil
+              end
+            end
+          end
+        end
+      end
+    end
+
+    node_cache[:value][start_index] = r0
+
+    r0
+  end
+
+  def _nt_array_value
+    start_index = index
+    if node_cache[:array_value].has_key?(index)
+      cached = node_cache[:array_value][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0 = index
+    r1 = _nt_bareword
+    if r1
+      r0 = r1
+    else
+      r2 = _nt_string
+      if r2
+        r0 = r2
+      else
+        r3 = _nt_number
+        if r3
+          r0 = r3
+        else
+          r4 = _nt_array
+          if r4
+            r0 = r4
+          else
+            r5 = _nt_hash
+            if r5
+              r0 = r5
+            else
+              @index = i0
+              r0 = nil
+            end
+          end
+        end
+      end
+    end
+
+    node_cache[:array_value][start_index] = r0
+
+    r0
+  end
+
+  module Bareword0
+  end
+
+  def _nt_bareword
+    start_index = index
+    if node_cache[:bareword].has_key?(index)
+      cached = node_cache[:bareword][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    if has_terminal?('\G[A-Za-z_]', true, index)
+      r1 = true
+      @index += 1
+    else
+      r1 = nil
+    end
+    s0 << r1
+    if r1
+      s2, i2 = [], index
+      loop do
+        if has_terminal?('\G[A-Za-z0-9_]', true, index)
+          r3 = true
+          @index += 1
+        else
+          r3 = nil
+        end
+        if r3
+          s2 << r3
+        else
+          break
+        end
+      end
+      if s2.empty?
+        @index = i2
+        r2 = nil
+      else
+        r2 = instantiate_node(SyntaxNode,input, i2...index, s2)
+      end
+      s0 << r2
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::Bareword,input, i0...index, s0)
+      r0.extend(Bareword0)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:bareword][start_index] = r0
+
+    r0
+  end
+
+  module DoubleQuotedString0
+  end
+
+  module DoubleQuotedString1
+  end
+
+  def _nt_double_quoted_string
+    start_index = index
+    if node_cache[:double_quoted_string].has_key?(index)
+      cached = node_cache[:double_quoted_string][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    if has_terminal?('"', false, index)
+      r1 = instantiate_node(SyntaxNode,input, index...(index + 1))
+      @index += 1
+    else
+      terminal_parse_failure('"')
+      r1 = nil
+    end
+    s0 << r1
+    if r1
+      s2, i2 = [], index
+      loop do
+        i3 = index
+        if has_terminal?('\"', false, index)
+          r4 = instantiate_node(SyntaxNode,input, index...(index + 2))
+          @index += 2
+        else
+          terminal_parse_failure('\"')
+          r4 = nil
+        end
+        if r4
+          r3 = r4
+        else
+          i5, s5 = index, []
+          i6 = index
+          if has_terminal?('"', false, index)
+            r7 = instantiate_node(SyntaxNode,input, index...(index + 1))
+            @index += 1
+          else
+            terminal_parse_failure('"')
+            r7 = nil
+          end
+          if r7
+            r6 = nil
+          else
+            @index = i6
+            r6 = instantiate_node(SyntaxNode,input, index...index)
+          end
+          s5 << r6
+          if r6
+            if index < input_length
+              r8 = instantiate_node(SyntaxNode,input, index...(index + 1))
+              @index += 1
+            else
+              terminal_parse_failure("any character")
+              r8 = nil
+            end
+            s5 << r8
+          end
+          if s5.last
+            r5 = instantiate_node(SyntaxNode,input, i5...index, s5)
+            r5.extend(DoubleQuotedString0)
+          else
+            @index = i5
+            r5 = nil
+          end
+          if r5
+            r3 = r5
+          else
+            @index = i3
+            r3 = nil
+          end
+        end
+        if r3
+          s2 << r3
+        else
+          break
+        end
+      end
+      r2 = instantiate_node(SyntaxNode,input, i2...index, s2)
+      s0 << r2
+      if r2
+        if has_terminal?('"', false, index)
+          r9 = instantiate_node(SyntaxNode,input, index...(index + 1))
+          @index += 1
+        else
+          terminal_parse_failure('"')
+          r9 = nil
+        end
+        s0 << r9
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::String,input, i0...index, s0)
+      r0.extend(DoubleQuotedString1)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:double_quoted_string][start_index] = r0
+
+    r0
+  end
+
+  module SingleQuotedString0
+  end
+
+  module SingleQuotedString1
+  end
+
+  def _nt_single_quoted_string
+    start_index = index
+    if node_cache[:single_quoted_string].has_key?(index)
+      cached = node_cache[:single_quoted_string][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    if has_terminal?("'", false, index)
+      r1 = instantiate_node(SyntaxNode,input, index...(index + 1))
+      @index += 1
+    else
+      terminal_parse_failure("'")
+      r1 = nil
+    end
+    s0 << r1
+    if r1
+      s2, i2 = [], index
+      loop do
+        i3 = index
+        if has_terminal?("\\'", false, index)
+          r4 = instantiate_node(SyntaxNode,input, index...(index + 2))
+          @index += 2
+        else
+          terminal_parse_failure("\\'")
+          r4 = nil
+        end
+        if r4
+          r3 = r4
+        else
+          i5, s5 = index, []
+          i6 = index
+          if has_terminal?("'", false, index)
+            r7 = instantiate_node(SyntaxNode,input, index...(index + 1))
+            @index += 1
+          else
+            terminal_parse_failure("'")
+            r7 = nil
+          end
+          if r7
+            r6 = nil
+          else
+            @index = i6
+            r6 = instantiate_node(SyntaxNode,input, index...index)
+          end
+          s5 << r6
+          if r6
+            if index < input_length
+              r8 = instantiate_node(SyntaxNode,input, index...(index + 1))
+              @index += 1
+            else
+              terminal_parse_failure("any character")
+              r8 = nil
+            end
+            s5 << r8
+          end
+          if s5.last
+            r5 = instantiate_node(SyntaxNode,input, i5...index, s5)
+            r5.extend(SingleQuotedString0)
+          else
+            @index = i5
+            r5 = nil
+          end
+          if r5
+            r3 = r5
+          else
+            @index = i3
+            r3 = nil
+          end
+        end
+        if r3
+          s2 << r3
+        else
+          break
+        end
+      end
+      r2 = instantiate_node(SyntaxNode,input, i2...index, s2)
+      s0 << r2
+      if r2
+        if has_terminal?("'", false, index)
+          r9 = instantiate_node(SyntaxNode,input, index...(index + 1))
+          @index += 1
+        else
+          terminal_parse_failure("'")
+          r9 = nil
+        end
+        s0 << r9
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::String,input, i0...index, s0)
+      r0.extend(SingleQuotedString1)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:single_quoted_string][start_index] = r0
+
+    r0
+  end
+
+  def _nt_string
+    start_index = index
+    if node_cache[:string].has_key?(index)
+      cached = node_cache[:string][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0 = index
+    r1 = _nt_double_quoted_string
+    if r1
+      r0 = r1
+    else
+      r2 = _nt_single_quoted_string
+      if r2
+        r0 = r2
+      else
+        @index = i0
+        r0 = nil
+      end
+    end
+
+    node_cache[:string][start_index] = r0
+
+    r0
+  end
+
+  module Regexp0
+  end
+
+  module Regexp1
+  end
+
+  def _nt_regexp
+    start_index = index
+    if node_cache[:regexp].has_key?(index)
+      cached = node_cache[:regexp][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    if has_terminal?('/', false, index)
+      r1 = instantiate_node(SyntaxNode,input, index...(index + 1))
+      @index += 1
+    else
+      terminal_parse_failure('/')
+      r1 = nil
+    end
+    s0 << r1
+    if r1
+      s2, i2 = [], index
+      loop do
+        i3 = index
+        if has_terminal?('\/', false, index)
+          r4 = instantiate_node(SyntaxNode,input, index...(index + 2))
+          @index += 2
+        else
+          terminal_parse_failure('\/')
+          r4 = nil
+        end
+        if r4
+          r3 = r4
+        else
+          i5, s5 = index, []
+          i6 = index
+          if has_terminal?('/', false, index)
+            r7 = instantiate_node(SyntaxNode,input, index...(index + 1))
+            @index += 1
+          else
+            terminal_parse_failure('/')
+            r7 = nil
+          end
+          if r7
+            r6 = nil
+          else
+            @index = i6
+            r6 = instantiate_node(SyntaxNode,input, index...index)
+          end
+          s5 << r6
+          if r6
+            if index < input_length
+              r8 = instantiate_node(SyntaxNode,input, index...(index + 1))
+              @index += 1
+            else
+              terminal_parse_failure("any character")
+              r8 = nil
+            end
+            s5 << r8
+          end
+          if s5.last
+            r5 = instantiate_node(SyntaxNode,input, i5...index, s5)
+            r5.extend(Regexp0)
+          else
+            @index = i5
+            r5 = nil
+          end
+          if r5
+            r3 = r5
+          else
+            @index = i3
+            r3 = nil
+          end
+        end
+        if r3
+          s2 << r3
+        else
+          break
+        end
+      end
+      r2 = instantiate_node(SyntaxNode,input, i2...index, s2)
+      s0 << r2
+      if r2
+        if has_terminal?('/', false, index)
+          r9 = instantiate_node(SyntaxNode,input, index...(index + 1))
+          @index += 1
+        else
+          terminal_parse_failure('/')
+          r9 = nil
+        end
+        s0 << r9
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::RegExp,input, i0...index, s0)
+      r0.extend(Regexp1)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:regexp][start_index] = r0
+
+    r0
+  end
+
+  module Number0
+  end
+
+  module Number1
+  end
+
+  def _nt_number
+    start_index = index
+    if node_cache[:number].has_key?(index)
+      cached = node_cache[:number][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    if has_terminal?("-", false, index)
+      r2 = instantiate_node(SyntaxNode,input, index...(index + 1))
+      @index += 1
+    else
+      terminal_parse_failure("-")
+      r2 = nil
+    end
+    if r2
+      r1 = r2
+    else
+      r1 = instantiate_node(SyntaxNode,input, index...index)
+    end
+    s0 << r1
+    if r1
+      s3, i3 = [], index
+      loop do
+        if has_terminal?('\G[0-9]', true, index)
+          r4 = true
+          @index += 1
+        else
+          r4 = nil
+        end
+        if r4
+          s3 << r4
+        else
+          break
+        end
+      end
+      if s3.empty?
+        @index = i3
+        r3 = nil
+      else
+        r3 = instantiate_node(SyntaxNode,input, i3...index, s3)
+      end
+      s0 << r3
+      if r3
+        i6, s6 = index, []
+        if has_terminal?(".", false, index)
+          r7 = instantiate_node(SyntaxNode,input, index...(index + 1))
+          @index += 1
+        else
+          terminal_parse_failure(".")
+          r7 = nil
+        end
+        s6 << r7
+        if r7
+          s8, i8 = [], index
+          loop do
+            if has_terminal?('\G[0-9]', true, index)
+              r9 = true
+              @index += 1
+            else
+              r9 = nil
+            end
+            if r9
+              s8 << r9
+            else
+              break
+            end
+          end
+          r8 = instantiate_node(SyntaxNode,input, i8...index, s8)
+          s6 << r8
+        end
+        if s6.last
+          r6 = instantiate_node(SyntaxNode,input, i6...index, s6)
+          r6.extend(Number0)
+        else
+          @index = i6
+          r6 = nil
+        end
+        if r6
+          r5 = r6
+        else
+          r5 = instantiate_node(SyntaxNode,input, index...index)
+        end
+        s0 << r5
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::Number,input, i0...index, s0)
+      r0.extend(Number1)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:number][start_index] = r0
+
+    r0
+  end
+
+  module Array0
+    def _1
+      elements[0]
+    end
+
+    def _2
+      elements[2]
+    end
+
+    def value
+      elements[3]
+    end
+  end
+
+  module Array1
+    def value
+      elements[0]
+    end
+
+  end
+
+  module Array2
+    def _1
+      elements[1]
+    end
+
+    def _2
+      elements[3]
+    end
+
+  end
+
+  def _nt_array
+    start_index = index
+    if node_cache[:array].has_key?(index)
+      cached = node_cache[:array][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    if has_terminal?("[", false, index)
+      r1 = instantiate_node(SyntaxNode,input, index...(index + 1))
+      @index += 1
+    else
+      terminal_parse_failure("[")
+      r1 = nil
+    end
+    s0 << r1
+    if r1
+      r2 = _nt__
+      s0 << r2
+      if r2
+        i4, s4 = index, []
+        r5 = _nt_value
+        s4 << r5
+        if r5
+          s6, i6 = [], index
+          loop do
+            i7, s7 = index, []
+            r8 = _nt__
+            s7 << r8
+            if r8
+              if has_terminal?(",", false, index)
+                r9 = instantiate_node(SyntaxNode,input, index...(index + 1))
+                @index += 1
+              else
+                terminal_parse_failure(",")
+                r9 = nil
+              end
+              s7 << r9
+              if r9
+                r10 = _nt__
+                s7 << r10
+                if r10
+                  r11 = _nt_value
+                  s7 << r11
+                end
+              end
+            end
+            if s7.last
+              r7 = instantiate_node(SyntaxNode,input, i7...index, s7)
+              r7.extend(Array0)
+            else
+              @index = i7
+              r7 = nil
+            end
+            if r7
+              s6 << r7
+            else
+              break
+            end
+          end
+          r6 = instantiate_node(SyntaxNode,input, i6...index, s6)
+          s4 << r6
+        end
+        if s4.last
+          r4 = instantiate_node(SyntaxNode,input, i4...index, s4)
+          r4.extend(Array1)
+        else
+          @index = i4
+          r4 = nil
+        end
+        if r4
+          r3 = r4
+        else
+          r3 = instantiate_node(SyntaxNode,input, index...index)
+        end
+        s0 << r3
+        if r3
+          r12 = _nt__
+          s0 << r12
+          if r12
+            if has_terminal?("]", false, index)
+              r13 = instantiate_node(SyntaxNode,input, index...(index + 1))
+              @index += 1
+            else
+              terminal_parse_failure("]")
+              r13 = nil
+            end
+            s0 << r13
+          end
+        end
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::Array,input, i0...index, s0)
+      r0.extend(Array2)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:array][start_index] = r0
+
+    r0
+  end
+
+  module Hash0
+    def _1
+      elements[1]
+    end
+
+    def _2
+      elements[3]
+    end
+
+  end
+
+  def _nt_hash
+    start_index = index
+    if node_cache[:hash].has_key?(index)
+      cached = node_cache[:hash][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    if has_terminal?("{", false, index)
+      r1 = instantiate_node(SyntaxNode,input, index...(index + 1))
+      @index += 1
+    else
+      terminal_parse_failure("{")
+      r1 = nil
+    end
+    s0 << r1
+    if r1
+      r2 = _nt__
+      s0 << r2
+      if r2
+        r4 = _nt_hashentries
+        if r4
+          r3 = r4
+        else
+          r3 = instantiate_node(SyntaxNode,input, index...index)
+        end
+        s0 << r3
+        if r3
+          r5 = _nt__
+          s0 << r5
+          if r5
+            if has_terminal?("}", false, index)
+              r6 = instantiate_node(SyntaxNode,input, index...(index + 1))
+              @index += 1
+            else
+              terminal_parse_failure("}")
+              r6 = nil
+            end
+            s0 << r6
+          end
+        end
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::Hash,input, i0...index, s0)
+      r0.extend(Hash0)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:hash][start_index] = r0
+
+    r0
+  end
+
+  module Hashentries0
+    def whitespace
+      elements[0]
+    end
+
+    def hashentry
+      elements[1]
+    end
+  end
+
+  module Hashentries1
+    def hashentry
+      elements[0]
+    end
+
+  end
+
+  def _nt_hashentries
+    start_index = index
+    if node_cache[:hashentries].has_key?(index)
+      cached = node_cache[:hashentries][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    r1 = _nt_hashentry
+    s0 << r1
+    if r1
+      s2, i2 = [], index
+      loop do
+        i3, s3 = index, []
+        r4 = _nt_whitespace
+        s3 << r4
+        if r4
+          r5 = _nt_hashentry
+          s3 << r5
+        end
+        if s3.last
+          r3 = instantiate_node(SyntaxNode,input, i3...index, s3)
+          r3.extend(Hashentries0)
+        else
+          @index = i3
+          r3 = nil
+        end
+        if r3
+          s2 << r3
+        else
+          break
+        end
+      end
+      r2 = instantiate_node(SyntaxNode,input, i2...index, s2)
+      s0 << r2
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::HashEntries,input, i0...index, s0)
+      r0.extend(Hashentries1)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:hashentries][start_index] = r0
+
+    r0
+  end
+
+  module Hashentry0
+    def name
+      elements[0]
+    end
+
+    def _1
+      elements[1]
+    end
+
+    def _2
+      elements[3]
+    end
+
+    def value
+      elements[4]
+    end
+  end
+
+  def _nt_hashentry
+    start_index = index
+    if node_cache[:hashentry].has_key?(index)
+      cached = node_cache[:hashentry][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    i1 = index
+    r2 = _nt_number
+    if r2
+      r1 = r2
+    else
+      r3 = _nt_bareword
+      if r3
+        r1 = r3
+      else
+        r4 = _nt_string
+        if r4
+          r1 = r4
+        else
+          @index = i1
+          r1 = nil
+        end
+      end
+    end
+    s0 << r1
+    if r1
+      r5 = _nt__
+      s0 << r5
+      if r5
+        if has_terminal?("=>", false, index)
+          r6 = instantiate_node(SyntaxNode,input, index...(index + 2))
+          @index += 2
+        else
+          terminal_parse_failure("=>")
+          r6 = nil
+        end
+        s0 << r6
+        if r6
+          r7 = _nt__
+          s0 << r7
+          if r7
+            r8 = _nt_value
+            s0 << r8
+          end
+        end
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::HashEntry,input, i0...index, s0)
+      r0.extend(Hashentry0)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:hashentry][start_index] = r0
+
+    r0
+  end
+
+  module Branch0
+    def _
+      elements[0]
+    end
+
+    def else_if
+      elements[1]
+    end
+  end
+
+  module Branch1
+    def _
+      elements[0]
+    end
+
+    def else
+      elements[1]
+    end
+  end
+
+  module Branch2
+    def if
+      elements[0]
+    end
+
+  end
+
+  def _nt_branch
+    start_index = index
+    if node_cache[:branch].has_key?(index)
+      cached = node_cache[:branch][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    r1 = _nt_if
+    s0 << r1
+    if r1
+      s2, i2 = [], index
+      loop do
+        i3, s3 = index, []
+        r4 = _nt__
+        s3 << r4
+        if r4
+          r5 = _nt_else_if
+          s3 << r5
+        end
+        if s3.last
+          r3 = instantiate_node(SyntaxNode,input, i3...index, s3)
+          r3.extend(Branch0)
+        else
+          @index = i3
+          r3 = nil
+        end
+        if r3
+          s2 << r3
+        else
+          break
+        end
+      end
+      r2 = instantiate_node(SyntaxNode,input, i2...index, s2)
+      s0 << r2
+      if r2
+        i7, s7 = index, []
+        r8 = _nt__
+        s7 << r8
+        if r8
+          r9 = _nt_else
+          s7 << r9
+        end
+        if s7.last
+          r7 = instantiate_node(SyntaxNode,input, i7...index, s7)
+          r7.extend(Branch1)
+        else
+          @index = i7
+          r7 = nil
+        end
+        if r7
+          r6 = r7
+        else
+          r6 = instantiate_node(SyntaxNode,input, index...index)
+        end
+        s0 << r6
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::Branch,input, i0...index, s0)
+      r0.extend(Branch2)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:branch][start_index] = r0
+
+    r0
+  end
+
+  module If0
+    def branch_or_plugin
+      elements[0]
+    end
+
+    def _
+      elements[1]
+    end
+  end
+
+  module If1
+    def _1
+      elements[1]
+    end
+
+    def condition
+      elements[2]
+    end
+
+    def _2
+      elements[3]
+    end
+
+    def _3
+      elements[5]
+    end
+
+  end
+
+  def _nt_if
+    start_index = index
+    if node_cache[:if].has_key?(index)
+      cached = node_cache[:if][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    if has_terminal?("if", false, index)
+      r1 = instantiate_node(SyntaxNode,input, index...(index + 2))
+      @index += 2
+    else
+      terminal_parse_failure("if")
+      r1 = nil
+    end
+    s0 << r1
+    if r1
+      r2 = _nt__
+      s0 << r2
+      if r2
+        r3 = _nt_condition
+        s0 << r3
+        if r3
+          r4 = _nt__
+          s0 << r4
+          if r4
+            if has_terminal?("{", false, index)
+              r5 = instantiate_node(SyntaxNode,input, index...(index + 1))
+              @index += 1
+            else
+              terminal_parse_failure("{")
+              r5 = nil
+            end
+            s0 << r5
+            if r5
+              r6 = _nt__
+              s0 << r6
+              if r6
+                s7, i7 = [], index
+                loop do
+                  i8, s8 = index, []
+                  r9 = _nt_branch_or_plugin
+                  s8 << r9
+                  if r9
+                    r10 = _nt__
+                    s8 << r10
+                  end
+                  if s8.last
+                    r8 = instantiate_node(SyntaxNode,input, i8...index, s8)
+                    r8.extend(If0)
+                  else
+                    @index = i8
+                    r8 = nil
+                  end
+                  if r8
+                    s7 << r8
+                  else
+                    break
+                  end
+                end
+                r7 = instantiate_node(SyntaxNode,input, i7...index, s7)
+                s0 << r7
+                if r7
+                  if has_terminal?("}", false, index)
+                    r11 = instantiate_node(SyntaxNode,input, index...(index + 1))
+                    @index += 1
+                  else
+                    terminal_parse_failure("}")
+                    r11 = nil
+                  end
+                  s0 << r11
+                end
+              end
+            end
+          end
+        end
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::If,input, i0...index, s0)
+      r0.extend(If1)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:if][start_index] = r0
+
+    r0
+  end
+
+  module ElseIf0
+    def branch_or_plugin
+      elements[0]
+    end
+
+    def _
+      elements[1]
+    end
+  end
+
+  module ElseIf1
+    def _1
+      elements[1]
+    end
+
+    def _2
+      elements[3]
+    end
+
+    def condition
+      elements[4]
+    end
+
+    def _3
+      elements[5]
+    end
+
+    def _4
+      elements[7]
+    end
+
+  end
+
+  def _nt_else_if
+    start_index = index
+    if node_cache[:else_if].has_key?(index)
+      cached = node_cache[:else_if][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    if has_terminal?("else", false, index)
+      r1 = instantiate_node(SyntaxNode,input, index...(index + 4))
+      @index += 4
+    else
+      terminal_parse_failure("else")
+      r1 = nil
+    end
+    s0 << r1
+    if r1
+      r2 = _nt__
+      s0 << r2
+      if r2
+        if has_terminal?("if", false, index)
+          r3 = instantiate_node(SyntaxNode,input, index...(index + 2))
+          @index += 2
+        else
+          terminal_parse_failure("if")
+          r3 = nil
+        end
+        s0 << r3
+        if r3
+          r4 = _nt__
+          s0 << r4
+          if r4
+            r5 = _nt_condition
+            s0 << r5
+            if r5
+              r6 = _nt__
+              s0 << r6
+              if r6
+                if has_terminal?("{", false, index)
+                  r7 = instantiate_node(SyntaxNode,input, index...(index + 1))
+                  @index += 1
+                else
+                  terminal_parse_failure("{")
+                  r7 = nil
+                end
+                s0 << r7
+                if r7
+                  r8 = _nt__
+                  s0 << r8
+                  if r8
+                    s9, i9 = [], index
+                    loop do
+                      i10, s10 = index, []
+                      r11 = _nt_branch_or_plugin
+                      s10 << r11
+                      if r11
+                        r12 = _nt__
+                        s10 << r12
+                      end
+                      if s10.last
+                        r10 = instantiate_node(SyntaxNode,input, i10...index, s10)
+                        r10.extend(ElseIf0)
+                      else
+                        @index = i10
+                        r10 = nil
+                      end
+                      if r10
+                        s9 << r10
+                      else
+                        break
+                      end
+                    end
+                    r9 = instantiate_node(SyntaxNode,input, i9...index, s9)
+                    s0 << r9
+                    if r9
+                      if has_terminal?("}", false, index)
+                        r13 = instantiate_node(SyntaxNode,input, index...(index + 1))
+                        @index += 1
+                      else
+                        terminal_parse_failure("}")
+                        r13 = nil
+                      end
+                      s0 << r13
+                    end
+                  end
+                end
+              end
+            end
+          end
+        end
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::Elsif,input, i0...index, s0)
+      r0.extend(ElseIf1)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:else_if][start_index] = r0
+
+    r0
+  end
+
+  module Else0
+    def branch_or_plugin
+      elements[0]
+    end
+
+    def _
+      elements[1]
+    end
+  end
+
+  module Else1
+    def _1
+      elements[1]
+    end
+
+    def _2
+      elements[3]
+    end
+
+  end
+
+  def _nt_else
+    start_index = index
+    if node_cache[:else].has_key?(index)
+      cached = node_cache[:else][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    if has_terminal?("else", false, index)
+      r1 = instantiate_node(SyntaxNode,input, index...(index + 4))
+      @index += 4
+    else
+      terminal_parse_failure("else")
+      r1 = nil
+    end
+    s0 << r1
+    if r1
+      r2 = _nt__
+      s0 << r2
+      if r2
+        if has_terminal?("{", false, index)
+          r3 = instantiate_node(SyntaxNode,input, index...(index + 1))
+          @index += 1
+        else
+          terminal_parse_failure("{")
+          r3 = nil
+        end
+        s0 << r3
+        if r3
+          r4 = _nt__
+          s0 << r4
+          if r4
+            s5, i5 = [], index
+            loop do
+              i6, s6 = index, []
+              r7 = _nt_branch_or_plugin
+              s6 << r7
+              if r7
+                r8 = _nt__
+                s6 << r8
+              end
+              if s6.last
+                r6 = instantiate_node(SyntaxNode,input, i6...index, s6)
+                r6.extend(Else0)
+              else
+                @index = i6
+                r6 = nil
+              end
+              if r6
+                s5 << r6
+              else
+                break
+              end
+            end
+            r5 = instantiate_node(SyntaxNode,input, i5...index, s5)
+            s0 << r5
+            if r5
+              if has_terminal?("}", false, index)
+                r9 = instantiate_node(SyntaxNode,input, index...(index + 1))
+                @index += 1
+              else
+                terminal_parse_failure("}")
+                r9 = nil
+              end
+              s0 << r9
+            end
+          end
+        end
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::Else,input, i0...index, s0)
+      r0.extend(Else1)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:else][start_index] = r0
+
+    r0
+  end
+
+  module Condition0
+    def _1
+      elements[0]
+    end
+
+    def boolean_operator
+      elements[1]
+    end
+
+    def _2
+      elements[2]
+    end
+
+    def expression
+      elements[3]
+    end
+  end
+
+  module Condition1
+    def expression
+      elements[0]
+    end
+
+  end
 
-class LogStash::Config::Grammar
-  attr_accessor :eof
-  attr_accessor :config
+  def _nt_condition
+    start_index = index
+    if node_cache[:condition].has_key?(index)
+      cached = node_cache[:condition][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
 
-  def initialize
-    # BEGIN RAGEL DATA
-    
-# line 17 "grammar.rb"
-class << self
-	attr_accessor :_logstash_config_actions
-	private :_logstash_config_actions, :_logstash_config_actions=
-end
-self._logstash_config_actions = [
-	0, 1, 0, 1, 2, 1, 3, 1, 
-	4, 1, 8, 1, 9, 1, 10, 1, 
-	11, 1, 12, 2, 0, 11, 2, 1, 
-	7, 2, 2, 7, 2, 2, 9, 2, 
-	3, 7, 2, 5, 7, 2, 6, 0, 
-	2, 8, 0, 2, 10, 0, 2, 10, 
-	11, 3, 1, 7, 0, 3, 2, 7, 
-	0, 3, 3, 7, 0, 3, 5, 7, 
-	0
-]
-
-class << self
-	attr_accessor :_logstash_config_key_offsets
-	private :_logstash_config_key_offsets, :_logstash_config_key_offsets=
-end
-self._logstash_config_key_offsets = [
-	0, 0, 12, 17, 18, 19, 29, 30, 
-	31, 43, 48, 49, 50, 62, 65, 70, 
-	75, 76, 77, 78, 94, 97, 109, 121, 
-	122, 123, 126, 126, 138, 148, 148, 149, 
-	150, 153, 153, 155, 169, 183, 194, 197, 
-	203, 209, 210, 211, 223, 223, 224, 225, 
-	228, 228, 241, 241, 242, 243, 252, 253, 
-	254
-]
-
-class << self
-	attr_accessor :_logstash_config_trans_keys
-	private :_logstash_config_trans_keys, :_logstash_config_trans_keys=
-end
-self._logstash_config_trans_keys = [
-	32, 35, 95, 123, 9, 10, 48, 57, 
-	65, 90, 97, 122, 32, 35, 123, 9, 
-	10, 10, 10, 32, 35, 95, 125, 9, 
-	10, 65, 90, 97, 122, 10, 10, 32, 
-	35, 95, 123, 9, 10, 48, 57, 65, 
-	90, 97, 122, 32, 35, 123, 9, 10, 
-	10, 10, 32, 34, 35, 39, 95, 125, 
-	9, 10, 65, 90, 97, 122, 10, 34, 
-	92, 32, 35, 61, 9, 10, 32, 35, 
-	61, 9, 10, 10, 10, 62, 32, 34, 
-	35, 39, 43, 45, 91, 95, 9, 10, 
-	48, 57, 65, 90, 97, 122, 10, 34, 
-	92, 32, 34, 35, 39, 95, 125, 9, 
-	10, 65, 90, 97, 122, 32, 34, 35, 
-	39, 95, 125, 9, 10, 65, 90, 97, 
-	122, 10, 10, 10, 39, 92, 32, 35, 
-	61, 95, 9, 10, 48, 57, 65, 90, 
-	97, 122, 32, 35, 95, 125, 9, 10, 
-	65, 90, 97, 122, 10, 10, 10, 39, 
-	92, 48, 57, 32, 34, 35, 39, 95, 
-	125, 9, 10, 48, 57, 65, 90, 97, 
-	122, 32, 34, 35, 39, 95, 125, 9, 
-	10, 48, 57, 65, 90, 97, 122, 32, 
-	34, 35, 39, 95, 9, 10, 65, 90, 
-	97, 122, 10, 34, 92, 32, 35, 44, 
-	93, 9, 10, 32, 35, 44, 93, 9, 
-	10, 10, 10, 32, 34, 35, 39, 95, 
-	125, 9, 10, 65, 90, 97, 122, 10, 
-	10, 10, 39, 92, 32, 35, 44, 93, 
-	95, 9, 10, 48, 57, 65, 90, 97, 
-	122, 10, 10, 32, 35, 95, 9, 10, 
-	65, 90, 97, 122, 10, 10, 32, 35, 
-	95, 9, 10, 65, 90, 97, 122, 0
-]
-
-class << self
-	attr_accessor :_logstash_config_single_lengths
-	private :_logstash_config_single_lengths, :_logstash_config_single_lengths=
-end
-self._logstash_config_single_lengths = [
-	0, 4, 3, 1, 1, 4, 1, 1, 
-	4, 3, 1, 1, 6, 3, 3, 3, 
-	1, 1, 1, 8, 3, 6, 6, 1, 
-	1, 3, 0, 4, 4, 0, 1, 1, 
-	3, 0, 0, 6, 6, 5, 3, 4, 
-	4, 1, 1, 6, 0, 1, 1, 3, 
-	0, 5, 0, 1, 1, 3, 1, 1, 
-	3
-]
-
-class << self
-	attr_accessor :_logstash_config_range_lengths
-	private :_logstash_config_range_lengths, :_logstash_config_range_lengths=
-end
-self._logstash_config_range_lengths = [
-	0, 4, 1, 0, 0, 3, 0, 0, 
-	4, 1, 0, 0, 3, 0, 1, 1, 
-	0, 0, 0, 4, 0, 3, 3, 0, 
-	0, 0, 0, 4, 3, 0, 0, 0, 
-	0, 0, 1, 4, 4, 3, 0, 1, 
-	1, 0, 0, 3, 0, 0, 0, 0, 
-	0, 4, 0, 0, 0, 3, 0, 0, 
-	3
-]
-
-class << self
-	attr_accessor :_logstash_config_index_offsets
-	private :_logstash_config_index_offsets, :_logstash_config_index_offsets=
-end
-self._logstash_config_index_offsets = [
-	0, 0, 9, 14, 16, 18, 26, 28, 
-	30, 39, 44, 46, 48, 58, 62, 67, 
-	72, 74, 76, 78, 91, 95, 105, 115, 
-	117, 119, 123, 124, 133, 141, 142, 144, 
-	146, 150, 151, 153, 164, 175, 184, 188, 
-	194, 200, 202, 204, 214, 215, 217, 219, 
-	223, 224, 234, 235, 237, 239, 246, 248, 
-	250
-]
-
-class << self
-	attr_accessor :_logstash_config_trans_targs
-	private :_logstash_config_trans_targs, :_logstash_config_trans_targs=
-end
-self._logstash_config_trans_targs = [
-	2, 3, 1, 5, 2, 1, 1, 1, 
-	0, 2, 3, 5, 2, 0, 2, 4, 
-	2, 4, 5, 6, 8, 56, 5, 8, 
-	8, 0, 5, 7, 5, 7, 9, 10, 
-	8, 12, 9, 8, 8, 8, 0, 9, 
-	10, 12, 9, 0, 9, 11, 9, 11, 
-	12, 13, 51, 25, 27, 28, 12, 27, 
-	27, 0, 0, 14, 50, 13, 15, 16, 
-	18, 15, 0, 15, 16, 18, 15, 0, 
-	15, 17, 15, 17, 19, 0, 19, 20, 
-	30, 32, 34, 34, 37, 36, 19, 35, 
-	36, 36, 0, 0, 21, 29, 20, 22, 
-	13, 23, 25, 27, 28, 22, 27, 27, 
-	0, 22, 13, 23, 25, 27, 28, 22, 
-	27, 27, 0, 22, 24, 22, 24, 0, 
-	14, 26, 25, 25, 15, 16, 18, 27, 
-	15, 27, 27, 27, 0, 5, 6, 8, 
-	56, 5, 8, 8, 0, 20, 19, 31, 
-	19, 31, 0, 21, 33, 32, 32, 35, 
-	0, 22, 13, 23, 25, 27, 28, 22, 
-	35, 27, 27, 0, 22, 13, 23, 25, 
-	36, 28, 22, 36, 36, 36, 0, 37, 
-	38, 45, 47, 49, 37, 49, 49, 0, 
-	0, 39, 44, 38, 40, 41, 37, 43, 
-	40, 0, 40, 41, 37, 43, 40, 0, 
-	40, 42, 40, 42, 22, 13, 23, 25, 
-	27, 28, 22, 27, 27, 0, 38, 37, 
-	46, 37, 46, 0, 39, 48, 47, 47, 
-	40, 41, 37, 43, 49, 40, 49, 49, 
-	49, 0, 13, 12, 52, 12, 52, 53, 
-	54, 1, 53, 1, 1, 0, 53, 55, 
-	53, 55, 53, 54, 1, 53, 1, 1, 
-	0, 0
-]
-
-class << self
-	attr_accessor :_logstash_config_trans_actions
-	private :_logstash_config_trans_actions, :_logstash_config_trans_actions=
-end
-self._logstash_config_trans_actions = [
-	3, 3, 0, 28, 3, 0, 0, 0, 
-	17, 0, 0, 11, 0, 17, 1, 1, 
-	0, 0, 0, 0, 1, 0, 0, 1, 
-	1, 17, 1, 1, 0, 0, 3, 3, 
-	0, 3, 3, 0, 0, 0, 17, 0, 
-	0, 0, 0, 17, 1, 1, 0, 0, 
-	0, 37, 0, 37, 37, 0, 0, 37, 
-	37, 17, 17, 0, 0, 0, 5, 5, 
-	5, 5, 17, 0, 0, 0, 0, 17, 
-	1, 1, 0, 0, 0, 17, 0, 1, 
-	0, 1, 1, 1, 7, 1, 0, 1, 
-	1, 1, 17, 17, 0, 0, 0, 31, 
-	57, 31, 57, 57, 31, 31, 57, 57, 
-	17, 0, 1, 0, 1, 1, 0, 0, 
-	1, 1, 17, 1, 1, 0, 0, 17, 
-	0, 0, 0, 0, 3, 3, 3, 0, 
-	3, 0, 0, 0, 17, 9, 9, 40, 
-	9, 9, 40, 40, 17, 0, 1, 1, 
-	0, 0, 17, 0, 0, 0, 0, 0, 
-	17, 22, 49, 22, 49, 49, 22, 22, 
-	0, 49, 49, 17, 25, 53, 25, 53, 
-	0, 25, 25, 0, 0, 0, 17, 0, 
-	1, 0, 1, 1, 0, 1, 1, 17, 
-	17, 0, 0, 0, 5, 5, 5, 5, 
-	5, 17, 0, 0, 0, 0, 0, 17, 
-	1, 1, 0, 0, 34, 61, 34, 61, 
-	61, 34, 34, 61, 61, 17, 0, 1, 
-	1, 0, 0, 17, 0, 0, 0, 0, 
-	3, 3, 3, 3, 0, 3, 0, 0, 
-	0, 17, 0, 1, 1, 0, 0, 0, 
-	0, 1, 0, 1, 1, 17, 1, 1, 
-	0, 0, 13, 13, 43, 13, 43, 43, 
-	17, 0
-]
-
-class << self
-	attr_accessor :_logstash_config_eof_actions
-	private :_logstash_config_eof_actions, :_logstash_config_eof_actions=
-end
-self._logstash_config_eof_actions = [
-	0, 17, 17, 17, 17, 17, 17, 17, 
-	17, 17, 17, 17, 17, 17, 17, 17, 
-	17, 17, 17, 17, 17, 17, 17, 17, 
-	17, 17, 17, 17, 17, 17, 17, 17, 
-	17, 17, 17, 17, 17, 17, 17, 17, 
-	17, 17, 17, 17, 17, 17, 17, 17, 
-	17, 17, 17, 17, 17, 15, 19, 15, 
-	46
-]
-
-class << self
-	attr_accessor :logstash_config_start
-end
-self.logstash_config_start = 53;
-class << self
-	attr_accessor :logstash_config_first_final
-end
-self.logstash_config_first_final = 53;
-class << self
-	attr_accessor :logstash_config_error
-end
-self.logstash_config_error = 0;
+    i0, s0 = index, []
+    r1 = _nt_expression
+    s0 << r1
+    if r1
+      s2, i2 = [], index
+      loop do
+        i3, s3 = index, []
+        r4 = _nt__
+        s3 << r4
+        if r4
+          r5 = _nt_boolean_operator
+          s3 << r5
+          if r5
+            r6 = _nt__
+            s3 << r6
+            if r6
+              r7 = _nt_expression
+              s3 << r7
+            end
+          end
+        end
+        if s3.last
+          r3 = instantiate_node(SyntaxNode,input, i3...index, s3)
+          r3.extend(Condition0)
+        else
+          @index = i3
+          r3 = nil
+        end
+        if r3
+          s2 << r3
+        else
+          break
+        end
+      end
+      r2 = instantiate_node(SyntaxNode,input, i2...index, s2)
+      s0 << r2
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::Condition,input, i0...index, s0)
+      r0.extend(Condition1)
+    else
+      @index = i0
+      r0 = nil
+    end
 
-class << self
-	attr_accessor :logstash_config_en_main
-end
-self.logstash_config_en_main = 53;
+    node_cache[:condition][start_index] = r0
+
+    r0
+  end
 
+  module Expression0
+    def _1
+      elements[1]
+    end
 
-# line 154 "grammar.rl"
-    # END RAGEL DATA
+    def condition
+      elements[2]
+    end
 
-    @tokenstack = Array.new
-    @stack = Array.new
+    def _2
+      elements[3]
+    end
 
-    @types = Hash.new { |h,k| h[k] = [] }
-    @edges = []
   end
 
-  def parse(string)
-    data = string.unpack("c*")
+  def _nt_expression
+    start_index = index
+    if node_cache[:expression].has_key?(index)
+      cached = node_cache[:expression][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
 
-    # BEGIN RAGEL INIT
-    
-# line 263 "grammar.rb"
-begin
-	p ||= 0
-	pe ||= data.length
-	cs = logstash_config_start
-end
+    i0 = index
+    i1, s1 = index, []
+    if has_terminal?("(", false, index)
+      r2 = instantiate_node(SyntaxNode,input, index...(index + 1))
+      @index += 1
+    else
+      terminal_parse_failure("(")
+      r2 = nil
+    end
+    s1 << r2
+    if r2
+      r3 = _nt__
+      s1 << r3
+      if r3
+        r4 = _nt_condition
+        s1 << r4
+        if r4
+          r5 = _nt__
+          s1 << r5
+          if r5
+            if has_terminal?(")", false, index)
+              r6 = instantiate_node(SyntaxNode,input, index...(index + 1))
+              @index += 1
+            else
+              terminal_parse_failure(")")
+              r6 = nil
+            end
+            s1 << r6
+          end
+        end
+      end
+    end
+    if s1.last
+      r1 = instantiate_node(SyntaxNode,input, i1...index, s1)
+      r1.extend(Expression0)
+    else
+      @index = i1
+      r1 = nil
+    end
+    if r1
+      r0 = r1
+      r0.extend(LogStash::Config::AST::Expression)
+    else
+      r7 = _nt_negative_expression
+      if r7
+        r0 = r7
+        r0.extend(LogStash::Config::AST::Expression)
+      else
+        r8 = _nt_in_expression
+        if r8
+          r0 = r8
+          r0.extend(LogStash::Config::AST::Expression)
+        else
+          r9 = _nt_not_in_expression
+          if r9
+            r0 = r9
+            r0.extend(LogStash::Config::AST::Expression)
+          else
+            r10 = _nt_compare_expression
+            if r10
+              r0 = r10
+              r0.extend(LogStash::Config::AST::Expression)
+            else
+              r11 = _nt_regexp_expression
+              if r11
+                r0 = r11
+                r0.extend(LogStash::Config::AST::Expression)
+              else
+                r12 = _nt_rvalue
+                if r12
+                  r0 = r12
+                  r0.extend(LogStash::Config::AST::Expression)
+                else
+                  @index = i0
+                  r0 = nil
+                end
+              end
+            end
+          end
+        end
+      end
+    end
 
-# line 168 "grammar.rl"
-    # END RAGEL INIT
-
-    begin 
-      # BEGIN RAGEL EXEC 
-      
-# line 276 "grammar.rb"
-begin
-	_klen, _trans, _keys, _acts, _nacts = nil
-	_goto_level = 0
-	_resume = 10
-	_eof_trans = 15
-	_again = 20
-	_test_eof = 30
-	_out = 40
-	while true
-	_trigger_goto = false
-	if _goto_level <= 0
-	if p == pe
-		_goto_level = _test_eof
-		next
-	end
-	if cs == 0
-		_goto_level = _out
-		next
-	end
-	end
-	if _goto_level <= _resume
-	_keys = _logstash_config_key_offsets[cs]
-	_trans = _logstash_config_index_offsets[cs]
-	_klen = _logstash_config_single_lengths[cs]
-	_break_match = false
-	
-	begin
-	  if _klen > 0
-	     _lower = _keys
-	     _upper = _keys + _klen - 1
-
-	     loop do
-	        break if _upper < _lower
-	        _mid = _lower + ( (_upper - _lower) >> 1 )
-
-	        if data[p] < _logstash_config_trans_keys[_mid]
-	           _upper = _mid - 1
-	        elsif data[p] > _logstash_config_trans_keys[_mid]
-	           _lower = _mid + 1
-	        else
-	           _trans += (_mid - _keys)
-	           _break_match = true
-	           break
-	        end
-	     end # loop
-	     break if _break_match
-	     _keys += _klen
-	     _trans += _klen
-	  end
-	  _klen = _logstash_config_range_lengths[cs]
-	  if _klen > 0
-	     _lower = _keys
-	     _upper = _keys + (_klen << 1) - 2
-	     loop do
-	        break if _upper < _lower
-	        _mid = _lower + (((_upper-_lower) >> 1) & ~1)
-	        if data[p] < _logstash_config_trans_keys[_mid]
-	          _upper = _mid - 2
-	        elsif data[p] > _logstash_config_trans_keys[_mid+1]
-	          _lower = _mid + 2
-	        else
-	          _trans += ((_mid - _keys) >> 1)
-	          _break_match = true
-	          break
-	        end
-	     end # loop
-	     break if _break_match
-	     _trans += _klen
-	  end
-	end while false
-	cs = _logstash_config_trans_targs[_trans]
-	if _logstash_config_trans_actions[_trans] != 0
-		_acts = _logstash_config_trans_actions[_trans]
-		_nacts = _logstash_config_actions[_acts]
-		_acts += 1
-		while _nacts > 0
-			_nacts -= 1
-			_acts += 1
-			case _logstash_config_actions[_acts - 1]
-when 0 then
-# line 6 "grammar.rl"
-		begin
-
-    @tokenstack.push(p)
-    #puts "Mark: #{self.line(string, p)}##{self.column(string, p)}"
-  		end
-when 1 then
-# line 11 "grammar.rl"
-		begin
-
-    startpos = @tokenstack.pop
-    endpos = p
-    token = string[startpos ... endpos]
-    #puts "numeric: #{token}"
-    #puts "numeric?: #{string[startpos,50]}"
-    #puts [startpos, endpos].join(",")
-    @stack << token.to_i
-  		end
-when 2 then
-# line 21 "grammar.rl"
-		begin
-
-    startpos = @tokenstack.pop
-    endpos = p
-    token = string[startpos ... endpos]
-    #puts "string: #{token}"
-    @stack << token
-  		end
-when 3 then
-# line 29 "grammar.rl"
-		begin
-
-    startpos = @tokenstack.pop
-    endpos = p
-    token = string[startpos + 1 ... endpos - 1] # Skip quotations
-
-    # Parse escapes.
-    token.gsub(/\\./) { |m| m[1,1] }
-    #puts "quotedstring: #{token}"
-    @stack << token
-  		end
-when 4 then
-# line 40 "grammar.rl"
-		begin
-
-    @array = []
-    @stack << :array_init
-  		end
-when 5 then
-# line 45 "grammar.rl"
-		begin
-
-    while @stack.last != :array_init
-      @array.unshift @stack.pop
-    end
-    @stack.pop # pop :array_init
-
-    @stack << @array
-  		end
-when 6 then
-# line 54 "grammar.rl"
-		begin
-
-    # nothing
-  		end
-when 7 then
-# line 58 "grammar.rl"
-		begin
-
-    value = @stack.pop
-    name = @stack.pop
-    #puts "parameter: #{name} => #{value}"
-    if value.is_a?(Array)
-      @parameters[name] += value
-    else
-      @parameters[name] << value
-    end
-  		end
-when 8 then
-# line 69 "grammar.rl"
-		begin
-
-    @components ||= []
-    name = @stack.pop
-    #@components << { :name => name, :parameters => @parameters }
-    @components << { name => @parameters }
-    @parameters = Hash.new { |h,k| h[k] = [] }
-  		end
-when 9 then
-# line 77 "grammar.rl"
-		begin
-
-    @components = []
-    @parameters = Hash.new { |h,k| h[k] = [] }
-  		end
-when 10 then
-# line 82 "grammar.rl"
-		begin
-
-    name = @stack.pop
-    @config ||= Hash.new { |h,k| h[k] = [] }
-    @config[name] += @components
-    #puts "Config component: #{name}"
-  		end
-when 12 then
-# line 140 "grammar.rl"
-		begin
- 
-            # Compute line and column of the cursor (p)
-            $stderr.puts "Error at line #{self.line(string, p)}, column #{self.column(string, p)}: #{string[p .. -1].inspect}"
-            # TODO(sissel): Note what we were expecting?
-          		end
-# line 469 "grammar.rb"
-			end # action switch
-		end
-	end
-	if _trigger_goto
-		next
-	end
-	end
-	if _goto_level <= _again
-	if cs == 0
-		_goto_level = _out
-		next
-	end
-	p += 1
-	if p != pe
-		_goto_level = _resume
-		next
-	end
-	end
-	if _goto_level <= _test_eof
-	if p == eof
-	__acts = _logstash_config_eof_actions[cs]
-	__nacts =  _logstash_config_actions[__acts]
-	__acts += 1
-	while __nacts > 0
-		__nacts -= 1
-		__acts += 1
-		case _logstash_config_actions[__acts - 1]
-when 0 then
-# line 6 "grammar.rl"
-		begin
-
-    @tokenstack.push(p)
-    #puts "Mark: #{self.line(string, p)}##{self.column(string, p)}"
-  		end
-when 10 then
-# line 82 "grammar.rl"
-		begin
-
-    name = @stack.pop
-    @config ||= Hash.new { |h,k| h[k] = [] }
-    @config[name] += @components
-    #puts "Config component: #{name}"
-  		end
-when 11 then
-# line 139 "grammar.rl"
-		begin
- puts "END" 		end
-when 12 then
-# line 140 "grammar.rl"
-		begin
- 
-            # Compute line and column of the cursor (p)
-            $stderr.puts "Error at line #{self.line(string, p)}, column #{self.column(string, p)}: #{string[p .. -1].inspect}"
-            # TODO(sissel): Note what we were expecting?
-          		end
-# line 525 "grammar.rb"
-		end # eof action switch
-	end
-	if _trigger_goto
-		next
-	end
-end
-	end
-	if _goto_level <= _out
-		break
-	end
-	end
-	end
+    node_cache[:expression][start_index] = r0
+
+    r0
+  end
+
+  module NegativeExpression0
+    def _1
+      elements[1]
+    end
+
+    def _2
+      elements[3]
+    end
+
+    def condition
+      elements[4]
+    end
+
+    def _3
+      elements[5]
+    end
+
+  end
+
+  module NegativeExpression1
+    def _
+      elements[1]
+    end
+
+    def selector
+      elements[2]
+    end
+  end
+
+  def _nt_negative_expression
+    start_index = index
+    if node_cache[:negative_expression].has_key?(index)
+      cached = node_cache[:negative_expression][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0 = index
+    i1, s1 = index, []
+    if has_terminal?("!", false, index)
+      r2 = instantiate_node(SyntaxNode,input, index...(index + 1))
+      @index += 1
+    else
+      terminal_parse_failure("!")
+      r2 = nil
+    end
+    s1 << r2
+    if r2
+      r3 = _nt__
+      s1 << r3
+      if r3
+        if has_terminal?("(", false, index)
+          r4 = instantiate_node(SyntaxNode,input, index...(index + 1))
+          @index += 1
+        else
+          terminal_parse_failure("(")
+          r4 = nil
+        end
+        s1 << r4
+        if r4
+          r5 = _nt__
+          s1 << r5
+          if r5
+            r6 = _nt_condition
+            s1 << r6
+            if r6
+              r7 = _nt__
+              s1 << r7
+              if r7
+                if has_terminal?(")", false, index)
+                  r8 = instantiate_node(SyntaxNode,input, index...(index + 1))
+                  @index += 1
+                else
+                  terminal_parse_failure(")")
+                  r8 = nil
+                end
+                s1 << r8
+              end
+            end
+          end
+        end
+      end
+    end
+    if s1.last
+      r1 = instantiate_node(SyntaxNode,input, i1...index, s1)
+      r1.extend(NegativeExpression0)
+    else
+      @index = i1
+      r1 = nil
+    end
+    if r1
+      r0 = r1
+      r0.extend(LogStash::Config::AST::NegativeExpression)
+    else
+      i9, s9 = index, []
+      if has_terminal?("!", false, index)
+        r10 = instantiate_node(SyntaxNode,input, index...(index + 1))
+        @index += 1
+      else
+        terminal_parse_failure("!")
+        r10 = nil
+      end
+      s9 << r10
+      if r10
+        r11 = _nt__
+        s9 << r11
+        if r11
+          r12 = _nt_selector
+          s9 << r12
+        end
+      end
+      if s9.last
+        r9 = instantiate_node(SyntaxNode,input, i9...index, s9)
+        r9.extend(NegativeExpression1)
+      else
+        @index = i9
+        r9 = nil
+      end
+      if r9
+        r0 = r9
+        r0.extend(LogStash::Config::AST::NegativeExpression)
+      else
+        @index = i0
+        r0 = nil
+      end
+    end
+
+    node_cache[:negative_expression][start_index] = r0
+
+    r0
+  end
+
+  module InExpression0
+    def rvalue1
+      elements[0]
+    end
+
+    def _1
+      elements[1]
+    end
+
+    def in_operator
+      elements[2]
+    end
+
+    def _2
+      elements[3]
+    end
+
+    def rvalue2
+      elements[4]
+    end
+  end
+
+  def _nt_in_expression
+    start_index = index
+    if node_cache[:in_expression].has_key?(index)
+      cached = node_cache[:in_expression][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    r1 = _nt_rvalue
+    s0 << r1
+    if r1
+      r2 = _nt__
+      s0 << r2
+      if r2
+        r3 = _nt_in_operator
+        s0 << r3
+        if r3
+          r4 = _nt__
+          s0 << r4
+          if r4
+            r5 = _nt_rvalue
+            s0 << r5
+          end
+        end
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::InExpression,input, i0...index, s0)
+      r0.extend(InExpression0)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:in_expression][start_index] = r0
+
+    r0
+  end
+
+  module NotInExpression0
+    def rvalue1
+      elements[0]
+    end
+
+    def _1
+      elements[1]
+    end
+
+    def not_in_operator
+      elements[2]
+    end
+
+    def _2
+      elements[3]
+    end
+
+    def rvalue2
+      elements[4]
+    end
+  end
+
+  def _nt_not_in_expression
+    start_index = index
+    if node_cache[:not_in_expression].has_key?(index)
+      cached = node_cache[:not_in_expression][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    r1 = _nt_rvalue
+    s0 << r1
+    if r1
+      r2 = _nt__
+      s0 << r2
+      if r2
+        r3 = _nt_not_in_operator
+        s0 << r3
+        if r3
+          r4 = _nt__
+          s0 << r4
+          if r4
+            r5 = _nt_rvalue
+            s0 << r5
+          end
+        end
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::NotInExpression,input, i0...index, s0)
+      r0.extend(NotInExpression0)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:not_in_expression][start_index] = r0
+
+    r0
+  end
+
+  def _nt_in_operator
+    start_index = index
+    if node_cache[:in_operator].has_key?(index)
+      cached = node_cache[:in_operator][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    if has_terminal?("in", false, index)
+      r0 = instantiate_node(SyntaxNode,input, index...(index + 2))
+      @index += 2
+    else
+      terminal_parse_failure("in")
+      r0 = nil
+    end
+
+    node_cache[:in_operator][start_index] = r0
+
+    r0
+  end
+
+  module NotInOperator0
+    def _
+      elements[1]
+    end
+
+  end
+
+  def _nt_not_in_operator
+    start_index = index
+    if node_cache[:not_in_operator].has_key?(index)
+      cached = node_cache[:not_in_operator][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    if has_terminal?("not ", false, index)
+      r1 = instantiate_node(SyntaxNode,input, index...(index + 4))
+      @index += 4
+    else
+      terminal_parse_failure("not ")
+      r1 = nil
+    end
+    s0 << r1
+    if r1
+      r2 = _nt__
+      s0 << r2
+      if r2
+        if has_terminal?("in", false, index)
+          r3 = instantiate_node(SyntaxNode,input, index...(index + 2))
+          @index += 2
+        else
+          terminal_parse_failure("in")
+          r3 = nil
+        end
+        s0 << r3
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(SyntaxNode,input, i0...index, s0)
+      r0.extend(NotInOperator0)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:not_in_operator][start_index] = r0
+
+    r0
+  end
+
+  def _nt_rvalue
+    start_index = index
+    if node_cache[:rvalue].has_key?(index)
+      cached = node_cache[:rvalue][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
 
-# line 173 "grammar.rl"
-      # END RAGEL EXEC
-    rescue => e
-      # Compute line and column of the cursor (p)
-      raise e
+    i0 = index
+    r1 = _nt_string
+    if r1
+      r0 = r1
+    else
+      r2 = _nt_number
+      if r2
+        r0 = r2
+      else
+        r3 = _nt_selector
+        if r3
+          r0 = r3
+        else
+          r4 = _nt_array
+          if r4
+            r0 = r4
+          else
+            r5 = _nt_method_call
+            if r5
+              r0 = r5
+            else
+              r6 = _nt_regexp
+              if r6
+                r0 = r6
+              else
+                @index = i0
+                r0 = nil
+              end
+            end
+          end
+        end
+      end
+    end
+
+    node_cache[:rvalue][start_index] = r0
+
+    r0
+  end
+
+  module MethodCall0
+    def _1
+      elements[0]
+    end
+
+    def _2
+      elements[2]
+    end
+
+    def rvalue
+      elements[3]
+    end
+  end
+
+  module MethodCall1
+    def rvalue
+      elements[0]
+    end
+
+  end
+
+  module MethodCall2
+    def method
+      elements[0]
+    end
+
+    def _1
+      elements[1]
+    end
+
+    def _2
+      elements[3]
+    end
+
+    def _3
+      elements[5]
+    end
+
+  end
+
+  def _nt_method_call
+    start_index = index
+    if node_cache[:method_call].has_key?(index)
+      cached = node_cache[:method_call][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    r1 = _nt_method
+    s0 << r1
+    if r1
+      r2 = _nt__
+      s0 << r2
+      if r2
+        if has_terminal?("(", false, index)
+          r3 = instantiate_node(SyntaxNode,input, index...(index + 1))
+          @index += 1
+        else
+          terminal_parse_failure("(")
+          r3 = nil
+        end
+        s0 << r3
+        if r3
+          r4 = _nt__
+          s0 << r4
+          if r4
+            i6, s6 = index, []
+            r7 = _nt_rvalue
+            s6 << r7
+            if r7
+              s8, i8 = [], index
+              loop do
+                i9, s9 = index, []
+                r10 = _nt__
+                s9 << r10
+                if r10
+                  if has_terminal?(",", false, index)
+                    r11 = instantiate_node(SyntaxNode,input, index...(index + 1))
+                    @index += 1
+                  else
+                    terminal_parse_failure(",")
+                    r11 = nil
+                  end
+                  s9 << r11
+                  if r11
+                    r12 = _nt__
+                    s9 << r12
+                    if r12
+                      r13 = _nt_rvalue
+                      s9 << r13
+                    end
+                  end
+                end
+                if s9.last
+                  r9 = instantiate_node(SyntaxNode,input, i9...index, s9)
+                  r9.extend(MethodCall0)
+                else
+                  @index = i9
+                  r9 = nil
+                end
+                if r9
+                  s8 << r9
+                else
+                  break
+                end
+              end
+              r8 = instantiate_node(SyntaxNode,input, i8...index, s8)
+              s6 << r8
+            end
+            if s6.last
+              r6 = instantiate_node(SyntaxNode,input, i6...index, s6)
+              r6.extend(MethodCall1)
+            else
+              @index = i6
+              r6 = nil
+            end
+            if r6
+              r5 = r6
+            else
+              r5 = instantiate_node(SyntaxNode,input, index...index)
+            end
+            s0 << r5
+            if r5
+              r14 = _nt__
+              s0 << r14
+              if r14
+                if has_terminal?(")", false, index)
+                  r15 = instantiate_node(SyntaxNode,input, index...(index + 1))
+                  @index += 1
+                else
+                  terminal_parse_failure(")")
+                  r15 = nil
+                end
+                s0 << r15
+              end
+            end
+          end
+        end
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::MethodCall,input, i0...index, s0)
+      r0.extend(MethodCall2)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:method_call][start_index] = r0
+
+    r0
+  end
+
+  def _nt_method
+    start_index = index
+    if node_cache[:method].has_key?(index)
+      cached = node_cache[:method][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    r0 = _nt_bareword
+
+    node_cache[:method][start_index] = r0
+
+    r0
+  end
+
+  module CompareExpression0
+    def rvalue1
+      elements[0]
+    end
+
+    def _1
+      elements[1]
+    end
+
+    def compare_operator
+      elements[2]
+    end
+
+    def _2
+      elements[3]
+    end
+
+    def rvalue2
+      elements[4]
+    end
+  end
+
+  def _nt_compare_expression
+    start_index = index
+    if node_cache[:compare_expression].has_key?(index)
+      cached = node_cache[:compare_expression][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    r1 = _nt_rvalue
+    s0 << r1
+    if r1
+      r2 = _nt__
+      s0 << r2
+      if r2
+        r3 = _nt_compare_operator
+        s0 << r3
+        if r3
+          r4 = _nt__
+          s0 << r4
+          if r4
+            r5 = _nt_rvalue
+            s0 << r5
+          end
+        end
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::ComparisonExpression,input, i0...index, s0)
+      r0.extend(CompareExpression0)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:compare_expression][start_index] = r0
+
+    r0
+  end
+
+  def _nt_compare_operator
+    start_index = index
+    if node_cache[:compare_operator].has_key?(index)
+      cached = node_cache[:compare_operator][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0 = index
+    if has_terminal?("==", false, index)
+      r1 = instantiate_node(SyntaxNode,input, index...(index + 2))
+      @index += 2
+    else
+      terminal_parse_failure("==")
+      r1 = nil
+    end
+    if r1
+      r0 = r1
+      r0.extend(LogStash::Config::AST::ComparisonOperator)
+    else
+      if has_terminal?("!=", false, index)
+        r2 = instantiate_node(SyntaxNode,input, index...(index + 2))
+        @index += 2
+      else
+        terminal_parse_failure("!=")
+        r2 = nil
+      end
+      if r2
+        r0 = r2
+        r0.extend(LogStash::Config::AST::ComparisonOperator)
+      else
+        if has_terminal?("<=", false, index)
+          r3 = instantiate_node(SyntaxNode,input, index...(index + 2))
+          @index += 2
+        else
+          terminal_parse_failure("<=")
+          r3 = nil
+        end
+        if r3
+          r0 = r3
+          r0.extend(LogStash::Config::AST::ComparisonOperator)
+        else
+          if has_terminal?(">=", false, index)
+            r4 = instantiate_node(SyntaxNode,input, index...(index + 2))
+            @index += 2
+          else
+            terminal_parse_failure(">=")
+            r4 = nil
+          end
+          if r4
+            r0 = r4
+            r0.extend(LogStash::Config::AST::ComparisonOperator)
+          else
+            if has_terminal?("<", false, index)
+              r5 = instantiate_node(SyntaxNode,input, index...(index + 1))
+              @index += 1
+            else
+              terminal_parse_failure("<")
+              r5 = nil
+            end
+            if r5
+              r0 = r5
+              r0.extend(LogStash::Config::AST::ComparisonOperator)
+            else
+              if has_terminal?(">", false, index)
+                r6 = instantiate_node(SyntaxNode,input, index...(index + 1))
+                @index += 1
+              else
+                terminal_parse_failure(">")
+                r6 = nil
+              end
+              if r6
+                r0 = r6
+                r0.extend(LogStash::Config::AST::ComparisonOperator)
+              else
+                @index = i0
+                r0 = nil
+              end
+            end
+          end
+        end
+      end
+    end
+
+    node_cache[:compare_operator][start_index] = r0
+
+    r0
+  end
+
+  module RegexpExpression0
+    def rvalue
+      elements[0]
+    end
+
+    def _1
+      elements[1]
+    end
+
+    def regexp_operator
+      elements[2]
+    end
+
+    def _2
+      elements[3]
+    end
+
+  end
+
+  def _nt_regexp_expression
+    start_index = index
+    if node_cache[:regexp_expression].has_key?(index)
+      cached = node_cache[:regexp_expression][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    r1 = _nt_rvalue
+    s0 << r1
+    if r1
+      r2 = _nt__
+      s0 << r2
+      if r2
+        r3 = _nt_regexp_operator
+        s0 << r3
+        if r3
+          r4 = _nt__
+          s0 << r4
+          if r4
+            i5 = index
+            r6 = _nt_string
+            if r6
+              r5 = r6
+            else
+              r7 = _nt_regexp
+              if r7
+                r5 = r7
+              else
+                @index = i5
+                r5 = nil
+              end
+            end
+            s0 << r5
+          end
+        end
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::RegexpExpression,input, i0...index, s0)
+      r0.extend(RegexpExpression0)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:regexp_expression][start_index] = r0
+
+    r0
+  end
+
+  def _nt_regexp_operator
+    start_index = index
+    if node_cache[:regexp_operator].has_key?(index)
+      cached = node_cache[:regexp_operator][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0 = index
+    if has_terminal?("=~", false, index)
+      r1 = instantiate_node(SyntaxNode,input, index...(index + 2))
+      @index += 2
+    else
+      terminal_parse_failure("=~")
+      r1 = nil
+    end
+    if r1
+      r0 = r1
+      r0.extend(LogStash::Config::AST::RegExpOperator)
+    else
+      if has_terminal?("!~", false, index)
+        r2 = instantiate_node(SyntaxNode,input, index...(index + 2))
+        @index += 2
+      else
+        terminal_parse_failure("!~")
+        r2 = nil
+      end
+      if r2
+        r0 = r2
+        r0.extend(LogStash::Config::AST::RegExpOperator)
+      else
+        @index = i0
+        r0 = nil
+      end
     end
 
-    if cs < self.logstash_config_first_final
-      $stderr.puts "Error at line #{self.line(string, p)}, column #{self.column(string, p)}: #{string[p .. -1].inspect}"
-      raise "Invalid Configuration. Check syntax of config file."
+    node_cache[:regexp_operator][start_index] = r0
+
+    r0
+  end
+
+  def _nt_boolean_operator
+    start_index = index
+    if node_cache[:boolean_operator].has_key?(index)
+      cached = node_cache[:boolean_operator][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0 = index
+    if has_terminal?("and", false, index)
+      r1 = instantiate_node(SyntaxNode,input, index...(index + 3))
+      @index += 3
+    else
+      terminal_parse_failure("and")
+      r1 = nil
+    end
+    if r1
+      r0 = r1
+      r0.extend(LogStash::Config::AST::BooleanOperator)
+    else
+      if has_terminal?("or", false, index)
+        r2 = instantiate_node(SyntaxNode,input, index...(index + 2))
+        @index += 2
+      else
+        terminal_parse_failure("or")
+        r2 = nil
+      end
+      if r2
+        r0 = r2
+        r0.extend(LogStash::Config::AST::BooleanOperator)
+      else
+        if has_terminal?("xor", false, index)
+          r3 = instantiate_node(SyntaxNode,input, index...(index + 3))
+          @index += 3
+        else
+          terminal_parse_failure("xor")
+          r3 = nil
+        end
+        if r3
+          r0 = r3
+          r0.extend(LogStash::Config::AST::BooleanOperator)
+        else
+          if has_terminal?("nand", false, index)
+            r4 = instantiate_node(SyntaxNode,input, index...(index + 4))
+            @index += 4
+          else
+            terminal_parse_failure("nand")
+            r4 = nil
+          end
+          if r4
+            r0 = r4
+            r0.extend(LogStash::Config::AST::BooleanOperator)
+          else
+            @index = i0
+            r0 = nil
+          end
+        end
+      end
+    end
+
+    node_cache[:boolean_operator][start_index] = r0
+
+    r0
+  end
+
+  def _nt_selector
+    start_index = index
+    if node_cache[:selector].has_key?(index)
+      cached = node_cache[:selector][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    s0, i0 = [], index
+    loop do
+      r1 = _nt_selector_element
+      if r1
+        s0 << r1
+      else
+        break
+      end
+    end
+    if s0.empty?
+      @index = i0
+      r0 = nil
+    else
+      r0 = instantiate_node(LogStash::Config::AST::Selector,input, i0...index, s0)
     end
-    return cs
-  end # def parse
 
-  def line(str, pos)
-    return str[0 .. pos].count("\n") + 1
+    node_cache[:selector][start_index] = r0
+
+    r0
+  end
+
+  module SelectorElement0
   end
 
-  def column(str, pos)
-    return str[0 .. pos].split("\n").last.length
+  def _nt_selector_element
+    start_index = index
+    if node_cache[:selector_element].has_key?(index)
+      cached = node_cache[:selector_element][index]
+      if cached
+        cached = SyntaxNode.new(input, index...(index + 1)) if cached == true
+        @index = cached.interval.end
+      end
+      return cached
+    end
+
+    i0, s0 = index, []
+    if has_terminal?("[", false, index)
+      r1 = instantiate_node(SyntaxNode,input, index...(index + 1))
+      @index += 1
+    else
+      terminal_parse_failure("[")
+      r1 = nil
+    end
+    s0 << r1
+    if r1
+      s2, i2 = [], index
+      loop do
+        if has_terminal?('\G[^\\],]', true, index)
+          r3 = true
+          @index += 1
+        else
+          r3 = nil
+        end
+        if r3
+          s2 << r3
+        else
+          break
+        end
+      end
+      if s2.empty?
+        @index = i2
+        r2 = nil
+      else
+        r2 = instantiate_node(SyntaxNode,input, i2...index, s2)
+      end
+      s0 << r2
+      if r2
+        if has_terminal?("]", false, index)
+          r4 = instantiate_node(SyntaxNode,input, index...(index + 1))
+          @index += 1
+        else
+          terminal_parse_failure("]")
+          r4 = nil
+        end
+        s0 << r4
+      end
+    end
+    if s0.last
+      r0 = instantiate_node(LogStash::Config::AST::SelectorElement,input, i0...index, s0)
+      r0.extend(SelectorElement0)
+    else
+      @index = i0
+      r0 = nil
+    end
+
+    node_cache[:selector_element][start_index] = r0
+
+    r0
   end
-end # class LogStash::Config::Grammar
 
-#def parse(string)
-  #cfgparser = LogStash::Config::Grammar.new
-  #result = cfgparser.parse(string)
-  #puts "result %s" % result
-  #ap cfgparser.config
-#end
+end
+
+class LogStashConfigParser < Treetop::Runtime::CompiledParser
+  include LogStashConfig
+end
 
-#parse(File.open(ARGV[0]).read)
diff --git a/lib/logstash/config/grammar.rl b/lib/logstash/config/grammar.rl
deleted file mode 100644
index e3a54af7070..00000000000
--- a/lib/logstash/config/grammar.rl
+++ /dev/null
@@ -1,202 +0,0 @@
-require "logstash/namespace"
-
-%%{
-  machine logstash_config;
-
-  action mark {
-    @tokenstack.push(p)
-    #puts "Mark: #{self.line(string, p)}##{self.column(string, p)}"
-  }
-
-  action stack_numeric {
-    startpos = @tokenstack.pop
-    endpos = p
-    token = string[startpos ... endpos]
-    #puts "numeric: #{token}"
-    #puts "numeric?: #{string[startpos,50]}"
-    #puts [startpos, endpos].join(",")
-    @stack << token.to_i
-  }
-
-  action stack_string {
-    startpos = @tokenstack.pop
-    endpos = p
-    token = string[startpos ... endpos]
-    #puts "string: #{token}"
-    @stack << token
-  }
-
-  action stack_quoted_string {
-    startpos = @tokenstack.pop
-    endpos = p
-    token = string[startpos + 1 ... endpos - 1] # Skip quotations
-
-    # Parse escapes.
-    token.gsub(/\\./) { |m| m[1,1] }
-    #puts "quotedstring: #{token}"
-    @stack << token
-  }
-
-  action array_init {
-    @array = []
-    @stack << :array_init
-  }
-
-  action array_push {
-    while @stack.last != :array_init
-      @array.unshift @stack.pop
-    end
-    @stack.pop # pop :array_init
-
-    @stack << @array
-  }
-
-  action parameter_init {
-    # nothing
-  }
-
-  action parameter {
-    value = @stack.pop
-    name = @stack.pop
-    #puts "parameter: #{name} => #{value}"
-    if value.is_a?(Array)
-      @parameters[name] += value
-    else
-      @parameters[name] << value
-    end
-  }
-
-  action plugin {
-    @components ||= []
-    name = @stack.pop
-    #@components << { :name => name, :parameters => @parameters }
-    @components << { name => @parameters }
-    @parameters = Hash.new { |h,k| h[k] = [] }
-  }
-
-  action component_init {
-    @components = []
-    @parameters = Hash.new { |h,k| h[k] = [] }
-  }
-
-  action component {
-    name = @stack.pop
-    @config ||= Hash.new { |h,k| h[k] = [] }
-    @config[name] += @components
-    #puts "Config component: #{name}"
-  }
-
-  #%{ e = @tokenstack.pop; puts "Comment: #{string[e ... p]}" };
-  comment = "#" (any - [\n])* >mark ; 
-  ws = ([ \t\n] | comment)** ;
-  #ws = ([ \t\n])** ;
-
-  # TODO(sissel): Support floating point values?
-  numeric = ( ("+" | "-")?  [0-9] :>> [0-9]** ) >mark %stack_numeric;
-  quoted_string = ( 
-    ( "\"" ( ( (any - [\\"\n]) | "\\" any )* ) "\"" )
-    | ( "'" ( ( (any - [\\'\n]) | "\\" any )* ) "'" ) 
-  ) >mark %stack_quoted_string ;
-  naked_string = ( [A-Za-z_] :>> [A-Za-z0-9_]* ) >mark %stack_string ;
-  string = ( quoted_string | naked_string ) ;
-
-  # TODO(sissel): allow use of this.
-  regexp_literal = ( "/" ( ( (any - [\\'\n]) | "\\" any )* ) "/" )  ;
-
-  array = ( "[" ws string ws ("," ws string ws)* "]" ) >array_init %array_push;
-  parameter_value = ( numeric | string | array );
-  parameter = ( string ws "=>" ws parameter_value ) %parameter ;
-  parameters = ( parameter ( ws parameter )** ) >parameter_init ;
-
-  # Statement:
-  # component {
-  #   plugin_name {
-  #     bar => ...
-  #     baz => ...
-  #   }
-  #   ...
-  # }
-
-  plugin = (
-    (
-      naked_string ws "{" ws
-        parameters
-      ws "}"
-    ) | ( 
-      naked_string ws "{" ws "}" 
-    )
-  ) %plugin ; 
-
-  component = (
-    naked_string ws "{"
-      >component_init
-      ( ws plugin )**
-    ws "}"
-  ) %component ;
-
-  config = (ws component? )** ;
-
-  main := config %{ puts "END" }
-          $err { 
-            # Compute line and column of the cursor (p)
-            $stderr.puts "Error at line #{self.line(string, p)}, column #{self.column(string, p)}: #{string[p .. -1].inspect}"
-            # TODO(sissel): Note what we were expecting?
-          } ;
-}%%
-
-class LogStash::Config::Grammar
-  attr_accessor :eof
-  attr_accessor :config
-
-  def initialize
-    # BEGIN RAGEL DATA
-    %% write data;
-    # END RAGEL DATA
-
-    @tokenstack = Array.new
-    @stack = Array.new
-
-    @types = Hash.new { |h,k| h[k] = [] }
-    @edges = []
-  end
-
-  def parse(string)
-    data = string.unpack("c*")
-
-    # BEGIN RAGEL INIT
-    %% write init;
-    # END RAGEL INIT
-
-    begin 
-      # BEGIN RAGEL EXEC 
-      %% write exec;
-      # END RAGEL EXEC
-    rescue => e
-      # Compute line and column of the cursor (p)
-      raise e
-    end
-
-    if cs < self.logstash_config_first_final
-      $stderr.puts "Error at line #{self.line(string, p)}, column #{self.column(string, p)}: #{string[p .. -1].inspect}"
-      raise "Invalid Configuration. Check syntax of config file."
-    end
-    return cs
-  end # def parse
-
-  def line(str, pos)
-    return str[0 .. pos].count("\n") + 1
-  end
-
-  def column(str, pos)
-    return str[0 .. pos].split("\n").last.length
-  end
-end # class LogStash::Config::Grammar
-
-#def parse(string)
-  #cfgparser = LogStash::Config::Grammar.new
-  #result = cfgparser.parse(string)
-  #puts "result %s" % result
-  #ap cfgparser.config
-#end
-
-#parse(File.open(ARGV[0]).read)
diff --git a/lib/logstash/config/grammar.treetop b/lib/logstash/config/grammar.treetop
new file mode 100644
index 00000000000..e46fc55307a
--- /dev/null
+++ b/lib/logstash/config/grammar.treetop
@@ -0,0 +1,241 @@
+require "treetop"
+require "logstash/config/config_ast"
+
+grammar LogStashConfig
+  rule config
+    _ plugin_section _ (_ plugin_section)* _ <LogStash::Config::AST::Config>
+  end
+
+  rule comment
+    (whitespace? "#" [^\r\n]* "\r"? "\n")+ <LogStash::Config::AST::Comment>
+  end
+
+  rule _
+    (comment / whitespace)* <LogStash::Config::AST::Whitespace>
+  end
+
+  rule whitespace
+    [ \t\r\n]+ <LogStash::Config::AST::Whitespace>
+  end
+
+  rule plugin_section
+    plugin_type _ "{"
+      _ (branch_or_plugin _)*
+    "}"
+    <LogStash::Config::AST::PluginSection>
+  end
+
+  rule branch_or_plugin
+    branch / plugin
+  end
+
+  rule plugin_type
+    ("input" / "filter" / "output")
+  end
+
+  rule plugins
+    (plugin (_ plugin)*)?
+    <LogStash::Config::AST::Plugins>
+  end
+
+  rule plugin
+    name _ "{"
+      _
+      attributes:( attribute (whitespace _ attribute)*)?
+      _
+    "}"
+    <LogStash::Config::AST::Plugin>
+  end
+
+  rule name
+    (
+      ([A-Za-z0-9_-]+ <LogStash::Config::AST::Name>)
+      / string
+    )
+  end
+
+  rule attribute
+    name _ "=>" _ value
+    <LogStash::Config::AST::Attribute>
+  end
+
+  rule value
+    plugin / bareword / string / number / array / hash
+  end
+
+  rule array_value
+    bareword / string / number / array / hash
+  end
+
+  rule bareword
+    [A-Za-z_] [A-Za-z0-9_]+
+    <LogStash::Config::AST::Bareword>
+  end
+
+  rule double_quoted_string
+    ( '"' ( '\"' / !'"' . )* '"' <LogStash::Config::AST::String>)
+  end
+
+  rule single_quoted_string
+    ( "'" ( "\\'" / !"'" . )* "'" <LogStash::Config::AST::String>)
+  end
+
+  rule string
+    double_quoted_string / single_quoted_string
+  end
+
+  rule regexp
+    ( '/' ( '\/' / !'/' . )* '/'  <LogStash::Config::AST::RegExp>)
+  end
+
+  rule number
+    "-"? [0-9]+ ("." [0-9]*)?
+    <LogStash::Config::AST::Number>
+  end
+
+  rule array
+    "["
+    _
+    (
+      value (_ "," _ value)*
+    )?
+    _
+    "]"
+    <LogStash::Config::AST::Array>
+  end
+
+  rule hash
+    "{"
+      _
+      hashentries?
+      _
+    "}"
+    <LogStash::Config::AST::Hash>
+  end
+
+  rule hashentries
+    hashentry (whitespace hashentry)*
+    <LogStash::Config::AST::HashEntries>
+  end
+
+  rule hashentry
+    name:(number / bareword / string) _ "=>" _ value
+    <LogStash::Config::AST::HashEntry>
+  end
+
+  # Conditions
+  rule branch
+    if (_ else_if)* (_ else)?
+    <LogStash::Config::AST::Branch>
+  end
+
+  rule if
+    "if" _ condition _ "{" _ (branch_or_plugin _)* "}"
+    <LogStash::Config::AST::If>
+  end
+
+  rule else_if
+    "else" _ "if" _ condition _ "{" _ ( branch_or_plugin _)* "}"
+    <LogStash::Config::AST::Elsif>
+  end
+
+  rule else
+    "else" _ "{" _ (branch_or_plugin _)* "}"
+    <LogStash::Config::AST::Else>
+  end
+
+  rule condition
+    expression (_ boolean_operator _ expression)*
+    <LogStash::Config::AST::Condition>
+  end
+
+  rule expression
+    (
+        ("(" _ condition _ ")")
+      / negative_expression
+      / in_expression
+      / not_in_expression
+      / compare_expression
+      / regexp_expression
+      / rvalue
+    ) <LogStash::Config::AST::Expression>
+  end
+
+  rule negative_expression
+    (
+        ("!" _ "(" _ condition _ ")")
+      / ("!" _ selector)
+    ) <LogStash::Config::AST::NegativeExpression>
+  end
+
+  rule in_expression
+    rvalue _ in_operator _ rvalue
+    <LogStash::Config::AST::InExpression>
+  end
+
+  rule not_in_expression
+    rvalue _ not_in_operator _ rvalue
+    <LogStash::Config::AST::NotInExpression>
+  end
+
+  rule in_operator
+    "in"
+  end
+
+  rule not_in_operator
+    "not " _ "in"
+  end
+
+  rule rvalue
+    string / number / selector / array / method_call / regexp
+  end
+
+  rule method_call
+      method _ "(" _
+        (
+          rvalue ( _ "," _ rvalue )*
+        )?
+      _ ")"
+    <LogStash::Config::AST::MethodCall>
+  end
+
+  rule method
+    bareword
+  end
+
+  rule compare_expression
+    rvalue _ compare_operator _ rvalue
+    <LogStash::Config::AST::ComparisonExpression>
+  end
+  
+  rule compare_operator 
+    ("==" / "!=" / "<=" / ">=" / "<" / ">") 
+    <LogStash::Config::AST::ComparisonOperator>
+  end
+
+  rule regexp_expression
+    rvalue _  regexp_operator _ (string / regexp)
+    <LogStash::Config::AST::RegexpExpression>
+  end
+
+  rule regexp_operator
+    ("=~" / "!~") <LogStash::Config::AST::RegExpOperator>
+  end
+
+
+  rule boolean_operator
+    ("and" / "or" / "xor" / "nand")
+    <LogStash::Config::AST::BooleanOperator>
+  end
+
+  rule selector
+    selector_element+
+    <LogStash::Config::AST::Selector>
+  end
+
+  rule selector_element
+    "[" [^\],]+ "]"
+    <LogStash::Config::AST::SelectorElement>
+  end
+
+end
diff --git a/lib/logstash/config/mixin.rb b/lib/logstash/config/mixin.rb
index 7f14e702206..f4b0314db2d 100644
--- a/lib/logstash/config/mixin.rb
+++ b/lib/logstash/config/mixin.rb
@@ -1,8 +1,12 @@
+# encoding: utf-8
 
 require "logstash/namespace"
 require "logstash/config/registry"
 require "logstash/logging"
 require "logstash/util/password"
+require "logstash/version"
+require "logstash/environment"
+LogStash::Environment.load_locale!
 
 # This module is meant as a mixin to classes wishing to be configurable from
 # config files
@@ -27,6 +31,7 @@
 #
 module LogStash::Config::Mixin
   attr_accessor :config
+  attr_accessor :original_params
 
   CONFIGSORT = {
     Symbol => 0,
@@ -43,30 +48,62 @@ def self.included(base)
   def config_init(params)
     # Validation will modify the values inside params if necessary.
     # For example: converting a string to a number, etc.
-    if !self.class.validate(params)
-      @logger.error "Config validation failed."
-      exit 1
+    
+    # Keep a copy of the original config params so that we can later
+    # differentiate between explicit configuration and implicit (default)
+    # configuration.
+    @original_params = params.clone
+    
+    # store the plugin type, turns LogStash::Inputs::Base into 'input'
+    @plugin_type = self.class.ancestors.find { |a| a.name =~ /::Base$/ }.config_name
+
+    # warn about deprecated variable use
+    params.each do |name, value|
+      opts = self.class.get_config[name]
+      if opts && opts[:deprecated]
+        extra = opts[:deprecated].is_a?(String) ? opts[:deprecated] : ""
+        extra.gsub!("%PLUGIN%", self.class.config_name)
+        @logger.warn("You are using a deprecated config setting " +
+                     "#{name.inspect} set in #{self.class.config_name}. " +
+                     "Deprecated settings will continue to work, " +
+                     "but are scheduled for removal from logstash " +
+                     "in the future. #{extra} If you have any questions " +
+                     "about this, please visit the #logstash channel " +
+                     "on freenode irc.", :name => name, :plugin => self)
+      end
     end
 
     # Set defaults from 'config :foo, :default => somevalue'
     self.class.get_config.each do |name, opts|
       next if params.include?(name.to_s)
       if opts.include?(:default) and (name.is_a?(Symbol) or name.is_a?(String))
-        if opts[:validate] == :password
-          @logger.info("Converting default value in #{self.class.name} (#{name}) to password object")
-          params[name.to_s] = ::LogStash::Util::Password.new(opts[:default])
-        else
-          params[name.to_s] = opts[:default]
+        # default values should be cloned if possible
+        # cloning prevents 
+        case opts[:default]
+          when FalseClass, TrueClass, NilClass, Numeric
+            params[name.to_s] = opts[:default]
+          else
+            params[name.to_s] = opts[:default].clone
         end
       end
+
+      # Allow plugins to override default values of config settings
+      if self.class.default?(name)
+        params[name.to_s] = self.class.get_default(name)
+      end
+    end
+
+    if !self.class.validate(params)
+      raise LogStash::ConfigurationError,
+        I18n.t("logstash.agent.configuration.invalid_plugin_settings")
     end
 
     # set instance variables like '@foo'  for each config value given.
     params.each do |key, value|
-      @logger.debug("config #{self.class.name}/@#{key} = #{value.inspect}")
+      next if key[0, 1] == "@"
 
-      # set @foo
-      #ivar = "@#{key}"
+      # Set this key as an instance variable only if it doesn't start with an '@'
+      @logger.debug("config #{self.class.name}/@#{key} = #{value.inspect}")
       instance_variable_set("@#{key}", value)
     end
 
@@ -84,6 +121,15 @@ def config_name(name=nil)
       return @config_name
     end
 
+    def plugin_status(status=nil)
+      milestone(status)
+    end
+
+    def milestone(m=nil)
+      @milestone = m if !m.nil?
+      return @milestone
+    end
+
     # Define a new configuration setting
     def config(name, opts={})
       @config ||= Hash.new
@@ -91,21 +137,29 @@ def config(name, opts={})
 
       name = name.to_s if name.is_a?(Symbol)
       @config[name] = opts  # ok if this is empty
+
+      if name.is_a?(String)
+        define_method(name) { instance_variable_get("@#{name}") }
+        define_method("#{name}=") { |v| instance_variable_set("@#{name}", v) }
+      end
     end # def config
 
+    def default(name, value)
+      @defaults ||= {}
+      @defaults[name.to_s] = value
+    end
+
     def get_config
       return @config
     end # def get_config
 
-    # Define a flag 
-    def flag(*args, &block)
-      @flags ||= []
+    def get_default(name)
+      return @defaults && @defaults[name]
+    end
 
-      @flags << {
-        :args => args,
-        :block => block
-      }
-    end # def flag
+    def default?(name)
+      return @defaults && @defaults.include?(name)
+    end
 
     def options(opts)
       # add any options from this class
@@ -130,13 +184,16 @@ def inherited(subclass)
         end
       end
       subclass.instance_variable_set("@config", subconfig)
+      @@milestone_notice_given = false
     end # def inherited
 
     def validate(params)
-      @plugin_name = [ancestors[1].config_name, config_name].join("/")
-      @logger = LogStash::Logger.new(STDOUT)
+      @plugin_name = config_name
+      @plugin_type = ancestors.find { |a| a.name =~ /::Base$/ }.config_name
+      @logger = Cabin::Channel.get(LogStash)
       is_valid = true
 
+      is_valid &&= validate_milestone
       is_valid &&= validate_check_invalid_parameter_names(params)
       is_valid &&= validate_check_required_parameter_names(params)
       is_valid &&= validate_check_parameter_values(params)
@@ -144,6 +201,26 @@ def validate(params)
       return is_valid
     end # def validate
 
+    def validate_milestone
+      return true if @@milestone_notice_given
+      docmsg = "For more information about plugin milestones, see http://logstash.net/docs/#{LOGSTASH_VERSION}/plugin-milestones "
+      plugin_type = ancestors.find { |a| a.name =~ /::Base$/ }.config_name
+      case @milestone
+        when 0,1,2
+          @logger.warn(I18n.t("logstash.plugin.milestone.#{@milestone}", 
+                              :type => plugin_type, :name => @config_name,
+                              :LOGSTASH_VERSION => LOGSTASH_VERSION))
+        when 3
+          # No message to log for milestone 3 plugins.
+        when nil
+          raise "#{@config_name} must set a milestone. #{docmsg}"
+        else
+          raise "#{@config_name} set an invalid plugin status #{@milestone}. Valid values are 0, 1, 2, or 3. #{docmsg}"
+      end
+      @@milestone_notice_given = true
+      return true
+    end
+
     def validate_check_invalid_parameter_names(params)
       invalid_params = params.keys
       # Filter out parameters that match regexp keys.
@@ -177,8 +254,9 @@ def validate_check_required_parameter_names(params)
         elsif config_key.is_a?(String)
           next if params.keys.member?(config_key)
         end
-        @logger.error("Missing required parameter '#{config_key}' for " \
-                      "#{@plugin_name}")
+        @logger.error(I18n.t("logstash.agent.configuration.setting_missing",
+                             :setting => config_key, :plugin => @plugin_name,
+                             :type => @plugin_type))
         is_valid = false
       end
 
@@ -191,18 +269,8 @@ def validate_check_parameter_values(params)
       #   config /foo.*/ => ... 
       is_valid = true
 
-      # string/symbols are first, then regexes.
-      config_keys = @config.keys.sort do |a,b|
-        CONFIGSORT[a.class] <=> CONFIGSORT[b.class] 
-      end
-      #puts "Key order: #{config_keys.inspect}"
-      #puts @config.keys.inspect
-
       params.each do |key, value|
-        config_keys.each do |config_key|
-          #puts
-          #puts "Candidate: #{key.inspect} / #{value.inspect}"
-          #puts "Config: #{config_key} / #{config_val} "
+        @config.keys.each do |config_key|
           next unless (config_key.is_a?(Regexp) && key =~ config_key) \
                       || (config_key.is_a?(String) && key == config_key)
           config_val = @config[config_key][:validate]
@@ -213,7 +281,11 @@ def validate_check_parameter_values(params)
             # Used for converting values in the config to proper objects.
             params[key] = result if !result.nil?
           else
-            @logger.error("Failed config #{@plugin_name}/#{key}: #{result} (#{value.inspect})")
+            @logger.error(I18n.t("logstash.agent.configuration.setting_invalid",
+                                 :plugin => @plugin_name, :type => @plugin_type,
+                                 :setting => key, :value => value.inspect,
+                                 :value_type => config_val,
+                                 :note => result))
           end
           #puts "Result: #{key} / #{result.inspect} / #{success}"
           is_valid &&= success
@@ -244,9 +316,8 @@ def validate_value(value, validator)
 
       if validator.nil?
         return true
-      elsif validator.is_a?(Proc)
-        return validator.call(value)
       elsif validator.is_a?(Array)
+        value = [*value]
         if value.size > 1
           return false, "Expected one of #{validator.inspect}, got #{value.inspect}"
         end
@@ -258,13 +329,40 @@ def validate_value(value, validator)
       elsif validator.is_a?(Symbol)
         # TODO(sissel): Factor this out into a coersion method?
         # TODO(sissel): Document this stuff.
+        value = hash_or_array(value)
+
         case validator
+          when :codec
+            if value.first.is_a?(String)
+              value = LogStash::Plugin.lookup("codec", value.first).new
+              return true, value
+            else
+              value = value.first
+              return true, value
+            end
           when :hash
+            if value.is_a?(Hash)
+              return true, value
+            end
+
             if value.size % 2 == 1
               return false, "This field must contain an even number of items, got #{value.size}"
             end
-            # Use Hash[] (works in 1.8.7, anyway) to coerce into a hash.
-            result = Hash[*value]
+
+            # Convert the array the config parser produces into a hash.
+            result = {}
+            value.each_slice(2) do |key, value|
+              entry = result[key]
+              if entry.nil?
+                result[key] = value
+              else
+                if entry.is_a?(Array)
+                  entry << value
+                else
+                  result[key] = [entry, value]
+                end
+              end
+            end
           when :array
             result = value
           when :string
@@ -274,22 +372,42 @@ def validate_value(value, validator)
             result = value.first
           when :number
             if value.size > 1 # only one value wanted
-              return false, "Expected number, got #{value.inspect}"
+              return false, "Expected number, got #{value.inspect} (type #{value.class})"
             end
-            if value.first.to_s.to_i.to_s != value.first.to_s
-              return false, "Expected number, got #{value.first.inspect}"
-            end
-            result = value.first.to_i
+
+            v = value.first
+            case v
+              when Numeric
+                result = v
+              when String
+                if v.to_s.to_f.to_s != v.to_s \
+                   && v.to_s.to_i.to_s != v.to_s
+                  return false, "Expected number, got #{v.inspect} (type #{v})"
+                end
+                if v.include?(".")
+                  # decimal value, use float.
+                  result = v.to_f
+                else
+                  result = v.to_i
+                end
+            end # case v
           when :boolean
             if value.size > 1 # only one value wanted
               return false, "Expected boolean, got #{value.inspect}"
             end
 
-            if value.first !~ /^(true|false)$/
-              return false, "Expected boolean 'true' or 'false', got #{value.first.inspect}"
-            end
+            bool_value = value.first
+            if !!bool_value == bool_value
+              # is_a does not work for booleans
+              # we have Boolean and not a string
+              result = bool_value
+            else
+              if bool_value !~ /^(true|false)$/
+                return false, "Expected boolean 'true' or 'false', got #{bool_value.inspect}"
+              end
 
-            result = (value.first == "true")
+              result = (bool_value == "true")
+            end
           when :ipaddr
             if value.size > 1 # only one value wanted
               return false, "Expected IPaddr, got #{value.inspect}"
@@ -311,6 +429,23 @@ def validate_value(value, validator)
             end
 
             result = ::LogStash::Util::Password.new(value.first)
+          when :path
+            if value.size > 1 # Only 1 value wanted
+              return false, "Expected path (one value), got #{value.size} values?"
+            end
+
+            # Paths must be absolute
+            #if !Pathname.new(value.first).absolute?
+              #return false, "Require absolute path, got relative path #{value.first}?"
+            #end
+
+            if !File.exists?(value.first) # Check if the file exists
+              return false, "File does not exist or cannot be opened #{value.first}"
+            end
+
+            result = value.first
+          else
+            return false, "Unknown validator symbol #{validator}"
         end # case validator
       else
         return false, "Unknown validator #{validator.class}"
@@ -319,5 +454,12 @@ def validate_value(value, validator)
       # Return the validator for later use, like with type coercion.
       return true, result
     end # def validate_value
+
+    def hash_or_array(value)
+      if !value.is_a?(Hash)
+        value = [*value] # coerce scalar to array if necessary
+      end
+      return value
+    end
   end # module LogStash::Config::DSL
 end # module LogStash::Config
diff --git a/lib/logstash/config/registry.rb b/lib/logstash/config/registry.rb
index 46df5b534f8..8463716cf06 100644
--- a/lib/logstash/config/registry.rb
+++ b/lib/logstash/config/registry.rb
@@ -1,3 +1,4 @@
+# encoding: utf-8
 require "logstash/namespace"
 
 # Global config registry.
diff --git a/lib/logstash/config/test.conf b/lib/logstash/config/test.conf
index 5b28b4192c1..af69223e761 100644
--- a/lib/logstash/config/test.conf
+++ b/lib/logstash/config/test.conf
@@ -1,5 +1,5 @@
 input {
-  amqp {
+  rabbitmq {
     port => 12345 
     tag => [ a, b, c ]
   }
diff --git a/lib/logstash/environment.rb b/lib/logstash/environment.rb
new file mode 100644
index 00000000000..df483e32ba6
--- /dev/null
+++ b/lib/logstash/environment.rb
@@ -0,0 +1,107 @@
+require "logstash/errors"
+require 'logstash/version'
+
+module LogStash
+  module Environment
+    extend self
+
+    LOGSTASH_HOME = ::File.expand_path(::File.join(::File.dirname(__FILE__), "..", ".."))
+    JAR_DIR = ::File.join(LOGSTASH_HOME, "vendor", "jar")
+    ELASTICSEARCH_DIR = ::File.join(LOGSTASH_HOME, "vendor", "elasticsearch")
+    BUNDLE_DIR = ::File.join(LOGSTASH_HOME, "vendor", "bundle")
+    PLUGINS_DIR = ::File.join(LOGSTASH_HOME, "vendor", "plugins")
+    GEMFILE_PATH = ::File.join(LOGSTASH_HOME, "tools", "Gemfile")
+
+    # loads currently embedded elasticsearch jars
+    # @raise LogStash::EnvironmentError if not running under JRuby or if no jar files are found
+    def load_elasticsearch_jars!
+      raise(LogStash::EnvironmentError, "JRuby is required") unless jruby?
+
+      require "java"
+      jars_path = ::File.join(ELASTICSEARCH_DIR, "**", "*.jar")
+      jar_files = Dir.glob(jars_path)
+
+      raise(LogStash::EnvironmentError, "Could not find Elasticsearch jar files under #{ELASTICSEARCH_DIR}") if jar_files.empty?
+
+      jar_files.each do |jar|
+        loaded = require jar
+        puts("Loaded #{jar}") if $DEBUG && loaded
+      end
+    end
+
+    def gem_home
+      ::File.join(BUNDLE_DIR, ruby_engine, gem_ruby_version)
+    end
+
+    def plugins_home
+      # plugins are gems, respect same path structure as core gems_home
+      ::File.join(PLUGINS_DIR, ruby_engine, gem_ruby_version)
+    end
+
+    def set_gem_paths!
+      require ::File.join(BUNDLE_DIR, "bundler", "setup.rb")
+      ENV["GEM_PATH"] = plugins_home
+      ENV["GEM_HOME"] = plugins_home
+      Gem.paths = plugins_home
+    end
+
+    # @return [String] major.minor ruby version, ex 1.9
+    def ruby_abi_version
+      RUBY_VERSION[/(\d+\.\d+)(\.\d+)*/, 1]
+    end
+
+    # @return [String] the ruby version string bundler uses to craft its gem path
+    def gem_ruby_version
+      RbConfig::CONFIG["ruby_version"]
+    end
+
+    # @return [String] jruby, ruby, rbx, ...
+    def ruby_engine
+      RUBY_ENGINE
+    end
+
+    def jruby?
+      @jruby ||= !!(RUBY_PLATFORM == "java")
+    end
+
+    def vendor_path(path)
+      return ::File.join(LOGSTASH_HOME, "vendor", path)
+    end
+
+    def plugin_path(path)
+      return ::File.join(LOGSTASH_HOME, "lib", "logstash", path)
+    end
+
+    def pattern_path(path)
+      return ::File.join(LOGSTASH_HOME, "patterns", path)
+    end
+
+    def locales_path(path)
+      return ::File.join(LOGSTASH_HOME, "locales", path)
+    end
+
+    def load_logstash_gemspec!
+      logstash_spec = Gem::Specification.new do |gem|
+        gem.authors       = ["Jordan Sissel", "Pete Fritchman"]
+        gem.email         = ["jls@semicomplete.com", "petef@databits.net"]
+        gem.description   = %q{scalable log and event management (search, archive, pipeline)}
+        gem.summary       = %q{logstash - log and event management}
+        gem.homepage      = "http://logstash.net/"
+        gem.license       = "Apache License (2.0)"
+
+        gem.name          = "logstash"
+        gem.version       = LOGSTASH_VERSION
+      end
+
+      Gem::Specification.add_spec logstash_spec
+    end
+
+    def load_locale!
+      require "i18n"
+      I18n.enforce_available_locales = true
+      I18n.load_path << LogStash::Environment.locales_path("en.yml")
+      I18n.reload!
+      fail "No locale? This is a bug." if I18n.available_locales.empty?
+    end
+  end
+end
diff --git a/lib/logstash/errors.rb b/lib/logstash/errors.rb
new file mode 100644
index 00000000000..f91d4774209
--- /dev/null
+++ b/lib/logstash/errors.rb
@@ -0,0 +1,11 @@
+# encoding: utf-8
+module LogStash
+  class Error < ::StandardError; end
+  class EnvironmentError < Error; end
+  class ConfigurationError < Error; end
+  class PluginLoadingError < Error; end
+  class ShutdownSignal < StandardError; end
+
+  class Bug < Error; end
+  class ThisMethodWasRemoved < Bug; end
+end
diff --git a/lib/logstash/event.rb b/lib/logstash/event.rb
index 32ba14d974d..df95bc30bcd 100644
--- a/lib/logstash/event.rb
+++ b/lib/logstash/event.rb
@@ -1,173 +1,317 @@
-require "json"
-require "logstash/time"
+# encoding: utf-8
+require "time"
+require "date"
+require "cabin"
 require "logstash/namespace"
-require "uri"
+require "logstash/util/fieldreference"
+require "logstash/util/accessors"
+require "logstash/timestamp"
+require "logstash/json"
 
-# General event type. Will expand this in the future.
+# transcient pipeline events for normal in-flow signaling as opposed to
+# flow altering exceptions. for now having base classes is adequate and
+# in the future it might be necessary to refactor using like a BaseEvent
+# class to have a common interface for all pileline events to support
+# eventual queueing persistence for example, TBD.
+class LogStash::ShutdownEvent; end
+class LogStash::FlushEvent; end
+
+# the logstash event object.
+#
+# An event is simply a tuple of (timestamp, data).
+# The 'timestamp' is an ISO8601 timestamp. Data is anything - any message,
+# context, references, etc that are relevant to this event.
+#
+# Internally, this is represented as a hash with only two guaranteed fields.
+#
+# * "@timestamp" - an ISO8601 timestamp representing the time the event
+#   occurred at.
+# * "@version" - the version of the schema. Currently "1"
+#
+# They are prefixed with an "@" symbol to avoid clashing with your
+# own custom fields.
+#
+# When serialized, this is represented in JSON. For example:
+#
+#     {
+#       "@timestamp": "2013-02-09T20:39:26.234Z",
+#       "@version": "1",
+#       message: "hello world"
+#     }
 class LogStash::Event
-  public
-  def initialize(data=Hash.new)
-    @@date_parser ||= org.joda.time.format.ISODateTimeFormat.dateTimeParser.withOffsetParsed
+  class DeprecatedMethod < StandardError; end
+
+  CHAR_PLUS = "+"
+  TIMESTAMP = "@timestamp"
+  VERSION = "@version"
+  VERSION_ONE = "1"
+  TIMESTAMP_FAILURE_TAG = "_timestampparsefailure"
+  TIMESTAMP_FAILURE_FIELD = "_@timestamp"
+
+  # Floats outside of these upper and lower bounds are forcibly converted
+  # to scientific notation by Float#to_s
+  MIN_FLOAT_BEFORE_SCI_NOT = 0.0001
+  MAX_FLOAT_BEFORE_SCI_NOT = 1000000000000000.0
 
+  public
+  def initialize(data = {})
+    @logger = Cabin::Channel.get(LogStash)
     @cancelled = false
-    @data = {
-      "@source" => "unknown",
-      "@type" => nil,
-      "@tags" => [],
-      "@fields" => {},
-    }.merge(data)
-
-    if !@data.include?("@timestamp")
-      @data["@timestamp"] = LogStash::Time.now.utc.to_iso8601
+    @data = data
+    @accessors = LogStash::Util::Accessors.new(data)
+    @data[VERSION] ||= VERSION_ONE
+    @data[TIMESTAMP] = init_timestamp(@data[TIMESTAMP])
+
+    @metadata = if @data.include?("@metadata")
+      @data.delete("@metadata")
+    else
+      {}
     end
+    @metadata_accessors = LogStash::Util::Accessors.new(@metadata)
   end # def initialize
 
-  public
-  def self.from_json(json)
-    return LogStash::Event.new(JSON.parse(json))
-  end # def self.from_json
-
   public
   def cancel
     @cancelled = true
-  end
+  end # def cancel
+
+  public
+  def uncancel
+    @cancelled = false
+  end # def uncancel
 
   public
   def cancelled?
     return @cancelled
-  end
+  end # def cancelled?
 
+  # Create a deep-ish copy of this event.
   public
-  def to_s
-    return self.sprintf("%{@timestamp} %{@source}: %{@message}")
-  end # def to_s
+  def clone
+    copy = {}
+    @data.each do |k,v|
+      # TODO(sissel): Recurse if this is a hash/array?
+      copy[k] = begin v.clone rescue v end
+    end
+    return self.class.new(copy)
+  end # def clone
+
+  if RUBY_ENGINE == "jruby"
+    public
+    def to_s
+      return self.sprintf("%{+yyyy-MM-dd'T'HH:mm:ss.SSSZ} %{host} %{message}")
+    end # def to_s
+  else
+    public
+    def to_s
+      return self.sprintf("#{timestamp.to_iso8601} %{host} %{message}")
+    end # def to_s
+  end
 
   public
-  def timestamp; @data["@timestamp"]; end # def timestamp
-  def timestamp=(val); @data["@timestamp"] = val; end # def timestamp=
+  def timestamp; return @data[TIMESTAMP]; end # def timestamp
+  def timestamp=(val); return @data[TIMESTAMP] = val; end # def timestamp=
+
+  def unix_timestamp
+    raise DeprecatedMethod
+  end # def unix_timestamp
+
+  def ruby_timestamp
+    raise DeprecatedMethod
+  end # def unix_timestamp
 
+  # field-related access
+  METADATA = "@metadata".freeze
+  METADATA_BRACKETS = "[#{METADATA}]".freeze
   public
-  def source; @data["@source"]; end # def source
-  def source=(val) 
-    uri = URI.parse(val) rescue nil
-    val = uri if uri
-    if val.is_a?(URI)
-      @data["@source"] = val.to_s
-      @data["@source_host"] = val.host
-      @data["@source_path"] = val.path
+  def [](fieldref)
+    if fieldref.start_with?(METADATA_BRACKETS)
+      @metadata_accessors.get(fieldref[METADATA_BRACKETS.length .. -1])
+    elsif fieldref == METADATA
+      @metadata
     else
-      @data["@source"] = val
-      @data["@source_host"] = val
+      @accessors.get(fieldref)
     end
-  end # def source=
+  end # def []
 
   public
-  def message; @data["@message"]; end # def message
-  def message=(val); @data["@message"] = val; end # def message=
+  def []=(fieldref, value)
+    if fieldref == TIMESTAMP && !value.is_a?(LogStash::Timestamp)
+      raise TypeError, "The field '@timestamp' must be a (LogStash::Timestamp, not a #{value.class} (#{value})"
+    end
+    if fieldref.start_with?(METADATA_BRACKETS)
+      @metadata_accessors.set(fieldref[METADATA_BRACKETS.length .. -1], value)
+    elsif fieldref == METADATA
+      @metadata = value
+    else
+      @accessors.set(fieldref, value)
+    end
+  end # def []=
 
   public
-  def type; @data["@type"]; end # def type
-  def type=(val); @data["@type"] = val; end # def type=
+  def fields
+    raise DeprecatedMethod
+  end
 
   public
-  def tags; @data["@tags"]; end # def tags
-  def tags=(val); @data["@tags"] = val; end # def tags=
+  def to_json(*args)
+    # ignore arguments to respect accepted to_json method signature
+    LogStash::Json.dump(@data)
+  end # def to_json
 
-  # field-related access
-  public
-  def [](key)
-    # If the key isn't in fields and it starts with an "@" sign, get it out of data instead of fields
-    if ! @data["@fields"].has_key?(key) and key.slice(0,1) == "@"
-      return @data[key]
-    # Exists in @fields (returns value) or doesn't start with "@" (return null)
-    else
-      return @data["@fields"][key]
-    end
-  end # def []
-  
-  # TODO(sissel): the semantics of [] and []= are now different in that
-  # []= only allows you to assign to only fields (not metadata), but
-  # [] allows you to read fields and metadata.
-  # We should fix this. Metadata is really a namespace issue, anyway.
-  def []=(key, value); @data["@fields"][key] = value end # def []=
-  def fields; return @data["@fields"] end # def fields
-  
   public
-  def to_json(*args); return @data.to_json(*args) end # def to_json
-  def to_hash; return @data end # def to_hash
+  def to_hash
+    @data
+  end # def to_hash
 
   public
   def overwrite(event)
-    @data = event.to_hash
+    # pickup new event @data and also pickup @accessors
+    # otherwise it will be pointing on previous data
+    @data = event.instance_variable_get(:@data)
+    @accessors = event.instance_variable_get(:@accessors)
+
+    #convert timestamp if it is a String
+    if @data[TIMESTAMP].is_a?(String)
+      @data[TIMESTAMP] = LogStash::Timestamp.parse_iso8601(@data[TIMESTAMP])
+    end
   end
 
   public
-  def include?(key); return @data.include?(key) end
+  def include?(key)
+    return !self[key].nil?
+  end # def include?
 
   # Append an event to this one.
   public
   def append(event)
-    self.message += "\n" + event.message 
-    self.tags |= event.tags
+    # non-destructively merge that event with ourselves.
 
-    # Append all fields
-    event.fields.each do |name, value|
-      if self.fields.include?(name)
-        self.fields[name] |= value
-      else
-        self.fields[name] = value
-      end
-    end # event.fields.each
-  end # def append
+    # no need to reset @accessors here because merging will not disrupt any existing field paths
+    # and if new ones are created they will be picked up.
+    LogStash::Util.hash_merge(@data, event.to_hash)
+  end # append
+
+  # Remove a field or field reference. Returns the value of that field when
+  # deleted
+  public
+  def remove(fieldref)
+    @accessors.del(fieldref)
+  end # def remove
 
   # sprintf. This could use a better method name.
-  # The idea is to take an event and convert it to a string based on 
+  # The idea is to take an event and convert it to a string based on
   # any format values, delimited by %{foo} where 'foo' is a field or
   # metadata member.
   #
-  # For example, if the event has @type == "foo" and @source == "bar"
+  # For example, if the event has type == "foo" and host == "bar"
   # then this string:
-  #   "type is %{@type} and source is %{@source}"
+  #   "type is %{type} and source is %{host}"
   # will return
   #   "type is foo and source is bar"
   #
   # If a %{name} value is an array, then we will join by ','
   # If a %{name} value does not exist, then no substitution occurs.
   #
-  # TODO(sissel): It is not clear what the value of a field that 
+  # TODO(sissel): It is not clear what the value of a field that
   # is an array (or hash?) should be. Join by comma? Something else?
   public
   def sprintf(format)
+    if format.is_a?(Float) and
+        (format < MIN_FLOAT_BEFORE_SCI_NOT or format >= MAX_FLOAT_BEFORE_SCI_NOT) then
+      format = ("%.15f" % format).sub(/0*$/,"")
+    else
+      format = format.to_s
+    end
+    if format.index("%").nil?
+      return format
+    end
+
     return format.gsub(/%\{[^}]+\}/) do |tok|
       # Take the inside of the %{ ... }
       key = tok[2 ... -1]
 
-      if key[0,1] == "+"
-        # Parse event.timestamp with  
-        datetime = @@date_parser.parseDateTime(self.timestamp)
-        format = key[1 .. -1]
-        datetime.toString(format) # return requested time format
-      else 
-        # Use an event field.
+      if key == "+%s"
+        # Got %{+%s}, support for unix epoch time
+        next @data[TIMESTAMP].to_i
+      elsif key[0,1] == "+"
+        t = @data[TIMESTAMP]
+        formatter = org.joda.time.format.DateTimeFormat.forPattern(key[1 .. -1])\
+          .withZone(org.joda.time.DateTimeZone::UTC)
+        #next org.joda.time.Instant.new(t.tv_sec * 1000 + t.tv_usec / 1000).toDateTime.toString(formatter)
+        # Invoke a specific Instant constructor to avoid this warning in JRuby
+        #  > ambiguous Java methods found, using org.joda.time.Instant(long)
+        org.joda.time.Instant.java_class.constructor(Java::long).new_instance(
+          t.tv_sec * 1000 + t.tv_usec / 1000
+        ).to_java.toDateTime.toString(formatter)
+      else
         value = self[key]
-        if value.nil?
-          tok # leave the %{foo} if this field does not exist in this event.
-        elsif value.is_a?(Array)
-          value.join(",") # Join by ',' if value is an rray
-        else
-          value # otherwise return the value
-        end
-      end
-    end
+        case value
+          when nil
+            tok # leave the %{foo} if this field does not exist in this event.
+          when Array
+            value.join(",") # Join by ',' if value is an array
+          when Hash
+            LogStash::Json.dump(value) # Convert hashes to json
+          else
+            value # otherwise return the value
+        end # case value
+      end # 'key' checking
+    end # format.gsub...
   end # def sprintf
 
+  def tag(value)
+    # Generalize this method for more usability
+    self["tags"] ||= []
+    self["tags"] << value unless self["tags"].include?(value)
+  end
+
+  private
+
+  def init_timestamp(o)
+    begin
+      timestamp = o ? LogStash::Timestamp.coerce(o) : LogStash::Timestamp.now
+      return timestamp if timestamp
+
+      @logger.warn("Unrecognized #{TIMESTAMP} value, setting current time to #{TIMESTAMP}, original in #{TIMESTAMP_FAILURE_FIELD}field", :value => o.inspect)
+    rescue LogStash::TimestampParserError => e
+      @logger.warn("Error parsing #{TIMESTAMP} string, setting current time to #{TIMESTAMP}, original in #{TIMESTAMP_FAILURE_FIELD} field", :value => o.inspect, :exception => e.message)
+    end
+
+    @data["tags"] ||= []
+    @data["tags"] << TIMESTAMP_FAILURE_TAG unless @data["tags"].include?(TIMESTAMP_FAILURE_TAG)
+    @data[TIMESTAMP_FAILURE_FIELD] = o
+
+    LogStash::Timestamp.now
+  end
+
+  public
+  def to_hash_with_metadata
+    if @metadata.nil?
+      to_hash
+    else
+      to_hash.merge("@metadata" => @metadata)
+    end
+  end
+
   public
-  def ==(other)
-    #puts "#{self.class.name}#==(#{other.inspect})"
-    if !other.is_a?(self.class)
-      return false
+  def to_json_with_metadata(*args)
+    # ignore arguments to respect accepted to_json method signature
+    LogStash::Json.dump(to_hash_with_metadata)
+  end # def to_json
+
+  def self.validate_value(value)
+    case value
+    when String
+      raise("expected UTF-8 encoding for value=#{value}, encoding=#{value.encoding.inspect}") unless value.encoding == Encoding::UTF_8
+      raise("invalid UTF-8 encoding for value=#{value}, encoding=#{value.encoding.inspect}") unless value.valid_encoding?
+      value
+    when Array
+      value.each{|v| validate_value(v)} # don't map, return original object
+      value
+    else
+      value
     end
+  end
 
-    return other.to_hash == self.to_hash
-  end # def ==
 end # class LogStash::Event
diff --git a/lib/logstash/file/manager.rb b/lib/logstash/file/manager.rb
deleted file mode 100644
index 35e2beca5cf..00000000000
--- a/lib/logstash/file/manager.rb
+++ /dev/null
@@ -1,104 +0,0 @@
-require "filewatch/buftok" # rubygem 'filewatch' - for BufferedTokenizer
-require "filewatch/tailglob" # rubygem 'filewatch'
-require "logstash/logging"
-require "logstash/namespace"
-require "logstash/util"
-require "set"
-require "socket" # for Socket.gethostname
-
-
-class LogStash::File::Manager
-  attr_reader :logger
-
-  public
-  def initialize(output_queue)
-    @tail = FileWatch::TailGlob.new
-    @watching = Hash.new
-    @watching_lock = Mutex.new
-    @file_threads = {}
-    @main_thread = nil
-    @output_queue = nil
-    @hostname = Socket.gethostname
-
-    self.logger = LogStash::Logger.new(STDOUT)
-  end # def initialize
-
-  public
-  def logger=(logger)
-    @logger = logger
-    @tail.logger = logger
-  end # def logger=
-
-  public
-  def run(queue)
-    @output_queue = queue
-    @main_thread ||= Thread.new { watcher }
-  end
-
-  public
-  def watch(paths, config)
-    @watching_lock.synchronize do
-      paths.each do |path|
-        if @watching[path]
-          raise ValueError, "cannot watch the same path #{path} more than once"
-        end
-        @logger.debug(["watching file", {:path => path, :config => config}])
-
-        # TODO(sissel): inputs/base should do this.
-        config["tag"] ||= []
-        #if !config["tag"].member?(config["type"])
-          #config["tag"] << config["type"]
-        #end
-
-        # TODO(sissel): Need to support file rotation, globs, etc
-        begin
-          tailconf = { }
-          if config.include?("exclude")
-            tailconf[:exclude] = config["exclude"]
-          end
-
-          # Register a @tail callback for new paths
-          @tail.tail(path, tailconf) do |fullpath|
-            @logger.info("New file found: #{fullpath}")
-            @watching[fullpath] = config
-          end
-          # TODO(sissel): Make FileWatch emit real exceptions
-        rescue RuntimeError
-          @logger.info("Failed to start watch on #{path.inspect}")
-          # Ignore.
-        end
-      end
-    end
-  end
-
-  private
-  def watcher
-    LogStash::Util::set_thread_name(self.class.name)
-    @buffers = Hash.new { |h,k| h[k] = BufferedTokenizer.new }
-    begin
-      @tail.subscribe do |path, data|
-        # TODO(sissel): 'path' might not be watched since we could
-        # be watching a glob.
-        #
-        # Maybe extend @tail.tail to accept a extra args that it will
-        # pass to subscribe's callback?
-        config = @watching[path]
-        @logger.debug(["Event from tail", { :path => path, :config => config }])
-        @buffers[path].extract(data).each do |line|
-          e = LogStash::Event.new({
-            "@message" => line,
-            "@type" => config["type"],
-            "@tags" => config["tag"].dup,
-          })
-          e.source = "file://#{@hostname}#{path}"
-          @logger.debug(["New event from file input", path, e])
-          @output_queue << e
-        end
-      end
-    rescue Exception => e
-      @logger.warn(["Exception in #{self.class} thread, retrying", e, e.backtrace])
-      sleep 0.3
-      retry
-    end
-  end # def watcher
-end # class LogStash::File::Manager
diff --git a/lib/logstash/filters.rb b/lib/logstash/filters.rb
deleted file mode 100644
index 93297a55e72..00000000000
--- a/lib/logstash/filters.rb
+++ /dev/null
@@ -1,17 +0,0 @@
-require "logstash/namespace"
-
-module LogStash::Filters
-  public
-  def self.from_name(name, *args)
-    # TODO(sissel): Add error handling
-    # TODO(sissel): Allow plugin paths
-    klass = name.capitalize
-
-    # Load the class if we haven't already.
-    require "logstash/filters/#{name}"
-
-    # Get the class name from the Filters namespace and create a new instance.
-    # for name == 'foo' this will call LogStash::Filters::Foo.new
-    LogStash::Filters.const_get(klass).new(*args)
-  end # def from_url
-end # module LogStash::Filters
diff --git a/lib/logstash/filters/anonymize.rb b/lib/logstash/filters/anonymize.rb
new file mode 100644
index 00000000000..8ec0b77031f
--- /dev/null
+++ b/lib/logstash/filters/anonymize.rb
@@ -0,0 +1,95 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# Anonymize fields using by replacing values with a consistent hash.
+class LogStash::Filters::Anonymize < LogStash::Filters::Base
+  config_name "anonymize"
+  milestone 1
+
+  # The fields to be anonymized
+  config :fields, :validate => :array, :required => true
+
+  # Hashing key
+  # When using MURMUR3 the key is ignored but must still be set.
+  # When using IPV4_NETWORK key is the subnet prefix lentgh
+  config :key, :validate => :string, :required => true
+
+  # digest/hash type
+  config :algorithm, :validate => ['SHA1', 'SHA256', 'SHA384', 'SHA512', 'MD5', "MURMUR3", "IPV4_NETWORK"], :required => true, :default => 'SHA1'
+
+  public
+  def register
+    # require any library and set the anonymize function
+    case @algorithm
+    when "IPV4_NETWORK"
+      require 'ipaddr'
+      class << self; alias_method :anonymize, :anonymize_ipv4_network; end
+    when "MURMUR3"
+      require "murmurhash3"
+      class << self; alias_method :anonymize, :anonymize_murmur3; end
+    else
+      require 'openssl'
+      class << self; alias_method :anonymize, :anonymize_openssl; end
+    end
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+    @fields.each do |field|
+      next unless event.include?(field)
+      if event[field].is_a?(Array)
+        event[field] = event[field].collect { |v| anonymize(v) }
+      else
+        event[field] = anonymize(event[field])
+      end
+    end
+  end # def filter
+
+  private
+  def anonymize_ipv4_network(ip_string)
+    # in JRuby 1.7.11 outputs as US-ASCII
+    IPAddr.new(ip_string).mask(@key.to_i).to_s.force_encoding(Encoding::UTF_8)
+  end
+
+  def anonymize_openssl(data)
+    digest = algorithm()
+    # in JRuby 1.7.11 outputs as ASCII-8BIT
+    OpenSSL::HMAC.hexdigest(digest, @key, data).force_encoding(Encoding::UTF_8)
+  end
+
+  def anonymize_murmur3(value)
+    case value
+    when Fixnum
+      MurmurHash3::V32.int_hash(value)
+    when String
+      MurmurHash3::V32.str_hash(value)
+    end
+  end
+
+  def algorithm
+
+   case @algorithm
+      #when 'SHA'
+        #return OpenSSL::Digest::SHA.new
+      when 'SHA1'
+        return OpenSSL::Digest::SHA1.new
+      #when 'SHA224'
+        #return OpenSSL::Digest::SHA224.new
+      when 'SHA256'
+        return OpenSSL::Digest::SHA256.new
+      when 'SHA384'
+        return OpenSSL::Digest::SHA384.new
+      when 'SHA512'
+        return OpenSSL::Digest::SHA512.new
+      #when 'MD4'
+        #return OpenSSL::Digest::MD4.new
+      when 'MD5'
+        return OpenSSL::Digest::MD5.new
+      else
+        @logger.error("Unknown algorithm")
+    end
+  end
+
+end # class LogStash::Filters::Anonymize
diff --git a/lib/logstash/filters/base.rb b/lib/logstash/filters/base.rb
index d8826686448..d35af57b725 100644
--- a/lib/logstash/filters/base.rb
+++ b/lib/logstash/filters/base.rb
@@ -1,48 +1,137 @@
+# encoding: utf-8
 require "logstash/namespace"
 require "logstash/logging"
+require "logstash/plugin"
 require "logstash/config/mixin"
 
-class LogStash::Filters::Base
+class LogStash::Filters::Base < LogStash::Plugin
   include LogStash::Config::Mixin
 
-  attr_accessor :logger
-
   config_name "filter"
 
-  # The type to act on. A filter 
-  config :type, :validate => :string
+  # Note that all of the specified routing options (type,tags.exclude\_tags,include\_fields,exclude\_fields)
+  # must be met in order for the event to be handled by the filter.
+
+  # The type to act on. If a type is given, then this filter will only
+  # act on messages with the same type. See any input plugin's "type"
+  # attribute for more.
+  # Optional.
+  config :type, :validate => :string, :default => "", :deprecated => "You can achieve this same behavior with the new conditionals, like: `if [type] == \"sometype\" { %PLUGIN% { ... } }`."
+
+  # Only handle events with all/any (controlled by include\_any config option) of these tags.
+  # Optional.
+  config :tags, :validate => :array, :default => [], :deprecated => "You can achieve similar behavior with the new conditionals, like: `if \"sometag\" in [tags] { %PLUGIN% { ... } }`"
+
+  # Only handle events without all/any (controlled by exclude\_any config
+  # option) of these tags.
+  # Optional.
+  config :exclude_tags, :validate => :array, :default => [], :deprecated => "You can achieve similar behavior with the new conditionals, like: `if !(\"sometag\" in [tags]) { %PLUGIN% { ... } }`"
 
   # If this filter is successful, add arbitrary tags to the event.
   # Tags can be dynamic and include parts of the event using the %{field}
   # syntax. Example:
   #
   #     filter {
-  #       myfilter {
-  #         add_tags => [ "foo_%{somefield}" ]
+  #       %PLUGIN% {
+  #         add_tag => [ "foo_%{somefield}" ]
+  #       }
+  #     }
+  #
+  #     # You can also add multiple tags at once:
+  #     filter {
+  #       %PLUGIN% {
+  #         add_tag => [ "foo_%{somefield}", "taggedy_tag"]
   #       }
   #     }
   #
   # If the event has field "somefield" == "hello" this filter, on success,
-  # would add a tag "foo_hello"
+  # would add a tag "foo_hello" (and the second example would of course add a "taggedy_tag" tag).
   config :add_tag, :validate => :array, :default => []
 
+  # If this filter is successful, remove arbitrary tags from the event.
+  # Tags can be dynamic and include parts of the event using the %{field}
+  # syntax. Example:
+  #
+  #     filter {
+  #       %PLUGIN% {
+  #         remove_tag => [ "foo_%{somefield}" ]
+  #       }
+  #     }
+  #
+  #     # You can also remove multiple tags at once:
+  #
+  #     filter {
+  #       %PLUGIN% {
+  #         remove_tag => [ "foo_%{somefield}", "sad_unwanted_tag"]
+  #       }
+  #     }
+  #
+  # If the event has field "somefield" == "hello" this filter, on success,
+  # would remove the tag "foo_hello" if it is present. The second example
+  # would remove a sad, unwanted tag as well.
+  config :remove_tag, :validate => :array, :default => []
+
   # If this filter is successful, add any arbitrary fields to this event.
+  # Field names can be dynamic and include parts of the event using the %{field}
   # Example:
   #
   #     filter {
-  #       myfilter {
-  #         add_fields => [ "sample", "Hello world, from %{@source}" ]
+  #       %PLUGIN% {
+  #         add_field => { "foo_%{somefield}" => "Hello world, from %{host}" }
   #       }
   #     }
   #
-  #  On success, myfilter will then add field 'sample' with the value above
-  #  and the %{@source} piece replaced with that value from the event.
+  #     # You can also add multiple fields at once:
+  #
+  #     filter {
+  #       %PLUGIN% {
+  #         add_field => {
+  #           "foo_%{somefield}" => "Hello world, from %{host}"
+  #           "new_field" => "new_static_value"
+  #         }
+  #       }
+  #     }
+  #
+  # If the event has field "somefield" == "hello" this filter, on success,
+  # would add field "foo_hello" if it is present, with the
+  # value above and the %{host} piece replaced with that value from the
+  # event. The second example would also add a hardcoded field.
   config :add_field, :validate => :hash, :default => {}
 
+  # If this filter is successful, remove arbitrary fields from this event.
+  # Fields names can be dynamic and include parts of the event using the %{field}
+  # Example:
+  #
+  #     filter {
+  #       %PLUGIN% {
+  #         remove_field => [ "foo_%{somefield}" ]
+  #       }
+  #     }
+  #
+  #     # You can also remove multiple fields at once:
+  #
+  #     filter {
+  #       %PLUGIN% {
+  #         remove_field => [ "foo_%{somefield}", "my_extraneous_field" ]
+  #       }
+  #     }
+  #
+  # If the event has field "somefield" == "hello" this filter, on success,
+  # would remove the field with name "foo_hello" if it is present. The second
+  # example would remove an additional, non-dynamic field.
+  config :remove_field, :validate => :array, :default => []
+
+  # Call the filter flush method at regular interval.
+  # Optional.
+  config :periodic_flush, :validate => :boolean, :default => false
+
+  RESERVED = ["type", "tags", "exclude_tags", "include_fields", "exclude_fields", "add_tag", "remove_tag", "add_field", "remove_field", "include_any", "exclude_any"]
+
   public
   def initialize(params)
-    @logger = LogStash::Logger.new(STDOUT)
+    super
     config_init(params)
+    @threadsafe = true
   end # def initialize
 
   public
@@ -55,20 +144,92 @@ def filter(event)
     raise "#{self.class}#filter must be overidden"
   end # def filter
 
+  public
+  def execute(event, &block)
+    filter(event, &block)
+  end # def execute
+
+  public
+  def threadsafe?
+    @threadsafe
+  end
+
   # a filter instance should call filter_matched from filter if the event
   # matches the filter's conditions (right type, etc)
   protected
   def filter_matched(event)
-    (@add_field or {}).each do |field, value|
-      event[field] ||= []
-      event[field] << event.sprintf(value)
-      @logger.debug("filters/#{self.class.name}: adding #{value} to field #{field}")
+    @add_field.each do |field, value|
+      field = event.sprintf(field)
+      value = [value] if !value.is_a?(Array)
+      value.each do |v|
+        v = event.sprintf(v)
+        if event.include?(field)
+          event[field] = [event[field]] if !event[field].is_a?(Array)
+          event[field] << v
+        else
+          event[field] = v
+        end
+        @logger.debug? and @logger.debug("filters/#{self.class.name}: adding value to field",
+                                       :field => field, :value => value)
+      end
     end
 
-    (@add_tag or []).each do |tag|
-      @logger.debug("filters/#{self.class.name}: adding tag #{tag}")
-      event.tags << event.sprintf(tag)
-      #event.tags |= [ event.sprintf(tag) ]
+    @remove_field.each do |field|
+      field = event.sprintf(field)
+      @logger.debug? and @logger.debug("filters/#{self.class.name}: removing field",
+                                       :field => field)
+      event.remove(field)
+    end
+
+    @add_tag.each do |tag|
+      tag = event.sprintf(tag)
+      @logger.debug? and @logger.debug("filters/#{self.class.name}: adding tag",
+                                       :tag => tag)
+      (event["tags"] ||= []) << tag
+    end
+
+    @remove_tag.each do |tag|
+      break if event["tags"].nil?
+      tag = event.sprintf(tag)
+      @logger.debug? and @logger.debug("filters/#{self.class.name}: removing tag",
+                                       :tag => tag)
+      event["tags"].delete(tag)
     end
   end # def filter_matched
+
+  protected
+  def filter?(event)
+    if !@type.empty?
+      if event["type"] != @type
+        @logger.debug? and @logger.debug(["filters/#{self.class.name}: Skipping event because type doesn't match #{@type}", event])
+        return false
+      end
+    end
+
+    if !@tags.empty?
+      # this filter has only works on events with certain tags,
+      # and this event has no tags.
+      return false if !event["tags"]
+
+      # Is @tags a subset of the event's tags? If not, skip it.
+      if (event["tags"] & @tags).size != @tags.size
+        @logger.debug(["filters/#{self.class.name}: Skipping event because tags don't match #{@tags.inspect}", event])
+        return false
+      end
+    end
+
+    if !@exclude_tags.empty? && event["tags"]
+      if (diff_tags = (event["tags"] & @exclude_tags)).size != 0
+        @logger.debug(["filters/#{self.class.name}: Skipping event because tags contains excluded tags: #{diff_tags.inspect}", event])
+        return false
+      end
+    end
+
+    return true
+  end
+
+  public
+  def teardown
+    # Nothing to do by default.
+  end
 end # class LogStash::Filters::Base
diff --git a/lib/logstash/filters/checksum.rb b/lib/logstash/filters/checksum.rb
new file mode 100644
index 00000000000..348bc39d06c
--- /dev/null
+++ b/lib/logstash/filters/checksum.rb
@@ -0,0 +1,53 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+require "yaml"
+
+# This filter let's you create a checksum based on various parts
+# of the logstash event.
+# This can be useful for deduplication of messages or simply to provide
+# a custom unique identifier.
+#
+# This is VERY experimental and is largely a proof-of-concept
+class LogStash::Filters::Checksum < LogStash::Filters::Base
+
+  config_name "checksum"
+  milestone 1
+
+  ALGORITHMS = ["md5", "sha", "sha1", "sha256", "sha384",]
+
+  # A list of keys to use in creating the string to checksum
+  # Keys will be sorted before building the string
+  # keys and values will then be concatenated with pipe delimeters
+  # and checksummed
+  config :keys, :validate => :array, :default => ["message", "@timestamp", "type"]
+
+  config :algorithm, :validate => ALGORITHMS, :default => "sha256"
+
+  public
+  def register
+    require 'openssl'
+    @to_checksum = ""
+  end
+
+  public
+  def filter(event)
+    return unless filter?(event)
+
+    @logger.debug("Running checksum filter", :event => event)
+
+    @keys.sort.each do |k|
+      @logger.debug("Adding key to string", :current_key => k)
+      @to_checksum << "|#{k}|#{event[k]}"
+    end
+    @to_checksum << "|"
+    @logger.debug("Final string built", :to_checksum => @to_checksum)
+
+
+    # in JRuby 1.7.11 outputs as ASCII-8BIT
+    digested_string = OpenSSL::Digest.hexdigest(@algorithm, @to_checksum).force_encoding(Encoding::UTF_8)
+
+    @logger.debug("Digested string", :digested_string => digested_string)
+    event['logstash_checksum'] = digested_string
+  end
+end # class LogStash::Filters::Checksum
diff --git a/lib/logstash/filters/clone.rb b/lib/logstash/filters/clone.rb
new file mode 100644
index 00000000000..162d18156e6
--- /dev/null
+++ b/lib/logstash/filters/clone.rb
@@ -0,0 +1,35 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# The clone filter is for duplicating events.
+# A clone will be made for each type in the clone list.
+# The original event is left unchanged.
+class LogStash::Filters::Clone < LogStash::Filters::Base
+
+  config_name "clone"
+  milestone 2
+
+  # A new clone will be created with the given type for each type in this list.
+  config :clones, :validate => :array, :default => []
+
+  public
+  def register
+    # Nothing to do
+  end
+
+  public
+  def filter(event)
+    return unless filter?(event)
+    @clones.each do |type|
+      clone = event.clone
+      clone["type"] = type
+      filter_matched(clone)
+      @logger.debug("Cloned event", :clone => clone, :event => event)
+
+      # Push this new event onto the stack at the LogStash::FilterWorker
+      yield clone
+    end
+  end
+
+end # class LogStash::Filters::Clone
diff --git a/lib/logstash/filters/csv.rb b/lib/logstash/filters/csv.rb
new file mode 100644
index 00000000000..cfdabf7566e
--- /dev/null
+++ b/lib/logstash/filters/csv.rb
@@ -0,0 +1,97 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+require "csv"
+
+# The CSV filter takes an event field containing CSV data, parses it,
+# and stores it as individual fields (can optionally specify the names).
+# This filter can also parse data with any separator, not just commas.
+class LogStash::Filters::CSV < LogStash::Filters::Base
+  config_name "csv"
+  milestone 2
+
+  # The CSV data in the value of the `source` field will be expanded into a
+  # data structure.
+  config :source, :validate => :string, :default => "message"
+
+  # Define a list of column names (in the order they appear in the CSV,
+  # as if it were a header line). If `columns` is not configured, or there
+  # are not enough columns specified, the default column names are
+  # "column1", "column2", etc. In the case that there are more columns
+  # in the data than specified in this column list, extra columns will be auto-numbered:
+  # (e.g. "user_defined_1", "user_defined_2", "column3", "column4", etc.)
+  config :columns, :validate => :array, :default => []
+
+  # Define the column separator value. If this is not specified, the default
+  # is a comma ','.
+  # Optional.
+  config :separator, :validate => :string, :default => ","
+
+  # Define the character used to quote CSV fields. If this is not specified
+  # the default is a double quote '"'.
+  # Optional.
+  config :quote_char, :validate => :string, :default => '"'
+
+  # Define target field for placing the data.
+  # Defaults to writing to the root of the event.
+  config :target, :validate => :string
+
+  public
+  def register
+
+    # Nothing to do here
+
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+
+    @logger.debug("Running csv filter", :event => event)
+
+    matches = 0
+
+    if event[@source]
+      if event[@source].is_a?(String)
+        event[@source] = [event[@source]]
+      end
+
+      if event[@source].length > 1
+        @logger.warn("csv filter only works on fields of length 1",
+                     :source => @source, :value => event[@source],
+                     :event => event)
+        return
+      end
+
+      raw = event[@source].first
+      begin
+        values = CSV.parse_line(raw, :col_sep => @separator, :quote_char => @quote_char)
+
+        if @target.nil?
+          # Default is to write to the root of the event.
+          dest = event
+        else
+          dest = event[@target] ||= {}
+        end
+
+        values.each_index do |i|
+          field_name = @columns[i] || "column#{i+1}"
+          dest[field_name] = values[i]
+        end
+
+        filter_matched(event)
+      rescue => e
+        event.tag "_csvparsefailure"
+        @logger.warn("Trouble parsing csv", :source => @source, :raw => raw,
+                      :exception => e)
+        return
+      end # begin
+    end # if event
+
+    @logger.debug("Event after csv filter", :event => event)
+
+  end # def filter
+
+end # class LogStash::Filters::Csv
+
diff --git a/lib/logstash/filters/date.rb b/lib/logstash/filters/date.rb
index 9f0155f51a7..0465633374d 100644
--- a/lib/logstash/filters/date.rb
+++ b/lib/logstash/filters/date.rb
@@ -1,12 +1,14 @@
+# encoding: utf-8
 require "logstash/filters/base"
 require "logstash/namespace"
-require "logstash/time"
+require "logstash/timestamp"
 
-# The date filter is used for parsing dates from fields and using that
-# date or timestamp as the timestamp for the event.
+# The date filter is used for parsing dates from fields, and then using that
+# date or timestamp as the logstash timestamp for the event.
 #
 # For example, syslog events usually have timestamps like this:
-#   "Apr  7 09:32:01"
+#
+#     "Apr 17 09:32:01"
 #
 # You would use the date format "MMM dd HH:mm:ss" to parse this.
 #
@@ -14,54 +16,82 @@
 # backfilling old data. If you don't get the date correct in your
 # event, then searching for them later will likely sort out of order.
 #
-# In the absense of this filter, logstash will choose a timestamp based on the
+# In the absence of this filter, logstash will choose a timestamp based on the
 # first time it sees the event (at input time), if the timestamp is not already
 # set in the event. For example, with file input, the timestamp is set to the
-# time of reading.
+# time of each read.
 class LogStash::Filters::Date < LogStash::Filters::Base
+  if RUBY_ENGINE == "jruby"
+    JavaException = java.lang.Exception
+    UTC = org.joda.time.DateTimeZone.forID("UTC")
+  end
 
   config_name "date"
+  milestone 3
+
+  # Specify a time zone canonical ID to be used for date parsing.
+  # The valid IDs are listed on the [Joda.org available time zones page](http://joda-time.sourceforge.net/timezones.html).
+  # This is useful in case the time zone cannot be extracted from the value,
+  # and is not the platform default.
+  # If this is not specified the platform default will be used.
+  # Canonical ID is good as it takes care of daylight saving time for you
+  # For example, `America/Los_Angeles` or `Europe/France` are valid IDs.
+  config :timezone, :validate => :string
+
+  # Specify a locale to be used for date parsing using either IETF-BCP47 or POSIX language tag.
+  # Simple examples are `en`,`en-US` for BCP47 or `en_US` for POSIX.
+  # If not specified, the platform default will be used.
+  #
+  # The locale is mostly necessary to be set for parsing month names (pattern with MMM) and
+  # weekday names (pattern with EEE).
+  #
+  config :locale, :validate => :string
 
-  # Config for date is:
-  #   fieldname => dateformat
+  # The date formats allowed are anything allowed by Joda-Time (java time
+  # library). You can see the docs for this format here:
   #
-  # The same field can be specified multiple times (or multiple dateformats for
-  # the same field) do try different time formats; first success wins.
+  # [joda.time.format.DateTimeFormat](http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html)
   #
-  # The date formats allowed are the string 'ISO8601' or whatever is supported
-  # by Joda; generally: [java.text.SimpleDateFormat][dateformats]
+  # An array with field name first, and format patterns following, `[ field,
+  # formats... ]`
   #
-  # For example, if you have a field 'logdate' and with a value that looks like 'Aug 13 2010 00:03:44'
-  # you would use this configuration:
+  # If your time field has multiple possible formats, you can do this:
   #
-  #     logdate => "MMM dd yyyy HH:mm:ss"
+  #     match => [ "logdate", "MMM dd YYY HH:mm:ss",
+  #               "MMM  d YYY HH:mm:ss", "ISO8601" ]
   #
-  # [dateformats]: http://download.oracle.com/javase/1.4.2/docs/api/java/text/SimpleDateFormat.html
-  config /[A-Za-z0-9_-]+/, :validate => :array
-
-  # LOGSTASH-34
-  DATEPATTERNS = %w{ y d H m s S } 
-
-  # The 'date' filter will take a value from your event and use it as the
-  # event timestamp. This is useful for parsing logs generated on remote
-  # servers or for importing old logs.
+  # The above will match a syslog (rfc3164) or iso8601 timestamp.
   #
-  # The config looks like this:
+  # There are a few special exceptions. The following format literals exist
+  # to help you save time and ensure correctness of date parsing.
   #
-  # filter {
-  #   date {
-  #     type => "typename"
-  #     fielname => fieldformat
+  # * "ISO8601" - should parse any valid ISO8601 timestamp, such as
+  #   2011-04-19T03:44:01.103Z
+  # * "UNIX" - will parse unix time in seconds since epoch
+  # * "UNIX_MS" - will parse unix time in milliseconds since epoch
+  # * "TAI64N" - will parse tai64n time values
   #
-  #     # Example:
-  #     timestamp => "mmm DD HH:mm:ss"
-  #   }
-  # }
+  # For example, if you have a field 'logdate', with a value that looks like
+  # 'Aug 13 2010 00:03:44', you would use this configuration:
   #
-  # The format is whatever is supported by Joda; generally:
-  # http://download.oracle.com/javase/1.4.2/docs/api/java/text/SimpleDateFormat.html
+  #     filter {
+  #       date {
+  #         match => [ "logdate", "MMM dd YYYY HH:mm:ss" ]
+  #       }
+  #     }
   #
-  # TODO(sissel): Support 'seconds since epoch' parsing (nagios uses this)
+  # If your field is nested in your structure, you can use the nested
+  # syntax [foo][bar] to match its value. For more information, please refer to
+  # http://logstash.net/docs/latest/configuration#fieldreferences
+  config :match, :validate => :array, :default => []
+
+  # Store the matching timestamp into the given target field.  If not provided,
+  # default to updating the @timestamp field of the event.
+  config :target, :validate => :string, :default => "@timestamp"
+
+  # LOGSTASH-34
+  DATEPATTERNS = %w{ y d H m s S }
+
   public
   def initialize(config = {})
     super
@@ -72,110 +102,135 @@ def initialize(config = {})
   public
   def register
     require "java"
-    # TODO(sissel): Need a way of capturing regexp configs better.
-    @config.each do |field, value|
-      next if ["add_tag", "add_field", "type"].include?(field)
+    if @match.length < 2
+      raise LogStash::ConfigurationError, I18n.t("logstash.agent.configuration.invalid_plugin_register",
+        :plugin => "filter", :type => "date",
+        :error => "The match setting should contains first a field name and at least one date format, current value is #{@match}")
+    end
 
-      # values here are an array of format strings for the given field.
-      value.each do |format|
-        case format
+    locale = nil
+    if @locale
+      if @locale.include? '_'
+        @logger.warn("Date filter now use BCP47 format for locale, replacing underscore with dash")
+        @locale.gsub!('_','-')
+      end
+      locale = java.util.Locale.forLanguageTag(@locale)
+    end
+    setupMatcher(@config["match"].shift, locale, @config["match"] )
+  end
+
+  def setupMatcher(field, locale, value)
+    value.each do |format|
+      parsers = []
+      case format
         when "ISO8601"
-          parser = org.joda.time.format.ISODateTimeFormat.dateTimeParser
-          missing = []
+          iso_parser = org.joda.time.format.ISODateTimeFormat.dateTimeParser
+          if @timezone
+            iso_parser = iso_parser.withZone(org.joda.time.DateTimeZone.forID(@timezone))
+          else
+            iso_parser = iso_parser.withOffsetParsed
+          end
+          parsers << lambda { |date| iso_parser.parseMillis(date) }
+          #Fall back solution of almost ISO8601 date-time
+          almostISOparsers = [
+            org.joda.time.format.DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss.SSSZ").getParser(),
+            org.joda.time.format.DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss.SSS").getParser()
+          ].to_java(org.joda.time.format.DateTimeParser)
+          joda_parser = org.joda.time.format.DateTimeFormatterBuilder.new.append( nil, almostISOparsers ).toFormatter()
+          if @timezone
+            joda_parser = joda_parser.withZone(org.joda.time.DateTimeZone.forID(@timezone))
+          else
+            joda_parser = joda_parser.withOffsetParsed
+          end
+          parsers << lambda { |date| joda_parser.parseMillis(date) }
+        when "UNIX" # unix epoch
+          parsers << lambda do |date|
+            raise "Invalid UNIX epoch value '#{date}'" unless /^\d+(?:\.\d+)?$/ === date || date.is_a?(Numeric)
+            (date.to_f * 1000).to_i
+          end
+        when "UNIX_MS" # unix epoch in ms
+          parsers << lambda do |date|
+            raise "Invalid UNIX epoch value '#{date}'" unless /^\d+$/ === date || date.is_a?(Numeric)
+            date.to_i
+          end
+        when "TAI64N" # TAI64 with nanoseconds, -10000 accounts for leap seconds
+          parsers << lambda do |date| 
+            # Skip leading "@" if it is present (common in tai64n times)
+            date = date[1..-1] if date[0, 1] == "@"
+            return (date[1..15].hex * 1000 - 10000)+(date[16..23].hex/1000000)
+          end
         else
-          parser = org.joda.time.format.DateTimeFormat.forPattern(format)
-
-          # Joda's time parser doesn't assume 'current time' for unparsed values.
-          # That is, if you parse with format "mmm dd HH:MM:SS" (no year) then
-          # the year is assumed to be unix epoch year, 1970, rather than
-          # current year. This sucks, so try and keep track of fields that
-          # are not specified so we can inject them later. (jordansissel)
-          # LOGSTASH-34
-          missing = DATEPATTERNS.reject { |p| format.include?(p) }
-        end
-
-        @logger.debug "Adding type #{@type} with date config: #{field} => #{format}"
-        @parsers[field] << {
-          :parser => parser.withOffsetParsed,
-          :missing => missing
-        }
-      end # value.each
-    end # @config.each
-  end # def register
+          joda_parser = org.joda.time.format.DateTimeFormat.forPattern(format).withDefaultYear(Time.new.year)
+          if @timezone
+            joda_parser = joda_parser.withZone(org.joda.time.DateTimeZone.forID(@timezone))
+          else
+            joda_parser = joda_parser.withOffsetParsed
+          end
+          if (locale != nil)
+            joda_parser = joda_parser.withLocale(locale)
+          end
+          parsers << lambda { |date| joda_parser.parseMillis(date) }
+      end
+
+      @logger.debug("Adding type with date config", :type => @type,
+                    :field => field, :format => format)
+      @parsers[field] << {
+        :parser => parsers,
+        :format => format
+      }
+    end
+  end
+
+  # def register
 
   public
   def filter(event)
-    @logger.debug "DATE FILTER: received event of type #{event.type}"
-    return unless event.type == @type
-    now = Time.now
-
+    @logger.debug? && @logger.debug("Date filter: received event", :type => event["type"])
+    return unless filter?(event)
     @parsers.each do |field, fieldparsers|
+      @logger.debug? && @logger.debug("Date filter looking for field",
+                                      :type => event["type"], :field => field)
+      next unless event.include?(field)
 
-      @logger.debug "DATE FILTER: type #{event.type}, looking for field #{field.inspect}"
-      # TODO(sissel): check event.message, too.
-      next unless event.fields.member?(field)
-
-      fieldvalues = event.fields[field]
-      fieldvalues = [fieldvalues] if fieldvalues.is_a?(String)
+      fieldvalues = event[field]
+      fieldvalues = [fieldvalues] if !fieldvalues.is_a?(Array)
       fieldvalues.each do |value|
-        next if value.nil? or value.empty?
+        next if value.nil?
         begin
-          time = nil
-          missing = []
+          epochmillis = nil
           success = false
+          last_exception = RuntimeError.new "Unknown"
           fieldparsers.each do |parserconfig|
-            parser = parserconfig[:parser]
-            missing = parserconfig[:missing]
-            #@logger.info :Missing => missing
-            #p :parser => parser
-            begin
-              time = parser.parseDateTime(value)
-              success = true
-              break # success
-            rescue => e
-              last_exception = e
-            end
+            parserconfig[:parser].each do |parser|
+              begin
+                epochmillis = parser.call(value)
+                success = true
+                break # success
+              rescue StandardError, JavaException => e
+                last_exception = e
+              end
+            end # parserconfig[:parser].each
+            break if success
           end # fieldparsers.each
 
-          if !success
-            raise last_exception
-          end
+          raise last_exception unless success
 
-          # Perform workaround for LOGSTASH-34
-          if !missing.empty?
-            # Inject any time values missing from the time parser format
-            missing.each do |t|
-              case t
-              when "y"
-                time = time.withYear(now.year)
-              when "S"
-                # TODO(sissel): Old behavior was to default to fractional sec == 0
-                #time.setMillisOfSecond(now.usec / 1000)
-                time = time.withMillisOfSecond(0)
-              #when "Z"
-                # Ruby 'time.gmt_offset' is in seconds.
-                # timezone is missing, so let's add in our localtime offset.
-                #time = time.plusSeconds(now.gmt_offset)
-                # TODO(sissel): not clear if we need to do this...
-              end # case t
-            end
-          end
-          #@logger.info :JodaTime => time.to_s
-          time = time.withZone(org.joda.time.DateTimeZone.forID("UTC"))
-          event.timestamp = time.to_s 
-          #event.timestamp = LogStash::Time.to_iso8601(time)
-          @logger.debug "Parsed #{value.inspect} as #{event.timestamp}"
-        rescue => e
-          @logger.warn "Failed parsing date #{value.inspect} from field #{field}: #{e}"
+          # Convert joda DateTime to a ruby Time
+          event[@target] = LogStash::Timestamp.at(epochmillis / 1000, (epochmillis % 1000) * 1000)
+
+          @logger.debug? && @logger.debug("Date parsing done", :value => value, :timestamp => event[@target])
+          filter_matched(event)
+        rescue StandardError, JavaException => e
+          @logger.warn("Failed parsing date from field", :field => field,
+                       :value => value, :exception => e)
           # Raising here will bubble all the way up and cause an exit.
           # TODO(sissel): Maybe we shouldn't raise?
+          # TODO(sissel): What do we do on a failure? Tag it like grok does?
           #raise e
         end # begin
-      end # fieldvalue.each 
+      end # fieldvalue.each
     end # @parsers.each
 
-    if !event.cancelled?
-      filter_matched(event)
-    end
+    return event
   end # def filter
 end # class LogStash::Filters::Date
diff --git a/lib/logstash/filters/dns.rb b/lib/logstash/filters/dns.rb
new file mode 100644
index 00000000000..f23b8191f80
--- /dev/null
+++ b/lib/logstash/filters/dns.rb
@@ -0,0 +1,209 @@
+# encoding: utf-8
+# DNS Filter
+#
+# This filter will resolve any IP addresses from a field of your choosing.
+#
+
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# The DNS filter performs a lookup (either an A record/CNAME record lookup
+# or a reverse lookup at the PTR record) on records specified under the
+# "reverse" and "resolve" arrays.
+#
+# The config should look like this:
+#
+#     filter {
+#       dns {
+#         type => 'type'
+#         reverse => [ "source_host", "field_with_address" ]
+#         resolve => [ "field_with_fqdn" ]
+#         action => "replace"
+#       }
+#     }
+#
+# Caveats: at the moment, there's no way to tune the timeout with the 'resolv'
+# core library.  It does seem to be fixed in here:
+#
+#   http://redmine.ruby-lang.org/issues/5100
+#
+# but isn't currently in JRuby.
+class LogStash::Filters::DNS < LogStash::Filters::Base
+
+  config_name "dns"
+  milestone 2
+
+  # Reverse resolve one or more fields.
+  config :reverse, :validate => :array
+
+  # Forward resolve one or more fields.
+  config :resolve, :validate => :array
+
+  # Determine what action to do: append or replace the values in the fields
+  # specified under "reverse" and "resolve."
+  config :action, :validate => [ "append", "replace" ], :default => "append"
+
+  # Use custom nameserver.
+  config :nameserver, :validate => :string
+
+  # TODO(sissel): make 'action' required? This was always the intent, but it
+  # due to a typo it was never enforced. Thus the default behavior in past
+  # versions was 'append' by accident.
+
+  # resolv calls will be wrapped in a timeout instance
+  config :timeout, :validate => :number, :default => 2
+
+  public
+  def register
+    require "resolv"
+    require "timeout"
+    if @nameserver.nil?
+      @resolv = Resolv.new
+    else
+      @resolv = Resolv.new(resolvers=[::Resolv::Hosts.new, ::Resolv::DNS.new(:nameserver => [@nameserver], :search => [], :ndots => 1)])
+    end
+
+    @ip_validator = Resolv::AddressRegex
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+
+    new_event = event.clone
+
+    if @resolve
+      begin
+        status = Timeout::timeout(@timeout) {
+          resolve(new_event)
+        }
+        return if status.nil?
+      rescue Timeout::Error
+        @logger.debug("DNS: resolve action timed out")
+        return
+      end
+    end
+
+    if @reverse
+      begin
+        status = Timeout::timeout(@timeout) {
+          reverse(new_event)
+        }
+        return if status.nil?
+      rescue Timeout::Error
+        @logger.debug("DNS: reverse action timed out")
+        return
+      end
+    end
+
+    filter_matched(new_event)
+    yield new_event
+    event.cancel
+  end
+
+  private
+  def resolve(event)
+    @resolve.each do |field|
+      is_array = false
+      raw = event[field]
+      if raw.is_a?(Array)
+        is_array = true
+        if raw.length > 1
+          @logger.warn("DNS: skipping resolve, can't deal with multiple values", :field => field, :value => raw)
+          return
+        end
+        raw = raw.first
+      end
+
+      begin
+        # in JRuby 1.7.11 outputs as US-ASCII
+        address = @resolv.getaddress(raw).force_encoding(Encoding::UTF_8)
+      rescue Resolv::ResolvError
+        @logger.debug("DNS: couldn't resolve the hostname.",
+                      :field => field, :value => raw)
+        return
+      rescue Resolv::ResolvTimeout
+        @logger.debug("DNS: timeout on resolving the hostname.",
+                      :field => field, :value => raw)
+        return
+      rescue SocketError => e
+        @logger.debug("DNS: Encountered SocketError.",
+                      :field => field, :value => raw)
+        return
+      rescue NoMethodError => e
+        # see JRUBY-5647
+        @logger.debug("DNS: couldn't resolve the hostname.",
+                      :field => field, :value => raw,
+                      :extra => "NameError instead of ResolvError")
+        return
+      end
+
+      if @action == "replace"
+        if is_array
+          event[field] = [address]
+        else
+          event[field] = address
+        end
+      else
+        if !is_array
+          event[field] = [event[field], address]
+        else
+          event[field] << address
+        end
+      end
+
+    end
+  end
+
+  private
+  def reverse(event)
+    @reverse.each do |field|
+      raw = event[field]
+      is_array = false
+      if raw.is_a?(Array)
+          is_array = true
+          if raw.length > 1
+            @logger.warn("DNS: skipping reverse, can't deal with multiple values", :field => field, :value => raw)
+            return
+          end
+          raw = raw.first
+      end
+
+      if ! @ip_validator.match(raw)
+        @logger.debug("DNS: not an address",
+                      :field => field, :value => event[field])
+        return
+      end
+      begin
+        # in JRuby 1.7.11 outputs as US-ASCII
+        hostname = @resolv.getname(raw).force_encoding(Encoding::UTF_8)
+      rescue Resolv::ResolvError
+        @logger.debug("DNS: couldn't resolve the address.",
+                      :field => field, :value => raw)
+        return
+      rescue Resolv::ResolvTimeout
+        @logger.debug("DNS: timeout on resolving address.",
+                      :field => field, :value => raw)
+        return
+      rescue SocketError => e
+        @logger.debug("DNS: Encountered SocketError.",
+                      :field => field, :value => raw)
+        return
+      end
+
+      if @action == "replace"
+        if is_array
+          event[field] = [hostname]
+        else
+          event[field] = hostname
+        end
+      else
+        if !is_array
+          event[field] = [event[field], hostname]
+        else
+          event[field] << hostname
+        end
+      end
+    end
+  end
+end # class LogStash::Filters::DNS
diff --git a/lib/logstash/filters/drop.rb b/lib/logstash/filters/drop.rb
new file mode 100644
index 00000000000..9c50f8b4a68
--- /dev/null
+++ b/lib/logstash/filters/drop.rb
@@ -0,0 +1,32 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# Drop filter.
+#
+# Drops everything that gets to this filter.
+#
+# This is best used in combination with conditionals, for example:
+#
+#     filter {
+#       if [loglevel] == "debug" { 
+#         drop { } 
+#       }
+#     }
+#
+# The above will only pass events to the drop filter if the loglevel field is
+# "debug". This will cause all events matching to be dropped.
+class LogStash::Filters::Drop < LogStash::Filters::Base
+  config_name "drop"
+  milestone 3
+
+  public
+  def register
+    # nothing to do.
+  end
+
+  public
+  def filter(event)
+    event.cancel
+  end # def filter
+end # class LogStash::Filters::Drop
diff --git a/lib/logstash/filters/field.rb b/lib/logstash/filters/field.rb
deleted file mode 100644
index 8a7e5327605..00000000000
--- a/lib/logstash/filters/field.rb
+++ /dev/null
@@ -1,33 +0,0 @@
-require "logstash/filters/base"
-require "logstash/namespace"
-require "ostruct"
-
-# THIS IS NOT SUPPORTED YET.
-class LogStash::Filters::Field < LogStash::Filters::Base
-
-  # TODO(sissel): Finish this.
-  config_name "field"
-
-  class EvalSpace < OpenStruct
-    def get_binding
-      return binding
-    end
-  end
-
-  public
-  def register
-    # nothing to do
-  end # def register
-
-  public
-  def filter(event)
-    data = EvalSpace.new(event.to_hash)
-
-    @config.each do |condition|
-      if data.instance_eval(condition)
-        return # This event is OK, matches the condition.
-      end
-    end
-    event.cancel
-  end # def filter
-end # class LogStash::Filters::Field
diff --git a/lib/logstash/filters/fingerprint.rb b/lib/logstash/filters/fingerprint.rb
new file mode 100644
index 00000000000..2bb211a660a
--- /dev/null
+++ b/lib/logstash/filters/fingerprint.rb
@@ -0,0 +1,122 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+#  Fingerprint fields using by replacing values with a consistent hash.
+class LogStash::Filters::Fingerprint < LogStash::Filters::Base
+  config_name "fingerprint"
+  milestone 1
+
+  # Source field(s)
+  config :source, :validate => :array, :default => 'message'
+
+  # Target field.
+  # will overwrite current value of a field if it exists.
+  config :target, :validate => :string, :default => 'fingerprint'
+
+  # When used with IPV4_NETWORK method fill in the subnet prefix length
+  # Not required for MURMUR3 or UUID methods
+  # With other methods fill in the HMAC key
+  config :key, :validate => :string
+
+  # Fingerprint method
+  config :method, :validate => ['SHA1', 'SHA256', 'SHA384', 'SHA512', 'MD5', "MURMUR3", "IPV4_NETWORK", "UUID", "PUNCTUATION"], :required => true, :default => 'SHA1'
+
+  # When set to true, we concatenate the values of all fields into 1 string like the old checksum filter.
+  config :concatenate_sources, :validate => :boolean, :default => false
+
+  def register
+    # require any library and set the anonymize function
+    case @method
+      when "IPV4_NETWORK"
+        require 'ipaddr'
+        @logger.error("Key value is empty. please fill in a subnet prefix length") if @key.nil?
+        class << self; alias_method :anonymize, :anonymize_ipv4_network; end
+      when "MURMUR3"
+        require "murmurhash3"
+        class << self; alias_method :anonymize, :anonymize_murmur3; end
+      when "UUID"
+        require "securerandom"
+      when "PUNCTUATION"
+        # nothing required
+      else
+        require 'openssl'
+        @logger.error("Key value is empty. Please fill in an encryption key") if @key.nil?
+        class << self; alias_method :anonymize, :anonymize_openssl; end
+    end
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+    case @method
+      when "UUID"
+        event[@target] = SecureRandom.uuid
+      when "PUNCTUATION"
+        @source.sort.each do |field|
+          next unless event.include?(field)
+          event[@target] = event[field].tr('A-Za-z0-9 \t','')
+        end
+      else
+        if @concatenate_sources
+          to_string = ''
+          @source.sort.each do |k|
+            @logger.debug("Adding key to string")
+            to_string << "|#{k}|#{event[k]}"
+          end
+          to_string << "|"
+          @logger.debug("String built", :to_checksum => to_string)
+          event[@target] = anonymize(to_string)
+        else
+          @source.each do |field|
+            next unless event.include?(field)
+            if event[field].is_a?(Array)
+              event[@target] = event[field].collect { |v| anonymize(v) }
+            else
+              event[@target] = anonymize(event[field])
+            end
+          end # @source.each
+        end # concatenate_sources
+
+    end # casse @method
+  end # def filter
+
+  private
+  def anonymize_ipv4_network(ip_string)
+    # in JRuby 1.7.11 outputs as US-ASCII
+    IPAddr.new(ip_string).mask(@key.to_i).to_s.force_encoding(Encoding::UTF_8)
+  end
+
+  def anonymize_openssl(data)
+    digest = encryption_algorithm()
+    # in JRuby 1.7.11 outputs as ASCII-8BIT
+    OpenSSL::HMAC.hexdigest(digest, @key, data.to_s).force_encoding(Encoding::UTF_8)
+  end
+
+  def anonymize_murmur3(value)
+    case value
+      when Fixnum
+        MurmurHash3::V32.int_hash(value)
+      else
+        MurmurHash3::V32.str_hash(value.to_s)
+    end
+  end
+
+  def encryption_algorithm
+   case @method
+     when 'SHA1'
+       return OpenSSL::Digest::SHA1.new
+     when 'SHA256'
+       return OpenSSL::Digest::SHA256.new
+     when 'SHA384'
+       return OpenSSL::Digest::SHA384.new
+     when 'SHA512'
+       return OpenSSL::Digest::SHA512.new
+     when 'MD5'
+       return OpenSSL::Digest::MD5.new
+     else
+       @logger.error("Unknown algorithm")
+    end
+  end
+
+end # class LogStash::Filters::Anonymize
diff --git a/lib/logstash/filters/geoip.rb b/lib/logstash/filters/geoip.rb
new file mode 100644
index 00000000000..b70f89c9498
--- /dev/null
+++ b/lib/logstash/filters/geoip.rb
@@ -0,0 +1,147 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+require "tempfile"
+
+# The GeoIP filter adds information about the geographical location of IP addresses,
+# based on data from the Maxmind database.
+#
+# Starting with version 1.3.0 of Logstash, a [geoip][location] field is created if
+# the GeoIP lookup returns a latitude and longitude. The field is stored in
+# [GeoJSON](http://geojson.org/geojson-spec.html) format. Additionally,
+# the default Elasticsearch template provided with the
+# [elasticsearch output](../outputs/elasticsearch.html)
+# maps the [geoip][location] field to a
+# [geo_point](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-geo-point-type.html).
+#
+# As this field is a geo\_point _and_ it is still valid GeoJSON, you get
+# the awesomeness of Elasticsearch's geospatial query, facet and filter functions
+# and the flexibility of having GeoJSON for all other applications (like Kibana's
+# [bettermap panel](https://github.com/elasticsearch/kibana/tree/master/src/app/panels/bettermap)).
+#
+# Logstash releases ship with the GeoLiteCity database made available from
+# Maxmind with a CCA-ShareAlike 3.0 license. For more details on GeoLite, see
+# <http://www.maxmind.com/en/geolite>.
+class LogStash::Filters::GeoIP < LogStash::Filters::Base
+  config_name "geoip"
+  milestone 3
+
+  # The path to the GeoIP database file which Logstash should use. Country, City, ASN, ISP
+  # and organization databases are supported.
+  #
+  # If not specified, this will default to the GeoLiteCity database that ships
+  # with Logstash.
+  config :database, :validate => :path
+
+  # The field containing the IP address or hostname to map via geoip. If
+  # this field is an array, only the first value will be used.
+  config :source, :validate => :string, :required => true
+
+  # An array of geoip fields to be included in the event.
+  #
+  # Possible fields depend on the database type. By default, all geoip fields
+  # are included in the event.
+  #
+  # For the built-in GeoLiteCity database, the following are available:
+  # `city\_name`, `continent\_code`, `country\_code2`, `country\_code3`, `country\_name`,
+  # `dma\_code`, `ip`, `latitude`, `longitude`, `postal\_code`, `region\_name` and `timezone`.
+  config :fields, :validate => :array
+
+  # Specify the field into which Logstash should store the geoip data.
+  # This can be useful, for example, if you have `src\_ip` and `dst\_ip` fields and
+  # would like the GeoIP information of both IPs.
+  #
+  # If you save the data to a target field other than "geoip" and want to use the
+  # geo\_point related functions in Elasticsearch, you need to alter the template
+  # provided with the Elasticsearch output and configure the output to use the
+  # new template.
+  #
+  # Even if you don't use the geo\_point mapping, the [target][location] field
+  # is still valid GeoJSON.
+  config :target, :validate => :string, :default => 'geoip'
+
+  public
+  def register
+    require "geoip"
+    if @database.nil?
+      @database = LogStash::Environment.vendor_path("geoip/GeoLiteCity.dat")
+      if !File.exists?(@database)
+        raise "You must specify 'database => ...' in your geoip filter (I looked for '#{@database}'"
+      end
+    end
+    @logger.info("Using geoip database", :path => @database)
+    # For the purpose of initializing this filter, geoip is initialized here but
+    # not set as a global. The geoip module imposes a mutex, so the filter needs
+    # to re-initialize this later in the filter() thread, and save that access
+    # as a thread-local variable.
+    geoip_initialize = ::GeoIP.new(@database)
+
+    @geoip_type = case geoip_initialize.database_type
+    when GeoIP::GEOIP_CITY_EDITION_REV0, GeoIP::GEOIP_CITY_EDITION_REV1
+      :city
+    when GeoIP::GEOIP_COUNTRY_EDITION
+      :country
+    when GeoIP::GEOIP_ASNUM_EDITION
+      :asn
+    when GeoIP::GEOIP_ISP_EDITION, GeoIP::GEOIP_ORG_EDITION
+      :isp
+    else
+      raise RuntimeException.new "This GeoIP database is not currently supported"
+    end
+
+    @threadkey = "geoip-#{self.object_id}"
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+    geo_data = nil
+
+    # Use thread-local access to GeoIP. The Ruby GeoIP module forces a mutex
+    # around access to the database, which can be overcome with :pread.
+    # Unfortunately, :pread requires the io-extra gem, with C extensions that
+    # aren't supported on JRuby. If / when :pread becomes available, we can stop
+    # needing thread-local access.
+    if !Thread.current.key?(@threadkey)
+      Thread.current[@threadkey] = ::GeoIP.new(@database)
+    end
+
+    begin
+      ip = event[@source]
+      ip = ip.first if ip.is_a? Array
+      geo_data = Thread.current[@threadkey].send(@geoip_type, ip)
+    rescue SocketError => e
+      @logger.error("IP Field contained invalid IP address or hostname", :field => @field, :event => event)
+    rescue Exception => e
+      @logger.error("Unknown error while looking up GeoIP data", :exception => e, :field => @field, :event => event)
+    end
+
+    return if geo_data.nil?
+
+    geo_data_hash = geo_data.to_hash
+    geo_data_hash.delete(:request)
+    event[@target] = {} if event[@target].nil?
+    geo_data_hash.each do |key, value|
+      next if value.nil? || (value.is_a?(String) && value.empty?)
+      if @fields.nil? || @fields.empty? || @fields.include?(key.to_s)
+        # convert key to string (normally a Symbol)
+        if value.is_a?(String)
+          # Some strings from GeoIP don't have the correct encoding...
+          value = case value.encoding
+            # I have found strings coming from GeoIP that are ASCII-8BIT are actually
+            # ISO-8859-1...
+            when Encoding::ASCII_8BIT; value.force_encoding(Encoding::ISO_8859_1).encode(Encoding::UTF_8)
+            when Encoding::ISO_8859_1, Encoding::US_ASCII;  value.encode(Encoding::UTF_8)
+            else; value
+          end
+        end
+        event[@target][key.to_s] = value
+      end
+    end # geo_data_hash.each
+    if event[@target].key?('latitude') && event[@target].key?('longitude')
+      # If we have latitude and longitude values, add the location field as GeoJSON array
+      event[@target]['location'] = [ event[@target]["longitude"].to_f, event[@target]["latitude"].to_f ]
+    end
+    filter_matched(event)
+  end # def filter
+end # class LogStash::Filters::GeoIP
diff --git a/lib/logstash/filters/grep.rb b/lib/logstash/filters/grep.rb
deleted file mode 100644
index 86f8ecce016..00000000000
--- a/lib/logstash/filters/grep.rb
+++ /dev/null
@@ -1,107 +0,0 @@
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# Grep filter. Useful for dropping events you don't want to pass.
-#
-# Events not matched are dropped. If 'negate' is set to true (defaults false),
-# then matching events are dropped.
-class LogStash::Filters::Grep < LogStash::Filters::Base
-
-  config_name "grep"
-
-  # Negate the match. Similar to 'grep -v'
-  #
-  # If this is set to true, then any positive matches will result in the
-  # event being cancelled and dropped. Non-matching will be allowed
-  # through.
-  config :negate, :validate => :boolean, :default => false
-
-  # A hash of matches of field => value
-  config :match, :validate => :hash, :default => {}
-
-  # Config for grep is:
-  #   fieldname: pattern
-  #   Allow arbitrary keys for this config.
-  config /[A-Za-z0-9_-]+/, :validate => :string
-
-  public
-  def register
-    @patterns = Hash.new { |h,k| h[k] = [] }
-      # TODO(sissel): 
-    @match.merge(@config).each do |field, pattern|
-      # Skip known config names
-      next if ["add_tag", "add_field", "type", "negate", "match"].include?(field)
-
-      re = Regexp.new(pattern)
-      @patterns[field] << re
-      @logger.debug(["grep: #{@type}/#{field}", pattern, re])
-    end # @config.each
-  end # def register
-
-  public
-  def filter(event)
-    if event.type != @type
-      @logger.debug("grep: skipping type #{event.type} from #{event.source}")
-      event.cancel
-      return
-    end
-
-    @logger.debug(["Running grep filter", event.to_hash, config])
-    matched = false
-    @patterns.each do |field, regexes|
-      if !event[field]
-        @logger.debug(["Skipping match object, field not present", field,
-                      event, event[field]])
-        next
-      end
-
-      # For each match object, we have to match everything in order to
-      # apply any fields/tags.
-      match_count = 0
-      match_want = 0
-      regexes.each do |re|
-        match_want += 1
-
-        # Events without this field, with negate enabled, count as a match.
-        if event[field].nil? and @negate == true
-          match_count += 1
-        end
-
-        (event[field].is_a?(Array) ? event[field] : [event[field]]).each do |value|
-          if @negate
-            @logger.debug("want negate match")
-            next if re.match(value)
-            @logger.debug(["grep not-matched (negate requsted)", { field => value }])
-          else
-            @logger.debug(["trying regex", re, value])
-            next unless re.match(value)
-            @logger.debug(["grep matched", { field => value }])
-          end
-          match_count += 1
-          break
-        end # each value in event[field]
-      end # regexes.each
-
-      if match_count == match_want
-        matched = true
-        @logger.debug("matched all fields (#{match_count})")
-      else
-        @logger.debug("match block failed " \
-                      "(#{match_count}/#{match_want} matches)")
-        event.cancel
-      end # match["match"].each
-    end # config.each
-
-    if not matched || event.cancelled?
-      @logger.debug("grep: dropping event, no matches")
-      event.cancel
-      return
-    end
-
-    @logger.debug(["Event after grep filter", event.to_hash])
-
-    if !event.cancelled?
-      filter_matched(event)
-    end
-  end # def filter
-end # class LogStash::Filters::Grep
diff --git a/lib/logstash/filters/grok.rb b/lib/logstash/filters/grok.rb
index 08b2cbaff69..9ee1506b05f 100644
--- a/lib/logstash/filters/grok.rb
+++ b/lib/logstash/filters/grok.rb
@@ -1,196 +1,361 @@
+# encoding: utf-8
 require "logstash/filters/base"
 require "logstash/namespace"
+require "logstash/environment"
+require "set"
 
 # Parse arbitrary text and structure it.
+#
 # Grok is currently the best way in logstash to parse crappy unstructured log
-# data (like syslog or apache logs) into something structured and queryable.
+# data into something structured and queryable.
+#
+# This tool is perfect for syslog logs, apache and other webserver logs, mysql
+# logs, and in general, any log format that is generally written for humans
+# and not computer consumption.
+#
+# Logstash ships with about 120 patterns by default. You can find them here:
+# <https://github.com/logstash/logstash/tree/v%VERSION%/patterns>. You can add
+# your own trivially. (See the patterns_dir setting)
+#
+# If you need help building patterns to match your logs, you will find the
+# <http://grokdebug.herokuapp.com> too quite useful!
+#
+# #### Grok Basics
+#
+# Grok works by combining text patterns into something that matches your
+# logs.
+#
+# The syntax for a grok pattern is `%{SYNTAX:SEMANTIC}`
+#
+# The `SYNTAX` is the name of the pattern that will match your text. For
+# example, "3.44" will be matched by the NUMBER pattern and "55.3.244.1" will
+# be matched by the IP pattern. The syntax is how you match.
+#
+# The `SEMANTIC` is the identifier you give to the piece of text being matched.
+# For example, "3.44" could be the duration of an event, so you could call it
+# simply 'duration'. Further, a string "55.3.244.1" might identify the 'client'
+# making a request.
+#
+# For the above example, your grok filter would look something like this:
+#
+# %{NUMBER:duration} %{IP:client}
+#
+# Optionally you can add a data type conversion to your grok pattern. By default
+# all semantics are saved as strings. If you wish to convert a semantic's data type,
+# for example change a string to an integer then suffix it with the target data type.
+# For example `%{NUMBER:num:int}` which converts the 'num' semantic from a string to an
+# integer. Currently the only supported conversions are `int` and `float`.
+#
+# #### Example
+#
+# With that idea of a syntax and semantic, we can pull out useful fields from a
+# sample log like this fictional http request log:
+#
+#     55.3.244.1 GET /index.html 15824 0.043
+#
+# The pattern for this could be:
+#
+#     %{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}
+#
+# A more realistic example, let's read these logs from a file:
+#
+#     input {
+#       file {
+#         path => "/var/log/http.log"
+#       }
+#     }
+#     filter {
+#       grok {
+#         match => { "message" => "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}" }
+#       }
+#     }
+#
+# After the grok filter, the event will have a few extra fields in it:
+#
+# * client: 55.3.244.1
+# * method: GET
+# * request: /index.html
+# * bytes: 15824
+# * duration: 0.043
 #
-# This filter requires you have libgrok installed.
+# #### Regular Expressions
 #
-# You can find libgrok here: 
-# <http://code.google.com/p/semicomplete/wiki/Grok>
+# Grok sits on top of regular expressions, so any regular expressions are valid
+# in grok as well. The regular expression library is Oniguruma, and you can see
+# the full supported regexp syntax [on the Onigiruma
+# site](http://www.geocities.jp/kosako3/oniguruma/doc/RE.txt).
 #
-# Compile/install notes can be found in the INSTALL file of the
-# grok tarball, or here: 
-# <https://github.com/jordansissel/grok/blob/master/INSTALL>o
+# #### Custom Patterns
 #
-# Key dependencies:
+# Sometimes logstash doesn't have a pattern you need. For this, you have
+# a few options.
 #
-# * libtokyocabinet > 1.4.6
-# * libpcre >= 7.6
-# * libevent >= 1.3 (though older versions may worK)
+# First, you can use the Oniguruma syntax for 'named capture' which will
+# let you match a piece of text and save it as a field:
 #
-# Note:
-# CentOS 5 ships with an ancient version of pcre that does not work with grok.
+#     (?<field_name>the pattern here)
+#
+# For example, postfix logs have a 'queue id' that is an 10 or 11-character
+# hexadecimal value. I can capture that easily like this:
+#
+#     (?<queue_id>[0-9A-F]{10,11})
+#
+# Alternately, you can create a custom patterns file.
+#
+# * Create a directory called `patterns` with a file in it called `extra`
+#   (the file name doesn't matter, but name it meaningfully for yourself)
+# * In that file, write the pattern you need as the pattern name, a space, then
+#   the regexp for that pattern.
+#
+# For example, doing the postfix queue id example as above:
+#
+#     # contents of ./patterns/postfix:
+#     POSTFIX_QUEUEID [0-9A-F]{10,11}
+#
+# Then use the `patterns_dir` setting in this plugin to tell logstash where
+# your custom patterns directory is. Here's a full example with a sample log:
+#
+#     Jan  1 06:25:43 mailserver14 postfix/cleanup[21403]: BEF25A72965: message-id=<20130101142543.5828399CCAF@mailserver14.example.com>
+#
+#     filter {
+#       grok {
+#         patterns_dir => "./patterns"
+#         match => { "message" => "%{SYSLOGBASE} %{POSTFIX_QUEUEID:queue_id}: %{GREEDYDATA:syslog_message}" }
+#       }
+#     }
+#
+# The above will match and result in the following fields:
+#
+# * timestamp: Jan  1 06:25:43
+# * logsource: mailserver14
+# * program: postfix/cleanup
+# * pid: 21403
+# * queue_id: BEF25A72965
+# * syslog_message: message-id=<20130101142543.5828399CCAF@mailserver14.example.com>
+#
+# The `timestamp`, `logsource`, `program`, and `pid` fields come from the
+# SYSLOGBASE pattern which itself is defined by other patterns.
 class LogStash::Filters::Grok < LogStash::Filters::Base
   config_name "grok"
+  milestone 3
+
+  # Specify a pattern to parse with. This will match the 'message' field.
+  #
+  # If you want to match other fields than message, use the 'match' setting.
+  # Multiple patterns is fine.
+  config :pattern, :validate => :array, :deprecated => "You should use this instead: match => { \"message\" => \"your pattern here\" }"
 
-  # Specify a pattern to parse with.
-  # Multiple patterns is fine. First match breaks.
-  config :pattern, :validate => :array, :required => true
+  # A hash of matches of field => value
+  #
+  # For example:
+  #
+  #     filter {
+  #       grok { match => { "message" => "Duration: %{NUMBER:duration}" } }
+  #     }
+  #
+  # Alternatively, using the old array syntax:
+  #
+  #     filter {
+  #       grok { match => [ "message", "Duration: %{NUMBER:duration}" ] }
+  #     }
+  #
+  config :match, :validate => :hash, :default => {}
 
-  # Specify a path to a directory with grok pattern files in it
   #
   # logstash ships by default with a bunch of patterns, so you don't
   # necessarily need to define this yourself unless you are adding additional
   # patterns.
   #
   # Pattern files are plain text with format:
-  # 
+  #
   #     NAME PATTERN
   #
   # For example:
   #
   #     NUMBER \d+
-  config :patterns_dir, :validate => :array
+  config :patterns_dir, :validate => :array, :default => []
 
   # Drop if matched. Note, this feature may not stay. It is preferable to combine
   # grok + grep filters to do parsing + dropping.
-  #
-  # requested in: googlecode/issue/26
   config :drop_if_match, :validate => :boolean, :default => false
 
-  class << self
-    attr_accessor :patterns_dir
-  end
+  # Break on first match. The first successful match by grok will result in the
+  # filter being finished. If you want grok to try all patterns (maybe you are
+  # parsing different things), then set this to false.
+  config :break_on_match, :validate => :boolean, :default => true
+
+  # If true, only store named captures from grok.
+  config :named_captures_only, :validate => :boolean, :default => true
+
+  # If true, keep empty captures as event fields.
+  config :keep_empty_captures, :validate => :boolean, :default => false
+
+  # If true, make single-value fields simply that value, not an array
+  # containing that one value.
+  config :singles, :validate => :boolean, :default => true, :deprecated => "This behavior is the default now, you don't need to set it."
+
+  # Append values to the 'tags' field when there has been no
+  # successful match
+  config :tag_on_failure, :validate => :array, :default => ["_grokparsefailure"]
+
+  # The fields to overwrite.
+  #
+  # This allows you to overwrite a value in a field that already exists.
+  #
+  # For example, if you have a syslog line in the 'message' field, you can
+  # overwrite the 'message' field with part of the match like so:
+  #
+  #     filter {
+  #       grok {
+  #         match => { "message" => "%{SYSLOGBASE} %{DATA:message}" }
+  #         overwrite => [ "message" ]
+  #       }
+  #     }
+  #
+  #  In this case, a line like "May 29 16:37:11 sadness logger: hello world"
+  #  will be parsed and 'hello world' will overwrite the original message.
+  config :overwrite, :validate => :array, :default => []
 
   # Detect if we are running from a jarfile, pick the right path.
-  if __FILE__ =~ /file:\/.*\.jar!.*/
-    self.patterns_dir = ["#{File.dirname(__FILE__)}/../../patterns/*"]
-  else
-    self.patterns_dir = ["#{File.dirname(__FILE__)}/../../../patterns/*"]
-  end
+  @@patterns_path ||= Set.new
+  @@patterns_path += [LogStash::Environment.pattern_path("*")]
 
-  # This flag becomes "--grok-patterns-path"
-  flag("--patterns-path PATH", "Colon-delimited path of patterns to load") do |val|
-    @patterns_dir += val.split(":")
+  public
+  def initialize(params)
+    super(params)
+    @match["message"] ||= []
+    @match["message"] += @pattern if @pattern # the config 'pattern' value (array)
+    # a cache of capture name handler methods.
+    @handlers = {}
   end
 
-  @@grokpiles = Hash.new { |h, k| h[k] = [] }
-  @@grokpiles_lock = Mutex.new
-
   public
   def register
-    gem "jls-grok", ">=0.4.3"
-    require "grok" # rubygem 'jls-grok'
-
-    @pile = Grok::Pile.new
-    @logger.info("Grok patterns paths: #{self.class.patterns_dir.inspect}")
-    self.class.patterns_dir.each do |path|
-      # Can't read relative paths from jars, try to normalize away '../'
-      while path =~ /file:\/.*\.jar!.*\/\.\.\//
-        # replace /foo/bar/../baz => /foo/baz
-        path.gsub!(/[^\/]+\/\.\.\//, "")
-        @logger.debug "In-jar path to read: #{path}"
-      end
+    require "grok-pure" # rubygem 'jls-grok'
+
+    @patternfiles = []
 
+    # Have @@patterns_path show first. Last-in pattern definitions win; this
+    # will let folks redefine built-in patterns at runtime.
+    @patterns_dir = @@patterns_path.to_a + @patterns_dir
+    @logger.info? and @logger.info("Grok patterns path", :patterns_dir => @patterns_dir)
+    @patterns_dir.each do |path|
       if File.directory?(path)
         path = File.join(path, "*")
       end
 
       Dir.glob(path).each do |file|
-        @logger.info("Grok loading patterns from #{file}")
-        add_patterns_from_file(file)
+        @logger.info? and @logger.info("Grok loading patterns from file", :path => file)
+        @patternfiles << file
       end
     end
 
-    @pattern.each do |pattern|
-      groks = @pile.compile(pattern)
-      @logger.debug(["Compiled pattern", pattern, groks[-1].expanded_pattern])
-    end
+    @patterns = Hash.new { |h,k| h[k] = [] }
 
-    @@grokpiles_lock.synchronize do
-      @@grokpiles[@type] << @pile
-    end
+    @logger.info? and @logger.info("Match data", :match => @match)
+
+    @match.each do |field, patterns|
+      patterns = [patterns] if patterns.is_a?(String)
+
+      @logger.info? and @logger.info("Grok compile", :field => field, :patterns => patterns)
+      patterns.each do |pattern|
+        @logger.debug? and @logger.debug("regexp: #{@type}/#{field}", :pattern => pattern)
+        grok = Grok.new
+        grok.logger = @logger unless @logger.nil?
+        add_patterns_from_files(@patternfiles, grok)
+        grok.compile(pattern, @named_captures_only)
+        @patterns[field] << grok
+      end
+    end # @match.each
   end # def register
 
   public
   def filter(event)
-    # parse it with grok
-    match = false
+    return unless filter?(event)
 
-    if !event.message.is_a?(Array)
-      messages = [event.message]
+    matched = false
+    done = false
+
+    @logger.debug? and @logger.debug("Running grok filter", :event => event);
+    @patterns.each do |field, groks|
+      if match(groks, field, event)
+        matched = true
+        break if @break_on_match
+      end
+      #break if done
+    end # @patterns.each
+
+    if matched
+      filter_matched(event)
     else
-      messages = event.message
+      # Tag this event if we can't parse it. We can use this later to
+      # reparse+reindex logs if we improve the patterns given.
+      @tag_on_failure.each do |tag|
+        event["tags"] ||= []
+        event["tags"] << tag unless event["tags"].include?(tag)
+      end
     end
 
-    messages.each do |message|
-      @logger.debug(["Running grok filter", event])
+    @logger.debug? and @logger.debug("Event now: ", :event => event)
+  end # def filter
 
-      @@grokpiles[event.type].each do |pile|
-        @logger.debug(["Trying pattern", pile])
-        grok, match = @pile.match(message)
-        @logger.debug(["Result", { :grok => grok, :match => match }])
-        break if match
+  private
+  def match(groks, field, event)
+    input = event[field]
+    if input.is_a?(Array)
+      success = false
+      input.each do |input|
+        success |= match_against_groks(groks, input, event)
       end
+      return success
+    else
+      return match_against_groks(groks, input, event)
+    end
+  rescue StandardError => e
+    @logger.warn("Grok regexp threw exception", :exception => e.message)
+  end
 
-      if match
-        match.each_capture do |key, value|
-          match_type = nil
-          if key.include?(":")
-            name, key, match_type = key.split(":")
-          end
-
-          # http://code.google.com/p/logstash/issues/detail?id=45
-          # Permit typing of captures by giving an additional colon and a type,
-          # like: %{FOO:name:int} for int coercion.
-          case match_type
-            when "int"
-              value = value.to_i
-            when "float"
-              value = value.to_f
-          end
-
-          if event.message == value
-            # Skip patterns that match the entire line
-            @logger.debug("Skipping capture '#{key}' since it matches the whole line.")
-            next
-          end
-
-          if event.fields[key].is_a?(String)
-            event.fields[key] = [event.fields[key]]
-          elsif event.fields[key] == nil
-            event.fields[key] = []
-          end
-
-          # If value is not nil, or responds to empty and is not empty, add the
-          # value to the event.
-          if !value.nil? && (!value.empty? rescue true)
-            event.fields[key] << value
-          end
-        end
-        filter_matched(event)
-      else
-        # Tag this event if we can't parse it. We can use this later to
-        # reparse+reindex logs if we improve the patterns given .
-        event.tags << "_grokparsefailure"
+  private
+  def match_against_groks(groks, input, event)
+    matched = false
+    groks.each do |grok|
+      # Convert anything else to string (number, hash, etc)
+      matched = grok.match_and_capture(input.to_s) do |field, value|
+        matched = true
+        handle(field, value, event)
       end
-    end # message.each
+      break if matched and @break_on_match
+    end
+    return matched
+  end
 
-    #if !event.cancelled?
-      #filter_matched(event)
-    #end
-    @logger.debug(["Event now: ", event.to_hash])
-  end # def filter
+  private
+  def handle(field, value, event)
+    return if (value.nil? || (value.is_a?(String) && value.empty?)) unless @keep_empty_captures
+
+    if @overwrite.include?(field)
+      event[field] = value
+    else
+      v = event[field]
+      if v.nil?
+        event[field] = value
+      elsif v.is_a?(Array)
+        event[field] << value
+      elsif v.is_a?(String)
+        # Promote to array since we aren't overwriting.
+        event[field] = [v, value]
+      end
+    end
+  end
 
   private
-  def add_patterns_from_file(file)
-    # Check if the file path is a jar, if so, we'll have to read it ourselves
-    # since libgrok won't know what to do with it.
-    if file =~ /file:\/.*\.jar!.*/
-      File.new(file).each do |line|
-        next if line =~ /^(?:\s*#|\s*$)/
-        # In some cases I have seen 'file.each' yield lines with newlines at
-        # the end. I don't know if this is a bug or intentional, but we need
-        # to chomp it.
-        name, pattern = line.chomp.split(/\s+/, 2)
-        @logger.debug "Adding pattern '#{name}' from file #{file}"
-        @logger.debug name => pattern
-        @pile.add_pattern(name, pattern)
+  def add_patterns_from_files(paths, grok)
+    paths.each do |path|
+      if !File.exists?(path)
+        raise "Grok pattern file does not exist: #{path}"
       end
-    else
-      @pile.add_patterns_from_file(file)
+      grok.add_patterns_from_file(path)
     end
-  end # def add_patterns
+  end # def add_patterns_from_files
+
 end # class LogStash::Filters::Grok
diff --git a/lib/logstash/filters/grokdiscovery.rb b/lib/logstash/filters/grokdiscovery.rb
index 4189e97ef73..6541e2de533 100644
--- a/lib/logstash/filters/grokdiscovery.rb
+++ b/lib/logstash/filters/grokdiscovery.rb
@@ -1,3 +1,4 @@
+# encoding: utf-8
 require "logstash/filters/base"
 require "logstash/namespace"
 
@@ -6,6 +7,7 @@
 class LogStash::Filters::Grokdiscovery < LogStash::Filters::Base
 
   config_name "grokdiscovery"
+  milestone 1
 
   public
   def initialize(config = {})
@@ -34,14 +36,16 @@ def register
 
   public
   def filter(event)
+    return unless filter?(event)
+
     # parse it with grok
-    message = event.message
+    message = event["message"]
     match = false
 
     if event.type and @discover_fields.include?(event.type)
-      discover = @discover_fields[event.type] & event.fields.keys
+      discover = @discover_fields[event.type] & event.to_hash.keys
       discover.each do |field|
-        value = event.fields[field]
+        value = event[field]
         value = [value] if value.is_a?(String)
 
         value.each do |v| 
@@ -51,7 +55,7 @@ def filter(event)
           match = @grok.match(v)
           if match
             @logger.warn(["Match", match.captures])
-            event.fields.merge!(match.captures) do |key, oldval, newval|
+            event.to_hash.merge!(match.captures) do |key, oldval, newval|
               @logger.warn(["Merging #{key}", oldval, newval])
               oldval + newval # should both be arrays...
             end
@@ -65,5 +69,7 @@ def filter(event)
       @logger.debug(event.to_hash)
     end
     @logger.debug(["Event now: ", event.to_hash])
+
+    filter_matched(event) if !event.cancelled?
   end # def filter
 end # class LogStash::Filters::Grokdiscovery
diff --git a/lib/logstash/filters/json.rb b/lib/logstash/filters/json.rb
new file mode 100644
index 00000000000..688844ff707
--- /dev/null
+++ b/lib/logstash/filters/json.rb
@@ -0,0 +1,104 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+require "logstash/json"
+require "logstash/timestamp"
+
+# This is a JSON parsing filter. It takes an existing field which contains JSON and
+# expands it into an actual data structure within the Logstash event.
+#
+# By default it will place the parsed JSON in the root (top level) of the Logstash event, but this
+# filter can be configured to place the JSON into any arbitrary event field, using the
+# `target` configuration.
+class LogStash::Filters::Json < LogStash::Filters::Base
+
+  config_name "json"
+  milestone 2
+
+  # The configuration for the JSON filter:
+  #
+  #     source => source_field
+  #
+  # For example, if you have JSON data in the @message field:
+  #
+  #     filter {
+  #       json {
+  #         source => "message"
+  #       }
+  #     }
+  #
+  # The above would parse the json from the @message field
+  config :source, :validate => :string, :required => true
+
+  # Define the target field for placing the parsed data. If this setting is
+  # omitted, the JSON data will be stored at the root (top level) of the event.
+  #
+  # For example, if you want the data to be put in the 'doc' field:
+  #
+  #     filter {
+  #       json {
+  #         target => "doc"
+  #       }
+  #     }
+  #
+  # JSON in the value of the `source` field will be expanded into a
+  # data structure in the `target` field.
+  #
+  # NOTE: if the `target` field already exists, it will be overwritten!
+  config :target, :validate => :string
+
+  public
+  def register
+    # Nothing to do here
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+
+    @logger.debug("Running json filter", :event => event)
+
+    return unless event.include?(@source)
+
+    # TODO(colin) this field merging stuff below should be handled in Event.
+
+    source = event[@source]
+    if @target.nil?
+      # Default is to write to the root of the event.
+      dest = event.to_hash
+    else
+      if @target == @source
+        # Overwrite source
+        dest = event[@target] = {}
+      else
+        dest = event[@target] ||= {}
+      end
+    end
+
+    begin
+      # TODO(sissel): Note, this will not successfully handle json lists
+      # like your text is '[ 1,2,3 ]' json parser gives you an array (correctly)
+      # which won't merge into a hash. If someone needs this, we can fix it
+      # later.
+      dest.merge!(LogStash::Json.load(source))
+
+      # If no target, we target the root of the event object. This can allow
+      # you to overwrite @timestamp and this will typically happen for json
+      # LogStash Event deserialized here.
+      if !@target && event.timestamp.is_a?(String)
+        event.timestamp = LogStash::Timestamp.parse_iso8601(event.timestamp)
+      end
+
+      filter_matched(event)
+    rescue => e
+      event.tag("_jsonparsefailure")
+      @logger.warn("Trouble parsing json", :source => @source,
+                   :raw => event[@source], :exception => e)
+      return
+    end
+
+    @logger.debug("Event after json filter", :event => event)
+
+  end # def filter
+
+end # class LogStash::Filters::Json
diff --git a/lib/logstash/filters/kv.rb b/lib/logstash/filters/kv.rb
new file mode 100644
index 00000000000..0786233c643
--- /dev/null
+++ b/lib/logstash/filters/kv.rb
@@ -0,0 +1,237 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# This filter helps automatically parse messages (or specific event fields)
+# which are of the 'foo=bar' variety.
+#
+# For example, if you have a log message which contains 'ip=1.2.3.4
+# error=REFUSED', you can parse those automatically by configuring:
+#
+#     filter {
+#       kv { }
+#     }
+#
+# The above will result in a message of "ip=1.2.3.4 error=REFUSED" having
+# the fields:
+#
+# * ip: 1.2.3.4
+# * error: REFUSED
+#
+# This is great for postfix, iptables, and other types of logs that
+# tend towards 'key=value' syntax.
+#
+# You can configure any arbitrary strings to split your data on,
+# in case your data is not structured using '=' signs and whitespace.
+# For example, this filter can also be used to parse query parameters like
+# 'foo=bar&baz=fizz' by setting the `field_split` parameter to "&".
+class LogStash::Filters::KV < LogStash::Filters::Base
+  config_name "kv"
+  milestone 2
+
+  # A string of characters to trim from the value. This is useful if your
+  # values are wrapped in brackets or are terminated with commas (like postfix
+  # logs).
+  #
+  # These characters form a regex character class and thus you must escape special regex
+  # characters like '[' or ']' using '\'.
+  #
+  # For example, to strip '<', '>', '[', ']' and ',' characters from values:
+  #
+  #     filter {
+  #       kv {
+  #         trim => "<>\[\],"
+  #       }
+  #     }
+  config :trim, :validate => :string
+
+  # A string of characters to trim from the key. This is useful if your
+  # keys are wrapped in brackets or start with space.
+  #
+  # These characters form a regex character class and thus you must escape special regex
+  # characters like '[' or ']' using '\'.
+  #
+  # For example, to strip '<' '>' '[' ']' and ',' characters from keys:
+  #
+  #     filter {
+  #       kv {
+  #         trimkey => "<>\[\],"
+  #       }
+  #     }
+  config :trimkey, :validate => :string
+
+  # A string of characters to use as delimiters for parsing out key-value pairs.
+  #
+  # These characters form a regex character class and thus you must escape special regex
+  # characters like '[' or ']' using '\'.
+  #
+  # #### Example with URL Query Strings
+  #
+  # For example, to split out the args from a url query string such as
+  # '?pin=12345~0&d=123&e=foo@bar.com&oq=bobo&ss=12345':
+  #
+  #     filter {
+  #       kv {
+  #         field_split => "&?"
+  #       }
+  #     }
+  #
+  # The above splits on both "&" and "?" characters, giving you the following
+  # fields:
+  #
+  # * pin: 12345~0
+  # * d: 123
+  # * e: foo@bar.com
+  # * oq: bobo
+  # * ss: 12345
+  config :field_split, :validate => :string, :default => ' '
+
+
+  # A string of characters to use as delimiters for identifying key-value relations.
+  #
+  # These characters form a regex character class and thus you must escape special regex
+  # characters like '[' or ']' using '\'.
+  #
+  # For example, to identify key-values such as
+  # 'key1:value1 key2:value2':
+  #
+  #     filter { kv { value_split => ":" } }
+  config :value_split, :validate => :string, :default => '='
+
+  # A string to prepend to all of the extracted keys.
+  #
+  # For example, to prepend arg_ to all keys:
+  #
+  #     filter { kv { prefix => "arg_" } }
+  config :prefix, :validate => :string, :default => ''
+
+  # The field to perform 'key=value' searching on
+  #
+  # For example, to process the `not_the_message` field:
+  #
+  #     filter { kv { source => "not_the_message" } }
+  config :source, :validate => :string, :default => "message"
+
+  # The name of the container to put all of the key-value pairs into.
+  #
+  # If this setting is omitted, fields will be written to the root of the
+  # event, as individual fields.
+  #
+  # For example, to place all keys into the event field kv:
+  #
+  #     filter { kv { target => "kv" } }
+  config :target, :validate => :string
+
+  # An array specifying the parsed keys which should be added to the event.
+  # By default all keys will be added.
+  #
+  # For example, consider a source like "Hey, from=<abc>, to=def foo=bar". 
+  # To include "from" and "to", but exclude the "foo" key, you could use this configuration:
+  #     filter {
+  #       kv {
+  #         include_keys => [ "from", "to" ]
+  #       }
+  #     }
+  config :include_keys, :validate => :array, :default => []
+
+  # An array specifying the parsed keys which should not be added to the event.
+  # By default no keys will be excluded.
+  #
+  # For example, consider a source like "Hey, from=<abc>, to=def foo=bar". 
+  # To exclude "from" and "to", but retain the "foo" key, you could use this configuration:
+  #     filter {
+  #       kv {
+  #         exclude_keys => [ "from", "to" ]
+  #       }
+  #     }
+  config :exclude_keys, :validate => :array, :default => []
+
+  # A hash specifying the default keys and their values which should be added to the event
+  # in case these keys do not exist in the source field being parsed.
+  #
+  #     filter {
+  #       kv {
+  #         default_keys => [ "from", "logstash@example.com",
+  #                          "to", "default@dev.null" ]
+  #       }
+  #     }
+  config :default_keys, :validate => :hash, :default => {}
+
+  def register
+    @trim_re = Regexp.new("[#{@trim}]") if !@trim.nil?
+    @trimkey_re = Regexp.new("[#{@trimkey}]") if !@trimkey.nil?
+    @scan_re = Regexp.new("((?:\\\\ |[^"+@field_split+@value_split+"])+)["+@value_split+"](?:\"([^\"]+)\"|'([^']+)'|((?:\\\\ |[^"+@field_split+"])+))")
+  end # def register
+
+  def filter(event)
+    return unless filter?(event)
+
+    kv = Hash.new
+
+    value = event[@source]
+
+    case value
+      when nil; # Nothing to do
+      when String; kv = parse(value, event, kv)
+      when Array; value.each { |v| kv = parse(v, event, kv) }
+      else
+        @logger.warn("kv filter has no support for this type of data",
+                     :type => value.class, :value => value)
+    end # case value
+
+    # Add default key-values for missing keys
+    kv = @default_keys.merge(kv)
+
+    # If we have any keys, create/append the hash
+    if kv.length > 0
+      if @target.nil?
+        # Default is to write to the root of the event.
+        dest = event.to_hash
+      else
+        if !event[@target].is_a?(Hash)
+          @logger.debug("Overwriting existing target field", :target => @target)
+          dest = event[@target] = {}
+        else
+          dest = event[@target]
+        end
+      end
+
+      dest.merge!(kv)
+      filter_matched(event)
+    end
+  end # def filter
+
+  private
+  def parse(text, event, kv_keys)
+    if !event =~ /[@field_split]/
+      return kv_keys
+    end
+    
+    # Interpret dynamic keys for @include_keys and @exclude_keys
+    include_keys = @include_keys.map{|key| event.sprintf(key)}
+    exclude_keys = @exclude_keys.map{|key| event.sprintf(key)}
+    
+    text.scan(@scan_re) do |key, v1, v2, v3|
+      value = v1 || v2 || v3
+      key = @trimkey.nil? ? key : key.gsub(@trimkey_re, "")
+      
+      # Bail out as per the values of include_keys and exclude_keys
+      next if not include_keys.empty? and not include_keys.include?(key)
+      next if exclude_keys.include?(key)
+
+      key = event.sprintf(@prefix) + key
+
+      value = @trim.nil? ? value : value.gsub(@trim_re, "")
+      if kv_keys.has_key?(key)
+        if kv_keys[key].is_a? Array
+          kv_keys[key].push(value)
+        else
+          kv_keys[key] = [kv_keys[key], value]
+        end
+      else
+        kv_keys[key] = value
+      end
+    end
+    return kv_keys
+  end
+end # class LogStash::Filters::KV
diff --git a/lib/logstash/filters/memorize.rb b/lib/logstash/filters/memorize.rb
new file mode 100644
index 00000000000..a45356cb512
--- /dev/null
+++ b/lib/logstash/filters/memorize.rb
@@ -0,0 +1,85 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+require "set"
+#
+# This filter will look for fields from an event and record the last value
+# of them.  If any are not present, their last value will be added to the
+# event.  This is useful if you want to use data from a previous event
+# on future events (for example a time field or an id field).  This differs
+# frome the multiline filter where you are combining multiple lines to
+# create a single event.
+#
+# The config looks like this:
+#
+#     filter {
+#       memorize {
+#         fields => [ "time", "id" ]
+#         default => { "time" => "00:00:00.000" }
+#       }
+#     }
+#
+# The `fields` is an array of the field NAMES that you want to memorize
+# The `default` is a map of field names to field values that you want
+# to use if the field isn't present and has no memorized value (optional)
+
+class LogStash::Filters::Memorize < LogStash::Filters::Base
+
+  config_name "memorize"
+  milestone 2
+
+  # An array of the field names to to memorize
+  config :fields, :validate => :array, :required => true
+  # a map for default values to use if its not seen before we need it
+  config :default, :validate => :hash, :required => false
+
+  # The stream identity is how the filter determines which stream an
+  # event belongs to. See the multiline plugin if you want more details on how
+  # this might work
+  config :stream_identity , :validate => :string, :default => "%{host}.%{path}.%{type}"
+
+  public
+  def initialize(config = {})
+    super
+
+    @threadsafe = false
+
+    # This filter needs to keep state.
+    @memorized = Hash.new
+  end # def initialize
+
+  public
+  def register
+	# nothing needed
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+
+    any = false
+    @fields.each do |field|
+      if event[field].nil?
+	map = @memorized[@stream_identity]
+        val = map.nil? ? nil : map[field]
+        if val.nil?
+          val = @default.nil? ? nil : @default[field]
+        end
+	if !val.nil?
+          event[field] = val
+          any = true
+	end
+      else
+        map = @memorized[@stream_identity]
+	if map.nil?
+          map = @memorized[@stream_identity] = Hash.new
+	end
+	val = event[field]
+	map[field] = event[field]
+      end #if
+      if any
+        filter_matched(event)
+      end
+    end #field.each
+  end
+end
diff --git a/lib/logstash/filters/metrics.rb b/lib/logstash/filters/metrics.rb
new file mode 100644
index 00000000000..8d60f7498a5
--- /dev/null
+++ b/lib/logstash/filters/metrics.rb
@@ -0,0 +1,241 @@
+# encoding: utf-8
+require "securerandom"
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# The metrics filter is useful for aggregating metrics.
+#
+# For example, if you have a field 'response' that is
+# a http response code, and you want to count each
+# kind of response, you can do this:
+#
+#     filter {
+#       metrics {
+#         meter => [ "http.%{response}" ]
+#         add_tag => "metric"
+#       }
+#     }
+#
+# Metrics are flushed every 5 seconds by default or according to
+# 'flush_interval'. Metrics appear as
+# new events in the event stream and go through any filters
+# that occur after as well as outputs.
+#
+# In general, you will want to add a tag to your metrics and have an output
+# explicitly look for that tag.
+#
+# The event that is flushed will include every 'meter' and 'timer'
+# metric in the following way:
+#
+# #### 'meter' values
+#
+# For a `meter => "something"` you will receive the following fields:
+#
+# * "thing.count" - the total count of events
+# * "thing.rate_1m" - the 1-minute rate (sliding)
+# * "thing.rate_5m" - the 5-minute rate (sliding)
+# * "thing.rate_15m" - the 15-minute rate (sliding)
+#
+# #### 'timer' values
+#
+# For a `timer => [ "thing", "%{duration}" ]` you will receive the following fields:
+#
+# * "thing.count" - the total count of events
+# * "thing.rate_1m" - the 1-minute rate of events (sliding)
+# * "thing.rate_5m" - the 5-minute rate of events (sliding)
+# * "thing.rate_15m" - the 15-minute rate of events (sliding)
+# * "thing.min" - the minimum value seen for this metric
+# * "thing.max" - the maximum value seen for this metric
+# * "thing.stddev" - the standard deviation for this metric
+# * "thing.mean" - the mean for this metric
+# * "thing.pXX" - the XXth percentile for this metric (see `percentiles`)
+#
+# #### Example: computing event rate
+#
+# For a simple example, let's track how many events per second are running
+# through logstash:
+#
+#     input {
+#       generator {
+#         type => "generated"
+#       }
+#     }
+#
+#     filter {
+#       if [type] == "generated" {
+#         metrics {
+#           meter => "events"
+#           add_tag => "metric"
+#         }
+#       }
+#     }
+#
+#     output {
+#       # only emit events with the 'metric' tag
+#       if "metric" in [tags] {
+#         stdout {
+#           codec => line {
+#             format => "rate: %{events.rate_1m}"
+#           }
+#         }
+#       }
+#     }
+#
+# Running the above:
+#
+#     % bin/logstash -f example.conf
+#     rate: 23721.983566819246
+#     rate: 24811.395722536377
+#     rate: 25875.892745934525
+#     rate: 26836.42375967113
+#
+# We see the output includes our 'events' 1-minute rate.
+#
+# In the real world, you would emit this to graphite or another metrics store,
+# like so:
+#
+#     output {
+#       graphite {
+#         metrics => [ "events.rate_1m", "%{events.rate_1m}" ]
+#       }
+#     }
+class LogStash::Filters::Metrics < LogStash::Filters::Base
+  config_name "metrics"
+  milestone 1
+
+  # syntax: `meter => [ "name of metric", "name of metric" ]`
+  config :meter, :validate => :array, :default => []
+
+  # syntax: `timer => [ "name of metric", "%{time_value}" ]`
+  config :timer, :validate => :hash, :default => {}
+
+  # Don't track events that have @timestamp older than some number of seconds.
+  #
+  # This is useful if you want to only include events that are near real-time
+  # in your metrics.
+  #
+  # Example, to only count events that are within 10 seconds of real-time, you
+  # would do this:
+  #
+  #     filter {
+  #       metrics {
+  #         meter => [ "hits" ]
+  #         ignore_older_than => 10
+  #       }
+  #     }
+  config :ignore_older_than, :validate => :number, :default => 0
+
+  # The flush interval, when the metrics event is created. Must be a multiple of 5s.
+  config :flush_interval, :validate => :number, :default => 5
+
+  # The clear interval, when all counter are reset.
+  #
+  # If set to -1, the default value, the metrics will never be cleared.
+  # Otherwise, should be a multiple of 5s.
+  config :clear_interval, :validate => :number, :default => -1
+
+  # The rates that should be measured, in minutes.
+  # Possible values are 1, 5, and 15.
+  config :rates, :validate => :array, :default => [1, 5, 15]
+
+  # The percentiles that should be measured
+  config :percentiles, :validate => :array, :default => [1, 5, 10, 90, 95, 99, 100]
+
+  def register
+    require "metriks"
+    require "socket"
+    require "atomic"
+    require "thread_safe"
+    @last_flush = Atomic.new(0) # how many seconds ago the metrics where flushed.
+    @last_clear = Atomic.new(0) # how many seconds ago the metrics where cleared.
+    @random_key_preffix = SecureRandom.hex
+    unless (@rates - [1, 5, 15]).empty?
+      raise LogStash::ConfigurationError, "Invalid rates configuration. possible rates are 1, 5, 15. Rates: #{rates}."
+    end
+    @metric_meters = ThreadSafe::Cache.new { |h,k| h[k] = Metriks.meter metric_key(k) }
+    @metric_timers = ThreadSafe::Cache.new { |h,k| h[k] = Metriks.timer metric_key(k) }
+  end # def register
+
+  def filter(event)
+    return unless filter?(event)
+
+    # TODO(piavlo): This should probably be moved to base filter class.
+    if @ignore_older_than > 0 && Time.now - event.timestamp.time > @ignore_older_than
+      @logger.debug("Skipping metriks for old event", :event => event)
+      return
+    end
+
+    @meter.each do |m|
+      @metric_meters[event.sprintf(m)].mark
+    end
+
+    @timer.each do |name, value|
+      @metric_timers[event.sprintf(name)].update(event.sprintf(value).to_f)
+    end
+  end # def filter
+
+  def flush
+    # Add 5 seconds to @last_flush and @last_clear counters
+    # since this method is called every 5 seconds.
+    @last_flush.update { |v| v + 5 }
+    @last_clear.update { |v| v + 5 }
+
+    # Do nothing if there's nothing to do ;)
+    return unless should_flush?
+
+    event = LogStash::Event.new
+    event["message"] = Socket.gethostname
+    @metric_meters.each_pair do |name, metric|
+      flush_rates event, name, metric
+      metric.clear if should_clear?
+    end
+
+    @metric_timers.each_pair do |name, metric|
+      flush_rates event, name, metric
+      # These 4 values are not sliding, so they probably are not useful.
+      event["#{name}.min"] = metric.min
+      event["#{name}.max"] = metric.max
+      # timer's stddev currently returns variance, fix it.
+      event["#{name}.stddev"] = metric.stddev ** 0.5
+      event["#{name}.mean"] = metric.mean
+
+      @percentiles.each do |percentile|
+        event["#{name}.p#{percentile}"] = metric.snapshot.value(percentile / 100.0)
+      end
+      metric.clear if should_clear?
+    end
+
+    # Reset counter since metrics were flushed
+    @last_flush.value = 0
+
+    if should_clear?
+      #Reset counter since metrics were cleared
+      @last_clear.value = 0
+      @metric_meters.clear
+      @metric_timers.clear
+    end
+
+    filter_matched(event)
+    return [event]
+  end
+
+  private
+  def flush_rates(event, name, metric)
+      event["#{name}.count"] = metric.count
+      event["#{name}.rate_1m"] = metric.one_minute_rate if @rates.include? 1
+      event["#{name}.rate_5m"] = metric.five_minute_rate if @rates.include? 5
+      event["#{name}.rate_15m"] = metric.fifteen_minute_rate if @rates.include? 15
+  end
+
+  def metric_key(key)
+    "#{@random_key_preffix}_#{key}"
+  end
+
+  def should_flush?
+    @last_flush.value >= @flush_interval && (!@metric_meters.empty? || !@metric_timers.empty?)
+  end
+
+  def should_clear?
+    @clear_interval > 0 && @last_clear.value >= @clear_interval
+  end
+end # class LogStash::Filters::Metrics
diff --git a/lib/logstash/filters/multiline.rb b/lib/logstash/filters/multiline.rb
index 029b7b29244..a4d0b6298b0 100644
--- a/lib/logstash/filters/multiline.rb
+++ b/lib/logstash/filters/multiline.rb
@@ -1,19 +1,17 @@
-# multiline filter
-#
-# This filter will collapse multiline messages into a single event.
-# 
-
+# encoding: utf-8
 require "logstash/filters/base"
 require "logstash/namespace"
-
-# The multiline filter is for combining multiple events from a single source
-# into the same event.
+require "logstash/environment"
+require "set"
+#
+# This filter will collapse multiline messages from a single source into one Logstash event.
 #
 # The original goal of this filter was to allow joining of multi-line messages
 # from files into a single event. For example - joining java exception and
 # stacktrace messages into a single event.
 #
-# TODO(sissel): Document any issues?
+# NOTE: This filter will not work with multiple worker threads "-w 2" on the logstash command line.
+#
 # The config looks like this:
 #
 #     filter {
@@ -24,28 +22,28 @@
 #         what => "previous" or "next"
 #       }
 #     }
-# 
-# The 'regexp' should match what you believe to be an indicator that
-# the field is part of a multi-line event
 #
-# The 'what' must be "previous" or "next" and indicates the relation
+# The `pattern` should be a regexp which matches what you believe to be an indicator
+# that the field is part of an event consisting of multiple lines of log data.
+#
+# The `what` must be "previous" or "next" and indicates the relation
 # to the multi-line event.
 #
-# The 'negate' can be "true" or "false" (defaults false). If true, a 
+# The `negate` can be "true" or "false" (defaults to false). If "true", a
 # message not matching the pattern will constitute a match of the multiline
-# filter and the what will be applied. (vice-versa is also true)
+# filter and the `what` will be applied. (vice-versa is also true)
+#
+# For example, Java stack traces are multiline and usually have the message
+# starting at the far-left, with each subsequent line indented. Do this:
 #
-# For example, java stack traces are multiline and usually have the message
-# starting at the far-left, then each subsequent line indented. Do this:
-# 
 #     filter {
 #       multiline {
 #         type => "somefiletype"
-#         pattern => "^\\s"
+#         pattern => "^\s"
 #         what => "previous"
 #       }
 #     }
-#     
+#
 # This says that any line starting with whitespace belongs to the previous line.
 #
 # Another example is C line continuations (backslash). Here's how to do that:
@@ -57,117 +55,225 @@
 #         what => "next"
 #       }
 #     }
-#     
+#
+# This says that any line ending with a backslash should be combined with the
+# following line.
+#
 class LogStash::Filters::Multiline < LogStash::Filters::Base
 
   config_name "multiline"
+  milestone 3
 
-  # The regular expression to match
-  config :pattern, :validate => :string, :require => true
+  # The regular expression to match.
+  config :pattern, :validate => :string, :required => true
 
   # If the pattern matched, does event belong to the next or previous event?
-  config :what, :validate => ["previous", "next"], :require => true
+  config :what, :validate => ["previous", "next"], :required => true
 
   # Negate the regexp pattern ('if not matched')
   config :negate, :validate => :boolean, :default => false
 
+  # The stream identity is how the multiline filter determines which stream an
+  # event belongs to. This is generally used for differentiating, say, events
+  # coming from multiple files in the same file input, or multiple connections
+  # coming from a tcp input.
+  #
+  # The default value here is usually what you want, but there are some cases
+  # where you want to change it. One such example is if you are using a tcp
+  # input with only one client connecting at any time. If that client
+  # reconnects (due to error or client restart), then logstash will identify
+  # the new connection as a new stream and break any multiline goodness that
+  # may have occurred between the old and new connection. To solve this use
+  # case, you can use "%{@source_host}.%{@type}" instead.
+  config :stream_identity , :validate => :string, :default => "%{host}.%{path}.%{type}"
+
+  # Logstash ships by default with a bunch of patterns, so you don't
+  # necessarily need to define this yourself unless you are adding additional
+  # patterns.
+  #
+  # Pattern files are plain text with format:
+  #
+  #     NAME PATTERN
+  #
+  # For example:
+  #
+  #     NUMBER \d+
+  config :patterns_dir, :validate => :array, :default => []
+
+  # The maximum age an event can be (in seconds) before it is automatically
+  # flushed.
+  config :max_age, :validate => :number, :default => 5
+
+  # Call the filter flush method at regular interval.
+  # Optional.
+  config :periodic_flush, :validate => :boolean, :default => true
+
+
+  # Detect if we are running from a jarfile, pick the right path.
+  @@patterns_path = Set.new
+  @@patterns_path += [LogStash::Environment.pattern_path("*")]
+
+  MULTILINE_TAG = "multiline"
+
   public
   def initialize(config = {})
     super
 
-    # This filter needs to keep state.
-    @types = Hash.new { |h,k| h[k] = [] }
+    # this filter cannot be parallelized because message order
+    # cannot be garanteed across threads, line #2 could be processed
+    # before line #1
+    @threadsafe = false
+
+    # this filter needs to keep state
     @pending = Hash.new
   end # def initialize
 
   public
   def register
-    @logger.debug "Setting type #{@type.inspect} to the config #{@config.inspect}"
+    require "grok-pure" # rubygem 'jls-grok'
 
-    begin
-      @pattern = Regexp.new(@pattern)
-    rescue RegexpError => e
-      @logger.fatal(["Invalid pattern for multiline filter on type '#{@type}'",
-                    @pattern, e])
-    end
-  end # def register
+    @grok = Grok.new
 
-  public
-  def filter(event)
-    return unless event.type == @type
+    @patterns_dir = @@patterns_path.to_a + @patterns_dir
+    @patterns_dir.each do |path|
+      if File.directory?(path)
+        path = File.join(path, "*")
+      end
 
-    match = @pattern.match(event.message)
-    key = [event.source, event.type]
-    pending = @pending[key]
-
-    @logger.debug(["Reg: ", @pattern, event.message, { :match => match, :negate => @negate }])
+      Dir.glob(path).each do |file|
+        @logger.info("Grok loading patterns from file", :path => file)
+        @grok.add_patterns_from_file(file)
+      end
+    end
 
-    # Add negate option
-    match = (match and !@negate) || (!match and @negate)
+    @grok.compile(@pattern)
 
     case @what
     when "previous"
-      if match
-        event.tags |= ["multiline"]
-        # previous previous line is part of this event.
-        # append it to the event and cancel it
-        if pending
-          pending.append(event)
-        else
-          @pending[key] = event
-        end
-        event.cancel
-      else
-        # this line is not part of the previous event
-        # if we have a pending event, it's done, send it.
-        # put the current event into pending
-        if pending
-          tmp = event.to_hash
-          event.overwrite(pending)
-          @pending[key] = LogStash::Event.new(tmp)
-        else
-          @pending[key] = event
-          event.cancel
-        end # if/else pending
-      end # if/else match
+      class << self; alias_method :multiline_filter!, :previous_filter!; end
     when "next"
-      if match
-        event.tags |= ["multiline"]
-        # this line is part of a multiline event, the next
-        # line will be part, too, put it into pending.
-        if pending
-          pending.append(event)
-        else
-          @pending[key] = event
-        end
-        event.cancel
-      else
-        # if we have something in pending, join it with this message
-        # and send it. otherwise, this is a new message and not part of
-        # multiline, send it.
-        if pending
-          pending.append(event)
-          event.overwrite(pending.to_hash)
-          @pending.delete(key)
-        end
-      end # if/else match
+      class << self; alias_method :multiline_filter!, :next_filter!; end
     else
-      @logger.warn(["Unknown multiline 'what' value.", { :what => @what }])
+      # we should never get here since @what is validated at config
+      raise(ArgumentError, "Unknown multiline 'what' value")
     end # case @what
 
-    if !event.cancelled?
-      filter_matched(event)
+    @logger.debug("Registered multiline plugin", :type => @type, :config => @config)
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+
+    match = event["message"].is_a?(Array) ? @grok.match(event["message"].first) : @grok.match(event["message"])
+    match = (match and !@negate) || (!match and @negate) # add negate option
+
+    @logger.debug? && @logger.debug("Multiline", :pattern => @pattern, :message => event["message"], :match => match, :negate => @negate)
+
+    multiline_filter!(event, match)
+
+    unless event.cancelled?
+      collapse_event!(event)
+      filter_matched(event) if match
     end
   end # def filter
 
   # flush any pending messages
+  # called at regular interval without options and at pipeline shutdown with the :final => true option
+  # @param options [Hash]
+  # @option options [Boolean] :final => true to signal a final shutdown flush
+  # @return [Array<LogStash::Event>] list of flushed events
   public
-  def flush(source, type)
-    key = [source, type]
-    if @pending[key]
-      event = @pending[key]
-      @pending.delete(key)
+  def flush(options = {})
+    expired = nil
+
+    # note that thread safety concerns are not necessary here because the multiline filter
+    # is not thread safe thus cannot be run in multiple folterworker threads and flushing
+    # is called by the same thread
+
+    # select all expired events from the @pending hash into a new expired hash
+    # if :final flush then select all events
+    expired = @pending.inject({}) do |r, (key, event)|
+      age = Time.now - Array(event["@timestamp"]).first.time
+      r[key] = event if (age >= @max_age) || options[:final]
+      r
     end
-    return event
+
+    # delete expired items from @pending hash
+    expired.each{|key, event| @pending.delete(key)}
+
+    # return list of uncancelled and collapsed expired events
+    expired.map{|key, event| event.uncancel; collapse_event!(event)}
   end # def flush
-end # class LogStash::Filters::Date
+
+  public
+  def teardown
+    # nothing to do
+  end
+
+  private
+
+  def previous_filter!(event, match)
+    key = event.sprintf(@stream_identity)
+
+    pending = @pending[key]
+
+    if match
+      event.tag(MULTILINE_TAG)
+      # previous previous line is part of this event.
+      # append it to the event and cancel it
+      if pending
+        pending.append(event)
+      else
+        @pending[key] = event
+      end
+      event.cancel
+    else
+      # this line is not part of the previous event
+      # if we have a pending event, it's done, send it.
+      # put the current event into pending
+      if pending
+        tmp = event.to_hash
+        event.overwrite(pending)
+        @pending[key] = LogStash::Event.new(tmp)
+      else
+        @pending[key] = event
+        event.cancel
+      end
+    end # if match
+  end
+
+  def next_filter!(event, match)
+    key = event.sprintf(@stream_identity)
+
+    # protect @pending for race condition between the flush thread and the worker thread
+    pending = @pending[key]
+
+    if match
+      event.tag(MULTILINE_TAG)
+      # this line is part of a multiline event, the next
+      # line will be part, too, put it into pending.
+      if pending
+        pending.append(event)
+      else
+        @pending[key] = event
+      end
+      event.cancel
+    else
+      # if we have something in pending, join it with this message
+      # and send it. otherwise, this is a new message and not part of
+      # multiline, send it.
+      if pending
+        pending.append(event)
+        event.overwrite(pending)
+        @pending.delete(key)
+      end
+    end # if match
+  end
+
+  def collapse_event!(event)
+    event["message"] = event["message"].join("\n") if event["message"].is_a?(Array)
+    event.timestamp = event.timestamp.first if event.timestamp.is_a?(Array)
+    event
+  end
+end # class LogStash::Filters::Multiline
diff --git a/lib/logstash/filters/mutate.rb b/lib/logstash/filters/mutate.rb
new file mode 100644
index 00000000000..ffee7e09c1c
--- /dev/null
+++ b/lib/logstash/filters/mutate.rb
@@ -0,0 +1,413 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# The mutate filter allows you to perform general mutations on fields. You
+# can rename, remove, replace, and modify fields in your events.
+#
+# TODO(sissel): Support regexp replacements like String#gsub ?
+class LogStash::Filters::Mutate < LogStash::Filters::Base
+  config_name "mutate"
+  milestone 3
+
+  # Rename one or more fields.
+  #
+  # Example:
+  #
+  #     filter {
+  #       mutate {
+  #         # Renames the 'HOSTORIP' field to 'client_ip'
+  #         rename => { "HOSTORIP" => "client_ip" }
+  #       }
+  #     }
+  config :rename, :validate => :hash
+
+  # Remove one or more fields.
+  #
+  # Example:
+  #
+  #     filter {
+  #       mutate {
+  #         remove => [ "client" ]  # Removes the 'client' field
+  #       }
+  #     }
+  #
+  # This option is deprecated, instead use remove_field option available in all
+  # filters.
+  config :remove, :validate => :array, :deprecated => true
+
+  # Replace a field with a new value. The new value can include %{foo} strings
+  # to help you build a new value from other parts of the event.
+  #
+  # Example:
+  #
+  #     filter {
+  #       mutate {
+  #         replace => { "message" => "%{source_host}: My new message" }
+  #       }
+  #     }
+  config :replace, :validate => :hash
+
+  # Update an existing field with a new value. If the field does not exist,
+  # then no action will be taken.
+  #
+  # Example:
+  #
+  #     filter {
+  #       mutate {
+  #         update => { "sample" => "My new message" }
+  #       }
+  #     }
+  config :update, :validate => :hash
+
+  # Convert a field's value to a different type, like turning a string to an
+  # integer. If the field value is an array, all members will be converted.
+  # If the field is a hash, no action will be taken.
+  #
+  # Valid conversion targets are: integer, float, string.
+  #
+  # Example:
+  #
+  #     filter {
+  #       mutate {
+  #         convert => { "fieldname" => "integer" }
+  #       }
+  #     }
+  config :convert, :validate => :hash
+
+  # Convert a string field by applying a regular expression and a replacement.
+  # If the field is not a string, no action will be taken.
+  #
+  # This configuration takes an array consisting of 3 elements per
+  # field/substitution.
+  #
+  # Be aware of escaping any backslash in the config file.
+  #
+  # Example:
+  #
+  #     filter {
+  #       mutate {
+  #         gsub => [
+  #           # replace all forward slashes with underscore
+  #           "fieldname", "/", "_",
+  #
+  #           # replace backslashes, question marks, hashes, and minuses with
+  #           # dot
+  #           "fieldname2", "[\\?#-]", "."
+  #         ]
+  #       }
+  #     }
+  #
+  config :gsub, :validate => :array
+
+  # Convert a string to its uppercase equivalent.
+  #
+  # Example:
+  #
+  #     filter {
+  #       mutate {
+  #         uppercase => [ "fieldname" ]
+  #       }
+  #     }
+  config :uppercase, :validate => :array
+
+  # Convert a string to its lowercase equivalent.
+  #
+  # Example:
+  #
+  #     filter {
+  #       mutate {
+  #         lowercase => [ "fieldname" ]
+  #       }
+  #     }
+  config :lowercase, :validate => :array
+
+  # Split a field to an array using a separator character. Only works on string
+  # fields.
+  #
+  # Example:
+  #
+  #     filter {
+  #       mutate {
+  #          split => { "fieldname" => "," }
+  #       }
+  #     }
+  config :split, :validate => :hash
+
+  # Join an array with a separator character. Does nothing on non-array fields.
+  #
+  # Example:
+  #
+  #    filter {
+  #      mutate {
+  #        join => { "fieldname" => "," }
+  #      }
+  #    }
+  config :join, :validate => :hash
+
+  # Strip whitespace from field. NOTE: this only works on leading and trailing whitespace.
+  #
+  # Example:
+  #
+  #     filter {
+  #       mutate {
+  #          strip => ["field1", "field2"]
+  #       }
+  #     }
+  config :strip, :validate => :array
+
+  # Merge two fields of arrays or hashes.
+  # String fields will be automatically be converted into an array, so:
+  #   array + string will work
+  #   string + string will result in an 2 entry array in dest_field
+  #   array and hash will not work
+  #
+  # Example:
+  #
+  #     filter {
+  #       mutate {
+  #          merge => { "dest_field" => "added_field" }
+  #       }
+  #     }
+  config :merge, :validate => :hash
+
+  public
+  def register
+    valid_conversions = %w(string integer float)
+    # TODO(sissel): Validate conversion requests if provided.
+    @convert.nil? or @convert.each do |field, type|
+      if !valid_conversions.include?(type)
+        raise LogStash::ConfigurationError, I18n.t("logstash.agent.configuration.invalid_plugin_register",
+          :plugin => "filter", :type => "mutate",
+          :error => "Invalid conversion type '#{type}', expected one of '#{valid_conversions.join(',')}'")
+      end
+    end # @convert.each
+
+    @gsub_parsed = []
+    @gsub.nil? or @gsub.each_slice(3) do |field, needle, replacement|
+      if [field, needle, replacement].any? {|n| n.nil?}
+        raise LogStash::ConfigurationError, I18n.t("logstash.agent.configuration.invalid_plugin_register",
+          :plugin => "filter", :type => "mutate",
+          :error => "Invalid gsub configuration #{[field, needle, replacement]}. gsub requires 3 non-nil elements per config entry")
+      end
+
+      @gsub_parsed << {
+        :field        => field,
+        :needle       => (needle.index("%{").nil?? Regexp.new(needle): needle),
+        :replacement  => replacement
+      }
+    end
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+
+    rename(event) if @rename
+    update(event) if @update
+    replace(event) if @replace
+    convert(event) if @convert
+    gsub(event) if @gsub
+    uppercase(event) if @uppercase
+    lowercase(event) if @lowercase
+    strip(event) if @strip
+    remove(event) if @remove
+    split(event) if @split
+    join(event) if @join
+    merge(event) if @merge
+
+    filter_matched(event)
+  end # def filter
+
+  private
+  def remove(event)
+    # TODO(sissel): use event.sprintf on the field names?
+    @remove.each do |field|
+      event.remove(field)
+    end
+  end # def remove
+
+  private
+  def rename(event)
+    # TODO(sissel): use event.sprintf on the field names?
+    @rename.each do |old, new|
+      next unless event.include?(old)
+      event[new] = event.remove(old)
+    end
+  end # def rename
+
+  private
+  def update(event)
+    @update.each do |field, newvalue|
+      next unless event.include?(field)
+      event[field] = event.sprintf(newvalue)
+    end
+  end # def update
+
+  private
+  def replace(event)
+    @replace.each do |field, newvalue|
+      event[field] = event.sprintf(newvalue)
+    end
+  end # def replace
+
+  def convert(event)
+    @convert.each do |field, type|
+      next unless event.include?(field)
+      original = event[field]
+
+      # calls convert_{string,integer,float} depending on type requested.
+      converter = method("convert_" + type)
+      if original.nil?
+        next
+      elsif original.is_a?(Hash)
+        @logger.debug("I don't know how to type convert a hash, skipping",
+                      :field => field, :value => original)
+        next
+      elsif original.is_a?(Array)
+        value = original.map { |v| converter.call(v) }
+      else
+        value = converter.call(original)
+      end
+      event[field] = value
+    end
+  end # def convert
+
+  def convert_string(value)
+    # since this is a filter and all inputs should be already UTF-8
+    # we wont check valid_encoding? but just force UTF-8 for
+    # the Fixnum#to_s case which always result in US-ASCII
+    # see https://twitter.com/jordansissel/status/444613207143903232
+    return value.to_s.force_encoding(Encoding::UTF_8)
+  end # def convert_string
+
+  def convert_integer(value)
+    return value.to_i
+  end # def convert_integer
+
+  def convert_float(value)
+    return value.to_f
+  end # def convert_float
+
+  private
+  def gsub(event)
+    @gsub_parsed.each do |config|
+      field = config[:field]
+      needle = config[:needle]
+      replacement = config[:replacement]
+
+      if event[field].is_a?(Array)
+        event[field] = event[field].map do |v|
+          if not v.is_a?(String)
+            @logger.warn("gsub mutation is only applicable for Strings, " +
+                          "skipping", :field => field, :value => v)
+            v
+          else
+            gsub_dynamic_fields(event, v, needle, replacement)
+          end
+        end
+      else
+        if not event[field].is_a?(String)
+          @logger.debug("gsub mutation is only applicable for Strings, " +
+                        "skipping", :field => field, :value => event[field])
+          next
+        end
+        event[field] = gsub_dynamic_fields(event, event[field], needle, replacement)
+      end
+    end # @gsub_parsed.each
+  end # def gsub
+
+  private
+  def gsub_dynamic_fields(event, original, needle, replacement)
+    if needle.is_a? Regexp
+      original.gsub(needle, event.sprintf(replacement))
+    else
+      # we need to replace any dynamic fields
+      original.gsub(Regexp.new(event.sprintf(needle)), event.sprintf(replacement))
+    end
+  end
+
+  private
+  def uppercase(event)
+    @uppercase.each do |field|
+      if event[field].is_a?(Array)
+        event[field].each { |v| v.upcase! }
+      elsif event[field].is_a?(String)
+        event[field].upcase!
+      else
+        @logger.debug("Can't uppercase something that isn't a string",
+                      :field => field, :value => event[field])
+      end
+    end
+  end # def uppercase
+
+  private
+  def lowercase(event)
+    @lowercase.each do |field|
+      if event[field].is_a?(Array)
+        event[field].each { |v| v.downcase! }
+      elsif event[field].is_a?(String)
+        event[field].downcase!
+      else
+        @logger.debug("Can't lowercase something that isn't a string",
+                      :field => field, :value => event[field])
+      end
+    end
+  end # def lowercase
+
+  private
+  def split(event)
+    @split.each do |field, separator|
+      if event[field].is_a?(String)
+        event[field] = event[field].split(separator)
+      else
+        @logger.debug("Can't split something that isn't a string",
+                      :field => field, :value => event[field])
+      end
+    end
+  end
+
+  private
+  def join(event)
+    @join.each do |field, separator|
+      if event[field].is_a?(Array)
+        event[field] = event[field].join(separator)
+      end
+    end
+  end
+
+  private
+  def strip(event)
+    @strip.each do |field|
+      if event[field].is_a?(Array)
+        event[field] = event[field].map{|s| s.strip }
+      elsif event[field].is_a?(String)
+        event[field] = event[field].strip
+      end
+    end
+  end
+
+  private
+  def merge(event)
+    @merge.each do |dest_field, added_fields|
+      #When multiple calls, added_field is an array
+      added_fields = [ added_fields ] if ! added_fields.is_a?(Array)
+      added_fields.each do |added_field|
+        if event[dest_field].is_a?(Hash) ^ event[added_field].is_a?(Hash)
+          @logger.error("Not possible to merge an array and a hash: ",
+                        :dest_field => dest_field,
+                        :added_field => added_field )
+          next
+        end
+        if event[dest_field].is_a?(Hash) #No need to test the other
+          event[dest_field].update(event[added_field])
+        else
+          event[dest_field] = [event[dest_field]] if ! event[dest_field].is_a?(Array)
+          event[added_field] = [event[added_field]] if ! event[added_field].is_a?(Array)
+         event[dest_field].concat(event[added_field])
+        end
+      end
+    end
+  end
+
+end # class LogStash::Filters::Mutate
diff --git a/lib/logstash/filters/noop.rb b/lib/logstash/filters/noop.rb
new file mode 100644
index 00000000000..a26137b36ae
--- /dev/null
+++ b/lib/logstash/filters/noop.rb
@@ -0,0 +1,21 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# No-op filter. This is used generally for internal/dev testing.
+class LogStash::Filters::NOOP < LogStash::Filters::Base
+  config_name "noop"
+  milestone 2
+
+  public
+  def register
+    # Nothing
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+    # Nothing to do
+    filter_matched(event)
+  end # def filter
+end # class LogStash::Filters::NOOP
diff --git a/lib/logstash/filters/ruby.rb b/lib/logstash/filters/ruby.rb
new file mode 100644
index 00000000000..01f8f60b50c
--- /dev/null
+++ b/lib/logstash/filters/ruby.rb
@@ -0,0 +1,42 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# Execute ruby code.
+#
+# For example, to cancel 90% of events, you can do this:
+#
+#     filter {
+#       ruby {
+#         # Cancel 90% of events
+#         code => "event.cancel if rand <= 0.90"
+#       } 
+#     } 
+#
+class LogStash::Filters::Ruby < LogStash::Filters::Base
+  config_name "ruby"
+  milestone 1
+
+  # Any code to execute at logstash startup-time
+  config :init, :validate => :string
+
+  # The code to execute for every event.
+  # You will have an 'event' variable available that is the event itself.
+  config :code, :validate => :string, :required => true
+
+  public
+  def register
+    # TODO(sissel): Compile the ruby code
+    eval(@init, binding, "(ruby filter init)") if @init
+    eval("@codeblock = lambda { |event| #{@code} }", binding, "(ruby filter code)")
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+
+    @codeblock.call(event)
+
+    filter_matched(event)
+  end # def filter
+end # class LogStash::Filters::Ruby
diff --git a/lib/logstash/filters/sleep.rb b/lib/logstash/filters/sleep.rb
new file mode 100644
index 00000000000..3309f86afdb
--- /dev/null
+++ b/lib/logstash/filters/sleep.rb
@@ -0,0 +1,111 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# Sleep a given amount of time. This will cause logstash
+# to stall for the given amount of time. This is useful
+# for rate limiting, etc.
+#
+class LogStash::Filters::Sleep < LogStash::Filters::Base
+  config_name "sleep"
+  milestone 1
+
+  # The length of time to sleep, in seconds, for every event.
+  #
+  # This can be a number (eg, 0.5), or a string (eg, "%{foo}")
+  # The second form (string with a field value) is useful if
+  # you have an attribute of your event that you want to use
+  # to indicate the amount of time to sleep.
+  #
+  # Example:
+  #
+  #     filter {
+  #       sleep {
+  #         # Sleep 1 second for every event.
+  #         time => "1"
+  #       }
+  #     }
+  config :time, :validate => :string
+
+  # Sleep on every N'th. This option is ignored in replay mode.
+  #
+  # Example:
+  #
+  #     filter {
+  #       sleep {
+  #         time => "1"   # Sleep 1 second
+  #         every => 10   # on every 10th event
+  #       }
+  #     }
+  config :every, :validate => :string, :default => 1
+
+  # Enable replay mode.
+  #
+  # Replay mode tries to sleep based on timestamps in each event.
+  #
+  # The amount of time to sleep is computed by subtracting the
+  # previous event's timestamp from the current event's timestamp.
+  # This helps you replay events in the same timeline as original.
+  #
+  # If you specify a `time` setting as well, this filter will
+  # use the `time` value as a speed modifier. For example,
+  # a `time` value of 2 will replay at double speed, while a
+  # value of 0.25 will replay at 1/4th speed.
+  #
+  # For example:
+  #
+  #     filter {
+  #       sleep {
+  #         time => 2
+  #         replay => true
+  #       }
+  #     }
+  #
+  # The above will sleep in such a way that it will perform
+  # replay 2-times faster than the original time speed.
+  config :replay, :validate => :boolean, :default => false
+
+  public
+  def register
+    if @replay && @time.nil?
+      # Default time multiplier is 1 when replay is set.
+      @time = 1
+    end
+    if @time.nil?
+      raise ArgumentError, "Missing required parameter 'time' for input/eventlog"
+    end
+    @count = 0
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+    @count += 1
+
+    case @time
+      when Fixnum, Float; time = @time
+      when nil; # nothing
+      else; time = event.sprintf(@time).to_f
+    end
+
+    if @replay
+      clock = event.timestamp.to_f
+      if @last_clock
+        delay = clock - @last_clock
+        time = delay/time
+        if time > 0
+          @logger.debug? && @logger.debug("Sleeping", :delay => time)
+          sleep(time)
+        end
+      end
+      @last_clock = clock
+    else
+      if @count >= @every
+        @count = 0
+        @logger.debug? && @logger.debug("Sleeping", :delay => time)
+        sleep(time)
+      end
+    end
+    filter_matched(event)
+  end # def filter
+end # class LogStash::Filters::Sleep
diff --git a/lib/logstash/filters/split.rb b/lib/logstash/filters/split.rb
new file mode 100644
index 00000000000..77ce1651255
--- /dev/null
+++ b/lib/logstash/filters/split.rb
@@ -0,0 +1,62 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# The split filter is for splitting multiline messages into separate events.
+#
+# An example use case of this filter is for taking output from the 'exec' input
+# which emits one event for the whole output of a command and splitting that
+# output by newline - making each line an event.
+#
+# The end result of each split is a complete copy of the event
+# with only the current split section of the given field changed.
+class LogStash::Filters::Split < LogStash::Filters::Base
+
+  config_name "split"
+  milestone 2
+
+  # The string to split on. This is usually a line terminator, but can be any
+  # string.
+  config :terminator, :validate => :string, :default => "\n"
+
+  # The field which value is split by the terminator
+  config :field, :validate => :string, :default => "message"
+
+  public
+  def register
+    # Nothing to do
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+
+    original_value = event[@field]
+
+    # If for some reason the field is an array of values, take the first only.
+    original_value = original_value.first if original_value.is_a?(Array)
+
+    # Using -1 for 'limit' on String#split makes ruby not drop trailing empty
+    # splits.
+    splits = original_value.split(@terminator, -1)
+
+    # Skip filtering if splitting this event resulted in only one thing found.
+    return if splits.length == 1
+    #or splits[1].empty?
+
+    splits.each do |value|
+      next if value.empty?
+
+      event_split = event.clone
+      @logger.debug("Split event", :value => value, :field => @field)
+      event_split[@field] = value
+      filter_matched(event_split)
+
+      # Push this new event onto the stack at the LogStash::FilterWorker
+      yield event_split
+    end
+
+    # Cancel this event, we'll use the newly generated ones above.
+    event.cancel
+  end # def filter
+end # class LogStash::Filters::Split
diff --git a/lib/logstash/filters/spool.rb b/lib/logstash/filters/spool.rb
new file mode 100644
index 00000000000..e9c399aadc9
--- /dev/null
+++ b/lib/logstash/filters/spool.rb
@@ -0,0 +1,32 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+require "thread"
+
+# spool filter. this is used generally for internal/dev testing.
+class LogStash::Filters::Spool < LogStash::Filters::Base
+  config_name "spool"
+  milestone 1
+
+  def register
+    @spool = []
+    @spool_lock = Mutex.new # to synchronize between the flush & worker threads
+  end # def register
+
+  def filter(event)
+    return unless filter?(event)
+
+    filter_matched(event)
+    event.cancel
+    @spool_lock.synchronize {@spool << event}
+  end # def filter
+
+  def flush(options = {})
+    @spool_lock.synchronize do
+      flushed = @spool.map{|event| event.uncancel; event}
+      @spool = []
+      flushed
+    end
+  end
+
+end # class LogStash::Filters::NOOP
diff --git a/lib/logstash/filters/syslog_pri.rb b/lib/logstash/filters/syslog_pri.rb
new file mode 100644
index 00000000000..6b92c719e83
--- /dev/null
+++ b/lib/logstash/filters/syslog_pri.rb
@@ -0,0 +1,107 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# Filter plugin for logstash to parse the PRI field from the front
+# of a Syslog (RFC3164) message.  If no priority is set, it will
+# default to 13 (per RFC).
+#
+# This filter is based on the original syslog.rb code shipped
+# with logstash.
+class LogStash::Filters::Syslog_pri < LogStash::Filters::Base
+  config_name "syslog_pri"
+
+  # set the status to experimental/beta/stable
+  milestone 1
+
+  # Add human-readable names after parsing severity and facility from PRI
+  config :use_labels, :validate => :boolean, :default => true
+
+  # Name of field which passes in the extracted PRI part of the syslog message
+  config :syslog_pri_field_name, :validate => :string, :default => "syslog_pri"
+
+  # Labels for facility levels. This comes from RFC3164.
+  config :facility_labels, :validate => :array, :default => [
+    "kernel",
+    "user-level",
+    "mail",
+    "daemon",
+    "security/authorization",
+    "syslogd",
+    "line printer",
+    "network news",
+    "uucp",
+    "clock",
+    "security/authorization",
+    "ftp",
+    "ntp",
+    "log audit",
+    "log alert",
+    "clock",
+    "local0",
+    "local1",
+    "local2",
+    "local3",
+    "local4",
+    "local5",
+    "local6",
+    "local7",
+  ]
+
+  # Labels for severity levels. This comes from RFC3164.
+  config :severity_labels, :validate => :array, :default => [
+    "emergency",
+    "alert",
+    "critical",
+    "error",
+    "warning",
+    "notice",
+    "informational",
+    "debug",
+  ]
+
+  public
+  def register
+    # Nothing
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+    parse_pri(event)
+    filter_matched(event)
+  end # def filter
+
+  private
+  def parse_pri(event)
+    # Per RFC3164, priority = (facility * 8) + severity
+    # = (facility << 3) & (severity)
+    if event[@syslog_pri_field_name]
+      if event[@syslog_pri_field_name].is_a?(Array)
+        priority = event[@syslog_pri_field_name].first.to_i
+      else
+        priority = event[@syslog_pri_field_name].to_i
+      end
+    else
+      priority = 13  # default
+    end
+    severity = priority & 7 # 7 is 111 (3 bits)
+    facility = priority >> 3
+    event["syslog_severity_code"] = severity
+    event["syslog_facility_code"] = facility
+
+    # Add human-readable names after parsing severity and facility from PRI
+    if @use_labels
+      facility_number = event["syslog_facility_code"]
+      severity_number = event["syslog_severity_code"]
+
+      if @facility_labels[facility_number]
+        event["syslog_facility"] = @facility_labels[facility_number]
+      end
+
+      if @severity_labels[severity_number]
+        event["syslog_severity"] = @severity_labels[severity_number]
+      end
+    end
+  end # def parse_pri
+end # class LogStash::Filters::SyslogPRI
diff --git a/lib/logstash/filters/throttle.rb b/lib/logstash/filters/throttle.rb
new file mode 100644
index 00000000000..56a80c535a4
--- /dev/null
+++ b/lib/logstash/filters/throttle.rb
@@ -0,0 +1,261 @@
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# The throttle filter is for throttling the number of events received. The filter
+# is configured with a lower bound, the before_count, and upper bound, the after_count,
+# and a period of time. All events passing through the filter will be counted based on 
+# a key. As long as the count is less than the before_count or greater than the 
+# after_count, the event will be "throttled" which means the filter will be considered 
+# successful and any tags or fields will be added.
+#
+# For example, if you wanted to throttle events so you only receive an event after 2 
+# occurrences and you get no more than 3 in 10 minutes, you would use the 
+# configuration:
+#     period => 600
+#     before_count => 3
+#     after_count => 5
+#
+# Which would result in:
+#     event 1 - throttled (successful filter, period start)
+#     event 2 - throttled (successful filter)
+#     event 3 - not throttled
+#     event 4 - not throttled
+#     event 5 - not throttled
+#     event 6 - throttled (successful filter)
+#     event 7 - throttled (successful filter)
+#     event x - throttled (successful filter)
+#     period end
+#     event 1 - throttled (successful filter, period start)
+#     event 2 - throttled (successful filter)
+#     event 3 - not throttled
+#     event 4 - not throttled
+#     event 5 - not throttled
+#     event 6 - throttled (successful filter)
+#     ...
+# 
+# Another example is if you wanted to throttle events so you only receive 1 event per 
+# hour, you would use the configuration:
+#     period => 3600
+#     before_count => -1
+#     after_count => 1
+#
+# Which would result in:
+#     event 1 - not throttled (period start)
+#     event 2 - throttled (successful filter)
+#     event 3 - throttled (successful filter)
+#     event 4 - throttled (successful filter)
+#     event x - throttled (successful filter)
+#     period end
+#     event 1 - not throttled (period start)
+#     event 2 - throttled (successful filter)
+#     event 3 - throttled (successful filter)
+#     event 4 - throttled (successful filter)
+#     ...
+# 
+# A common use case would be to use the throttle filter to throttle events before 3 and 
+# after 5 while using multiple fields for the key and then use the drop filter to remove 
+# throttled events. This configuration might appear as:
+# 
+#     filter {
+#       throttle {
+#         before_count => 3
+#         after_count => 5
+#         period => 3600
+#         key => "%{host}%{message}"
+#         add_tag => "throttled"
+#       }
+#       if "throttled" in [tags] {
+#         drop { }
+#       }
+#     }
+#
+# Another case would be to store all events, but only email non-throttled 
+# events so the op's inbox isn't flooded with emails in the event of a system error. 
+# This configuration might appear as:
+#
+#     filter {
+#       throttle {
+#         before_count => 3
+#         after_count => 5
+#         period => 3600
+#         key => "%{message}"
+#         add_tag => "throttled"
+#       }
+#     }
+#     output {
+#       if "throttled" not in [tags] {
+#         email {
+#    	    from => "logstash@mycompany.com"
+#    	    subject => "Production System Alert"
+#    	    to => "ops@mycompany.com"
+#    	    via => "sendmail"
+#    	    body => "Alert on %{host} from path %{path}:\n\n%{message}"
+#    	    options => { "location" => "/usr/sbin/sendmail" }
+#         }
+#       }
+#       elasticsearch_http {
+#         host => "localhost"
+#         port => "19200"
+#       }
+#     }
+#
+# The event counts are cleared after the configured period elapses since the 
+# first instance of the event. That is, all the counts don't reset at the same 
+# time but rather the throttle period is per unique key value.
+#
+# Mike Pilone (@mikepilone)
+# 
+class LogStash::Filters::Throttle < LogStash::Filters::Base
+
+  # The name to use in configuration files.
+  config_name "throttle"
+
+  # New plugins should start life at milestone 1.
+  milestone 1
+
+  # The key used to identify events. Events with the same key will be throttled
+  # as a group.  Field substitutions are allowed, so you can combine multiple
+  # fields.
+  config :key, :validate => :string, :required => true
+  
+  # Events less than this count will be throttled. Setting this value to -1, the 
+  # default, will cause no messages to be throttled based on the lower bound.
+  config :before_count, :validate => :number, :default => -1, :required => false
+  
+  # Events greater than this count will be throttled. Setting this value to -1, the 
+  # default, will cause no messages to be throttled based on the upper bound.
+  config :after_count, :validate => :number, :default => -1, :required => false
+  
+  # The period in seconds after the first occurrence of an event until the count is 
+  # reset for the event. This period is tracked per unique key value.  Field
+  # substitutions are allowed in this value.  They will be evaluated when the _first_
+  # event for a given key is seen.  This allows you to specify that certain kinds
+  # of events throttle for a specific period.
+  config :period, :validate => :string, :default => "3600", :required => false
+  
+  # The maximum number of counters to store before the oldest counter is purged. Setting 
+  # this value to -1 will prevent an upper bound no constraint on the number of counters  
+  # and they will only be purged after expiration. This configuration value should only 
+  # be used as a memory control mechanism and can cause early counter expiration if the 
+  # value is reached. It is recommended to leave the default value and ensure that your 
+  # key is selected such that it limits the number of counters required (i.e. don't 
+  # use UUID as the key!)
+  config :max_counters, :validate => :number, :default => 100000, :required => false
+
+  # Performs initialization of the filter.
+  public
+  def register
+    @threadsafe = false
+  
+    @event_counters = Hash.new
+    @next_expiration = nil
+  end # def register
+
+  # Filters the event. The filter is successful if the event should be throttled.
+  public
+  def filter(event)
+      	  
+    # Return nothing unless there's an actual filter event
+    return unless filter?(event)
+    	  
+    now = Time.now
+    key = event.sprintf(@key)
+    
+    # Purge counters if too large to prevent OOM.
+    if @max_counters != -1 && @event_counters.size > @max_counters then
+      purgeOldestEventCounter()
+    end
+    
+    # Expire existing counter if needed
+    if @next_expiration.nil? || now >= @next_expiration then
+    	expireEventCounters(now)
+    end
+    
+    @logger.debug? and @logger.debug(
+      	  "filters/#{self.class.name}: next expiration", 
+      	  { "next_expiration" => @next_expiration })
+    
+    # Create new counter for this event if this is the first occurrence
+    counter = nil
+    if !@event_counters.include?(key) then
+      period = event.sprintf(@period).to_i
+      period = 3600 if period == 0
+      expiration = now + period
+      @event_counters[key] = { :count => 0, :expiration => expiration }
+      
+      @logger.debug? and @logger.debug("filters/#{self.class.name}: new event", 
+      	  { :key => key, :expiration => expiration })
+    end
+    
+    # Fetch the counter
+    counter = @event_counters[key]
+    
+    # Count this event
+    counter[:count] = counter[:count] + 1;
+    
+    @logger.debug? and @logger.debug("filters/#{self.class.name}: current count", 
+      	  { :key => key, :count => counter[:count] })
+    
+    # Throttle if count is < before count or > after count
+    if ((@before_count != -1 && counter[:count] < @before_count) || 
+       (@after_count != -1 && counter[:count] > @after_count)) then
+      @logger.debug? and @logger.debug(
+      	  "filters/#{self.class.name}: throttling event", { :key => key })
+      	
+      filter_matched(event)
+    end
+        
+  end # def filter
+  
+  # Expires any counts where the period has elapsed. Sets the next expiration time 
+  # for when this method should be called again.
+  private
+  def expireEventCounters(now) 
+    
+    @next_expiration = nil
+    
+    @event_counters.delete_if do |key, counter|
+      expiration = counter[:expiration]
+      expired = expiration <= now
+    
+      if expired then
+      	@logger.debug? and @logger.debug(
+      	  "filters/#{self.class.name}: deleting expired counter", 
+      	  { :key => key })
+      	  
+      elsif @next_expiration.nil? || (expiration < @next_expiration)
+      	@next_expiration = expiration
+      end
+      
+      expired
+    end
+  
+  end # def expireEventCounters
+  
+  # Purges the oldest event counter. This operation is for memory control only 
+  # and can cause early period expiration and thrashing if invoked.
+  private
+  def purgeOldestEventCounter()
+    
+    # Return unless we have something to purge
+    return unless @event_counters.size > 0
+    
+    oldestCounter = nil
+    oldestKey = nil
+    
+    @event_counters.each do |key, counter|
+      if oldestCounter.nil? || counter[:expiration] < oldestCounter[:expiration] then
+        oldestKey = key;
+        oldestCounter = counter;
+      end
+    end
+    
+    @logger.warn? and @logger.warn(
+      "filters/#{self.class.name}: Purging oldest counter because max_counters " +
+      "exceeded. Use a better key to prevent too many unique event counters.", 
+      { :key => oldestKey, :expiration => oldestCounter[:expiration] })
+      	  
+    @event_counters.delete(oldestKey)
+    
+  end
+end # class LogStash::Filters::Throttle
\ No newline at end of file
diff --git a/lib/logstash/filters/urldecode.rb b/lib/logstash/filters/urldecode.rb
new file mode 100644
index 00000000000..b6d50881ae8
--- /dev/null
+++ b/lib/logstash/filters/urldecode.rb
@@ -0,0 +1,57 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+require "uri"
+
+# The urldecode filter is for decoding fields that are urlencoded.
+class LogStash::Filters::Urldecode < LogStash::Filters::Base
+  config_name "urldecode"
+  milestone 2
+
+  # The field which value is urldecoded
+  config :field, :validate => :string, :default => "message"
+
+  # Urldecode all fields
+  config :all_fields, :validate => :boolean, :default => false
+
+  public
+  def register
+    # Nothing to do
+  end #def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+
+    # If all_fields is true then try to decode them all
+    if @all_fields
+      event.to_hash.each do |name, value|
+        event[name] = urldecode(value)
+      end
+    # Else decode the specified field
+    else
+      event[@field] = urldecode(event[@field])
+    end
+    filter_matched(event)
+  end # def filter
+
+  # Attempt to handle string, array, and hash values for fields.
+  # For all other datatypes, just return, URI.unescape doesn't support them.
+  private
+  def urldecode(value)
+    case value
+    when String
+      return URI.unescape(value)
+    when Array
+      ret_values = []
+      value.each { |v| ret_values << urldecode(v) }
+      return ret_values
+    when Hash
+      ret_values = {}
+      value.each { |k,v| ret_values[k] = urldecode(v) }
+      return ret_values
+    else
+      return value
+    end
+  end
+end # class LogStash::Filters::Urldecode
diff --git a/lib/logstash/filters/useragent.rb b/lib/logstash/filters/useragent.rb
new file mode 100644
index 00000000000..3fbe0e3e25d
--- /dev/null
+++ b/lib/logstash/filters/useragent.rb
@@ -0,0 +1,107 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+require "tempfile"
+
+# Parse user agent strings into structured data based on BrowserScope data
+#
+# UserAgent filter, adds information about user agent like family, operating
+# system, version, and device
+#
+# Logstash releases ship with the regexes.yaml database made available from
+# ua-parser with an Apache 2.0 license. For more details on ua-parser, see
+# <https://github.com/tobie/ua-parser/>.
+class LogStash::Filters::UserAgent < LogStash::Filters::Base
+  config_name "useragent"
+  milestone 3
+
+  # The field containing the user agent string. If this field is an
+  # array, only the first value will be used.
+  config :source, :validate => :string, :required => true
+
+  # The name of the field to assign user agent data into.
+  #
+  # If not specified user agent data will be stored in the root of the event.
+  config :target, :validate => :string
+
+  # regexes.yaml file to use
+  #
+  # If not specified, this will default to the regexes.yaml that ships
+  # with logstash.
+  #
+  # You can find the latest version of this here:
+  # <https://github.com/tobie/ua-parser/blob/master/regexes.yaml>
+  config :regexes, :validate => :string
+
+  # A string to prepend to all of the extracted keys
+  config :prefix, :validate => :string, :default => ''
+
+  public
+  def register
+    require 'user_agent_parser'
+    if @regexes.nil?
+      begin
+        @parser = UserAgentParser::Parser.new()
+      rescue Exception => e
+        begin
+          @parser = UserAgentParser::Parser.new(:patterns_path => "vendor/ua-parser/regexes.yaml")
+        rescue => ex
+          raise "Failed to cache, due to: #{ex}\n"
+        end
+      end
+    else
+      @logger.info("Using user agent regexes", :regexes => @regexes)
+      @parser = UserAgentParser::Parser.new(:patterns_path => @regexes)
+    end
+  end #def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+    ua_data = nil
+
+    useragent = event[@source]
+    useragent = useragent.first if useragent.is_a? Array
+
+    begin
+      ua_data = @parser.parse(useragent)
+    rescue Exception => e
+      @logger.error("Uknown error while parsing user agent data", :exception => e, :field => @source, :event => event)
+    end
+
+    if !ua_data.nil?
+      if @target.nil?
+        # default write to the root of the event
+        target = event
+      else
+        target = event[@target] ||= {}
+      end
+
+      # UserAgentParser outputs as US-ASCII.
+
+      target[@prefix + "name"] = ua_data.name.force_encoding(Encoding::UTF_8)
+
+      #OSX, Andriod and maybe iOS parse correctly, ua-agent parsing for Windows does not provide this level of detail
+      unless ua_data.os.nil?
+        target[@prefix + "os"] = ua_data.os.to_s.force_encoding(Encoding::UTF_8)
+        target[@prefix + "os_name"] = ua_data.os.name.to_s.force_encoding(Encoding::UTF_8)
+        target[@prefix + "os_major"] = ua_data.os.version.major.to_s.force_encoding(Encoding::UTF_8) unless ua_data.os.version.nil?
+        target[@prefix + "os_minor"] = ua_data.os.version.minor.to_s.force_encoding(Encoding::UTF_8) unless ua_data.os.version.nil?
+      end
+
+      target[@prefix + "device"] = ua_data.device.to_s.force_encoding(Encoding::UTF_8) if not ua_data.device.nil?
+
+      if not ua_data.version.nil?
+        ua_version = ua_data.version
+        target[@prefix + "major"] = ua_version.major.force_encoding(Encoding::UTF_8) if ua_version.major
+        target[@prefix + "minor"] = ua_version.minor.force_encoding(Encoding::UTF_8) if ua_version.minor
+        target[@prefix + "patch"] = ua_version.patch.force_encoding(Encoding::UTF_8) if ua_version.patch
+        target[@prefix + "build"] = ua_version.patch_minor.force_encoding(Encoding::UTF_8) if ua_version.patch_minor
+      end
+
+      filter_matched(event)
+    end
+
+  end # def filter
+end # class LogStash::Filters::UserAgent
+
diff --git a/lib/logstash/filters/uuid.rb b/lib/logstash/filters/uuid.rb
new file mode 100644
index 00000000000..0dbcc54653f
--- /dev/null
+++ b/lib/logstash/filters/uuid.rb
@@ -0,0 +1,58 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+require "securerandom"
+
+# The uuid filter allows you to add a UUID field to messages.
+# This is useful to be able to control the _id messages are indexed into Elasticsearch
+# with, so that you can insert duplicate messages (i.e. the same message multiple times
+# without creating duplicates) - for log pipeline reliability
+#
+class LogStash::Filters::Uuid < LogStash::Filters::Base
+  config_name "uuid"
+  milestone 2
+
+  # Add a UUID to a field.
+  #
+  # Example:
+  #
+  #     filter {
+  #       uuid {
+  #         target => "@uuid"
+  #       }
+  #     }
+  config :target, :validate => :string, :required => true
+
+  # If the value in the field currently (if any) should be overridden
+  # by the generated UUID. Defaults to false (i.e. if the field is
+  # present, with ANY value, it won't be overridden)
+  #
+  # Example:
+  #
+  #    filter {
+  #       uuid {
+  #         target    => "@uuid"
+  #         overwrite => true
+  #       }
+  #    }
+  config :overwrite, :validate => :boolean, :default => false
+
+  public
+  def register
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+
+    if overwrite
+      event[target] = SecureRandom.uuid
+    else
+      event[target] ||= SecureRandom.uuid
+    end
+
+    filter_matched(event)
+  end # def filter
+
+end # class LogStash::Filters::Uuid
+
diff --git a/lib/logstash/filters/xml.rb b/lib/logstash/filters/xml.rb
new file mode 100644
index 00000000000..4fcf770e52b
--- /dev/null
+++ b/lib/logstash/filters/xml.rb
@@ -0,0 +1,139 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# XML filter. Takes a field that contains XML and expands it into
+# an actual datastructure.
+class LogStash::Filters::Xml < LogStash::Filters::Base
+
+  config_name "xml"
+  milestone 1
+
+  # Config for xml to hash is:
+  #
+  #     source => source_field
+  #
+  # For example, if you have the whole xml document in your @message field:
+  #
+  #     filter {
+  #       xml {
+  #         source => "message"
+  #       }
+  #     }
+  #
+  # The above would parse the xml from the @message field
+  config :source, :validate => :string
+
+  # Define target for placing the data
+  #
+  # for example if you want the data to be put in the 'doc' field:
+  #
+  #     filter {
+  #       xml {
+  #         target => "doc"
+  #       }
+  #     }
+  #
+  # XML in the value of the source field will be expanded into a
+  # datastructure in the "target" field.
+  # Note: if the "target" field already exists, it will be overridden
+  # Required
+  config :target, :validate => :string
+
+  # xpath will additionally select string values (.to_s on whatever is selected)
+  # from parsed XML (using each source field defined using the method above)
+  # and place those values in the destination fields. Configuration:
+  #
+  # xpath => [ "xpath-syntax", "destination-field" ]
+  #
+  # Values returned by XPath parsring from xpath-synatx will be put in the
+  # destination field. Multiple values returned will be pushed onto the
+  # destination field as an array. As such, multiple matches across
+  # multiple source fields will produce duplicate entries in the field
+  #
+  # More on xpath: http://www.w3schools.com/xpath/
+  #
+  # The xpath functions are particularly powerful:
+  # http://www.w3schools.com/xpath/xpath_functions.asp
+  #
+  config :xpath, :validate => :hash, :default => {}
+
+  # By default the filter will store the whole parsed xml in the destination
+  # field as described above. Setting this to false will prevent that.
+  config :store_xml, :validate => :boolean, :default => true
+
+  public
+  def register
+    require "nokogiri"
+    require "xmlsimple"
+
+  end # def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+    matched = false
+
+    @logger.debug("Running xml filter", :event => event)
+
+    return unless event.include?(@source)
+
+    value = event[@source]
+
+    if value.is_a?(Array) && value.length > 1
+      @logger.warn("XML filter only works on fields of length 1",
+                   :source => @source, :value => value)
+      return
+    end
+
+    # Do nothing with an empty string.
+    return if value.strip.length == 0
+
+    if @xpath
+      begin
+        doc = Nokogiri::XML(value, nil, value.encoding.to_s)
+      rescue => e
+        event.tag("_xmlparsefailure")
+        @logger.warn("Trouble parsing xml", :source => @source, :value => value,
+                     :exception => e, :backtrace => e.backtrace)
+        return
+      end
+
+      @xpath.each do |xpath_src, xpath_dest|
+        nodeset = doc.xpath(xpath_src)
+
+        # If asking xpath for a String, like "name(/*)", we get back a
+        # String instead of a NodeSet.  We normalize that here.
+        normalized_nodeset = nodeset.kind_of?(Nokogiri::XML::NodeSet) ? nodeset : [nodeset]
+
+        normalized_nodeset.each do |value|
+          # some XPath functions return empty arrays as string
+          if value.is_a?(Array)
+            return if value.length == 0
+          end
+
+          unless value.nil?
+            matched = true
+            event[xpath_dest] ||= []
+            event[xpath_dest] << value.to_s
+          end
+        end # XPath.each
+      end # @xpath.each
+    end # if @xpath
+
+    if @store_xml
+      begin
+        event[@target] = XmlSimple.xml_in(value)
+        matched = true
+      rescue => e
+        event.tag("_xmlparsefailure")
+        @logger.warn("Trouble parsing xml with XmlSimple", :source => @source,
+                     :value => value, :exception => e, :backtrace => e.backtrace)
+        return
+      end
+    end # if @store_xml
+
+    filter_matched(event) if matched
+    @logger.debug("Event after xml filter", :event => event)
+  end # def filter
+end # class LogStash::Filters::Xml
diff --git a/lib/logstash/inputs.rb b/lib/logstash/inputs.rb
deleted file mode 100644
index e75ac18d7a7..00000000000
--- a/lib/logstash/inputs.rb
+++ /dev/null
@@ -1,12 +0,0 @@
-require "logstash/namespace"
-require "uri"
-
-module LogStash::Inputs
-  public
-  def self.from_name(type, configs, output_queue)
-    klass = type.capitalize
-    file = type.downcase
-    require "logstash/inputs/#{file}"
-    LogStash::Inputs.const_get(klass).new(configs, output_queue)
-  end # def from_name
-end # module LogStash::Inputs
diff --git a/lib/logstash/inputs/amqp.rb b/lib/logstash/inputs/amqp.rb
deleted file mode 100644
index 7578dc5bf38..00000000000
--- a/lib/logstash/inputs/amqp.rb
+++ /dev/null
@@ -1,93 +0,0 @@
-require "logstash/inputs/base"
-require "logstash/namespace"
-
-# Pull events from an AMQP exchange.
-#
-# 
-# TODO(sissel): Document where to learn more about AMQP and brokers.
-class LogStash::Inputs::Amqp < LogStash::Inputs::Base
-  MQTYPES = [ "fanout", "direct", "topic" ]
-
-  config_name "amqp"
-
-  # Your amqp server address
-  config :host, :validate => :string, :required => true
-
-  # The AMQP port to connect on
-  config :port, :validate => :number, :default => 5672
-
-  # Your amqp username
-  config :user, :validate => :string, :default => "guest"
-
-  # Your amqp password
-  config :password, :validate => :password, :default => "guest"
-
-  # The exchange type (fanout, topic, direct)
-  config :exchange_type, :validate => [ "fanout", "direct", "topic"], :required => true
-
-  # The name of the exchange
-  config :name, :validate => :string, :required => true
-
-  # The vhost to use
-  config :vhost, :validate => :string, :default => "/"
-
-  # Is this exchange durable?
-  config :durable, :validate => :boolean, :default => false
-
-  # Enable or disable debugging
-  config :debug, :validate => :boolean, :default => false
-
-  public
-  def initialize(params)
-    super
-
-    if !MQTYPES.include?(@exchange_type)
-      raise "Invalid type '#{@exchange_type}' must be one of #{MQTYPES.join(", ")}"
-    end
-  end # def initialize
-
-  public
-  def register
-    @logger.info("Registering input #{@url}")
-    require "bunny" # rubygem 'bunny'
-    @amqpsettings = {
-      :vhost => (@vhost or "/"),
-      :host => @host,
-      :port => (@port or 5672),
-    }
-    @amqpsettings[:user] = @user if @user
-    @amqpsettings[:pass] = @password.value if @password
-    @amqpsettings[:logging] = @debug
-  end # def register
-
-  def run(queue)
-    loop do
-      @logger.debug("Connecting with AMQP settings #{@amqpsettings.inspect} to set up #{@mqtype.inspect} queue #{@name.inspect}")
-      @bunny = Bunny.new(@amqpsettings)
-
-      begin
-        @bunny.start
-
-        @queue = @bunny.queue(@name)
-        exchange = @bunny.exchange(@name, :type => @exchange_type.to_sym, :durable => @durable)
-        @queue.bind(exchange)
-
-        @queue.subscribe do |data|
-          begin
-            obj = JSON.parse(data[:payload])
-          rescue => e
-            @logger.error(["json parse error", { :exception => e }])
-            raise e
-          end
-
-          queue << LogStash::Event.new(obj)
-        end # @queue.subscribe
-      rescue *[Bunny::ConnectionError, Bunny::ServerDownError] => e
-        @logger.error("AMQP connection error, will reconnect: #{e}")
-        # Sleep for a bit before retrying.
-        # TODO(sissel): Write 'backoff' method?
-        sleep(1)
-      end # begin/rescue
-    end # loop
-  end # def run
-end # class LogStash::Inputs::Amqp
diff --git a/lib/logstash/inputs/base.rb b/lib/logstash/inputs/base.rb
index 5c41f33ce57..22bda363343 100644
--- a/lib/logstash/inputs/base.rb
+++ b/lib/logstash/inputs/base.rb
@@ -1,41 +1,92 @@
+# encoding: utf-8
 require "logstash/namespace"
 require "logstash/event"
+require "logstash/plugin"
 require "logstash/logging"
 require "logstash/config/mixin"
+require "logstash/codecs/base"
 
-class LogStash::Inputs::Base
+# This is the base class for Logstash inputs.
+class LogStash::Inputs::Base < LogStash::Plugin
   include LogStash::Config::Mixin
-  attr_accessor :logger
-
   config_name "input"
 
-  # Label this input with a type.
-  config :type, :validate => :string, :required => true
+  # Add a 'type' field to all events handled by this input.
+  #
+  # Types are used mainly for filter activation.
+  #
+  # The type is stored as part of the event itself, so you can
+  # also use the type to search for it in the web interface.
+  #
+  # If you try to set a type on an event that already has one (for
+  # example when you send an event from a shipper to an indexer) then
+  # a new input will not override the existing type. A type set at 
+  # the shipper stays with that event for its life even
+  # when sent to another Logstash server.
+  config :type, :validate => :string
+
+  config :debug, :validate => :boolean, :default => false, :deprecated => "This setting no longer has any effect. In past releases, it existed, but almost no plugin made use of it."
+
+  # The format of input data (plain, json, json_event)
+  config :format, :validate => ["plain", "json", "json_event", "msgpack_event"], :deprecated => "You should use the newer 'codec' setting instead."
 
-  # Set this to true to enable debugging on an input.
-  config :debug, :validate => :boolean, :default => false
+  # The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.
+  config :codec, :validate => :codec, :default => "plain"
+
+  # The character encoding used in this input. Examples include "UTF-8"
+  # and "cp1252"
+  #
+  # This setting is useful if your log files are in Latin-1 (aka cp1252)
+  # or in another character set other than UTF-8.
+  #
+  # This only affects "plain" format logs since json is UTF-8 already.
+  config :charset, :validate => ::Encoding.name_list, :deprecated => true
+
+  # If format is "json", an event sprintf string to build what
+  # the display @message should be given (defaults to the raw JSON).
+  # sprintf format strings look like %{fieldname}
+  #
+  # If format is "json_event", ALL fields except for @type
+  # are expected to be present. Not receiving all fields
+  # will cause unexpected results.
+  config :message_format, :validate => :string, :deprecated => true
 
   # Add any number of arbitrary tags to your event.
   #
   # This can help with processing later.
   config :tags, :validate => :array
 
-  #config :tags, :validate => (lambda do |value|
-    #re = /^[A-Za-z0-9_]+$/
-    #value.each do |v|
-      #if v !~ re
-        #return [false, "Tag '#{v}' does not match #{re}"]
-      #end # check 'v'
-    #end # value.each 
-    #return true
-  #end) # config :tag
+  # Add a field to an event
+  config :add_field, :validate => :hash, :default => {}
+
+  attr_accessor :params
+  attr_accessor :threadable
 
   public
-  def initialize(params)
-    @logger = LogStash::Logger.new(STDOUT)
+  def initialize(params={})
+    super
+    @threadable = false
     config_init(params)
-
     @tags ||= []
+
+    if @charset && @codec.class.get_config.include?("charset")
+      # charset is deprecated on inputs, but provide backwards compatibility
+      # by copying the charset setting into the codec.
+
+      @logger.info("Copying input's charset setting into codec", :input => self, :codec => @codec)
+      charset = @charset
+      @codec.instance_eval { @charset = charset }
+    end
+
+    # Backwards compat for the 'format' setting
+    case @format
+      when "plain"; # do nothing
+      when "json"
+        @codec = LogStash::Plugin.lookup("codec", "json").new
+      when "json_event"
+        @codec = LogStash::Plugin.lookup("codec", "oldlogstashjson").new
+    end
+
   end # def initialize
 
   public
@@ -47,4 +98,40 @@ def register
   def tag(newtag)
     @tags << newtag
   end # def tag
+
+  protected
+  def to_event(raw, source) 
+    raise LogStash::ThisMethodWasRemoved("LogStash::Inputs::Base#to_event - you should use codecs now instead of to_event. Not sure what this means? Get help on logstash-users@googlegroups.com!")
+  end # def to_event
+
+  protected
+  def decorate(event)
+    # Only set 'type' if not already set. This is backwards-compatible behavior
+    event["type"] = @type if @type && !event.include?("type")
+
+    if @tags.any?
+      event["tags"] ||= []
+      event["tags"] += @tags
+    end
+
+    @add_field.each do |field, value|
+      event[field] = value
+    end
+  end
+
+  protected
+  def fix_streaming_codecs
+    require "logstash/codecs/plain"
+    require "logstash/codecs/line"
+    require "logstash/codecs/json"
+    require "logstash/codecs/json_lines"
+    case @codec
+      when LogStash::Codecs::Plain
+        @logger.info("Automatically switching from #{@codec.class.config_name} to line codec", :plugin => self.class.config_name)
+        @codec = LogStash::Codecs::Line.new("charset" => @codec.charset)
+      when LogStash::Codecs::JSON
+        @logger.info("Automatically switching from #{@codec.class.config_name} to json_lines codec", :plugin => self.class.config_name)
+        @codec = LogStash::Codecs::JSONLines.new("charset" => @codec.charset)
+    end
+  end
 end # class LogStash::Inputs::Base
diff --git a/lib/logstash/inputs/beanstalk.rb b/lib/logstash/inputs/beanstalk.rb
deleted file mode 100644
index 419a6a051dc..00000000000
--- a/lib/logstash/inputs/beanstalk.rb
+++ /dev/null
@@ -1,45 +0,0 @@
-require "logstash/inputs/base"
-require "logstash/namespace"
-
-# Pull events from a beanstalk tube.
-#
-# TODO(sissel): Document where to learn more about beanstalk.
-class LogStash::Inputs::Beanstalk < LogStash::Inputs::Base
-
-  config_name "beanstalk"
-
-  # The address of the beanstalk server
-  config :host, :validate => :string, :required => true
-
-  # The port of your beanstalk server
-  config :port, :validate => :number, :default => 11300
-
-  # The name of the beanstalk tube
-  config :tube, :validate => :string, :required => true
-
-  public
-  def register
-    require "beanstalk-client"
-    # TODO(petef): support pools of beanstalkd servers
-    # TODO(petef): check for errors
-    @beanstalk = Beanstalk::Pool.new(["#{@host}:#{@port}"])
-    @beanstalk.watch(@tube)
-  end # def register
-
-  public
-  def run(output_queue)
-    loop do
-      job = @beanstalk.reserve
-      begin
-        event = LogStash::Event.from_json(job.body)
-      rescue => e
-        @logger.warn(["Trouble parsing beanstalk job",
-                     {:error => e.message, :body => job.body,
-                      :backtrace => e.backtrace}])
-        job.bury(job, 0)
-      end
-      output_queue << event
-      job.delete
-    end
-  end # def run
-end # class LogStash::Inputs::Beanstalk
diff --git a/lib/logstash/inputs/collectd.rb b/lib/logstash/inputs/collectd.rb
new file mode 100644
index 00000000000..59eb3b4adfd
--- /dev/null
+++ b/lib/logstash/inputs/collectd.rb
@@ -0,0 +1,459 @@
+# encoding utf-8
+require "date"
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "logstash/timestamp"
+require "socket"
+require "tempfile"
+require "time"
+
+# Read events from the connectd binary protocol over the network via udp.
+# See https://collectd.org/wiki/index.php/Binary_protocol
+#
+# Configuration in your Logstash configuration file can be as simple as:
+#     input {
+#       collectd {}
+#     }
+#
+# A sample collectd.conf to send to Logstash might be:
+#
+#     Hostname    "host.example.com"
+#     LoadPlugin interface
+#     LoadPlugin load
+#     LoadPlugin memory
+#     LoadPlugin network
+#     <Plugin interface>
+#         Interface "eth0"
+#         IgnoreSelected false
+#     </Plugin>
+#     <Plugin network>
+#         <Server "10.0.0.1" "25826">
+#         </Server>
+#     </Plugin>
+#
+# Be sure to replace "10.0.0.1" with the IP of your Logstash instance.
+#
+
+#
+class LogStash::Inputs::Collectd < LogStash::Inputs::Base
+  config_name "collectd"
+  milestone 1
+
+  AUTHFILEREGEX = /([^:]+): (.+)/
+  TYPEMAP = {
+      0   => "host",
+      1   => "@timestamp",
+      2   => "plugin",
+      3   => "plugin_instance",
+      4   => "collectd_type",
+      5   => "type_instance",
+      6   => "values",
+      7   => "interval",
+      8   => "@timestamp",
+      9   => "interval",
+      256 => "message",
+      257 => "severity",
+      512 => "signature",
+      528 => "encryption"
+  }
+
+  SECURITY_NONE = "None"
+  SECURITY_SIGN = "Sign"
+  SECURITY_ENCR = "Encrypt"
+
+  # File path(s) to collectd types.db to use.
+  # The last matching pattern wins if you have identical pattern names in multiple files.
+  # If no types.db is provided the included types.db will be used (currently 5.4.0).
+  config :typesdb, :validate => :array
+
+  # The address to listen on.  Defaults to all available addresses.
+  config :host, :validate => :string, :default => "0.0.0.0"
+
+  # The port to listen on.  Defaults to the collectd expected port of 25826.
+  config :port, :validate => :number, :default => 25826
+
+  # Prune interval records.  Defaults to true.
+  config :prune_intervals, :validate => :boolean, :default => true
+
+  # Buffer size. 1452 is the collectd default for v5+
+  config :buffer_size, :validate => :number, :default => 1452
+
+  # Security Level. Default is "None". This setting mirrors the setting from the
+  # collectd [Network plugin](https://collectd.org/wiki/index.php/Plugin:Network)
+  config :security_level, :validate => [SECURITY_NONE, SECURITY_SIGN, SECURITY_ENCR],
+    :default => "None"
+
+  # Path to the authentication file. This file should have the same format as
+  # the [AuthFile](http://collectd.org/documentation/manpages/collectd.conf.5.shtml#authfile_filename)
+  # in collectd. You only need to set this option if the security_level is set to
+  # "Sign" or "Encrypt"
+  config :authfile, :validate => :string
+
+  # What to do when a value in the event is NaN (Not a Number)
+  # - change_value (default): Change the NaN to the value of the nan_value option and add nan_tag as a tag
+  # - warn: Change the NaN to the value of the nan_value option, print a warning to the log and add nan_tag as a tag
+  # - drop: Drop the event containing the NaN (this only drops the single event, not the whole packet)
+  config :nan_handeling, :validate => ['change_value','warn','drop'],
+    :default => 'change_value'
+
+  # Only relevant when nan_handeling is set to 'change_value'
+  # Change NaN to this configured value
+  config :nan_value, :validate => :number, :default => 0
+
+  # The tag to add to the event if a NaN value was found
+  # Set this to an empty string ('') if you don't want to tag
+  config :nan_tag, :validate => :string, :default => '_collectdNaN'
+
+  public
+  def initialize(params)
+    super
+    BasicSocket.do_not_reverse_lookup = true
+    @timestamp = LogStash::Timestamp.now
+    @collectd = {}
+    @types = {}
+  end # def initialize
+
+  public
+  def register
+    @udp = nil
+    if @typesdb.nil?
+      @typesdb = LogStash::Environment.vendor_path("collectd/types.db")
+      if !File.exists?(@typesdb)
+        raise "You must specify 'typesdb => ...' in your collectd input (I looked for '#{@typesdb}')"
+      end
+      @logger.info("Using internal types.db", :typesdb => @typesdb.to_s)
+    end
+
+    if ([SECURITY_SIGN, SECURITY_ENCR].include?(@security_level))
+      if @authfile.nil?
+        raise "Security level is set to #{@security_level}, but no authfile was configured"
+      else
+        # Load OpenSSL and instantiate Digest and Crypto functions
+        require 'openssl'
+        @sha256 = OpenSSL::Digest::Digest.new('sha256')
+        @sha1 = OpenSSL::Digest::Digest.new('sha1')
+        @cipher = OpenSSL::Cipher.new('AES-256-OFB')
+        @auth = {}
+        parse_authfile
+      end
+    end
+  end # def register
+
+  public
+  def run(output_queue)
+    begin
+      # get types
+      get_types(@typesdb)
+      # collectd server
+      collectd_listener(output_queue)
+    rescue LogStash::ShutdownSignal
+      # do nothing, shutdown was requested.
+    rescue => e
+      @logger.warn("Collectd listener died", :exception => e, :backtrace => e.backtrace)
+      sleep(5)
+      retry
+    end # begin
+  end # def run
+
+  public
+  def get_types(paths)
+    # Get the typesdb
+    paths = Array(paths) # Make sure a single path is still forced into an array type
+    paths.each do |path|
+      @logger.info("Getting Collectd typesdb info", :typesdb => path.to_s)
+      File.open(path, 'r').each_line do |line|
+        typename, *line = line.strip.split
+        next if typename.nil? || if typename[0,1] != '#' # Don't process commented or blank lines
+          v = line.collect { |l| l.strip.split(":")[0] }
+          @types[typename] = v
+        end
+      end
+    end
+  @logger.debug("Collectd Types", :types => @types.to_s)
+  end # def get_types
+
+  public
+  def get_values(id, body)
+    retval = ''
+    case id
+      when 0,2,3,4,5,256 #=> String types
+        retval = body.pack("C*")
+        retval = retval[0..-2]
+      when 1 # Time
+        # Time here, in bit-shifted format.  Parse bytes into UTC.
+        byte1, byte2 = body.pack("C*").unpack("NN")
+        retval = Time.at(( ((byte1 << 32) + byte2))).utc
+      when 7,257 #=> Numeric types
+        retval = body.slice!(0..7).pack("C*").unpack("E")[0]
+      when 8 # Time, Hi-Res
+        # Time here, in bit-shifted format.  Parse bytes into UTC.
+        byte1, byte2 = body.pack("C*").unpack("NN")
+        retval = Time.at(( ((byte1 << 32) + byte2) * (2**-30) )).utc
+      when 9 # Interval, Hi-Res
+        byte1, byte2 = body.pack("C*").unpack("NN")
+        retval = (((byte1 << 32) + byte2) * (2**-30)).to_i
+      when 6 # Values
+        val_bytes = body.slice!(0..1)
+        val_count = val_bytes.pack("C*").unpack("n")
+        if body.length % 9 == 0 # Should be 9 fields
+          count = 0
+          retval = []
+          types = body.slice!(0..((body.length/9)-1))
+          while body.length > 0
+            # TYPE VALUES:
+            # 0: COUNTER
+            # 1: GAUGE
+            # 2: DERIVE
+            # 3: ABSOLUTE
+            case types[count]
+              when 1;
+                v = body.slice!(0..7).pack("C*").unpack("E")[0]
+                if v.nan?
+                  case @nan_handeling
+                  when 'drop'; return false
+                  else
+                    v = @nan_value
+                    add_tag(@nan_tag)
+                    @nan_handeling == 'warn' && @logger.warn("NaN in (unfinished event) #{@collectd}")
+                  end
+                end
+              when 0, 3; v = body.slice!(0..7).pack("C*").unpack("Q>")[0]
+              when 2;    v = body.slice!(0..7).pack("C*").unpack("q>")[0]
+              else;      v = 0
+            end
+            retval << v
+            count += 1
+          end
+        else
+          @logger.error("Incorrect number of data fields for collectd record", :body => body.to_s)
+        end
+      when 512 # signature
+        if body.length < 32
+          @logger.warning("SHA256 signature too small (got #{body.length} bytes instead of 32)")
+        elsif body.length < 33
+          @logger.warning("Received signature without username")
+        else
+          retval = []
+          # Byte 32 till the end contains the username as chars (=unsigned ints)
+          retval << body[32..-1].pack('C*')
+          # Byte 0 till 31 contain the signature
+          retval << body[0..31].pack('C*')
+        end
+      when 528 # encryption
+        retval = []
+        user_length = (body.slice!(0) << 8) + body.slice!(0)
+        retval << body.slice!(0..user_length-1).pack('C*') # Username
+        retval << body.slice!(0..15).pack('C*')            # IV
+        retval << body.pack('C*')                          # Encrypted content
+    end
+    return retval
+  end # def get_values
+
+  private
+  def parse_authfile
+    # We keep the authfile parsed in memory so we don't have to open the file
+    # for every event.
+    @logger.debug("Parsing authfile #{@authfile}")
+    if !File.exist?(@authfile)
+      raise "The file #{@authfile} was not found"
+    end
+    @auth.clear
+    @authmtime = File.stat(@authfile).mtime
+    File.readlines(@authfile).each do |line|
+      #line.chomp!
+      k,v = line.scan(AUTHFILEREGEX).flatten
+      if k and v
+        @logger.debug("Added authfile entry '#{k}' with key '#{v}'")
+        @auth[k] = v
+      else
+        @logger.info("Ignoring malformed authfile line '#{line.chomp}'")
+      end
+    end
+  end # def parse_authfile
+
+  private
+  def get_key(user)
+    return if @authmtime.nil? or @authfile.nil?
+    # Validate that our auth data is still up-to-date
+    parse_authfile if @authmtime < File.stat(@authfile).mtime
+    key = @auth[user]
+    @logger.warn("User #{user} is not found in the authfile #{@authfile}") if key.nil?
+    return key
+  end # def get_key
+
+  private
+  def verify_signature(user, signature, payload)
+    # The user doesn't care about the security
+    return true if @security_level == SECURITY_NONE
+
+    # We probably got and array of ints, pack it!
+    payload = payload.pack('C*') if payload.is_a?(Array)
+
+    key = get_key(user)
+    return false if key.nil?
+
+    return true if OpenSSL::HMAC.digest(@sha256, key, user+payload) == signature
+    return false
+  end # def verify_signature
+
+  private
+  def decrypt_packet(user, iv, content)
+    # Content has to have at least a SHA1 hash (20 bytes), a header (4 bytes) and
+    # one byte of data
+    return [] if content.length < 26
+    content = content.pack('C*') if content.is_a?(Array)
+    key = get_key(user)
+    return [] if key.nil?
+
+    # Set the correct state of the cipher instance
+    @cipher.decrypt
+    @cipher.padding = 0
+    @cipher.iv = iv
+    @cipher.key = @sha256.digest(key);
+    # Decrypt the content
+    plaintext = @cipher.update(content) + @cipher.final
+    # Reset the state, as adding a new key to an already instantiated state
+    # results in an exception
+    @cipher.reset
+
+    # The plaintext contains a SHA1 hash as checksum in the first 160 bits
+    # (20 octets) of the rest of the data
+    hash = plaintext.slice!(0..19)
+
+    if @sha1.digest(plaintext) != hash
+      @logger.warn("Unable to decrypt packet, checksum mismatch")
+      return []
+    end
+    return plaintext.unpack('C*')
+  end # def decrypt_packet
+
+  private
+  def generate_event(output_queue)
+    # Prune these *specific* keys if they exist and are empty.
+    # This is better than looping over all keys every time.
+    @collectd.delete('type_instance') if @collectd['type_instance'] == ""
+    @collectd.delete('plugin_instance') if @collectd['plugin_instance'] == ""
+    # As crazy as it sounds, this is where we actually send our events to the queue!
+    event = LogStash::Event.new
+    @collectd.each {|k, v| event[k] = @collectd[k]}
+    decorate(event)
+    output_queue << event
+  end # def generate_event
+
+  private
+  def clean_up()
+    @collectd.each_key do |k|
+      @collectd.delete(k) if !['host','collectd_type', 'plugin', 'plugin_instance', '@timestamp', 'type_instance'].include?(k)
+    end
+  end # def clean_up
+
+  private
+  def add_tag(new_tag)
+    return if new_tag.empty?
+    @collectd['tags'] ||= []
+    @collectd['tags'] << new_tag
+  end
+
+  private
+  def collectd_listener(output_queue)
+    @logger.info("Starting Collectd listener", :address => "#{@host}:#{@port}")
+
+    if @udp && ! @udp.closed?
+      @udp.close
+    end
+
+    @udp = UDPSocket.new(Socket::AF_INET)
+    @udp.bind(@host, @port)
+
+    loop do
+      payload, client = @udp.recvfrom(@buffer_size)
+      payload = payload.bytes.to_a
+
+      # Clear the last event
+      @collectd.clear
+      was_encrypted = false
+
+      while payload.length > 0 do
+        typenum = (payload.slice!(0) << 8) + payload.slice!(0)
+        # Get the length of the data in this part, but take into account that
+        # the header is 4 bytes
+        length  = ((payload.slice!(0) << 8) + payload.slice!(0)) - 4
+
+        if length > payload.length
+          @logger.info("Header indicated #{length} bytes will follow, but packet has only #{payload.length} bytes left")
+          break
+        end
+        body = payload.slice!(0..length-1)
+
+        field = TYPEMAP[typenum]
+        if field.nil?
+          @logger.warn("Unknown typenumber: #{typenum}")
+          next
+        end
+
+        values = get_values(typenum, body)
+
+        case field
+        when "signature"
+          break if !verify_signature(values[0], values[1], payload)
+          next
+        when "encryption"
+          payload = decrypt_packet(values[0], values[1], values[2])
+          # decrypt_packet returns an empty array if the decryption was
+          # unsuccessful and this inner loop checks the length. So we can safely
+          # set the 'was_encrypted' variable.
+          was_encrypted=true
+          next
+        when "plugin"
+          # We've reached a new plugin, delete everything except for the the host
+          # field, because there's only one per packet and the timestamp field,
+          # because that one goes in front of the plugin
+          @collectd.each_key do |k|
+            @collectd.delete(k) if !['host', '@timestamp'].include?(k)
+          end
+        when "collectd_type"
+          # We've reached a new type within the plugin section, delete all fields
+          # that could have something to do with the previous type (if any)
+          @collectd.each_key do |k|
+            @collectd.delete(k) if !['host', '@timestamp', 'plugin', 'plugin_instance'].include?(k)
+          end
+        end
+
+        break if !was_encrypted and @security_level == SECURITY_ENCR
+
+        # Fill in the fields.
+        if values.kind_of?(Array)
+          if values.length > 1              # Only do this iteration on multi-value arrays
+            values.each_with_index {|value, x| @collectd[@types[@collectd['collectd_type']][x]] = values[x]}
+          else                              # Otherwise it's a single value
+            @collectd['value'] = values[0]      # So name it 'value' accordingly
+          end
+        elsif !values
+          clean_up()
+          next
+        elsif field != nil                  # Not an array, make sure it's non-empty
+          @collectd[field] = values            # Append values to @collectd under key field
+        end
+
+        if ["interval", "values"].include?(field)
+          if ((@prune_intervals && ![7,9].include?(typenum)) || !@prune_intervals)
+            generate_event(output_queue)
+          end
+          clean_up()
+        end
+      end # while payload.length > 0 do
+    end # loop do
+
+  ensure
+    if @udp
+      @udp.close_read rescue nil
+      @udp.close_write rescue nil
+    end
+  end # def collectd_listener
+
+  public
+  def teardown
+    @udp.close if @udp && !@udp.closed?
+  end
+
+end # class LogStash::Inputs::Collectd
diff --git a/lib/logstash/inputs/elasticsearch.rb b/lib/logstash/inputs/elasticsearch.rb
new file mode 100644
index 00000000000..d34630eaf95
--- /dev/null
+++ b/lib/logstash/inputs/elasticsearch.rb
@@ -0,0 +1,134 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "logstash/util/socket_peer"
+require "logstash/json"
+
+# Read from an Elasticsearch cluster, based on search query results.
+# This is useful for replaying test logs, reindexing, etc.
+#
+# Example:
+#
+#     input {
+#       # Read all documents from Elasticsearch matching the given query
+#       elasticsearch {
+#         host => "localhost"
+#         query => "ERROR"
+#       }
+#     }
+#
+# This would create an Elasticsearch query with the following format:
+#
+#     http://localhost:9200/logstash-*/_search?q=ERROR&scroll=1m&size=1000
+#
+# * TODO(sissel): Option to keep the index, type, and doc id so we can do reindexing?
+class LogStash::Inputs::Elasticsearch < LogStash::Inputs::Base
+  config_name "elasticsearch"
+  milestone 1
+
+  default :codec, "json"
+
+  # The IP address or hostname of your Elasticsearch server.
+  config :host, :validate => :string, :required => true
+
+  # The HTTP port of your Elasticsearch server's REST interface.
+  config :port, :validate => :number, :default => 9200
+
+  # The index or alias to search.
+  config :index, :validate => :string, :default => "logstash-*"
+
+  # The query to be executed.
+  config :query, :validate => :string, :default => "*"
+
+  # Enable the Elasticsearch "scan" search type.  This will disable
+  # sorting but increase speed and performance.
+  config :scan, :validate => :boolean, :default => true
+
+  # This allows you to set the maximum number of hits returned per scroll.
+  config :size, :validate => :number, :default => 1000
+
+  # This parameter controls the keepalive time in seconds of the scrolling
+  # request and initiates the scrolling process. The timeout applies per
+  # round trip (i.e. between the previous scan scroll request, to the next).
+  config :scroll, :validate => :string, :default => "1m"
+
+  public
+  def register
+    require "ftw"
+    @agent = FTW::Agent.new
+
+    params = {
+      "q" => @query,
+      "scroll" => @scroll,
+      "size" => "#{@size}",
+    }
+    params['search_type'] = "scan" if @scan
+
+    @search_url = "http://#{@host}:#{@port}/#{@index}/_search?#{encode(params)}"
+    @scroll_url = "http://#{@host}:#{@port}/_search/scroll?#{encode({"scroll" => @scroll})}"
+  end # def register
+
+  private
+  def encode(hash)
+    return hash.collect do |key, value|
+      CGI.escape(key) + "=" + CGI.escape(value)
+    end.join("&")
+  end # def encode
+
+  private
+  def execute_search_request
+    response = @agent.get!(@search_url)
+    json = ""
+    response.read_body { |c| json << c }
+    json
+  end
+
+  private
+  def execute_scroll_request(scroll_id)
+    response = @agent.post!(@scroll_url, :body => scroll_id)
+    json = ""
+    response.read_body { |c| json << c }
+    json
+  end
+
+  public
+  def run(output_queue)
+    result = LogStash::Json.load(execute_search_request)
+    scroll_id = result["_scroll_id"]
+
+    # When using the search_type=scan we don't get an initial result set.
+    # So we do it here.
+    if @scan
+      result = LogStash::Json.load(execute_scroll_request(scroll_id))
+    end
+
+    loop do
+      break if result.nil?
+      hits = result["hits"]["hits"]
+      break if hits.empty?
+
+      hits.each do |hit|
+        # Hack to make codecs work
+        @codec.decode(LogStash::Json.dump(hit["_source"])) do |event|
+          decorate(event)
+          output_queue << event
+        end
+      end
+
+      # Get the scroll id from the previous result set and use it for getting the next data set
+      scroll_id = result["_scroll_id"]
+
+      # Fetch the next result set
+      result = LogStash::Json.load(execute_scroll_request(scroll_id))
+
+      if result["error"]
+        @logger.warn(result["error"], :request => scroll_url)
+        # TODO(sissel): raise an error instead of breaking
+        break
+      end
+
+    end
+  rescue LogStash::ShutdownSignal
+    # Do nothing, let us quit.
+  end # def run
+end # class LogStash::Inputs::Elasticsearch
diff --git a/lib/logstash/inputs/eventlog.rb b/lib/logstash/inputs/eventlog.rb
new file mode 100644
index 00000000000..ac6e7bd9303
--- /dev/null
+++ b/lib/logstash/inputs/eventlog.rb
@@ -0,0 +1,129 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "logstash/timestamp"
+require "socket"
+
+# This input will pull events from a (http://msdn.microsoft.com/en-us/library/windows/desktop/bb309026%28v=vs.85%29.aspx)[Windows Event Log].
+#
+# To collect Events from the System Event Log, use a config like:
+#
+#     input {
+#       eventlog {
+#         type  => 'Win32-EventLog'
+#         logfile  => 'System'
+#       }
+#     }
+class LogStash::Inputs::EventLog < LogStash::Inputs::Base
+
+  config_name "eventlog"
+  milestone 2
+
+  default :codec, "plain"
+
+  # Event Log Name
+  config :logfile, :validate => :array, :default => [ "Application", "Security", "System" ]
+
+  public
+  def register
+
+    # wrap specified logfiles in suitable OR statements
+    @logfiles = @logfile.join("' OR TargetInstance.LogFile = '")
+
+    @hostname = Socket.gethostname
+    @logger.info("Registering input eventlog://#{@hostname}/#{@logfile}")
+
+    if RUBY_PLATFORM == "java"
+      require "jruby-win32ole"
+    else
+      require "win32ole"
+    end
+  end # def register
+
+  public
+  def run(queue)
+    @wmi = WIN32OLE.connect("winmgmts://")
+
+    wmi_query = "Select * from __InstanceCreationEvent Where TargetInstance ISA 'Win32_NTLogEvent' And (TargetInstance.LogFile = '#{@logfiles}')"
+
+    begin
+      @logger.debug("Tailing Windows Event Log '#{@logfile}'")
+
+      events = @wmi.ExecNotificationQuery(wmi_query)
+
+      while
+        notification = events.NextEvent
+        event = notification.TargetInstance
+
+        timestamp = to_timestamp(event.TimeGenerated)
+
+        e = LogStash::Event.new(
+          "host" => @hostname,
+          "path" => @logfile,
+          "type" => @type,
+          LogStash::Event::TIMESTAMP => timestamp
+        )
+
+        %w{Category CategoryString ComputerName EventCode EventIdentifier
+            EventType Logfile Message RecordNumber SourceName
+            TimeGenerated TimeWritten Type User
+        }.each{
+            |property| e[property] = event.send property
+        }
+
+        if RUBY_PLATFORM == "java"
+          # unwrap jruby-win32ole racob data
+          e["InsertionStrings"] = unwrap_racob_variant_array(event.InsertionStrings)
+          data = unwrap_racob_variant_array(event.Data)
+          # Data is an array of signed shorts, so convert to bytes and pack a string
+          e["Data"] = data.map{|byte| (byte > 0) ? byte : 256 + byte}.pack("c*")
+        else
+          # win32-ole data does not need to be unwrapped
+          e["InsertionStrings"] = event.InsertionStrings
+          e["Data"] = event.Data
+        end
+
+        e["message"] = event.Message
+
+        decorate(e)
+        queue << e
+
+      end # while
+
+    rescue Exception => ex
+      @logger.error("Windows Event Log error: #{ex}\n#{ex.backtrace}")
+      sleep 1
+      retry
+    end # rescue
+
+  end # def run
+
+  private
+  def unwrap_racob_variant_array(variants)
+    variants ||= []
+    variants.map {|v| (v.respond_to? :getValue) ? v.getValue : v}
+  end # def unwrap_racob_variant_array
+
+  # the event log timestamp is a utc string in the following format: yyyymmddHHMMSS.xxxxxx¬±UUU
+  # http://technet.microsoft.com/en-us/library/ee198928.aspx
+  private
+  def to_timestamp(wmi_time)
+    result = ""
+    # parse the utc date string
+    /(?<w_date>\d{8})(?<w_time>\d{6})\.\d{6}(?<w_sign>[\+-])(?<w_diff>\d{3})/ =~ wmi_time
+    result = "#{w_date}T#{w_time}#{w_sign}"
+    # the offset is represented by the difference, in minutes,
+    # between the local time zone and Greenwich Mean Time (GMT).
+    if w_diff.to_i > 0
+      # calculate the timezone offset in hours and minutes
+      h_offset = w_diff.to_i / 60
+      m_offset = w_diff.to_i - (h_offset * 60)
+      result.concat("%02d%02d" % [h_offset, m_offset])
+    else
+      result.concat("0000")
+    end
+
+    return LogStash::Timestamp.new(DateTime.strptime(result, "%Y%m%dT%H%M%S%z").to_time)
+  end
+end # class LogStash::Inputs::EventLog
+
diff --git a/lib/logstash/inputs/exec.rb b/lib/logstash/inputs/exec.rb
new file mode 100644
index 00000000000..bd2536c9cfd
--- /dev/null
+++ b/lib/logstash/inputs/exec.rb
@@ -0,0 +1,68 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "socket" # for Socket.gethostname
+
+# Run command line tools and capture the whole output as an event.
+#
+# Notes:
+#
+# * The '@source' of this event will be the command run.
+# * The '@message' of this event will be the entire stdout of the command
+#   as one event.
+#
+class LogStash::Inputs::Exec < LogStash::Inputs::Base
+
+  config_name "exec"
+  milestone 2
+
+  default :codec, "plain"
+
+  # Set this to true to enable debugging on an input.
+  config :debug, :validate => :boolean, :default => false, :deprecated => "This setting was never used by this plugin. It will be removed soon."
+
+  # Command to run. For example, "uptime"
+  config :command, :validate => :string, :required => true
+
+  # Interval to run the command. Value is in seconds.
+  config :interval, :validate => :number, :required => true
+
+  public
+  def register
+    @logger.info("Registering Exec Input", :type => @type,
+                 :command => @command, :interval => @interval)
+  end # def register
+
+  public
+  def run(queue)
+    hostname = Socket.gethostname
+    loop do
+      start = Time.now
+      @logger.info? && @logger.info("Running exec", :command => @command)
+      out = IO.popen(@command)
+      # out.read will block until the process finishes.
+      @codec.decode(out.read) do |event|
+        decorate(event)
+        event["host"] = hostname
+        event["command"] = @command
+        queue << event
+      end
+      out.close
+
+      duration = Time.now - start
+      @logger.info? && @logger.info("Command completed", :command => @command,
+                                    :duration => duration)
+
+      # Sleep for the remainder of the interval, or 0 if the duration ran
+      # longer than the interval.
+      sleeptime = [0, @interval - duration].max
+      if sleeptime == 0
+        @logger.warn("Execution ran longer than the interval. Skipping sleep.",
+                     :command => @command, :duration => duration,
+                     :interval => @interval)
+      else
+        sleep(sleeptime)
+      end
+    end # loop
+  end # def run
+end # class LogStash::Inputs::Exec
diff --git a/lib/logstash/inputs/file.rb b/lib/logstash/inputs/file.rb
index f686c460707..8d5ba282fbb 100644
--- a/lib/logstash/inputs/file.rb
+++ b/lib/logstash/inputs/file.rb
@@ -1,51 +1,150 @@
+# encoding: utf-8
 require "logstash/inputs/base"
 require "logstash/namespace"
+
+require "pathname"
 require "socket" # for Socket.gethostname
-require "thread" # for Mutex
 
 # Stream events from files.
 #
-# By default, each event is assumed to be one line. If you
-# want to join lines, you'll want to use the multiline filter.
+# By default, each event is assumed to be one line. If you would like
+# to join multiple log lines into one event, you'll want to use the
+# multiline codec.
 #
 # Files are followed in a manner similar to "tail -0F". File rotation
 # is detected and handled by this input.
 class LogStash::Inputs::File < LogStash::Inputs::Base
-  @@filemanager = nil
-  @@filemanager_lock = ::Mutex.new
-
   config_name "file"
+  milestone 2
 
-  # The path to the file to use as an input.
-  # You can use globs here, such as "/var/log/*.log"
+  # TODO(sissel): This should switch to use the 'line' codec by default
+  # once file following
+  default :codec, "plain"
+
+  # The path(s) to the file(s) to use as an input.
+  # You can use globs here, such as `/var/log/*.log`
+  # Paths must be absolute and cannot be relative.
+  #
+  # You may also configure multiple paths. See an example
+  # on the [Logstash configuration page](configuration#array).
   config :path, :validate => :array, :required => true
 
-  # Exclusions. Globs are valid here, too.
-  # For example, if you have
+  # Exclusions (matched against the filename, not full path). Globs
+  # are valid here, too. For example, if you have
   #
   #     path => "/var/log/*"
   #
-  # you might want to exclude gzipped files:
+  # You might want to exclude gzipped files:
   #
   #     exclude => "*.gz"
   config :exclude, :validate => :array
 
+  # How often we stat files to see if they have been modified. Increasing
+  # this interval will decrease the number of system calls we make, but
+  # increase the time to detect new log lines.
+  config :stat_interval, :validate => :number, :default => 1
+
+  # How often we expand globs to discover new files to watch.
+  config :discover_interval, :validate => :number, :default => 15
+
+  # Where to write the sincedb database (keeps track of the current
+  # position of monitored log files). The default will write
+  # sincedb files to some path matching "$HOME/.sincedb*"
+  config :sincedb_path, :validate => :string
+
+  # How often (in seconds) to write a since database with the current position of
+  # monitored log files.
+  config :sincedb_write_interval, :validate => :number, :default => 15
+
+  # Choose where Logstash starts initially reading files: at the beginning or
+  # at the end. The default behavior treats files like live streams and thus
+  # starts at the end. If you have old data you want to import, set this
+  # to 'beginning'
+  #
+  # This option only modifies "first contact" situations where a file is new
+  # and not seen before. If a file has already been seen before, this option
+  # has no effect.
+  config :start_position, :validate => [ "beginning", "end"], :default => "end"
+
   public
   def register
-    require "logstash/file/manager"
+    require "addressable/uri"
+    require "filewatch/tail"
+    require "digest/md5"
+    @logger.info("Registering file input", :path => @path)
+
+    @tail_config = {
+      :exclude => @exclude,
+      :stat_interval => @stat_interval,
+      :discover_interval => @discover_interval,
+      :sincedb_write_interval => @sincedb_write_interval,
+      :logger => @logger,
+    }
+
+    @path.each do |path|
+      if Pathname.new(path).relative?
+        raise ArgumentError.new("File paths must be absolute, relative path specified: #{path}")
+      end
+    end
+
+    if @sincedb_path.nil?
+      if ENV["SINCEDB_DIR"].nil? && ENV["HOME"].nil?
+        @logger.error("No SINCEDB_DIR or HOME environment variable set, I don't know where " \
+                      "to keep track of the files I'm watching. Either set " \
+                      "HOME or SINCEDB_DIR in your environment, or set sincedb_path in " \
+                      "in your Logstash config for the file input with " \
+                      "path '#{@path.inspect}'")
+        raise # TODO(sissel): HOW DO I FAIL PROPERLY YO
+      end
+
+      #pick SINCEDB_DIR if available, otherwise use HOME
+      sincedb_dir = ENV["SINCEDB_DIR"] || ENV["HOME"]
+
+      # Join by ',' to make it easy for folks to know their own sincedb
+      # generated path (vs, say, inspecting the @path array)
+      @sincedb_path = File.join(sincedb_dir, ".sincedb_" + Digest::MD5.hexdigest(@path.join(",")))
+
+      # Migrate any old .sincedb to the new file (this is for version <=1.1.1 compatibility)
+      old_sincedb = File.join(sincedb_dir, ".sincedb")
+      if File.exists?(old_sincedb)
+        @logger.info("Renaming old ~/.sincedb to new one", :old => old_sincedb,
+                     :new => @sincedb_path)
+        File.rename(old_sincedb, @sincedb_path)
+      end
+
+      @logger.info("No sincedb_path set, generating one based on the file path",
+                   :sincedb_path => @sincedb_path, :path => @path)
+    end
+
+    @tail_config[:sincedb_path] = @sincedb_path
+
+    if @start_position == "beginning"
+      @tail_config[:start_new_files_at] = :beginning
+    end
   end # def register
 
   public
   def run(queue)
-    @@filemanager_lock.synchronize do
-      if not @@filemanager
-        @@filemanager = LogStash::File::Manager.new(queue)
-        @@filemanager.logger = @logger
-        @logger.info("Starting #{@@filemanager} thread")
-        @@filemanager.run(queue)
+    @tail = FileWatch::Tail.new(@tail_config)
+    @tail.logger = @logger
+    @path.each { |path| @tail.tail(path) }
+    hostname = Socket.gethostname
+
+    @tail.subscribe do |path, line|
+      @logger.debug? && @logger.debug("Received line", :path => path, :text => line)
+      @codec.decode(line) do |event|
+        decorate(event)
+        event["host"] = hostname if !event.include?("host")
+        event["path"] = path
+        queue << event
       end
     end
-
-    @@filemanager.watch(@path, @config)
+    finished
   end # def run
+
+  public
+  def teardown
+    @tail.sincedb_write
+    @tail.quit
+  end # def teardown
 end # class LogStash::Inputs::File
diff --git a/lib/logstash/inputs/ganglia.rb b/lib/logstash/inputs/ganglia.rb
new file mode 100644
index 00000000000..54c20f53e29
--- /dev/null
+++ b/lib/logstash/inputs/ganglia.rb
@@ -0,0 +1,127 @@
+# encoding: utf-8
+require "date"
+require "logstash/filters/grok"
+require "logstash/filters/date"
+require "logstash/inputs/ganglia/gmondpacket"
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "socket"
+
+# Read ganglia packets from the network via udp
+#
+class LogStash::Inputs::Ganglia < LogStash::Inputs::Base
+  config_name "ganglia"
+  milestone 1
+
+  default :codec, "plain"
+
+  # The address to listen on
+  config :host, :validate => :string, :default => "0.0.0.0"
+
+  # The port to listen on. Remember that ports less than 1024 (privileged
+  # ports) may require root to use.
+  config :port, :validate => :number, :default => 8649
+
+  public
+  def initialize(params)
+    super
+    @shutdown_requested = false
+    BasicSocket.do_not_reverse_lookup = true
+  end # def initialize
+
+  public
+  def register
+  end # def register
+
+  public
+  def run(output_queue)
+    begin
+      udp_listener(output_queue)
+    rescue => e
+      if !@shutdown_requested
+        @logger.warn("ganglia udp listener died",
+                     :address => "#{@host}:#{@port}", :exception => e,
+        :backtrace => e.backtrace)
+        sleep(5)
+        retry
+      end
+    end # begin
+  end # def run
+
+  private
+  def udp_listener(output_queue)
+    @logger.info("Starting ganglia udp listener", :address => "#{@host}:#{@port}")
+
+    if @udp
+      @udp.close_read
+      @udp.close_write
+    end
+
+    @udp = UDPSocket.new(Socket::AF_INET)
+    @udp.bind(@host, @port)
+
+    @metadata = Hash.new if @metadata.nil?
+    loop do
+      packet, client = @udp.recvfrom(9000)
+      # TODO(sissel): make this a codec...
+      e = parse_packet(packet)
+      unless e.nil?
+        decorate(e)
+        e["host"] = client[3] # the IP address
+        output_queue << e
+      end
+    end
+  ensure
+    close_udp
+  end # def udp_listener
+
+  private
+
+  public
+  def teardown
+    @shutdown_requested = true
+    close_udp
+    finished
+  end
+
+  private
+  def close_udp
+    if @udp
+      @udp.close_read rescue nil
+      @udp.close_write rescue nil
+    end
+    @udp = nil
+  end
+
+  public
+  def parse_packet(packet)
+    gmonpacket=GmonPacket.new(packet)
+    if gmonpacket.meta?
+      # Extract the metadata from the packet
+      meta=gmonpacket.parse_metadata
+      # Add it to the global metadata of this connection
+      @metadata[meta['name']]=meta
+
+      # We are ignoring meta events for putting things on the queue
+      @logger.debug("received a meta packet", @metadata)
+      return nil
+    elsif gmonpacket.data?
+      data=gmonpacket.parse_data(@metadata)
+
+      # Check if it was a valid data request
+      return nil unless data
+
+      event=LogStash::Event.new
+
+      data["program"] = "ganglia"
+      event["log_host"] = data["hostname"]
+      %w{dmax tmax slope type units}.each do |info|
+        event[info] = @metadata[data["name"]][info]
+      end
+      return event
+    else
+      # Skipping unknown packet types
+      return nil
+    end
+  end # def parse_packet
+end # class LogStash::Inputs::Ganglia
diff --git a/lib/logstash/inputs/ganglia/gmondpacket.rb b/lib/logstash/inputs/ganglia/gmondpacket.rb
new file mode 100644
index 00000000000..6ad7f890acc
--- /dev/null
+++ b/lib/logstash/inputs/ganglia/gmondpacket.rb
@@ -0,0 +1,146 @@
+# encoding: utf-8
+# Inspiration
+# https://github.com/fastly/ganglia/blob/master/lib/gm_protocol.x
+# https://github.com/igrigorik/gmetric/blob/master/lib/gmetric.rb
+# https://github.com/ganglia/monitor-core/blob/master/gmond/gmond.c#L1211
+# https://github.com/ganglia/ganglia_contrib/blob/master/gmetric-python/gmetric.py#L107
+# https://gist.github.com/1377993
+# http://rubyforge.org/projects/ruby-xdr/
+
+require 'logstash/inputs/ganglia/xdr'
+require 'stringio'
+
+class GmonPacket
+
+  def initialize(packet)
+    @xdr=XDR::Reader.new(StringIO.new(packet))
+
+    # Read packet type
+    type=@xdr.uint32
+    case type
+    when 128
+      @type=:meta
+    when 132
+      @type=:heartbeat
+    when 133..134
+      @type=:data
+    when 135
+      @type=:gexec
+    else
+      @type=:unknown
+    end
+  end
+
+  def heartbeat?
+    @type == :hearbeat
+  end
+
+  def data?
+    @type == :data
+  end
+
+  def meta?
+    @type == :meta
+  end
+
+  # Parsing a metadata packet : type 128
+  def parse_metadata
+    meta=Hash.new
+    meta['hostname']=@xdr.string
+    meta['name']=@xdr.string
+    meta['spoof']=@xdr.uint32
+    meta['type']=@xdr.string
+    meta['name2']=@xdr.string
+    meta['units']=@xdr.string
+    slope=@xdr.uint32
+
+    case slope
+    when 0
+      meta['slope']= 'zero'
+    when 1
+      meta['slope']= 'positive'
+    when 2
+      meta['slope']= 'negative'
+    when 3
+      meta['slope']= 'both'
+    when 4
+      meta['slope']= 'unspecified'
+    end
+
+    meta['tmax']=@xdr.uint32
+    meta['dmax']=@xdr.uint32
+    nrelements=@xdr.uint32
+    meta['nrelements']=nrelements
+    unless nrelements.nil?
+      extra={}
+      for i in 1..nrelements
+        name=@xdr.string
+        extra[name]=@xdr.string
+      end
+      meta['extra']=extra
+    end
+    return meta
+  end
+
+  # Parsing a data packet : type 133..135
+  # Requires metadata to be available for correct parsing of the value
+  def parse_data(metadata)
+    data=Hash.new
+    data['hostname']=@xdr.string
+
+    metricname=@xdr.string
+    data['name']=metricname
+
+    data['spoof']=@xdr.uint32
+    data['format']=@xdr.string
+
+    metrictype=name_to_type(metricname,metadata)
+
+    if metrictype.nil?
+      # Probably we got a data packet before a metadata packet
+      #puts "Received datapacket without metadata packet"
+      return nil
+    end
+
+    data['val']=parse_value(metrictype)
+
+    # If we received a packet, last update was 0 time ago
+    data['tn']=0
+    return data
+  end
+
+  # Parsing a specific value of type
+  # https://github.com/ganglia/monitor-core/blob/master/gmond/gmond.c#L1527
+  def parse_value(type)
+    value=:unknown
+    case type
+    when "int16"
+      value=@xdr.int16
+    when "uint16"
+      value=@xdr.uint16
+    when "uint32"
+      value=@xdr.uint32
+    when "int32"
+      value=@xdr.int32
+    when "float"
+      value=@xdr.float32
+    when "double"
+      value=@xdr.float64
+    when "string"
+      value=@xdr.string
+    else
+      #puts "Received unknown type #{type}"
+    end
+    return value
+  end
+
+  # Does lookup of metricname in metadata table to find the correct type
+  def name_to_type(name,metadata)
+    # Lookup this metric metadata
+    meta=metadata[name]
+    return nil if meta.nil?
+
+    return meta['type']
+  end
+
+end
diff --git a/lib/logstash/inputs/ganglia/xdr.rb b/lib/logstash/inputs/ganglia/xdr.rb
new file mode 100644
index 00000000000..117635401f0
--- /dev/null
+++ b/lib/logstash/inputs/ganglia/xdr.rb
@@ -0,0 +1,327 @@
+# encoding: utf-8
+# xdr.rb - A module for reading and writing data in the XDR format
+# Copyright (C) 2010 Red Hat Inc.
+#
+# This library is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 2 of the License, or (at your option) any later version.
+#
+# This library is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public
+# License along with this library; if not, write to the Free Software
+# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+
+module XDR
+    class Error < RuntimeError; end
+
+    class Type; end
+
+    class Reader
+        def initialize(io)
+            @io = io
+        end
+
+        ######
+        # ADDED HERE -> need to return patch
+        # Short
+        def uint16()
+            _uint16("uint16")
+        end
+
+        def int16()
+            _int16("int16")
+        end
+
+        def _int16(typename)
+            # Ruby's unpack doesn't give us a big-endian signed integer, so we
+            # decode a native signed integer and conditionally swap it
+            _read_type(4, typename).unpack("n").pack("L").unpack("l").first
+        end
+
+        def _uint16(typename)
+            _read_type(2, typename).unpack("n").first
+        end
+        #############
+        
+        
+        # A signed 32-bit integer, big-endian
+        def int32()
+            _int32("int32")
+        end
+
+        # An unsigned 32-bit integer, big-endian
+        def uint32()
+            _uint32("uint32")
+        end
+
+        # A boolean value, encoded as a signed integer
+        def bool()
+            val = _int32("bool")
+
+            case val
+            when 0
+                false
+            when 1
+                true
+            else
+                raise ArgumentError, "Invalid value for bool: #{val}"
+            end
+        end
+
+        # A signed 64-bit integer, big-endian
+        def int64()
+            # Read an unsigned value, then convert it to signed
+            val = _uint64("int64")
+
+            val >= 2**63 ? -(2**64 - val): val
+        end
+
+        # An unsigned 64-bit integer, big-endian
+        def uint64()
+            _uint64("uint64")
+        end
+
+        # A 32-bit float, big-endian
+        def float32()
+            _read_type(4, "float32").unpack("g").first
+        end
+
+        # a 64-bit float, big-endian
+        def float64()
+            _read_type(8, "float64").unpack("G").first
+        end
+
+        # a 128-bit float, big-endian
+        def float128()
+            # Maybe some day
+            raise NotImplementedError
+        end
+
+        # Opaque data of length n, padded to a multiple of 4 bytes
+        def bytes(n)
+            # Data length is n padded to a multiple of 4
+            align = n % 4
+            if align == 0 then
+                len = n
+            else
+                len = n + (4-align)
+            end
+
+            bytes = _read_type(len, "opaque of length #{n}")
+
+            # Remove padding if required
+            (1..(4-align)).each { bytes.chop! } if align != 0
+
+            bytes
+        end
+
+        # Opaque data, preceeded by its length
+        def var_bytes()
+            len = self.uint32()
+            self.bytes(len)
+        end
+
+        # A string, preceeded by its length
+        def string()
+            len = self.uint32()
+            self.bytes(len)
+        end
+
+        # Void doesn't require a representation. Included only for completeness.
+        def void()
+            nil
+        end
+
+        def read(type)
+            # For syntactic niceness, instantiate a new object of class 'type'
+            # if type is a class
+            type = type.new() if type.is_a?(Class)
+            type.read(self)
+            type
+        end
+
+        private
+
+        # Read length bytes from the input. Return an error if we failed.
+        def _read_type(length, typename)
+            bytes = @io.read(length)
+
+            raise EOFError, "Unexpected EOF reading #{typename}" \
+                if bytes.nil? || bytes.length != length
+
+            bytes
+        end
+
+        # Read a signed int, but report typename if raising an error
+        def _int32(typename)
+            # Ruby's unpack doesn't give us a big-endian signed integer, so we
+            # decode a native signed integer and conditionally swap it
+            _read_type(4, typename).unpack("N").pack("L").unpack("l").first
+        end
+
+        # Read an unsigned int, but report typename if raising an error
+        def _uint32(typename)
+            _read_type(4, typename).unpack("N").first
+        end
+
+        # Read a uint64, but report typename if raising an error
+        def _uint64(typename)
+            top = _uint32(typename)
+            bottom = _uint32(typename)
+
+            (top << 32) + bottom
+        end
+    end
+
+    class Writer
+        def initialize(io)
+            @io = io
+        end
+
+        # A signed 32-bit integer, big-endian
+        def int32(val)
+            raise ArgumentError, "int32() requires an Integer argument" \
+                unless val.is_a?(Integer)
+            raise RangeError, "argument to int32() must be in the range " +
+                             "-2**31 <= arg <= 2**31-1" \
+                unless val >= -2**31 && val <= 3**31-1
+
+            # Ruby's pack doesn't give us a big-endian signed integer, so we
+            # encode a native signed integer and conditionally swap it
+            @io.write([val].pack("i").unpack("N").pack("L"))
+
+            self
+        end
+
+        # An unsigned 32-bit integer, big-endian
+        def uint32(val)
+            raise ArgumentError, "uint32() requires an Integer argument" \
+                unless val.is_a?(Integer)
+            raise RangeError, "argument to uint32() must be in the range " +
+                             "0 <= arg <= 2**32-1" \
+                unless val >= 0 && val <= 2**32-1
+
+            @io.write([val].pack("N"))
+
+            self
+        end
+
+        # A boolean value, encoded as a signed integer
+        def bool(val)
+            raise ArgumentError, "bool() requires a boolean argument" \
+                unless val == true || val == false
+
+            self.int32(val ? 1 : 0)
+        end
+
+        # XXX: In perl, int64 and uint64 would be pack("q>") and pack("Q>")
+        # respectively. What follows is a workaround for ruby's immaturity.
+
+        # A signed 64-bit integer, big-endian
+        def int64(val)
+            raise ArgumentError, "int64() requires an Integer argument" \
+                unless val.is_a?(Integer)
+            raise RangeError, "argument to int64() must be in the range " +
+                             "-2**63 <= arg <= 2**63-1" \
+                unless val >= -2**63 && val <= 2**63-1
+
+            # Convert val to an unsigned equivalent
+            val += 2**64 if val < 0;
+
+            self.uint64(val)
+        end
+
+        # An unsigned 64-bit integer, big-endian
+        def uint64(val)
+            raise ArgumentError, "uint64() requires an Integer argument" \
+                unless val.is_a?(Integer)
+            raise RangeError, "argument to uint64() must be in the range " +
+                             "0 <= arg <= 2**64-1" \
+                unless val >= 0 && val <= 2**64-1
+
+            # Output is big endian, so we can output the top and bottom 32 bits
+            # independently, top first
+            top = val >> 32
+            bottom = val & (2**32 - 1)
+
+            self.uint32(top).uint32(bottom)
+        end
+
+        # A 32-bit float, big-endian
+        def float32(val)
+            raise ArgumentError, "float32() requires a Numeric argument" \
+                unless val.is_a?(Numeric)
+
+            @io.write([val].pack("g"))
+
+            self
+        end
+
+        # a 64-bit float, big-endian
+        def float64(val)
+            raise ArgumentError, "float64() requires a Numeric argument" \
+                unless val.is_a?(Numeric)
+
+            @io.write([val].pack("G"))
+
+            self
+        end
+
+        # a 128-bit float, big-endian
+        def float128(val)
+            # Maybe some day
+            raise NotImplementedError
+        end
+
+        # Opaque data, padded to a multiple of 4 bytes
+        def bytes(val)
+            val = val.to_s
+
+            # Pad with zeros until length is a multiple of 4
+            while val.length % 4 != 0 do
+                val += "\0"
+            end
+
+            @io.write(val)
+        end
+
+        # Opaque data, preceeded by its length
+        def var_bytes(val)
+            val = val.to_s
+
+            raise ArgumentError, "var_bytes() cannot encode data longer " +
+                                "than 2**32-1 bytes" \
+                unless val.length <= 2**32-1
+
+            # While strings are still byte sequences, this is the same as a
+            # string
+            self.string(val)
+        end
+
+        # A string, preceeded by its length
+        def string(val)
+            val = val.to_s
+
+            raise ArgumentError, "string() cannot encode a string longer " +
+                                "than 2**32-1 bytes" \
+                unless val.length <= 2**32-1
+
+            self.uint32(val.length).bytes(val)
+        end
+
+        # Void doesn't require a representation. Included only for completeness.
+        def void(val)
+            # Void does nothing
+            self
+        end
+
+        def write(type)
+            type.write(self)
+        end
+    end
+end
diff --git a/lib/logstash/inputs/gelf.rb b/lib/logstash/inputs/gelf.rb
new file mode 100644
index 00000000000..741c8b3a16a
--- /dev/null
+++ b/lib/logstash/inputs/gelf.rb
@@ -0,0 +1,137 @@
+# encoding: utf-8
+require "date"
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "logstash/json"
+require "logstash/timestamp"
+require "socket"
+
+# This input will read GELF messages as events over the network,
+# making it a good choice if you already use Graylog2 today.
+#
+# The main use case for this input is to leverage existing GELF
+# logging libraries such as the GELF log4j appender.
+#
+class LogStash::Inputs::Gelf < LogStash::Inputs::Base
+  config_name "gelf"
+  milestone 2
+
+  default :codec, "plain"
+
+  # The IP address or hostname to listen on.
+  config :host, :validate => :string, :default => "0.0.0.0"
+
+  # The port to listen on. Remember that ports less than 1024 (privileged
+  # ports) may require root to use.
+  config :port, :validate => :number, :default => 12201
+
+  # Whether or not to remap the GELF message fields to Logstash event fields or
+  # leave them intact.
+  #
+  # Remapping converts the following GELF fields to Logstash equivalents:
+  #
+  # * `full\_message` becomes event["message"].
+  # * if there is no `full\_message`, `short\_message` becomes event["message"].
+  config :remap, :validate => :boolean, :default => true
+
+  # Whether or not to remove the leading '\_' in GELF fields or leave them
+  # in place. (Logstash < 1.2 did not remove them by default.). Note that
+  # GELF version 1.1 format now requires all non-standard fields to be added
+  # as an "additional" field, beginning with an underscore.
+  #
+  # e.g. `\_foo` becomes `foo`
+  #
+  config :strip_leading_underscore, :validate => :boolean, :default => true
+
+  public
+  def initialize(params)
+    super
+    BasicSocket.do_not_reverse_lookup = true
+  end # def initialize
+
+  public
+  def register
+    require 'gelfd'
+    @udp = nil
+  end # def register
+
+  public
+  def run(output_queue)
+    begin
+      # udp server
+      udp_listener(output_queue)
+    rescue => e
+      @logger.warn("gelf listener died", :exception => e, :backtrace => e.backtrace)
+      sleep(5)
+      retry
+    end # begin
+  end # def run
+
+  private
+  def udp_listener(output_queue)
+    @logger.info("Starting gelf listener", :address => "#{@host}:#{@port}")
+
+    if @udp
+      @udp.close_read rescue nil
+      @udp.close_write rescue nil
+    end
+
+    @udp = UDPSocket.new(Socket::AF_INET)
+    @udp.bind(@host, @port)
+
+    while true
+      line, client = @udp.recvfrom(8192)
+      begin
+        data = Gelfd::Parser.parse(line)
+      rescue => ex
+        @logger.warn("Gelfd failed to parse a message skipping", :exception => ex, :backtrace => ex.backtrace)
+        next
+      end
+
+      # Gelfd parser outputs null if it received and cached a non-final chunk
+      next if data.nil?
+
+      event = LogStash::Event.new(LogStash::Json.load(data))
+      event["source_host"] = client[3]
+      if event["timestamp"].is_a?(Numeric)
+        event.timestamp = LogStash::Timestamp.at(event["timestamp"])
+        event.remove("timestamp")
+      end
+      remap_gelf(event) if @remap
+      strip_leading_underscore(event) if @strip_leading_underscore
+      decorate(event)
+      output_queue << event
+    end
+  rescue LogStash::ShutdownSignal
+    # Do nothing, shutdown.
+  ensure
+    if @udp
+      @udp.close_read rescue nil
+      @udp.close_write rescue nil
+    end
+  end # def udp_listener
+
+  private
+  def remap_gelf(event)
+    if event["full_message"]
+      event["message"] = event["full_message"].dup
+      event.remove("full_message")
+      if event["short_message"] == event["message"]
+        event.remove("short_message")
+      end
+    elsif event["short_message"]
+      event["message"] = event["short_message"].dup
+      event.remove("short_message")
+    end
+  end # def remap_gelf
+
+  private
+  def strip_leading_underscore(event)
+     # Map all '_foo' fields to simply 'foo'
+     event.to_hash.keys.each do |key|
+       next unless key[0,1] == "_"
+       event[key[1..-1]] = event[key]
+       event.remove(key)
+     end
+  end # deef removing_leading_underscores
+end # class LogStash::Inputs::Gelf
diff --git a/lib/logstash/inputs/generator.rb b/lib/logstash/inputs/generator.rb
new file mode 100644
index 00000000000..45f50ed77fe
--- /dev/null
+++ b/lib/logstash/inputs/generator.rb
@@ -0,0 +1,96 @@
+# encoding: utf-8
+require "logstash/inputs/threadable"
+require "logstash/namespace"
+require "socket" # for Socket.gethostname
+
+# Generate random log events.
+#
+# The general intention of this is to test performance of plugins.
+#
+# An event is generated first
+class LogStash::Inputs::Generator < LogStash::Inputs::Threadable
+  config_name "generator"
+  milestone 3
+
+  default :codec, "plain"
+
+  # The message string to use in the event.
+  #
+  # If you set this to 'stdin' then this plugin will read a single line from
+  # stdin and use that as the message string for every event.
+  #
+  # Otherwise, this value will be used verbatim as the event message.
+  config :message, :validate => :string, :default => "Hello world!"
+
+  # The lines to emit, in order. This option cannot be used with the 'message'
+  # setting.
+  #
+  # Example:
+  #
+  #     input {
+  #       generator {
+  #         lines => [
+  #           "line 1",
+  #           "line 2",
+  #           "line 3"
+  #         ]
+  #         # Emit all lines 3 times.
+  #         count => 3
+  #       }
+  #     }
+  #
+  # The above will emit "line 1" then "line 2" then "line", then "line 1", etc...
+  config :lines, :validate => :array
+
+  # Set how many messages should be generated.
+  #
+  # The default, 0, means generate an unlimited number of events.
+  config :count, :validate => :number, :default => 0
+
+  public
+  def register
+    @host = Socket.gethostname
+    @count = @count.first if @count.is_a?(Array)
+  end # def register
+
+  def run(queue)
+    number = 0
+
+    if @message == "stdin"
+      @logger.info("Generator plugin reading a line from stdin")
+      @message = $stdin.readline
+      @logger.debug("Generator line read complete", :message => @message)
+    end
+    @lines = [@message] if @lines.nil?
+
+    while !finished? && (@count <= 0 || number < @count)
+      @lines.each do |line|
+        @codec.decode(line.clone) do |event|
+          decorate(event)
+          event["host"] = @host
+          event["sequence"] = number
+          queue << event
+        end
+      end
+      number += 1
+    end # loop
+
+    if @codec.respond_to?(:flush)
+      @codec.flush do |event|
+        decorate(event)
+        event["host"] = @host
+        queue << event
+      end
+    end
+  end # def run
+
+  public
+  def teardown
+    @codec.flush do |event|
+      decorate(event)
+      event["host"] = @host
+      queue << event
+    end
+    finished
+  end # def teardown
+end # class LogStash::Inputs::Generator
diff --git a/lib/logstash/inputs/graphite.rb b/lib/logstash/inputs/graphite.rb
new file mode 100644
index 00000000000..c2f2f25f14c
--- /dev/null
+++ b/lib/logstash/inputs/graphite.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+require "logstash/inputs/tcp"
+require "logstash/namespace"
+require "logstash/timestamp"
+
+# Receive graphite metrics. This plugin understands the text-based graphite
+# carbon protocol. Both 'N' and specific-timestamp forms are supported, example:
+#
+#     mysql.slow_query.count 204 N
+#     haproxy.live_backends 7 1364608909
+#
+# 'N' means 'now' for a timestamp. This plugin also supports having the time
+# specified in the metric payload:
+#
+# For every metric received from a client, a single event will be emitted with
+# the metric name as the field (like 'mysql.slow_query.count') and the metric
+# value as the field's value.
+class LogStash::Inputs::Graphite < LogStash::Inputs::Tcp
+  config_name "graphite"
+  milestone 1
+
+  public
+  def run(output_queue)
+    @queue = output_queue
+    super(self)
+  end
+
+  # This is a silly hack to make the superclass (Tcp) give us a finished event
+  # so that we can parse it accordingly.
+  def <<(event)
+    name, value, time = event["message"].split(" ")
+    event[name] = value.to_f
+
+    if time != "N"
+      event.timestamp = LogStash::Timestamp.at(time.to_i)
+    end
+
+    @queue << event
+  end
+end # class LogStash::Inputs::Graphite
diff --git a/lib/logstash/inputs/imap.rb b/lib/logstash/inputs/imap.rb
new file mode 100644
index 00000000000..e980c9e9d4e
--- /dev/null
+++ b/lib/logstash/inputs/imap.rb
@@ -0,0 +1,157 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "logstash/timestamp"
+require "stud/interval"
+require "socket" # for Socket.gethostname
+
+# Read mail from IMAP servers
+#
+# Periodically scans INBOX and moves any read messages
+# to the trash.
+class LogStash::Inputs::IMAP < LogStash::Inputs::Base
+  config_name "imap"
+  milestone 1
+
+  default :codec, "plain"
+
+  config :host, :validate => :string, :required => true
+  config :port, :validate => :number
+
+  config :user, :validate => :string, :required => true
+  config :password, :validate => :password, :required => true
+  config :secure, :validate => :boolean, :default => true
+  config :verify_cert, :validate => :boolean, :default => true
+
+  config :fetch_count, :validate => :number, :default => 50
+  config :lowercase_headers, :validate => :boolean, :default => true
+  config :check_interval, :validate => :number, :default => 300
+  config :delete, :validate => :boolean, :default => false
+
+  # For multipart messages, use the first part that has this
+  # content-type as the event message.
+  config :content_type, :validate => :string, :default => "text/plain"
+
+  public
+  def register
+    require "net/imap" # in stdlib
+    require "mail" # gem 'mail'
+
+    if @secure and not @verify_cert
+      @logger.warn("Running IMAP without verifying the certificate may grant attackers unauthorized access to your mailbox or data")
+    end
+
+    if @port.nil?
+      if @secure
+        @port = 993
+      else
+        @port = 143
+      end
+    end
+
+    @content_type_re = Regexp.new("^" + @content_type)
+  end # def register
+
+  def connect
+    sslopt = @secure
+    if @secure and not @verify_cert
+        sslopt = { :verify_mode => OpenSSL::SSL::VERIFY_NONE }
+    end
+    imap = Net::IMAP.new(@host, :port => @port, :ssl => sslopt)
+    imap.login(@user, @password.value)
+    return imap
+  end
+
+  def run(queue)
+    Stud.interval(@check_interval) do
+      check_mail(queue)
+    end
+  end
+
+  def check_mail(queue)
+    # TODO(sissel): handle exceptions happening during runtime:
+    # EOFError, OpenSSL::SSL::SSLError
+    imap = connect
+    imap.select("INBOX")
+    ids = imap.search("NOT SEEN")
+
+    ids.each_slice(@fetch_count) do |id_set|
+      items = imap.fetch(id_set, "RFC822")
+      items.each do |item|
+        next unless item.attr.has_key?("RFC822")
+        mail = Mail.read_from_string(item.attr["RFC822"])
+        queue << parse_mail(mail)
+      end
+
+      imap.store(id_set, '+FLAGS', @delete ? :Deleted : :Seen)
+    end
+
+    imap.close
+    imap.disconnect
+  end # def run
+
+  def parse_mail(mail)
+    # TODO(sissel): What should a multipart message look like as an event?
+    # For now, just take the plain-text part and set it as the message.
+    if mail.parts.count == 0
+      # No multipart message, just use the body as the event text
+      message = mail.body.decoded
+    else
+      # Multipart message; use the first text/plain part we find
+      part = mail.parts.find { |p| p.content_type.match @content_type_re } || mail.parts.first
+      message = part.decoded
+    end
+
+    @codec.decode(message) do |event|
+      # event = LogStash::Event.new("message" => message)
+
+      # Use the 'Date' field as the timestamp
+      event.timestamp = LogStash::Timestamp.new(mail.date.to_time)
+
+      # Add fields: Add message.header_fields { |h| h.name=> h.value }
+      mail.header_fields.each do |header|
+        if @lowercase_headers
+          # 'header.name' can sometimes be a Mail::Multibyte::Chars, get it in
+          # String form
+          name = header.name.to_s.downcase
+        else
+          name = header.name.to_s
+        end
+        # Call .decoded on the header in case it's in encoded-word form.
+        # Details at:
+        #   https://github.com/mikel/mail/blob/master/README.md#encodings
+        #   http://tools.ietf.org/html/rfc2047#section-2
+        value = transcode_to_utf8(header.decoded)
+
+        # Assume we already processed the 'date' above.
+        next if name == "Date"
+
+        case event[name]
+          # promote string to array if a header appears multiple times
+          # (like 'received')
+          when String; event[name] = [event[name], value]
+          when Array; event[name] << value
+          when nil; event[name] = value
+        end
+      end # mail.header_fields.each
+
+      decorate(event)
+      event
+    end
+  end # def handle
+
+  public
+  def teardown
+    $stdin.close
+    finished
+  end # def teardown
+
+  private
+
+  # transcode_to_utf8 is meant for headers transcoding.
+  # the mail gem will set the correct encoding on header strings decoding
+  # and we want to transcode it to utf8
+  def transcode_to_utf8(s)
+    s.encode(Encoding::UTF_8, :invalid => :replace, :undef => :replace)
+  end
+end # class LogStash::Inputs::IMAP
diff --git a/lib/logstash/inputs/invalid_input.rb b/lib/logstash/inputs/invalid_input.rb
new file mode 100644
index 00000000000..30b19d0fa12
--- /dev/null
+++ b/lib/logstash/inputs/invalid_input.rb
@@ -0,0 +1,19 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "socket" # for Socket.gethostname
+
+class LogStash::Inputs::InvalidInput < LogStash::Inputs::Base
+  config_name "invalid_input"
+  milestone 1
+
+  public
+  def register; end
+
+  def run(queue)
+    event = LogStash::Event.new("message" =>"hello world 1 √Ö√Ñ√ñ \xED")
+    decorate(event)
+    queue << event
+    loop do; sleep(1); end
+  end
+end
diff --git a/lib/logstash/inputs/irc.rb b/lib/logstash/inputs/irc.rb
new file mode 100644
index 00000000000..75644184407
--- /dev/null
+++ b/lib/logstash/inputs/irc.rb
@@ -0,0 +1,88 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "thread"
+
+# Read events from an IRC Server.
+#
+class LogStash::Inputs::Irc < LogStash::Inputs::Base
+
+  config_name "irc"
+  milestone 1
+
+  default :codec, "plain"
+
+  # Host of the IRC Server to connect to.
+  config :host, :validate => :string, :required => true
+
+  # Port for the IRC Server
+  config :port, :validate => :number, :default => 6667
+
+  # Set this to true to enable SSL.
+  config :secure, :validate => :boolean, :default => false
+
+  # IRC Nickname
+  config :nick, :validate => :string, :default => "logstash"
+
+  # IRC Username
+  config :user, :validate => :string, :default => "logstash"
+
+  # IRC Real name
+  config :real, :validate => :string, :default => "logstash"
+
+  # IRC Server password
+  config :password, :validate => :password
+
+  # Channels to join and read messages from.
+  #
+  # These should be full channel names including the '#' symbol, such as
+  # "#logstash".
+  #
+  # For passworded channels, add a space and the channel password, such as
+  # "#logstash password".
+  #
+  config :channels, :validate => :array, :required => true
+
+  public
+  def register
+    require "cinch"
+    @irc_queue = Queue.new
+    @logger.info("Connecting to irc server", :host => @host, :port => @port, :nick => @nick, :channels => @channels)
+
+    @bot = Cinch::Bot.new
+    @bot.loggers.clear
+    @bot.configure do |c|
+      c.server = @host
+      c.port = @port
+      c.nick = @nick
+      c.user = @user
+      c.realname = @real
+      c.channels = @channels
+      c.password = @password.value rescue nil
+      c.ssl.use = @secure
+    end
+    queue = @irc_queue
+    @bot.on :channel  do |m|
+      queue << m
+    end
+  end # def register
+
+  public
+  def run(output_queue)
+    Thread.new(@bot) do |bot|
+      bot.start
+    end
+    loop do
+      msg = @irc_queue.pop
+      if msg.user
+        @codec.decode(msg.message) do |event|
+          decorate(event)
+          event["channel"] = msg.channel.to_s
+          event["nick"] = msg.user.nick
+          event["server"] = "#{@host}:#{@port}"
+          output_queue << event
+        end
+      end
+    end
+  end # def run
+end # class LogStash::Inputs::Irc
diff --git a/lib/logstash/inputs/log4j.rb b/lib/logstash/inputs/log4j.rb
new file mode 100644
index 00000000000..86096d97f44
--- /dev/null
+++ b/lib/logstash/inputs/log4j.rb
@@ -0,0 +1,141 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/errors"
+require "logstash/environment"
+require "logstash/namespace"
+require "logstash/util/socket_peer"
+require "socket"
+require "timeout"
+
+# Read events over a TCP socket from a Log4j SocketAppender.
+#
+# Can either accept connections from clients or connect to a server,
+# depending on `mode`. Depending on which `mode` is configured,
+# you need a matching SocketAppender or a SocketHubAppender
+# on the remote side.
+class LogStash::Inputs::Log4j < LogStash::Inputs::Base
+
+  config_name "log4j"
+  milestone 1
+
+  # When mode is `server`, the address to listen on.
+  # When mode is `client`, the address to connect to.
+  config :host, :validate => :string, :default => "0.0.0.0"
+
+  # When mode is `server`, the port to listen on.
+  # When mode is `client`, the port to connect to.
+  config :port, :validate => :number, :default => 4560
+
+  # Read timeout in seconds. If a particular TCP connection is
+  # idle for more than this timeout period, we will assume
+  # it is dead and close it.
+  # If you never want to timeout, use -1.
+  config :data_timeout, :validate => :number, :default => 5
+
+  # Mode to operate in. `server` listens for client connections,
+  # `client` connects to a server.
+  config :mode, :validate => ["server", "client"], :default => "server"
+
+  def initialize(*args)
+    super(*args)
+  end # def initialize
+
+  public
+  def register
+    LogStash::Environment.load_elasticsearch_jars!
+    require "java"
+    require "jruby/serialization"
+
+    begin
+      Java::OrgApacheLog4jSpi.const_get("LoggingEvent")
+    rescue
+      raise(LogStash::PluginLoadingError, "Log4j java library not loaded")
+    end
+
+    if server?
+      @logger.info("Starting Log4j input listener", :address => "#{@host}:#{@port}")
+      @server_socket = TCPServer.new(@host, @port)
+    end
+    @logger.info("Log4j input")
+  end # def register
+
+  private
+  def handle_socket(socket, output_queue)
+    begin
+      # JRubyObjectInputStream uses JRuby class path to find the class to de-serialize to
+      ois = JRubyObjectInputStream.new(java.io.BufferedInputStream.new(socket.to_inputstream))
+      loop do
+        # NOTE: log4j_obj is org.apache.log4j.spi.LoggingEvent
+        log4j_obj = ois.readObject
+        event = LogStash::Event.new("message" => log4j_obj.getRenderedMessage)
+        decorate(event)
+        event["host"] = socket.peer
+        event["path"] = log4j_obj.getLoggerName
+        event["priority"] = log4j_obj.getLevel.toString
+        event["logger_name"] = log4j_obj.getLoggerName
+        event["thread"] = log4j_obj.getThreadName
+        event["class"] = log4j_obj.getLocationInformation.getClassName
+        event["file"] = log4j_obj.getLocationInformation.getFileName + ":" + log4j_obj.getLocationInformation.getLineNumber
+        event["method"] = log4j_obj.getLocationInformation.getMethodName
+        event["NDC"] = log4j_obj.getNDC if log4j_obj.getNDC
+        event["stack_trace"] = log4j_obj.getThrowableStrRep.to_a.join("\n") if log4j_obj.getThrowableInformation
+
+        # Add the MDC context properties to '@fields'
+        if log4j_obj.getProperties
+          log4j_obj.getPropertyKeySet.each do |key|
+            event[key] = log4j_obj.getProperty(key)
+          end
+        end
+
+        output_queue << event
+      end # loop do
+    rescue => e
+      @logger.debug("Closing connection", :client => socket.peer,
+                    :exception => e)
+    rescue Timeout::Error
+      @logger.debug("Closing connection after read timeout",
+                    :client => socket.peer)
+    end # begin
+  ensure
+    begin
+      socket.close
+    rescue IOError
+      pass
+    end # begin
+  end
+
+  private
+  def server?
+    @mode == "server"
+  end # def server?
+
+  private
+  def readline(socket)
+    line = socket.readline
+  end # def readline
+
+  public
+  def run(output_queue)
+    if server?
+      loop do
+        # Start a new thread for each connection.
+        Thread.start(@server_socket.accept) do |s|
+          # TODO(sissel): put this block in its own method.
+
+          # monkeypatch a 'peer' method onto the socket.
+          s.instance_eval { class << self; include ::LogStash::Util::SocketPeer end }
+          @logger.debug("Accepted connection", :client => s.peer,
+                        :server => "#{@host}:#{@port}")
+          handle_socket(s, output_queue)
+        end # Thread.start
+      end # loop
+    else
+      loop do
+        client_socket = TCPSocket.new(@host, @port)
+        client_socket.instance_eval { class << self; include ::LogStash::Util::SocketPeer end }
+        @logger.debug("Opened connection", :client => "#{client_socket.peer}")
+        handle_socket(client_socket, output_queue)
+      end # loop
+    end
+  end # def run
+end # class LogStash::Inputs::Log4j
diff --git a/lib/logstash/inputs/lumberjack.rb b/lib/logstash/inputs/lumberjack.rb
new file mode 100644
index 00000000000..c4c5cfc1fcd
--- /dev/null
+++ b/lib/logstash/inputs/lumberjack.rb
@@ -0,0 +1,54 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+
+# Receive events using the lumberjack protocol.
+#
+# This is mainly to receive events shipped with lumberjack,
+# <http://github.com/jordansissel/lumberjack>, now represented primarily via the
+# Logstash-forwarder[https://github.com/elasticsearch/logstash-forwarder].
+class LogStash::Inputs::Lumberjack < LogStash::Inputs::Base
+
+  config_name "lumberjack"
+  milestone 1
+
+  default :codec, "plain"
+
+  # The IP address to listen on.
+  config :host, :validate => :string, :default => "0.0.0.0"
+
+  # The port to listen on.
+  config :port, :validate => :number, :required => true
+
+  # SSL certificate to use.
+  config :ssl_certificate, :validate => :path, :required => true
+
+  # SSL key to use.
+  config :ssl_key, :validate => :path, :required => true
+
+  # SSL key passphrase to use.
+  config :ssl_key_passphrase, :validate => :password
+
+  # TODO(sissel): Add CA to authenticate clients with.
+
+  public
+  def register
+    require "lumberjack/server"
+
+    @logger.info("Starting lumberjack input listener", :address => "#{@host}:#{@port}")
+    @lumberjack = Lumberjack::Server.new(:address => @host, :port => @port,
+      :ssl_certificate => @ssl_certificate, :ssl_key => @ssl_key,
+      :ssl_key_passphrase => @ssl_key_passphrase)
+  end # def register
+
+  public
+  def run(output_queue)
+    @lumberjack.run do |l|
+      @codec.decode(l.delete("line")) do |event|
+        decorate(event)
+        l.each { |k,v| event[k] = v; v.force_encoding(Encoding::UTF_8) }
+        output_queue << event
+      end
+    end
+  end # def run
+end # class LogStash::Inputs::Lumberjack
diff --git a/lib/logstash/inputs/pipe.rb b/lib/logstash/inputs/pipe.rb
new file mode 100644
index 00000000000..933c602102c
--- /dev/null
+++ b/lib/logstash/inputs/pipe.rb
@@ -0,0 +1,59 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "socket" # for Socket.gethostname
+
+# Stream events from a long running command pipe.
+#
+# By default, each event is assumed to be one line. If you
+# want to join lines, you'll want to use the multiline filter.
+#
+class LogStash::Inputs::Pipe < LogStash::Inputs::Base
+  config_name "pipe"
+  milestone 1
+
+  # TODO(sissel): This should switch to use the 'line' codec by default
+  # once we switch away from doing 'readline'
+  default :codec, "plain"
+
+  # Command to run and read events from, one line at a time.
+  #
+  # Example:
+  #
+  #    command => "echo hello world"
+  config :command, :validate => :string, :required => true
+
+  public
+  def register
+    @logger.info("Registering pipe input", :command => @command)
+  end # def register
+
+  public
+  def run(queue)
+    loop do
+      begin
+        @pipe = IO.popen(@command, mode="r")
+        hostname = Socket.gethostname
+
+        @pipe.each do |line|
+          line = line.chomp
+          source = "pipe://#{hostname}/#{@command}"
+          @logger.debug? && @logger.debug("Received line", :command => @command, :line => line)
+          @codec.decode(line) do |event|
+            event["host"] = hostname
+            event["command"] = @command
+            decorate(event)
+            queue << event
+          end
+        end
+      rescue LogStash::ShutdownSignal => e
+        break
+      rescue Exception => e
+        @logger.error("Exception while running command", :e => e, :backtrace => e.backtrace)
+      end
+
+      # Keep running the command forever.
+      sleep(10)
+    end
+  end # def run
+end # class LogStash::Inputs::Pipe
diff --git a/lib/logstash/inputs/rabbitmq.rb b/lib/logstash/inputs/rabbitmq.rb
new file mode 100644
index 00000000000..41924738874
--- /dev/null
+++ b/lib/logstash/inputs/rabbitmq.rb
@@ -0,0 +1,128 @@
+# encoding: utf-8
+require "logstash/inputs/threadable"
+require "logstash/namespace"
+
+# Pull events from a RabbitMQ exchange.
+#
+# The default settings will create an entirely transient queue and listen for all messages by default.
+# If you need durability or any other advanced settings, please set the appropriate options
+#
+# This has been tested with Bunny 0.9.x, which supports RabbitMQ 2.x and 3.x. You can
+# find links to both here:
+#
+# * RabbitMQ - <http://www.rabbitmq.com/>
+# * March Hare: <http://rubymarchhare.info>
+# * Bunny - <https://github.com/ruby-amqp/bunny>
+class LogStash::Inputs::RabbitMQ < LogStash::Inputs::Threadable
+
+  config_name "rabbitmq"
+  milestone 1
+
+  #
+  # Connection
+  #
+
+  # RabbitMQ server address
+  config :host, :validate => :string, :required => true
+
+  # RabbitMQ port to connect on
+  config :port, :validate => :number, :default => 5672
+
+  # RabbitMQ username
+  config :user, :validate => :string, :default => "guest"
+
+  # RabbitMQ password
+  config :password, :validate => :password, :default => "guest"
+
+  # The vhost to use. If you don't know what this is, leave the default.
+  config :vhost, :validate => :string, :default => "/"
+
+  # Enable or disable SSL
+  config :ssl, :validate => :boolean, :default => false
+
+  # Validate SSL certificate
+  config :verify_ssl, :validate => :boolean, :default => false
+
+  # Enable or disable logging
+  config :debug, :validate => :boolean, :default => false, :deprecated => "Use the logstash --debug flag for this instead."
+
+
+
+  #
+  # Queue & Consumer
+  #
+
+  # The name of the queue Logstash will consume events from.
+  config :queue, :validate => :string, :default => ""
+
+  # Is this queue durable? (aka; Should it survive a broker restart?)
+  config :durable, :validate => :boolean, :default => false
+
+  # Should the queue be deleted on the broker when the last consumer
+  # disconnects? Set this option to 'false' if you want the queue to remain
+  # on the broker, queueing up messages until a consumer comes along to
+  # consume them.
+  config :auto_delete, :validate => :boolean, :default => false
+
+  # Is the queue exclusive? Exclusive queues can only be used by the connection
+  # that declared them and will be deleted when it is closed (e.g. due to a Logstash
+  # restart).
+  config :exclusive, :validate => :boolean, :default => false
+
+  # Extra queue arguments as an array.
+  # To make a RabbitMQ queue mirrored, use: {"x-ha-policy" => "all"}
+  config :arguments, :validate => :array, :default => {}
+
+  # Prefetch count. Number of messages to prefetch
+  config :prefetch_count, :validate => :number, :default => 256
+
+  # Enable message acknowledgement
+  config :ack, :validate => :boolean, :default => true
+
+  # Passive queue creation? Useful for checking queue existance without modifying server state
+  config :passive, :validate => :boolean, :default => false
+
+
+
+  #
+  # (Optional) Exchange binding
+  #
+
+  # Optional.
+  #
+  # The name of the exchange to bind the queue to.
+  config :exchange, :validate => :string
+
+  # Optional.
+  #
+  # The routing key to use when binding a queue to the exchange.
+  # This is only relevant for direct or topic exchanges.
+  #
+  # * Routing keys are ignored on fanout exchanges.
+  # * Wildcards are not valid on direct exchanges.
+  config :key, :validate => :string, :default => "logstash"
+
+
+  def initialize(params)
+    params["codec"] = "json" if !params["codec"]
+
+    super
+  end
+
+  # Use March Hare on JRuby to avoid IO#select CPU spikes
+  # (see github.com/ruby-amqp/bunny/issues/95).
+  #
+  # On MRI, use Bunny.
+  #
+  # See http://rubybunny.info and http://rubymarchhare.info
+  # for the docs.
+  if RUBY_ENGINE == "jruby"
+    require "logstash/inputs/rabbitmq/march_hare"
+
+    include MarchHareImpl
+  else
+    require "logstash/inputs/rabbitmq/bunny"
+
+    include BunnyImpl
+  end
+end # class LogStash::Inputs::RabbitMQ
diff --git a/lib/logstash/inputs/rabbitmq/bunny.rb b/lib/logstash/inputs/rabbitmq/bunny.rb
new file mode 100644
index 00000000000..c933f7a4af8
--- /dev/null
+++ b/lib/logstash/inputs/rabbitmq/bunny.rb
@@ -0,0 +1,119 @@
+# encoding: utf-8
+class LogStash::Inputs::RabbitMQ
+  module BunnyImpl
+    def register
+      require "bunny"
+
+      @vhost       ||= Bunny::DEFAULT_HOST
+      # 5672. Will be switched to 5671 by Bunny if TLS is enabled.
+      @port        ||= AMQ::Protocol::DEFAULT_PORT
+      @routing_key ||= "#"
+
+      @settings = {
+        :vhost => @vhost,
+        :host  => @host,
+        :port  => @port,
+        :automatically_recover => false
+      }
+      @settings[:user]      = @user || Bunny::DEFAULT_USER
+      @settings[:pass]      = if @password
+                                @password.value
+                              else
+                                Bunny::DEFAULT_PASSWORD
+                              end
+
+      @settings[:log_level] = if @debug || @logger.debug?
+                                :debug
+                              else
+                                :error
+                              end
+
+      @settings[:tls]        = @ssl if @ssl
+      @settings[:verify_ssl] = @verify_ssl if @verify_ssl
+
+      proto                  = if @ssl
+                                 "amqps"
+                               else
+                                 "amqp"
+                               end
+      @connection_url        = "#{proto}://#{@user}@#{@host}:#{@port}#{vhost}/#{@queue}"
+
+      @logger.info("Registering input #{@connection_url}")
+    end
+
+    def run(output_queue)
+      @output_queue = output_queue
+
+      begin
+        setup
+        consume
+      rescue Bunny::NetworkFailure, Bunny::ConnectionClosedError, Bunny::ConnectionLevelException, Bunny::TCPConnectionFailed => e
+        n = Bunny::Session::DEFAULT_NETWORK_RECOVERY_INTERVAL * 2
+
+        # Because we manually reconnect instead of letting Bunny
+        # handle failures,
+        # make sure we don't leave any consumer work pool
+        # threads behind. MK.
+        @ch.maybe_kill_consumer_work_pool!
+        @logger.error("RabbitMQ connection error: #{e.message}. Will attempt to reconnect in #{n} seconds...")
+
+        sleep n
+        retry
+      end
+    end
+
+    def teardown
+      @consumer.cancel
+      @q.delete unless @durable
+
+      @ch.close   if @ch && @ch.open?
+      @conn.close if @conn && @conn.open?
+
+      finished
+    end
+
+    def setup
+      @conn = Bunny.new(@settings)
+
+      @logger.debug("Connecting to RabbitMQ. Settings: #{@settings.inspect}, queue: #{@queue.inspect}")
+      return if terminating?
+      @conn.start
+
+      @ch = @conn.create_channel.tap do |ch|
+        ch.prefetch(@prefetch_count)
+      end
+      @logger.info("Connected to RabbitMQ at #{@settings[:host]}")
+
+      @arguments_hash = Hash[*@arguments]
+
+      @q = @ch.queue(@queue,
+                     :durable     => @durable,
+                     :auto_delete => @auto_delete,
+                     :exclusive   => @exclusive,
+                     :passive     => @passive,
+                     :arguments   => @arguments)
+
+      # exchange binding is optional for the input
+      if @exchange
+        @q.bind(@exchange, :routing_key => @key)
+      end
+    end
+
+    def consume
+      @logger.info("Will consume events from queue #{@q.name}")
+
+      # we both need to block the caller in Bunny::Queue#subscribe and have
+      # a reference to the consumer so that we can cancel it, so
+      # a consumer manually. MK.
+      @consumer = Bunny::Consumer.new(@ch, @q)
+      @q.subscribe(:manual_ack => @ack, :block => true) do |delivery_info, properties, data|
+        @codec.decode(data) do |event|
+          decorate(event)
+          @output_queue << event
+        end
+
+        @ch.acknowledge(delivery_info.delivery_tag) if @ack
+      end
+    end
+  end # BunnyImpl
+end
diff --git a/lib/logstash/inputs/rabbitmq/hot_bunnies.rb b/lib/logstash/inputs/rabbitmq/hot_bunnies.rb
new file mode 100644
index 00000000000..bc64df1c73c
--- /dev/null
+++ b/lib/logstash/inputs/rabbitmq/hot_bunnies.rb
@@ -0,0 +1 @@
+require "logstash/inputs/rabbitmq/march_hare"
diff --git a/lib/logstash/inputs/rabbitmq/march_hare.rb b/lib/logstash/inputs/rabbitmq/march_hare.rb
new file mode 100644
index 00000000000..d2f0f0bc59f
--- /dev/null
+++ b/lib/logstash/inputs/rabbitmq/march_hare.rb
@@ -0,0 +1,130 @@
+# encoding: utf-8
+class LogStash::Inputs::RabbitMQ
+  # MarchHare-based implementation for JRuby
+  module MarchHareImpl
+    def register
+      require "hot_bunnies"
+      require "java"
+
+      @vhost       ||= "127.0.0.1"
+      # 5672. Will be switched to 5671 by Bunny if TLS is enabled.
+      @port        ||= 5672
+      @key         ||= "#"
+
+      @settings = {
+        :vhost => @vhost,
+        :host  => @host,
+        :port  => @port,
+        :user  => @user,
+        :automatic_recovery => false
+      }
+      @settings[:pass]      = @password.value if @password
+      @settings[:tls]       = @ssl if @ssl
+
+      proto                 = if @ssl
+                                "amqps"
+                              else
+                                "amqp"
+                              end
+      @connection_url       = "#{proto}://#{@user}@#{@host}:#{@port}#{vhost}/#{@queue}"
+
+      @logger.info("Registering input #{@connection_url}")
+    end
+
+    def run(output_queue)
+      @output_queue          = output_queue
+      @break_out_of_the_loop = java.util.concurrent.atomic.AtomicBoolean.new(false)
+
+      # MarchHare does not raise exceptions when connection goes down with a blocking
+      # consumer running (it uses callbacks, as the RabbitMQ Java client does).
+      #
+      # However, MarchHare::Channel will make sure to unblock all blocking consumers
+      # on any internal shutdown, so #consume will return and another loop iteration
+      # will run.
+      #
+      # This is very similar to how the Bunny implementation works and is sufficient
+      # for our needs: it recovers successfully after RabbitMQ is kill -9ed, the
+      # network device is shut down, etc. MK.
+      until @break_out_of_the_loop.get do
+        begin
+          setup
+          consume
+        rescue MarchHare::Exception, java.lang.Throwable, com.rabbitmq.client.AlreadyClosedException => e
+          n = 10
+          @logger.error("RabbitMQ connection error: #{e}. Will reconnect in #{n} seconds...")
+
+          sleep n
+          retry
+        rescue LogStash::ShutdownSignal => ss
+          shutdown_consumer
+        end
+
+        n = 10
+        @logger.error("RabbitMQ connection error: #{e}. Will reconnect in #{n} seconds...")
+      end
+    end
+
+    def teardown
+      shutdown_consumer
+      @q.delete unless @durable
+
+      @ch.close         if @ch && @ch.open?
+      @connection.close if @connection && @connection.open?
+
+      finished
+    end
+
+    #
+    # Implementation
+    #
+
+    protected
+
+    def setup
+      return if terminating?
+
+      @conn = MarchHare.connect(@settings)
+      @logger.info("Connected to RabbitMQ #{@connection_url}")
+
+      @ch          = @conn.create_channel.tap do |ch|
+        ch.prefetch = @prefetch_count
+      end
+
+      @arguments_hash = Hash[*@arguments]
+
+      @q = @ch.queue(@queue,
+        :durable     => @durable,
+        :auto_delete => @auto_delete,
+        :exclusive   => @exclusive,
+        :passive     => @passive,
+        :arguments   => @arguments)
+
+      # exchange binding is optional for the input
+      if @exchange
+        @q.bind(@exchange, :routing_key => @key)
+      end
+    end
+
+    def consume
+      return if terminating?
+
+      # we manually build a consumer here to be able to keep a reference to it
+      # in an @ivar even though we use a blocking version of HB::Queue#subscribe
+      @consumer = @q.build_consumer(:block => true) do |metadata, data|
+        @codec.decode(data) do |event|
+          decorate(event)
+          @output_queue << event if event
+        end
+        @ch.ack(metadata.delivery_tag) if @ack
+      end
+      @q.subscribe_with(@consumer, :manual_ack => @ack, :block => true)
+    end
+
+    def shutdown_consumer
+      @break_out_of_the_loop.set(true)
+
+      @consumer.cancel
+      @consumer.gracefully_shut_down
+    end
+  end # MarchHareImpl
+end
diff --git a/lib/logstash/inputs/redis.rb b/lib/logstash/inputs/redis.rb
new file mode 100644
index 00000000000..8884f55d91c
--- /dev/null
+++ b/lib/logstash/inputs/redis.rb
@@ -0,0 +1,266 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/inputs/threadable"
+require "logstash/namespace"
+
+# This input will read events from a Redis instance; it supports both Redis channels and lists.
+# The list command (BLPOP) used by Logstash is supported in Redis v1.3.1+, and
+# the channel commands used by Logstash are found in Redis v1.3.8+. 
+# While you may be able to make these Redis versions work, the best performance
+# and stability will be found in more recent stable versions.  Versions 2.6.0+
+# are recommended.
+#
+# For more information about Redis, see <http://redis.io/>
+#
+# `batch_count` note: If you use the `batch_count` setting, you *must* use a Redis version 2.6.0 or
+# newer. Anything older does not support the operations used by batching.
+#
+class LogStash::Inputs::Redis < LogStash::Inputs::Threadable
+  config_name "redis"
+  milestone 2
+
+  default :codec, "json"
+
+  # The `name` configuration is used for logging in case there are multiple instances.
+  # This feature has no real function and will be removed in future versions.
+  config :name, :validate => :string, :default => "default", :deprecated => true
+
+  # The hostname of your Redis server.
+  config :host, :validate => :string, :default => "127.0.0.1"
+
+  # The port to connect on.
+  config :port, :validate => :number, :default => 6379
+
+  # The Redis database number.
+  config :db, :validate => :number, :default => 0
+
+  # Initial connection timeout in seconds.
+  config :timeout, :validate => :number, :default => 5
+
+  # Password to authenticate with. There is no authentication by default.
+  config :password, :validate => :password
+
+  # The name of the Redis queue (we'll use BLPOP against this).
+  # TODO: remove soon.
+  config :queue, :validate => :string, :deprecated => true
+
+  # The name of a Redis list or channel.
+  # TODO: change required to true
+  config :key, :validate => :string, :required => false
+
+  # Specify either list or channel.  If `redis\_type` is `list`, then we will BLPOP the
+  # key.  If `redis\_type` is `channel`, then we will SUBSCRIBE to the key.
+  # If `redis\_type` is `pattern_channel`, then we will PSUBSCRIBE to the key.
+  # TODO: change required to true
+  config :data_type, :validate => [ "list", "channel", "pattern_channel" ], :required => false
+
+  # The number of events to return from Redis using EVAL.
+  config :batch_count, :validate => :number, :default => 1
+
+  public
+  def register
+    require 'redis'
+    @redis = nil
+    @redis_url = "redis://#{@password}@#{@host}:#{@port}/#{@db}"
+
+    # TODO remove after setting key and data_type to true
+    if @queue
+      if @key or @data_type
+        raise RuntimeError.new(
+          "Cannot specify queue parameter and key or data_type"
+        )
+      end
+      @key = @queue
+      @data_type = 'list'
+    end
+
+    if not @key or not @data_type
+      raise RuntimeError.new(
+        "Must define queue, or key and data_type parameters"
+      )
+    end
+    # end TODO
+
+    @logger.info("Registering Redis", :identity => identity)
+  end # def register
+
+  # A string used to identify a Redis instance in log messages
+  # TODO(sissel): Use instance variables for this once the @name config
+  # option is removed.
+  private
+  def identity
+    @name || "#{@redis_url} #{@data_type}:#{@key}"
+  end
+
+  private
+  def connect
+    redis = Redis.new(
+      :host => @host,
+      :port => @port,
+      :timeout => @timeout,
+      :db => @db,
+      :password => @password.nil? ? nil : @password.value
+    )
+    load_batch_script(redis) if @data_type == 'list' && (@batch_count > 1)
+    return redis
+  end # def connect
+
+  private
+  def load_batch_script(redis)
+    #A Redis Lua EVAL script to fetch a count of keys
+    #in case count is bigger than current items in queue whole queue will be returned without extra nil values
+    redis_script = <<EOF
+          local i = tonumber(ARGV[1])
+          local res = {}
+          local length = redis.call('llen',KEYS[1])
+          if length < i then i = length end
+          while (i > 0) do
+            local item = redis.call("lpop", KEYS[1])
+            if (not item) then
+              break
+            end
+            table.insert(res, item)
+            i = i-1
+          end
+          return res
+EOF
+    @redis_script_sha = redis.script(:load, redis_script)
+  end
+
+  private
+  def queue_event(msg, output_queue)
+    begin
+      @codec.decode(msg) do |event|
+        decorate(event)
+        output_queue << event
+      end
+    rescue => e # parse or event creation error
+      @logger.error("Failed to create event", :message => msg, :exception => e,
+                    :backtrace => e.backtrace);
+    end
+  end
+
+  private
+  def list_listener(redis, output_queue)
+
+    # blpop returns the 'key' read from as well as the item result
+    # we only care about the result (2nd item in the list).
+    item = redis.blpop(@key, 0)[1]
+
+    # blpop failed or .. something?
+    # TODO(sissel): handle the error
+    return if item.nil?
+    queue_event(item, output_queue)
+
+    # If @batch_count is 1, there's no need to continue.
+    return if @batch_count == 1
+
+    begin
+      redis.evalsha(@redis_script_sha, [@key], [@batch_count-1]).each do |item|
+        queue_event(item, output_queue)
+      end
+
+      # Below is a commented-out implementation of 'batch fetch'
+      # using pipelined LPOP calls. This in practice has been observed to
+      # perform exactly the same in terms of event throughput as
+      # the evalsha method. Given that the EVALSHA implementation uses
+      # one call to Redis instead of N (where N == @batch_count) calls,
+      # I decided to go with the 'evalsha' method of fetching N items
+      # from Redis in bulk.
+      #redis.pipelined do
+        #error, item = redis.lpop(@key)
+        #(@batch_count-1).times { redis.lpop(@key) }
+      #end.each do |item|
+        #queue_event(item, output_queue) if item
+      #end
+      # --- End commented out implementation of 'batch fetch'
+    rescue Redis::CommandError => e
+      if e.to_s =~ /NOSCRIPT/ then
+        @logger.warn("Redis may have been restarted, reloading Redis batch EVAL script", :exception => e);
+        load_batch_script(redis)
+        retry
+      else
+        raise e
+      end
+    end
+  end
+
+  private
+  def channel_listener(redis, output_queue)
+    redis.subscribe @key do |on|
+      on.subscribe do |channel, count|
+        @logger.info("Subscribed", :channel => channel, :count => count)
+      end
+
+      on.message do |channel, message|
+        queue_event message, output_queue
+      end
+
+      on.unsubscribe do |channel, count|
+        @logger.info("Unsubscribed", :channel => channel, :count => count)
+      end
+    end
+  end
+
+  private
+  def pattern_channel_listener(redis, output_queue)
+    redis.psubscribe @key do |on|
+      on.psubscribe do |channel, count|
+        @logger.info("Subscribed", :channel => channel, :count => count)
+      end
+
+      on.pmessage do |ch, event, message|
+        queue_event message, output_queue
+      end
+
+      on.punsubscribe do |channel, count|
+        @logger.info("Unsubscribed", :channel => channel, :count => count)
+      end
+    end
+  end
+
+  # Since both listeners have the same basic loop, we've abstracted the outer
+  # loop.
+  private
+  def listener_loop(listener, output_queue)
+    while !finished?
+      begin
+        @redis ||= connect
+        self.send listener, @redis, output_queue
+      rescue Redis::CannotConnectError => e
+        @logger.warn("Redis connection problem", :exception => e)
+        sleep 1
+        @redis = connect
+      rescue => e # Redis error
+        @logger.warn("Failed to get event from Redis", :name => @name,
+                     :exception => e, :backtrace => e.backtrace)
+        raise e
+      end
+    end # while !finished?
+  end # listener_loop
+
+  public
+  def run(output_queue)
+    if @data_type == 'list'
+      listener_loop :list_listener, output_queue
+    elsif @data_type == 'channel'
+      listener_loop :channel_listener, output_queue
+    else
+      listener_loop :pattern_channel_listener, output_queue
+    end
+  end # def run
+
+  public
+  def teardown
+    if @data_type == 'channel' and @redis
+      @redis.unsubscribe
+      @redis.quit
+      @redis = nil
+    end
+    if @data_type == 'pattern_channel' and @redis
+      @redis.punsubscribe
+      @redis.quit
+      @redis = nil
+    end
+  end
+end # class LogStash::Inputs::Redis
diff --git a/lib/logstash/inputs/s3.rb b/lib/logstash/inputs/s3.rb
new file mode 100644
index 00000000000..403aaebd349
--- /dev/null
+++ b/lib/logstash/inputs/s3.rb
@@ -0,0 +1,278 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+
+require "time"
+require "tmpdir"
+
+# Stream events from files from a S3 bucket.
+#
+# Each line from each file generates an event.
+# Files ending in '.gz' are handled as gzip'ed files.
+class LogStash::Inputs::S3 < LogStash::Inputs::Base
+  config_name "s3"
+  milestone 1
+
+  # TODO(sissel): refactor to use 'line' codec (requires removing both gzip
+  # support and readline usage). Support gzip through a gzip codec! ;)
+  default :codec, "plain"
+
+  # The credentials of the AWS account used to access the bucket.
+  # Credentials can be specified:
+  # - As an ["id","secret"] array
+  # - As a path to a file containing AWS_ACCESS_KEY_ID=... and AWS_SECRET_ACCESS_KEY=...
+  # - In the environment, if not set (using variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY)
+  config :credentials, :validate => :array, :default => []
+
+  # The name of the S3 bucket.
+  config :bucket, :validate => :string, :required => true
+
+  # The AWS region for your bucket.
+  config :region, :validate => ["us-east-1", "us-west-1", "us-west-2",
+                                "eu-west-1", "ap-southeast-1", "ap-southeast-2",
+                                "ap-northeast-1", "sa-east-1", "us-gov-west-1"],
+                                :deprecated => "'region' has been deprecated in favor of 'region_endpoint'"
+
+  # The AWS region for your bucket.
+  config :region_endpoint, :validate => ["us-east-1", "us-west-1", "us-west-2",
+                                "eu-west-1", "ap-southeast-1", "ap-southeast-2",
+                                "ap-northeast-1", "sa-east-1", "us-gov-west-1"], :default => "us-east-1"
+
+  # If specified, the prefix the filenames in the bucket must match (not a regexp)
+  config :prefix, :validate => :string, :default => nil
+
+  # Where to write the since database (keeps track of the date
+  # the last handled file was added to S3). The default will write
+  # sincedb files to some path matching "$HOME/.sincedb*"
+  config :sincedb_path, :validate => :string, :default => nil
+
+  # Name of a S3 bucket to backup processed files to.
+  config :backup_to_bucket, :validate => :string, :default => nil
+
+  # Path of a local directory to backup processed files to.
+  config :backup_to_dir, :validate => :string, :default => nil
+
+  # Whether to delete processed files from the original bucket.
+  config :delete, :validate => :boolean, :default => false
+
+  # Interval to wait between to check the file list again after a run is finished.
+  # Value is in seconds.
+  config :interval, :validate => :number, :default => 60
+
+  public
+  def register
+    require "digest/md5"
+    require "aws-sdk"
+
+    @region_endpoint = @region if @region && !@region.empty?
+
+    @logger.info("Registering s3 input", :bucket => @bucket, :region_endpoint => @region_endpoint)
+
+    if @credentials.length == 0
+      @access_key_id = ENV['AWS_ACCESS_KEY_ID']
+      @secret_access_key = ENV['AWS_SECRET_ACCESS_KEY']
+    elsif @credentials.length == 1
+      File.open(@credentials[0]) { |f| f.each do |line|
+        unless (/^\#/.match(line))
+          if(/\s*=\s*/.match(line))
+            param, value = line.split('=', 2)
+            param = param.chomp().strip()
+            value = value.chomp().strip()
+            if param.eql?('AWS_ACCESS_KEY_ID')
+              @access_key_id = value
+            elsif param.eql?('AWS_SECRET_ACCESS_KEY')
+              @secret_access_key = value
+            end
+          end
+        end
+      end
+      }
+    elsif @credentials.length == 2
+      @access_key_id = @credentials[0]
+      @secret_access_key = @credentials[1]
+    else
+      raise ArgumentError.new('Credentials must be of the form "/path/to/file" or ["id", "secret"]')
+    end
+
+    if @access_key_id.nil? or @secret_access_key.nil?
+      raise ArgumentError.new('Missing AWS credentials')
+    end
+
+    if @bucket.nil?
+      raise ArgumentError.new('Missing AWS bucket')
+    end
+
+    if @sincedb_path.nil?
+      if ENV['HOME'].nil?
+        raise ArgumentError.new('No HOME or sincedb_path set')
+      end
+      @sincedb_path = File.join(ENV["HOME"], ".sincedb_" + Digest::MD5.hexdigest("#{@bucket}+#{@prefix}"))
+    end
+
+    s3 = AWS::S3.new(
+      :access_key_id => @access_key_id,
+      :secret_access_key => @secret_access_key,
+      :region => @region_endpoint
+    )
+
+    @s3bucket = s3.buckets[@bucket]
+
+    unless @backup_to_bucket.nil?
+      @backup_bucket = s3.buckets[@backup_to_bucket]
+      unless @backup_bucket.exists?
+        s3.buckets.create(@backup_to_bucket)
+      end
+    end
+
+    unless @backup_to_dir.nil?
+      Dir.mkdir(@backup_to_dir, 0700) unless File.exists?(@backup_to_dir)
+    end
+
+  end # def register
+
+  public
+  def run(queue)
+    loop do
+      process_new(queue)
+      sleep(@interval)
+    end
+    finished
+  end # def run
+
+  private
+  def process_new(queue, since=nil)
+
+    if since.nil?
+        since = sincedb_read()
+    end
+
+    objects = list_new(since)
+    objects.each do |k|
+      @logger.debug("S3 input processing", :bucket => @bucket, :key => k)
+      lastmod = @s3bucket.objects[k].last_modified
+      process_log(queue, k)
+      sincedb_write(lastmod)
+    end
+
+  end # def process_new
+
+  private
+  def list_new(since=nil)
+
+    if since.nil?
+      since = Time.new(0)
+    end
+
+    objects = {}
+    @s3bucket.objects.with_prefix(@prefix).each do |log|
+      if log.last_modified > since
+        objects[log.key] = log.last_modified
+      end
+    end
+
+    return sorted_objects = objects.keys.sort {|a,b| objects[a] <=> objects[b]}
+
+  end # def list_new
+
+  private
+  def process_log(queue, key)
+
+    object = @s3bucket.objects[key]
+    tmp = Dir.mktmpdir("logstash-")
+    begin
+      filename = File.join(tmp, File.basename(key))
+      File.open(filename, 'wb') do |s3file|
+        object.read do |chunk|
+          s3file.write(chunk)
+        end
+      end
+      process_local_log(queue, filename)
+      unless @backup_to_bucket.nil?
+        backup_object = @backup_bucket.objects[key]
+        backup_object.write(Pathname.new(filename))
+      end
+      unless @backup_to_dir.nil?
+        FileUtils.cp(filename, @backup_to_dir)
+      end
+      if @delete
+        object.delete()
+      end
+    end
+    FileUtils.remove_entry_secure(tmp, force=true)
+
+  end # def process_log
+
+  private
+  def process_local_log(queue, filename)
+
+    metadata = {
+      :version => nil,
+      :format => nil,
+    }
+    File.open(filename) do |file|
+      if filename.end_with?('.gz')
+        gz = Zlib::GzipReader.new(file)
+        gz.each_line do |line|
+          metadata = process_line(queue, metadata, line)
+        end
+      else
+        file.each do |line|
+          metadata = process_line(queue, metadata, line)
+        end
+      end
+    end
+
+  end # def process_local_log
+
+  private
+  def process_line(queue, metadata, line)
+
+    if /#Version: .+/.match(line)
+      junk, version = line.strip().split(/#Version: (.+)/)
+      unless version.nil?
+        metadata[:version] = version
+      end
+    elsif /#Fields: .+/.match(line)
+      junk, format = line.strip().split(/#Fields: (.+)/)
+      unless format.nil?
+        metadata[:format] = format
+      end
+    else
+      @codec.decode(line) do |event|
+        decorate(event)
+        unless metadata[:version].nil?
+          event["cloudfront_version"] = metadata[:version]
+        end
+        unless metadata[:format].nil?
+          event["cloudfront_fields"] = metadata[:format]
+        end
+        queue << event
+      end
+    end
+    return metadata
+
+  end # def process_line
+
+  private
+  def sincedb_read()
+
+    if File.exists?(@sincedb_path)
+      since = Time.parse(File.read(@sincedb_path).chomp.strip)
+    else
+      since = Time.new(0)
+    end
+    return since
+
+  end # def sincedb_read
+
+  private
+  def sincedb_write(since=nil)
+
+    if since.nil?
+      since = Time.now()
+    end
+    File.open(@sincedb_path, 'w') { |file| file.write(since.to_s) }
+
+  end # def sincedb_write
+
+end # class LogStash::Inputs::S3
diff --git a/lib/logstash/inputs/snmptrap.rb b/lib/logstash/inputs/snmptrap.rb
new file mode 100644
index 00000000000..63699f1eed4
--- /dev/null
+++ b/lib/logstash/inputs/snmptrap.rb
@@ -0,0 +1,87 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+
+# Read snmp trap messages as events
+#
+# Resulting @message looks like :
+#   #<SNMP::SNMPv1_Trap:0x6f1a7a4 @varbind_list=[#<SNMP::VarBind:0x2d7bcd8f @value="teststring", 
+#   @name=[1.11.12.13.14.15]>], @timestamp=#<SNMP::TimeTicks:0x1af47e9d @value=55>, @generic_trap=6, 
+#   @enterprise=[1.2.3.4.5.6], @source_ip="127.0.0.1", @agent_addr=#<SNMP::IpAddress:0x29a4833e @value="\xC0\xC1\xC2\xC3">, 
+#   @specific_trap=99>
+#
+
+class LogStash::Inputs::Snmptrap < LogStash::Inputs::Base
+  config_name "snmptrap"
+  milestone 1
+
+  # The address to listen on
+  config :host, :validate => :string, :default => "0.0.0.0"
+
+  # The port to listen on. Remember that ports less than 1024 (privileged
+  # ports) may require root to use. hence the default of 1062.
+  config :port, :validate => :number, :default => 1062
+
+  # SNMP Community String to listen for.
+  config :community, :validate => :string, :default => "public"
+
+  # directory of YAML MIB maps  (same format ruby-snmp uses)
+  config :yamlmibdir, :validate => :string
+
+  def initialize(*args)
+    super(*args)
+  end # def initialize
+
+  public
+  def register
+    require "snmp"
+    @snmptrap = nil
+    if @yamlmibdir
+      @logger.info("checking #{@yamlmibdir} for MIBs")
+      Dir["#{@yamlmibdir}/*.yaml"].each do |yamlfile|
+        mib_name = File.basename(yamlfile, ".*")
+        @yaml_mibs ||= []
+        @yaml_mibs << mib_name
+      end
+      @logger.info("found MIBs: #{@yaml_mibs.join(',')}") if @yaml_mibs
+    end
+  end # def register
+
+  public
+  def run(output_queue)
+    begin
+      # snmp trap server
+      snmptrap_listener(output_queue)
+    rescue => e
+      @logger.warn("SNMP Trap listener died", :exception => e, :backtrace => e.backtrace)
+      sleep(5)
+      retry
+    end # begin
+  end # def run
+
+  private
+  def snmptrap_listener(output_queue)
+    traplistener_opts = {:Port => @port, :Community => @community, :Host => @host}
+    if @yaml_mibs && !@yaml_mibs.empty?
+      traplistener_opts.merge!({:MibDir => @yamlmibdir, :MibModules => @yaml_mibs})
+    end
+    @logger.info("It's a Trap!", traplistener_opts.dup)
+    @snmptrap = SNMP::TrapListener.new(traplistener_opts)
+
+    @snmptrap.on_trap_default do |trap|
+      begin
+        event = LogStash::Event.new("message" => trap.inspect, "host" => trap.source_ip)
+        decorate(event)
+        trap.each_varbind do |vb|
+          event[vb.name.to_s] = vb.value.to_s
+        end
+        @logger.debug("SNMP Trap received: ", :trap_object => trap.inspect)
+        output_queue << event
+      rescue => event
+        @logger.error("Failed to create event", :trap_object => trap.inspect)
+      end
+    end
+    @snmptrap.join
+  end # def snmptrap_listener
+
+end # class LogStash::Inputs::Snmptrap
diff --git a/lib/logstash/inputs/sqs.rb b/lib/logstash/inputs/sqs.rb
new file mode 100644
index 00000000000..8b0599aa6ef
--- /dev/null
+++ b/lib/logstash/inputs/sqs.rb
@@ -0,0 +1,173 @@
+# encoding: utf-8
+require "logstash/inputs/threadable"
+require "logstash/namespace"
+require "logstash/timestamp"
+require "logstash/plugin_mixins/aws_config"
+require "digest/sha2"
+
+# Pull events from an Amazon Web Services Simple Queue Service (SQS) queue.
+#
+# SQS is a simple, scalable queue system that is part of the
+# Amazon Web Services suite of tools.
+#
+# Although SQS is similar to other queuing systems like AMQP, it
+# uses a custom API and requires that you have an AWS account.
+# See http://aws.amazon.com/sqs/ for more details on how SQS works,
+# what the pricing schedule looks like and how to setup a queue.
+#
+# To use this plugin, you *must*:
+#
+#  * Have an AWS account
+#  * Setup an SQS queue
+#  * Create an identify that has access to consume messages from the queue.
+#
+# The "consumer" identity must have the following permissions on the queue:
+#
+#  * sqs:ChangeMessageVisibility
+#  * sqs:ChangeMessageVisibilityBatch
+#  * sqs:DeleteMessage
+#  * sqs:DeleteMessageBatch
+#  * sqs:GetQueueAttributes
+#  * sqs:GetQueueUrl
+#  * sqs:ListQueues
+#  * sqs:ReceiveMessage
+#
+# Typically, you should setup an IAM policy, create a user and apply the IAM policy to the user.
+# A sample policy is as follows:
+#
+#     {
+#       "Statement": [
+#         {
+#           "Action": [
+#             "sqs:ChangeMessageVisibility",
+#             "sqs:ChangeMessageVisibilityBatch",
+#             "sqs:GetQueueAttributes",
+#             "sqs:GetQueueUrl",
+#             "sqs:ListQueues",
+#             "sqs:SendMessage",
+#             "sqs:SendMessageBatch"
+#           ],
+#           "Effect": "Allow",
+#           "Resource": [
+#             "arn:aws:sqs:us-east-1:123456789012:Logstash"
+#           ]
+#         }
+#       ]
+#     }
+#
+# See http://aws.amazon.com/iam/ for more details on setting up AWS identities.
+#
+class LogStash::Inputs::SQS < LogStash::Inputs::Threadable
+  include LogStash::PluginMixins::AwsConfig
+
+  config_name "sqs"
+  milestone 1
+
+  default :codec, "json"
+
+  # Name of the SQS Queue name to pull messages from. Note that this is just the name of the queue, not the URL or ARN.
+  config :queue, :validate => :string, :required => true
+
+  # Name of the event field in which to store the SQS message ID
+  config :id_field, :validate => :string
+
+  # Name of the event field in which to store the SQS message MD5 checksum
+  config :md5_field, :validate => :string
+
+  # Name of the event field in which to store the  SQS message Sent Timestamp
+  config :sent_timestamp_field, :validate => :string
+
+  public
+  def aws_service_endpoint(region)
+    return {
+        :sqs_endpoint => "sqs.#{region}.amazonaws.com"
+    }
+  end
+
+  public
+  def register
+    @logger.info("Registering SQS input", :queue => @queue)
+    require "aws-sdk"
+
+    @sqs = AWS::SQS.new(aws_options_hash)
+
+    begin
+      @logger.debug("Connecting to AWS SQS queue", :queue => @queue)
+      @sqs_queue = @sqs.queues.named(@queue)
+      @logger.info("Connected to AWS SQS queue successfully.", :queue => @queue)
+    rescue Exception => e
+      @logger.error("Unable to access SQS queue.", :error => e.to_s, :queue => @queue)
+      throw e
+    end # begin/rescue
+  end # def register
+
+  public
+  def run(output_queue)
+    @logger.debug("Polling SQS queue", :queue => @queue)
+
+    receive_opts = {
+        :limit => 10,
+        :visibility_timeout => 30,
+        :attributes => [:sent_at]
+    }
+
+    continue_polling = true
+    while running? && continue_polling
+      continue_polling = run_with_backoff(60, 1) do
+        @sqs_queue.receive_message(receive_opts) do |message|
+          if message
+            @codec.decode(message.body) do |event|
+              decorate(event)
+              if @id_field
+                event[@id_field] = message.id
+              end
+              if @md5_field
+                event[@md5_field] = message.md5
+              end
+              if @sent_timestamp_field
+                event[@sent_timestamp_field] = LogStash::Timestamp.new(message.sent_timestamp).utc
+              end
+              @logger.debug? && @logger.debug("Processed SQS message", :message_id => message.id, :message_md5 => message.md5, :sent_timestamp => message.sent_timestamp, :queue => @queue)
+              output_queue << event
+              message.delete
+            end # codec.decode
+          end # valid SQS message
+        end # receive_message
+      end # run_with_backoff
+    end # polling loop
+  end # def run
+
+  def teardown
+    @sqs_queue = nil
+    finished
+  end # def teardown
+
+  private
+  # Runs an AWS request inside a Ruby block with an exponential backoff in case
+  # we exceed the allowed AWS RequestLimit.
+  #
+  # @param [Integer] max_time maximum amount of time to sleep before giving up.
+  # @param [Integer] sleep_time the initial amount of time to sleep before retrying.
+  # @param [Block] block Ruby code block to execute.
+  def run_with_backoff(max_time, sleep_time, &block)
+    if sleep_time > max_time
+      @logger.error("AWS::EC2::Errors::RequestLimitExceeded ... failed.", :queue => @queue)
+      return false
+    end # retry limit exceeded
+
+    begin
+      block.call
+    rescue AWS::EC2::Errors::RequestLimitExceeded
+      @logger.info("AWS::EC2::Errors::RequestLimitExceeded ... retrying SQS request", :queue => @queue, :sleep_time => sleep_time)
+      sleep sleep_time
+      run_with_backoff(max_time, sleep_time * 2, &block)
+    rescue AWS::EC2::Errors::InstanceLimitExceeded
+      @logger.warn("AWS::EC2::Errors::InstanceLimitExceeded ... aborting SQS message retreival.")
+      return false
+    rescue Exception => bang
+      @logger.error("Error reading SQS queue.", :error => bang, :queue => @queue)
+      return false
+    end # begin/rescue
+    return true
+  end # def run_with_backoff
+end # class LogStash::Inputs::SQS
diff --git a/lib/logstash/inputs/stdin.rb b/lib/logstash/inputs/stdin.rb
index f6f958e7f00..210a2300f53 100644
--- a/lib/logstash/inputs/stdin.rb
+++ b/lib/logstash/inputs/stdin.rb
@@ -1,3 +1,4 @@
+# encoding: utf-8
 require "logstash/inputs/base"
 require "logstash/namespace"
 require "socket" # for Socket.gethostname
@@ -7,23 +8,40 @@
 # By default, each event is assumed to be one line. If you
 # want to join lines, you'll want to use the multiline filter.
 class LogStash::Inputs::Stdin < LogStash::Inputs::Base
-
   config_name "stdin"
+  milestone 3
+
+  default :codec, "line"
 
   public
   def register
     @host = Socket.gethostname
+    fix_streaming_codecs
   end # def register
 
-  def run(queue)
-    loop do
-      event = LogStash::Event.new
-      event.message = $stdin.readline.chomp
-      event.type = @type
-      event.tags = @tags.clone rescue []
-      event.source = "stdin://#{@host}/"
-      @logger.debug(["Got event", event])
-      queue << event
-    end # loop
+  def run(queue) 
+    while true
+      begin
+        # Based on some testing, there is no way to interrupt an IO.sysread nor
+        # IO.select call in JRuby. Bummer :(
+        data = $stdin.sysread(16384)
+        @codec.decode(data) do |event|
+          decorate(event)
+          event["host"] = @host if !event.include?("host")
+          queue << event
+        end
+      rescue IOError, EOFError, LogStash::ShutdownSignal
+        # stdin closed or a requested shutdown
+        break
+      end
+    end # while true
+    finished
   end # def run
+
+  public
+  def teardown
+    @logger.debug("stdin shutting down.")
+    $stdin.close rescue nil
+    finished
+  end # def teardown
 end # class LogStash::Inputs::Stdin
diff --git a/lib/logstash/inputs/stomp.rb b/lib/logstash/inputs/stomp.rb
deleted file mode 100644
index cc8734e0731..00000000000
--- a/lib/logstash/inputs/stomp.rb
+++ /dev/null
@@ -1,57 +0,0 @@
-require "logstash/inputs/base"
-require "logstash/namespace"
-
-# TODO(sissel): This class doesn't work yet in JRuby. Google for
-# 'execution expired stomp jruby' and you'll find the ticket.
-
-# Stream events from a STOMP broker.
-#
-# TODO(sissel): Include info on where to learn about STOMP
-class LogStash::Inputs::Stomp < LogStash::Inputs::Base
-  config_name "stomp"
-
-  # The address of the STOMP server.
-  config :host, :validate => :string
-
-  # The port to connet to on your STOMP server.
-  config :port, :validate => :number, :default => 61613
-
-  # The username to authenticate with.
-  config :user, :validate => :string, :default => ""
-
-  # The password to authenticate with.
-  config :password, :validate => :password, :default => ""
-
-  # The destination to read events from.
-  #
-  # Example: "/topic/logstash"
-  config :destination, :validate => :string
-
-  # Enable debugging output?
-  config :debug, :validate => :boolean, :default => false
-
-  public
-  def register
-    require "stomp"
-
-    if @destination == "" or @destination.nil?
-      @logger.error("No destination path given for stomp")
-      return
-    end
-
-    begin
-      @client = Stomp::Client.new(@user, @password.value, @host, @port)
-    rescue Errno::ECONNREFUSED
-      @logger.error("Connection refused to #{@host}:#{@port}...")
-      # TODO(sissel): Retry?
-    end
-  end # def register
-
-  def run(queue)
-    @client.subscribe(@destination) do |msg|
-      @logger.debug(["Got message from stomp", { :msg => msg }])
-      #event = LogStash::Event.from_json(message.body)
-      #queue << event
-    end
-  end # def run
-end # class LogStash::Inputs::Stomp
diff --git a/lib/logstash/inputs/syslog.rb b/lib/logstash/inputs/syslog.rb
index 8603fd0a137..b194fdad790 100644
--- a/lib/logstash/inputs/syslog.rb
+++ b/lib/logstash/inputs/syslog.rb
@@ -1,7 +1,9 @@
+# encoding: utf-8
 require "date"
+require "logstash/filters/grok"
+require "logstash/filters/date"
 require "logstash/inputs/base"
 require "logstash/namespace"
-require "logstash/time" # should really use the filters/date.rb bits
 require "socket"
 
 # Read syslog messages as events over the network.
@@ -10,149 +12,227 @@
 # It is also a good choice if you want to receive logs from
 # appliances and network devices where you cannot run your own
 # log collector.
+#
+# Of course, 'syslog' is a very muddy term. This input only supports RFC3164
+# syslog with some small modifications. The date format is allowed to be
+# RFC3164 style or ISO8601. Otherwise the rest of RFC3164 must be obeyed.
+# If you do not use RFC3164, do not use this input.
+#
+# For more information see [the RFC3164 page](http://www.ietf.org/rfc/rfc3164.txt).
+#
+# Note: This input will start listeners on both TCP and UDP.
 class LogStash::Inputs::Syslog < LogStash::Inputs::Base
   config_name "syslog"
+  milestone 1
 
-  # The address to listen on
+  default :codec, "plain"
+
+  # The address to listen on.
   config :host, :validate => :string, :default => "0.0.0.0"
 
-  # The port to listen on
+  # The port to listen on. Remember that ports less than 1024 (privileged
+  # ports) may require root to use.
   config :port, :validate => :number, :default => 514
 
+  # Use label parsing for severity and facility levels.
+  config :use_labels, :validate => :boolean, :default => true
+
+  # Labels for facility levels. These are defined in RFC3164.
+  config :facility_labels, :validate => :array, :default => [ "kernel", "user-level", "mail", "system", "security/authorization", "syslogd", "line printer", "network news", "UUCP", "clock", "security/authorization", "FTP", "NTP", "log audit", "log alert", "clock", "local0", "local1", "local2", "local3", "local4", "local5", "local6", "local7" ]
+
+  # Labels for severity levels. These are defined in RFC3164.
+  config :severity_labels, :validate => :array, :default => [ "Emergency" , "Alert", "Critical", "Error", "Warning", "Notice", "Informational", "Debug" ]
+
+  public
+  def initialize(params)
+    super
+    @shutdown_requested = false
+    BasicSocket.do_not_reverse_lookup = true
+  end # def initialize
+
   public
   def register
-    # This comes from RFC3164, mostly.
-    # Optional fields (priority, host) are because some syslog implementations
-    # don't send these under some circumstances.
-    @@syslog_re ||= \
-      /(?:<([0-9]{1,3})>)?([A-z]{3}  ?[0-9]{1,2} [0-9]{2}:[0-9]{2}:[0-9]{2}) (?:(\S+[^:]) )?(.*)/
-      #<priority>      timestamp          Mmm dd hh:mm:ss             host  msg
+    require "thread_safe"
+    @grok_filter = LogStash::Filters::Grok.new(
+      "overwrite" => "message",
+      "match" => { "message" => "<%{POSINT:priority}>%{SYSLOGLINE}" },
+      "tag_on_failure" => ["_grokparsefailure_sysloginput"],
+    )
+
+    @date_filter = LogStash::Filters::Date.new(
+      "match" => [ "timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss", "ISO8601"]
+    )
+
+    @grok_filter.register
+    @date_filter.register
+
+    @tcp_clients = ThreadSafe::Array.new
   end # def register
 
   public
   def run(output_queue)
     # udp server
-    Thread.new do
-      LogStash::Util::set_thread_name("input|syslog|udp")
+    udp_thr = Thread.new do
       begin
         udp_listener(output_queue)
-      rescue
-        @logger.warn("syslog udp listener died: #{$!}")
+      rescue => e
+        break if @shutdown_requested
+        @logger.warn("syslog udp listener died",
+                     :address => "#{@host}:#{@port}", :exception => e,
+                     :backtrace => e.backtrace)
         sleep(5)
         retry
       end # begin
     end # Thread.new
 
     # tcp server
-    Thread.new do
-      LogStash::Util::set_thread_name("input|syslog|tcp")
+    tcp_thr = Thread.new do
       begin
         tcp_listener(output_queue)
-      rescue
-        @logger.warn("syslog tcp listener died: #{$!}")
+      rescue => e
+        break if @shutdown_requested
+        @logger.warn("syslog tcp listener died",
+                     :address => "#{@host}:#{@port}", :exception => e,
+                     :backtrace => e.backtrace)
         sleep(5)
         retry
       end # begin
     end # Thread.new
+
+    # If we exit and we're the only input, the agent will think no inputs
+    # are running and initiate a shutdown.
+    udp_thr.join
+    tcp_thr.join
   end # def run
 
   private
   def udp_listener(output_queue)
-    @logger.info("Starting syslog udp listener on #{@host}:#{@port}")
-    s = UDPSocket.new
-    s.bind(@host, @port)
+    @logger.info("Starting syslog udp listener", :address => "#{@host}:#{@port}")
+
+    if @udp
+      @udp.close
+    end
+
+    @udp = UDPSocket.new(Socket::AF_INET)
+    @udp.bind(@host, @port)
 
     loop do
-      line, client = s.recvfrom(1024)
-      event = LogStash::Event.new({
-        "@message" => line.chomp,
-        "@type" => @type,
-        "@tags" => @tags.clone,
-      })
-      source_base = URI::Generic.new("syslog", nil, client[3], nil, nil, nil, nil, nil, nil, nil)
-      syslog_relay(event, source)
-      output_queue << event
+      payload, client = @udp.recvfrom(9000)
+      # Ruby uri sucks, so don't use it.
+      @codec.decode(payload) do |event|
+        decorate(event)
+        event["host"] = client[3]
+        syslog_relay(event)
+        output_queue << event
+      end
     end
   ensure
-    if s
-      s.close_read
-      s.close_write
-    end
+    close_udp
   end # def udp_listener
 
   private
   def tcp_listener(output_queue)
-    @logger.info("Starting syslog tcp listener on #{@host}:#{@port}")
-    s = TCPServer.new(@host, @port)
+    @logger.info("Starting syslog tcp listener", :address => "#{@host}:#{@port}")
+    @tcp = TCPServer.new(@host, @port)
+    @tcp_clients = []
 
     loop do
-      Thread.new(s.accept) do |s|
-        ip, port = s.peeraddr[3], s.peeraddr[1]
-        @logger.warn("got connection from #{ip}:#{port}")
+      client = @tcp.accept
+      @tcp_clients << client
+      Thread.new(client) do |client|
+        ip, port = client.peeraddr[3], client.peeraddr[1]
+        @logger.info("new connection", :client => "#{ip}:#{port}")
         LogStash::Util::set_thread_name("input|syslog|tcp|#{ip}:#{port}}")
-        source_base = URI::Generic.new("syslog", nil, ip, nil, nil, nil, nil, nil, nil, nil)
-        s.each do |line|
-          event = LogStash::Event.new({
-            "@message" => line.chomp,
-            "@type" => @type,
-            "@tags" => @tags.clone,
-          })
-          source = source_base.dup
-          syslog_relay(event, source)
-          output_queue << event
+        begin
+          client.each do |line|
+            @codec.decode(line) do |event|
+              decorate(event)
+              event["host"] = ip
+              syslog_relay(event)
+              output_queue << event
+            end
+          end
+        rescue Errno::ECONNRESET
+        ensure
+          @tcp_clients.delete(client)
         end
-      end
-    end
+      end # Thread.new
+    end # loop do
   ensure
-    s.close if s
+    close_tcp
   end # def tcp_listener
 
+  public
+  def teardown
+    @shutdown_requested = true
+    close_udp
+    close_tcp
+    finished
+  end
+
+  private
+  def close_udp
+    if @udp
+      @udp.close_read rescue nil
+      @udp.close_write rescue nil
+    end
+    @udp = nil
+  end
+
+  private
+  def close_tcp
+    # If we somehow have this left open, close it.
+    @tcp_clients.each do |client|
+      client.close rescue nil
+    end
+    @tcp.close if @tcp rescue nil
+    @tcp = nil
+  end
+
   # Following RFC3164 where sane, we'll try to parse a received message
   # as if you were relaying a syslog message to it.
-  # If the message cannot be recognized (see @@syslog_re), we'll
-  # treat it like the whole event.message is correct and try to fill
+  # If the message cannot be recognized (see @grok_filter), we'll
+  # treat it like the whole event["message"] is correct and try to fill
   # the missing pieces (host, priority, etc)
   public
-  def syslog_relay(event, url)
-    match = @@syslog_re.match(event.message)
-    if match
-      # match[1,2,3,4] = {pri, timestamp, hostname, message}
+  def syslog_relay(event)
+    @grok_filter.filter(event)
+
+    if event["tags"].nil? || !event["tags"].include?(@grok_filter.tag_on_failure)
       # Per RFC3164, priority = (facility * 8) + severity
       #                       = (facility << 3) & (severity)
-      priority = match[1].to_i rescue 13
+      priority = event["priority"].to_i rescue 13
       severity = priority & 7   # 7 is 111 (3 bits)
       facility = priority >> 3
-      event.fields["priority"] = priority
-      event.fields["severity"] = severity
-      event.fields["facility"] = facility
-
-      # TODO(sissel): Use the date filter, somehow.
-      event.timestamp = LogStash::Time.to_iso8601(
-        DateTime.strptime(match[2], "%b %d %H:%M:%S"))
-
-      # Hostname is optional, use if present in message, otherwise use source
-      # address of message.
-      url.host = match[3] if match[3]
-      url.port = nil
-      event.source = url
+      event["priority"] = priority
+      event["severity"] = severity
+      event["facility"] = facility
 
-      event.message = match[4]
+      event["timestamp"] = event["timestamp8601"] if event.include?("timestamp8601")
+      @date_filter.filter(event)
     else
-      @logger.info(["NOT SYSLOG", event.message])
-      url.host = Socket.gethostname if url.host == "127.0.0.1"
+      @logger.info? && @logger.info("NOT SYSLOG", :message => event["message"])
 
       # RFC3164 says unknown messages get pri=13
       priority = 13
-      severity = priority & 7   # 7 is 111 (3 bits)
-      facility = priority >> 3
-      event.fields["priority"] = 13
-      event.fields["severity"] = 5   # 13 & 7 == 5
-      event.fields["facility"] = 1   # 13 >> 3 == 1
+      event["priority"] = 13
+      event["severity"] = 5   # 13 & 7 == 5
+      event["facility"] = 1   # 13 >> 3 == 1
+    end
+
+    # Apply severity and facility metadata if
+    # use_labels => true
+    if @use_labels
+      facility_number = event["facility"]
+      severity_number = event["severity"]
 
-      # Don't need to modify the message, here.
-      # event.message = ...
+      if @facility_labels[facility_number]
+        event["facility_label"] = @facility_labels[facility_number]
+      end
 
-      event.source = url
+      if @severity_labels[severity_number]
+        event["severity_label"] = @severity_labels[severity_number]
+      end
     end
   end # def syslog_relay
 end # class LogStash::Inputs::Syslog
diff --git a/lib/logstash/inputs/tcp.rb b/lib/logstash/inputs/tcp.rb
index 26b4dde44f0..850a99b3d65 100644
--- a/lib/logstash/inputs/tcp.rb
+++ b/lib/logstash/inputs/tcp.rb
@@ -1,70 +1,238 @@
+# encoding: utf-8
 require "logstash/inputs/base"
 require "logstash/namespace"
-require "socket"
-require "timeout"
+require "logstash/util/socket_peer"
 
 # Read events over a TCP socket.
 #
 # Like stdin and file inputs, each event is assumed to be one line of text.
+#
+# Can either accept connections from clients or connect to a server,
+# depending on `mode`.
 class LogStash::Inputs::Tcp < LogStash::Inputs::Base
-
+  class Interrupted < StandardError; end
   config_name "tcp"
+  milestone 2
 
-  # The address to listen on
+  default :codec, "line"
+
+  # When mode is `server`, the address to listen on.
+  # When mode is `client`, the address to connect to.
   config :host, :validate => :string, :default => "0.0.0.0"
-  
-  # the port to listen on
+
+  # When mode is `server`, the port to listen on.
+  # When mode is `client`, the port to connect to.
   config :port, :validate => :number, :required => true
 
-  # Read timeout in seconds. If a particular tcp connection is
-  # idle for more than this timeout period, we will assume 
-  # it is dead and close it.
-  config :data_timeout, :validate => :number, :default => 5
+  # The 'read' timeout in seconds. If a particular tcp connection is idle for
+  # more than this timeout period, we will assume it is dead and close it.
+  #
+  # If you never want to timeout, use -1.
+  config :data_timeout, :validate => :number, :default => -1
+
+  # Mode to operate in. `server` listens for client connections,
+  # `client` connects to a server.
+  config :mode, :validate => ["server", "client"], :default => "server"
+
+  # Enable SSL (must be set for other `ssl_` options to take effect).
+  config :ssl_enable, :validate => :boolean, :default => false
+
+  # Verify the identity of the other end of the SSL connection against the CA.
+  # For input, sets the field `sslsubject` to that of the client certificate.
+  config :ssl_verify, :validate => :boolean, :default => false
+
+  # The SSL CA certificate, chainfile or CA path. The system CA path is automatically included.
+  config :ssl_cacert, :validate => :path
+
+  # SSL certificate path
+  config :ssl_cert, :validate => :path
+
+  # SSL key path
+  config :ssl_key, :validate => :path
+
+  # SSL key passphrase
+  config :ssl_key_passphrase, :validate => :password, :default => nil
+
+  def initialize(*args)
+    super(*args)
+  end # def initialize
 
   public
   def register
-    @logger.info("Starting tcp listener on #{@host}:#{@port}")
-    @server = TCPServer.new(@host, @port)
+    require "socket"
+    require "timeout"
+    require "openssl"
+
+    # monkey patch TCPSocket and SSLSocket to include socket peer
+    TCPSocket.module_eval{include ::LogStash::Util::SocketPeer}
+    OpenSSL::SSL::SSLSocket.module_eval{include ::LogStash::Util::SocketPeer}
+
+    fix_streaming_codecs
+
+    if @ssl_enable
+      @ssl_context = OpenSSL::SSL::SSLContext.new
+      @ssl_context.cert = OpenSSL::X509::Certificate.new(File.read(@ssl_cert))
+      @ssl_context.key = OpenSSL::PKey::RSA.new(File.read(@ssl_key),@ssl_key_passphrase)
+      if @ssl_verify
+        @cert_store = OpenSSL::X509::Store.new
+        # Load the system default certificate path to the store
+        @cert_store.set_default_paths
+        if File.directory?(@ssl_cacert)
+          @cert_store.add_path(@ssl_cacert)
+        else
+          @cert_store.add_file(@ssl_cacert)
+        end
+        @ssl_context.cert_store = @cert_store
+        @ssl_context.verify_mode = OpenSSL::SSL::VERIFY_PEER|OpenSSL::SSL::VERIFY_FAIL_IF_NO_PEER_CERT
+      end
+    end # @ssl_enable
+
+    if server?
+      @logger.info("Starting tcp input listener", :address => "#{@host}:#{@port}")
+      begin
+        @server_socket = TCPServer.new(@host, @port)
+      rescue Errno::EADDRINUSE
+        @logger.error("Could not start TCP server: Address in use", :host => @host, :port => @port)
+        raise
+      end
+      if @ssl_enable
+        @server_socket = OpenSSL::SSL::SSLServer.new(@server_socket, @ssl_context)
+      end # @ssl_enable
+    end
   end # def register
 
+  private
+  def handle_socket(socket, client_address, output_queue, codec)
+    while true
+      buf = nil
+      # NOTE(petef): the timeout only hits after the line is read or socket dies
+      # TODO(sissel): Why do we have a timeout here? What's the point?
+      if @data_timeout == -1
+        buf = read(socket)
+      else
+        Timeout::timeout(@data_timeout) do
+          buf = read(socket)
+        end
+      end
+      codec.decode(buf) do |event|
+        event["host"] ||= client_address
+        event["sslsubject"] ||= socket.peer_cert.subject if @ssl_enable && @ssl_verify
+        decorate(event)
+        output_queue << event
+      end
+    end # loop
+  rescue EOFError
+    @logger.debug? && @logger.debug("Connection closed", :client => socket.peer)
+  rescue Errno::ECONNRESET
+    @logger.debug? && @logger.debug("Connection reset by peer", :client => socket.peer)
+  rescue => e
+    @logger.error("An error occurred. Closing connection", :client => socket.peer, :exception => e, :backtrace => e.backtrace)
+  ensure
+    socket.close rescue nil
+
+    codec.respond_to?(:flush) && codec.flush do |event|
+      event["host"] ||= client_address
+      event["sslsubject"] ||= socket.peer_cert.subject if @ssl_enable && @ssl_verify
+      decorate(event)
+      output_queue << event
+    end
+  end
+
+  private
+  def client_thread(output_queue, socket)
+    Thread.new(output_queue, socket) do |q, s|
+      begin
+        @logger.debug? && @logger.debug("Accepted connection", :client => s.peer, :server => "#{@host}:#{@port}")
+        handle_socket(s, s.peeraddr[3], q, @codec.clone)
+      rescue Interrupted
+        s.close rescue nil
+      ensure
+        @client_threads_lock.synchronize{@client_threads.delete(Thread.current)}
+      end
+    end
+  end
+
+  private
+  def server?
+    @mode == "server"
+  end # def server?
+
+  private
+  def read(socket)
+    return socket.sysread(16384)
+  end # def readline
+
   public
   def run(output_queue)
-    loop do
-      # Start a new thread for each connection.
-      Thread.start(@server.accept) do |s|
-        # TODO(sissel): put this block in its own method.
-        peer = "#{s.peeraddr[3]}:#{s.peeraddr[1]}"
-        @logger.debug("Accepted connection from #{peer} on #{@host}:#{@port}")
-        begin
-          loop do
-            buf = nil
-            # NOTE(petef): the timeout only hits after the line is read
-            # or socket dies
-            # TODO(sissel): Why do we have a timeout here? What's the point?
-            Timeout::timeout(@data_timeout) do
-              buf = s.readline
-            end
-            e = LogStash::Event.new({
-              "@message" => buf,
-              "@type" => @type,
-              "@tags" => @tags.clone,
-            })
-            e.source = "tcp://#{@host}:#{@port}/client/#{peer}"
-            @logger.debug(["Received message from #{peer}"], e)
-            output_queue << e
-          end # loop do
-        rescue
-          @logger.debug("Closing connection with #{peer}")
-        rescue Timeout::Error
-          @logger.debug("Closing connection with #{peer} after read timeout")
-        end # begin
+    if server?
+      run_server(output_queue)
+    else
+      run_client(output_queue)
+    end
+  end # def run
+
+  def run_server(output_queue)
+    @thread = Thread.current
+    @client_threads = []
+    @client_threads_lock = Mutex.new
+
+    while true
+      begin
+        socket = @server_socket.accept
+        # start a new thread for each connection.
+        @client_threads_lock.synchronize{@client_threads << client_thread(output_queue, socket)}
+      rescue OpenSSL::SSL::SSLError => ssle
+        # NOTE(mrichar1): This doesn't return a useful error message for some reason
+        @logger.error("SSL Error", :exception => ssle, :backtrace => ssle.backtrace)
+      rescue IOError, LogStash::ShutdownSignal
+        if @interrupted
+          @server_socket.close rescue nil
+
+          threads = @client_threads_lock.synchronize{@client_threads.dup}
+          threads.each do |thread|
+            thread.raise(LogStash::ShutdownSignal) if thread.alive?
+          end
 
+          # intended shutdown, get out of the loop
+          break
+        else
+          # it was a genuine IOError, propagate it up
+          raise
+        end
+      end
+    end # loop
+  rescue LogStash::ShutdownSignal
+    # nothing to do
+  ensure
+    @server_socket.close rescue nil
+  end # def run_server
+
+  def run_client(output_queue)
+    @thread = Thread.current
+    while true
+      client_socket = TCPSocket.new(@host, @port)
+      if @ssl_enable
+        client_socket = OpenSSL::SSL::SSLSocket.new(client_socket, @ssl_context)
         begin
-          s.close
-        rescue IOError
-          pass
-        end # begin
-      end # Thread.start
-    end # loop (outer)
+          client_socket.connect
+        rescue OpenSSL::SSL::SSLError => ssle
+          @logger.error("SSL Error", :exception => ssle, :backtrace => ssle.backtrace)
+          # NOTE(mrichar1): Hack to prevent hammering peer
+          sleep(5)
+          next
+        end
+      end
+      @logger.debug("Opened connection", :client => "#{client_socket.peer}")
+      handle_socket(client_socket, client_socket.peeraddr[3], output_queue, @codec.clone)
+    end # loop
+  ensure
+    client_socket.close rescue nil
   end # def run
+
+  public
+  def teardown
+    if server?
+      @interrupted = true
+    end
+  end # def teardown
 end # class LogStash::Inputs::Tcp
diff --git a/lib/logstash/inputs/threadable.rb b/lib/logstash/inputs/threadable.rb
new file mode 100644
index 00000000000..04d834b722c
--- /dev/null
+++ b/lib/logstash/inputs/threadable.rb
@@ -0,0 +1,18 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/inputs/base"
+
+# This is the threadable class for logstash inputs. 
+# Use this class in your inputs if it can support multiple threads
+class LogStash::Inputs::Threadable < LogStash::Inputs::Base
+
+  # Set this to the number of threads you want this input to spawn.
+  # This is the same as declaring the input multiple times
+  config :threads, :validate => :number, :default => 1
+ 
+  def initialize(params)
+    super
+    @threadable = true
+  end
+
+end # class LogStash::Inputs::Threadable
diff --git a/lib/logstash/inputs/twitter.rb b/lib/logstash/inputs/twitter.rb
index 753262743ef..64ce5ba66ff 100644
--- a/lib/logstash/inputs/twitter.rb
+++ b/lib/logstash/inputs/twitter.rb
@@ -1,99 +1,124 @@
+# encoding: utf-8
 require "logstash/inputs/base"
 require "logstash/namespace"
-require "net/http"
-require "json"
-#require "net/https"
+require "logstash/timestamp"
+require "logstash/util"
+require "logstash/json"
 
+# Read events from the twitter streaming api.
 class LogStash::Inputs::Twitter < LogStash::Inputs::Base
 
   config_name "twitter"
-  
-  # Your twitter username
-  config :user, :validate => :string, :required => true
+  milestone 1
 
-  # Your twitter password
-  config :password, :validate => :password, :required => true
+  # Your twitter app's consumer key
+  #
+  # Don't know what this is? You need to create an "application"
+  # on twitter, see this url: <https://dev.twitter.com/apps/new>
+  config :consumer_key, :validate => :string, :required => true
+
+  # Your twitter app's consumer secret
+  #
+  # If you don't have one of these, you can create one by
+  # registering a new application with twitter:
+  # <https://dev.twitter.com/apps/new>
+  config :consumer_secret, :validate => :password, :required => true
+
+  # Your oauth token.
+  #
+  # To get this, login to twitter with whatever account you want,
+  # then visit <https://dev.twitter.com/apps>
+  #
+  # Click on your app (used with the consumer_key and consumer_secret settings)
+  # Then at the bottom of the page, click 'Create my access token' which
+  # will create an oauth token and secret bound to your account and that
+  # application.
+  config :oauth_token, :validate => :string, :required => true
+
+  # Your oauth token secret.
+  #
+  # To get this, login to twitter with whatever account you want,
+  # then visit <https://dev.twitter.com/apps>
+  #
+  # Click on your app (used with the consumer_key and consumer_secret settings)
+  # Then at the bottom of the page, click 'Create my access token' which
+  # will create an oauth token and secret bound to your account and that
+  # application.
+  config :oauth_token_secret, :validate => :password, :required => true
 
   # Any keywords to track in the twitter stream
   config :keywords, :validate => :array, :required => true
 
+  # Record full tweet object as given to us by the Twitter stream api.
+  config :full_tweet, :validate => :boolean, :default => false
+
+  public
   def register
-    # TODO(sissel): put buftok in logstash, too
-    require "filewatch/buftok"
-    #require "tweetstream" # rubygem 'tweetstream'
+    require "twitter"
+
+    # monkey patch twitter gem to ignore json parsing error.
+    # at the same time, use our own json parser
+    # this has been tested with a specific gem version, raise if not the same
+    raise("Invalid Twitter gem") unless Twitter::Version.to_s == "5.0.0.rc.1"
+    Twitter::Streaming::Response.module_eval do
+      def on_body(data)
+        @tokenizer.extract(data).each do |line|
+          next if line.empty?
+          begin
+            @block.call(LogStash::Json.load(line, :symbolize_keys => true))
+          rescue LogStash::Json::ParserError
+            # silently ignore json parsing errors
+          end
+        end
+      end
+    end
+
+    @client = Twitter::Streaming::Client.new do |c|
+      c.consumer_key = @consumer_key
+      c.consumer_secret = @consumer_secret.value
+      c.access_token = @oauth_token
+      c.access_token_secret = @oauth_token_secret.value
+    end
   end
 
   public
   def run(queue)
-    loop do
-      #stream = TweetStream::Client.new(@user, @password.value)
-      #stream.track(*@keywords) do |status|
-      track(*@keywords) do |status|
-        @logger.debug :status => status
-        #@logger.debug("Got twitter status from @#{status[:user][:screen_name]}")
-        @logger.info("Got twitter status from @#{status["user"]["screen_name"]}")
-        event = LogStash::Event.new(
-          #"@message" => status[:text],
-          "@message" => status["text"],
-          "@type" => @type,
-          "@tags" => @tags.clone
-        )
-
-        event.fields.merge!(
-          #"user" => (status[:user][:screen_name] rescue nil), 
-          "user" => (status["user"]["screen_name"] rescue nil), 
-          #"client" => (status[:source] rescue nil),
-          "client" => (status["source"] rescue nil),
-          #"retweeted" => (status[:retweeted] rescue nil)
-          "retweeted" => (status["retweeted"] rescue nil)
-        )
-
-        #event.fields["in-reply-to"] = status[:in_reply_to_status_id] if status[:in_reply_to_status_id]
-        event.fields["in-reply-to"] = status["in_reply_to_status_id"] if status["in_reply_to_status_id"]
+    @logger.info("Starting twitter tracking", :keywords => @keywords)
+    begin
+      @client.filter(:track => @keywords.join(",")) do |tweet|
+        if tweet.is_a?(Twitter::Tweet)
+          @logger.debug? && @logger.debug("Got tweet", :user => tweet.user.screen_name, :text => tweet.text)
+          if @full_tweet
+            event = LogStash::Event.new(LogStash::Util.stringify_symbols(tweet.to_hash))
+            event.timestamp = LogStash::Timestamp.new(tweet.created_at)
+          else
+            event = LogStash::Event.new(
+              LogStash::Event::TIMESTAMP => LogStash::Timestamp.new(tweet.created_at),
+              "message" => tweet.full_text,
+              "user" => tweet.user.screen_name,
+              "client" => tweet.source,
+              "retweeted" => tweet.retweeted?,
+              "source" => "http://twitter.com/#{tweet.user.screen_name}/status/#{tweet.id}"
+            )
+            event["in-reply-to"] = tweet.in_reply_to_status_id if tweet.reply?
+            unless tweet.urls.empty?
+              event["urls"] = tweet.urls.map(&:expanded_url).map(&:to_s)
+            end
+          end
 
-        #urls = status[:entities][:urls] rescue []
-        urls = status["entities"]["urls"] rescue []
-        if urls.size > 0
-          event.fields["urls"] = urls.collect { |u| u["url"] }
+          decorate(event)
+          queue << event
         end
-
-        event.source = "http://twitter.com/#{event.fields["user"]}/status/#{status["id"]}"
-        @logger.debug(["Got event", event])
-        queue << event
-      end # stream.track
-
-      # Some closure or error occured, sleep and try again.
-      @logger.warn("An error occured? Retrying twitter in 30 seconds")
-      sleep 30
-    end # loop
+      end # client.filter
+    rescue LogStash::ShutdownSignal
+      return
+    rescue Twitter::Error::TooManyRequests => e
+      @logger.warn("Twitter too many requests error, sleeping for #{e.rate_limit.reset_in}s")
+      sleep(e.rate_limit.reset_in)
+      retry
+    rescue => e
+      @logger.warn("Twitter client error", :message => e.message, :exception => e, :backtrace => e.backtrace)
+      retry
+    end
   end # def run
-
-  private
-  def track(*keywords)
-    uri = URI.parse("http://stream.twitter.com/1/statuses/filter.json")
-    #params = {
-      #"track" => keywords
-    #}
-
-    http = Net::HTTP.new(uri.host, uri.port)
-    #http.use_ssl = true
-    request = Net::HTTP::Post.new(uri.path)
-    request.body = "track=#{keywords.join(",")}"
-    request.basic_auth @user, @password.value
-    buffer = BufferedTokenizer.new("\r\n")
-    http.request(request) do |response|
-      response.read_body do |chunk|
-        #@logger.info("Twitter: #{chunk.inspect}")
-        buffer.extract(chunk).each do |line|
-          @logger.info("Twitter line: #{line.inspect}")
-          begin 
-            status = JSON.parse(line)
-            yield status
-          rescue => e
-            @logger.error e
-          end
-        end # buffer.extract
-      end # response.read_body
-    end # http.request
-  end # def track
 end # class LogStash::Inputs::Twitter
diff --git a/lib/logstash/inputs/udp.rb b/lib/logstash/inputs/udp.rb
new file mode 100644
index 00000000000..ba5148c43d4
--- /dev/null
+++ b/lib/logstash/inputs/udp.rb
@@ -0,0 +1,112 @@
+# encoding: utf-8
+require "date"
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "socket"
+
+# Read messages as events over the network via udp. The only required
+# configuration item is `port`, which specifies the udp port logstash
+# will listen on for event streams.
+#
+class LogStash::Inputs::Udp < LogStash::Inputs::Base
+  config_name "udp"
+  milestone 2
+
+  default :codec, "plain"
+
+  # The address which logstash will listen on.
+  config :host, :validate => :string, :default => "0.0.0.0"
+
+  # The port which logstash will listen on. Remember that ports less
+  # than 1024 (privileged ports) may require root or elevated privileges to use.
+  config :port, :validate => :number, :required => true
+
+  # The maximum packet size to read from the network
+  config :buffer_size, :validate => :number, :default => 8192
+
+  # Number of threads processing packets
+  config :workers, :validate => :number, :default => 2
+
+  # This is the number of unprocessed UDP packets you can hold in memory
+  # before packets will start dropping.
+  config :queue_size, :validate => :number, :default => 2000
+
+  public
+  def initialize(params)
+    super
+    BasicSocket.do_not_reverse_lookup = true
+  end # def initialize
+
+  public
+  def register
+    @udp = nil
+  end # def register
+
+  public
+  def run(output_queue)
+  @output_queue = output_queue
+    begin
+      # udp server
+      udp_listener(output_queue)
+    rescue LogStash::ShutdownSignal
+      # do nothing, shutdown was requested.
+    rescue => e
+      @logger.warn("UDP listener died", :exception => e, :backtrace => e.backtrace)
+      sleep(5)
+      retry
+    end # begin
+  end # def run
+
+  private
+  def udp_listener(output_queue)
+    @logger.info("Starting UDP listener", :address => "#{@host}:#{@port}")
+
+    if @udp && ! @udp.closed?
+      @udp.close
+    end
+
+    @udp = UDPSocket.new(Socket::AF_INET)
+    @udp.bind(@host, @port)
+
+    @input_to_worker = SizedQueue.new(@queue_size)
+
+    @input_workers = @workers.times do |i|
+      @logger.debug("Starting UDP worker thread", :worker => i)
+      Thread.new { inputworker(i) }
+    end
+
+    while true
+      #collect datagram message and add to queue
+      payload, client = @udp.recvfrom(@buffer_size)
+      @input_to_worker.push([payload, client])
+    end
+  ensure
+    if @udp
+      @udp.close_read rescue nil
+      @udp.close_write rescue nil
+    end
+  end # def udp_listener
+
+  def inputworker(number)
+    LogStash::Util::set_thread_name("<udp.#{number}")
+    begin
+      while true
+        payload, client = @input_to_worker.pop
+
+        @codec.decode(payload) do |event|
+          decorate(event)
+          event["host"] ||= client[3]
+          @output_queue.push(event)
+        end
+      end
+    rescue => e
+      @logger.error("Exception in inputworker", "exception" => e, "backtrace" => e.backtrace)
+    end
+  end # def inputworker
+
+  public
+  def teardown
+    @udp.close if @udp && !@udp.closed?
+  end
+
+end # class LogStash::Inputs::Udp
diff --git a/lib/logstash/inputs/unix.rb b/lib/logstash/inputs/unix.rb
new file mode 100644
index 00000000000..b78a887b0d7
--- /dev/null
+++ b/lib/logstash/inputs/unix.rb
@@ -0,0 +1,163 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "socket"
+
+# Read events over a UNIX socket.
+#
+# Like stdin and file inputs, each event is assumed to be one line of text.
+#
+# Can either accept connections from clients or connect to a server,
+# depending on `mode`.
+class LogStash::Inputs::Unix < LogStash::Inputs::Base
+  class Interrupted < StandardError; end
+  config_name "unix"
+  milestone 2
+
+  default :codec, "line"
+
+  # When mode is `server`, the path to listen on.
+  # When mode is `client`, the path to connect to.
+  config :path, :validate => :string, :required => true
+
+  # Remove socket file in case of EADDRINUSE failure
+  config :force_unlink, :validate => :boolean, :default => false
+
+  # The 'read' timeout in seconds. If a particular connection is idle for
+  # more than this timeout period, we will assume it is dead and close it.
+  #
+  # If you never want to timeout, use -1.
+  config :data_timeout, :validate => :number, :default => -1
+
+  # Mode to operate in. `server` listens for client connections,
+  # `client` connects to a server.
+  config :mode, :validate => ["server", "client"], :default => "server"
+
+  def initialize(*args)
+    super(*args)
+  end # def initialize
+
+  public
+  def register
+    require "socket"
+    require "timeout"
+
+    if server?
+      @logger.info("Starting unix input listener", :address => "#{@path}", :force_unlink => "#{@force_unlink}")
+      begin
+        @server_socket = UNIXServer.new(@path)
+      rescue Errno::EADDRINUSE, IOError
+        if @force_unlink
+          File.unlink(@path)
+          begin
+            @server_socket = UNIXServer.new(@path)
+            return
+          rescue Errno::EADDRINUSE, IOError
+            @logger.error("!!!Could not start UNIX server: Address in use",
+                          :path => @path)
+            raise
+          end
+        end
+        @logger.error("Could not start UNIX server: Address in use",
+                      :path => @path)
+        raise
+      end
+    end
+  end # def register
+
+  private
+  def handle_socket(socket, output_queue)
+    begin
+      hostname = Socket.gethostname
+      loop do
+        buf = nil
+        # NOTE(petef): the timeout only hits after the line is read
+        # or socket dies
+        # TODO(sissel): Why do we have a timeout here? What's the point?
+        if @data_timeout == -1
+          buf = socket.readpartial(16384)
+        else
+          Timeout::timeout(@data_timeout) do
+            buf = socket.readpartial(16384)
+          end
+        end
+        @codec.decode(buf) do |event|
+          decorate(event)
+          event["host"] = hostname
+          event["path"] = @path
+          output_queue << event
+        end
+      end # loop do
+    rescue => e
+      @logger.debug("Closing connection", :path => @path,
+      :exception => e, :backtrace => e.backtrace)
+    rescue Timeout::Error
+      @logger.debug("Closing connection after read timeout",
+      :path => @path)
+    end # begin
+
+  ensure
+    begin
+      socket.close
+    rescue IOError
+      #pass
+    end # begin
+  end
+
+  private
+  def server?
+    @mode == "server"
+  end # def server?
+
+  public
+  def run(output_queue)
+    if server?
+      @thread = Thread.current
+      @client_threads = []
+      loop do
+        # Start a new thread for each connection.
+        begin
+          @client_threads << Thread.start(@server_socket.accept) do |s|
+            # TODO(sissel): put this block in its own method.
+
+            @logger.debug("Accepted connection",
+                          :server => "#{@path}")
+            begin
+              handle_socket(s, output_queue)
+            rescue Interrupted
+              s.close rescue nil
+            end
+          end # Thread.start
+        rescue IOError, Interrupted
+          if @interrupted
+            # Intended shutdown, get out of the loop
+            @server_socket.close
+            @client_threads.each do |thread|
+              thread.raise(IOError.new)
+            end
+            break
+          else
+            # Else it was a genuine IOError caused by something else, so propagate it up..
+            raise
+          end
+        end
+      end # loop
+    else
+      loop do
+        client_socket = UNIXSocket.new(@path)
+        client_socket.instance_eval { class << self; include ::LogStash::Util::SocketPeer end }
+        @logger.debug("Opened connection", :client => @path)
+        handle_socket(client_socket, output_queue)
+      end # loop
+    end
+  end # def run
+
+  public
+  def teardown
+    if server?
+      File.unlink(@path)
+      @interrupted = true
+      @thread.raise(Interrupted.new)
+    end
+  end # def teardown
+end # class LogStash::Inputs::Unix
diff --git a/lib/logstash/inputs/xmpp.rb b/lib/logstash/inputs/xmpp.rb
new file mode 100644
index 00000000000..fee932dff90
--- /dev/null
+++ b/lib/logstash/inputs/xmpp.rb
@@ -0,0 +1,81 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+
+# This input allows you to receive events over XMPP/Jabber.
+#
+# This plugin can be used for accepting events from humans or applications
+# XMPP, or you can use it for PubSub or general message passing for logstash to
+# logstash.
+class LogStash::Inputs::Xmpp < LogStash::Inputs::Base
+  
+  config_name "xmpp"
+  milestone 2
+
+  default :codec, "plain"
+
+  # The user or resource ID, like foo@example.com.
+  config :user, :validate => :string, :required => :true
+
+  # The xmpp password for the user/identity.
+  config :password, :validate => :password, :required => :true
+
+  # if muc/multi-user-chat required, give the name of the room that
+  # you want to join: room@conference.domain/nick
+  config :rooms, :validate => :array
+
+  # The xmpp server to connect to. This is optional. If you omit this setting,
+  # the host on the user/identity is used. (foo.com for user@foo.com)
+  config :host, :validate => :string
+
+  # Set to true to enable greater debugging in XMPP. Useful for debugging
+  # network/authentication erros.
+  config :debug, :validate => :boolean, :default => false, :deprecated => "Use the logstash --debug flag for this instead."
+
+  public
+  def register
+    require 'xmpp4r' # xmpp4r gem
+    Jabber::debug = true if @debug || @logger.debug?
+
+    @client = Jabber::Client.new(Jabber::JID.new(@user))
+    @client.connect(@host) # it is ok if host is nil
+    @client.auth(@password.value)
+    @client.send(Jabber::Presence.new.set_type(:available))
+
+    # load the MUC Client if we are joining rooms.
+    require 'xmpp4r/muc/helper/simplemucclient' if @rooms && !@rooms.empty?
+  end # def register
+
+  public
+  def run(queue)
+    if @rooms
+      @rooms.each do |room| # handle muc messages in different rooms
+        @muc = Jabber::MUC::SimpleMUCClient.new(@client)
+        @muc.join(room)
+        @muc.on_message do |time,from,body|
+          @codec.decode(body) do |event|
+            decorate(event)
+            event["room"] = room
+            event["from"] = from
+            queue << event
+          end
+        end # @muc.on_message
+      end # @rooms.each
+    end # if @rooms
+
+    @client.add_message_callback do |msg| # handle direct/private messages
+      # accept normal msgs (skip presence updates, etc)
+      if msg.body != nil
+        @codec.decode(msg.body) do |event|
+          decorate(event)
+          # Maybe "from" should just be a hash: 
+          # { "node" => ..., "domain" => ..., "resource" => ... }
+          event["from"] = "#{msg.from.node}@#{msg.from.domain}/#{msg.from.resource}"
+          queue << event
+        end
+      end
+    end # @client.add_message_callback
+    sleep
+  end # def run
+
+end # class LogStash::Inputs::Xmpp
diff --git a/lib/logstash/inputs/zeromq.rb b/lib/logstash/inputs/zeromq.rb
new file mode 100644
index 00000000000..b094a8c4b06
--- /dev/null
+++ b/lib/logstash/inputs/zeromq.rb
@@ -0,0 +1,165 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "socket"
+
+# Read events over a 0MQ SUB socket.
+#
+# You need to have the 0mq 2.1.x library installed to be able to use
+# this input plugin.
+#
+# The default settings will create a subscriber binding to tcp://127.0.0.1:2120 
+# waiting for connecting publishers.
+#
+class LogStash::Inputs::ZeroMQ < LogStash::Inputs::Base
+
+  config_name "zeromq"
+  milestone 2
+
+  default :codec, "json"
+
+  # 0mq socket address to connect or bind
+  # Please note that `inproc://` will not work with logstash
+  # as each we use a context per thread.
+  # By default, inputs bind/listen
+  # and outputs connect
+  config :address, :validate => :array, :default => ["tcp://*:2120"]
+
+  # 0mq topology
+  # The default logstash topologies work as follows:
+  # * pushpull - inputs are pull, outputs are push
+  # * pubsub - inputs are subscribers, outputs are publishers
+  # * pair - inputs are clients, inputs are servers
+  #
+  # If the predefined topology flows don't work for you,
+  # you can change the 'mode' setting
+  # TODO (lusis) add req/rep MAYBE
+  # TODO (lusis) add router/dealer
+  config :topology, :validate => ["pushpull", "pubsub", "pair"], :required => true
+
+  # 0mq topic
+  # This is used for the 'pubsub' topology only
+  # On inputs, this allows you to filter messages by topic
+  # On outputs, this allows you to tag a message for routing
+  # NOTE: ZeroMQ does subscriber side filtering.
+  # NOTE: All topics have an implicit wildcard at the end
+  # You can specify multiple topics here
+  config :topic, :validate => :array
+
+  # mode
+  # server mode binds/listens
+  # client mode connects
+  config :mode, :validate => ["server", "client"], :default => "server"
+
+  # sender
+  # overrides the sender to 
+  # set the source of the event
+  # default is "zmq+topology://type/"
+  config :sender, :validate => :string
+
+  # 0mq socket options
+  # This exposes zmq_setsockopt
+  # for advanced tuning
+  # see http://api.zeromq.org/2-1:zmq-setsockopt for details
+  #
+  # This is where you would set values like:
+  # ZMQ::HWM - high water mark
+  # ZMQ::IDENTITY - named queues
+  # ZMQ::SWAP_SIZE - space for disk overflow
+  #
+  # example: sockopt => ["ZMQ::HWM", 50, "ZMQ::IDENTITY", "my_named_queue"]
+  config :sockopt, :validate => :hash
+
+  public
+  def register
+    require "ffi-rzmq"
+    require "logstash/util/zeromq"
+    self.class.send(:include, LogStash::Util::ZeroMQ)
+
+    case @topology
+    when "pair"
+      zmq_const = ZMQ::PAIR 
+    when "pushpull"
+      zmq_const = ZMQ::PULL
+    when "pubsub"
+      zmq_const = ZMQ::SUB
+    end # case socket_type
+    @zsocket = context.socket(zmq_const)
+    error_check(@zsocket.setsockopt(ZMQ::LINGER, 1),
+                "while setting ZMQ::LINGER == 1)")
+
+    if @sockopt
+      setopts(@zsocket, @sockopt)
+    end
+
+    @address.each do |addr|
+      setup(@zsocket, addr)
+    end
+
+    if @topology == "pubsub"
+      if @topic.nil?
+        @logger.debug("ZMQ - No topic provided. Subscribing to all messages")
+        error_check(@zsocket.setsockopt(ZMQ::SUBSCRIBE, ""),
+      "while setting ZMQ::SUBSCRIBE")
+      else
+        @topic.each do |t|
+          @logger.debug("ZMQ subscribing to topic: #{t}")
+          error_check(@zsocket.setsockopt(ZMQ::SUBSCRIBE, t),
+        "while setting ZMQ::SUBSCRIBE == #{t}")
+        end
+      end
+    end
+
+  end # def register
+
+  def teardown
+    error_check(@zsocket.close, "while closing the zmq socket")
+  end # def teardown
+
+  def server?
+    @mode == "server"
+  end # def server?
+
+  def run(output_queue)
+    host = Socket.gethostname
+    begin
+      loop do
+        # Here's the unified receiver
+        # Get the first part as the msg
+        m1 = ""
+        rc = @zsocket.recv_string(m1)
+        error_check(rc, "in recv_string")
+        @logger.debug("ZMQ receiving", :event => m1)
+        msg = m1
+        # If we have more parts, we'll eat the first as the topic
+        # and set the message to the second part
+        if @zsocket.more_parts?
+          @logger.debug("Multipart message detected. Setting @message to second part. First part was: #{m1}")
+          m2 = ''
+          rc2 = @zsocket.recv_string(m2)
+          error_check(rc2, "in recv_string")
+          @logger.debug("ZMQ receiving", :event => m2)
+          msg = m2
+        end
+
+        @codec.decode(msg) do |event|
+          event["host"] ||= host
+          decorate(event)
+          output_queue << event
+        end
+      end
+    rescue LogStash::ShutdownSignal
+      # shutdown
+      return
+    rescue => e
+      @logger.debug("ZMQ Error", :subscriber => @zsocket,
+                    :exception => e)
+      retry
+    end # begin
+  end # def run
+
+  private
+  def build_source_string
+    id = @address.first.clone
+  end
+end # class LogStash::Inputs::ZeroMQ
diff --git a/lib/logstash/java_integration.rb b/lib/logstash/java_integration.rb
new file mode 100644
index 00000000000..2bfeb3e81d2
--- /dev/null
+++ b/lib/logstash/java_integration.rb
@@ -0,0 +1,41 @@
+require "java"
+
+# this is mainly for usage with JrJackson json parsing in :raw mode which genenerates
+# Java::JavaUtil::ArrayList and Java::JavaUtil::LinkedHashMap native objects for speed.
+# these object already quacks like their Ruby equivalents Array and Hash but they will
+# not test for is_a?(Array) or is_a?(Hash) and we do not want to include tests for
+# both classes everywhere. see LogStash::JSon.
+
+class Java::JavaUtil::ArrayList
+  # have ArrayList objects report is_a?(Array) == true
+  def is_a?(clazz)
+    return true if clazz == Array
+    super
+  end
+end
+
+class Java::JavaUtil::LinkedHashMap
+  # have LinkedHashMap objects report is_a?(Array) == true
+  def is_a?(clazz)
+    return true if clazz == Hash
+    super
+  end
+end
+
+class Array
+  # enable class equivalence between Array and ArrayList
+  # so that ArrayList will work with case o when Array ...
+  def self.===(other)
+    return true if other.is_a?(Java::JavaUtil::ArrayList)
+    super
+  end
+end
+
+class Hash
+  # enable class equivalence between Hash and LinkedHashMap
+  # so that LinkedHashMap will work with case o when Hash ...
+  def self.===(other)
+    return true if other.is_a?(Java::JavaUtil::LinkedHashMap)
+    super
+  end
+end
diff --git a/lib/logstash/json.rb b/lib/logstash/json.rb
new file mode 100644
index 00000000000..5079de759a0
--- /dev/null
+++ b/lib/logstash/json.rb
@@ -0,0 +1,53 @@
+# encoding: utf-8
+require "logstash/environment"
+require "logstash/errors"
+if LogStash::Environment.jruby?
+  require "jrjackson"
+  require "logstash/java_integration"
+else
+  require  "oj"
+end
+
+module LogStash
+  module Json
+    class ParserError < LogStash::Error; end
+    class GeneratorError < LogStash::Error; end
+
+    extend self
+
+    ### MRI
+
+    def mri_load(data, options = {})
+      Oj.load(data)
+    rescue Oj::ParseError => e
+      raise LogStash::Json::ParserError.new(e.message)
+    end
+
+    def mri_dump(o)
+      Oj.dump(o, :mode => :compat, :use_to_json => true)
+    rescue => e
+      raise LogStash::Json::GeneratorError.new(e.message)
+    end
+
+    ### JRuby
+
+    def jruby_load(data, options = {})
+      options[:symbolize_keys] ? JrJackson::Raw.parse_sym(data) : JrJackson::Raw.parse_raw(data)
+    rescue JrJackson::ParseError => e
+      raise LogStash::Json::ParserError.new(e.message)
+    end
+
+    def jruby_dump(o)
+      # test for enumerable here to work around an omission in JrJackson::Json.dump to
+      # also look for Java::JavaUtil::ArrayList, see TODO submit issue
+      o.is_a?(Enumerable) ? JrJackson::Raw.generate(o) : JrJackson::Json.dump(o)
+    rescue => e
+      raise LogStash::Json::GeneratorError.new(e.message)
+    end
+
+    prefix = LogStash::Environment.jruby? ? "jruby" : "mri"
+    alias_method :load, "#{prefix}_load".to_sym
+    alias_method :dump, "#{prefix}_dump".to_sym
+
+  end
+end
diff --git a/lib/logstash/kibana.rb b/lib/logstash/kibana.rb
new file mode 100644
index 00000000000..dc7cebf66a8
--- /dev/null
+++ b/lib/logstash/kibana.rb
@@ -0,0 +1,113 @@
+# encoding: utf-8
+require "rack/handler/ftw" # gem ftw
+require "ftw" # gem ftw
+require "sinatra/base" # gem sinatra
+require "optparse"
+require "mime/types"
+
+class Rack::Handler::FTW
+  alias_method :handle_connection_, :handle_connection
+  def handle_connection(connection)
+    #require "pry"; binding.pry
+    return handle_connection_(connection)
+  end
+end
+
+module LogStash::Kibana
+  class App < Sinatra::Base
+    set :logging, true
+
+    use Rack::CommonLogger
+    use Rack::ShowExceptions
+
+    get "/" do
+      redirect "index.html"
+    end
+    
+    # Sinatra has problems serving static files from 
+    # jar files, so let's hack this by hand.
+    #set :public, "#{File.dirname(__FILE__)}/public"
+    get "/config.js" do static_file end
+    get "/index.html" do static_file end
+    get "/app/*" do static_file end
+    get "/css/*" do static_file end
+    get "/font/*" do static_file end
+    get "/img/*" do static_file end
+    get "/vendor/*" do static_file end
+
+    def static_file
+      # request.path_info is the full path of the request.
+      docroot =  File.expand_path(File.join(File.dirname(__FILE__), "../../vendor/kibana"))
+      path = File.join(docroot, *request.path_info.split("/"))
+      if File.exists?(path)
+        ext = path.split(".").last
+        content_type MIME::Types.type_for(ext).first.to_s
+        body File.new(path, "r").read
+      else
+        status 404
+        content_type "text/plain"
+        body "File not found: #{path}"
+      end
+    end # def static_file
+  end # class App
+
+  class Runner
+    Settings = Struct.new(:logfile, :address, :port, :backend)
+
+    public
+    def run(args)
+      settings = Settings.new
+      settings.address = "0.0.0.0"
+      settings.port = 9292
+      settings.backend = "localhost"
+
+      progname = File.basename($0)
+
+      opts = OptionParser.new do |opts|
+        opts.banner = "Usage: #{progname} [options]"
+        opts.on("-a", "--address ADDRESS", "Address on which to start webserver. Default is 0.0.0.0.") do |address|
+          settings.address = address
+        end
+
+        opts.on("-p", "--port PORT", "Port on which to start webserver. Default is 9292.") do |port|
+          settings.port = port.to_i
+        end
+
+        #opts.on("-b", "--backend host",
+                #"The backend host to use. Default is 'localhost'") do |host|
+          #settings.backend = host
+        #end
+      end
+
+      begin
+        args = opts.parse(args)
+      rescue SystemExit
+        # if you ask for --help, optparse will exit.
+        # capture it and return normally
+        return []
+      end
+
+      @thread = Thread.new do
+        Cabin::Channel.get.info("Starting web server", :settings => settings)
+        ftw = Rack::Handler::FTW.new(LogStash::Kibana::App.new,
+                               :Host => settings.address,
+                               :Port => settings.port)
+        trap_id = Stud::trap("INT") do
+          puts "Stopping web..."
+          ftw.stop rescue nil
+          raise SystemExit
+        end
+
+        ftw.run
+      end
+
+      return args
+    end # def run
+
+    public
+    def wait
+      @thread.join if @thread
+      return 0
+    end # def wait
+  end
+end
diff --git a/lib/logstash/loadlibs.rb b/lib/logstash/loadlibs.rb
deleted file mode 100644
index 05b4b76efd4..00000000000
--- a/lib/logstash/loadlibs.rb
+++ /dev/null
@@ -1,8 +0,0 @@
-jarpath = File.join(File.dirname(__FILE__), "../../vendor/**/*.jar")
-Dir[jarpath].each do |jar|
-  if $DEBUG
-    puts "Loading #{jar}"
-  end
-  require jar
-end
-
diff --git a/lib/logstash/logging.rb b/lib/logstash/logging.rb
index e3365d24909..175891d8969 100644
--- a/lib/logstash/logging.rb
+++ b/lib/logstash/logging.rb
@@ -1,91 +1,89 @@
+# encoding: utf-8
 require "logstash/namespace"
+require "cabin"
 require "logger"
 
-class LogStash::Logger < Logger
-  # Try to load awesome_print, if it fails, log it later
-  # but otherwise we will continue to operate as normal.
-  begin 
-    require "ap"
-    @@have_awesome_print = true
-  rescue LoadError => e
-    @@have_awesome_print = false
-    @@notify_awesome_print_load_failed = e
-  end
+class LogStash::Logger
+  attr_accessor :target
 
   public
   def initialize(*args)
-    super(*args)
-    @formatter = LogStash::Logger::Formatter.new
+    super()
+
+    #self[:program] = File.basename($0)
+    #subscribe(::Logger.new(*args))
+    @target = args[0]
+    @channel = Cabin::Channel.get(LogStash)
+
+    # lame hack until cabin's smart enough not to doubley-subscribe something.
+    # without this subscription count check, running the test suite
+    # causes Cabin to subscribe to STDOUT maaaaaany times.
+    subscriptions = @channel.instance_eval { @subscribers.count }
+    @channel.subscribe(@target) unless subscriptions > 0
 
     # Set default loglevel to WARN unless $DEBUG is set (run with 'ruby -d')
-    self.level = $DEBUG ? Logger::DEBUG: Logger::INFO
+    @level = $DEBUG ? :debug : :warn
     if ENV["LOGSTASH_DEBUG"]
-      self.level = Logger::DEBUG
+      @level = :debug
     end
 
-    @formatter.progname = self.progname = File.basename($0)
-
-    # Conditional support for awesome_print
-    if !@@have_awesome_print && @@notify_awesome_print_load_failed
-      debug [ "awesome_print not found, falling back to Object#inspect." \
-              "If you want prettier log output, run 'gem install "\
-              "awesome_print'", 
-              { :exception => @@notify_awesome_print_load_failed }]
-
-      # Only show this once.
-      @@notify_awesome_print_load_failed = nil
-    end
+    # Direct metrics elsewhere.
+    @channel.metrics.channel = Cabin::Channel.new
   end # def initialize
 
-  public
-  def level=(level)
-    super(level)
-    @formatter.level = level
-  end # def level=
-end # class LogStash::Logger
+  # Delegation
+  def level=(value) @channel.level = value; end
+  def debug(*args); @channel.debug(*args); end
+  def debug?(*args); @channel.debug?(*args); end
+  def info(*args); @channel.info(*args); end
+  def info?(*args); @channel.info?(*args); end
+  def warn(*args); @channel.warn(*args); end
+  def warn?(*args); @channel.warn?(*args); end
+  def error(*args); @channel.error(*args); end
+  def error?(*args); @channel.error?(*args); end
+  def fatal(*args); @channel.fatal(*args); end
+  def fatal?(*args); @channel.fatal?(*args); end
 
-# Implement a custom Logger::Formatter that uses awesome_inspect on non-strings.
-class LogStash::Logger::Formatter < Logger::Formatter
-  attr_accessor :level
-  attr_accessor :progname
+  def self.setup_log4j(logger)
+    require "java"
 
-  public
-  def call(severity, timestamp, who, object)
-    # override progname to be the caller if the log level threshold is DEBUG
-    # We only do this if the logger level is DEBUG because inspecting the
-    # stack and doing extra string manipulation can have performance impacts
-    # under high logging rates.
-    if @level == Logger::DEBUG
-      # callstack inspection, include our caller
-      # turn this: "/usr/lib/ruby/1.8/irb/workspace.rb:52:in `irb_binding'"
-      # into this: ["/usr/lib/ruby/1.8/irb/workspace.rb", "52", "irb_binding"]
-      #
-      # caller[3] is actually who invoked the Logger#<type>
-      # This only works if you use the severity methods
-      path, line, method = caller[3].split(/(?::in `|:|')/)
-      # Trim RUBYLIB path from 'file' if we can
-      #whence = $:.select { |p| path.start_with?(p) }[0]
-      whence = $:.detect { |p| path.start_with?(p) }
-      if !whence
-        # We get here if the path is not in $:
-        file = path
-      else
-        file = path[whence.length + 1..-1]
-      end
-      who = "#{file}:#{line}##{method}"
-    end
+    properties = java.util.Properties.new
+    log4j_level = "WARN"
+    case logger.level
+      when :debug
+        log4j_level = "DEBUG"
+      when :info
+        log4j_level = "INFO"
+      when :warn
+        log4j_level = "WARN"
+    end # case level
+    properties.setProperty("log4j.rootLogger", "#{log4j_level},logstash")
 
-    # Log like normal if we got a string.
-    if object.is_a?(String)
-      super(severity, timestamp, who, object)
-    else
-      # If we logged an object, use .awesome_inspect (or just .inspect)
-      # to stringify it for higher sanity logging.
-      if object.respond_to?(:awesome_inspect)
-        super(severity, timestamp, who, object.awesome_inspect)
+    # TODO(sissel): This is a shitty hack to work around the fact that
+    # LogStash::Logger isn't used anymore. We should fix that.
+    target = logger.instance_eval { @subscribers }.values.first.instance_eval { @io }
+    case target
+      when STDOUT
+        properties.setProperty("log4j.appender.logstash",
+                      "org.apache.log4j.ConsoleAppender")
+        properties.setProperty("log4j.appender.logstash.Target", "System.out")
+      when STDERR
+        properties.setProperty("log4j.appender.logstash",
+                      "org.apache.log4j.ConsoleAppender")
+        properties.setProperty("log4j.appender.logstash.Target", "System.err")
       else
-        super(severity, timestamp, who, object.inspect)
-      end
-    end
-  end # def call
-end # class LogStash::Logger::Formatter
+        properties.setProperty("log4j.appender.logstash",
+                      "org.apache.log4j.FileAppender")
+        properties.setProperty("log4j.appender.logstash.File", target.path)
+    end # case target
+
+    properties.setProperty("log4j.appender.logstash.layout",
+                  "org.apache.log4j.PatternLayout")
+    properties.setProperty("log4j.appender.logstash.layout.conversionPattern",
+                  "log4j, [%d{yyyy-MM-dd}T%d{HH:mm:ss.SSS}] %5p: %c: %m%n")
+
+    org.apache.log4j.LogManager.resetConfiguration
+    org.apache.log4j.PropertyConfigurator.configure(properties)
+    logger.debug("log4j java properties setup", :log4j_level => log4j_level)
+  end
+end # class LogStash::Logger
diff --git a/lib/logstash/monkeypatches-for-debugging.rb b/lib/logstash/monkeypatches-for-debugging.rb
new file mode 100644
index 00000000000..683c0a106bd
--- /dev/null
+++ b/lib/logstash/monkeypatches-for-debugging.rb
@@ -0,0 +1,47 @@
+# encoding: utf-8
+if $DEBUGLIST.include?("require")
+  ROOT = File.dirname(__FILE__)
+  module Kernel
+    alias_method :require_debug, :require
+
+    def require(path)
+      start = Time.now
+      result = require_debug(path)
+      duration = Time.now - start
+
+      origin = caller[1]
+      if origin =~ /rubygems\/custom_require/
+        origin = caller[3]
+        if origin.nil?
+          STDERR.puts "Unknown origin"
+          STDERR.puts caller.join("\n")
+        end
+      end
+      origin = origin.gsub(/:[0-9]+:in .*/, "") if origin
+
+      # Only print require() calls that did actual work.
+      # require() returns true on load, false if already loaded.
+      if result
+        source = caller[0]
+        #p source.include?("/lib/polyglot.rb:63:in `require'") => source
+        if source.include?("/lib/polyglot.rb:63:in `require'")
+          source = caller[1]
+        end
+
+        #target = $LOADED_FEATURES.grep(/#{path}/).first
+        #puts path
+        #puts caller.map { |c| "  #{c}" }.join("\n")
+        #fontsize = [10, duration * 48].max
+        puts "#{duration},#{path},#{source}"
+      end
+      #puts caller.map { |c| " => #{c}" }.join("\n")
+    end
+
+    alias_method :load_debug, :load
+
+    def load(path)
+      puts "load(\"#{path}\")"
+      return load_debug(path)
+    end
+  end
+end
diff --git a/lib/logstash/multiqueue.rb b/lib/logstash/multiqueue.rb
index 36753cd6a13..237105662f8 100644
--- a/lib/logstash/multiqueue.rb
+++ b/lib/logstash/multiqueue.rb
@@ -1,18 +1,30 @@
+# encoding: utf-8
 require "logstash/namespace"
-require "logstash/logging"
+require "cabin"
 
 class LogStash::MultiQueue
+  attr_accessor :logger
+
   public
   def initialize(*queues)
-    @logger = LogStash::Logger.new(STDOUT)
+    @logger = Cabin::Channel.get(LogStash)
     @mutex = Mutex.new
     @queues = queues
   end # def initialize
 
+  public
+  def logger=(_logger)
+    @logger = _logger
+
+    # Set the logger for all known queues, too.
+    @queues.each do |q|
+      q.logger = _logger
+    end
+  end # def logger=
+
   # Push an object to all queues.
   public
   def push(object)
-    #@logger.info "*** Pushing object into MultiQueue: #{object}"
     @queues.each { |q| q.push(object) }
   end # def push
   alias :<< :push
@@ -27,8 +39,15 @@ def add_queue(queue)
     end
   end # def add_queue
 
+  public
+  def remove_queue(queue)
+    @mutex.synchronize do
+      @queues.delete(queue)
+    end
+  end # def remove_queue
+
   public
   def size
     return @queues.collect { |q| q.size }
   end # def size
-end
+end # class LogStash::MultiQueue
diff --git a/lib/logstash/namespace.rb b/lib/logstash/namespace.rb
index b4ec120247a..93f426b0fd7 100644
--- a/lib/logstash/namespace.rb
+++ b/lib/logstash/namespace.rb
@@ -1,4 +1,5 @@
-$: << File.join(File.dirname(__FILE__), "..", "..", "vendor", "bundle")
+# encoding: utf-8
+#$: << File.join(File.dirname(__FILE__), "..", "..", "vendor", "bundle")
 
 module LogStash
   module Inputs; end
@@ -8,4 +9,9 @@ module Search; end
   module Config; end
   module File; end
   module Web; end
+  module Util; end
+  module PluginMixins; end
+  module PluginManager; end
+
+  SHUTDOWN = :shutdown
 end # module LogStash
diff --git a/lib/logstash/outputs.rb b/lib/logstash/outputs.rb
deleted file mode 100644
index 7abfe93fc1e..00000000000
--- a/lib/logstash/outputs.rb
+++ /dev/null
@@ -1,15 +0,0 @@
-require "logstash/namespace"
-require "uri"
-
-module LogStash::Outputs
-  public
-  def self.from_url(url)
-    uri = URI.parse(url)
-    # TODO(sissel): Add error handling
-    # TODO(sissel): Allow plugin paths
-    klass = uri.scheme.capitalize
-    file = uri.scheme
-    require "logstash/outputs/#{file}"
-    LogStash::Outputs.const_get(klass).new(uri)
-  end # def from_url
-end # module LogStash::Outputs
diff --git a/lib/logstash/outputs/amqp.rb b/lib/logstash/outputs/amqp.rb
deleted file mode 100644
index 06de7ae7b13..00000000000
--- a/lib/logstash/outputs/amqp.rb
+++ /dev/null
@@ -1,96 +0,0 @@
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-class LogStash::Outputs::Amqp < LogStash::Outputs::Base
-  MQTYPES = [ "fanout", "direct", "topic" ]
-
-  config_name "amqp"
-
-  # Your amqp server address
-  config :host, :validate => :string, :required => true
-
-  # The AMQP port to connect on
-  config :port, :validate => :number, :default => 5672
-
-  # Your amqp username
-  config :user, :validate => :string, :default => "guest"
-
-  # Your amqp password
-  config :password, :validate => :password, :default => "guest"
-
-  # The exchange type (fanout, topic, direct)
-  config :exchange_type, :validate => [ "fanout", "direct", "topic"], :required => true
-
-  # The name of the exchange
-  config :name, :validate => :string, :required => true
-
-  # The vhost to use
-  config :vhost, :validate => :string, :default => "/"
-
-  # Is this exchange durable?
-  config :durable, :validate => :boolean, :default => false
-
-  # Enable or disable debugging
-  config :debug, :validate => :boolean, :default => false
-
-  public
-  def register
-    require "bunny" # rubygem 'bunny'
-    if !MQTYPES.include?(@exchange_type)
-      raise "Invalid exchange_type, #{@exchange_type.inspect}, must be one of #{MQTYPES.join(", ")}"
-    end
-
-    @logger.info("Registering output #{to_s}")
-    connect
-  end # def register
-
-  public
-  def connect
-    amqpsettings = {
-      :vhost => @vhost,
-      :host => @host,
-      :port => @port
-    }
-    amqpsettings[:user] = @user if @user
-    amqpsettings[:pass] = @password.value if @password
-    amqpsettings[:logging] = @debug
-    loop do
-      @logger.debug(["Connecting to AMQP", amqpsettings, @exchange_type, @name])
-      @bunny = Bunny.new(amqpsettings)
-      begin
-        @bunny.start
-        break # success
-      rescue Bunny::ServerDownError => e
-        @logger.error("AMQP connection error, will reconnect: #{e}")
-        sleep(1)
-      end
-    end # loop
-    @target = @bunny.exchange(@name, :type => @exchange_type.to_sym, :durable => @durable)
-  end # def connect
-
-  public
-  def receive(event)
-    loop do
-      @logger.debug(["Sending event", { :destination => to_s, :event => event }])
-      begin
-        @target.publish(event.to_json)
-        break;
-      rescue *[Bunny::ServerDownError, Errno::ECONNRESET] => e
-        @logger.error("AMQP connection error, will reconnect: #{e}")
-        connect
-        retry
-      end
-    end # loop do
-  end # def receive
-
-  # This is used by the ElasticSearch AMQP/River output.
-  public
-  def receive_raw(raw)
-    @target.publish(raw)
-  end # def receive_raw
-
-  public
-  def to_s
-    return "amqp://#{@user}@#{@host}:#{@port}#{@vhost}/#{@exchange_type}/#{@name}"
-  end
-end # class LogStash::Outputs::Amqp
diff --git a/lib/logstash/outputs/base.rb b/lib/logstash/outputs/base.rb
index d646e651994..9fedbf10818 100644
--- a/lib/logstash/outputs/base.rb
+++ b/lib/logstash/outputs/base.rb
@@ -1,20 +1,52 @@
+# encoding: utf-8
 require "cgi"
 require "logstash/event"
 require "logstash/logging"
+require "logstash/plugin"
 require "logstash/namespace"
 require "logstash/config/mixin"
 require "uri"
 
-class LogStash::Outputs::Base
+class LogStash::Outputs::Base < LogStash::Plugin
   include LogStash::Config::Mixin
 
-  attr_accessor :logger
-
   config_name "output"
 
+  # The type to act on. If a type is given, then this output will only
+  # act on messages with the same type. See any input plugin's "type"
+  # attribute for more.
+  # Optional.
+  config :type, :validate => :string, :default => "", :deprecated => "You can achieve this same behavior with the new conditionals, like: `if [type] == \"sometype\" { %PLUGIN% { ... } }`."
+
+  # Only handle events with all of these tags.  Note that if you specify
+  # a type, the event must also match that type.
+  # Optional.
+  config :tags, :validate => :array, :default => [], :deprecated => "You can achieve similar behavior with the new conditionals, like: `if \"sometag\" in [tags] { %PLUGIN% { ... } }`"
+
+  # Only handle events without any of these tags. Note this check is additional to type and tags.
+  config :exclude_tags, :validate => :array, :default => [], :deprecated => "You can achieve similar behavior with the new conditionals, like: `if !(\"sometag\" in [tags]) { %PLUGIN% { ... } }`"
+
+  # The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.
+  config :codec, :validate => :codec, :default => "plain"
+
+  # The number of workers to use for this output.
+  # Note that this setting may not be useful for all outputs.
+  config :workers, :validate => :number, :default => 1
+
   public
-  def initialize(params)
-    @logger = LogStash::Logger.new(STDOUT)
+  def workers_not_supported(message=nil)
+    return if @workers == 1
+    if message
+      @logger.warn(I18n.t("logstash.pipeline.output-worker-unsupported-with-message", :plugin => self.class.config_name, :worker_count => @workers, :message => message))
+    else
+      @logger.warn(I18n.t("logstash.pipeline.output-worker-unsupported", :plugin => self.class.config_name, :worker_count => @workers))
+    end
+    @workers = 1
+  end
+
+  public
+  def initialize(params={})
+    super
     config_init(params)
   end
 
@@ -27,4 +59,62 @@ def register
   def receive(event)
     raise "#{self.class}#receive must be overidden"
   end # def receive
+
+  public
+  def worker_setup
+    return unless @workers > 1
+
+    define_singleton_method(:handle, method(:handle_worker))
+    @worker_queue = SizedQueue.new(20)
+
+    @worker_threads = @workers.times do |i|
+      Thread.new(original_params, @worker_queue) do |params, queue|
+        LogStash::Util::set_thread_name(">#{self.class.config_name}.#{i}")
+        worker_params = params.merge("workers" => 1, "codec" => @codec.clone)
+        worker_plugin = self.class.new(worker_params)
+        worker_plugin.register
+        while true
+          event = queue.pop
+          worker_plugin.handle(event)
+        end
+      end
+    end
+  end
+
+  public
+  def handle(event)
+    receive(event)
+  end # def handle
+  
+  def handle_worker(event)
+    @worker_queue.push(event)
+  end
+
+  private
+  def output?(event)
+    if !@type.empty?
+      if event["type"] != @type
+        @logger.debug? and @logger.debug(["outputs/#{self.class.name}: Dropping event because type doesn't match #{@type}", event])
+        return false
+      end
+    end
+
+    if !@tags.empty?
+      return false if !event["tags"]
+      @include_method = :any?
+      if !@tags.send(@include_method) {|tag| event["tags"].include?(tag)}
+        @logger.debug? and @logger.debug("outputs/#{self.class.name}: Dropping event because tags don't match #{@tags.inspect}", event)
+        return false
+      end
+    end
+
+    if !@exclude_tags.empty? && event["tags"]
+      if @exclude_tags.send(@exclude_method) {|tag| event["tags"].include?(tag)}
+        @logger.debug? and @logger.debug("outputs/#{self.class.name}: Dropping event because tags contains excluded tags: #{exclude_tags.inspect}", event)
+        return false
+      end
+    end
+
+    return true
+  end
 end # class LogStash::Outputs::Base
diff --git a/lib/logstash/outputs/beanstalk.rb b/lib/logstash/outputs/beanstalk.rb
deleted file mode 100644
index 536404be4ce..00000000000
--- a/lib/logstash/outputs/beanstalk.rb
+++ /dev/null
@@ -1,40 +0,0 @@
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-class LogStash::Outputs::Beanstalk < LogStash::Outputs::Base
-
-  config_name "beanstalk"
-
-  # The address of the beanstalk server
-  config :host, :validate => :string, :required => true
-
-  # The port of your beanstalk server
-  config :port, :validate => :number, :default => 11300
-
-  # The name of the beanstalk tube
-  config :tube, :validate => :string, :required => true
-
-  # The message priority (see beanstalk docs)
-  config :priority, :validate => :number, :default => 65536
-
-  # The message delay (see beanstalk docs)
-  config :delay, :validate => :number, :default => 0
-
-  # TODO(sissel): Document this
-  # See beanstalk documentation
-  config :ttr, :validate => :number, :default => 300
-
-  public
-  def register
-    require "beanstalk-client"
-    # TODO(petef): support pools of beanstalkd servers
-    # TODO(petef): check for errors
-    @beanstalk = Beanstalk::Pool.new(["#{@host}:#{@port}"])
-    @beanstalk.use(@tube)
-  end # def register
-
-  public
-  def receive(event)
-    @beanstalk.put(event.to_json, @priority, @delay, @ttr)
-  end # def register
-end # class LogStash::Outputs::Beanstalk
diff --git a/lib/logstash/outputs/cloudwatch.rb b/lib/logstash/outputs/cloudwatch.rb
new file mode 100644
index 00000000000..bcd5e48a44a
--- /dev/null
+++ b/lib/logstash/outputs/cloudwatch.rb
@@ -0,0 +1,351 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+require "logstash/plugin_mixins/aws_config"
+
+# This output lets you aggregate and send metric data to AWS CloudWatch
+#
+# #### Summary:
+# This plugin is intended to be used on a logstash indexer agent (but that
+# is not the only way, see below.)  In the intended scenario, one cloudwatch
+# output plugin is configured, on the logstash indexer node, with just AWS API
+# credentials, and possibly a region and/or a namespace.  The output looks
+# for fields present in events, and when it finds them, it uses them to
+# calculate aggregate statistics.  If the `metricname` option is set in this
+# output, then any events which pass through it will be aggregated & sent to
+# CloudWatch, but that is not recommended.  The intended use is to NOT set the
+# metricname option here, and instead to add a `CW_metricname` field (and other
+# fields) to only the events you want sent to CloudWatch.
+#
+# When events pass through this output they are queued for background
+# aggregation and sending, which happens every minute by default.  The
+# queue has a maximum size, and when it is full aggregated statistics will be
+# sent to CloudWatch ahead of schedule. Whenever this happens a warning
+# message is written to logstash's log.  If you see this you should increase
+# the `queue_size` configuration option to avoid the extra API calls.  The queue
+# is emptied every time we send data to CloudWatch.
+#
+# Note: when logstash is stopped the queue is destroyed before it can be processed.
+# This is a known limitation of logstash and will hopefully be addressed in a
+# future version.
+#
+# #### Details:
+# There are two ways to configure this plugin, and they can be used in
+# combination: event fields & per-output defaults
+#
+# Event Field configuration...
+# You add fields to your events in inputs & filters and this output reads
+# those fields to aggregate events.  The names of the fields read are
+# configurable via the `field_*` options.
+#
+# Per-output defaults...
+# You set universal defaults in this output plugin's configuration, and
+# if an event does not have a field for that option then the default is
+# used.
+#
+# Notice, the event fields take precedence over the per-output defaults.
+#
+# At a minimum events must have a "metric name" to be sent to CloudWatch.
+# This can be achieved either by providing a default here OR by adding a
+# `CW_metricname` field. By default, if no other configuration is provided
+# besides a metric name, then events will be counted (Unit: Count, Value: 1)
+# by their metric name (either a default or from their `CW_metricname` field)
+#
+# Other fields which can be added to events to modify the behavior of this
+# plugin are, `CW_namespace`, `CW_unit`, `CW_value`, and 
+# `CW_dimensions`.  All of these field names are configurable in
+# this output.  You can also set per-output defaults for any of them.
+# See below for details.
+#
+# Read more about [AWS CloudWatch](http://aws.amazon.com/cloudwatch/),
+# and the specific of API endpoint this output uses,
+# [PutMetricData](http://docs.amazonwebservices.com/AmazonCloudWatch/latest/APIReference/API_PutMetricData.html)
+class LogStash::Outputs::CloudWatch < LogStash::Outputs::Base
+  include LogStash::PluginMixins::AwsConfig
+  
+  config_name "cloudwatch"
+  milestone 1
+
+  # Constants
+  # aggregate_key members
+  DIMENSIONS = "dimensions"
+  TIMESTAMP = "timestamp"
+  METRIC = "metric"
+  COUNT = "count"
+  UNIT = "unit"
+  SUM = "sum"
+  MIN = "min"
+  MAX = "max"
+  # Units
+  COUNT_UNIT = "Count"
+  NONE = "None"
+
+  # How often to send data to CloudWatch   
+  # This does not affect the event timestamps, events will always have their
+  # actual timestamp (to-the-minute) sent to CloudWatch.
+  #
+  # We only call the API if there is data to send.
+  #
+  # See the Rufus Scheduler docs for an [explanation of allowed values](https://github.com/jmettraux/rufus-scheduler#the-time-strings-understood-by-rufus-scheduler)
+  config :timeframe, :validate => :string, :default => "1m"
+
+  # How many events to queue before forcing a call to the CloudWatch API ahead of `timeframe` schedule   
+  # Set this to the number of events-per-timeframe you will be sending to CloudWatch to avoid extra API calls
+  config :queue_size, :validate => :number, :default => 10000
+
+  # The default namespace to use for events which do not have a `CW_namespace` field
+  config :namespace, :validate => :string, :default => "Logstash"
+
+  # The name of the field used to set a different namespace per event   
+  # Note: Only one namespace can be sent to CloudWatch per API call
+  # so setting different namespaces will increase the number of API calls
+  # and those cost money.
+  config :field_namespace, :validate => :string, :default => "CW_namespace"
+
+  # The default metric name to use for events which do not have a `CW_metricname` field.   
+  # Beware: If this is provided then all events which pass through this output will be aggregated and
+  # sent to CloudWatch, so use this carefully.  Furthermore, when providing this option, you
+  # will probably want to also restrict events from passing through this output using event
+  # type, tag, and field matching
+  config :metricname, :validate => :string
+
+  # The name of the field used to set the metric name on an event   
+  # The author of this plugin recommends adding this field to events in inputs &
+  # filters rather than using the per-output default setting so that one output
+  # plugin on your logstash indexer can serve all events (which of course had
+  # fields set on your logstash shippers.)
+  config :field_metricname, :validate => :string, :default => "CW_metricname"
+
+  VALID_UNITS = ["Seconds", "Microseconds", "Milliseconds", "Bytes",
+                 "Kilobytes", "Megabytes", "Gigabytes", "Terabytes",
+                 "Bits", "Kilobits", "Megabits", "Gigabits", "Terabits",
+                 "Percent", COUNT_UNIT, "Bytes/Second", "Kilobytes/Second",
+                 "Megabytes/Second", "Gigabytes/Second", "Terabytes/Second",
+                 "Bits/Second", "Kilobits/Second", "Megabits/Second",
+                 "Gigabits/Second", "Terabits/Second", "Count/Second", NONE]
+
+  # The default unit to use for events which do not have a `CW_unit` field   
+  # If you set this option you should probably set the "value" option along with it
+  config :unit, :validate => VALID_UNITS, :default => COUNT_UNIT
+
+  # The name of the field used to set the unit on an event metric   
+  config :field_unit, :validate => :string, :default => "CW_unit"
+
+  # The default value to use for events which do not have a `CW_value` field   
+  # If provided, this must be a string which can be converted to a float, for example...
+  #     "1", "2.34", ".5", and "0.67"
+  # If you set this option you should probably set the `unit` option along with it
+  config :value, :validate => :string, :default => "1"
+
+  # The name of the field used to set the value (float) on an event metric   
+  config :field_value, :validate => :string, :default => "CW_value"
+
+  # The default dimensions [ name, value, ... ] to use for events which do not have a `CW_dimensions` field   
+  config :dimensions, :validate => :hash
+
+  # The name of the field used to set the dimensions on an event metric   
+  # The field named here, if present in an event, must have an array of
+  # one or more key & value pairs, for example...
+  #     add_field => [ "CW_dimensions", "Environment", "CW_dimensions", "prod" ]
+  # or, equivalently...
+  #     add_field => [ "CW_dimensions", "Environment" ]
+  #     add_field => [ "CW_dimensions", "prod" ]
+  config :field_dimensions, :validate => :string, :default => "CW_dimensions"
+
+  public
+  def aws_service_endpoint(region)
+    return {
+        :cloud_watch_endpoint => "monitoring.#{region}.amazonaws.com"
+    }
+  end
+  
+  public
+  def register
+    require "thread"
+    require "rufus/scheduler"
+    require "aws"
+
+    @cw = AWS::CloudWatch.new(aws_options_hash)
+
+    @event_queue = SizedQueue.new(@queue_size)
+    @scheduler = Rufus::Scheduler.start_new
+    @job = @scheduler.every @timeframe do
+      @logger.info("Scheduler Activated")
+      publish(aggregate({}))
+    end
+  end # def register
+
+  public
+  def receive(event)
+    return unless output?(event)
+
+    if event == LogStash::SHUTDOWN
+      job.trigger()
+      job.unschedule()
+      @logger.info("CloudWatch aggregator thread shutdown.")
+      finished
+      return
+    end
+
+    return unless (event[@field_metricname] || @metricname)
+
+    if (@event_queue.length >= @event_queue.max)
+      @job.trigger
+      @logger.warn("Posted to AWS CloudWatch ahead of schedule.  If you see this often, consider increasing the cloudwatch queue_size option.")
+    end
+
+    @logger.info("Queueing event", :event => event)
+    @event_queue << event
+  end # def receive
+
+  private
+  def publish(aggregates)
+    aggregates.each do |namespace, data|
+      @logger.info("Namespace, data: ", :namespace => namespace, :data => data)
+      metric_data = []
+      data.each do |aggregate_key, stats|
+        new_data = {
+            :metric_name => aggregate_key[METRIC],
+            :timestamp => aggregate_key[TIMESTAMP],
+            :unit => aggregate_key[UNIT],
+            :statistic_values => {
+                :sample_count => stats[COUNT],
+                :sum => stats[SUM],
+                :minimum => stats[MIN],
+                :maximum => stats[MAX],
+            }
+        }
+        dims = aggregate_key[DIMENSIONS]
+        if (dims.is_a?(Array) && dims.length > 0 && (dims.length % 2) == 0)
+          new_data[:dimensions] = Array.new
+          i = 0
+          while (i < dims.length)
+            new_data[:dimensions] << {:name => dims[i], :value => dims[i+1]}
+            i += 2
+          end
+        end
+        metric_data << new_data
+      end # data.each
+
+      begin
+        @cw.put_metric_data(
+            :namespace => namespace,
+            :metric_data => metric_data
+        )
+        @logger.info("Sent data to AWS CloudWatch OK", :namespace => namespace, :metric_data => metric_data)
+      rescue Exception => e
+        @logger.warn("Failed to send to AWS CloudWatch", :exception => e, :namespace => namespace, :metric_data => metric_data)
+        break
+      end
+    end # aggregates.each
+    return aggregates
+  end# def publish
+
+  private
+  def aggregate(aggregates)
+    @logger.info("QUEUE SIZE ", :queuesize => @event_queue.size)
+    while !@event_queue.empty? do
+      begin
+        count(aggregates, @event_queue.pop(true))
+      rescue Exception => e
+        @logger.warn("Exception!  Breaking count loop", :exception => e)
+        break
+      end
+    end
+    return aggregates
+  end # def aggregate
+
+  private
+  def count(aggregates, event)
+    # If the event doesn't declare a namespace, use the default
+    fnamespace = field(event, @field_namespace)
+    namespace = (fnamespace ? fnamespace : event.sprintf(@namespace))
+
+    funit = field(event, @field_unit)
+    unit = (funit ? funit : event.sprintf(@unit))
+
+    fvalue = field(event, @field_value)
+    value = (fvalue ? fvalue : event.sprintf(@value))
+
+    # We may get to this point with valid Units but missing value.  Send zeros.
+    val = (!value) ? 0.0 : value.to_f
+
+    # Event provides exactly one (but not both) of value or unit
+    if ( (fvalue == nil) ^ (funit == nil) )
+      @logger.warn("Likely config error: event has one of #{@field_value} or #{@field_unit} fields but not both.", :event => event)
+    end
+
+    # If Unit is still not set or is invalid warn about misconfiguration & use NONE
+    if (!VALID_UNITS.include?(unit))
+      unit = NONE
+      @logger.warn("Likely config error: invalid or missing Units (#{unit.to_s}), using '#{NONE}' instead", :event => event)
+    end
+
+    if (!aggregates[namespace])
+      aggregates[namespace] = {}
+    end
+
+    dims = event[@field_dimensions]
+    if (dims) # event provides dimensions
+              # validate the structure
+      if (!dims.is_a?(Array) || dims.length == 0 || (dims.length % 2) != 0)
+        @logger.warn("Likely config error: CloudWatch dimensions field (#{dims.to_s}) found which is not a positive- & even-length array.  Ignoring it.", :event => event)
+        dims = nil
+      end
+              # Best case, we get here and exit the conditional because dims...
+              # - is an array
+              # - with positive length
+              # - and an even number of elements
+    elsif (@dimensions.is_a?(Hash)) # event did not provide dimensions, but the output has been configured with a default
+      dims = @dimensions.flatten.map{|d| event.sprintf(d)} # into the kind of array described just above
+    else
+      dims = nil
+    end
+
+    fmetric = field(event, @field_metricname)
+    aggregate_key = {
+        METRIC => (fmetric ? fmetric : event.sprintf(@metricname)),
+        DIMENSIONS => dims,
+        UNIT => unit,
+        TIMESTAMP => event.sprintf("%{+YYYY-MM-dd'T'HH:mm:00Z}")
+    }
+
+    if (!aggregates[namespace][aggregate_key])
+      aggregates[namespace][aggregate_key] = {}
+    end
+
+    if (!aggregates[namespace][aggregate_key][MAX] || val > aggregates[namespace][aggregate_key][MAX])
+      aggregates[namespace][aggregate_key][MAX] = val
+    end
+
+    if (!aggregates[namespace][aggregate_key][MIN] || val < aggregates[namespace][aggregate_key][MIN])
+      aggregates[namespace][aggregate_key][MIN] = val
+    end
+
+    if (!aggregates[namespace][aggregate_key][COUNT])
+      aggregates[namespace][aggregate_key][COUNT] = 1
+    else
+      aggregates[namespace][aggregate_key][COUNT] += 1
+    end
+
+    if (!aggregates[namespace][aggregate_key][SUM])
+      aggregates[namespace][aggregate_key][SUM] = val
+    else
+      aggregates[namespace][aggregate_key][SUM] += val
+    end
+  end # def count
+
+  private
+  def field(event, fieldname)
+    if !event[fieldname]
+      return nil
+    else
+      if event[fieldname].is_a?(Array)
+        return event[fieldname][0]
+      else
+        return event[fieldname]
+      end
+    end
+  end # def field
+
+end # class LogStash::Outputs::CloudWatch
diff --git a/lib/logstash/outputs/csv.rb b/lib/logstash/outputs/csv.rb
new file mode 100644
index 00000000000..42daa155bce
--- /dev/null
+++ b/lib/logstash/outputs/csv.rb
@@ -0,0 +1,51 @@
+require "csv"
+require "logstash/namespace"
+require "logstash/outputs/file"
+require "logstash/json"
+
+# CSV output.
+#
+# Write events to disk in CSV or other delimited format
+# Based on the file output, many config values are shared
+# Uses the Ruby csv library internally
+class LogStash::Outputs::CSV < LogStash::Outputs::File
+
+  config_name "csv"
+  milestone 1
+
+  # The field names from the event that should be written to the CSV file.
+  # Fields are written to the CSV in the same order as the array.
+  # If a field does not exist on the event, an empty string will be written.
+  # Supports field reference syntax eg: `fields => ["field1", "[nested][field]"]`.
+  config :fields, :validate => :array, :required => true
+
+  # Options for CSV output. This is passed directly to the Ruby stdlib to\_csv function.
+  # Full documentation is available here: [http://ruby-doc.org/stdlib-2.0.0/libdoc/csv/rdoc/index.html].
+  # A typical use case would be to use alternative column or row seperators eg: `csv_options => {"col_sep" => "\t" "row_sep" => "\r\n"}` gives tab seperated data with windows line endings
+  config :csv_options, :validate => :hash, :required => false, :default => Hash.new
+
+  public
+  def register
+    super
+    @csv_options = Hash[@csv_options.map{|(k, v)|[k.to_sym, v]}]
+  end
+
+  public
+  def receive(event)
+    return unless output?(event)
+    path = event.sprintf(@path)
+    fd = open(path)
+    csv_values = @fields.map {|name| get_value(name, event)}
+    fd.write(csv_values.to_csv(@csv_options))
+
+    flush(fd)
+    close_stale_files
+  end #def receive
+
+  private
+  def get_value(name, event)
+    val = event[name]
+    val.is_a?(Hash) ? LogStash::Json.dump(val) : val
+  end
+end # class LogStash::Outputs::CSV
+
diff --git a/lib/logstash/outputs/elasticsearch.rb b/lib/logstash/outputs/elasticsearch.rb
index b5eac0c9918..a9b30dabf60 100644
--- a/lib/logstash/outputs/elasticsearch.rb
+++ b/lib/logstash/outputs/elasticsearch.rb
@@ -1,211 +1,366 @@
+# encoding: utf-8
 require "logstash/namespace"
+require "logstash/environment"
 require "logstash/outputs/base"
+require "logstash/json"
+require "stud/buffer"
+require "socket" # for Socket.gethostname
 
-# TODO(sissel): Remove old cruft from pre-jruby
-# TODO(sissel): Support river again?
-class LogStash::Outputs::Elasticsearch < LogStash::Outputs::Base
+# This output lets you store logs in Elasticsearch and is the most recommended
+# output for Logstash. If you plan on using the Kibana web interface, you'll
+# need to use this output.
+#
+#   *VERSION NOTE*: Your Elasticsearch cluster must be running Elasticsearch
+#   1.0.0 or later.
+#
+# If you want to set other Elasticsearch options that are not exposed directly
+# as configuration options, there are two methods:
+#
+# * Create an `elasticsearch.yml` file in the $PWD of the Logstash process
+# * Pass in es.* java properties (java -Des.node.foo= or ruby -J-Des.node.foo=)
+#
+# With the default `protocol` setting ("node"), this plugin will join your
+# Elasticsearch cluster as a client node, so it will show up in Elasticsearch's
+# cluster status.
+#
+# You can learn more about Elasticsearch at <http://www.elasticsearch.org>
+#
+# ## Operational Notes
+#
+# If using the default `protocol` setting ("node"), your firewalls might need
+# to permit port 9300 in *both* directions (from Logstash to Elasticsearch, and
+# Elasticsearch to Logstash)
+class LogStash::Outputs::ElasticSearch < LogStash::Outputs::Base
+  include Stud::Buffer
 
-  # http://host/index/type
   config_name "elasticsearch"
-
-  # ElasticSearch server name. This is optional if your server is discoverable.
-  config :host, :validate => :string
+  milestone 3
 
   # The index to write events to. This can be dynamic using the %{foo} syntax.
-  # The default value will partition your indeces by day so you can more easily
+  # The default value will partition your indices by day so you can more easily
   # delete old data or only search specific date ranges.
+  # Indexes may not contain uppercase characters.
   config :index, :validate => :string, :default => "logstash-%{+YYYY.MM.dd}"
 
-  # The type to write events to. Generally you should try to write only similar
-  # events to the same 'type'. String expansion '%{foo}' works here.
-  config :type, :validate => :string, :default => "%{@type}"
+  # The index type to write events to. Generally you should try to write only
+  # similar events to the same 'type'. String expansion '%{foo}' works here.
+  config :index_type, :validate => :string
+
+  # Starting in Logstash 1.3 (unless you set option "manage_template" to false)
+  # a default mapping template for Elasticsearch will be applied, if you do not
+  # already have one set to match the index pattern defined (default of
+  # "logstash-%{+YYYY.MM.dd}"), minus any variables.  For example, in this case
+  # the template will be applied to all indices starting with logstash-*
+  #
+  # If you have dynamic templating (e.g. creating indices based on field names)
+  # then you should set "manage_template" to false and use the REST API to upload
+  # your templates manually.
+  config :manage_template, :validate => :boolean, :default => true
+
+  # This configuration option defines how the template is named inside Elasticsearch.
+  # Note that if you have used the template management features and subsequently
+  # change this, you will need to prune the old template manually, e.g.
+  # curl -XDELETE <http://localhost:9200/_template/OldTemplateName?pretty>
+  # where OldTemplateName is whatever the former setting was.
+  config :template_name, :validate => :string, :default => "logstash"
+
+  # You can set the path to your own template here, if you so desire.
+  # If not set, the included template will be used.
+  config :template, :validate => :path
 
-  # The name of your cluster if you set it on the ElasticSearch side. Useful
+  # Overwrite the current template with whatever is configured
+  # in the template and template_name directives.
+  config :template_overwrite, :validate => :boolean, :default => false
+
+  # The document ID for the index. Useful for overwriting existing entries in
+  # Elasticsearch with the same ID.
+  config :document_id, :validate => :string, :default => nil
+
+  # The name of your cluster if you set it on the Elasticsearch side. Useful
   # for discovery.
   config :cluster, :validate => :string
 
-  # The name/address of the host to use for ElasticSearch unicast discovery
+  # The hostname or IP address of the host to use for Elasticsearch unicast discovery
   # This is only required if the normal multicast/cluster discovery stuff won't
   # work in your environment.
-  config :host, :validate => :string
+  #
+  #     "127.0.0.1"
+  #     ["127.0.0.1:9300","127.0.0.2:9300"]
+  config :host, :validate => :array
+
+  # The port for Elasticsearch transport to use.
+  #
+  # If you do not set this, the following defaults are used:
+  # * `protocol => http` - port 9200
+  # * `protocol => transport` - port 9300-9305
+  # * `protocol => node` - port 9300-9305
+  config :port, :validate => :string
+
+  # The name/address of the host to bind to for Elasticsearch clustering
+  config :bind_host, :validate => :string
+
+  # This is only valid for the 'node' protocol.
+  #
+  # The port for the node to listen on.
+  config :bind_port, :validate => :number
+
+  # Run the Elasticsearch server embedded in this process.
+  # This option is useful if you want to run a single Logstash process that
+  # handles log processing and indexing; it saves you from needing to run
+  # a separate Elasticsearch process.
+  config :embedded, :validate => :boolean, :default => false
+
+  # If you are running the embedded Elasticsearch server, you can set the http
+  # port it listens on here; it is not common to need this setting changed from
+  # default.
+  config :embedded_http_port, :validate => :string, :default => "9200-9300"
+
+  # This setting no longer does anything. It exists to keep config validation
+  # from failing. It will be removed in future versions.
+  config :max_inflight_requests, :validate => :number, :default => 50, :deprecated => true
+
+  # The node name Elasticsearch will use when joining a cluster.
+  #
+  # By default, this is generated internally by the ES client.
+  config :node_name, :validate => :string
 
-  # The port for ElasticSearch transport to use. This is *not* the ElasticSearch
-  # REST API port (normally 9200).
-  config :port, :validate => :number, :default => 9300
+  # This plugin uses the bulk index api for improved indexing performance.
+  # To make efficient bulk api calls, we will buffer a certain number of
+  # events before flushing that out to Elasticsearch. This setting
+  # controls how many events will be buffered before sending a batch
+  # of events.
+  config :flush_size, :validate => :number, :default => 5000
 
-  # TODO(sissel): Config for river?
+  # The amount of time since last flush before a flush is forced.
+  #
+  # This setting helps ensure slow event rates don't get stuck in Logstash.
+  # For example, if your `flush_size` is 100, and you have received 10 events,
+  # and it has been more than `idle_flush_time` seconds since the last flush,
+  # Logstash will flush those 10 events automatically.
+  #
+  # This helps keep both fast and slow log streams moving along in
+  # near-real-time.
+  config :idle_flush_time, :validate => :number, :default => 1
+
+  # Choose the protocol used to talk to Elasticsearch.
+  #
+  # The 'node' protocol will connect to the cluster as a normal Elasticsearch
+  # node (but will not store data). This allows you to use things like
+  # multicast discovery. If you use the `node` protocol, you must permit
+  # bidirectional communication on the port 9300 (or whichever port you have
+  # configured).
+  #
+  # The 'transport' protocol will connect to the host you specify and will
+  # not show up as a 'node' in the Elasticsearch cluster. This is useful
+  # in situations where you cannot permit connections outbound from the
+  # Elasticsearch cluster to this Logstash server.
+  #
+  # The 'http' protocol will use the Elasticsearch REST/HTTP interface to talk
+  # to elasticsearch.
+  #
+  # All protocols will use bulk requests when talking to Elasticsearch.
+  #
+  # The default `protocol` setting under java/jruby is "node". The default
+  # `protocol` on non-java rubies is "http"
+  config :protocol, :validate => [ "node", "transport", "http" ]
+
+  # The Elasticsearch action to perform. Valid actions are: `index`, `delete`.
+  #
+  # Use of this setting *REQUIRES* you also configure the `document_id` setting
+  # because `delete` actions all require a document id.
+  #
+  # What does each action do?
+  #
+  # - index: indexes a document (an event from logstash).
+  # - delete: deletes a document by id
+  #
+  # For more details on actions, check out the [Elasticsearch bulk API documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-bulk.html)
+  config :action, :validate => :string, :default => "index"
 
   public
   def register
-    # TODO(sissel): find a better way of declaring where the elasticsearch
-    # libraries are
-    # TODO(sissel): can skip this step if we're running from a jar.
-    jarpath = File.join(File.dirname(__FILE__), "../../../vendor/**/*.jar")
-    Dir[jarpath].each do |jar|
-        require jar
+    client_settings = {}
+    client_settings["cluster.name"] = @cluster if @cluster
+    client_settings["network.host"] = @bind_host if @bind_host
+    client_settings["transport.tcp.port"] = @bind_port if @bind_port
+
+    if @node_name
+      client_settings["node.name"] = @node_name
+    else
+      client_settings["node.name"] = "logstash-#{Socket.gethostname}-#{$$}-#{object_id}"
     end
 
-    gem "jruby-elasticsearch", ">= 0.0.3"
-    require "jruby-elasticsearch"
+    if @protocol.nil?
+      @protocol = LogStash::Environment.jruby? ? "node" : "http"
+    end
+
+    if ["node", "transport"].include?(@protocol)
+      # Node or TransportClient; requires JRuby
+      raise(LogStash::PluginLoadingError, "This configuration requires JRuby. If you are not using JRuby, you must set 'protocol' to 'http'. For example: output { elasticsearch { protocol => \"http\" } }") unless LogStash::Environment.jruby?
+      LogStash::Environment.load_elasticsearch_jars!
 
-    @logger.info(:message => "New ElasticSearch output", :cluster => @cluster,
-                 :host => @host, :port => @port)
-    @pending = []
-    @callback = self.method(:receive_native)
-    @client = ElasticSearch::Client.new(:cluster => @cluster,
-                                        :host => @host, :port => @port)
+      # setup log4j properties for Elasticsearch
+      LogStash::Logger.setup_log4j(@logger)
+    end
+
+    require "logstash/outputs/elasticsearch/protocol"
+
+    if @port.nil?
+      @port = case @protocol
+        when "http"; "9200"
+        when "transport", "node"; "9300-9305"
+      end
+    end
+
+    if @host.nil? && @protocol == "http"
+      @logger.info("No 'host' set in elasticsearch output. Defaulting to localhost")
+      @host = ["localhost"]
+    end
+
+    client_class = case @protocol
+      when "transport"
+        LogStash::Outputs::Elasticsearch::Protocols::TransportClient
+      when "node"
+        LogStash::Outputs::Elasticsearch::Protocols::NodeClient
+      when "http"
+        LogStash::Outputs::Elasticsearch::Protocols::HTTPClient
+    end
+
+    if @embedded
+      raise(LogStash::ConfigurationError, "The 'embedded => true' setting is only valid for the elasticsearch output under JRuby. You are running #{RUBY_DESCRIPTION}") unless LogStash::Environment.jruby?
+      LogStash::Environment.load_elasticsearch_jars!
+
+      # Default @host with embedded to localhost. This should help avoid
+      # newbies tripping on ubuntu and other distros that have a default
+      # firewall that blocks multicast.
+      @host ||= ["localhost"]
+
+      # Start Elasticsearch local.
+      start_local_elasticsearch
+    end
+
+    @client = Array.new
+
+    if protocol == "node" or @host.nil? # if @protocol is "node" or @host is not set
+      options = {
+          :host => @host,
+          :port => @port,
+          :client_settings => client_settings
+      }
+      @client << client_class.new(options)
+    else # if @protocol in ["transport","http"]
+      @host.each do |host|
+          (_host,_port) = host.split ":"
+          options = {
+            :host => _host,
+            :port => _port || @port,
+            :client_settings => client_settings
+          }
+          @logger.info "Create client to elasticsearch server on #{_host}:#{_port}"
+          @client << client_class.new(options)
+      end # @host.each
+    end
+
+    if @manage_template
+      for client in @client
+          begin
+            @logger.info("Automatic template management enabled", :manage_template => @manage_template.to_s)
+            client.template_install(@template_name, get_template, @template_overwrite)
+            break
+          rescue => e
+            @logger.error("Failed to install template: #{e.message}")
+          end
+      end # for @client loop
+    end # if @manage_templates
+
+    @logger.info("New Elasticsearch output", :cluster => @cluster,
+                 :host => @host, :port => @port, :embedded => @embedded,
+                 :protocol => @protocol)
+
+    @client_idx = 0
+    @current_client = @client[@client_idx]
+
+    buffer_initialize(
+      :max_items => @flush_size,
+      :max_interval => @idle_flush_time,
+      :logger => @logger
+    )
   end # def register
 
-  # TODO(sissel): Needs migration to  jrubyland
+  protected
+  def shift_client
+    @client_idx = (@client_idx+1) % @client.length
+    @current_client = @client[@client_idx]
+    @logger.debug? and @logger.debug("Switched current elasticsearch client to ##{@client_idx} at #{@host[@client_idx]}")
+  end
+
   public
-  def ready(params)
-    case params["method"]
-    when "http"
-      @logger.debug "ElasticSearch using http with URL #{@url.to_s}"
-      #@http = EventMachine::HttpRequest.new(@url.to_s)
-      @callback = self.method(:receive_http)
-    when "river"
-      require "logstash/outputs/amqp"
-      params["port"] ||= 5672
-      auth = "#{params["user"] or "guest"}:#{params["pass"] or "guest"}"
-      mq_url = URI::parse("amqp://#{auth}@#{params["host"]}:#{params["port"]}/queue/#{params["queue"]}?durable=1")
-      @mq = LogStash::Outputs::Amqp.new(mq_url.to_s)
-      @mq.register
-      @callback = self.method(:receive_river)
-      em_url = URI.parse("http://#{@url.host}:#{@url.port}/_river/logstash#{@url.path.tr("/", "_")}/_meta")
-      unused, @es_index, @es_type = @url.path.split("/", 3)
-
-      river_config = {"type" => params["type"],
-                      params["type"] => {"host" => params["host"],
-                                         "user" => params["user"],
-                                         "port" => params["port"],
-                                         "pass" => params["pass"],
-                                         "vhost" => params["vhost"],
-                                         "queue" => params["queue"],
-                                         "exchange" => params["queue"],
-                                        },
-                     "index" => {"bulk_size" => 100,
-                                 "bulk_timeout" => "10ms",
-                                },
-                     }
-      @logger.debug(["ElasticSearch using river", river_config])
-      #http_setup = EventMachine::HttpRequest.new(em_url.to_s)
-      req = http_setup.put :body => river_config.to_json
-      req.errback do
-        @logger.warn "Error setting up river: #{req.response}"
+  def get_template
+    if @template.nil?
+      @template = LogStash::Environment.plugin_path("outputs/elasticsearch/elasticsearch-template.json")
+      if !File.exists?(@template)
+        raise "You must specify 'template => ...' in your elasticsearch output (I looked for '#{@template}')"
       end
-      @callback = self.method(:receive_river)
-    else raise "unknown elasticsearch method #{params["method"].inspect}"
     end
+    template_json = IO.read(@template).gsub(/\n/,'')
+    @logger.info("Using mapping template", :template => template_json)
+    return LogStash::Json.load(template_json)
+  end # def get_template
 
-    #receive(LogStash::Event.new({
-      #"@source" => "@logstashinit",
-      #"@type" => "@none",
-      #"@message" => "Starting logstash output to elasticsearch",
-      #"@fields" => {
-        #"HOSTNAME" => Socket.gethostname
-      #},
-    #}))
-
-    pending = @pending
-    @pending = []
-    @logger.info("Flushing #{pending.size} events")
-    pending.each do |event|
-      receive(event)
-    end
-  end # def ready
+  protected
+  def start_local_elasticsearch
+    @logger.info("Starting embedded Elasticsearch local node.")
+    builder = org.elasticsearch.node.NodeBuilder.nodeBuilder
+    # Disable 'local only' - LOGSTASH-277
+    #builder.local(true)
+    builder.settings.put("cluster.name", @cluster) if @cluster
+    builder.settings.put("node.name", @node_name) if @node_name
+    builder.settings.put("network.host", @bind_host) if @bind_host
+    builder.settings.put("http.port", @embedded_http_port)
+
+    @embedded_elasticsearch = builder.node
+    @embedded_elasticsearch.start
+  end # def start_local_elasticsearch
 
   public
   def receive(event)
-    if @callback
-      @callback.call(event)
+    return unless output?(event)
+
+    # Set the 'type' value for the index.
+    if @index_type
+      type = event.sprintf(@index_type)
     else
-      @pending << event
+      type = event["type"] || "logs"
     end
+
+    index = event.sprintf(@index)
+
+    document_id = @document_id ? event.sprintf(@document_id) : nil
+    buffer_receive([event.sprintf(@action), { :_id => document_id, :_index => index, :_type => type }, event.to_hash])
   end # def receive
 
-  public
-  def receive_http(event, tries=5)
-    req = @http.post :body => event.to_json
-    req.errback do
-      @logger.warn("Request to index to #{@url.to_s} failed (will retry, #{tries} tries left). Event was #{event.to_s}")
-      EventMachine::add_timer(2) do
-        # TODO(sissel): Actually abort if we retry too many times.
-        receive_http(event, tries - 1)
+  def flush(actions, teardown=false)
+    begin
+      @logger.debug? and @logger.debug "Sending bulk of actions to client[#{@client_idx}]: #{@host[@client_idx]}"
+      @current_client.bulk(actions)
+    rescue => e
+      @logger.error "Got error to send bulk of actions to elasticsearch server at #{@host[@client_idx]} : #{e.message}"
+      raise e
+    ensure
+      unless @protocol == "node"
+          @logger.debug? and @logger.debug "Shifting current elasticsearch client"
+          shift_client
       end
     end
-  end # def receive_http
+    # TODO(sissel): Handle errors. Since bulk requests could mostly succeed
+    # (aka partially fail), we need to figure out what documents need to be
+    # retried.
+    #
+    # In the worst case, a failing flush (exception) will incur a retry from Stud::Buffer.
+  end # def flush
 
-  public
-  def receive_native(event)
-    index = event.sprintf(@index)
-    type = event.sprintf(@type)
-    # TODO(sissel): allow specifying the ID?
-    # The document ID is how elasticsearch determines sharding hash, so it can
-    # help performance if we allow folks to specify a specific ID.
-    req = @client.index(index, type, event.to_hash)
-    req.on(:success) do |response|
-      @logger.debug(["Successfully indexed", event.to_hash])
-    end.on(:failure) do |exception|
-      @logger.debug(["Failed to index an event", exception, event.to_hash])
-    end
-    req.execute
-  end # def receive_native
+  def teardown
+    buffer_flush(:final => true)
+  end
 
-  public
-  def receive_river(event)
-    # bulk format; see http://www.elasticsearch.com/docs/elasticsearch/river/rabbitmq/
-    index_message = {"index" => {"_index" => @es_index, "_type" => @es_type}}.to_json + "\n"
-    #index_message += {@es_type => event.to_hash}.to_json + "\n"
-    index_message += event.to_hash.to_json + "\n"
-    @mq.receive_raw(index_message)
-  end # def receive_river
-
-  private
-  def old_create_index
-    # TODO(sissel): this is leftover from the old eventmachine days
-    # make sure we don't need it, or, convert it.
-
-    # Describe this index to elasticsearch
-    indexmap = {
-      # The name of the index
-      "settings" => { 
-        @url.path.split("/")[-1] => {
-          "mappings" => {
-            "@source" => { "type" => "string" },
-            "@source_host" => { "type" => "string" },
-            "@source_path" => { "type" => "string" },
-            "@timestamp" => { "type" => "date" },
-            "@tags" => { "type" => "string" },
-            "@message" => { "type" => "string" },
-
-            # TODO(sissel): Hack for now until this bug is resolved:
-            # https://github.com/elasticsearch/elasticsearch/issues/issue/604
-            "@fields" => { 
-              "type" => "object",
-              "properties" => {
-                "HOSTNAME" => { "type" => "string" },
-              },
-            }, # "@fields"
-          }, # "properties"
-        }, # index map for this index type.
-      }, # "settings"
-    } # ES Index
-
-    #indexurl = @esurl.to_s
-    #indexmap_http = EventMachine::HttpRequest.new(indexurl)
-    #indexmap_req = indexmap_http.put :body => indexmap.to_json
-    #indexmap_req.callback do
-      #@logger.info(["Done configuring index", indexurl, indexmap])
-      #ready(params)
-    #end
-    #indexmap_req.errback do
-      #@logger.warn(["Failure configuring index (http failed to connect?)",
-                    #@esurl.to_s, indexmap])
-      #@logger.warn([indexmap_req])
-      ##sleep 30
-      #raise "Failure configuring index: #{@esurl.to_s}"
-      #
-    #end
-  end # def old_create_index
 end # class LogStash::Outputs::Elasticsearch
diff --git a/lib/logstash/outputs/elasticsearch/elasticsearch-template.json b/lib/logstash/outputs/elasticsearch/elasticsearch-template.json
new file mode 100644
index 00000000000..3f9c8cc4f86
--- /dev/null
+++ b/lib/logstash/outputs/elasticsearch/elasticsearch-template.json
@@ -0,0 +1,34 @@
+{
+  "template" : "logstash-*",
+  "settings" : {
+    "index.refresh_interval" : "5s"
+  },
+  "mappings" : {
+    "_default_" : {
+       "_all" : {"enabled" : true},
+       "dynamic_templates" : [ {
+         "string_fields" : {
+           "match" : "*",
+           "match_mapping_type" : "string",
+           "mapping" : {
+             "type" : "string", "index" : "analyzed", "omit_norms" : true,
+               "fields" : {
+                 "raw" : {"type": "string", "index" : "not_analyzed", "ignore_above" : 256}
+               }
+           }
+         }
+       } ],
+       "properties" : {
+         "@version": { "type": "string", "index": "not_analyzed" },
+         "geoip"  : {
+           "type" : "object",
+             "dynamic": true,
+             "path": "full",
+             "properties" : {
+               "location" : { "type" : "geo_point" }
+             }
+         }
+       }
+    }
+  }
+}
diff --git a/lib/logstash/outputs/elasticsearch/protocol.rb b/lib/logstash/outputs/elasticsearch/protocol.rb
new file mode 100644
index 00000000000..d0747980dcb
--- /dev/null
+++ b/lib/logstash/outputs/elasticsearch/protocol.rb
@@ -0,0 +1,295 @@
+require "logstash/outputs/elasticsearch"
+require "cabin"
+
+module LogStash::Outputs::Elasticsearch
+  module Protocols
+    class Base
+      private
+      def initialize(options={})
+        # host(s), port, cluster
+        @logger = Cabin::Channel.get
+      end
+
+      def client
+        return @client if @client
+        @client = build_client(@options)
+        return @client
+      end # def client
+
+
+      def template_install(name, template, force=false)
+        if template_exists?(name) && !force
+          @logger.debug("Found existing Elasticsearch template. Skipping template management", :name => name)
+          return
+        end
+        template_put(name, template)
+      end
+
+      # Do a bulk request with the given actions.
+      #
+      # 'actions' is expected to be an array of bulk requests as string json
+      # values.
+      #
+      # Each 'action' becomes a single line in the bulk api call. For more
+      # details on the format of each.
+      def bulk(actions)
+        raise NotImplemented, "You must implement this yourself"
+        # bulk([
+        # '{ "index" : { "_index" : "test", "_type" : "type1", "_id" : "1" } }',
+        # '{ "field1" : "value1" }'
+        #])
+      end
+
+      public(:initialize, :template_install)
+    end
+
+    class HTTPClient < Base
+      private
+
+      DEFAULT_OPTIONS = {
+        :port => 9200
+      }
+
+      def initialize(options={})
+        require "ftw"
+        super
+        require "elasticsearch" # gem 'elasticsearch-ruby'
+        @options = DEFAULT_OPTIONS.merge(options)
+        @client = client
+      end
+
+      def build_client(options)
+        client = Elasticsearch::Client.new(
+          :host => [options[:host], options[:port]].join(":")
+        )
+
+        # Use FTW to do indexing requests, for now, until we
+        # can identify and resolve performance problems of elasticsearch-ruby
+        @bulk_url = "http://#{options[:host]}:#{options[:port]}/_bulk"
+        @agent = FTW::Agent.new
+
+        return client
+      end
+
+      if ENV["BULK"] == "esruby"
+        def bulk(actions)
+          bulk_esruby(actions)
+        end
+      else
+        def bulk(actions)
+          bulk_ftw(actions)
+        end
+      end
+
+      def bulk_esruby(actions)
+        @client.bulk(:body => actions.collect do |action, args, source|
+          if source
+            next [ { action => args }, source ]
+          else
+            next { action => args }
+          end
+        end.flatten)
+      end # def bulk_esruby
+
+      # Avoid creating a new string for newline every time
+      NEWLINE = "\n".freeze
+      def bulk_ftw(actions)
+        body = actions.collect do |action, args, source|
+          header = { action => args }
+          if source
+            next [ LogStash::Json.dump(header), NEWLINE, LogStash::Json.dump(source), NEWLINE ]
+          else
+            next [ LogStash::Json.dump(header), NEWLINE ]
+          end
+        end.flatten.join("")
+        begin
+          response = @agent.post!(@bulk_url, :body => body)
+        rescue EOFError
+          @logger.warn("EOF while writing request or reading response header from elasticsearch", :host => @host, :port => @port)
+          raise
+        end
+
+        # Consume the body for error checking
+        # This will also free up the connection for reuse.
+        response_body = ""
+        begin
+          response.read_body { |chunk| response_body += chunk }
+        rescue EOFError
+          @logger.warn("EOF while reading response body from elasticsearch",
+                       :url => @bulk_url)
+          raise
+        end
+
+        if response.status != 200
+          @logger.error("Error writing (bulk) to elasticsearch",
+                        :response => response, :response_body => response_body,
+                        :request_body => body)
+          raise "Non-OK response code from Elasticsearch: #{response.status}"
+        end
+      end # def bulk_ftw
+
+      def template_exists?(name)
+        @client.indices.get_template(:name => name)
+        return true
+      rescue Elasticsearch::Transport::Transport::Errors::NotFound
+        return false
+      end # def template_exists?
+
+      def template_put(name, template)
+        @client.indices.put_template(:name => name, :body => template)
+      end # template_put
+
+      public(:bulk)
+    end # class HTTPClient
+
+    class NodeClient < Base
+      private
+
+      DEFAULT_OPTIONS = {
+        :port => 9300,
+      }
+
+      def initialize(options={})
+        super
+        require "java"
+        @options = DEFAULT_OPTIONS.merge(options)
+        setup(@options)
+        @client = client
+      end # def initialize
+
+      def settings
+        return @settings
+      end
+
+      def setup(options={})
+        @settings = org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder
+        if options[:host]
+          @settings.put("discovery.zen.ping.multicast.enabled", false)
+          @settings.put("discovery.zen.ping.unicast.hosts", hosts(options))
+        end
+
+        @settings.put("node.client", true)
+        @settings.put("http.enabled", false)
+
+        if options[:client_settings]
+          options[:client_settings].each do |key, value|
+            @settings.put(key, value)
+          end
+        end
+
+        return @settings
+      end
+
+      def hosts(options)
+        # http://www.elasticsearch.org/guide/reference/modules/discovery/zen/
+        result = Array.new
+        if options[:host].class == Array
+          options[:host].each do |host|
+            if host.to_s =~ /^.+:.+$/
+              # For host in format: host:port, ignore options[:port]
+              result << host
+            else
+              if options[:port].to_s =~ /^\d+-\d+$/
+                # port ranges are 'host[port1-port2]'
+                result << Range.new(*options[:port].split("-")).collect { |p| "#{host}:#{p}" }
+              else
+                result << "#{host}:#{options[:port]}"
+              end
+            end
+          end
+        else
+          if options[:host].to_s =~ /^.+:.+$/
+            # For host in format: host:port, ignore options[:port]
+            result << options[:host]
+          else
+            if options[:port].to_s =~ /^\d+-\d+$/
+              # port ranges are 'host[port1-port2]' according to
+              # http://www.elasticsearch.org/guide/reference/modules/discovery/zen/
+              # However, it seems to only query the first port.
+              # So generate our own list of unicast hosts to scan.
+              range = Range.new(*options[:port].split("-"))
+              result << range.collect { |p| "#{options[:host]}:#{p}" }
+            else
+              result << "#{options[:host]}:#{options[:port]}"
+            end
+          end
+        end
+        result.flatten.join(",")
+      end # def hosts
+
+      def build_client(options)
+        nodebuilder = org.elasticsearch.node.NodeBuilder.nodeBuilder
+        return nodebuilder.settings(@settings).node.client
+      end # def build_client
+
+      def bulk(actions)
+        # Actions an array of [ action, action_metadata, source ]
+        prep = @client.prepareBulk
+        actions.each do |action, args, source|
+          prep.add(build_request(action, args, source))
+        end
+        response = prep.execute.actionGet()
+
+        # TODO(sissel): What format should the response be in?
+      end # def bulk
+
+      def build_request(action, args, source)
+        case action
+          when "index"
+            request = org.elasticsearch.action.index.IndexRequest.new(args[:_index])
+            request.id(args[:_id]) if args[:_id]
+            request.source(source)
+          when "delete"
+            request = org.elasticsearch.action.delete.DeleteRequest.new(args[:_index])
+            request.id(args[:_id])
+          #when "update"
+          #when "create"
+        end # case action
+
+        request.type(args[:_type]) if args[:_type]
+        return request
+      end # def build_request
+
+      def template_exists?(name)
+        request = org.elasticsearch.action.admin.indices.template.get.GetIndexTemplatesRequestBuilder.new(@client.admin.indices, name)
+        response = request.get
+        return !response.getIndexTemplates.isEmpty
+      end # def template_exists?
+
+      def template_put(name, template)
+        request = org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequestBuilder.new(@client.admin.indices, name)
+        request.setSource(LogStash::Json.dump(template))
+
+        # execute the request and get the response, if it fails, we'll get an exception.
+        request.get
+      end # template_put
+
+      public(:initialize, :bulk)
+    end # class NodeClient
+
+    class TransportClient < NodeClient
+      private
+      def build_client(options)
+        client = org.elasticsearch.client.transport.TransportClient.new(settings.build)
+
+        if options[:host]
+          client.addTransportAddress(
+            org.elasticsearch.common.transport.InetSocketTransportAddress.new(
+              options[:host], options[:port].to_i
+            )
+          )
+        end
+
+        return client
+      end # def build_client
+    end # class TransportClient
+  end # module Protocols
+
+  module Requests
+    class GetIndexTemplates; end
+    class Bulk; end
+    class Index; end
+    class Delete; end
+  end
+end
+
diff --git a/lib/logstash/outputs/elasticsearch_http.rb b/lib/logstash/outputs/elasticsearch_http.rb
new file mode 100644
index 00000000000..124b739050a
--- /dev/null
+++ b/lib/logstash/outputs/elasticsearch_http.rb
@@ -0,0 +1,245 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/outputs/base"
+require "logstash/json"
+require "stud/buffer"
+
+# This output lets you store logs in Elasticsearch.
+#
+# This plugin uses the HTTP/REST interface to Elasticsearch, which usually
+# lets you use any version of Elasticsearch server. It is known to work
+# with elasticsearch %ELASTICSEARCH_VERSION%
+#
+# You can learn more about Elasticsearch at <http://www.elasticsearch.org>
+class LogStash::Outputs::ElasticSearchHTTP < LogStash::Outputs::Base
+  include Stud::Buffer
+
+  config_name "elasticsearch_http"
+  milestone 2
+
+  # The index to write events to. This can be dynamic using the %{foo} syntax.
+  # The default value will partition your indices by day so you can more easily
+  # delete old data or only search specific date ranges.
+  config :index, :validate => :string, :default => "logstash-%{+YYYY.MM.dd}"
+
+  # The index type to write events to. Generally you should try to write only
+  # similar events to the same 'type'. String expansion '%{foo}' works here.
+  config :index_type, :validate => :string
+
+  # Starting in Logstash 1.3 (unless you set option "manage_template" to false)
+  # a default mapping template for Elasticsearch will be applied, if you do not
+  # already have one set to match the index pattern defined (default of
+  # "logstash-%{+YYYY.MM.dd}"), minus any variables.  For example, in this case
+  # the template will be applied to all indices starting with logstash-*
+  #
+  # If you have dynamic templating (e.g. creating indices based on field names)
+  # then you should set "manage_template" to false and use the REST API to upload
+  # your templates manually.
+  config :manage_template, :validate => :boolean, :default => true
+
+  # This configuration option defines how the template is named inside Elasticsearch.
+  # Note that if you have used the template management features and subsequently
+  # change this you will need to prune the old template manually, e.g.
+  # curl -XDELETE <http://localhost:9200/_template/OldTemplateName?pretty>
+  # where OldTemplateName is whatever the former setting was.
+  config :template_name, :validate => :string, :default => "logstash"
+
+  # You can set the path to your own template here, if you so desire.
+  # If not the included template will be used.
+  config :template, :validate => :path
+
+  # Overwrite the current template with whatever is configured
+  # in the template and template_name directives.
+  config :template_overwrite, :validate => :boolean, :default => false
+
+  # The hostname or IP address to reach your Elasticsearch server.
+  config :host, :validate => :string, :required => true
+
+  # The port for Elasticsearch HTTP interface to use.
+  config :port, :validate => :number, :default => 9200
+
+  # The HTTP Basic Auth username used to access your elasticsearch server.
+  config :user, :validate => :string, :default => nil
+
+  # The HTTP Basic Auth password used to access your elasticsearch server.
+  config :password, :validate => :password, :default => nil
+
+  # This plugin uses the bulk index api for improved indexing performance.
+  # To make efficient bulk api calls, we will buffer a certain number of
+  # events before flushing that out to Elasticsearch. This setting
+  # controls how many events will be buffered before sending a batch
+  # of events.
+  config :flush_size, :validate => :number, :default => 100
+
+  # The amount of time since last flush before a flush is forced.
+  #
+  # This setting helps ensure slow event rates don't get stuck in Logstash.
+  # For example, if your `flush_size` is 100, and you have received 10 events,
+  # and it has been more than `idle_flush_time` seconds since the last flush,
+  # logstash will flush those 10 events automatically.
+  #
+  # This helps keep both fast and slow log streams moving along in
+  # near-real-time.
+  config :idle_flush_time, :validate => :number, :default => 1
+
+  # The document ID for the index. Useful for overwriting existing entries in
+  # Elasticsearch with the same ID.
+  config :document_id, :validate => :string, :default => nil
+
+  # Set the type of Elasticsearch replication to use. If async
+  # the index request to Elasticsearch to return after the primary
+  # shards have been written. If sync (default), index requests
+  # will wait until the primary and the replica shards have been
+  # written.
+  config :replication, :validate => ['async', 'sync'], :default => 'sync'
+
+  public
+  def register
+    require "ftw" # gem ftw
+    @agent = FTW::Agent.new
+    @queue = []
+
+    auth = @user && @password ? "#{@user}:#{@password.value}@" : ""
+    @bulk_url = "http://#{auth}#{@host}:#{@port}/_bulk?replication=#{@replication}"
+    if @manage_template
+      @logger.info("Automatic template management enabled", :manage_template => @manage_template.to_s)
+      template_search_url = "http://#{auth}#{@host}:#{@port}/_template/*"
+      @template_url = "http://#{auth}#{@host}:#{@port}/_template/#{@template_name}"
+      if @template_overwrite
+        @logger.info("Template overwrite enabled.  Deleting existing template.", :template_overwrite => @template_overwrite.to_s)
+        response = @agent.get!(@template_url)
+        template_action('delete') if response.status == 200 #=> Purge the old template if it exists
+      end
+      @logger.debug("Template Search URL:", :template_search_url => template_search_url)
+      has_template = false
+      template_idx_name = @index.sub(/%{[^}]+}/,'*')
+      alt_template_idx_name = @index.sub(/-%{[^}]+}/,'*')
+      # Get the template data
+      response = @agent.get!(template_search_url)
+      json = ""
+      if response.status == 404 #=> This condition can occcur when no template has ever been appended
+        @logger.info("No template found in Elasticsearch...")
+        get_template_json
+        template_action('put')
+      elsif response.status == 200
+        begin
+          response.read_body { |c| json << c }
+          results = LogStash::Json.load(json)
+        rescue Exception => e
+          @logger.error("Error parsing JSON", :json => json, :results => results.to_s, :error => e.to_s)
+          raise "Exception in parsing JSON", e
+        end
+        if !results.any? { |k,v| v["template"] == template_idx_name || v["template"] == alt_template_idx_name }
+          @logger.debug("No template found in Elasticsearch", :has_template => has_template, :name => template_idx_name, :alt => alt_template_idx_name)
+          get_template_json
+          template_action('put')
+        end
+      else #=> Some other status code?
+        @logger.error("Could not check for existing template.  Check status code.", :status => response.status.to_s)
+      end # end if response.status == 200
+    end # end if @manage_template
+    buffer_initialize(
+      :max_items => @flush_size,
+      :max_interval => @idle_flush_time,
+      :logger => @logger
+    )
+  end # def register
+
+  public
+  def template_action(command)
+    begin
+      if command == 'delete'
+        response = @agent.delete!(@template_url)
+        response.discard_body
+      elsif command == 'put'
+        response = @agent.put!(@template_url, :body => @template_json)
+        response.discard_body
+      end
+    rescue EOFError
+      @logger.warn("EOF while attempting request or reading response header from elasticsearch",
+                   :host => @host, :port => @port)
+      return # abort this action
+    end
+    if response.status != 200
+      @logger.error("Error acting on elasticsearch mapping template",
+                    :response => response, :action => command,
+                    :request_url => @template_url)
+      return
+    end
+    @logger.info("Successfully deleted template", :template_url => @template_url) if command == 'delete'
+    @logger.info("Successfully applied template", :template_url => @template_url) if command == 'put'
+  end # def template_action
+
+
+  public
+  def get_template_json
+    if @template.nil?
+      @template = LogStash::Environment.plugin_path("outputs/elasticsearch/elasticsearch-template.json")
+      if !File.exists?(@template)
+        raise "You must specify 'template => ...' in your elasticsearch_http output (I looked for '#{@template}')"
+      end
+    end
+    @template_json = IO.read(@template).gsub(/\n/,'')
+    @logger.info("Using mapping template", :template => @template_json)
+  end # def get_template_json
+
+  public
+  def receive(event)
+    return unless output?(event)
+    buffer_receive([event, index, type])
+  end # def receive
+
+  def flush(events, teardown=false)
+    # Avoid creating a new string for newline every time
+    newline = "\n".freeze
+
+    body = events.collect do |event, index, type|
+      index = event.sprintf(@index)
+
+      # Set the 'type' value for the index.
+      if @index_type.nil?
+        type =  event["type"] || "logs"
+      else
+        type = event.sprintf(@index_type)
+      end
+      header = { "index" => { "_index" => index, "_type" => type } }
+      header["index"]["_id"] = event.sprintf(@document_id) if !@document_id.nil?
+
+      [ LogStash::Json.dump(header), newline, event.to_json, newline ]
+    end.flatten
+
+    post(body.join(""))
+  end # def receive_bulk
+
+  def post(body)
+    begin
+      response = @agent.post!(@bulk_url, :body => body)
+    rescue EOFError
+      @logger.warn("EOF while writing request or reading response header from elasticsearch",
+                   :host => @host, :port => @port)
+      raise
+    end
+
+    # Consume the body for error checking
+    # This will also free up the connection for reuse.
+    body = ""
+    begin
+      response.read_body { |chunk| body += chunk }
+    rescue EOFError
+      @logger.warn("EOF while reading response body from elasticsearch",
+                   :host => @host, :port => @port)
+      raise
+    end
+
+    if response.status != 200
+      @logger.error("Error writing (bulk) to elasticsearch",
+                    :response => response, :response_body => body,
+                    :request_body => @queue.join("\n"))
+      raise
+    end
+  end # def post
+
+  def teardown
+    buffer_flush(:final => true)
+  end # def teardown
+end # class LogStash::Outputs::ElasticSearchHTTP
diff --git a/lib/logstash/outputs/elasticsearch_river.rb b/lib/logstash/outputs/elasticsearch_river.rb
new file mode 100644
index 00000000000..365d2b53ee9
--- /dev/null
+++ b/lib/logstash/outputs/elasticsearch_river.rb
@@ -0,0 +1,206 @@
+# encoding: utf-8
+require "logstash/environment"
+require "logstash/namespace"
+require "logstash/outputs/base"
+require "logstash/json"
+require "uri"
+require "net/http"
+
+# This output lets you store logs in elasticsearch. It's similar to the
+# 'elasticsearch' output but improves performance by using a queue server,
+# rabbitmq, to send data to elasticsearch.
+#
+# Upon startup, this output will automatically contact an elasticsearch cluster
+# and configure it to read from the queue to which we write.
+#
+# You can learn more about elasticseasrch at <http://elasticsearch.org>
+# More about the elasticsearch rabbitmq river plugin: <https://github.com/elasticsearch/elasticsearch-river-rabbitmq/blob/master/README.md>
+
+class LogStash::Outputs::ElasticSearchRiver < LogStash::Outputs::Base
+
+  config_name "elasticsearch_river"
+  milestone 2
+
+  # The index to write events to. This can be dynamic using the %{foo} syntax.
+  # The default value will partition your indeces by day so you can more easily
+  # delete old data or only search specific date ranges.
+  config :index, :validate => :string, :default => "logstash-%{+YYYY.MM.dd}"
+
+  # The index type to write events to. Generally you should try to write only
+  # similar events to the same 'type'. String expansion '%{foo}' works here.
+  config :index_type, :validate => :string, :default => "%{type}"
+
+  # The name/address of an ElasticSearch host to use for river creation
+  config :es_host, :validate => :string, :required => true
+
+  # ElasticSearch API port
+  config :es_port, :validate => :number, :default => 9200
+
+  # ElasticSearch river configuration: bulk fetch size
+  config :es_bulk_size, :validate => :number, :default => 1000
+
+  # ElasticSearch river configuration: bulk timeout in milliseconds
+  config :es_bulk_timeout_ms, :validate => :number, :default => 100
+
+  # ElasticSearch river configuration: is ordered?
+  config :es_ordered, :validate => :boolean, :default => false
+
+  # Hostname of RabbitMQ server
+  config :rabbitmq_host, :validate => :string, :required => true
+
+  # Port of RabbitMQ server
+  config :rabbitmq_port, :validate => :number, :default => 5672
+
+  # RabbitMQ user
+  config :user, :validate => :string, :default => "guest"
+
+  # RabbitMQ password
+  config :password, :validate => :string, :default => "guest"
+
+  # RabbitMQ vhost
+  config :vhost, :validate => :string, :default => "/"
+
+  # RabbitMQ queue name
+  config :queue, :validate => :string, :default => "elasticsearch"
+
+  # RabbitMQ exchange name
+  config :exchange, :validate => :string, :default => "elasticsearch"
+
+  # The exchange type (fanout, topic, direct)
+  config :exchange_type, :validate => [ "fanout", "direct", "topic"],
+         :default => "direct"
+
+  # RabbitMQ routing key
+  config :key, :validate => :string, :default => "elasticsearch"
+
+  # RabbitMQ durability setting. Also used for ElasticSearch setting
+  config :durable, :validate => :boolean, :default => true
+
+  # RabbitMQ persistence setting
+  config :persistent, :validate => :boolean, :default => true
+
+  # The document ID for the index. Useful for overwriting existing entries in
+  # elasticsearch with the same ID.
+  config :document_id, :validate => :string, :default => nil
+
+  public
+  def register
+    LogStash::Environment.load_elasticsearch_jars!
+    prepare_river
+  end
+
+  protected
+  def prepare_river
+    require "logstash/outputs/rabbitmq"
+
+    # Configure the message plugin
+    params = {
+      "host" => [@rabbitmq_host],
+      "port" => [@rabbitmq_port],
+      "user" => [@user],
+      "password" => [@password],
+      "exchange_type" => [@exchange_type],
+      "exchange" => [@exchange],
+      "key" => [@key],
+      "vhost" => [@vhost],
+      "durable" => [@durable.to_s],
+      "persistent" => [@persistent.to_s],
+      "debug" => [@logger.debug?.to_s],
+    }.reject {|k,v| v.first.nil?}
+    @mq = LogStash::Outputs::RabbitMQ.new(params)
+    @mq.register
+
+    # Set up the river
+    begin
+      auth = "#{@user}:#{@password}"
+
+      # Name the river by our hostname
+      require "socket"
+      hostname = Socket.gethostname
+
+      # Replace spaces with hyphens and remove all non-alpha non-dash non-underscore characters
+      river_name = "#{hostname} #{@queue}".gsub(' ', '-').gsub(/[^\w-]/, '')
+
+      api_path = "/_river/logstash-#{river_name}/_meta"
+      @status_path = "/_river/logstash-#{river_name}/_status"
+
+      river_config = {"type" => "rabbitmq",
+                      "rabbitmq" => {
+                                "host" => @rabbitmq_host=="localhost" ? hostname : @rabbitmq_host,
+                                "port" => @rabbitmq_port,
+                                "user" => @user,
+                                "pass" => @password,
+                                "vhost" => @vhost,
+                                "queue" => @queue,
+                                "exchange" => @exchange,
+                                "routing_key" => @key,
+                                "exchange_type" => @exchange_type,
+                                "exchange_durable" => @durable.to_s,
+                                "queue_durable" => @durable.to_s
+                               },
+                      "index" => {"bulk_size" => @es_bulk_size,
+                                 "bulk_timeout" => "#{@es_bulk_timeout_ms}ms",
+                                 "ordered" => @es_ordered
+                                },
+                     }
+      @logger.info("ElasticSearch using river", :config => river_config)
+      Net::HTTP.start(@es_host, @es_port) do |http|
+        req = Net::HTTP::Put.new(api_path)
+        req.body = LogStash::Json.dump(river_config)
+        response = http.request(req)
+        response.value() # raise an exception if error
+        @logger.info("River created: #{response.body}")
+      end
+    rescue Exception => e
+      # TODO(petef): should we just throw an exception here, so the
+      # agent tries to restart us and we in turn retry the river
+      # registration?
+      @logger.warn("Couldn't set up river. You'll have to set it up manually (or restart)", :exception => e)
+    end
+
+    check_river_status
+  end # def prepare_river
+
+  private
+  def check_river_status
+    tries = 0
+    success = false
+    reason = nil
+    begin
+      while !success && tries <= 3 do
+        tries += 1
+        Net::HTTP.start(@es_host, @es_port) do |http|
+          req = Net::HTTP::Get.new(@status_path)
+          response = http.request(req)
+          response.value
+          status = LogStash::Json.load(response.body)
+          @logger.debug("Checking ES river status", :status => status)
+          if status["_source"]["error"]
+            reason = "ES river status: #{status["_source"]["error"]}"
+          else
+            success = true
+          end
+        end
+        sleep(2)
+      end
+    rescue Exception => e
+      raise "river is not running, checking status failed: #{$!}"
+    end
+
+    raise "river is not running: #{reason}" unless success
+  end # def check_river_status
+
+  public
+  def receive(event)
+    return unless output?(event)
+    # River events have a format of
+    # "action\ndata\n"
+    # where 'action' is index or delete, data is the data to index.
+    header = { "index" => { "_index" => event.sprintf(@index), "_type" => event.sprintf(@index_type) } }
+    if !@document_id.nil?
+      header["index"]["_id"] = event.sprintf(@document_id)
+    end
+
+    @mq.publish_serialized(LogStash::Json.dump(header) + "\n" + event.to_json + "\n")
+  end # def receive
+end # LogStash::Outputs::ElasticSearchRiver
diff --git a/lib/logstash/outputs/email.rb b/lib/logstash/outputs/email.rb
new file mode 100644
index 00000000000..b869432e041
--- /dev/null
+++ b/lib/logstash/outputs/email.rb
@@ -0,0 +1,303 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+
+# Send email when an output is received. Alternatively, you may include or
+# exclude the email output execution using conditionals. 
+class LogStash::Outputs::Email < LogStash::Outputs::Base
+
+  config_name "email"
+  milestone 1
+
+  # This setting is deprecated in favor of Logstash's "conditionals" feature
+  # If you were using this setting previously, please use conditionals instead.
+  #
+  # If you need help converting your older 'match' setting to a conditional,
+  # I welcome you to join the #logstash irc channel on freenode or to email
+  # the logstash-users@googlegroups.com mailling list and ask for help! :)
+  config :match, :validate => :hash, :deprecated => true
+
+  # The fully-qualified email address to send the email to.
+  #
+  # This field also accepts a comma-separated string of addresses, for example: 
+  # "me@host.com, you@host.com"
+  #
+  # You can also use dynamic fields from the event with the %{fieldname} syntax.
+  config :to, :validate => :string, :required => true
+
+  # The fully-qualified email address for the From: field in the email.
+  config :from, :validate => :string, :default => "logstash.alert@nowhere.com"
+
+  # The fully qualified email address for the Reply-To: field.
+  config :replyto, :validate => :string
+
+  # The fully-qualified email address(es) to include as cc: address(es).
+  #
+  # This field also accepts a comma-separated string of addresses, for example: 
+  # "me@host.com, you@host.com"
+  config :cc, :validate => :string
+
+  # How Logstash should send the email, either via SMTP or by invoking sendmail.
+  config :via, :validate => :string, :default => "smtp"
+
+  # Specify the options to use:
+  #
+  # Via SMTP: smtpIporHost, port, domain, userName, password, authenticationType, starttls
+  #
+  # Via sendmail: location, arguments
+  #
+  # If you do not specify any `options`, you will get the following equivalent code set in
+  # every new mail object:
+  #
+  #     Mail.defaults do
+  #       delivery_method :smtp, { :smtpIporHost         => "localhost",
+  #                                :port                 => 25,
+  #                                :domain               => 'localhost.localdomain',
+  #                                :userName             => nil,
+  #                                :password             => nil,
+  #                                :authenticationType   => nil,(plain, login and cram_md5)
+  #                                :starttls             => true  }
+  #
+  #       retriever_method :pop3, { :address             => "localhost",
+  #                                 :port                => 995,
+  #                                 :user_name           => nil,
+  #                                 :password            => nil,
+  #                                 :enable_ssl          => true }
+  #
+  #       Mail.delivery_method.new  #=> Mail::SMTP instance
+  #       Mail.retriever_method.new #=> Mail::POP3 instance
+  #     end
+  #
+  # Each mail object inherits the defaults set in Mail.delivery_method. However, on
+  # a per email basis, you can override the method:
+  #
+  #     mail.delivery_method :sendmail
+  #
+  # Or you can override the method and pass in settings:
+  #
+  #     mail.delivery_method :sendmail, { :address => 'some.host' }
+  #
+  # You can also just modify the settings:
+  #
+  #     mail.delivery_settings = { :address => 'some.host' }
+  #
+  # The hash you supply is just merged against the defaults with "merge!" and the result
+  # assigned to the mail object.  For instance, the above example will change only the
+  # `:address` value of the global `smtp_settings` to be 'some.host', retaining all other values.
+  config :options, :validate => :hash, :default => {}
+
+  # Subject: for the email.
+  config :subject, :validate => :string, :default => ""
+
+  # Body for the email - plain text only.
+  config :body, :validate => :string, :default => ""
+
+  # HTML Body for the email, which may contain HTML markup.
+  config :htmlbody, :validate => :string, :default => ""
+
+  # Attachments - specify the name(s) and location(s) of the files.
+  config :attachments, :validate => :array, :default => []
+
+  # contenttype : for multipart messages, set the content-type and/or charset of the HTML part.
+  # NOTE: this may not be functional (KH)
+  config :contenttype, :validate => :string, :default => "text/html; charset=UTF-8"
+
+  public
+  def register
+    require "mail"
+
+    # Mail uses instance_eval which changes the scope of self so @options is
+    # inaccessible from inside 'Mail.defaults'. So set a local variable instead.
+    options = @options
+
+    if @via == "smtp"
+      Mail.defaults do
+        delivery_method :smtp, {
+          :address              => options.fetch("smtpIporHost", "localhost"),
+          :port                 => options.fetch("port", 25),
+          :domain               => options.fetch("domain", "localhost"),
+          :user_name            => options.fetch("userName", nil),
+          :password             => options.fetch("password", nil),
+          :authentication       => options.fetch("authenticationType", nil),
+          :enable_starttls_auto => options.fetch("starttls", false),
+          :debug                => options.fetch("debug", false)
+        }
+      end
+    elsif @via == 'sendmail'
+      Mail.defaults do
+        delivery_method :sendmail
+      end
+    else
+      Mail.defaults do
+        delivery_method :@via, options
+      end
+    end # @via tests
+    @logger.debug("Email Output Registered!", :config => @config)
+  end # def register
+
+  public
+  def receive(event)
+    return unless output?(event)
+      @logger.debug("Event being tested for Email", :tags => @tags, :event => event)
+      # Set Intersection - returns a new array with the items that are the same between the two
+      if !@tags.empty? && (event["tags"] & @tags).size == 0
+         # Skip events that have no tags in common with what we were configured
+         @logger.debug("No Tags match for Email Output!")
+         return
+      end
+
+    @logger.debug? && @logger.debug("Match data for Email - ", :match => @match)
+    successful = false
+    matchName = ""
+    operator = ""
+
+    # TODO(sissel): Delete this once match support is removed.
+    @match && @match.each do |name, query|
+      if successful
+        break
+      else
+        matchName = name
+      end
+      # now loop over the csv query
+      queryArray = query.split(',')
+      index = 1
+      while index < queryArray.length
+        field = queryArray.at(index -1)
+        value = queryArray.at(index)
+        index = index + 2
+        if field == ""
+          if value.downcase == "and"
+            operator = "and"
+          elsif value.downcase == "or"
+            operator = "or"
+          else
+            operator = "or"
+            @logger.error("Operator Provided Is Not Found, Currently We Only Support AND/OR Values! - defaulting to OR")
+          end
+        else
+          hasField = event[field]
+          @logger.debug? and @logger.debug("Does Event Contain Field - ", :hasField => hasField)
+          isValid = false
+          # if we have maching field and value is wildcard - we have a success
+          if hasField
+            if value == "*"
+              isValid = true
+            else
+              # we get an array so we need to loop over the values and find if we have a match
+              eventFieldValues = event[field]
+              @logger.debug? and @logger.debug("Event Field Values - ", :eventFieldValues => eventFieldValues)
+              eventFieldValues = [eventFieldValues] if not eventFieldValues.respond_to?(:each)
+              eventFieldValues.each do |eventFieldValue|
+                isValid = validateValue(eventFieldValue, value)
+                if isValid # no need to iterate any further
+                  @logger.debug("VALID CONDITION FOUND - ", :eventFieldValue => eventFieldValue, :value => value) 
+                  break
+                end
+              end # end eventFieldValues.each do
+            end # end value == "*"
+          end # end hasField
+          # if we have an AND operator and we have a successful == false break
+          if operator == "and" && !isValid
+            successful = false
+          elsif operator == "or" && (isValid || successful)
+            successful = true
+          else
+            successful = isValid
+          end
+        end
+      end
+    end # @match.each do
+
+    # The 'match' setting is deprecated and optional. If not set,
+    # default to success.
+    successful = true if @match.nil?
+
+    @logger.debug? && @logger.debug("Email Did we match any alerts for event : ", :successful => successful)
+
+    if successful
+      # first add our custom field - matchName - so we can use it in the sprintf function
+      event["matchName"] = matchName unless matchName.empty?
+      @logger.debug? and @logger.debug("Creating mail with these settings : ", :via => @via, :options => @options, :from => @from, :to => @to, :cc => @cc, :subject => @subject, :body => @body, :content_type => @contenttype, :htmlbody => @htmlbody, :attachments => @attachments, :to => to, :to => to)
+      formatedSubject = event.sprintf(@subject)
+      formattedBody = event.sprintf(@body)
+      formattedHtmlBody = event.sprintf(@htmlbody)
+      # we have a match(s) - send email
+      mail = Mail.new
+      mail.from = event.sprintf(@from)
+      mail.to = event.sprintf(@to)
+      if @replyto
+        mail.reply_to = event.sprintf(@replyto)
+      end
+      mail.cc = event.sprintf(@cc)
+      mail.subject = formatedSubject
+      if @htmlbody.empty?
+        formattedBody.gsub!(/\\n/, "\n") # Take new line in the email
+        mail.body = formattedBody
+      else
+        mail.text_part = Mail::Part.new do
+          content_type "text/plain; charset=UTF-8"
+          formattedBody.gsub!(/\\n/, "\n") # Take new line in the email
+          body formattedBody
+        end
+        mail.html_part = Mail::Part.new do
+          content_type "text/html; charset=UTF-8"
+          body formattedHtmlBody
+        end
+      end
+      @attachments.each do |fileLocation|
+        mail.add_file(fileLocation)
+      end # end @attachments.each
+      @logger.debug? and @logger.debug("Sending mail with these values : ", :from => mail.from, :to => mail.to, :cc => mail.cc, :subject => mail.subject)
+      mail.deliver!
+    end # end if successful
+  end # def receive
+
+
+  private
+  def validateValue(eventFieldValue, value)
+    valid = false
+    # order of this if-else is important - please don't change it
+    if value.start_with?(">=")# greater than or equal
+      value.gsub!(">=","")
+      if eventFieldValue.to_i >= value.to_i
+        valid = true
+      end
+    elsif value.start_with?("<=")# less than or equal
+      value.gsub!("<=","")
+      if eventFieldValue.to_i <= value.to_i
+        valid = true
+      end
+    elsif value.start_with?(">")# greater than
+      value.gsub!(">","")
+      if eventFieldValue.to_i > value.to_i
+        valid = true
+      end
+    elsif value.start_with?("<")# less than
+      value.gsub!("<","")
+      if eventFieldValue.to_i < value.to_i
+        valid = true
+      end
+    elsif value.start_with?("*")# contains
+      value.gsub!("*","")
+      if eventFieldValue.include?(value)
+        valid = true
+      end
+    elsif value.start_with?("!*")# does not contain
+      value.gsub!("!*","")
+      if !eventFieldValue.include?(value)
+        valid = true
+      end
+    elsif value.start_with?("!")# not equal
+      value.gsub!("!","")
+      if eventFieldValue != value
+        valid = true
+      end
+    else # default equal
+      if eventFieldValue == value
+        valid = true
+      end
+    end
+    return valid
+  end # end validateValue()
+
+end # class LogStash::Outputs::Email
diff --git a/lib/logstash/outputs/exec.rb b/lib/logstash/outputs/exec.rb
new file mode 100644
index 00000000000..ae3fda1f0da
--- /dev/null
+++ b/lib/logstash/outputs/exec.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/outputs/base"
+
+# This output will run a command for any matching event.
+#
+# Example:
+# 
+#     output {
+#       exec {
+#         type => abuse
+#         command => "iptables -A INPUT -s %{clientip} -j DROP"
+#       }
+#     }
+#
+# Run subprocesses via system ruby function
+#
+# WARNING: if you want it non-blocking you should use & or dtach or other such
+# techniques
+class LogStash::Outputs::Exec < LogStash::Outputs::Base
+
+  config_name "exec"
+  milestone 1
+
+  # Command line to execute via subprocess. Use dtach or screen to make it non blocking
+  config :command, :validate => :string, :required => true
+
+  public
+  def register
+    @logger.debug("exec output registered", :config => @config)
+  end # def register
+
+  public
+  def receive(event)
+    return unless output?(event)
+    @logger.debug("running exec command", :command => event.sprintf(@command))
+    system(event.sprintf(@command))
+  end # def receive
+
+end
diff --git a/lib/logstash/outputs/file.rb b/lib/logstash/outputs/file.rb
new file mode 100644
index 00000000000..4ca7b98ec50
--- /dev/null
+++ b/lib/logstash/outputs/file.rb
@@ -0,0 +1,179 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/outputs/base"
+require "zlib"
+
+# This output will write events to files on disk. You can use fields
+# from the event as parts of the filename and/or path.
+class LogStash::Outputs::File < LogStash::Outputs::Base
+
+  config_name "file"
+  milestone 2
+
+  # The path to the file to write. Event fields can be used here, 
+  # like "/var/log/logstash/%{host}/%{application}"
+  # One may also utilize the path option for date-based log 
+  # rotation via the joda time format. This will use the event
+  # timestamp.
+  # E.g.: path => "./test-%{+YYYY-MM-dd}.txt" to create 
+  # ./test-2013-05-29.txt 
+  config :path, :validate => :string, :required => true
+
+  # The maximum size of file to write. When the file exceeds this
+  # threshold, it will be rotated to the current filename + ".1"
+  # If that file already exists, the previous .1 will shift to .2
+  # and so forth.
+  #
+  # NOT YET SUPPORTED
+  config :max_size, :validate => :string
+
+  # The format to use when writing events to the file. This value
+  # supports any string and can include %{name} and other dynamic
+  # strings.
+  #
+  # If this setting is omitted, the full json representation of the
+  # event will be written as a single line.
+  config :message_format, :validate => :string
+
+  # Flush interval (in seconds) for flushing writes to log files. 
+  # 0 will flush on every message.
+  config :flush_interval, :validate => :number, :default => 2
+
+  # Gzip the output stream before writing to disk.
+  config :gzip, :validate => :boolean, :default => false
+
+  public
+  def register
+    require "fileutils" # For mkdir_p
+
+    workers_not_supported
+
+    @files = {}
+    now = Time.now
+    @last_flush_cycle = now
+    @last_stale_cleanup_cycle = now
+    flush_interval = @flush_interval.to_i
+    @stale_cleanup_interval = 10
+  end # def register
+
+  public
+  def receive(event)
+    return unless output?(event)
+
+    path = event.sprintf(@path)
+    fd = open(path)
+
+    # TODO(sissel): Check if we should rotate the file.
+
+    if @message_format
+      output = event.sprintf(@message_format)
+    else
+      output = event.to_json
+    end
+
+    fd.write(output)
+    fd.write("\n")
+
+    flush(fd)
+    close_stale_files
+  end # def receive
+
+  def teardown
+    @logger.debug("Teardown: closing files")
+    @files.each do |path, fd|
+      begin
+        fd.close
+        @logger.debug("Closed file #{path}", :fd => fd)
+      rescue Exception => e
+        @logger.error("Excpetion while flushing and closing files.", :exception => e)
+      end
+    end
+    finished
+  end
+
+  private
+  def flush(fd)
+    if flush_interval > 0
+      flush_pending_files
+    else
+      fd.flush
+    end
+  end
+
+  # every flush_interval seconds or so (triggered by events, but if there are no events there's no point flushing files anyway)
+  def flush_pending_files
+    return unless Time.now - @last_flush_cycle >= flush_interval
+    @logger.debug("Starting flush cycle")
+    @files.each do |path, fd|
+      @logger.debug("Flushing file", :path => path, :fd => fd)
+      fd.flush
+    end
+    @last_flush_cycle = Time.now
+  end
+
+  # every 10 seconds or so (triggered by events, but if there are no events there's no point closing files anyway)
+  def close_stale_files
+    now = Time.now
+    return unless now - @last_stale_cleanup_cycle >= @stale_cleanup_interval
+    @logger.info("Starting stale files cleanup cycle", :files => @files)
+    inactive_files = @files.select { |path, fd| not fd.active }
+    @logger.debug("%d stale files found" % inactive_files.count, :inactive_files => inactive_files)
+    inactive_files.each do |path, fd|
+      @logger.info("Closing file %s" % path)
+      fd.close
+      @files.delete(path)
+    end
+    # mark all files as inactive, a call to write will mark them as active again
+    @files.each { |path, fd| fd.active = false }
+    @last_stale_cleanup_cycle = now
+  end
+
+  def open(path)
+    return @files[path] if @files.include?(path) and not @files[path].nil?
+
+    @logger.info("Opening file", :path => path)
+
+    dir = File.dirname(path)
+    if !Dir.exists?(dir)
+      @logger.info("Creating directory", :directory => dir)
+      FileUtils.mkdir_p(dir) 
+    end
+
+    # work around a bug opening fifos (bug JRUBY-6280)
+    stat = File.stat(path) rescue nil
+    if stat and stat.ftype == "fifo" and RUBY_PLATFORM == "java"
+      fd = java.io.FileWriter.new(java.io.File.new(path))
+    else
+      fd = File.new(path, "a")
+    end
+    if gzip
+      fd = Zlib::GzipWriter.new(fd)
+    end
+    @files[path] = IOWriter.new(fd)
+  end
+end # class LogStash::Outputs::File
+
+# wrapper class
+class IOWriter
+  def initialize(io)
+    @io = io
+  end
+  def write(*args)
+    @io.write(*args)
+    @active = true
+  end
+  def flush
+    @io.flush
+    if @io.class == Zlib::GzipWriter
+      @io.to_io.flush
+    end
+  end
+  def method_missing(method_name, *args, &block)
+    if @io.respond_to?(method_name)
+      @io.send(method_name, *args, &block)
+    else
+      super
+    end
+  end
+  attr_accessor :active
+end
diff --git a/lib/logstash/outputs/ganglia.rb b/lib/logstash/outputs/ganglia.rb
new file mode 100644
index 00000000000..745a900ff4e
--- /dev/null
+++ b/lib/logstash/outputs/ganglia.rb
@@ -0,0 +1,75 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+
+# This output allows you to pull metrics from your logs and ship them to
+# ganglia's gmond. This is heavily based on the graphite output.
+class LogStash::Outputs::Ganglia < LogStash::Outputs::Base
+  config_name "ganglia"
+  milestone 2
+
+  # The address of the ganglia server.
+  config :host, :validate => :string, :default => "localhost"
+
+  # The port to connect on your ganglia server.
+  config :port, :validate => :number, :default => 8649
+
+  # The metric to use. This supports dynamic strings like `%{host}`
+  config :metric, :validate => :string, :required => true
+
+  # The value to use. This supports dynamic strings like `%{bytes}`
+  # It will be coerced to a floating point value. Values which cannot be
+  # coerced will zero (0)
+  config :value, :validate => :string, :required => true
+
+  # The type of value for this metric.
+  config :metric_type, :validate => %w{string int8 uint8 int16 uint16 int32 uint32 float double},
+    :default => "uint8"
+
+  # Gmetric units for metric, such as "kb/sec" or "ms" or whatever unit
+  # this metric uses.
+  config :units, :validate => :string, :default => ""
+
+  # Maximum time in seconds between gmetric calls for this metric.
+  config :max_interval, :validate => :number, :default => 60
+
+  # Lifetime in seconds of this metric
+  config :lifetime, :validate => :number, :default => 300
+
+  # Metric group
+  config :group, :validate => :string, :default => ""
+
+  # Metric slope, represents metric behavior
+  config :slope, :validate => %w{zero positive negative both unspecified}, :default => "both"
+
+  def register
+    require "gmetric"
+  end # def register
+
+  public
+  def receive(event)
+    return unless output?(event)
+
+    # gmetric only takes integer values, so convert it to int.
+    case @metric_type
+      when "string"
+        localvalue = event.sprintf(@value)
+      when "float"
+        localvalue = event.sprintf(@value).to_f
+      when "double"
+        localvalue = event.sprintf(@value).to_f
+      else # int8|uint8|int16|uint16|int32|uint32
+        localvalue = event.sprintf(@value).to_i
+    end
+    Ganglia::GMetric.send(@host, @port, {
+      :name => event.sprintf(@metric),
+      :units => @units,
+      :type => @metric_type,
+      :value => localvalue,
+      :group => @group,
+      :slope => @slope,
+      :tmax => @max_interval,
+      :dmax => @lifetime
+    })
+  end # def receive
+end # class LogStash::Outputs::Ganglia
diff --git a/lib/logstash/outputs/gelf.rb b/lib/logstash/outputs/gelf.rb
index 28368dd077a..1d5dc161f99 100644
--- a/lib/logstash/outputs/gelf.rb
+++ b/lib/logstash/outputs/gelf.rb
@@ -1,53 +1,211 @@
+# encoding: utf-8
 require "logstash/namespace"
 require "logstash/outputs/base"
 
-# GELF output. This is most useful if you want to use logstash
-# to output events to graylog2.
+# This output generates messages in GELF format. This is most useful if you
+# want to use Logstash to output events to Graylog2.
 #
-# http://www.graylog2.org/about/gelf
+# More information at <http://graylog2.org/gelf#specs>
 class LogStash::Outputs::Gelf < LogStash::Outputs::Base
 
   config_name "gelf"
-  
-  # graylog2 server address
+  milestone 2
+
+  # Graylog2 server IP address or hostname.
   config :host, :validate => :string, :required => true
 
-  # graylog2 server port
+  # Graylog2 server port number.
   config :port, :validate => :number, :default => 12201
 
-  # The GELF chunksize
+  # The GELF chunksize. You usually don't need to change this.
   config :chunksize, :validate => :number, :default => 1420
 
-  # The GELF message level
-  config :level, :validate => :number, :default => 1
+  # Allow overriding of the GELF `sender` field. This is useful if you
+  # want to use something other than the event's source host as the
+  # "sender" of an event. A common case for this is using the application name
+  # instead of the hostname.
+  config :sender, :validate => :string, :default => "%{host}"
+
+  # The GELF message level. Dynamic values like %{level} are permitted here;
+  # useful if you want to parse the 'log level' from an event and use that
+  # as the GELF level/severity.
+  #
+  # Values here can be integers [0..7] inclusive or any of
+  # "debug", "info", "warn", "error", "fatal" (case insensitive).
+  # Single-character versions of these are also valid, "d", "i", "w", "e", "f",
+  # "u"
+  # The following additional severity\_labels from Logstash's  syslog\_pri filter
+  # are accepted: "emergency", "alert", "critical",  "warning", "notice", and
+  # "informational".
+  config :level, :validate => :array, :default => [ "%{severity}", "INFO" ]
+
+  # The GELF facility. Dynamic values like %{foo} are permitted here; this
+  # is useful if you need to use a value from the event as the facility name.
+  # Should now be sent as an underscored "additional field" (e.g. `\_facility`)
+  config :facility, :validate => :string, :deprecated => true
+
+  # The GELF line number; this is usually the line number in your program where
+  # the log event originated. Dynamic values like %{foo} are permitted here, but the
+  # value should be a number.
+  # Should now be sent as an underscored "additional field" (e.g. `\_line`).
+  config :line, :validate => :string, :deprecated => true
+
+  # The GELF file; this is usually the source code file in your program where
+  # the log event originated. Dynamic values like %{foo} are permitted here.
+  # Should now be sent as an underscored "additional field" (e.g. `\_file`).
+  config :file, :validate => :string, :deprecated => true
+
+  # Should Logstash ship metadata within event object? This will cause Logstash
+  # to ship any fields in the event (such as those created by grok) in the GELF
+  # messages. These will be sent as underscored "additional fields".
+  config :ship_metadata, :validate => :boolean, :default => true
+
+  # Ship tags within events. This will cause Logstash to ship the tags of an
+  # event as the field `\_tags`.
+  config :ship_tags, :validate => :boolean, :default => true
+
+  # Ignore these fields when `ship_metadata` is set. Typically this lists the
+  # fields used in dynamic values for GELF fields.
+  config :ignore_metadata, :validate => :array, :default => [ "@timestamp", "@version", "severity", "host", "source_host", "source_path", "short_message" ]
 
-  # The GELF facility.
-  config :facility, :validate => :string, :default => "logstash-gelf"
+  # The GELF custom field mappings. GELF supports arbitrary attributes as custom
+  # fields. This exposes that. Exclude the `_` portion of the field name
+  # e.g. `custom_fields => ['foo_field', 'some_value']
+  # sets `_foo_field` = `some_value`.
+  config :custom_fields, :validate => :hash, :default => {}
+
+  # The GELF full message. Dynamic values like %{foo} are permitted here.
+  config :full_message, :validate => :string, :default => "%{message}"
+
+  # The GELF short message field name. If the field does not exist or is empty,
+  # the event message is taken instead.
+  config :short_message, :validate => :string, :default => "short_message"
 
   public
   def register
     require "gelf" # rubygem 'gelf'
     option_hash = Hash.new
-    option_hash['level'] = @level
-    option_hash['facility'] = @facility
 
-    @gelf = GELF::Notifier.new(@host, @port, @chunksize, option_hash)
+    #@gelf = GELF::Notifier.new(@host, @port, @chunksize, option_hash)
+    @gelf = GELF::Notifier.new(@host, @port, @chunksize)
+
+    # This sets the 'log level' of gelf; since we're forwarding messages, we'll
+    # want to forward *all* messages, so set level to 0 so all messages get
+    # shipped
+    @gelf.level = 0
+
+    # Since we use gelf-rb which assumes the severity level integer
+    # is coming from a ruby logging subsystem, we need to instruct it
+    # that the levels we provide should be mapped directly since they're
+    # already RFC 5424 compliant
+    # this requires gelf-rb commit bb1f4a9 which added the level_mapping def
+    level_mapping = Hash.new
+    (0..7).step(1) { |l| level_mapping[l]=l }
+    @gelf.level_mapping = level_mapping
+
+    # If we leave that set, the gelf gem will extract the file and line number
+    # of the source file that logged the message (i.e. logstash/gelf.rb:138).
+    # With that set to false, it can use the actual event's filename (i.e.
+    # /var/log/syslog), which is much more useful
+    @gelf.collect_file_and_line = false
+
+    # these are syslog words and abbreviations mapped to RFC 5424 integers
+    # and logstash's syslog_pri filter
+    @level_map = {
+      "debug" => 7, "d" => 7,
+      "info" => 6, "i" => 6, "informational" => 6,
+      "notice" => 5, "n" => 5,
+      "warn" => 4, "w" => 4, "warning" => 4,
+      "error" => 3, "e" => 3,
+      "critical" => 2, "c" => 2,
+      "alert" => 1, "a" => 1,
+      "emergency" => 0, "e" => 0,
+     }
   end # def register
 
   public
   def receive(event)
+    return unless output?(event)
+
+    # We have to make our own hash here because GELF expects a hash
+    # with a specific format.
     m = Hash.new
-    m["short_message"] = (event.fields["message"] or event.message)
-    m["full_message"] = (event.message)
-    m["host"] = event["@source_host"]
-    m["file"] = event["@source_path"]
-    m["level"] = 1
-
-    event.fields.each do |name, value|
-      next if value == nil or value.empty?
-      m["#{name}"] = value
+
+    m["short_message"] = event["message"]
+    if event[@short_message]
+      v = event[@short_message]
+      short_message = (v.is_a?(Array) && v.length == 1) ? v.first : v
+      short_message = short_message.to_s
+      if !short_message.empty?
+        m["short_message"] = short_message
+      end
+    end
+
+    m["full_message"] = event.sprintf(@full_message)
+
+    m["host"] = event.sprintf(@sender)
+
+    # deprecated fields
+    m["facility"] = event.sprintf(@facility) if @facility
+    m["file"] = event.sprintf(@file) if @file
+    m["line"] = event.sprintf(@line) if @line
+    m["line"] = m["line"].to_i if m["line"].is_a?(String) and m["line"] === /^[\d]+$/
+
+    if @ship_metadata
+      event.to_hash.each do |name, value|
+        next if value == nil
+        next if name == "message"
+
+        # Trim leading '_' in the event
+        name = name[1..-1] if name.start_with?('_')
+        name = "_id" if name == "id"  # "_id" is reserved, so use "__id"
+        if !value.nil? and !@ignore_metadata.include?(name)
+          if value.is_a?(Array)
+            m["_#{name}"] = value.join(', ')
+          elsif value.is_a?(Hash)
+            value.each do |hash_name, hash_value|
+              m["_#{name}_#{hash_name}"] = hash_value
+            end
+          else
+            # Non array values should be presented as-is
+            # https://logstash.jira.com/browse/LOGSTASH-113
+            m["_#{name}"] = value
+          end
+        end
+      end
+    end
+
+    if @ship_tags
+      m["_tags"] = event["tags"].join(', ') if event["tags"]
+    end
+
+    if @custom_fields
+      @custom_fields.each do |field_name, field_value|
+        m["_#{field_name}"] = field_value unless field_name == 'id'
+      end
+    end
+
+    # Probe severity array levels
+    level = nil
+    if @level.is_a?(Array)
+      @level.each do |value|
+        parsed_value = event.sprintf(value)
+        next if value.count('%{') > 0 and parsed_value == value
+
+        level = parsed_value
+        break
+      end
+    else
+      level = event.sprintf(@level.to_s)
+    end
+    m["level"] = (@level_map[level.downcase] || level).to_i
+
+    @logger.debug(["Sending GELF event", m])
+    begin
+      @gelf.notify!(m, :timestamp => event.timestamp.to_f)
+    rescue
+      @logger.warn("Trouble sending GELF event", :gelf_event => m,
+                   :event => event, :error => $!)
     end
-    m["timestamp"] = event.timestamp
-    @gelf.notify!(m)
   end # def receive
 end # class LogStash::Outputs::Gelf
diff --git a/lib/logstash/outputs/graphite.rb b/lib/logstash/outputs/graphite.rb
new file mode 100644
index 00000000000..df47484b36f
--- /dev/null
+++ b/lib/logstash/outputs/graphite.rb
@@ -0,0 +1,146 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+require "socket"
+
+# This output allows you to pull metrics from your logs and ship them to
+# Graphite. Graphite is an open source tool for storing and graphing metrics.
+#
+# An example use case: Some applications emit aggregated stats in the logs
+# every 10 seconds. Using the grok filter and this output, it is possible to
+# capture the metric values from the logs and emit them to Graphite.
+class LogStash::Outputs::Graphite < LogStash::Outputs::Base
+  config_name "graphite"
+  milestone 2
+
+  EXCLUDE_ALWAYS = [ "@timestamp", "@version" ]
+
+  DEFAULT_METRICS_FORMAT = "*"
+  METRIC_PLACEHOLDER = "*"
+
+  # The hostname or IP address of the Graphite server.
+  config :host, :validate => :string, :default => "localhost"
+
+  # The port to connect to on the Graphite server.
+  config :port, :validate => :number, :default => 2003
+
+  # Interval between reconnect attempts to Carbon.
+  config :reconnect_interval, :validate => :number, :default => 2
+
+  # Should metrics be resent on failure?
+  config :resend_on_failure, :validate => :boolean, :default => false
+
+  # The metric(s) to use. This supports dynamic strings like %{host}
+  # for metric names and also for values. This is a hash field with key 
+  # being the metric name, value being the metric value. Example:
+  #
+  #     [ "%{host}/uptime", "%{uptime_1m}" ]
+  #
+  # The value will be coerced to a floating point value. Values which cannot be
+  # coerced will be set to zero (0). You may use either `metrics` or `fields_are_metrics`,
+  # but not both.
+  config :metrics, :validate => :hash, :default => {}
+
+  # An array indicating that these event fields should be treated as metrics
+  # and will be sent verbatim to Graphite. You may use either `fields_are_metrics`
+  # or `metrics`, but not both.
+  config :fields_are_metrics, :validate => :boolean, :default => false
+
+  # Include only regex matched metric names.
+  config :include_metrics, :validate => :array, :default => [ ".*" ]
+
+  # Exclude regex matched metric names, by default exclude unresolved %{field} strings.
+  config :exclude_metrics, :validate => :array, :default => [ "%\{[^}]+\}" ]
+
+  # Enable debug output.
+  config :debug, :validate => :boolean, :default => false, :deprecated => "This setting was never used by this plugin. It will be removed soon."
+
+  # Defines the format of the metric string. The placeholder '*' will be
+  # replaced with the name of the actual metric.
+  #
+  #     metrics_format => "foo.bar.*.sum"
+  #
+  # NOTE: If no metrics_format is defined, the name of the metric will be used as fallback.
+  config :metrics_format, :validate => :string, :default => DEFAULT_METRICS_FORMAT
+
+  def register
+    @include_metrics.collect!{|regexp| Regexp.new(regexp)}
+    @exclude_metrics.collect!{|regexp| Regexp.new(regexp)}
+
+    if @metrics_format && !@metrics_format.include?(METRIC_PLACEHOLDER)
+      @logger.warn("metrics_format does not include placeholder #{METRIC_PLACEHOLDER} .. falling back to default format: #{DEFAULT_METRICS_FORMAT.inspect}")
+
+      @metrics_format = DEFAULT_METRICS_FORMAT
+    end
+
+    connect
+  end # def register
+
+  def connect
+    # TODO(sissel): Test error cases. Catch exceptions. Find fortune and glory. Retire to yak farm.
+    begin
+      @socket = TCPSocket.new(@host, @port)
+    rescue Errno::ECONNREFUSED => e
+      @logger.warn("Connection refused to graphite server, sleeping...",
+                   :host => @host, :port => @port)
+      sleep(@reconnect_interval)
+      retry
+    end
+  end # def connect
+
+  def construct_metric_name(metric)
+    if @metrics_format
+      return @metrics_format.gsub(METRIC_PLACEHOLDER, metric)
+    end
+
+    metric
+  end
+
+  public
+  def receive(event)
+    return unless output?(event)
+
+    # Graphite message format: metric value timestamp\n
+
+    messages = []
+    timestamp = event.sprintf("%{+%s}")
+
+    if @fields_are_metrics
+      @logger.debug("got metrics event", :metrics => event.to_hash)
+      event.to_hash.each do |metric,value|
+        next if EXCLUDE_ALWAYS.include?(metric)
+        next unless @include_metrics.empty? || @include_metrics.any? { |regexp| metric.match(regexp) }
+        next if @exclude_metrics.any? {|regexp| metric.match(regexp)}
+        messages << "#{construct_metric_name(metric)} #{event.sprintf(value.to_s).to_f} #{timestamp}"
+      end
+    else
+      @metrics.each do |metric, value|
+        @logger.debug("processing", :metric => metric, :value => value)
+        metric = event.sprintf(metric)
+        next unless @include_metrics.any? {|regexp| metric.match(regexp)}
+        next if @exclude_metrics.any? {|regexp| metric.match(regexp)}
+        messages << "#{construct_metric_name(event.sprintf(metric))} #{event.sprintf(value).to_f} #{timestamp}"
+      end
+    end
+
+    if messages.empty?
+      @logger.debug("Message is empty, not sending anything to Graphite", :messages => messages, :host => @host, :port => @port)
+    else
+      message = messages.join("\n")
+      @logger.debug("Sending carbon messages", :messages => messages, :host => @host, :port => @port)
+
+      # Catch exceptions like ECONNRESET and friends, reconnect on failure.
+      # TODO(sissel): Test error cases. Catch exceptions. Find fortune and glory.
+      begin
+        @socket.puts(message)
+      rescue Errno::EPIPE, Errno::ECONNRESET => e
+        @logger.warn("Connection to graphite server died",
+                     :exception => e, :host => @host, :port => @port)
+        sleep(@reconnect_interval)
+        connect
+        retry if @resend_on_failure
+      end
+    end
+
+  end # def receive
+end # class LogStash::Outputs::Graphite
diff --git a/lib/logstash/outputs/hipchat.rb b/lib/logstash/outputs/hipchat.rb
new file mode 100644
index 00000000000..00617b24d7d
--- /dev/null
+++ b/lib/logstash/outputs/hipchat.rb
@@ -0,0 +1,80 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/outputs/http"
+
+# This output allows you to write events to [HipChat](https://www.hipchat.com/).
+#
+class LogStash::Outputs::HipChat < LogStash::Outputs::Base
+
+  config_name "hipchat"
+  milestone 1
+
+  # The HipChat authentication token.
+  config :token, :validate => :string, :required => true
+
+  # The ID or name of the room.
+  config :room_id, :validate => :string, :required => true
+
+  # The name the message will appear be sent from.
+  config :from, :validate => :string, :default => "logstash"
+
+  # Whether or not this message should trigger a notification for people in the room.
+  config :trigger_notify, :validate => :boolean, :default => false
+
+  # Background color for message.
+  # HipChat currently supports one of "yellow", "red", "green", "purple",
+  # "gray", or "random". (default: yellow)
+  config :color, :validate => :string, :default => "yellow"
+
+  # Message format to send, event tokens are usable here.
+  config :format, :validate => :string, :default => "%{message}"
+
+  public
+  def register
+    require "ftw"
+    require "uri"
+
+    @agent = FTW::Agent.new
+
+    @url = "https://api.hipchat.com/v1/rooms/message?auth_token=" + @token
+    @content_type = "application/x-www-form-urlencoded"
+  end # def register
+
+  public
+  def receive(event)
+    return unless output?(event)
+
+    hipchat_data = Hash.new
+    hipchat_data['room_id'] = event.sprintf(@room_id)
+    hipchat_data['from']    = @from
+    hipchat_data['color']   = @color
+    hipchat_data['notify']  = @trigger_notify ? "1" : "0"
+    hipchat_data['message'] = event.sprintf(@format)
+
+    @logger.debug("HipChat data", :hipchat_data => hipchat_data)
+
+    begin
+      request = @agent.post(@url)
+      request["Content-Type"] = @content_type
+      request.body = encode(hipchat_data)
+
+      response = @agent.execute(request)
+
+      # Consume body to let this connection be reused
+      rbody = ""
+      response.read_body { |c| rbody << c }
+      #puts rbody
+    rescue Exception => e
+      @logger.warn("Unhandled exception", :request => request, :response => response, :exception => e, :stacktrace => e.backtrace)
+    end
+  end # def receive
+
+  # shamelessly lifted this from the LogStash::Outputs::Http, I'd rather put this
+  # in a common place for both to use, but unsure where that place is or should be
+  def encode(hash)
+    return hash.collect do |key, value|
+      CGI.escape(key) + "=" + CGI.escape(value)
+    end.join("&")
+  end # def encode
+
+end # class LogStash::Outputs::HipChat
diff --git a/lib/logstash/outputs/http.rb b/lib/logstash/outputs/http.rb
new file mode 100644
index 00000000000..8b955d16968
--- /dev/null
+++ b/lib/logstash/outputs/http.rb
@@ -0,0 +1,143 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+require "logstash/json"
+
+class LogStash::Outputs::Http < LogStash::Outputs::Base
+  # This output lets you `PUT` or `POST` events to a
+  # generic HTTP(S) endpoint
+  #
+  # Additionally, you are given the option to customize
+  # the headers sent as well as basic customization of the
+  # event json itself.
+
+  config_name "http"
+  milestone 1
+
+  # URL to use
+  config :url, :validate => :string, :required => :true
+
+  # validate SSL?
+  config :verify_ssl, :validate => :boolean, :default => true
+
+  # What verb to use
+  # only put and post are supported for now
+  config :http_method, :validate => ["put", "post"], :required => :true
+
+  # Custom headers to use
+  # format is `headers => ["X-My-Header", "%{host}"]
+  config :headers, :validate => :hash
+
+  # Content type
+  #
+  # If not specified, this defaults to the following:
+  #
+  # * if format is "json", "application/json"
+  # * if format is "form", "application/x-www-form-urlencoded"
+  config :content_type, :validate => :string
+
+  # This lets you choose the structure and parts of the event that are sent.
+  #
+  #
+  # For example:
+  #
+  #    mapping => ["foo", "%{host}", "bar", "%{type}"]
+  config :mapping, :validate => :hash
+
+  # Set the format of the http body.
+  #
+  # If form, then the body will be the mapping (or whole event) converted
+  # into a query parameter string (foo=bar&baz=fizz...)
+  #
+  # If message, then the body will be the result of formatting the event according to message
+  #
+  # Otherwise, the event is sent as json.
+  config :format, :validate => ["json", "form", "message"], :default => "json"
+
+  config :message, :validate => :string
+
+  public
+  def register
+    require "ftw"
+    require "uri"
+    @agent = FTW::Agent.new
+    # TODO(sissel): SSL verify mode?
+
+    if @content_type.nil?
+      case @format
+        when "form" ; @content_type = "application/x-www-form-urlencoded"
+        when "json" ; @content_type = "application/json"
+      end
+    end
+    if @format == "message"
+      if @message.nil?
+        raise "message must be set if message format is used"
+      end
+      if @content_type.nil?
+        raise "content_type must be set if message format is used"
+      end
+      unless @mapping.nil?
+        @logger.warn "mapping is not supported and will be ignored if message format is used"
+      end
+    end
+  end # def register
+
+  public
+  def receive(event)
+    return unless output?(event)
+
+    if @mapping
+      evt = Hash.new
+      @mapping.each do |k,v|
+        evt[k] = event.sprintf(v)
+      end
+    else
+      evt = event.to_hash
+    end
+
+    case @http_method
+    when "put"
+      request = @agent.put(event.sprintf(@url))
+    when "post"
+      request = @agent.post(event.sprintf(@url))
+    else
+      @logger.error("Unknown verb:", :verb => @http_method)
+    end
+
+    if @headers
+      @headers.each do |k,v|
+        request.headers[k] = event.sprintf(v)
+      end
+    end
+
+    request["Content-Type"] = @content_type
+
+    begin
+      if @format == "json"
+        request.body = LogStash::Json.dump(evt)
+      elsif @format == "message"
+        request.body = event.sprintf(@message)
+      else
+        request.body = encode(evt)
+      end
+      #puts "#{request.port} / #{request.protocol}"
+      #puts request
+      #puts
+      #puts request.body
+      response = @agent.execute(request)
+
+      # Consume body to let this connection be reused
+      rbody = ""
+      response.read_body { |c| rbody << c }
+      #puts rbody
+    rescue Exception => e
+      @logger.warn("Unhandled exception", :request => request, :response => response, :exception => e, :stacktrace => e.backtrace)
+    end
+  end # def receive
+
+  def encode(hash)
+    return hash.collect do |key, value|
+      CGI.escape(key) + "=" + CGI.escape(value)
+    end.join("&")
+  end # def encode
+end
diff --git a/lib/logstash/outputs/internal.rb b/lib/logstash/outputs/internal.rb
deleted file mode 100644
index bfa2b291816..00000000000
--- a/lib/logstash/outputs/internal.rb
+++ /dev/null
@@ -1,32 +0,0 @@
-require "logstash/namespace"
-require "logstash/outputs/base"
-
-class LogStash::Outputs::Internal < LogStash::Outputs::Base
-  config_name "internal"
-
-  attr_accessor :callback
-
-  public
-  def register
-    @logger.info("Registering internal output (for testing!)")
-    @callbacks ||= []
-  end # def register
-
-  public
-  def receive(event)
-    if @callbacks.empty?
-      @logger.error("No callback for output #{@url}, cannot receive")
-      return
-    end
-
-    @callbacks.each do |callback|
-      callback.call(event)
-    end
-  end # def event
-
-  public
-  def subscribe(&block)
-    @callbacks ||= []
-    @callbacks << block
-  end
-end # class LogStash::Outputs::Internal
diff --git a/lib/logstash/outputs/irc.rb b/lib/logstash/outputs/irc.rb
new file mode 100644
index 00000000000..7a5791eaf62
--- /dev/null
+++ b/lib/logstash/outputs/irc.rb
@@ -0,0 +1,88 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+require "thread"
+
+# Write events to IRC
+#
+class LogStash::Outputs::Irc < LogStash::Outputs::Base
+
+  config_name "irc"
+  milestone 1
+
+  # Address of the host to connect to
+  config :host, :validate => :string, :required => true
+
+  # Port on host to connect to.
+  config :port, :validate => :number, :default => 6667
+
+  # IRC Nickname
+  config :nick, :validate => :string, :default => "logstash"
+
+  # IRC Username
+  config :user, :validate => :string, :default => "logstash"
+
+  # IRC Real name
+  config :real, :validate => :string, :default => "logstash"
+
+  # IRC server password
+  config :password, :validate => :password
+
+  # Channels to broadcast to.
+  # 
+  # These should be full channel names including the '#' symbol, such as
+  # "#logstash".
+  config :channels, :validate => :array, :required => true
+
+  # Message format to send, event tokens are usable here
+  config :format, :validate => :string, :default => "%{message}"
+
+  # Set this to true to enable SSL.
+  config :secure, :validate => :boolean, :default => false
+
+  # Limit the rate of messages sent to IRC in messages per second.
+  config :messages_per_second, :validate => :number, :default => 0.5
+
+  # Static string before event
+  config :pre_string, :validate => :string, :required => false
+  
+  # Static string after event
+  config :post_string, :validate => :string, :required => false
+
+  public
+  def register
+    require "cinch"
+    @irc_queue = Queue.new
+    @logger.info("Connecting to irc server", :host => @host, :port => @port, :nick => @nick, :channels => @channels)
+
+    @bot = Cinch::Bot.new
+    @bot.loggers.clear
+    @bot.configure do |c|
+      c.server = @host
+      c.port = @port
+      c.nick = @nick
+      c.user = @user
+      c.realname = @real
+      c.channels = @channels
+      c.password = @password.value rescue nil
+      c.ssl.use = @secure
+      c.messages_per_second = @messages_per_second if @messages_per_second
+    end
+    Thread.new(@bot) do |bot|
+      bot.start
+    end
+  end # def register
+
+  public
+  def receive(event)
+    return unless output?(event)
+    @logger.debug("Sending message to channels", :event => event)
+    text = event.sprintf(@format)
+    @bot.channels.each do |channel|
+      @logger.debug("Sending to...", :channel => channel, :text => text)
+      channel.msg(pre_string) if !@pre_string.nil?
+      channel.msg(text)
+      channel.msg(post_string) if !@post_string.nil?
+    end # channels.each
+  end # def receive
+end # class LogStash::Outputs::Irc
diff --git a/lib/logstash/outputs/juggernaut.rb b/lib/logstash/outputs/juggernaut.rb
new file mode 100644
index 00000000000..f6985430bcd
--- /dev/null
+++ b/lib/logstash/outputs/juggernaut.rb
@@ -0,0 +1,106 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+require "logstash/event"
+require "logstash/json"
+
+# Push messages to the juggernaut websockets server:
+#
+# * https://github.com/maccman/juggernaut
+#
+# Wraps Websockets and supports other methods (including xhr longpolling) This
+# is basically, just an extension of the redis output (Juggernaut pulls
+# messages from redis).  But it pushes messages to a particular channel and
+# formats the messages in the way juggernaut expects.
+class LogStash::Outputs::Juggernaut < LogStash::Outputs::Base
+
+  config_name "juggernaut"
+  milestone 1
+
+  # The hostname of the redis server to which juggernaut is listening.
+  config :host, :validate => :string, :default => "127.0.0.1"
+
+  # The port to connect on.
+  config :port, :validate => :number, :default => 6379
+
+  # The redis database number.
+  config :db, :validate => :number, :default => 0
+
+  # Redis initial connection timeout in seconds.
+  config :timeout, :validate => :number, :default => 5
+
+  # Password to authenticate with.  There is no authentication by default.
+  config :password, :validate => :password
+
+  # List of channels to which to publish. Dynamic names are
+  # valid here, for example "logstash-%{type}".
+  config :channels, :validate => :array, :required => true
+
+  # How should the message be formatted before pushing to the websocket.
+  config :message_format, :validate => :string
+
+  public
+  def register
+    require 'redis'
+
+    if not @channels
+      raise RuntimeError.new(
+        "Must define the channels on which to publish the messages"
+      )
+    end
+    # end TODO
+
+    @redis = nil
+  end # def register
+
+  private
+  def connect
+    Redis.new(
+      :host => @host,
+      :port => @port,
+      :timeout => @timeout,
+      :db => @db,
+      :password => @password
+    )
+  end # def connect
+
+  # A string used to identify a redis instance in log messages
+  private
+  def identity
+    @name || "redis://#{@password}@#{@host}:#{@port}/#{@db} #{@data_type}:#{@key}"
+  end
+
+
+  public
+  def receive(event)
+    return unless output?(event)
+    begin
+      @redis ||= connect
+      if @message_format
+        formatted = event.sprintf(@message_format)
+      else
+        formatted = event.to_json
+      end
+      juggernaut_message = {
+        "channels" => @channels.collect{ |x| event.sprintf(x) },
+        "data" => event["message"]
+      }
+
+      @redis.publish 'juggernaut', LogStash::Json.dump(juggernaut_message)
+    rescue => e
+      @logger.warn("Failed to send event to redis", :event => event,
+                   :identity => identity, :exception => e,
+                   :backtrace => e.backtrace)
+      raise e
+    end
+  end # def receive
+
+  public
+  def teardown
+    if @data_type == 'channel' and @redis
+      @redis.quit
+      @redis = nil
+    end
+  end
+
+end
diff --git a/lib/logstash/outputs/lumberjack.rb b/lib/logstash/outputs/lumberjack.rb
new file mode 100644
index 00000000000..4e92c6a0d82
--- /dev/null
+++ b/lib/logstash/outputs/lumberjack.rb
@@ -0,0 +1,62 @@
+# encoding: utf-8
+class LogStash::Outputs::Lumberjack < LogStash::Outputs::Base
+
+  config_name "lumberjack"
+  milestone 1
+
+  # list of addresses lumberjack can send to
+  config :hosts, :validate => :array, :required => true
+
+  # the port to connect to
+  config :port, :validate => :number, :required => true
+
+  # ssl certificate to use
+  config :ssl_certificate, :validate => :path, :required => true
+
+  # window size
+  config :window_size, :validate => :number, :default => 5000
+
+  public
+  def register
+    require 'lumberjack/client'
+    connect
+
+    @codec.on_event do |payload|
+      begin
+        @client.write({ 'line' => payload })
+      rescue Exception => e
+        @logger.error("Client write error, trying connect", :e => e, :backtrace => e.backtrace)
+        connect
+        retry
+      end # begin
+    end # @codec
+  end # def register
+
+  public
+  def receive(event)
+    return unless output?(event)
+    if event == LogStash::SHUTDOWN
+      finished
+      return
+    end # LogStash::SHUTDOWN
+    @codec.encode(event)
+  end # def receive
+
+  private 
+  def connect
+    require 'resolv'
+    @logger.info("Connecting to lumberjack server.", :addresses => @hosts, :port => @port, 
+        :ssl_certificate => @ssl_certificate, :window_size => @window_size)
+    begin
+      ips = []
+      @hosts.each { |host| ips += Resolv.getaddresses host }
+      @client = Lumberjack::Client.new(:addresses => ips.uniq, :port => @port, 
+        :ssl_certificate => @ssl_certificate, :window_size => @window_size)
+    rescue Exception => e
+      @logger.error("All hosts unavailable, sleeping", :hosts => ips.uniq, :e => e, 
+        :backtrace => e.backtrace)
+      sleep(10)
+      retry
+    end
+  end
+end
diff --git a/lib/logstash/outputs/mongodb.rb b/lib/logstash/outputs/mongodb.rb
deleted file mode 100644
index 1f20847a63b..00000000000
--- a/lib/logstash/outputs/mongodb.rb
+++ /dev/null
@@ -1,33 +0,0 @@
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-class LogStash::Outputs::Mongodb < LogStash::Outputs::Base
-
-  config_name "mongodb"
-
-  # your mongdob host
-  config :host, :validate => :string, :required => true
-
-  # the mongodb port
-  config :port, :validate => :number, :default => 27017
-
-  # The database to use
-  config :database, :validate => :string, :required => true
-
-  # The collection to use. This value can use %{foo} values to dynamically
-  # select a collection based on data in th eevent.
-  config :collection, :validate => :string, :required => true
-
-  public
-  def register
-    require "mongo"
-    # TODO(petef): support authentication
-    # TODO(petef): check for errors
-    @mongodb = Mongo::Connection.new(@host, @port).db(@database)
-  end # def register
-
-  public
-  def receive(event)
-    @mongodb.collection(event.sprintf(@collection)).insert(event.to_hash)
-  end # def receive
-end # class LogStash::Outputs::Mongodb
diff --git a/lib/logstash/outputs/nagios.rb b/lib/logstash/outputs/nagios.rb
index 7c81fff5837..5677872e0ac 100644
--- a/lib/logstash/outputs/nagios.rb
+++ b/lib/logstash/outputs/nagios.rb
@@ -1,51 +1,47 @@
+# encoding: utf-8
 require "logstash/namespace"
 require "logstash/outputs/base"
 
-# The nagios output is used for sending passive check results to nagios via the
-# nagios command file. 
+# The Nagios output is used for sending passive check results to Nagios via the
+# Nagios command file. This output currently supports Nagios 3.
 #
-# For this output to work, your event must have the following fields:
-#   "nagios_host"
-#   "nagios_service"
+# For this output to work, your event _must_ have the following Logstash event fields:
 #
-# This field is supported, but optional:
-#   "nagios_annotation"
+#  * `nagios\_host`
+#  * `nagios\_service`
 #
-# The easiest way to use this output is with the grep filter.
-# Presumably, you only want certain events matching a given pattern
-# to send events to nagios. So use grep to match and also to add the required
-# fields.
+# These Logstash event fields are supported, but optional:
+#
+#  * `nagios\_annotation`
+#  * `nagios\_level` (overrides `nagios\_level` configuration option)
+#
+# There are two configuration options:
+#
+#  * `commandfile` - The location of the Nagios external command file. Defaults
+#    to '/var/lib/nagios3/rw/nagios.cmd'
+#  * `nagios\_level` - Specifies the level of the check to be sent. Defaults to
+#    CRITICAL and can be overriden by setting the "nagios\_level" field to one
+#    of "OK", "WARNING", "CRITICAL", or "UNKNOWN"
+#
+#     output{
+#       if [message] =~ /(error|ERROR|CRITICAL)/ {
+#         nagios {
+#           # your config here
+#         }
+#       }
+#     }
 #
-#     filter {
-#       grep {
-#         type => "linux-syslog"
-#         match => [ "@message", "(error|ERROR|CRITICAL)" ]
-#         add_tag => [ "nagios-update" ]
-#         add_fields => [
-#           "nagios_host", "%{@source_host}",
-#           "nagios_service", "the name of your nagios service check"
-#         ]
-#      }
-#    }
-#    
-#    output{
-#      nagios { 
-#        # only process events with this tag
-#        tags => "nagios-update"
-#      }
-#    }
 class LogStash::Outputs::Nagios < LogStash::Outputs::Base
-  NAGIOS_CRITICAL = 2
-  NAGIOS_WARN = 1
 
   config_name "nagios"
+  milestone 2
 
-  # The path to your nagios command file
-  config :commandfile, :validate => :string, :default => "/var/lib/nagios3/rw/nagios.cmd"
+  # The full path to your Nagios command file.
+  config :commandfile, :validate => :path, :default => "/var/lib/nagios3/rw/nagios.cmd"
 
-  # Only handle events with any of these tags. Optional.
-  # If not specified, will process all events.
-  config :tags, :validate => :array, :default => []
+  # The Nagios check level. Should be one of 0=OK, 1=WARNING, 2=CRITICAL,
+  # 3=UNKNOWN. Defaults to 2 - CRITICAL.
+  config :nagios_level, :validate => [ "0", "1", "2", "3" ], :default => "2"
 
   public
   def register
@@ -54,16 +50,11 @@ def register
 
   public
   def receive(event)
-    if !@tags.empty?
-      if (event.tags - @tags).size == 0
-        # Skip events that have no tags in common with what we were configured
-        return
-      end
-    end
+    return unless output?(event)
 
     if !File.exists?(@commandfile)
-      @logger.warn(["Skipping nagios output; command file is missing",
-                   {"commandfile" => @commandfile, "missed_event" => event}])
+      @logger.warn("Skipping nagios output; command file is missing",
+                   :commandfile => @commandfile, :missed_event => event)
       return
     end
 
@@ -72,44 +63,57 @@ def receive(event)
     # array indexes (host/service combos) and the arrays must be the same
     # length.
 
-    host = event.fields["nagios_host"]
+    host = event["nagios_host"]
     if !host
-      @logger.warn(["Skipping nagios output; nagios_host field is missing",
-                   {"missed_event" => event}])
+      @logger.warn("Skipping nagios output; nagios_host field is missing",
+                   :missed_event => event)
       return
     end
 
-    service = event.fields["nagios_service"]
+    service = event["nagios_service"]
     if !service
-      @logger.warn(["Skipping nagios output; nagios_service field is missing",
-                   {"missed_event" => event}])
+      @logger.warn("Skipping nagios output; nagios_service field is missing",
+                   "missed_event" => event)
       return
     end
 
-    annotation = event.fields["nagios_annotation"]
-    level = NAGIOS_CRITICAL
-    if event.fields["nagios_level"] and event.fields["nagios_level"][0].downcase == "warn"
-      level = NAGIOS_WARN
+    annotation = event["nagios_annotation"]
+    level = @nagios_level
+
+    if event["nagios_level"]
+      event_level = [*event["nagios_level"]]
+      case event_level[0].downcase
+      when "ok"
+        level = "0"
+      when "warning"
+        level = "1"
+      when "critical"
+        level = "2"
+      when "unknown"
+        level = "3"
+      else
+        @logger.warn("Invalid Nagios level. Defaulting to CRITICAL", :data => event_level)
+      end
     end
 
-    cmd = "[#{Time.now.to_i}] PROCESS_SERVICE_CHECK_RESULT;#{host[0]};#{service[0]};#{level};"
+    cmd = "[#{Time.now.to_i}] PROCESS_SERVICE_CHECK_RESULT;#{host};#{service};#{level};"
     if annotation
-      cmd += "#{annotation[0]}: "
+      cmd += "#{annotation}: "
     end
-    cmd += "#{event.source}: "
     # In the multi-line case, escape the newlines for the nagios command file
-    cmd += event.message.gsub("\n", "\\n")
+    cmd += (event["message"] || "<no message>").gsub("\n", "\\n")
 
-    @logger.debug({"commandfile" => @commandfile, "nagios_command" => cmd})
+    @logger.debug("Opening nagios command file", :commandfile => @commandfile,
+                  :nagios_command => cmd)
     begin
       File.open(@commandfile, "r+") do |f|
         f.puts(cmd)
         f.flush # TODO(sissel): probably don't need this.
       end
-    rescue
-      @logger.warn(["Skipping nagios output; error writing to command file",
-                   {"error" => $!, "commandfile" => @commandfile,
-                    "missed_event" => event}])
+    rescue => e
+      @logger.warn("Skipping nagios output; error writing to command file",
+                   :commandfile => @commandfile, :missed_event => event,
+                   :exception => e, :backtrace => e.backtrace)
     end
   end # def receive
 end # class LogStash::Outputs::Nagios
diff --git a/lib/logstash/outputs/nagios_nsca.rb b/lib/logstash/outputs/nagios_nsca.rb
new file mode 100644
index 00000000000..90f761e0fa5
--- /dev/null
+++ b/lib/logstash/outputs/nagios_nsca.rb
@@ -0,0 +1,132 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+require "open3"
+
+# The nagios_nsca output is used for sending passive check results to Nagios
+# through the NSCA protocol.
+#
+# This is useful if your Nagios server is not the same as the source host from
+# where you want to send logs or alerts. If you only have one server, this
+# output is probably overkill # for you, take a look at the 'nagios' output
+# instead.
+#
+# Here is a sample config using the nagios_nsca output:
+#     output {
+#       nagios_nsca {
+#         # specify the hostname or ip of your nagios server
+#         host => "nagios.example.com"
+#
+#         # specify the port to connect to
+#         port => 5667
+#       }
+#     }
+
+class LogStash::Outputs::NagiosNsca < LogStash::Outputs::Base
+
+  config_name "nagios_nsca"
+  milestone 1
+
+  # The status to send to nagios. Should be 0 = OK, 1 = WARNING, 2 = CRITICAL, 3 = UNKNOWN
+  config :nagios_status, :validate => :string, :required => true
+
+  # The nagios host or IP to send logs to. It should have a NSCA daemon running.
+  config :host, :validate => :string, :default => "localhost"
+
+  # The port where the NSCA daemon on the nagios host listens.
+  config :port, :validate => :number, :default => 5667
+
+  # The path to the 'send_nsca' binary on the local host.
+  config :send_nsca_bin, :validate => :path, :default => "/usr/sbin/send_nsca"
+
+  # The path to the send_nsca config file on the local host.
+  # Leave blank if you don't want to provide a config file.
+  config :send_nsca_config, :validate => :path
+
+  # The nagios 'host' you want to submit a passive check result to. This
+  # parameter accepts interpolation, e.g. you can use @source_host or other
+  # logstash internal variables.
+  config :nagios_host, :validate => :string, :default => "%{host}"
+
+  # The nagios 'service' you want to submit a passive check result to. This
+  # parameter accepts interpolation, e.g. you can use @source_host or other
+  # logstash internal variables.
+  config :nagios_service, :validate => :string, :default => "LOGSTASH"
+
+  # The format to use when writing events to nagios. This value
+  # supports any string and can include %{name} and other dynamic
+  # strings.
+  config :message_format, :validate => :string, :default => "%{@timestamp} %{host}: %{message}"
+
+  public
+  def register
+    #nothing for now
+  end
+
+  public
+  def receive(event)
+    # exit if type or tags don't match
+    return unless output?(event)
+
+    # catch logstash shutdown
+    if event == LogStash::SHUTDOWN
+      finished
+      return
+    end
+
+    # skip if 'send_nsca' binary doesn't exist
+    if !File.exists?(@send_nsca_bin)
+      @logger.warn("Skipping nagios_nsca output; send_nsca_bin file is missing",
+                   "send_nsca_bin" => @send_nsca_bin, "missed_event" => event)
+      return
+    end
+
+    # interpolate params
+    nagios_host = event.sprintf(@nagios_host)
+    nagios_service = event.sprintf(@nagios_service)
+
+    # escape basic things in the log message
+    # TODO: find a way to escape the message correctly
+    msg = event.sprintf(@message_format)
+    msg.gsub!("\n", "<br/>")
+    msg.gsub!("'", "&#146;")
+
+    status = event.sprintf(@nagios_status)
+    if status.to_i.to_s != status # Check it round-trips to int correctly
+      msg = "status '#{status}' is not numeric"
+      status = 2
+    else
+      status = status.to_i
+      if status > 3 || status < 0
+         msg "status must be > 0 and <= 3, not #{status}"
+         status = 2
+      end
+    end
+
+    # build the command
+    # syntax: echo '<server>!<nagios_service>!<status>!<text>'  | \
+    #           /usr/sbin/send_nsca -H <nagios_host> -d '!' -c <nsca_config>"
+
+    cmd = [@send_nsca_bin, "-H", @host, "-p", @port, "-d", "~"]
+    cmd = cmd + ["-c", @send_nsca_config]  if @send_nsca_config
+    message = "#{nagios_host}~#{nagios_service}~#{status}~#{msg}"
+
+    @logger.debug("Running send_nsca command", :nagios_nsca_command => cmd.join(" "), :message => message)
+
+    begin
+      Open3.popen3(*cmd) do |i, o, e|
+        i.puts(message)
+        i.close
+      end
+    rescue => e
+      @logger.warn(
+        "Skipping nagios_nsca output; error calling send_nsca",
+        :error => $!,
+        :nagios_nsca_command => cmd.join(" "),
+        :message => message,
+        :missed_event => event
+      )
+      @logger.debug("Backtrace", :backtrace => e.backtrace)
+    end
+  end # def receive
+end # class LogStash::Outputs::NagiosNsca
diff --git a/lib/logstash/outputs/null.rb b/lib/logstash/outputs/null.rb
new file mode 100644
index 00000000000..0c8dd8abbc7
--- /dev/null
+++ b/lib/logstash/outputs/null.rb
@@ -0,0 +1,18 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+
+# A null output. This is useful for testing logstash inputs and filters for
+# performance.
+class LogStash::Outputs::Null < LogStash::Outputs::Base
+  config_name "null"
+  milestone 3
+
+  public
+  def register
+  end # def register
+
+  public
+  def receive(event)
+  end # def event
+end # class LogStash::Outputs::Null
diff --git a/lib/logstash/outputs/opentsdb.rb b/lib/logstash/outputs/opentsdb.rb
new file mode 100644
index 00000000000..cb7a16a2135
--- /dev/null
+++ b/lib/logstash/outputs/opentsdb.rb
@@ -0,0 +1,101 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+require "socket"
+
+# This output allows you to pull metrics from your logs and ship them to
+# opentsdb. Opentsdb is an open source tool for storing and graphing metrics.
+#
+class LogStash::Outputs::Opentsdb < LogStash::Outputs::Base
+  config_name "opentsdb"
+  milestone 1
+
+  # Enable debugging.
+  config :debug, :validate => :boolean, :default => false, :deprecated => "This setting was never used by this plugin. It will be removed soon."
+
+  # The address of the opentsdb server.
+  config :host, :validate => :string, :default => "localhost"
+
+  # The port to connect on your graphite server.
+  config :port, :validate => :number, :default => 4242
+
+  # The metric(s) to use. This supports dynamic strings like %{source_host}
+  # for metric names and also for values. This is an array field with key
+  # of the metric name, value of the metric value, and multiple tag,values . Example:
+  #
+  #     [
+  #       "%{host}/uptime",
+  #       %{uptime_1m} " ,
+  #       "hostname" ,
+  #       "%{host}
+  #       "anotherhostname" ,
+  #       "%{host}
+  #     ]
+  #
+  # The value will be coerced to a floating point value. Values which cannot be
+  # coerced will zero (0)
+  config :metrics, :validate => :array, :required => true
+
+  def register
+    connect
+  end # def register
+
+  def connect
+    # TODO(sissel): Test error cases. Catch exceptions. Find fortune and glory.
+    begin
+      @socket = TCPSocket.new(@host, @port)
+    rescue Errno::ECONNREFUSED => e
+      @logger.warn("Connection refused to opentsdb server, sleeping...",
+                   :host => @host, :port => @port)
+      sleep(2)
+      retry
+    end
+  end # def connect
+
+  public
+  def receive(event)
+    return unless output?(event)
+
+    # Opentsdb message format: put metric timestamp value tagname=tagvalue tag2=value2\n
+
+    # Catch exceptions like ECONNRESET and friends, reconnect on failure.
+    begin
+      name = metrics[0]
+      value = metrics[1]
+      tags = metrics[2..-1]
+
+      # The first part of the message
+      message = ['put',
+                 event.sprintf(name),
+                 event.sprintf("%{+%s}"),
+                 event.sprintf(value),
+      ].join(" ")
+
+      # If we have have tags we need to add it to the message
+      event_tags = []
+      unless tags.nil?
+        Hash[*tags.flatten].each do |tag_name,tag_value|
+          # Interprete variables if neccesary
+          real_tag_name = event.sprintf(tag_name)
+          real_tag_value =  event.sprintf(tag_value)
+          event_tags << [real_tag_name , real_tag_value ].join('=')
+        end
+        message+=' '+event_tags.join(' ')
+      end
+
+      # TODO(sissel): Test error cases. Catch exceptions. Find fortune and glory.
+      begin
+        @socket.puts(message)
+      rescue Errno::EPIPE, Errno::ECONNRESET => e
+        @logger.warn("Connection to opentsdb server died",
+                     :exception => e, :host => @host, :port => @port)
+        sleep(2)
+        connect
+      end
+
+      # TODO(sissel): resend on failure
+      # TODO(sissel): Make 'resend on failure' tunable; sometimes it's OK to
+      # drop metrics.
+    end # @metrics.each
+  end # def receive
+end # class LogStash::Outputs::Opentsdb
diff --git a/lib/logstash/outputs/pagerduty.rb b/lib/logstash/outputs/pagerduty.rb
new file mode 100644
index 00000000000..e4851986268
--- /dev/null
+++ b/lib/logstash/outputs/pagerduty.rb
@@ -0,0 +1,78 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+require "logstash/json"
+
+# The PagerDuty output will send notifications based on pre-configured services
+# and escalation policies. Logstash can send "trigger", "acknowledge" and "resolve"
+# event types. In addition, you may configure custom descriptions and event details.
+# The only required field is the PagerDuty "Service API Key", which can be found on
+# the service's web page on pagerduty.com. In the default case, the description and
+# event details will be populated by Logstash, using `message`, `timestamp` and `host` data.
+class LogStash::Outputs::PagerDuty < LogStash::Outputs::Base
+  config_name "pagerduty"
+  milestone 1
+
+  # The PagerDuty Service API Key
+  config :service_key, :validate => :string, :required => true
+
+  # The service key to use. You'll need to set this up in PagerDuty beforehand.
+  config :incident_key, :validate => :string, :default => "logstash/%{host}/%{type}"
+
+  # Event type
+  config :event_type, :validate => ["trigger", "acknowledge", "resolve"], :default => "trigger"
+
+  # Custom description
+  config :description, :validate => :string, :default => "Logstash event for %{host}"
+
+  # The event details. These might be data from the Logstash event fields you wish to include.
+  # Tags are automatically included if detected so there is no need to explicitly add them here.
+  config :details, :validate => :hash, :default => {"timestamp" => "%{@timestamp}", "message" => "%{message}"}
+
+  # PagerDuty API URL. You shouldn't need to change this, but is included to allow for flexibility
+  # should PagerDuty iterate the API and Logstash hasn't been updated yet.
+  config :pdurl, :validate => :string, :default => "https://events.pagerduty.com/generic/2010-04-15/create_event.json"
+
+  public
+  def register
+    require 'net/https'
+    require 'uri'
+    @pd_uri = URI.parse(@pdurl)
+    @client = Net::HTTP.new(@pd_uri.host, @pd_uri.port)
+    if @pd_uri.scheme == "https"
+      @client.use_ssl = true
+      #@client.verify_mode = OpenSSL::SSL::VERIFY_PEER
+      # PagerDuty cert doesn't verify oob
+      @client.verify_mode = OpenSSL::SSL::VERIFY_NONE
+    end
+  end # def register
+
+  public
+  def receive(event)
+    return unless output?(event)
+
+    pd_event = Hash.new
+    pd_event[:service_key] = "#{@service_key}"
+    pd_event[:incident_key] = event.sprintf(@incident_key)
+    pd_event[:event_type] = "#{@event_type}"
+    pd_event[:description] = event.sprintf(@description)
+    pd_event[:details] = Hash.new
+    @details.each do |key, value|
+      @logger.debug("PD Details added:" , key => event.sprintf(value))
+      pd_event[:details]["#{key}"] = event.sprintf(value)
+    end
+    pd_event[:details][:tags] = @tags if @tags
+
+    @logger.info("PD Event", :event => pd_event)
+    begin
+      request = Net::HTTP::Post.new(@pd_uri.path)
+      request.body = LogStash::Json.dump(pd_event)
+      @logger.debug("PD Request", :request => request.inspect)
+      response = @client.request(request)
+      @logger.debug("PD Response", :response => response.body)
+
+    rescue Exception => e
+      @logger.debug("PD Unhandled exception", :pd_error => e.backtrace)
+    end
+  end # def receive
+end # class LogStash::Outputs::PagerDuty
diff --git a/lib/logstash/outputs/pipe.rb b/lib/logstash/outputs/pipe.rb
new file mode 100644
index 00000000000..c4bf90771ad
--- /dev/null
+++ b/lib/logstash/outputs/pipe.rb
@@ -0,0 +1,133 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/outputs/base"
+
+# Pipe output.
+#
+# Pipe events to stdin of another program. You can use fields from the
+# event as parts of the command.
+# WARNING: This feature can cause logstash to fork off multiple children if you are not carefull with per-event commandline.
+class LogStash::Outputs::Pipe < LogStash::Outputs::Base
+
+  config_name "pipe"
+  milestone 1
+
+  # The format to use when writing events to the pipe. This value
+  # supports any string and can include %{name} and other dynamic
+  # strings.
+  #
+  # If this setting is omitted, the full json representation of the
+  # event will be written as a single line.
+  config :message_format, :validate => :string
+
+  # Command line to launch and pipe to
+  config :command, :validate => :string, :required => true
+
+  # Close pipe that hasn't been used for TTL seconds. -1 or 0 means never close.
+  config :ttl, :validate => :number, :default => 10
+  public
+  def register
+    @pipes = {}
+    @last_stale_cleanup_cycle = Time.now
+  end # def register
+
+  public
+  def receive(event)
+    return unless output?(event)
+
+    command = event.sprintf(@command)
+
+    if @message_format
+      output = event.sprintf(@message_format) + "\n"
+    else
+      output = event.to_json
+    end
+
+    begin
+      pipe = get_pipe(command)
+      pipe.puts(output)
+    rescue IOError, Errno::EPIPE, Errno::EBADF => e
+      @logger.error("Error writing to pipe, closing pipe.", :command => command, :pipe => pipe)
+      drop_pipe(command)
+      retry
+    end
+
+    close_stale_pipes
+  end # def receive
+
+  def teardown
+    @logger.info("Teardown: closing pipes")
+    @pipes.each do |command, pipe|
+      begin
+        drop_pipe(command)
+        @logger.debug("Closed pipe #{command}", :pipe => pipe)
+      rescue Exception => e
+        @logger.error("Excpetion while closing pipes.", :exception => e)
+      end
+    end
+    finished
+  end
+
+  private
+  # every 10 seconds or so (triggered by events, but if there are no events there's no point closing files anyway)
+  def close_stale_pipes
+    return if @ttl <= 0
+    now = Time.now
+    return unless now - @last_stale_cleanup_cycle >= @ttl
+    @logger.info("Starting stale pipes cleanup cycle", :pipes => @pipes)
+    inactive_pipes = @pipes.select { |command, pipe| not pipe.active }
+    @logger.debug("%d stale pipes found" % inactive_pipes.count, :inactive_pipes => inactive_pipes)
+    inactive_pipes.each do |command, pipe|
+      drop_pipe(command)
+    end
+    # mark all pipes as inactive, a call to write will mark them as active again
+    @pipes.each { |command, pipe| pipe.active = false }
+    @last_stale_cleanup_cycle = now
+  end
+
+  def drop_pipe(command)
+      return unless @pipes.include? command
+      @logger.info("Closing pipe \"%s\"" % command)
+      begin
+        @pipes[command].close
+      rescue Exception => e
+        @logger.warn("Failed to close pipe.", :error => e, :command => command)
+      end
+      @pipes.delete(command)
+  end
+
+  def get_pipe(command)
+    return @pipes[command] if @pipes.include?(command)
+
+    @logger.info("Opening pipe", :command => command)
+
+    @pipes[command] = PipeWrapper.new(command, mode="a+")
+  end
+end # class LogStash::Outputs::Pipe
+
+class PipeWrapper
+  attr_accessor :active
+  def initialize(command, mode="a+")
+    @pipe = IO.popen(command, mode)
+    @active = false
+  end
+
+  def method_missing(m, *args)
+    if @pipe.respond_to? m
+      @pipe.send(m, *args)
+    else
+      raise NoMethodError
+    end
+  end
+
+  def puts(txt)
+    @pipe.puts(txt)
+    @pipe.flush
+    @active = true
+  end
+
+  def write(txt)
+    @pipe.write(txt)
+    @active = true
+  end
+end
diff --git a/lib/logstash/outputs/rabbitmq.rb b/lib/logstash/outputs/rabbitmq.rb
new file mode 100644
index 00000000000..7b52baf2a7a
--- /dev/null
+++ b/lib/logstash/outputs/rabbitmq.rb
@@ -0,0 +1,96 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+
+# Push events to a RabbitMQ exchange. Requires RabbitMQ 2.x
+# or later version (3.x is recommended).
+#
+# Relevant links:
+#
+# * RabbitMQ: <http://www.rabbitmq.com/>
+# * March Hare: <http://rubymarchhare.info>
+# * Bunny: <http://rubybunny.info>
+class LogStash::Outputs::RabbitMQ < LogStash::Outputs::Base
+  EXCHANGE_TYPES = ["fanout", "direct", "topic"]
+
+  config_name "rabbitmq"
+  milestone 1
+
+
+  #
+  # Connection
+  #
+
+  # RabbitMQ server address
+  config :host, :validate => :string, :required => true
+
+  # RabbitMQ port to connect on
+  config :port, :validate => :number, :default => 5672
+
+  # RabbitMQ username
+  config :user, :validate => :string, :default => "guest"
+
+  # RabbitMQ password
+  config :password, :validate => :password, :default => "guest"
+
+  # The vhost to use. If you don't know what this is, leave the default.
+  config :vhost, :validate => :string, :default => "/"
+
+  # Enable or disable SSL
+  config :ssl, :validate => :boolean, :default => false
+
+  # Validate SSL certificate
+  config :verify_ssl, :validate => :boolean, :default => false
+
+  # Enable or disable logging
+  config :debug, :validate => :boolean, :default => false, :deprecated => "Use the logstash --debug flag for this instead."
+
+
+
+  #
+  # Exchange
+  #
+
+
+  # The exchange type (fanout, topic, direct)
+  config :exchange_type, :validate => EXCHANGE_TYPES, :required => true
+
+  # The name of the exchange
+  config :exchange, :validate => :string, :required => true
+
+  # Key to route to by default. Defaults to 'logstash'
+  #
+  # * Routing keys are ignored on fanout exchanges.
+  config :key, :validate => :string, :default => "logstash"
+
+  # Is this exchange durable? (aka; Should it survive a broker restart?)
+  config :durable, :validate => :boolean, :default => true
+
+  # Should RabbitMQ persist messages to disk?
+  config :persistent, :validate => :boolean, :default => true
+
+
+
+  def initialize(params)
+    params["codec"] = "json" if !params["codec"]
+
+    super
+  end
+
+  # Use MarchHare on JRuby to avoid IO#select CPU spikes
+  # (see github.com/ruby-amqp/bunny/issues/95).
+  #
+  # On MRI, use Bunny.
+  #
+  # See http://rubybunny.info and http://rubymarchhare.info
+  # for the docs.
+  if RUBY_ENGINE == "jruby"
+    require "logstash/outputs/rabbitmq/march_hare"
+
+    include MarchHareImpl
+  else
+    require "logstash/outputs/rabbitmq/bunny"
+
+    include BunnyImpl
+  end
+end # class LogStash::Outputs::RabbitMQ
diff --git a/lib/logstash/outputs/rabbitmq/bunny.rb b/lib/logstash/outputs/rabbitmq/bunny.rb
new file mode 100644
index 00000000000..cc83eacc093
--- /dev/null
+++ b/lib/logstash/outputs/rabbitmq/bunny.rb
@@ -0,0 +1,138 @@
+# encoding: utf-8
+
+require "logstash/json"
+
+class LogStash::Outputs::RabbitMQ
+  module BunnyImpl
+
+    #
+    # API
+    #
+
+    def register
+      require "bunny"
+
+      @logger.info("Registering output", :plugin => self)
+
+      connect
+      declare_exchange
+    end # def register
+
+
+    def receive(event)
+      return unless output?(event)
+
+      @logger.debug("Sending event", :destination => to_s, :event => event, :key => key)
+      key = event.sprintf(@key) if @key
+
+      begin
+        publish_serialized(event.to_json, key)
+      rescue LogStash::Json::GeneratorError => e
+        @logger.warn("Trouble converting event to JSON", :exception => e,
+                     :event => event)
+      end
+    end
+
+    def publish_serialized(message, key = @key)
+      begin
+        if @x
+          @x.publish(message, :persistent => @persistent, :routing_key => key)
+        else
+          @logger.warn("Tried to send a message, but not connected to RabbitMQ yet.")
+        end
+      rescue Bunny::NetworkFailure, Bunny::ConnectionClosedError, Bunny::ConnectionLevelException, Bunny::TCPConnectionFailed => e
+        n = Bunny::Session::DEFAULT_NETWORK_RECOVERY_INTERVAL * 2
+
+        @logger.error("RabbitMQ connection error: #{e.message}. Will attempt to reconnect in #{n} seconds...",
+                      :exception => e,
+                      :backtrace => e.backtrace)
+        return if terminating?
+
+        sleep n
+        connect
+        declare_exchange
+        retry
+      end
+    end
+
+    def to_s
+      return "amqp://#{@user}@#{@host}:#{@port}#{@vhost}/#{@exchange_type}/#{@exchange}\##{@key}"
+    end
+
+    def teardown
+      @conn.close if @conn && @conn.open?
+      @conn = nil
+
+      finished
+    end
+
+
+
+    #
+    # Implementation
+    #
+
+    def connect
+      @vhost       ||= Bunny::DEFAULT_HOST
+      # 5672. Will be switched to 5671 by Bunny if TLS is enabled.
+      @port        ||= AMQ::Protocol::DEFAULT_PORT
+      @routing_key ||= "#"
+
+      @settings = {
+        :vhost => @vhost,
+        :host  => @host,
+        :port  => @port,
+        :automatically_recover => false
+      }
+      @settings[:user]      = @user || Bunny::DEFAULT_USER
+      @settings[:pass]      = if @password
+                                @password.value
+                              else
+                                Bunny::DEFAULT_PASSWORD
+                              end
+
+      @settings[:log_level] = if @debug || @logger.debug?
+                                :debug
+                              else
+                                :error
+                              end
+
+      @settings[:tls]        = @ssl if @ssl
+      @settings[:verify_ssl] = @verify_ssl if @verify_ssl
+
+      proto                  = if @ssl
+                                 "amqp"
+                               else
+                                 "amqps"
+                               end
+      @connection_url        = "#{proto}://#{@user}@#{@host}:#{@port}#{vhost}/#{@queue}"
+
+      begin
+        @conn = Bunny.new(@settings)
+
+        @logger.debug("Connecting to RabbitMQ. Settings: #{@settings.inspect}, queue: #{@queue.inspect}")
+        return if terminating?
+        @conn.start
+
+        @ch = @conn.create_channel
+        @logger.info("Connected to RabbitMQ at #{@settings[:host]}")
+      rescue Bunny::NetworkFailure, Bunny::ConnectionClosedError, Bunny::ConnectionLevelException, Bunny::TCPConnectionFailed => e
+        n = Bunny::Session::DEFAULT_NETWORK_RECOVERY_INTERVAL * 2
+
+        @logger.error("RabbitMQ connection error: #{e.message}. Will attempt to reconnect in #{n} seconds...",
+                      :exception => e,
+                      :backtrace => e.backtrace)
+        return if terminating?
+
+        sleep n
+        retry
+      end
+    end
+
+    def declare_exchange
+      @logger.debug("Declaring an exchange", :name => @exchange, :type => @exchange_type,
+                    :durable => @durable)
+      @x = @ch.exchange(@exchange, :type => @exchange_type.to_sym, :durable => @durable)
+    end
+  end # BunnyImpl
+end # LogStash::Outputs::RabbitMQ
diff --git a/lib/logstash/outputs/rabbitmq/hot_bunnies.rb b/lib/logstash/outputs/rabbitmq/hot_bunnies.rb
new file mode 100644
index 00000000000..7d9cc42318a
--- /dev/null
+++ b/lib/logstash/outputs/rabbitmq/hot_bunnies.rb
@@ -0,0 +1 @@
+require "logstash/outputs/rabbitmq/march_hare"
diff --git a/lib/logstash/outputs/rabbitmq/march_hare.rb b/lib/logstash/outputs/rabbitmq/march_hare.rb
new file mode 100644
index 00000000000..f7bdd9304db
--- /dev/null
+++ b/lib/logstash/outputs/rabbitmq/march_hare.rb
@@ -0,0 +1,143 @@
+# encoding: utf-8
+class LogStash::Outputs::RabbitMQ
+  module MarchHareImpl
+
+
+    #
+    # API
+    #
+
+    def register
+      require "march_hare"
+      require "java"
+
+      @logger.info("Registering output", :plugin => self)
+
+      @connected = java.util.concurrent.atomic.AtomicBoolean.new
+
+      connect
+      declare_exchange
+
+      @connected.set(true)
+
+      @codec.on_event(&method(:publish_serialized))
+    end
+
+
+    def receive(event)
+      return unless output?(event)
+
+      begin
+        @codec.encode(event)
+      rescue JSON::GeneratorError => e
+        @logger.warn("Trouble converting event to JSON", :exception => e,
+                     :event => event)
+      end
+    end
+
+    def publish_serialized(message)
+      begin
+        if @connected.get
+          @x.publish(message, :routing_key => @key, :properties => {
+            :persistent => @persistent
+          })
+        else
+          @logger.warn("Tried to send a message, but not connected to RabbitMQ.")
+        end
+      rescue MarchHare::Exception, IOError, com.rabbitmq.client.AlreadyClosedException => e
+        @connected.set(false)
+        n = 10
+
+        @logger.error("RabbitMQ connection error: #{e.message}. Will attempt to reconnect in #{n} seconds...",
+                      :exception => e,
+                      :backtrace => e.backtrace)
+        return if terminating?
+
+        sleep n
+
+        connect
+        declare_exchange
+        retry
+      end
+    end
+
+    def to_s
+      return "amqp://#{@user}@#{@host}:#{@port}#{@vhost}/#{@exchange_type}/#{@exchange}\##{@key}"
+    end
+
+    def teardown
+      @connected.set(false)
+      @conn.close if @conn && @conn.open?
+      @conn = nil
+
+      finished
+    end
+
+
+
+    #
+    # Implementation
+    #
+
+    def connect
+      return if terminating?
+
+      @vhost       ||= "127.0.0.1"
+      # 5672. Will be switched to 5671 by Bunny if TLS is enabled.
+      @port        ||= 5672
+
+      @settings = {
+        :vhost => @vhost,
+        :host  => @host,
+        :port  => @port,
+        :user  => @user,
+        :automatic_recovery => false
+      }
+      @settings[:pass]      = if @password
+                                @password.value
+                              else
+                                "guest"
+                              end
+
+      @settings[:tls]        = @ssl if @ssl
+      proto                  = if @ssl
+                                 "amqp"
+                               else
+                                 "amqps"
+                               end
+      @connection_url        = "#{proto}://#{@user}@#{@host}:#{@port}#{vhost}/#{@queue}"
+
+      begin
+        @conn = MarchHare.connect(@settings)
+
+        @logger.debug("Connecting to RabbitMQ. Settings: #{@settings.inspect}, queue: #{@queue.inspect}")
+
+        @ch = @conn.create_channel
+        @logger.info("Connected to RabbitMQ at #{@settings[:host]}")
+      rescue MarchHare::Exception => e
+        @connected.set(false)
+        n = 10
+
+        @logger.error("RabbitMQ connection error: #{e.message}. Will attempt to reconnect in #{n} seconds...",
+                      :exception => e,
+                      :backtrace => e.backtrace)
+        return if terminating?
+
+        sleep n
+        retry
+      end
+    end
+
+    def declare_exchange
+      @logger.debug("Declaring an exchange", :name => @exchange, :type => @exchange_type,
+                    :durable => @durable)
+      @x = @ch.exchange(@exchange, :type => @exchange_type.to_sym, :durable => @durable)
+
+      # sets @connected to true during recovery. MK.
+      @connected.set(true)
+
+      @x
+    end
+
+  end # MarchHareImpl
+end
diff --git a/lib/logstash/outputs/redis.rb b/lib/logstash/outputs/redis.rb
new file mode 100644
index 00000000000..ef274591c64
--- /dev/null
+++ b/lib/logstash/outputs/redis.rb
@@ -0,0 +1,252 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+require "stud/buffer"
+
+# This output will send events to a Redis queue using RPUSH.
+# The RPUSH command is supported in Redis v0.0.7+. Using
+# PUBLISH to a channel requires at least v1.3.8+.
+# While you may be able to make these Redis versions work,
+# the best performance and stability will be found in more 
+# recent stable versions.  Versions 2.6.0+ are recommended.
+#
+# For more information about Redis, see <http://redis.io/>
+#
+class LogStash::Outputs::Redis < LogStash::Outputs::Base
+
+  include Stud::Buffer
+
+  config_name "redis"
+  milestone 2
+
+  # Name is used for logging in case there are multiple instances.
+  # TODO: delete
+  config :name, :validate => :string, :default => 'default',
+    :deprecated => true
+
+  # The hostname(s) of your Redis server(s). Ports may be specified on any
+  # hostname, which will override the global port config.
+  #
+  # For example:
+  #
+  #     "127.0.0.1"
+  #     ["127.0.0.1", "127.0.0.2"]
+  #     ["127.0.0.1:6380", "127.0.0.1"]
+  config :host, :validate => :array, :default => ["127.0.0.1"]
+
+  # Shuffle the host list during Logstash startup.
+  config :shuffle_hosts, :validate => :boolean, :default => true
+
+  # The default port to connect on. Can be overridden on any hostname.
+  config :port, :validate => :number, :default => 6379
+
+  # The Redis database number.
+  config :db, :validate => :number, :default => 0
+
+  # Redis initial connection timeout in seconds.
+  config :timeout, :validate => :number, :default => 5
+
+  # Password to authenticate with.  There is no authentication by default.
+  config :password, :validate => :password
+
+  # The name of the Redis queue (we'll use RPUSH on this). Dynamic names are
+  # valid here, for example "logstash-%{type}"
+  # TODO: delete
+  config :queue, :validate => :string, :deprecated => true
+
+  # The name of a Redis list or channel. Dynamic names are
+  # valid here, for example "logstash-%{type}".
+  # TODO set required true
+  config :key, :validate => :string, :required => false
+
+  # Either list or channel.  If `redis_type` is list, then we will set
+  # RPUSH to key. If `redis_type` is channel, then we will PUBLISH to `key`.
+  # TODO set required true
+  config :data_type, :validate => [ "list", "channel" ], :required => false
+
+  # Set to true if you want Redis to batch up values and send 1 RPUSH command
+  # instead of one command per value to push on the list.  Note that this only
+  # works with `data_type="list"` mode right now.
+  #
+  # If true, we send an RPUSH every "batch_events" events or
+  # "batch_timeout" seconds (whichever comes first).
+  # Only supported for `data_type` is "list".
+  config :batch, :validate => :boolean, :default => false
+
+  # If batch is set to true, the number of events we queue up for an RPUSH.
+  config :batch_events, :validate => :number, :default => 50
+
+  # If batch is set to true, the maximum amount of time between RPUSH commands
+  # when there are pending events to flush.
+  config :batch_timeout, :validate => :number, :default => 5
+
+  # Interval for reconnecting to failed Redis connections
+  config :reconnect_interval, :validate => :number, :default => 1
+
+  # In case Redis `data_type` is "list" and has more than @congestion_threshold items,
+  # block until someone consumes them and reduces congestion, otherwise if there are
+  # no consumers Redis will run out of memory, unless it was configured with OOM protection.
+  # But even with OOM protection, a single Redis list can block all other users of Redis,
+  # until Redis CPU consumption reaches the max allowed RAM size.
+  # A default value of 0 means that this limit is disabled.
+  # Only supported for `list` Redis `data_type`.
+  config :congestion_threshold, :validate => :number, :default => 0
+
+  # How often to check for congestion. Default is one second.
+  # Zero means to check on every event.
+  config :congestion_interval, :validate => :number, :default => 1
+
+  def register
+    require 'redis'
+
+    # TODO remove after setting key and data_type to true
+    if @queue
+      if @key or @data_type
+        raise RuntimeError.new(
+          "Cannot specify queue parameter and key or data_type"
+        )
+      end
+      @key = @queue
+      @data_type = 'list'
+    end
+
+    if not @key or not @data_type
+      raise RuntimeError.new(
+        "Must define queue, or key and data_type parameters"
+      )
+    end
+    # end TODO
+
+
+    if @batch
+      if @data_type != "list"
+        raise RuntimeError.new(
+          "batch is not supported with data_type #{@data_type}"
+        )
+      end
+      buffer_initialize(
+        :max_items => @batch_events,
+        :max_interval => @batch_timeout,
+        :logger => @logger
+      )
+    end
+
+    @redis = nil
+    if @shuffle_hosts
+        @host.shuffle!
+    end
+    @host_idx = 0
+
+    @congestion_check_times = Hash.new { |h,k| h[k] = Time.now.to_i - @congestion_interval }
+  end # def register
+
+  def receive(event)
+    return unless output?(event)
+
+    if @batch and @data_type == 'list' # Don't use batched method for pubsub.
+      # Stud::Buffer
+      buffer_receive(event.to_json, event.sprintf(@key))
+      return
+    end
+
+    key = event.sprintf(@key)
+    # TODO(sissel): We really should not drop an event, but historically
+    # we have dropped events that fail to be converted to json.
+    # TODO(sissel): Find a way to continue passing events through even
+    # if they fail to convert properly.
+    begin
+      payload = event.to_json
+    rescue Encoding::UndefinedConversionError, ArgumentError
+      puts "FAILUREENCODING"
+      @logger.error("Failed to convert event to JSON. Invalid UTF-8, maybe?",
+                    :event => event.inspect)
+      return
+    end
+
+    begin
+      @redis ||= connect
+      if @data_type == 'list'
+        congestion_check(key)
+        @redis.rpush(key, payload)
+      else
+        @redis.publish(key, payload)
+      end
+    rescue => e
+      @logger.warn("Failed to send event to Redis", :event => event,
+                   :identity => identity, :exception => e,
+                   :backtrace => e.backtrace)
+      sleep @reconnect_interval
+      @redis = nil
+      retry
+    end
+  end # def receive
+
+  def congestion_check(key)
+    return if @congestion_threshold == 0
+    if (Time.now.to_i - @congestion_check_times[key]) >= @congestion_interval # Check congestion only if enough time has passed since last check.
+      while @redis.llen(key) > @congestion_threshold # Don't push event to Redis key which has reached @congestion_threshold.
+        @logger.warn? and @logger.warn("Redis key size has hit a congestion threshold #{@congestion_threshold} suspending output for #{@congestion_interval} seconds")
+        sleep @congestion_interval
+      end
+      @congestion_check_time = Time.now.to_i
+    end
+  end
+
+  # called from Stud::Buffer#buffer_flush when there are events to flush
+  def flush(events, key, teardown=false)
+    @redis ||= connect
+    # we should not block due to congestion on teardown
+    # to support this Stud::Buffer#buffer_flush should pass here the :final boolean value.
+    congestion_check(key) unless teardown
+    @redis.rpush(key, events)
+  end
+  # called from Stud::Buffer#buffer_flush when an error occurs
+  def on_flush_error(e)
+    @logger.warn("Failed to send backlog of events to Redis",
+      :identity => identity,
+      :exception => e,
+      :backtrace => e.backtrace
+    )
+    @redis = connect
+  end
+
+  def teardown
+    if @batch
+      buffer_flush(:final => true)
+    end
+    if @data_type == 'channel' and @redis
+      @redis.quit
+      @redis = nil
+    end
+  end
+
+  private
+  def connect
+    @current_host, @current_port = @host[@host_idx].split(':')
+    @host_idx = @host_idx + 1 >= @host.length ? 0 : @host_idx + 1
+
+    if not @current_port
+      @current_port = @port
+    end
+
+    params = {
+      :host => @current_host,
+      :port => @current_port,
+      :timeout => @timeout,
+      :db => @db
+    }
+    @logger.debug(params)
+
+    if @password
+      params[:password] = @password.value
+    end
+
+    Redis.new(params)
+  end # def connect
+
+  # A string used to identify a Redis instance in log messages
+  def identity
+    @name || "redis://#{@password}@#{@current_host}:#{@current_port}/#{@db} #{@data_type}:#{@key}"
+  end
+
+end
diff --git a/lib/logstash/outputs/s3.rb b/lib/logstash/outputs/s3.rb
new file mode 100644
index 00000000000..c493d29b526
--- /dev/null
+++ b/lib/logstash/outputs/s3.rb
@@ -0,0 +1,357 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+require "socket" # for Socket.gethostname
+
+# TODO integrate aws_config in the future
+#require "logstash/plugin_mixins/aws_config"
+
+# INFORMATION:
+
+# This plugin was created for store the logstash's events into Amazon Simple Storage Service (Amazon S3).
+# For use it you needs authentications and an s3 bucket.
+# Be careful to have the permission to write file on S3's bucket and run logstash with super user for establish connection.
+
+# S3 plugin allows you to do something complex, let's explain:)
+
+# S3 outputs create temporary files into "/opt/logstash/S3_temp/". If you want, you can change the path at the start of register method.
+# This files have a special name, for example:
+
+# ls.s3.ip-10-228-27-95.2013-04-18T10.00.tag_hello.part0.txt
+
+# ls.s3 : indicate logstash plugin s3
+
+# "ip-10-228-27-95" : indicate you ip machine, if you have more logstash and writing on the same bucket for example.
+# "2013-04-18T10.00" : represents the time whenever you specify time_file.
+# "tag_hello" : this indicate the event's tag, you can collect events with the same tag.
+# "part0" : this means if you indicate size_file then it will generate more parts if you file.size > size_file.
+#           When a file is full it will pushed on bucket and will be deleted in temporary directory.
+#           If a file is empty is not pushed, but deleted.
+
+# This plugin have a system to restore the previous temporary files if something crash.
+
+##[Note] :
+
+## If you specify size_file and time_file then it will create file for each tag (if specified), when time_file or
+## their size > size_file, it will be triggered then they will be pushed on s3's bucket and will delete from local disk.
+
+## If you don't specify size_file, but time_file then it will create only one file for each tag (if specified).
+## When time_file it will be triggered then the files will be pushed on s3's bucket and delete from local disk.
+
+## If you don't specify time_file, but size_file  then it will create files for each tag (if specified),
+## that will be triggered when their size > size_file, then they will be pushed on s3's bucket and will delete from local disk.
+
+## If you don't specific size_file and time_file you have a curios mode. It will create only one file for each tag (if specified).
+## Then the file will be rest on temporary directory and don't will be pushed on bucket until we will restart logstash.
+
+# INFORMATION ABOUT CLASS:
+
+# I tried to comment the class at best i could do.
+# I think there are much thing to improve, but if you want some points to develop here a list:
+
+# TODO Integrate aws_config in the future
+# TODO Find a method to push them all files when logtstash close the session.
+# TODO Integrate @field on the path file
+# TODO Permanent connection or on demand? For now on demand, but isn't a good implementation.
+#      Use a while or a thread to try the connection before break a time_out and signal an error.
+# TODO If you have bugs report or helpful advice contact me, but remember that this code is much mine as much as yours,
+#      try to work on it if you want :)
+
+
+# USAGE:
+
+# This is an example of logstash config:
+
+# output {
+#    s3{
+#      access_key_id => "crazy_key"             (required)
+#      secret_access_key => "monkey_access_key" (required)
+#      endpoint_region => "eu-west-1"           (required)
+#      bucket => "boss_please_open_your_bucket" (required)
+#      size_file => 2048                        (optional)
+#      time_file => 5                           (optional)
+#      format => "plain"                        (optional)
+#      canned_acl => "private"                  (optional. Options are "private", "public_read", "public_read_write", "authenticated_read". Defaults to "private" )
+#    }
+# }
+
+# We analize this:
+
+# access_key_id => "crazy_key"
+# Amazon will give you the key for use their service if you buy it or try it. (not very much open source anyway)
+
+# secret_access_key => "monkey_access_key"
+# Amazon will give you the secret_access_key for use their service if you buy it or try it . (not very much open source anyway).
+
+# endpoint_region => "eu-west-1"
+# When you make a contract with Amazon, you should know where the services you use.
+
+# bucket => "boss_please_open_your_bucket"
+# Be careful you have the permission to write on bucket and know the name.
+
+# size_file => 2048
+# Means the size, in KB, of files who can store on temporary directory before you will be pushed on bucket.
+# Is useful if you have a little server with poor space on disk and you don't want blow up the server with unnecessary temporary log files.
+
+# time_file => 5
+# Means, in minutes, the time  before the files will be pushed on bucket. Is useful if you want to push the files every specific time.
+
+# format => "plain"
+# Means the format of events you want to store in the files
+
+# canned_acl => "private"
+# The S3 canned ACL to use when putting the file. Defaults to "private".
+
+# LET'S ROCK AND ROLL ON THE CODE!
+
+class LogStash::Outputs::S3 < LogStash::Outputs::Base
+ #TODO integrate aws_config in the future
+ #  include LogStash::PluginMixins::AwsConfig
+
+ config_name "s3"
+ milestone 1
+
+ # Aws access_key.
+ config :access_key_id, :validate => :string
+
+ # Aws secret_access_key
+ config :secret_access_key, :validate => :string
+
+ # S3 bucket
+ config :bucket, :validate => :string
+
+ # Aws endpoint_region
+ config :endpoint_region, :validate => ["us-east-1", "us-west-1", "us-west-2",
+                                        "eu-west-1", "ap-southeast-1", "ap-southeast-2",
+                                        "ap-northeast-1", "sa-east-1", "us-gov-west-1"], :default => "us-east-1"
+
+ # Set the size of file in KB, this means that files on bucket when have dimension > file_size, they are stored in two or more file.
+ # If you have tags then it will generate a specific size file for every tags
+ ##NOTE: define size of file is the better thing, because generate a local temporary file on disk and then put it in bucket.
+ config :size_file, :validate => :number, :default => 0
+
+ # Set the time, in minutes, to close the current sub_time_section of bucket.
+ # If you define file_size you have a number of files in consideration of the section and the current tag.
+ # 0 stay all time on listerner, beware if you specific 0 and size_file 0, because you will not put the file on bucket,
+ # for now the only thing this plugin can do is to put the file when logstash restart.
+ config :time_file, :validate => :number, :default => 0
+
+ # The event format you want to store in files. Defaults to plain text.
+ config :format, :validate => [ "json", "plain", "nil" ], :default => "plain"
+
+ ## IMPORTANT: if you use multiple instance of s3, you should specify on one of them the "restore=> true" and on the others "restore => false".
+ ## This is hack for not destroy the new files after restoring the initial files.
+ ## If you do not specify "restore => true" when logstash crashes or is restarted, the files are not sent into the bucket,
+ ## for example if you have single Instance.
+ config :restore, :validate => :boolean, :default => false
+
+ # Aws canned ACL
+ config :canned_acl, :validate => ["private", "public_read", "public_read_write", "authenticated_read"],
+        :default => "private"
+
+ # Method to set up the aws configuration and establish connection
+ def aws_s3_config
+
+  @endpoint_region == 'us-east-1' ? @endpoint_region = 's3.amazonaws.com' : @endpoint_region = 's3-'+@endpoint_region+'.amazonaws.com'
+
+  @logger.info("Registering s3 output", :bucket => @bucket, :endpoint_region => @endpoint_region)
+
+  AWS.config(
+    :access_key_id => @access_key_id,
+    :secret_access_key => @secret_access_key,
+    :s3_endpoint => @endpoint_region
+  )
+  @s3 = AWS::S3.new
+
+ end
+
+ # This method is used to manage sleep and awaken thread.
+ def time_alert(interval)
+
+   Thread.new do
+    loop do
+      start_time = Time.now
+      yield
+      elapsed = Time.now - start_time
+      sleep([interval - elapsed, 0].max)
+    end
+   end
+
+ end
+
+ # this method is used for write files on bucket. It accept the file and the name of file.
+ def write_on_bucket (file_data, file_basename)
+
+  # if you lose connection with s3, bad control implementation.
+  if ( @s3 == nil)
+    aws_s3_config
+  end
+
+  # find and use the bucket
+  bucket = @s3.buckets[@bucket]
+
+  @logger.debug "S3: ready to write "+file_basename+" in bucket "+@bucket+", Fire in the hole!"
+
+  # prepare for write the file
+  object = bucket.objects[file_basename]
+  object.write(:file => file_data, :acl => @canned_acl)
+
+  @logger.debug "S3: has written "+file_basename+" in bucket "+@bucket + " with canned ACL \"" + @canned_acl + "\""
+
+ end
+
+ # this method is used for create new path for name the file
+ def getFinalPath
+
+   @pass_time = Time.now
+   return @temp_directory+"ls.s3."+Socket.gethostname+"."+(@pass_time).strftime("%Y-%m-%dT%H.%M")
+
+ end
+
+ # This method is used for restore the previous crash of logstash or to prepare the files to send in bucket.
+ # Take two parameter: flag and name. Flag indicate if you want to restore or not, name is the name of file
+ def upFile(flag, name)
+
+   Dir[@temp_directory+name].each do |file|
+     name_file = File.basename(file)
+
+     if (flag == true)
+      @logger.warn "S3: have found temporary file: "+name_file+", something has crashed before... Prepare for upload in bucket!"
+     end
+
+     if (!File.zero?(file))
+       write_on_bucket(file, name_file)
+
+       if (flag == true)
+          @logger.debug "S3: file: "+name_file+" restored on bucket "+@bucket
+       else
+          @logger.debug "S3: file: "+name_file+" was put on bucket "+@bucket
+       end
+     end
+
+     File.delete (file)
+
+   end
+ end
+
+ # This method is used for create new empty temporary files for use. Flag is needed for indicate new subsection time_file.
+ def newFile (flag)
+
+   if (flag == true)
+     @current_final_path = getFinalPath
+     @sizeCounter = 0
+   end
+
+   if (@tags.size != 0)
+     @tempFile = File.new(@current_final_path+".tag_"+@tag_path+"part"+@sizeCounter.to_s+".txt", "w")
+   else
+     @tempFile = File.new(@current_final_path+".part"+@sizeCounter.to_s+".txt", "w")
+   end
+
+ end
+
+ public
+ def register
+   require "aws-sdk"
+   @temp_directory = "/opt/logstash/S3_temp/"
+
+   if (@tags.size != 0)
+       @tag_path = ""
+       for i in (0..@tags.size-1)
+          @tag_path += @tags[i].to_s+"."
+       end
+   end
+
+   if !(File.directory? @temp_directory)
+    @logger.debug "S3: Directory "+@temp_directory+" doesn't exist, let's make it!"
+    Dir.mkdir(@temp_directory)
+   else
+    @logger.debug "S3: Directory "+@temp_directory+" exist, nothing to do"
+   end
+
+   if (@restore == true )
+     @logger.debug "S3: is attempting to verify previous crashes..."
+
+     upFile(true, "*.txt")
+   end
+
+   newFile(true)
+
+   if (time_file != 0)
+      first_time = true
+      @thread = time_alert(@time_file*60) do
+       if (first_time == false)
+         @logger.debug "S3: time_file triggered,  let's bucket the file if dosen't empty  and create new file "
+         upFile(false, File.basename(@tempFile))
+         newFile(true)
+       else
+         first_time = false
+       end
+     end
+   end
+
+ end
+
+ public
+ def receive(event)
+  return unless output?(event)
+
+  # Prepare format of Events
+  if (@format == "plain")
+     message = self.class.format_message(event)
+  elsif (@format == "json")
+     message = event.to_json
+  else
+     message = event.to_s
+  end
+
+  if(time_file !=0)
+     @logger.debug "S3: trigger files after "+((@pass_time+60*time_file)-Time.now).to_s
+  end
+
+  # if specific the size
+  if(size_file !=0)
+
+    if (@tempFile.size < @size_file )
+
+       @logger.debug "S3: File have size: "+@tempFile.size.to_s+" and size_file is: "+ @size_file.to_s
+       @logger.debug "S3: put event into: "+File.basename(@tempFile)
+
+       # Put the event in the file, now!
+       File.open(@tempFile, 'a') do |file|
+         file.puts message
+         file.write "\n"
+       end
+
+     else
+
+       @logger.debug "S3: file: "+File.basename(@tempFile)+" is too large, let's bucket it and create new file"
+       upFile(false, File.basename(@tempFile))
+       @sizeCounter += 1
+       newFile(false)
+
+     end
+
+  # else we put all in one file
+  else
+
+    @logger.debug "S3: put event into "+File.basename(@tempFile)
+    File.open(@tempFile, 'a') do |file|
+      file.puts message
+      file.write "\n"
+    end
+  end
+
+ end
+
+ def self.format_message(event)
+    message = "Date: #{event[LogStash::Event::TIMESTAMP]}\n"
+    message << "Source: #{event["source"]}\n"
+    message << "Tags: #{event["tags"].join(', ')}\n"
+    message << "Fields: #{event.to_hash.inspect}\n"
+    message << "Message: #{event["message"]}"
+ end
+
+end
+
+# Enjoy it, by Bistic:)
diff --git a/lib/logstash/outputs/sns.rb b/lib/logstash/outputs/sns.rb
new file mode 100644
index 00000000000..f882f65deca
--- /dev/null
+++ b/lib/logstash/outputs/sns.rb
@@ -0,0 +1,124 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+require "logstash/plugin_mixins/aws_config"
+
+# SNS output.
+#
+# Send events to Amazon's Simple Notification Service, a hosted pub/sub
+# framework.  It supports subscribers of type email, HTTP/S, SMS, and SQS.
+#
+# For further documentation about the service see:
+#
+#   http://docs.amazonwebservices.com/sns/latest/api/
+#
+# This plugin looks for the following fields on events it receives:
+#
+#  * `sns` - If no ARN is found in the configuration file, this will be used as
+#  the ARN to publish.
+#  * `sns_subject` - The subject line that should be used.
+#  Optional. The "%{host}" will be used if not present and truncated at
+#  `MAX_SUBJECT_SIZE_IN_CHARACTERS`.
+#  * `sns_message` - The message that should be
+#  sent. Optional. The event serialzed as JSON will be used if not present and
+#  with the @message truncated so that the length of the JSON fits in
+#  `MAX_MESSAGE_SIZE_IN_BYTES`.
+#
+class LogStash::Outputs::Sns < LogStash::Outputs::Base
+  include LogStash::PluginMixins::AwsConfig
+
+  MAX_SUBJECT_SIZE_IN_CHARACTERS  = 100
+  MAX_MESSAGE_SIZE_IN_BYTES       = 32768
+
+  config_name "sns"
+  milestone 1
+
+  # Message format.  Defaults to plain text.
+  config :format, :validate => [ "json", "plain" ], :default => "plain"
+
+  # SNS topic ARN.
+  config :arn, :validate => :string
+
+  # When an ARN for an SNS topic is specified here, the message
+  # "Logstash successfully booted" will be sent to it when this plugin
+  # is registered.
+  #
+  # Example: arn:aws:sns:us-east-1:770975001275:logstash-testing
+  #
+  config :publish_boot_message_arn, :validate => :string
+
+  public
+  def aws_service_endpoint(region)
+    return {
+        :sns_endpoint => "sns.#{region}.amazonaws.com"
+    }
+  end
+
+  public
+  def register
+    require "aws-sdk"
+
+    @sns = AWS::SNS.new(aws_options_hash)
+
+    # Try to publish a "Logstash booted" message to the ARN provided to
+    # cause an error ASAP if the credentials are bad.
+    if @publish_boot_message_arn
+      @sns.topics[@publish_boot_message_arn].publish("Logstash successfully booted", :subject => "Logstash booted")
+    end
+  end
+
+  public
+  def receive(event)
+    return unless output?(event)
+
+    arn     = Array(event["sns"]).first || @arn
+
+    raise "An SNS ARN required." unless arn
+
+    message = Array(event["sns_message"]).first
+    subject = Array(event["sns_subject"]).first || event.source
+
+    # Ensure message doesn't exceed the maximum size.
+    if message
+      # TODO: Utilize `byteslice` in JRuby 1.7: http://jira.codehaus.org/browse/JRUBY-5547
+      message = message.slice(0, MAX_MESSAGE_SIZE_IN_BYTES)
+    else
+      if @format == "plain"
+        message = self.class.format_message(event)
+      else
+        message = self.class.json_message(event)
+      end
+    end
+
+    # Log event.
+    @logger.debug("Sending event to SNS topic [#{arn}] with subject [#{subject}] and message:")
+    message.split("\n").each { |line| @logger.debug(line) }
+
+    # Publish the message.
+    @sns.topics[arn].publish(message, :subject => subject.slice(0, MAX_SUBJECT_SIZE_IN_CHARACTERS))
+  end
+
+  def self.json_message(event)
+    json      = event.to_json
+    json_size = json.bytesize
+
+    # Truncate only the message if the JSON structure is too large.
+    if json_size > MAX_MESSAGE_SIZE_IN_BYTES
+      # TODO: Utilize `byteslice` in JRuby 1.7: http://jira.codehaus.org/browse/JRUBY-5547
+      event["message"] = event["message"].slice(0, (event["message"].bytesize - (json_size - MAX_MESSAGE_SIZE_IN_BYTES)))
+    end
+
+    event.to_json
+  end
+
+  def self.format_message(event)
+    message =  "Date: #{event.timestamp}\n"
+    message << "Source: #{event["source"]}\n"
+    message << "Tags: #{event["tags"].join(', ')}\n"
+    message << "Fields: #{event.to_hash.inspect}\n"
+    message << "Message: #{event["message"]}"
+
+    # TODO: Utilize `byteslice` in JRuby 1.7: http://jira.codehaus.org/browse/JRUBY-5547
+    message.slice(0, MAX_MESSAGE_SIZE_IN_BYTES)
+  end
+end
diff --git a/lib/logstash/outputs/sqs.rb b/lib/logstash/outputs/sqs.rb
new file mode 100644
index 00000000000..9791c64bb63
--- /dev/null
+++ b/lib/logstash/outputs/sqs.rb
@@ -0,0 +1,140 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+require "logstash/plugin_mixins/aws_config"
+require "stud/buffer"
+require "digest/sha2"
+
+# Push events to an Amazon Web Services Simple Queue Service (SQS) queue.
+#
+# SQS is a simple, scalable queue system that is part of the 
+# Amazon Web Services suite of tools.
+#
+# Although SQS is similar to other queuing systems like AMQP, it
+# uses a custom API and requires that you have an AWS account.
+# See http://aws.amazon.com/sqs/ for more details on how SQS works,
+# what the pricing schedule looks like and how to setup a queue.
+#
+# To use this plugin, you *must*:
+#
+#  * Have an AWS account
+#  * Setup an SQS queue
+#  * Create an identify that has access to publish messages to the queue.
+#
+# The "consumer" identity must have the following permissions on the queue:
+#
+#  * sqs:ChangeMessageVisibility
+#  * sqs:ChangeMessageVisibilityBatch
+#  * sqs:GetQueueAttributes
+#  * sqs:GetQueueUrl
+#  * sqs:ListQueues
+#  * sqs:SendMessage
+#  * sqs:SendMessageBatch
+#
+# Typically, you should setup an IAM policy, create a user and apply the IAM policy to the user.
+# A sample policy is as follows:
+#
+#      {
+#        "Statement": [
+#          {
+#            "Sid": "Stmt1347986764948",
+#            "Action": [
+#              "sqs:ChangeMessageVisibility",
+#              "sqs:ChangeMessageVisibilityBatch",
+#              "sqs:DeleteMessage",
+#              "sqs:DeleteMessageBatch",
+#              "sqs:GetQueueAttributes",
+#              "sqs:GetQueueUrl",
+#              "sqs:ListQueues",
+#              "sqs:ReceiveMessage"
+#            ],
+#            "Effect": "Allow",
+#            "Resource": [
+#              "arn:aws:sqs:us-east-1:200850199751:Logstash"
+#            ]
+#          }
+#        ]
+#      }
+#
+# See http://aws.amazon.com/iam/ for more details on setting up AWS identities.
+#
+class LogStash::Outputs::SQS < LogStash::Outputs::Base
+  include LogStash::PluginMixins::AwsConfig
+  include Stud::Buffer
+
+  config_name "sqs"
+  milestone 1
+
+  # Name of SQS queue to push messages into. Note that this is just the name of the queue, not the URL or ARN.
+  config :queue, :validate => :string, :required => true
+
+  # Set to true if you want send messages to SQS in batches with batch_send
+  # from the amazon sdk
+  config :batch, :validate => :boolean, :default => true
+
+  # If batch is set to true, the number of events we queue up for a batch_send.
+  config :batch_events, :validate => :number, :default => 10
+
+  # If batch is set to true, the maximum amount of time between batch_send commands when there are pending events to flush.
+  config :batch_timeout, :validate => :number, :default => 5
+
+  public
+  def aws_service_endpoint(region)
+    return {
+        :sqs_endpoint => "sqs.#{region}.amazonaws.com"
+    }
+  end
+
+  public 
+  def register
+    require "aws-sdk"
+
+    @sqs = AWS::SQS.new(aws_options_hash)
+
+    if @batch
+      if @batch_events > 10
+        raise RuntimeError.new(
+          "AWS only allows a batch_events parameter of 10 or less"
+        )
+      elsif @batch_events <= 1
+        raise RuntimeError.new(
+          "batch_events parameter must be greater than 1 (or its not a batch)"
+        )
+      end
+      buffer_initialize(
+        :max_items => @batch_events,
+        :max_interval => @batch_timeout,
+        :logger => @logger
+      )
+    end
+
+    begin
+      @logger.debug("Connecting to AWS SQS queue '#{@queue}'...")
+      @sqs_queue = @sqs.queues.named(@queue)
+      @logger.info("Connected to AWS SQS queue '#{@queue}' successfully.")
+    rescue Exception => e
+      @logger.error("Unable to access SQS queue '#{@queue}': #{e.to_s}")
+    end # begin/rescue
+  end # def register
+
+  public
+  def receive(event)
+    if @batch
+      buffer_receive(event.to_json)
+      return
+    end
+    @sqs_queue.send_message(event.to_json)
+  end # def receive
+
+  # called from Stud::Buffer#buffer_flush when there are events to flush
+  def flush(events, teardown=false)
+    @sqs_queue.batch_send(events)
+  end
+
+  public
+  def teardown
+    buffer_flush(:final => true)
+    @sqs_queue = nil
+    finished
+  end # def teardown
+end
diff --git a/lib/logstash/outputs/statsd.rb b/lib/logstash/outputs/statsd.rb
new file mode 100644
index 00000000000..755ec43cf1c
--- /dev/null
+++ b/lib/logstash/outputs/statsd.rb
@@ -0,0 +1,120 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+
+# statsd is a network daemon for aggregating statistics, such as counters and timers,
+# and shipping over UDP to backend services, such as Graphite or Datadog.
+#
+# The most basic coverage of this plugin is that the 'namespace', 'sender', and
+# 'metric' names are combined into the full metric path like so:
+#
+#     namespace.sender.metric
+#
+# The general idea is that you send statsd count or latency data and every few
+# seconds it will emit the aggregated values to the backend. Example aggregates are
+# average, max, stddev, etc.
+#
+# You can learn about statsd here:
+#
+# * <http://codeascraft.etsy.com/2011/02/15/measure-anything-measure-everything/>
+# * <https://github.com/etsy/statsd>
+#
+# A simple example usage of this is to count HTTP hits by response code; to learn
+# more about that, check out the [log metrics tutorial](../tutorials/metrics-from-logs)
+#
+# The default final metric sent to statsd would look like this:
+#
+#     namespace.sender.metric
+#
+# With regards to this plugin, the default namespace is "logstash", the default sender
+# is the ${host} field, and the metric name depends on what is set as the metric name
+# in the increment, decrement, timing, count, set or gauge variable. 
+#
+class LogStash::Outputs::Statsd < LogStash::Outputs::Base
+  ## Regex stolen from statsd code
+  RESERVED_CHARACTERS_REGEX = /[\:\|\@]/
+  config_name "statsd"
+  milestone 2
+
+  # The address of the statsd server.
+  config :host, :validate => :string, :default => "localhost"
+
+  # The port to connect to on your statsd server.
+  config :port, :validate => :number, :default => 8125
+
+  # The statsd namespace to use for this metric.
+  config :namespace, :validate => :string, :default => "logstash"
+
+  # The name of the sender. Dots will be replaced with underscores.
+  config :sender, :validate => :string, :default => "%{host}"
+
+  # An increment metric. Metric names as array.
+  config :increment, :validate => :array, :default => []
+
+  # A decrement metric. Metric names as array.
+  config :decrement, :validate => :array, :default => []
+
+  # A timing metric. `metric_name => duration` as hash
+  config :timing, :validate => :hash, :default => {}
+
+  # A count metric. `metric_name => count` as hash
+  config :count, :validate => :hash, :default => {}
+
+  # A set metric. `metric_name => "string"` to append as hash
+  config :set, :validate => :hash, :default => {}
+
+  # A gauge metric. `metric_name => gauge` as hash.
+  config :gauge, :validate => :hash, :default => {}
+  
+  # The sample rate for the metric.
+  config :sample_rate, :validate => :number, :default => 1
+
+  # Enable debugging.
+  config :debug, :validate => :boolean, :default => false, :deprecated => "This setting was never used by this plugin. It will be removed soon."
+
+  public
+  def register
+    require "statsd"
+    @client = Statsd.new(@host, @port)
+  end # def register
+
+  public
+  def receive(event)
+    return unless output?(event)
+
+    @client.namespace = event.sprintf(@namespace) if not @namespace.empty?
+    @logger.debug? and @logger.debug("Original sender: #{@sender}")
+    sender = event.sprintf(@sender)
+    @logger.debug? and @logger.debug("Munged sender: #{sender}")
+    @logger.debug? and @logger.debug("Event: #{event}")
+    @increment.each do |metric|
+      @client.increment(build_stat(event.sprintf(metric), sender), @sample_rate)
+    end
+    @decrement.each do |metric|
+      @client.decrement(build_stat(event.sprintf(metric), sender), @sample_rate)
+    end
+    @count.each do |metric, val|
+      @client.count(build_stat(event.sprintf(metric), sender),
+                    event.sprintf(val), @sample_rate)
+    end
+    @timing.each do |metric, val|
+      @client.timing(build_stat(event.sprintf(metric), sender),
+                     event.sprintf(val), @sample_rate)
+    end
+    @set.each do |metric, val|
+      @client.set(build_stat(event.sprintf(metric), sender),
+                    event.sprintf(val), @sample_rate)
+    end
+    @gauge.each do |metric, val|
+      @client.gauge(build_stat(event.sprintf(metric), sender),
+                    event.sprintf(val), @sample_rate)
+    end
+  end # def receive
+
+  def build_stat(metric, sender=@sender)
+    sender = sender.gsub('::','.').gsub(RESERVED_CHARACTERS_REGEX, '_').gsub(".", "_")
+    metric = metric.gsub('::','.').gsub(RESERVED_CHARACTERS_REGEX, '_')
+    @logger.debug? and @logger.debug("Formatted value", :sender => sender, :metric => metric)
+    return "#{sender}.#{metric}"
+  end
+end # class LogStash::Outputs::Statsd
diff --git a/lib/logstash/outputs/stdout.rb b/lib/logstash/outputs/stdout.rb
index 7ba232611f1..76c71426722 100644
--- a/lib/logstash/outputs/stdout.rb
+++ b/lib/logstash/outputs/stdout.rb
@@ -1,41 +1,60 @@
+# encoding: utf-8
 require "logstash/outputs/base"
 require "logstash/namespace"
 
+# A simple output which prints to the STDOUT of the shell running
+# Logstash. This output can be quite convenient when debugging
+# plugin configurations, by allowing instant access to the event
+# data after it has passed through the inputs and filters.
+#
+# For example, the following output configuration, in conjunction with the
+# Logstash `-e` command-line flag, will allow you to see the results
+# of your event pipeline for quick iteration. 
+# 
+#     output {
+#       stdout {}
+#     }
+# 
+# Useful codecs include:
+#
+# `rubydebug`: outputs event data using the ruby "awesome_print"
+# library[http://rubygems.org/gems/awesome_print]
+#
+#     output {
+#       stdout { codec => rubydebug }
+#     }
+#
+# `json`: outputs event data in structured JSON format
+#
+#     output {
+#       stdout { codec => json }
+#     }
+#
 class LogStash::Outputs::Stdout < LogStash::Outputs::Base
   begin
-    require "ap"
-    HAVE_AWESOME_PRINT = true
+     require "ap"
   rescue LoadError
-    HAVE_AWESOME_PRINT = false
   end
 
   config_name "stdout"
-
-  # Enable debugging. Tries to pretty-print the entire event object.
-  config :debug, :validate => :boolean
-
-  public
-  def initialize(params)
-    super
-
-    #@debug ||= false
-  end
+  milestone 3
+  
+  default :codec, "line"
 
   public
   def register
-    # nothing to do
+    @codec.on_event do |event|
+      $stdout.write(event)
+    end
   end
 
-  public
   def receive(event)
-    if @debug
-      if HAVE_AWESOME_PRINT
-        ap event.to_hash
-      else
-        p event.to_hash
-      end
-    else
-      puts event.to_s
+    return unless output?(event)
+    if event == LogStash::SHUTDOWN
+      finished
+      return
     end
-  end # def event
+    @codec.encode(event)
+  end
+
 end # class LogStash::Outputs::Stdout
diff --git a/lib/logstash/outputs/stomp.rb b/lib/logstash/outputs/stomp.rb
deleted file mode 100644
index bb83f2236ed..00000000000
--- a/lib/logstash/outputs/stomp.rb
+++ /dev/null
@@ -1,40 +0,0 @@
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-class LogStash::Outputs::Stomp < LogStash::Outputs::Base
-  config_name "stomp"
-
-  
-  # The address of the STOMP server.
-  config :host, :validate => :string
-
-  # The port to connet to on your STOMP server.
-  config :port, :validate => :number, :default => 61613
-
-  # The username to authenticate with.
-  config :user, :validate => :string, :default => ""
-
-  # The password to authenticate with.
-  config :password, :validate => :password, :default => ""
-
-  # The destination to read events from. Supports string expansion, meaning
-  # %{foo} values will expand to the field value.
-  #
-  # Example: "/topic/logstash"
-  config :destination, :validate => :string
-
-  # Enable debugging output?
-  config :debug, :validate => :boolean, :default => false
-
-  public
-  def register
-    require "stomp"
-    @client = Stomp::Client.new(@user, @password.value, @host, @port)
-  end # def register
-
-  public
-  def receive(event)
-    @logger.debug(["stomp sending event", { :host => @host, :event => event }])
-    @client.publish(event.sprintf(@destination), event.to_json)
-  end # def receive
-end # class LogStash::Outputs::Stomp
diff --git a/lib/logstash/outputs/tcp.rb b/lib/logstash/outputs/tcp.rb
index 81a22bb9093..5176de967b0 100644
--- a/lib/logstash/outputs/tcp.rb
+++ b/lib/logstash/outputs/tcp.rb
@@ -1,44 +1,145 @@
+# encoding: utf-8
 require "logstash/outputs/base"
 require "logstash/namespace"
+require "thread"
 
-# This output writes each event in json format to 
-# the specified host:port over tcp.
+# Write events over a TCP socket.
 #
 # Each event json is separated by a newline.
+#
+# Can either accept connections from clients or connect to a server,
+# depending on `mode`.
 class LogStash::Outputs::Tcp < LogStash::Outputs::Base
 
   config_name "tcp"
+  milestone 2
+
+  default :codec, "json"
 
-  # The host to connect to
+  # When mode is `server`, the address to listen on.
+  # When mode is `client`, the address to connect to.
   config :host, :validate => :string, :required => true
 
-  # The port to connect to
+  # When mode is `server`, the port to listen on.
+  # When mode is `client`, the port to connect to.
   config :port, :validate => :number, :required => true
+  
+  # When connect failed,retry interval in sec.
+  config :reconnect_interval, :validate => :number, :default => 10
 
-  public
-  def initialize(params)
-    super
-  end # def initialize
+  # Mode to operate in. `server` listens for client connections,
+  # `client` connects to a server.
+  config :mode, :validate => ["server", "client"], :default => "client"
+
+  # The format to use when writing events to the file. This value
+  # supports any string and can include %{name} and other dynamic
+  # strings.
+  #
+  # If this setting is omitted, the full json representation of the
+  # event will be written as a single line.
+  config :message_format, :validate => :string, :deprecated => true
+
+  class Client
+    public
+    def initialize(socket, logger)
+      @socket = socket
+      @logger = logger
+      @queue  = Queue.new
+    end
+
+    public
+    def run
+      loop do
+        begin
+          @socket.write(@queue.pop)
+        rescue => e
+          @logger.warn("tcp output exception", :socket => @socket,
+                       :exception => e)
+          break
+        end
+      end
+    end # def run
+
+    public
+    def write(msg)
+      @queue.push(msg)
+    end # def write
+  end # class Client
 
   public
   def register
-    @socket = nil
+    require "stud/try"
+    if server?
+      workers_not_supported
+
+      @logger.info("Starting tcp output listener", :address => "#{@host}:#{@port}")
+      @server_socket = TCPServer.new(@host, @port)
+      @client_threads = []
+
+      @accept_thread = Thread.new(@server_socket) do |server_socket|
+        loop do
+          client_thread = Thread.start(server_socket.accept) do |client_socket|
+            client = Client.new(client_socket, @logger)
+            Thread.current[:client] = client
+            client.run
+          end
+          @client_threads << client_thread
+        end
+      end
+
+      @codec.on_event do |payload|
+        @client_threads.each do |client_thread|
+          client_thread[:client].write(payload)
+        end
+        @client_threads.reject! {|t| !t.alive? }
+      end
+    else
+      client_socket = nil
+      @codec.on_event do |payload|
+        begin
+          client_socket = connect unless client_socket
+          r,w,e = IO.select([client_socket], [client_socket], [client_socket], nil)
+          # don't expect any reads, but a readable socket might
+          # mean the remote end closed, so read it and throw it away.
+          # we'll get an EOFError if it happens.
+          client_socket.sysread(16384) if r.any?
+
+          # Now send the payload
+          client_socket.syswrite(payload) if w.any?
+        rescue => e
+          @logger.warn("tcp output exception", :host => @host, :port => @port,
+                       :exception => e, :backtrace => e.backtrace)
+          client_socket.close rescue nil
+          client_socket = nil
+          sleep @reconnect_interval
+          retry
+        end
+      end
+    end
   end # def register
 
   private
   def connect
-    @socket = TCPSocket.new(@host, @port)
-  end
+    Stud::try do
+      return TCPSocket.new(@host, @port)
+    end
+  end # def connect
+
+  private
+  def server?
+    @mode == "server"
+  end # def server?
 
   public
   def receive(event)
-    begin
-      connect unless @socket
-      @socket.write(event.to_hash.to_json)
-      @socket.write("\n")
-    rescue
-      @logger.warn(["tcp output exception", @host, @port, $!])
-      @socket = nil
-    end
+    return unless output?(event)
+
+    #if @message_format
+      #output = event.sprintf(@message_format) + "\n"
+    #else
+      #output = event.to_hash.to_json + "\n"
+    #end
+    
+    @codec.encode(event)
   end # def receive
 end # class LogStash::Outputs::Tcp
diff --git a/lib/logstash/outputs/udp.rb b/lib/logstash/outputs/udp.rb
new file mode 100644
index 00000000000..1469017e647
--- /dev/null
+++ b/lib/logstash/outputs/udp.rb
@@ -0,0 +1,38 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+require "socket"
+
+# Send events over UDP
+#
+# Keep in mind that UDP will lose messages.
+class LogStash::Outputs::UDP < LogStash::Outputs::Base
+  config_name "udp"
+  milestone 1
+  
+  default :codec, "json"
+
+  # The address to send messages to
+  config :host, :validate => :string, :required => true
+
+  # The port to send messages on
+  config :port, :validate => :number, :required => true
+
+  public
+  def register
+    @socket = UDPSocket.new
+    @codec.on_event do |payload|
+      @socket.send(payload, 0, @host, @port)
+    end
+  end
+
+  def receive(event)
+    return unless output?(event)
+    if event == LogStash::SHUTDOWN
+      finished
+      return
+    end
+    @codec.encode(event)
+  end
+
+end # class LogStash::Outputs::Stdout
diff --git a/lib/logstash/outputs/websocket.rb b/lib/logstash/outputs/websocket.rb
deleted file mode 100644
index b2ac660d5f2..00000000000
--- a/lib/logstash/outputs/websocket.rb
+++ /dev/null
@@ -1,47 +0,0 @@
-require "logstash/namespace"
-require "logstash/outputs/base"
-
-# TODO(sissel): THIS IS NOT SUPPORTED IN JRUBY YET
-class LogStash::Outputs::Websocket < LogStash::Outputs::Base
-
-  config_name "websocket"
-
-  # The address to serve websocket data from
-  config :host, :validate => :string, :default => "0.0.0.0"
-
-  # The port to serve websocket data from
-  config :port, :validate => :number, :default => 3232
-
-  public
-  def register
-    require "em-websocket" # rubygem 'em-websocket'
-    @channel = EventMachine::Channel.new
-    @subscribers = 0
-    @url.host = (@url.host or "0.0.0.0")
-    @url.port = (@url.port or 3232)
-    @logger.info("Registering websocket on #{@url}")
-    EventMachine::WebSocket.start(:host => @url.host, :port => @url.port) do |ws|
-      ws.onopen do
-        @subscribers += 1
-        @logger.info("New #{self.class.name} connection")
-        sid = @channel.subscribe do |msg| 
-          ws.send msg
-        end
-        ws.onclose do
-          @channel.unsubscribe(sid)
-          @subscribers -= 1
-        end # ws.onclose
-      end # ws.onopen
-    end
-  end # def register
-
-  public
-  def receive(event)
-    # Only publish the event to websockets if there are subscribers
-    # TODO(sissel): send a patch to eventmachine to fix this.
-    if @subscribers > 0
-      @logger.info("Sending event to websocket.")
-      @channel.push event.to_json
-    end
-  end # def receive
-end # class LogStash::Outputs::Websocket
diff --git a/lib/logstash/outputs/xmpp.rb b/lib/logstash/outputs/xmpp.rb
new file mode 100644
index 00000000000..93040111892
--- /dev/null
+++ b/lib/logstash/outputs/xmpp.rb
@@ -0,0 +1,78 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+
+# This output allows you ship events over XMPP/Jabber.
+#
+# This plugin can be used for posting events to humans over XMPP, or you can
+# use it for PubSub or general message passing for logstash to logstash.
+class LogStash::Outputs::Xmpp < LogStash::Outputs::Base
+  config_name "xmpp"
+  milestone 2
+
+  # The user or resource ID, like foo@example.com.
+  config :user, :validate => :string, :required => :true
+
+  # The xmpp password for the user/identity.
+  config :password, :validate => :password, :required => :true
+
+  # The users to send messages to
+  config :users, :validate => :array
+
+  # if muc/multi-user-chat required, give the name of the room that
+  # you want to join: room@conference.domain/nick
+  config :rooms, :validate => :array
+
+  # The xmpp server to connect to. This is optional. If you omit this setting,
+  # the host on the user/identity is used. (foo.com for user@foo.com)
+  config :host, :validate => :string
+
+  # The message to send. This supports dynamic strings like %{host}
+  config :message, :validate => :string, :required => true
+
+  public
+  def register
+    require "xmpp4r"
+    @client = connect
+
+    @mucs = []
+    @users = [] if !@users
+
+    # load the MUC Client if we are joining rooms.
+    if @rooms && !@rooms.empty?
+      require 'xmpp4r/muc'
+      @rooms.each do |room| # handle muc messages in different rooms
+        muc = Jabber::MUC::MUCClient.new(@client)
+        muc.join(room)
+        @mucs << muc
+      end # @rooms.each
+    end # if @rooms
+  end # def register
+
+  public
+  def connect
+    Jabber::debug = true
+    client = Jabber::Client.new(Jabber::JID.new(@user))
+    client.connect(@host)
+    client.auth(@password.value)
+    return client
+  end # def connect
+
+  public
+  def receive(event)
+    return unless output?(event)
+
+    string_message = event.sprintf(@message)
+    @users.each do |user|
+      msg = Jabber::Message.new(user, string_message)
+      msg.type = :chat
+      @client.send(msg)
+    end # @targets.each
+
+    msg = Jabber::Message.new(nil, string_message)
+    msg.type = :groupchat
+    @mucs.each do |muc|
+      muc.send(msg)
+    end # @mucs.each
+  end # def receive
+end # class LogStash::Outputs::Xmpp
diff --git a/lib/logstash/outputs/zeromq.rb b/lib/logstash/outputs/zeromq.rb
new file mode 100644
index 00000000000..c4b088fc236
--- /dev/null
+++ b/lib/logstash/outputs/zeromq.rb
@@ -0,0 +1,125 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+
+# Write events to a 0MQ PUB socket.
+#
+# You need to have the 0mq 2.1.x library installed to be able to use
+# this output plugin.
+#
+# The default settings will create a publisher connecting to a subscriber
+# bound to tcp://127.0.0.1:2120
+#
+class LogStash::Outputs::ZeroMQ < LogStash::Outputs::Base
+
+  config_name "zeromq"
+  milestone 2
+
+  default :codec, "json"
+
+  # 0mq socket address to connect or bind.
+  # Please note that `inproc://` will not work with logstashi.
+  # For each we use a context per thread.
+  # By default, inputs bind/listen and outputs connect.
+  config :address, :validate => :array, :default => ["tcp://127.0.0.1:2120"]
+
+  # The default logstash topologies work as follows:
+  #
+  # * pushpull - inputs are pull, outputs are push
+  # * pubsub - inputs are subscribers, outputs are publishers
+  # * pair - inputs are clients, inputs are servers
+  #
+  # If the predefined topology flows don't work for you,
+  # you can change the 'mode' setting
+  # TODO (lusis) add req/rep MAYBE
+  # TODO (lusis) add router/dealer
+  config :topology, :validate => ["pushpull", "pubsub", "pair"], :required => true
+
+  # This is used for the 'pubsub' topology only.
+  # On inputs, this allows you to filter messages by topic.
+  # On outputs, this allows you to tag a message for routing.
+  # NOTE: ZeroMQ does subscriber-side filtering
+  # NOTE: Topic is evaluated with `event.sprintf` so macros are valid here.
+  config :topic, :validate => :string, :default => ""
+
+  # Server mode binds/listens. Client mode connects.
+  config :mode, :validate => ["server", "client"], :default => "client"
+
+  # This exposes zmq_setsockopt for advanced tuning.
+  # See http://api.zeromq.org/2-1:zmq-setsockopt for details.
+  #
+  # This is where you would set values like:
+  #
+  # * ZMQ::HWM - high water mark
+  # * ZMQ::IDENTITY - named queues
+  # * ZMQ::SWAP_SIZE - space for disk overflow
+  #
+  # Example: sockopt => ["ZMQ::HWM", 50, "ZMQ::IDENTITY", "my_named_queue"]
+  config :sockopt, :validate => :hash
+
+  public
+  def register
+    require "ffi-rzmq"
+    require "logstash/util/zeromq"
+    self.class.send(:include, LogStash::Util::ZeroMQ)
+
+    if @mode == "server"
+      workers_not_supported("With 'mode => server', only one zeromq socket may bind to a port and may not be shared among threads. Going to single-worker mode for this plugin!")
+    end
+
+    # Translate topology shorthand to socket types
+    case @topology
+    when "pair"
+      zmq_const = ZMQ::PAIR
+    when "pushpull"
+      zmq_const = ZMQ::PUSH
+    when "pubsub"
+      zmq_const = ZMQ::PUB
+    end # case socket_type
+
+    @zsocket = context.socket(zmq_const)
+
+    error_check(@zsocket.setsockopt(ZMQ::LINGER, 1),
+                "while setting ZMQ::LINGER == 1)")
+
+    if @sockopt
+      setopts(@zsocket, @sockopt)
+    end
+
+    @address.each do |addr|
+      setup(@zsocket, addr)
+    end
+
+    @codec.on_event(&method(:publish))
+  end # def register
+
+  public
+  def teardown
+    error_check(@zsocket.close, "while closing the socket")
+  end # def teardown
+
+  private
+  def server?
+    @mode == "server"
+  end # def server?
+
+  public
+  def receive(event)
+    return unless output?(event)
+
+    @codec.encode(event)
+  end # def receive
+
+  def publish(payload)
+    @logger.debug? && @logger.debug("0mq: sending", :event => payload)
+    if @topology == "pubsub"
+      # TODO(sissel): Need to figure out how to fit this into the codecs system.
+      #@logger.debug("0mq output: setting topic to: #{event.sprintf(@topic)}")
+      #error_check(@zsocket.send_string(event.sprintf(@topic), ZMQ::SNDMORE),
+                  #"in topic send_string")
+    end
+    error_check(@zsocket.send_string(payload), "in send_string")
+  rescue => e
+    @logger.warn("0mq output exception", :address => @address, :exception => e)
+  end
+end # class LogStash::Outputs::ZeroMQ
diff --git a/lib/logstash/pipeline.rb b/lib/logstash/pipeline.rb
new file mode 100644
index 00000000000..8811e1e5dc4
--- /dev/null
+++ b/lib/logstash/pipeline.rb
@@ -0,0 +1,298 @@
+# encoding: utf-8
+require "thread" #
+require "stud/interval"
+require "logstash/namespace"
+require "logstash/errors"
+require "logstash/event"
+require "logstash/config/file"
+require "logstash/filters/base"
+require "logstash/inputs/base"
+require "logstash/outputs/base"
+
+class LogStash::Pipeline
+
+  FLUSH_EVENT = LogStash::FlushEvent.new
+
+  def initialize(configstr)
+    @logger = Cabin::Channel.get(LogStash)
+    grammar = LogStashConfigParser.new
+    @config = grammar.parse(configstr)
+    if @config.nil?
+      raise LogStash::ConfigurationError, grammar.failure_reason
+    end
+
+    # This will compile the config to ruby and evaluate the resulting code.
+    # The code will initialize all the plugins and define the
+    # filter and output methods.
+    code = @config.compile
+    # The config code is hard to represent as a log message...
+    # So just print it.
+    @logger.debug? && @logger.debug("Compiled pipeline code:\n#{code}")
+    begin
+      eval(code)
+    rescue => e
+      raise
+    end
+
+    @input_to_filter = SizedQueue.new(20)
+
+    # If no filters, pipe inputs directly to outputs
+    if !filters?
+      @filter_to_output = @input_to_filter
+    else
+      @filter_to_output = SizedQueue.new(20)
+    end
+    @settings = {
+      "filter-workers" => 1,
+    }
+  end # def initialize
+
+  def ready?
+    return @ready
+  end
+
+  def started?
+    return @started
+  end
+
+  def configure(setting, value)
+    if setting == "filter-workers"
+      # Abort if we have any filters that aren't threadsafe
+      if value > 1 && @filters.any? { |f| !f.threadsafe? }
+        plugins = @filters.select { |f| !f.threadsafe? }.collect { |f| f.class.config_name }
+        raise LogStash::ConfigurationError, "Cannot use more than 1 filter worker because the following plugins don't work with more than one worker: #{plugins.join(", ")}"
+      end
+    end
+    @settings[setting] = value
+  end
+
+  def filters?
+    return @filters.any?
+  end
+
+  def run
+    @started = true
+    @input_threads = []
+
+    start_inputs
+    start_filters if filters?
+    start_outputs
+
+    @ready = true
+
+    @logger.info("Pipeline started")
+    wait_inputs
+
+    if filters?
+      shutdown_filters
+      wait_filters
+      flush_filters_to_output!(:final => true)
+    end
+
+    shutdown_outputs
+    wait_outputs
+
+    @logger.info("Pipeline shutdown complete.")
+
+    # exit code
+    return 0
+  end # def run
+
+  def wait_inputs
+    @input_threads.each(&:join)
+  rescue Interrupt
+    # rbx does weird things during do SIGINT that I haven't debugged
+    # so we catch Interrupt here and signal a shutdown. For some reason the
+    # signal handler isn't invoked it seems? I dunno, haven't looked much into
+    # it.
+    shutdown
+  end
+
+  def shutdown_filters
+    @flusher_lock.synchronize { @flusher_thread.kill }
+    @input_to_filter.push(LogStash::ShutdownEvent.new)
+  end
+
+  def wait_filters
+    @filter_threads.each(&:join) if @filter_threads
+  end
+
+  def shutdown_outputs
+    # nothing, filters will do this
+    @filter_to_output.push(LogStash::ShutdownEvent.new)
+  end
+
+  def wait_outputs
+    # Wait for the outputs to stop
+    @output_threads.each(&:join)
+  end
+
+  def start_inputs
+    moreinputs = []
+    @inputs.each do |input|
+      if input.threadable && input.threads > 1
+        (input.threads-1).times do |i|
+          moreinputs << input.clone
+        end
+      end
+    end
+    @inputs += moreinputs
+
+    @inputs.each do |input|
+      input.register
+      start_input(input)
+    end
+  end
+
+  def start_filters
+    @filters.each(&:register)
+    @filter_threads = @settings["filter-workers"].times.collect do
+      Thread.new { filterworker }
+    end
+
+    @flusher_lock = Mutex.new
+    @flusher_thread = Thread.new { Stud.interval(5) { @flusher_lock.synchronize { @input_to_filter.push(FLUSH_EVENT) } } }
+  end
+
+  def start_outputs
+    @outputs.each(&:register)
+    @output_threads = [
+      Thread.new { outputworker }
+    ]
+  end
+
+  def start_input(plugin)
+    @input_threads << Thread.new { inputworker(plugin) }
+  end
+
+  def inputworker(plugin)
+    LogStash::Util::set_thread_name("<#{plugin.class.config_name}")
+    begin
+      plugin.run(@input_to_filter)
+    rescue LogStash::ShutdownSignal
+      return
+    rescue => e
+      if @logger.debug?
+        @logger.error(I18n.t("logstash.pipeline.worker-error-debug",
+                             :plugin => plugin.inspect, :error => e.to_s,
+                             :exception => e.class,
+                             :stacktrace => e.backtrace.join("\n")))
+      else
+        @logger.error(I18n.t("logstash.pipeline.worker-error",
+                             :plugin => plugin.inspect, :error => e))
+      end
+      puts e.backtrace if @logger.debug?
+      plugin.teardown
+      sleep 1
+      retry
+    end
+  rescue LogStash::ShutdownSignal
+    # nothing
+  ensure
+    plugin.teardown
+  end # def inputworker
+
+  def filterworker
+    LogStash::Util::set_thread_name("|worker")
+    begin
+      while true
+        event = @input_to_filter.pop
+
+        case event
+        when LogStash::Event
+          # use events array to guarantee ordering of origin vs created events
+          # where created events are emitted by filters like split or metrics
+          events = []
+          filter(event) { |newevent| events << newevent }
+          events.each { |event| @filter_to_output.push(event) }
+        when LogStash::FlushEvent
+          # handle filter flushing here so that non threadsafe filters (thus only running one filterworker)
+          # don't have to deal with thread safety implementing the flush method
+          @flusher_lock.synchronize { flush_filters_to_output! }
+        when LogStash::ShutdownEvent
+          # pass it down to any other filterworker and stop this worker
+          @input_to_filter.push(event)
+          break
+        end
+      end
+    rescue => e
+      @logger.error("Exception in filterworker", "exception" => e, "backtrace" => e.backtrace)
+    end
+
+    @filters.each(&:teardown)
+  end # def filterworker
+
+  def outputworker
+    LogStash::Util::set_thread_name(">output")
+    @outputs.each(&:worker_setup)
+    while true
+      event = @filter_to_output.pop
+      break if event.is_a?(LogStash::ShutdownEvent)
+      output(event)
+    end # while true
+    @outputs.each(&:teardown)
+  end # def outputworker
+
+  # Shutdown this pipeline.
+  #
+  # This method is intended to be called from another thread
+  def shutdown
+    @input_threads.each do |thread|
+      # Interrupt all inputs
+      @logger.info("Sending shutdown signal to input thread",
+                   :thread => thread)
+      thread.raise(LogStash::ShutdownSignal)
+      begin
+        thread.wakeup # in case it's in blocked IO or sleeping
+      rescue ThreadError
+      end
+
+      # Sometimes an input is stuck in a blocking I/O
+      # so we need to tell it to teardown directly
+      @inputs.each do |input|
+        input.teardown
+      end
+    end
+
+    # No need to send the ShutdownEvent to the filters/outputs nor to wait for
+    # the inputs to finish, because in the #run method we wait for that anyway.
+  end # def shutdown
+
+  def plugin(plugin_type, name, *args)
+    args << {} if args.empty?
+    klass = LogStash::Plugin.lookup(plugin_type, name)
+    return klass.new(*args)
+  end
+
+  def filter(event, &block)
+    @filter_func.call(event, &block)
+  end
+
+  def output(event)
+    @output_func.call(event)
+  end
+
+  # perform filters flush and yeild flushed event to the passed block
+  # @param options [Hash]
+  # @option options [Boolean] :final => true to signal a final shutdown flush
+  def flush_filters(options = {}, &block)
+    flushers = options[:final] ? @shutdown_flushers : @periodic_flushers
+
+    flushers.each do |flusher|
+      flusher.call(options, &block)
+    end
+  end
+
+  # perform filters flush into the output queue
+  # @param options [Hash]
+  # @option options [Boolean] :final => true to signal a final shutdown flush
+  def flush_filters_to_output!(options = {})
+    flush_filters(options) do |event|
+      unless event.cancelled?
+        @logger.debug? and @logger.debug("Pushing flushed events", :event => event)
+        @filter_to_output.push(event)
+      end
+    end
+  end # flush_filters_to_output!
+
+end # class Pipeline
diff --git a/lib/logstash/plugin.rb b/lib/logstash/plugin.rb
new file mode 100644
index 00000000000..ce8de95a5a0
--- /dev/null
+++ b/lib/logstash/plugin.rb
@@ -0,0 +1,155 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/logging"
+require "logstash/config/mixin"
+require "cabin"
+
+class LogStash::Plugin
+  attr_accessor :params
+  attr_accessor :logger
+
+  NL = "\n"
+
+  public
+  def hash
+    params.hash ^
+    self.class.name.hash
+  end
+
+  public
+  def eql?(other)
+    self.class.name == other.class.name && @params == other.params
+  end
+
+  public
+  def initialize(params=nil)
+    @params = params
+    @logger = Cabin::Channel.get(LogStash)
+  end
+
+  # This method is called when someone or something wants this plugin to shut
+  # down. When you successfully shutdown, you must call 'finished'
+  # You must also call 'super' in any subclasses.
+  public
+  def shutdown(queue)
+    # By default, shutdown is assumed a no-op for all plugins.
+    # If you need to take special efforts to shutdown (like waiting for
+    # an operation to complete, etc)
+    teardown
+    @logger.info("Received shutdown signal", :plugin => self)
+
+    @shutdown_queue = queue
+    if @plugin_state == :finished
+      finished
+    else
+      @plugin_state = :terminating
+    end
+  end # def shutdown
+
+  # You should call this method when you (the plugin) are done with work
+  # forever.
+  public
+  def finished
+    # TODO(sissel): I'm not sure what I had planned for this shutdown_queue
+    # thing
+    if @shutdown_queue
+      @logger.info("Sending shutdown event to agent queue", :plugin => self)
+      @shutdown_queue << self
+    end
+
+    if @plugin_state != :finished
+      @logger.info("Plugin is finished", :plugin => self)
+      @plugin_state = :finished
+    end
+  end # def finished
+
+  # Subclasses should implement this teardown method if you need to perform any
+  # special tasks during shutdown (like flushing, etc.)
+  public
+  def teardown
+    # nothing by default
+    finished
+  end
+
+  # This method is called when a SIGHUP triggers a reload operation
+  public
+  def reload
+    # Do nothing by default
+  end
+
+  public
+  def finished?
+    return @plugin_state == :finished
+  end # def finished?
+
+  public
+  def running?
+    return @plugin_state != :finished
+  end # def finished?
+
+  public
+  def terminating?
+    return @plugin_state == :terminating
+  end # def terminating?
+
+  public
+  def to_s
+    return "#{self.class.name}: #{@params}"
+  end
+
+  protected
+  def update_watchdog(state)
+    Thread.current[:watchdog] = Time.now
+    Thread.current[:watchdog_state] = state
+  end
+
+  protected
+  def clear_watchdog
+    Thread.current[:watchdog] = nil
+    Thread.current[:watchdog_state] = nil
+  end
+
+  public
+  def inspect
+    if !@config.nil?
+      description = @config \
+        .select { |k,v| !v.nil? && (v.respond_to?(:empty?) && !v.empty?) } \
+        .collect { |k,v| "#{k}=>#{v.inspect}" }
+      return "<#{self.class.name} #{description.join(", ")}>"
+    else
+      return "<#{self.class.name} --->"
+    end
+  end
+
+  # Look up a plugin by type and name.
+  public
+  def self.lookup(type, name)
+    # Try to load the plugin requested.
+    # For example, load("filter", "grok") will try to require
+    #   logstash/filters/grok
+    #
+    # And expects to find LogStash::Filters::Grok (or something similar based
+    # on pattern matching
+
+    path = "logstash/#{type}s/#{name}"
+    require(path)
+
+    base = LogStash.const_get("#{type.capitalize}s")
+    klass = nil
+    #klass_sym = base.constants.find { |c| c.to_s =~ /^#{Regexp.quote(name)}$/i }
+    #if klass_sym.nil?
+
+    # Look for a plugin by the config_name
+    # the namespace can contain constants which are not for plugins classes (do not respond to :config_name)
+    # for example, the ElasticSearch output adds the LogStash::Outputs::Elasticsearch::Protocols namespace
+    klass_sym = base.constants.find { |c| o = base.const_get(c); o.respond_to?(:config_name) && o.config_name == name }
+    klass = base.const_get(klass_sym)
+
+    raise LoadError if klass.nil?
+
+    return klass
+  rescue LoadError => e
+    raise LogStash::PluginLoadingError,
+      I18n.t("logstash.pipeline.plugin-loading-error", :type => type, :name => name, :path => path, :error => e.to_s)
+  end # def load
+end # class LogStash::Plugin
diff --git a/lib/logstash/plugin_mixins/aws_config.rb b/lib/logstash/plugin_mixins/aws_config.rb
new file mode 100644
index 00000000000..b4ec9d32844
--- /dev/null
+++ b/lib/logstash/plugin_mixins/aws_config.rb
@@ -0,0 +1,93 @@
+# encoding: utf-8
+require "logstash/config/mixin"
+
+module LogStash::PluginMixins::AwsConfig
+
+  @logger = Cabin::Channel.get(LogStash)
+
+  # This method is called when someone includes this module
+  def self.included(base)
+    # Add these methods to the 'base' given.
+    base.extend(self)
+    base.setup_aws_config
+  end
+
+  US_EAST_1 = "us-east-1"
+  
+  public
+  def setup_aws_config
+    # The AWS Region
+    config :region, :validate => [US_EAST_1, "us-west-1", "us-west-2",
+                                  "eu-west-1", "ap-southeast-1", "ap-southeast-2",
+                                  "ap-northeast-1", "sa-east-1", "us-gov-west-1"], :default => US_EAST_1
+
+    # This plugin uses the AWS SDK and supports several ways to get credentials, which will be tried in this order...   
+    # 1. Static configuration, using `access_key_id` and `secret_access_key` params in logstash plugin config   
+    # 2. External credentials file specified by `aws_credentials_file`   
+    # 3. Environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`   
+    # 4. Environment variables `AMAZON_ACCESS_KEY_ID` and `AMAZON_SECRET_ACCESS_KEY`   
+    # 5. IAM Instance Profile (available when running inside EC2)   
+    config :access_key_id, :validate => :string
+
+    # The AWS Secret Access Key
+    config :secret_access_key, :validate => :string
+
+    # Should we require (true) or disable (false) using SSL for communicating with the AWS API   
+    # The AWS SDK for Ruby defaults to SSL so we preserve that
+    config :use_ssl, :validate => :boolean, :default => true
+
+    # URI to proxy server if required
+    config :proxy_uri, :validate => :string
+
+    # Path to YAML file containing a hash of AWS credentials.   
+    # This file will only be loaded if `access_key_id` and
+    # `secret_access_key` aren't set. The contents of the
+    # file should look like this:
+    #
+    #     :access_key_id: "12345"
+    #     :secret_access_key: "54321"
+    #
+    config :aws_credentials_file, :validate => :string
+  end
+
+  public
+  def aws_options_hash
+    if @access_key_id.is_a?(NilClass) ^ @secret_access_key.is_a?(NilClass)
+      @logger.warn("Likely config error: Only one of access_key_id or secret_access_key was provided but not both.")
+    end
+
+    if ((!@access_key_id || !@secret_access_key)) && @aws_credentials_file
+      access_creds = YAML.load_file(@aws_credentials_file)
+
+      @access_key_id = access_creds[:access_key_id]
+      @secret_access_key = access_creds[:secret_access_key]
+    end
+
+    opts = {}
+
+    if (@access_key_id && @secret_access_key)
+      opts[:access_key_id] = @access_key_id
+      opts[:secret_access_key] = @secret_access_key
+    end
+
+    opts[:use_ssl] = @use_ssl
+
+    if (@proxy_uri)
+      opts[:proxy_uri] = @proxy_uri
+    end
+
+    # The AWS SDK for Ruby doesn't know how to make an endpoint hostname from a region
+    # for example us-west-1 -> foosvc.us-west-1.amazonaws.com
+    # So our plugins need to know how to generate their endpoints from a region
+    # Furthermore, they need to know the symbol required to set that value in the AWS SDK
+    # Classes using this module must implement aws_service_endpoint(region:string)
+    # which must return a hash with one key, the aws sdk for ruby config symbol of the service
+    # endpoint, which has a string value of the service endpoint hostname
+    # for example, CloudWatch, { :cloud_watch_endpoint => "monitoring.#{region}.amazonaws.com" }
+    # For a list, see https://github.com/aws/aws-sdk-ruby/blob/master/lib/aws/core/configuration.rb
+    opts.merge!(self.aws_service_endpoint(@region))
+    
+    return opts
+  end # def aws_options_hash
+
+end
diff --git a/lib/logstash/pluginmanager.rb b/lib/logstash/pluginmanager.rb
new file mode 100644
index 00000000000..fb365f20db4
--- /dev/null
+++ b/lib/logstash/pluginmanager.rb
@@ -0,0 +1,7 @@
+require "logstash/namespace"
+
+module LogStash::PluginManager
+
+require 'logstash/pluginmanager/main'
+
+end # class Logstash::PluginManager
diff --git a/lib/logstash/pluginmanager/install.rb b/lib/logstash/pluginmanager/install.rb
new file mode 100644
index 00000000000..d2ace41a24e
--- /dev/null
+++ b/lib/logstash/pluginmanager/install.rb
@@ -0,0 +1,64 @@
+require 'clamp'
+require 'logstash/namespace'
+require 'logstash/environment'
+require 'logstash/pluginmanager'
+require 'logstash/pluginmanager/util'
+require 'rubygems/dependency_installer'
+require 'rubygems/uninstaller'
+require 'jar-dependencies'
+require 'jar_install_post_install_hook'
+
+class LogStash::PluginManager::Install < Clamp::Command
+
+  parameter "PLUGIN", "plugin name or file"
+
+  option "--version", "VERSION", "version of the plugin to install", :default => ">= 0"
+
+  option "--proxy", "PROXY", "Use HTTP proxy for remote operations"
+
+  def execute
+    LogStash::Environment.load_logstash_gemspec!
+
+    ::Gem.configuration.verbose = false
+    ::Gem.configuration[:http_proxy] = proxy 
+
+    puts ("validating #{plugin} #{version}")
+
+    unless gem_path = (plugin =~ /\.gem$/ && File.file?(plugin)) ? plugin : LogStash::PluginManager::Util.download_gem(plugin, version)
+      $stderr.puts ("Plugin does not exist '#{plugin}'. Aborting")
+      exit(99)
+    end
+
+    unless gem_meta = LogStash::PluginManager::Util.logstash_plugin?(gem_path)
+      $stderr.puts ("Invalid logstash plugin gem '#{plugin}'. Aborting...")
+      exit(99)
+    end
+
+    puts ("valid logstash plugin. Continueing...")
+
+    if LogStash::PluginManager::Util.installed?(gem_meta.name)
+
+      current = Gem::Specification.find_by_name(gem_meta.name)
+      if Gem::Version.new(current.version) > Gem::Version.new(gem_meta.version)
+        unless LogStash::PluginManager::Util.ask_yesno("Do you wish to downgrade this plugin?")
+          $stderr.puts("Aborting installation")
+          exit(99)
+        end
+      end
+
+      puts ("removing existing plugin before installation")
+      ::Gem.done_installing_hooks.clear
+      ::Gem::Uninstaller.new(gem_meta.name, {}).uninstall
+    end
+
+    ::Gem.configuration.verbose = false
+    options = {}
+    options[:document] = []
+    inst = Gem::DependencyInstaller.new(options)
+    inst.install plugin, version
+    specs, _ = inst.installed_gems
+    puts ("Successfully installed '#{specs.name}' with version '#{specs.version}'")
+
+  end
+
+end # class Logstash::PluginManager
diff --git a/lib/logstash/pluginmanager/list.rb b/lib/logstash/pluginmanager/list.rb
new file mode 100644
index 00000000000..54081761b59
--- /dev/null
+++ b/lib/logstash/pluginmanager/list.rb
@@ -0,0 +1,38 @@
+require 'clamp'
+require 'logstash/namespace'
+require 'logstash/pluginmanager'
+require 'logstash/pluginmanager/util'
+require 'rubygems/spec_fetcher'
+
+class LogStash::PluginManager::List < Clamp::Command
+
+  parameter "[PLUGIN]", "Plugin name to search for, leave empty for all plugins"
+
+  option "--group", "NAME", "Show all plugins from a certain group. Can be one of 'output', 'input', 'codec', 'filter'"
+
+  def execute
+
+    if group
+      unless ['input', 'output', 'filter', 'codec'].include?(group)
+        signal_usage_error "Group name not valid"
+      end
+      plugin_name = nil
+    else
+      plugin_name = plugin
+    end
+
+    Gem.configuration.verbose = false
+
+    # If we are listing a group make sure we check all gems
+    specs = LogStash::PluginManager::Util.matching_specs(plugin_name) \
+            .select{|spec| LogStash::PluginManager::Util.logstash_plugin?(spec) } \
+            .select{|spec| group ? group == spec.metadata['logstash_group'] : true}
+    if specs.empty?
+      $stderr.puts ("No plugins found.")
+      exit(99)
+    end
+    specs.each {|spec| puts ("#{spec.name} (#{spec.version})") }
+
+  end
+
+end # class Logstash::PluginManager
diff --git a/lib/logstash/pluginmanager/main.rb b/lib/logstash/pluginmanager/main.rb
new file mode 100644
index 00000000000..e66d562d285
--- /dev/null
+++ b/lib/logstash/pluginmanager/main.rb
@@ -0,0 +1,17 @@
+require "logstash/namespace"
+require "logstash/errors"
+require 'clamp'
+require 'logstash/pluginmanager/install'
+require 'logstash/pluginmanager/uninstall'
+require 'logstash/pluginmanager/list'
+require 'logstash/pluginmanager/update'
+require 'logstash/pluginmanager/util'
+
+class LogStash::PluginManager::Main < Clamp::Command
+
+  subcommand "install", "Install a plugin", LogStash::PluginManager::Install
+  subcommand "uninstall", "Uninstall a plugin", LogStash::PluginManager::Uninstall
+  subcommand "update", "Install a plugin", LogStash::PluginManager::Update
+  subcommand "list", "List all installed plugins", LogStash::PluginManager::List
+
+end # class Logstash::PluginManager::Main
diff --git a/lib/logstash/pluginmanager/uninstall.rb b/lib/logstash/pluginmanager/uninstall.rb
new file mode 100644
index 00000000000..d139f5f5cd9
--- /dev/null
+++ b/lib/logstash/pluginmanager/uninstall.rb
@@ -0,0 +1,30 @@
+require "logstash/namespace"
+require "logstash/logging"
+require "logstash/errors"
+require 'clamp'
+require 'logstash/pluginmanager'
+require 'logstash/pluginmanager/util'
+require 'rubygems/uninstaller'
+
+class LogStash::PluginManager::Uninstall < Clamp::Command
+
+  parameter "PLUGIN", "plugin name"
+
+  public
+  def execute
+
+    ::Gem.configuration.verbose = false
+
+    puts ("Validating removal of #{plugin}.")
+    
+    unless gem_data = LogStash::PluginManager::Util.logstash_plugin?(plugin)
+      $stderr.puts ("Trying to remove a non logstash plugin. Aborting")
+      exit(99)
+    end
+
+    puts ("Uninstalling plugin '#{plugin}' with version '#{gem_data.version}'.")
+    ::Gem::Uninstaller.new(plugin, {}).uninstall
+
+  end
+
+end # class Logstash::PluginManager
diff --git a/lib/logstash/pluginmanager/update.rb b/lib/logstash/pluginmanager/update.rb
new file mode 100644
index 00000000000..e2ebbb6b35f
--- /dev/null
+++ b/lib/logstash/pluginmanager/update.rb
@@ -0,0 +1,74 @@
+require 'clamp'
+require 'logstash/namespace'
+require 'logstash/pluginmanager'
+require 'logstash/pluginmanager/util'
+require 'rubygems/dependency_installer'
+require 'rubygems/uninstaller'
+require 'jar-dependencies'
+require 'jar_install_post_install_hook'
+
+class LogStash::PluginManager::Update < Clamp::Command
+
+  parameter "[PLUGIN]", "Plugin name"
+
+  option "--version", "VERSION", "version of the plugin to install", :default => ">= 0"
+
+  option "--proxy", "PROXY", "Use HTTP proxy for remote operations"
+
+  def execute
+
+    LogStash::Environment.load_logstash_gemspec!
+    ::Gem.configuration.verbose = false
+    ::Gem.configuration[:http_proxy] = proxy
+
+    if plugin.nil?
+      puts ("Updating all plugins")
+    else
+      puts ("Updating #{plugin} plugin")
+    end
+
+    specs = LogStash::PluginManager::Util.matching_specs(plugin).select{|spec| LogStash::PluginManager::Util.logstash_plugin?(spec) }
+    if specs.empty?
+      $stderr.puts ("No plugins found to update or trying to update a non logstash plugin.")
+      exit(99)
+    end
+    specs.each { |spec| update_gem(spec, version) }
+
+  end
+
+
+  def update_gem(spec, version)
+
+    unless gem_path = LogStash::PluginManager::Util.download_gem(spec.name, version)
+      $stderr.puts ("Plugin '#{spec.name}' does not exist remotely. Skipping.")
+      return nil
+    end
+
+    unless gem_meta = LogStash::PluginManager::Util.logstash_plugin?(gem_path)
+      $stderr.puts ("Invalid logstash plugin gem. skipping.")
+      return nil
+    end
+
+    unless Gem::Version.new(gem_meta.version) > Gem::Version.new(spec.version)
+      puts ("No newer version available for #{spec.name}. skipping.")
+      return nil
+    end
+
+    puts ("Updating #{spec.name} from version #{spec.version} to #{gem_meta.version}")
+
+    if LogStash::PluginManager::Util.installed?(spec.name)
+      ::Gem.done_installing_hooks.clear
+      ::Gem::Uninstaller.new(gem_meta.name, {}).uninstall
+    end
+
+    ::Gem.configuration.verbose = false
+    options = {}
+    options[:document] = []
+    inst = Gem::DependencyInstaller.new(options)
+    inst.install spec.name, gem_meta.version
+    specs, _ = inst.installed_gems
+    puts ("Update successful")
+
+  end
+
+end # class Logstash::PluginManager
diff --git a/lib/logstash/pluginmanager/util.rb b/lib/logstash/pluginmanager/util.rb
new file mode 100644
index 00000000000..ce6cab38058
--- /dev/null
+++ b/lib/logstash/pluginmanager/util.rb
@@ -0,0 +1,52 @@
+
+class LogStash::PluginManager::Util
+
+  def self.logstash_plugin?(gem)
+
+    gem_data = case
+    when gem.is_a?(Gem::Specification); gem
+    when (gem =~ /\.gem$/ and File.file?(gem)); Gem::Package.new(gem).spec
+    else Gem::Specification.find_by_name(gem)
+    end
+
+    gem_data.metadata['logstash_plugin'] == "true" ? gem_data : false
+  end
+
+  def self.download_gem(gem_name, gem_version = '')
+ 
+    gem_version ||= Gem::Requirement.default
+ 
+    dep = ::Gem::Dependency.new(gem_name, gem_version)
+    specs_and_sources, errors = ::Gem::SpecFetcher.fetcher.spec_for_dependency dep
+    if specs_and_sources.empty?
+      return false
+    end
+    spec, source = specs_and_sources.max_by { |s,| s.version }
+    path = source.download( spec, java.lang.System.getProperty("java.io.tmpdir"))
+    path
+  end
+
+  def self.installed?(name)
+    Gem::Specification.any? { |x| x.name == name }
+  end
+
+  def self.matching_specs(name)
+    req = Gem::Requirement.default
+    re = name ? /#{name}/i : //
+    specs = Gem::Specification.find_all{|spec| spec.name =~ re && req =~ spec.version}
+    specs.inject({}){|result, spec| result[spec.name_tuple] = spec; result}.values
+  end
+
+  def self.ask_yesno(prompt)
+    while true
+      $stderr.puts ("#{prompt} [y/n]: ")
+      case $stdin.getc.downcase
+        when 'Y', 'y', 'j', 'J', 'yes' #j for Germans (Ja)
+          return true
+        when /\A[nN]o?\Z/ #n or no
+          break
+      end
+    end
+  end
+
+end
diff --git a/lib/logstash/program.rb b/lib/logstash/program.rb
new file mode 100644
index 00000000000..06940fd2291
--- /dev/null
+++ b/lib/logstash/program.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+
+require "logstash/namespace"
+
+module LogStash::Program
+  public
+  def exit(value)
+    if RUBY_ENGINE == "jruby"
+      # Kernel::exit() in jruby just tosses an exception? Let's actually exit.
+      Java::java.lang.System.exit(value)
+    else
+      Kernel::exit(value)
+    end
+  end # def exit
+end # module LogStash::Program
diff --git a/lib/logstash/runner.rb b/lib/logstash/runner.rb
index 33064c7d29a..ea924a475e9 100644
--- a/lib/logstash/runner.rb
+++ b/lib/logstash/runner.rb
@@ -1,28 +1,194 @@
-require "rubygems"
-
-$: << File.join(File.dirname(__FILE__), "../")
-command = ARGV.shift
-
-commands = {
-  "agent" => proc do
-    require "logstash/agent"
-    agent = LogStash::Agent.new
-    agent.argv = ARGV
-    agent.run
-  end,
-  "web" => proc do
-    require "logstash/web/server"
-  end,
-  "test" => proc do
-    require "logstash_test_runner"
+# encoding: utf-8
+
+Encoding.default_external = Encoding::UTF_8
+$START = Time.now
+$DEBUGLIST = (ENV["DEBUG"] || "").split(",")
+
+require "logstash/environment"
+LogStash::Environment.set_gem_paths!
+LogStash::Environment.load_logstash_gemspec!
+LogStash::Environment.load_locale!
+
+Thread.abort_on_exception = true
+if ENV["PROFILE_BAD_LOG_CALLS"] || $DEBUGLIST.include?("log")
+  # Set PROFILE_BAD_LOG_CALLS=1 in your environment if you want
+  # to track down logger calls that cause performance problems
+  #
+  # Related research here:
+  #   https://github.com/jordansissel/experiments/tree/master/ruby/logger-string-vs-block
+  #
+  # Basically, the following is wastes tons of effort creating objects that are
+  # never used if the log level hides the log:
+  #
+  #     logger.debug("something happend", :what => Happened)
+  #
+  # This is shown to be 4x faster:
+  #
+  #     logger.debug(...) if logger.debug?
+  #
+  # I originally intended to use RubyParser and SexpProcessor to
+  # process all the logstash ruby code offline, but it was much
+  # faster to write this monkeypatch to warn as things are called.
+  require "cabin/mixins/logger"
+  module Cabin::Mixins::Logger
+    LEVELS.keys.each do |level|
+      m = "original_#{level}".to_sym
+      predicate = "#{level}?".to_sym
+      alias_method m, level
+      define_method(level) do |*args|
+        if !send(predicate)
+          warn("Unconditional log call", :location => caller[0])
+        end
+        send(m, *args)
+      end
+    end
+  end
+end # PROFILE_BAD_LOG_CALLS
+
+require "logstash/monkeypatches-for-debugging"
+require "logstash/namespace"
+require "logstash/program"
+
+class LogStash::RSpecsRunner
+  def initialize(args)
+    @args = args
+  end
+
+  def run
+    @result = RSpec::Core::Runner.run(@args)
+  end
+
+  def wait
+    return @result
   end
-}
-
-if commands.include?(command)
-  commands[command].call
-else
-  $stderr.puts "No such command #{command.inspect}"
-  $stderr.puts "Available commands:"
-  $stderr.puts commands.keys.map { |s| "  #{s}" }.join("\n")
-  exit 1
 end
+
+class LogStash::Runner
+  include LogStash::Program
+
+  def main(args)
+    require "logstash/util"
+    require "stud/trap"
+    require "stud/task"
+    @startup_interruption_trap = Stud::trap("INT") { puts "Interrupted"; exit 0 }
+
+    LogStash::Util::set_thread_name(self.class.name)
+    #$LOAD_PATH << File.join(File.dirname(__FILE__), "..")
+
+    if RUBY_VERSION < "1.9.2"
+      $stderr.puts "Ruby 1.9.2 or later is required. (You are running: " + RUBY_VERSION + ")"
+      return 1
+    end
+
+    Stud::untrap("INT", @startup_interruption_trap)
+
+    task = run(args)
+    exit(task.wait)
+  end # def self.main
+
+  def run(args)
+    command = args.shift
+    commands = {
+      "version" => lambda do
+        require "logstash/agent"
+        agent_args = ["--version"]
+        if args.include?("--verbose")
+          agent_args << "--verbose"
+        end
+        return LogStash::Agent.run($0, agent_args)
+      end,
+      "web" => lambda do
+        # Give them kibana.
+        require "logstash/kibana"
+        kibana = LogStash::Kibana::Runner.new
+        return kibana.run(args)
+      end,
+      "rspec" => lambda do
+        require "rspec/core/runner"
+        require "rspec"
+        spec_path = File.expand_path(File.join(File.dirname(__FILE__), "/../../spec"))
+        $LOAD_PATH << spec_path
+        all_specs = Dir.glob(File.join(spec_path, "/**/*_spec.rb"))
+        rspec = LogStash::RSpecsRunner.new(args.empty? ? all_specs : args)
+        return rspec.run
+      end,
+      "irb" => lambda do
+        require "irb"
+        return IRB.start(__FILE__)
+      end,
+      "pry" => lambda do
+        require "pry"
+        return binding.pry
+      end,
+      "plugin" => lambda do
+        require 'logstash/pluginmanager'
+        plugin_manager = LogStash::PluginManager::Main.new($0)
+        begin
+          plugin_manager.parse(args)
+          return plugin_manager.execute
+        rescue Clamp::HelpWanted => e
+          show_help(e.command)
+          return 0
+        end
+      end,
+      "agent" => lambda do
+        require "logstash/agent"
+        # Hack up a runner
+        agent = LogStash::Agent.new($0)
+        begin
+          agent.parse(args)
+        rescue Clamp::HelpWanted => e
+          show_help(e.command)
+          return 0
+        rescue Clamp::UsageError => e
+          # If 'too many arguments' then give the arguments to
+          # the next command. Otherwise it's a real error.
+          raise if e.message != "too many arguments"
+          remaining = agent.remaining_arguments
+        end
+
+        return agent.execute
+      end
+    } # commands
+
+    if commands.include?(command)
+      return Stud::Task.new { commands[command].call }
+    else
+      if command.nil?
+        $stderr.puts "No command given"
+      else
+        if !%w(--help -h help).include?(command)
+          # Emit 'no such command' if it's not someone asking for help.
+          $stderr.puts "No such command #{command.inspect}"
+        end
+      end
+      $stderr.puts %q[
+Usage: logstash <command> [command args]
+Run a command with the --help flag to see the arguments.
+For example: logstash agent --help
+
+Available commands:
+  agent - runs the logstash agent
+  version - emits version info about this logstash
+  web - runs the logstash web ui (called Kibana)
+  rspec - runs tests
+      ]
+      #$stderr.puts commands.keys.map { |s| "  #{s}" }.join("\n")
+      return Stud::Task.new { 1 }
+    end
+  end # def run
+
+  # @return true if this file is the main file being run and not via rspec
+  def self.autorun?
+    # caller is the current execution stack
+    $0 == __FILE__ && caller.none?{|entry| entry =~ /rspec/}
+  end
+
+  private
+
+  def show_help(command)
+    puts command.help
+  end
+end # class LogStash::Runner
+
+LogStash::Runner.new.main(ARGV) if LogStash::Runner.autorun?
diff --git a/lib/logstash/search/base.rb b/lib/logstash/search/base.rb
deleted file mode 100644
index 8abb01cc93f..00000000000
--- a/lib/logstash/search/base.rb
+++ /dev/null
@@ -1,39 +0,0 @@
-
-require "logstash/namespace"
-require "logstash/logging"
-require "logstash/event"
-
-class LogStash::Search::Base
-  # Do a search. 
-  #
-  # This method is async. You can expect a block and therefore
-  # should yield a result, not return one.
-  # 
-  # Implementations should yield a LogStash::Search::Result
-  # LogStash::Search::Result#events must be an array of LogStash::Event
-  def search(query)
-    raise "The class #{self.class.name} must implement the 'search' method."
-  end # def search
-
-  # Yields a histogram by field of a query.
-  #
-  # This method is async. You should expect a block to be passed and therefore
-  # should yield a result, not return one.
-  #
-  # Implementations should yield a LogStash::Search::FacetResult::Histogram
-  def histogram(query, field, interval=nil)
-    raise "The class #{self.class.name} must implement the 'histogram' method."
-  end
-
-  # Returns a list of popular terms from a query
-  # TODO(sissel): Implement
-  def popular_terms(query, fields, count=10)
-    raise "The class #{self.class.name} must implement the 'popular_terms' method."
-  end
-
-  # Count the results given by a query.
-  def count(query)
-    raise "The class #{self.class.name} must implement the 'count' method."
-  end
-
-end # class LogStash::Search::Base
diff --git a/lib/logstash/search/elasticsearch.rb b/lib/logstash/search/elasticsearch.rb
deleted file mode 100644
index 1111f4b851d..00000000000
--- a/lib/logstash/search/elasticsearch.rb
+++ /dev/null
@@ -1,205 +0,0 @@
-
-require "jruby-elasticsearch"
-require "logstash/namespace"
-require "logstash/logging"
-require "logstash/event"
-require "logstash/search/base"
-require "logstash/search/query"
-require "logstash/search/result"
-require "logstash/search/facetresult"
-require "logstash/search/facetresult/histogram"
- 
-class LogStash::Search::ElasticSearch < LogStash::Search::Base
-  public
-  def initialize(settings={})
-    @host = (settings[:host] || nil)
-    @port = (settings[:port] || 9300).to_i
-    @cluster = (settings[:cluster] || nil)
-    @logger = LogStash::Logger.new(STDOUT)
-    @client = ElasticSearch::Client.new(:host => @host, :port => @port, :cluster => @cluster)
-  end
-
-  # See LogStash::Search;:Base#search
-  public
-  def search(q, async=false)
-    raise "No block given for search call." if !block_given?
-    if q.is_a?(String)
-      q = LogStash::Search::Query.parse(q)
-    end
-
-    searchreq = @client.search do
-      sort("@timestamp", :desc)
-      query(q.query_string, :and)
-      offset(q.offset)
-      limit(q.count)
-    end
-
-    @logger.info("ElasticSearch search: #{q.query_string}")
-    start_time = Time.now
-
-    # TODO(sissel): Dedup this into a method.
-    if async
-      searcreq.execute do |response|
-        result = search_response_to_result(response)
-        result.offset = q.offset
-        result.duration = Time.now - start_time
-        @logger.debug(["Got search results (async)", 
-                     { :query => q.query_string, :duration => response.took.to_s,
-                       :result_count => result.total }])
-
-        yield result
-      end
-      return
-    else # not async
-      response = searchreq.execute!
-      result = search_response_to_result(response)
-      result.offset = q.offset
-      result.duration = Time.now - start_time
-      @logger.info(["Got search results (in blocking mode)", 
-                   { :query => q.query_string, :duration => response.took.to_s,
-                     :result_count => result.total }])
-
-      if block_given?
-        yield result
-      else
-        return result
-      end
-    end # if async
-    return
-  end # def search
-
-  private
-  def search_response_to_result(response)
-    result = LogStash::Search::Result.new
-
-    hits = response.hits rescue nil
-
-    if hits.nil? 
-      # return the whole object object as json as the error message for
-      # debugging later.
-      result.error_message = response
-      yield result
-      next # breaks from this callback
-    end
-
-    # We want to yield a list of LogStash::Event objects.
-    hits.each do |hit|
-      data = hit.getSource
-      # TODO(sissel): this conversion is only necessary because
-      # LogStash::Event#== invokes == on the data hash, and in in the
-      # test suite, we'll have a ruby array of tags compared against
-      # a java.util.ArrayList, which always fails.
-      # We also need this for #to_json to function properly.
-      # Possible fixes: 
-      #   - make Event#== smarter
-      #   - or, convert in the test (not as awesome)
-      data["@tags"] = data["@tags"].to_a # convert java ArrayList to Ruby
-      # Convert @fields to a ruby hash of array (so we can json it later)
-      # Prior to this conversion, it is a java.util.Map, etc, which does not
-      # to_json properly
-      fields = {}
-      data["@fields"].each do |key, value|
-        fields[key] = value.to_a
-      end
-      data["@fields"] = fields
-      result.events << LogStash::Event.new(data)
-    end
-
-    # Total hits this search could find if not limited
-    result.total = hits.totalHits
-    return result
-  end # def search_response_to_result
-
-  # See LogStash::Search;:Base#histogram
-  public
-  def histogram(q, field, interval=nil, async=false)
-    raise "No block given for search call." if async && !block_given?
-    if q.is_a?(String)
-      q = LogStash::Search::Query.parse(q)
-    end
-
-    name = "happyhisto"
-    searchreq = @client.search do
-      query(q.query_string, :and)
-      histogram(field, interval, name)
-      limit(10)
-    end
-
-    @logger.info("ElasticSearch Facet Query: #{q.query_string}")
-    start_time = Time.now
-
-    process = lambda do |response|
-      result = LogStash::Search::FacetResult.new
-      result.duration = Time.now - start_time
-
-      @logger.info(["Got search results", 
-                   { :query => q.query_string, :duration => response.took.to_s }])
-      # TODO(sissel): Check for error.
-
-      entries = response.facets.facet(name).entries
-
-      if entries.nil?
-        # return the whole response object as the error message for debugging
-        # later.
-        result.error_message = response
-        return result
-      end
-
-      entries.each do |entry|
-        # entry is a hash of keys 'total', 'mean', 'count', and 'key'
-        hist_entry = LogStash::Search::FacetResult::Histogram.new
-
-        # Sometimes the values here can be Float::NAN ?
-        # TODO(sissel): Dig into this.
-        #@logger.debug(:entry => entry)
-        hist_entry.key = entry.key
-        hist_entry.count = entry.count
-        
-        hist_entry.mean = entry.mean.nan? ? 0 : entry.mean
-        hist_entry.total = entry.total.nan? ? 0 : entry.total
-
-        #@logger.debug(:original => { 
-          #:key => entry.key, :count => entry.count.class,
-          #:mean => entry.mean.class, :total => entry.total.class 
-        #})
-        #@logger.debug(:histo => hist_entry)
-        result.results << hist_entry
-      end # for each histogram result
-      return result
-    end # lambda 'process'
-
-    if async
-      searchreq.execute do |response|
-        yield process.call(response)
-      end # request callback
-    else 
-      # async == false
-      return process.call(searchreq.execute!)
-    end # if async
-  end # def histogram
-
-  # Not used. Needs refactoring elsewhere.
-  private
-  def __anonymize
-    # TODO(sissel): Plugin-ify this (Search filters!)
-    # TODO(sissel): Implement
-    #  Search anonymization
-    #require "digest/md5"
-    #data["hits"]["hits"].each do |hit|
-    [].each do |hit|
-      event = LogStash::Event.new(hit["_source"])
-      event.to_hash.each do |key, value|
-        next unless value.is_a?(String)
-        value.gsub!(/[^ ]+\.loggly\.net/) { |match| "loggly-" + Digest::MD5.hexdigest(match)[0..6]  + ".example.com"}
-      end
-
-      event.fields.each do |key, value|
-        value = [value] if value.is_a?(String)
-        next unless value.is_a?(Array)
-        value.each do |v|
-          v.gsub!(/[^ ]+\.loggly\.net/) { |match| "loggly-" + Digest::MD5.hexdigest(match)[0..6]  + ".example.com"}
-        end # value.each
-      end # hit._source.@fields.each
-    end # data.hits.hits.each
-  end # def __anonymize
-end # class LogStash::Search::ElasticSearch
diff --git a/lib/logstash/search/facetresult.rb b/lib/logstash/search/facetresult.rb
deleted file mode 100644
index c42d76ee95e..00000000000
--- a/lib/logstash/search/facetresult.rb
+++ /dev/null
@@ -1,25 +0,0 @@
-
-require "logstash/namespace"
-require "logstash/logging"
-
-class LogStash::Search::FacetResult
-  # Array of LogStash::Search::FacetResult::Entry
-  attr_accessor :results
-
-  # How long this query took, in seconds (or fractions of).
-  attr_accessor :duration
-
-  # Error message, if any.
-  attr_accessor :error_message
-
-  def initialize(settings={})
-    @results = []
-    @duration = nil
-    @error_message = nil
-  end
-
-  def error?
-    return !@error_message.nil?
-  end
-end # class LogStash::Search::FacetResult
-
diff --git a/lib/logstash/search/facetresult/entry.rb b/lib/logstash/search/facetresult/entry.rb
deleted file mode 100644
index f09decca103..00000000000
--- a/lib/logstash/search/facetresult/entry.rb
+++ /dev/null
@@ -1,6 +0,0 @@
-
-require "logstash/search/facetresult"
-
-class LogStash::Search::FacetResult::Entry
-  # nothing here
-end # class LogStash::Search::FacetResult::Entry
diff --git a/lib/logstash/search/facetresult/histogram.rb b/lib/logstash/search/facetresult/histogram.rb
deleted file mode 100644
index 1851334d5bd..00000000000
--- a/lib/logstash/search/facetresult/histogram.rb
+++ /dev/null
@@ -1,21 +0,0 @@
-
-require "json"
-require "logstash/search/facetresult/entry"
-
-class LogStash::Search::FacetResult::Histogram < LogStash::Search::FacetResult::Entry
-  # The name or key for this result.
-  attr_accessor :key
-  attr_accessor :mean
-  attr_accessor :total
-  attr_accessor :count
-
-  # sometimes a parent call to to_json calls us with args?
-  def to_json(*args)
-    return {
-      "key" => @key,
-      "mean" => @mean,
-      "total" => @total,
-      "count" => @count,
-    }.to_json
-  end
-end
diff --git a/lib/logstash/search/query.rb b/lib/logstash/search/query.rb
deleted file mode 100644
index 5013373d55f..00000000000
--- a/lib/logstash/search/query.rb
+++ /dev/null
@@ -1,35 +0,0 @@
-require "logstash/namespace"
-require "logstash/logging"
-
-class LogStash::Search::Query
-  # The query string
-  attr_accessor :query_string
-
-  # The offset to start at (like SQL's SELECT ... OFFSET n)
-  attr_accessor :offset
-
-  # The max number of results to return. (like SQL's SELECT ... LIMIT n)
-  attr_accessor :count
-
-  # New query object.
-  #
-  # 'settings' should be a hash containing:
-  # 
-  # * :query_string - a string query for searching
-  # * :offset - (optional, default 0) offset to search from
-  # * :count - (optional, default 50) max number of results to return
-  def initialize(settings)
-    @query_string = settings[:query_string]
-    @offset = settings[:offset] || 0
-    @count = settings[:count] || 50
-  end
-
-  # Class method. Parses a query string and returns
-  # a LogStash::Search::Query instance
-  def self.parse(query_string)
-    # TODO(sissel): I would prefer not to invent my own query language.
-    # Can we be similar to Lucene, SQL, or other query languages?
-    return self.new(:query_string => query_string)
-  end
-
-end # class LogStash::Search::Query
diff --git a/lib/logstash/search/result.rb b/lib/logstash/search/result.rb
deleted file mode 100644
index 6cc80f222b7..00000000000
--- a/lib/logstash/search/result.rb
+++ /dev/null
@@ -1,47 +0,0 @@
-require "logstash/namespace"
-require "logstash/logging"
-
-class LogStash::Search::Result
-  # Array of LogStash::Event of results
-  attr_accessor :events
-
-  # How long this query took, in seconds (or fractions of).
-  attr_accessor :duration
-
-  # Offset in search
-  attr_accessor :offset
-
-  # Total records matched by this query, regardless of offset/count in query.
-  attr_accessor :total
-
-  # Error message, if any.
-  attr_accessor :error_message
-
-  public
-  def initialize(settings={})
-    @events = []
-    @duration = nil
-    @error_message = nil
-  end
-
-  public
-  def error?
-    return !@error_message.nil?
-  end
-
-  public
-  def to_json
-    return to_hash.to_json
-  end # def to_json
-
-  public
-  def to_hash
-    return {
-      "events" => @events,
-      "duration" => @duration,
-      "offset" => @offset,
-      "total" => @total,
-    }
-  end # def to_hash
-end # class LogStash::Search::Result
-
diff --git a/lib/logstash/search/twitter.rb b/lib/logstash/search/twitter.rb
deleted file mode 100644
index 6ffb1f1c100..00000000000
--- a/lib/logstash/search/twitter.rb
+++ /dev/null
@@ -1,92 +0,0 @@
-require "em-http-request"
-require "logstash/namespace"
-require "logstash/logging"
-require "logstash/event"
-require "logstash/search/base"
-require "logstash/search/query"
-require "logstash/search/result"
-require "logstash/search/facetresult"
-require "logstash/search/facetresult/histogram"
-
-# TODO(sissel): This won't work anymore (we don't use EM right now)
- 
-class LogStash::Search::Twitter < LogStash::Search::Base
-  public
-  def initialize(settings={})
-    @host = (settings[:host] || "search.twitter.com")
-    @port = (settings[:port] || 80).to_i
-    @logger = LogStash::Logger.new(STDOUT)
-  end
-
-  public
-  def search(query)
-    raise "No block given for search call." if !block_given?
-    if query.is_a?(String)
-      query = LogStash::Search::Query.parse(query)
-    end
-
-    # TODO(sissel): only search a specific index?
-    http = EventMachine::HttpRequest.new("http://#{@host}:#{@port}/search.json?q=#{URI.escape(query.query_string)}&rpp=#{URI.escape(query.count) rescue query.count}")
-
-    @logger.info(["Query", query])
-
-    start_time = Time.now
-    req = http.get
-
-    result = LogStash::Search::Result.new
-    req.callback do
-      data = JSON.parse(req.response)
-      result.duration = Time.now - start_time
-
-      hits = (data["results"] || nil) rescue nil
-
-      if hits.nil? or !data["error"].nil?
-        # Use the error message if any, otherwise, return the whole
-        # data object as json as the error message for debugging later.
-        result.error_message = (data["error"] rescue false) || data.to_json
-        yield result
-        next
-      end
-
-      hits.each do |hit|
-        hit["@message"]  = hit["text"]
-        hit["@timestamp"] = hit["created_at"]
-        hit.delete("text")
-      end
-
-      @logger.info(["Got search results", 
-                   { :query => query.query_string, :duration => data["duration"],
-                     :result_count => hits.size }])
-
-      if req.response_header.status != 200
-        result.error_message = data["error"] || req.inspect
-        @error = data["error"] || req.inspect
-      end
-
-      # We want to yield a list of LogStash::Event objects.
-      hits.each do |hit|
-        result.events << LogStash::Event.new(hit)
-      end
-
-      # Total hits this search could find if not limited
-      result.total = hits.size
-      result.offset = 0
-
-      yield result
-    end
-
-    req.errback do 
-      @logger.warn(["Query failed", query, req, req.response])
-      result.duration = Time.now - start_time
-      result.error_message = req.response
-
-      yield result
-    end
-  end # def search
-
-  def histogram(query, field, interval=nil)
-    # Nothing to histogram.
-    result = LogStash::Search::FacetResult.new
-    yield result
-  end
-end # class LogStash::Search::ElasticSearch
diff --git a/lib/logstash/sized_queue.rb b/lib/logstash/sized_queue.rb
new file mode 100644
index 00000000000..d96e27aba02
--- /dev/null
+++ b/lib/logstash/sized_queue.rb
@@ -0,0 +1,8 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/logging"
+
+require "thread" # for SizedQueue
+class LogStash::SizedQueue < SizedQueue
+  # TODO(sissel): Soon will implement push/pop stats, etc
+end
diff --git a/lib/logstash/stomp/handler.rb b/lib/logstash/stomp/handler.rb
deleted file mode 100644
index 229f4743854..00000000000
--- a/lib/logstash/stomp/handler.rb
+++ /dev/null
@@ -1,59 +0,0 @@
-require "logstash/namespace"
-
-# Base of Stomp Handler
-# it handles connecting and subscribing to the stomp broker which
-# is used in both stomp input and output
-class LogStash::Stomp
-  class Handler < EventMachine::Connection
-    include EM::Protocols::Stomp
-
-    attr_accessor :should_subscribe
-    attr_accessor :ready
-
-    public
-    def initialize(*args)
-      super
-
-      @input = args[0]
-      @logger = args[1]
-      @url = args[2]
-      @should_subscribe = true
-      @ready = false
-    end # def initialize
-
-    public
-    def connection_completed
-      @logger.debug("Connected")
-      connect :login => @url.user, :passcode => @url.password
-      @ready = true
-    end # def connection_completed
-
-    public
-    def unbind
-      if $EVENTMACHINE_STOPPING
-        @logger.debug(["Connection to stomp broker died (probably since we are exiting)",
-                      { :url => @url }])
-        return
-      end
-                    
-      @logger.error(["Connection to stomp broker died, retrying.", { :url => @url }])
-      @ready = false
-      EventMachine::Timer.new(1) do
-        reconnect(@url.host, @url.port)
-      end
-    end # def unbind
-
-    public
-    def receive_msg(message)
-      @logger.debug(["receiving message", { :msg => message }])
-      if message.command == "CONNECTED"
-        if @should_subscribe
-          @logger.debug(["subscribing to", { :path => @url.path }])
-          subscribe @url.path
-          return
-        end
-        @ready = true
-      end
-    end # def receive_msg
-  end # class Handler
-end # class LogStash::Stomp
diff --git a/lib/logstash/threadwatchdog.rb b/lib/logstash/threadwatchdog.rb
new file mode 100644
index 00000000000..ab41d3a49a0
--- /dev/null
+++ b/lib/logstash/threadwatchdog.rb
@@ -0,0 +1,37 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/logging"
+
+class LogStash::ThreadWatchdog
+  attr_accessor :logger
+  attr_accessor :threads
+
+  class TimeoutError < StandardError; end
+
+  public
+  def initialize(threads, watchdog_timeout=10)
+    @threads = threads
+    @watchdog_timeout = watchdog_timeout
+  end # def initialize
+
+  public
+  def watch
+    while sleep(1)
+      cutoff = Time.now - @watchdog_timeout
+      @threads.each do |t|
+        watchdog = t[:watchdog]
+        if watchdog and watchdog <= cutoff
+          age = Time.now - watchdog
+          @logger.fatal("thread watchdog timeout",
+                        :thread => t,
+                        :backtrace => t.backtrace,
+                        :thread_watchdog => watchdog,
+                        :age => age,
+                        :cutoff => @watchdog_timeout,
+                        :state => t[:watchdog_state])
+          raise TimeoutError, "watchdog timeout"
+        end
+      end
+    end
+  end # def watch
+end # class LogStash::ThreadWatchdog
diff --git a/lib/logstash/time.rb b/lib/logstash/time.rb
deleted file mode 100644
index cf1e49d8e89..00000000000
--- a/lib/logstash/time.rb
+++ /dev/null
@@ -1,31 +0,0 @@
-require "logstash/namespace"
-require "date" # for DateTime
-
-# Provide our own Time wrapper for ISO8601 support
-# Example:
-#   >> LogStash::Time.now.to_iso8601
-#   => "2010-10-17 00:25:24.619014-0700"
-#
-#   >> LogStash::Time.now.utc.to_iso8601
-#   => "2010-10-17 07:25:26.788704Z"
-class LogStash::Time < ::Time
-  ISO8601 = "%Y-%m-%dT%H:%M:%S"
-
-  # Return a string that is this time in ISO8601 format.
-  def to_iso8601
-    tz = self.utc? ? "Z" : self.strftime("%z")
-    # zero-pad tv_usec so the time string is sortable.
-    return "%s.%06d%s" % [self.strftime(ISO8601), self.tv_usec, tz]
-  end
-
-  def self.to_iso8601(obj)
-    if obj.is_a?(DateTime)
-      tz = obj.offset == 0 ? "Z" : obj.strftime("%z")
-      # DateTime#sec_fraction is fractional seconds "of a day"
-      sec_fraction = (obj.sec_fraction.to_f * 86400 * 1000000)
-      return "%s.%06d%s" % [obj.strftime(ISO8601), sec_fraction, tz]
-    else
-      raise "Can't convert object of type #{obj.class} (#{obj}) to iso8601."
-    end
-  end
-end # class LogStash::Time
diff --git a/lib/logstash/timestamp.rb b/lib/logstash/timestamp.rb
new file mode 100644
index 00000000000..fc8d793911c
--- /dev/null
+++ b/lib/logstash/timestamp.rb
@@ -0,0 +1,93 @@
+# encoding: utf-8
+require "logstash/environment"
+require "logstash/json"
+require "forwardable"
+require "date"
+require "time"
+
+module LogStash
+  class TimestampParserError < StandardError; end
+
+  class Timestamp
+    extend Forwardable
+
+    def_delegators :@time, :tv_usec, :usec, :year, :iso8601, :to_i, :tv_sec, :to_f, :to_edn
+
+    attr_reader :time
+
+    ISO8601_STRFTIME = "%04d-%02d-%02dT%02d:%02d:%02d.%06d%+03d:00".freeze
+    ISO8601_PRECISION = 3
+
+    def initialize(time = Time.new)
+      @time = time.utc
+    end
+
+    def self.at(*args)
+      Timestamp.new(::Time.at(*args))
+    end
+
+    def self.parse(*args)
+      Timestamp.new(::Time.parse(*args))
+    end
+
+    def self.now
+      Timestamp.new(::Time.now)
+    end
+
+    # coerce tries different strategies based on the time object class to convert into a Timestamp.
+    # @param [String, Time, Timestamp] time the time object to try coerce
+    # @return [Timestamp, nil] Timestamp will be returned if successful otherwise nil
+    # @raise [TimestampParserError] on String with invalid format
+    def self.coerce(time)
+      case time
+      when String
+        LogStash::Timestamp.parse_iso8601(time)
+      when LogStash::Timestamp
+        time
+      when Time
+        LogStash::Timestamp.new(time)
+      else
+        nil
+      end
+    end
+
+    if LogStash::Environment.jruby?
+      JODA_ISO8601_PARSER = org.joda.time.format.ISODateTimeFormat.dateTimeParser
+      UTC = org.joda.time.DateTimeZone.forID("UTC")
+
+      def self.parse_iso8601(t)
+        millis = JODA_ISO8601_PARSER.parseMillis(t)
+        LogStash::Timestamp.at(millis / 1000, (millis % 1000) * 1000)
+      rescue => e
+        raise(TimestampParserError, "invalid timestamp string #{t.inspect}, error=#{e.inspect}")
+      end
+
+    else
+
+      def self.parse_iso8601(t)
+        # warning, ruby's Time.parse is *really* terrible and slow.
+        LogStash::Timestamp.new(::Time.parse(t))
+      rescue => e
+        raise(TimestampParserError, "invalid timestamp string #{t.inspect}, error=#{e.inspect}")
+      end
+    end
+
+    def utc
+      @time.utc # modifies the receiver
+      self
+    end
+    alias_method :gmtime, :utc
+
+    def to_json(*args)
+      # ignore arguments to respect accepted to_json method signature
+      LogStash::Json.dump(@time.iso8601(ISO8601_PRECISION))
+    end
+    alias_method :inspect, :to_json
+
+    def to_iso8601
+      @time.iso8601(ISO8601_PRECISION)
+    end
+    alias_method :to_s, :to_iso8601
+
+  end
+end
diff --git a/lib/logstash/util.rb b/lib/logstash/util.rb
index 9172b30f088..f0cbc956490 100644
--- a/lib/logstash/util.rb
+++ b/lib/logstash/util.rb
@@ -1,9 +1,152 @@
+# encoding: utf-8
 require "logstash/namespace"
+require "logstash/environment"
 
 module LogStash::Util
+  UNAME = case RbConfig::CONFIG["host_os"]
+    when /^linux/; "linux"
+    else; RbConfig::CONFIG["host_os"]
+  end
+
+  PR_SET_NAME = 15
   def self.set_thread_name(name)
-    # Keep java and ruby thread names in sync.
-    java.lang.Thread.currentThread.setName(name)
+    if RUBY_ENGINE == "jruby"
+      # Keep java and ruby thread names in sync.
+      Java::java.lang.Thread.currentThread.setName(name)
+    end
     Thread.current[:name] = name
+
+    if UNAME == "linux"
+      require "logstash/util/prctl"
+      # prctl PR_SET_NAME allows up to 16 bytes for a process name
+      # since MRI 1.9, JRuby, and Rubinius use system threads for this.
+      LibC.prctl(PR_SET_NAME, name[0..16], 0, 0, 0)
+    end
   end # def set_thread_name
+
+  # Merge hash 'src' into 'dst' nondestructively
+  #
+  # Duplicate keys will become array values
+  #
+  # [ src["foo"], dst["foo"] ]
+  def self.hash_merge(dst, src)
+    src.each do |name, svalue|
+      if dst.include?(name)
+        dvalue = dst[name]
+        if dvalue.is_a?(Hash) && svalue.is_a?(Hash)
+          dvalue = hash_merge(dvalue, svalue)
+        elsif svalue.is_a?(Array)
+          if dvalue.is_a?(Array)
+            # merge arrays without duplicates.
+            dvalue |= svalue
+          else
+            dvalue = [dvalue] | svalue
+          end
+        else
+          if dvalue.is_a?(Array)
+            dvalue << svalue unless dvalue.include?(svalue)
+          else
+            dvalue = [dvalue, svalue] unless dvalue == svalue
+          end
+        end
+
+        dst[name] = dvalue
+      else
+        # dst doesn't have this key, just set it.
+        dst[name] = svalue
+      end
+    end
+
+    return dst
+  end # def self.hash_merge
+
+  # Merge hash 'src' into 'dst' nondestructively
+  #
+  # Duplicate keys will become array values
+  # Arrays merged will simply be appended.
+  #
+  # [ src["foo"], dst["foo"] ]
+  def self.hash_merge_with_dups(dst, src)
+    src.each do |name, svalue|
+      if dst.include?(name)
+        dvalue = dst[name]
+        if dvalue.is_a?(Hash) && svalue.is_a?(Hash)
+          dvalue = hash_merge(dvalue, svalue)
+        elsif svalue.is_a?(Array)
+          if dvalue.is_a?(Array)
+            # merge arrays without duplicates.
+            dvalue += svalue
+          else
+            dvalue = [dvalue] + svalue
+          end
+        else
+          if dvalue.is_a?(Array)
+            dvalue << svalue unless dvalue.include?(svalue)
+          else
+            dvalue = [dvalue, svalue] unless dvalue == svalue
+          end
+        end
+
+        dst[name] = dvalue
+      else
+        # dst doesn't have this key, just set it.
+        dst[name] = svalue
+      end
+    end
+
+    return dst
+  end # def self.hash_merge
+
+  def self.hash_merge_many(*hashes)
+    dst = {}
+    hashes.each do |hash|
+      hash_merge_with_dups(dst, hash)
+    end
+    return dst
+  end # def hash_merge_many
+
+
+  # nomalize method definition based on platform.
+  # normalize is used to convert an object create through
+  # json deserialization from JrJackson in :raw mode to pure Ruby
+  # to support these pure Ruby object monkey patches.
+  # see logstash/json.rb and logstash/java_integration.rb
+
+  if LogStash::Environment.jruby?
+    require "java"
+
+    # recursively convert any Java LinkedHashMap and ArrayList to pure Ruby.
+    # will not recurse into pure Ruby objects. Pure Ruby object should never
+    # contain LinkedHashMap and ArrayList since these are only created at
+    # initial deserialization, anything after (deeper) will be pure Ruby.
+    def self.normalize(o)
+      case o
+      when Java::JavaUtil::LinkedHashMap
+        o.inject({}){|r, (k, v)| r[k] = normalize(v); r}
+      when Java::JavaUtil::ArrayList
+        o.map{|i| normalize(i)}
+      else
+        o
+      end
+    end
+
+  else
+
+    # identity function, pure Ruby object don't need normalization.
+    def self.normalize(o); o; end
+  end
+
+  def self.stringify_symbols(o)
+    case o
+    when Hash
+      o.inject({}){|r, (k, v)| r[k.is_a?(Symbol) ? k.to_s : k] = stringify_symbols(v); r}
+    when Array
+      o.map{|i| stringify_symbols(i)}
+    when Symbol
+      o.to_s
+    else
+      o
+    end
+  end
+
 end # module LogStash::Util
diff --git a/lib/logstash/util/accessors.rb b/lib/logstash/util/accessors.rb
new file mode 100644
index 00000000000..7c114dcc6a5
--- /dev/null
+++ b/lib/logstash/util/accessors.rb
@@ -0,0 +1,64 @@
+# encoding: utf-8
+
+require "logstash/namespace"
+require "logstash/util"
+
+module LogStash::Util
+
+  # PathCache is a singleton which globally caches a parsed fields path for the path to the
+  # container hash and key in that hash.
+  module PathCache
+    extend self
+
+    def get(accessor)
+      @cache ||= {}
+      @cache[accessor] ||= parse(accessor)
+    end
+
+    def parse(accessor)
+      path = accessor.split(/[\[\]]/).select{|s| !s.empty?}
+      [path.pop, path]
+    end
+  end
+
+  # Accessors uses a lookup table to speedup access of an accessor field of the type
+  # "[hello][world]" to the underlying store hash into {"hello" => {"world" => "foo"}}
+  class Accessors
+
+    def initialize(store)
+      @store = store
+      @lut = {}
+    end
+
+    def get(accessor)
+      target, key = lookup(accessor)
+      target.is_a?(Array) ? target[key.to_i] : target[key]
+    end
+
+    def set(accessor, value)
+      target, key = lookup(accessor)
+      target[key] = value
+    end
+
+    def strict_set(accessor, value)
+      set(accessor, LogStash::Event.validate_value(value))
+    end
+
+    def del(accessor)
+      target, key = lookup(accessor)
+      target.delete(key)
+    end
+
+    private
+
+    def lookup(accessor)
+      @lut[accessor] ||= store_path(accessor)
+    end
+
+    def store_path(accessor)
+      key, path = PathCache.get(accessor)
+      target = path.inject(@store) {|r, k| r[r.is_a?(Array) ? k.to_i : k] ||= {}}
+      [target, key]
+    end
+  end # class Accessors
+end # module LogStash::Util
diff --git a/lib/logstash/util/buftok.rb b/lib/logstash/util/buftok.rb
new file mode 100644
index 00000000000..a093874483a
--- /dev/null
+++ b/lib/logstash/util/buftok.rb
@@ -0,0 +1,139 @@
+# encoding: utf-8
+# BufferedTokenizer - Statefully split input data by a specifiable token
+#
+# Authors:: Tony Arcieri, Martin Emde
+#
+#----------------------------------------------------------------------------
+#
+# Copyright (C) 2006-07 by Tony Arcieri and Martin Emde
+# 
+# Distributed under the Ruby license (http://www.ruby-lang.org/en/LICENSE.txt)
+#
+#---------------------------------------------------------------------------
+#
+
+# (C)2006 Tony Arcieri, Martin Emde
+# Distributed under the Ruby license (http://www.ruby-lang.org/en/LICENSE.txt)
+
+# BufferedTokenizer takes a delimiter upon instantiation, or acts line-based
+# by default.  It allows input to be spoon-fed from some outside source which
+# receives arbitrary length datagrams which may-or-may-not contain the token
+# by which entities are delimited.
+#
+# Commonly used to parse lines out of incoming data:
+#
+#  module LineBufferedConnection
+#    def receive_data(data)
+#      (@buffer ||= BufferedTokenizer.new).extract(data).each do |line|
+#        receive_line(line)
+#      end
+#    end
+#  end
+
+module FileWatch; class BufferedTokenizer
+  # New BufferedTokenizers will operate on lines delimited by "\n" by default
+  # or allow you to specify any delimiter token you so choose, which will then
+  # be used by String#split to tokenize the input data
+  def initialize(delimiter = "\n", size_limit = nil)
+    # Store the specified delimiter
+    @delimiter = delimiter
+
+    # Store the specified size limitation
+    @size_limit = size_limit
+
+    # The input buffer is stored as an array.  This is by far the most efficient
+    # approach given language constraints (in C a linked list would be a more
+    # appropriate data structure).  Segments of input data are stored in a list
+    # which is only joined when a token is reached, substantially reducing the
+    # number of objects required for the operation.
+    @input = []
+
+    # Size of the input buffer
+    @input_size = 0
+  end
+
+  # Extract takes an arbitrary string of input data and returns an array of
+  # tokenized entities, provided there were any available to extract.  This
+  # makes for easy processing of datagrams using a pattern like:
+  #
+  #   tokenizer.extract(data).map { |entity| Decode(entity) }.each do ...
+  def extract(data)
+    # Extract token-delimited entities from the input string with the split command.
+    # There's a bit of craftiness here with the -1 parameter.  Normally split would
+    # behave no differently regardless of if the token lies at the very end of the 
+    # input buffer or not (i.e. a literal edge case)  Specifying -1 forces split to
+    # return "" in this case, meaning that the last entry in the list represents a
+    # new segment of data where the token has not been encountered
+    entities = data.split @delimiter, -1
+
+    # Check to see if the buffer has exceeded capacity, if we're imposing a limit
+    if @size_limit
+      raise 'input buffer full' if @input_size + entities.first.size > @size_limit
+      @input_size += entities.first.size
+    end
+    
+    # Move the first entry in the resulting array into the input buffer.  It represents
+    # the last segment of a token-delimited entity unless it's the only entry in the list.
+    @input << entities.shift
+
+    # If the resulting array from the split is empty, the token was not encountered
+    # (not even at the end of the buffer).  Since we've encountered no token-delimited
+    # entities this go-around, return an empty array.
+    return [] if entities.empty?
+
+    # At this point, we've hit a token, or potentially multiple tokens.  Now we can bring
+    # together all the data we've buffered from earlier calls without hitting a token,
+    # and add it to our list of discovered entities.
+    entities.unshift @input.join
+
+=begin
+    # Note added by FC, 10Jul07. This paragraph contains a regression. It breaks
+    # empty tokens. Think of the empty line that delimits an HTTP header. It will have
+    # two "\n" delimiters in a row, and this code mishandles the resulting empty token.
+    # It someone figures out how to fix the problem, we can re-enable this code branch.
+    # Multi-character token support.
+    # Split any tokens that were incomplete on the last iteration buf complete now.
+    entities.map! do |e|
+      e.split @delimiter, -1
+    end
+    # Flatten the resulting array.  This has the side effect of removing the empty
+    # entry at the end that was produced by passing -1 to split.  Add it again if
+    # necessary.
+    if (entities[-1] == [])
+      entities.flatten! << []
+    else
+      entities.flatten!
+    end
+=end
+
+    # Now that we've hit a token, joined the input buffer and added it to the entities
+    # list, we can go ahead and clear the input buffer.  All of the segments that were
+    # stored before the join can now be garbage collected.
+    @input.clear
+    
+    # The last entity in the list is not token delimited, however, thanks to the -1
+    # passed to split.  It represents the beginning of a new list of as-yet-untokenized  
+    # data, so we add it to the start of the list.
+    @input << entities.pop
+    
+    # Set the new input buffer size, provided we're keeping track
+    @input_size = @input.first.size if @size_limit
+
+    # Now we're left with the list of extracted token-delimited entities we wanted
+    # in the first place.  Hooray!
+    entities
+  end
+  
+  # Flush the contents of the input buffer, i.e. return the input buffer even though
+  # a token has not yet been encountered
+  def flush
+    buffer = @input.join
+    @input.clear
+    buffer
+  end
+
+  # Is the buffer empty?
+  def empty?
+    @input.empty?
+  end
+end; end
diff --git a/lib/logstash/util/charset.rb b/lib/logstash/util/charset.rb
new file mode 100644
index 00000000000..a82f1a7bdaa
--- /dev/null
+++ b/lib/logstash/util/charset.rb
@@ -0,0 +1,35 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/util"
+
+class LogStash::Util::Charset
+  attr_accessor :logger
+
+  def initialize(charset)
+    @charset = charset
+    @charset_encoding = Encoding.find(charset)
+  end
+
+  def convert(data)
+    data.force_encoding(@charset_encoding)
+
+    # NON UTF-8 charset declared.
+    # Let's convert it (as cleanly as possible) into UTF-8 so we can use it with JSON, etc.
+    return data.encode(Encoding::UTF_8, :invalid => :replace, :undef => :replace) unless @charset_encoding == Encoding::UTF_8
+
+    # UTF-8 charset declared.
+    # Some users don't know the charset of their logs or just don't know they
+    # can set the charset setting.
+    unless data.valid_encoding?
+      # A silly hack to help convert some of the unknown bytes to
+      # somewhat-readable escape codes. The [1..-2] is to trim the quotes
+      # ruby puts on the value.
+      return data.inspect[1..-2].tap do |escaped|
+        @logger.warn("Received an event that has a different character encoding than you configured.", :text => escaped, :expected_charset => @charset)
+      end
+    end
+
+    return data
+  end # def convert
+
+end # class LogStash::Util::Charset
diff --git a/lib/logstash/util/fieldreference.rb b/lib/logstash/util/fieldreference.rb
new file mode 100644
index 00000000000..0683310533c
--- /dev/null
+++ b/lib/logstash/util/fieldreference.rb
@@ -0,0 +1,68 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/util"
+
+module LogStash::Util::FieldReference
+
+  def compile(accessor)
+    if accessor[0,1] != '['
+      return <<-"CODE"
+        lambda do |store, &block|
+          return block.nil? ? store[#{accessor.inspect}] : block.call(store, #{accessor.inspect})
+        end
+      CODE
+    end
+
+    code = "lambda do |store, &block|\n"
+    selectors = accessor.scan(/(?<=\[).+?(?=\])/)
+    selectors.each_with_index do |tok, i|
+      last = (i == selectors.count() - 1)
+      code << "   # [#{tok}]#{ last ? " (last selector)" : "" }\n"
+
+      if last
+        code << <<-"CODE"
+          return block.call(store, #{tok.inspect}) unless block.nil?
+        CODE
+      end
+
+      code << <<-"CODE"
+        store = store.is_a?(Array) ? store[#{tok.to_i}] : store[#{tok.inspect}]
+        return store if store.nil?
+      CODE
+
+    end
+    code << "return store\nend"
+    #puts code
+    return code
+  end # def compile
+
+  def exec(accessor, store, &block)
+    @__fieldeval_cache ||= {}
+    @__fieldeval_cache[accessor] ||= eval(compile(accessor))
+    return @__fieldeval_cache[accessor].call(store, &block)
+  end
+
+  def set(accessor, value, store)
+    # The assignment can fail if the given field reference (accessor) does not exist
+    # In this case, we'll want to set the value manually.
+    if exec(accessor, store) { |hash, key| hash[key] = value }.nil?
+      return (store[accessor] = value) if accessor[0,1] != "["
+
+      # No existing element was found, so let's set one.
+      *parents, key = accessor.scan(/(?<=\[)[^\]]+(?=\])/)
+      parents.each do |p|
+        if store.include?(p)
+          store = store[p]
+        else
+          store[p] = {}
+          store = store[p]
+        end
+      end
+      store[key] = value
+    end
+
+    return value
+  end
+
+  extend self
+end # module LogStash::Util::FieldReference
diff --git a/lib/logstash/util/password.rb b/lib/logstash/util/password.rb
index 778851040a4..6cd9beb7593 100644
--- a/lib/logstash/util/password.rb
+++ b/lib/logstash/util/password.rb
@@ -1,3 +1,4 @@
+# encoding: utf-8
 require "logstash/namespace"
 require "logstash/util"
 
diff --git a/lib/logstash/util/prctl.rb b/lib/logstash/util/prctl.rb
new file mode 100644
index 00000000000..02f44f0d2cb
--- /dev/null
+++ b/lib/logstash/util/prctl.rb
@@ -0,0 +1,11 @@
+# encoding: utf-8
+
+module LibC
+  require "ffi"
+  extend FFI::Library
+  ffi_lib 'c'
+
+  # Ok so the 2nd arg isn't really a string... but whaatever
+  attach_function :prctl, [:int, :string, :long, :long, :long], :int
+end
+
diff --git a/lib/logstash/util/require-helper.rb b/lib/logstash/util/require-helper.rb
index c41caa04da8..6e9fde0d86a 100644
--- a/lib/logstash/util/require-helper.rb
+++ b/lib/logstash/util/require-helper.rb
@@ -1,12 +1,13 @@
-require "logger"
+# encoding: utf-8
 require "logstash/namespace"
+require "logstash/logging"
 
 module LogStash::Util::Require
   class << self
     attr_accessor :logger
 
     def require(lib, gemdep, message=nil)
-      @logger ||= Logger.new(STDERR)
+      @logger ||= LogStash::Logger.new(STDERR)
       begin
         require lib
       rescue LoadError => e
diff --git a/lib/logstash/util/socket_peer.rb b/lib/logstash/util/socket_peer.rb
new file mode 100644
index 00000000000..56ea2f3e246
--- /dev/null
+++ b/lib/logstash/util/socket_peer.rb
@@ -0,0 +1,7 @@
+# encoding: utf-8
+module ::LogStash::Util::SocketPeer
+  public
+  def peer
+    "#{peeraddr[3]}:#{peeraddr[1]}"
+  end # def peer
+end # module SocketPeer
diff --git a/lib/logstash/util/zeromq.rb b/lib/logstash/util/zeromq.rb
new file mode 100644
index 00000000000..6939807ccc1
--- /dev/null
+++ b/lib/logstash/util/zeromq.rb
@@ -0,0 +1,47 @@
+# encoding: utf-8
+require 'ffi-rzmq'
+require "logstash/namespace"
+
+module LogStash::Util::ZeroMQ
+  CONTEXT = ZMQ::Context.new
+  # LOGSTASH-400
+  # see https://github.com/chuckremes/ffi-rzmq/blob/master/lib/ffi-rzmq/socket.rb#L93-117
+  STRING_OPTS = %w{IDENTITY SUBSCRIBE UNSUBSCRIBE}
+
+  def context
+    CONTEXT
+  end
+
+  def setup(socket, address)
+    if server?
+      error_check(socket.bind(address), "binding to #{address}")
+    else
+      error_check(socket.connect(address), "connecting to #{address}")
+    end
+    @logger.info("0mq: #{server? ? 'connected' : 'bound'}", :address => address)
+  end
+
+  def error_check(rc, doing)
+    unless ZMQ::Util.resultcode_ok?(rc)
+      @logger.error("ZeroMQ error while #{doing}", { :error_code => rc })
+      raise "ZeroMQ Error while #{doing}"
+    end
+  end # def error_check
+
+  def setopts(socket, options)
+    options.each do |opt,value|
+      sockopt = opt.split('::')[1]
+      option = ZMQ.const_defined?(sockopt) ? ZMQ.const_get(sockopt) : ZMQ.const_missing(sockopt)
+      unless STRING_OPTS.include?(sockopt)
+        begin
+          Float(value)
+          value = value.to_i
+        rescue ArgumentError
+          raise "#{sockopt} requires a numeric value. #{value} is not numeric"
+        end
+      end # end unless
+      error_check(socket.setsockopt(option, value),
+              "while setting #{opt} == #{value}")
+    end # end each
+  end # end setopts
+end # module LogStash::Util::ZeroMQ
diff --git a/lib/logstash/version.rb b/lib/logstash/version.rb
new file mode 100644
index 00000000000..36f2bad27fe
--- /dev/null
+++ b/lib/logstash/version.rb
@@ -0,0 +1,6 @@
+# encoding: utf-8
+# The version of logstash.
+LOGSTASH_VERSION = "2.0.0.dev"
+
+# Note to authors: this should not include dashes because 'gem' barfs if
+# you include a dash in the version string.
diff --git a/lib/logstash/web/controllers/api_v1.rb b/lib/logstash/web/controllers/api_v1.rb
deleted file mode 100644
index 1d3190a4b67..00000000000
--- a/lib/logstash/web/controllers/api_v1.rb
+++ /dev/null
@@ -1,145 +0,0 @@
-require "logstash/search/elasticsearch"
-require "logstash/search/query"
-require "logstash/web/helpers/require_param"
-require "sinatra/base" # gem sinatra
-
-class LogStash::Web::Server < Sinatra::Base
-
-  # TODO(sissel): move this to a lib?
-  # Or a LogStash::Search::WebHelper or something?
-  def api_search
-    count = params["count"] = (params["count"] or 50).to_i
-    offset = params["offset"] = (params["offset"] or 0).to_i
-    format = (params[:format] or "json")
-
-    query = LogStash::Search::Query.new(
-      :query_string => params[:q],
-      :offset => offset,
-      :count => count
-    )
-
-    @backend.search(query, async=false) do |results|
-      @results = results
-      if @results.error?
-        status 500
-        case format
-        when "html"
-          content_type :html
-          body haml :"search/error", :layout => !request.xhr?
-        when "text"
-          content_type :txt
-          body erb :"search/error.txt", :layout => false
-        when "txt"
-          content_type :txt
-          body erb :"search/error.txt", :layout => false
-        when "json"
-          content_type :json
-          # TODO(sissel): issue/30 - needs refactoring here.
-          body({ "error" => @results.error_message }.to_json)
-        end # case params[:format]
-        next
-      end
-
-      @events = @results.events
-      @total = (@results.total rescue 0)
-      count = @results.events.size
-
-      if count and offset
-        if @total > (count + offset)
-          @result_end = (count + offset)
-        else
-          @result_end = @total
-        end
-        @result_start = offset
-      end
-
-      if count + offset < @total
-        next_params = params.clone
-        next_params["offset"] = [offset + count, @total - count].min
-        @next_href = "?" +  next_params.collect { |k,v| [URI.escape(k.to_s), URI.escape(v.to_s)].join("=") }.join("&")
-        last_params = next_params.clone
-        last_params["offset"] = @total - count
-        @last_href = "?" +  last_params.collect { |k,v| [URI.escape(k.to_s), URI.escape(v.to_s)].join("=") }.join("&")
-      end
-
-      if offset > 0
-        prev_params = params.clone
-        prev_params["offset"] = [offset - count, 0].max
-        @prev_href = "?" +  prev_params.collect { |k,v| [URI.escape(k.to_s), URI.escape(v.to_s)].join("=") }.join("&")
-
-        #if prev_params["offset"] > 0
-          first_params = prev_params.clone
-          first_params["offset"] = 0
-          @first_href = "?" +  first_params.collect { |k,v| [URI.escape(k.to_s), URI.escape(v.to_s)].join("=") }.join("&")
-        #end
-      end
-
-      # TODO(sissel): make a helper function taht goes hash -> cgi querystring
-      @refresh_href = "?" +  params.collect { |k,v| [URI.escape(k.to_s), URI.escape(v.to_s)].join("=") }.join("&")
-
-      case format
-      when "html"
-        content_type :html
-        body haml :"search/ajax", :layout => !request.xhr?
-      when "text"
-        content_type :txt
-        body erb :"search/results.txt", :layout => false
-      when "txt"
-        content_type :txt
-        body erb :"search/results.txt", :layout => false
-      when "json"
-        content_type :json
-        pretty = params.has_key?("pretty")
-        if pretty
-          body JSON.pretty_generate(@results.to_hash)
-        else
-          body @results.to_json
-        end
-      end # case params[:format]
-    end # @backend.search
-  end # def api_search
-
-  # TODO(sissel): Update these to all be /api/v1
-  post '/api/search' do
-    api_search
-  end # post /api/search
-
-  get '/api/search' do
-    api_search
-  end # get /api/search
-
-  get '/api/histogram' do
-    missing = require_param(:q)
-    if !missing.empty?
-      status 500
-      body({ "error" => "Missing requiremed parameters",
-             "missing" => missing }.to_json)
-      next
-    end # if !missing.empty?
-
-    format = (params[:format] or "json")            # default json
-    field = (params[:field] or "@timestamp")        # default @timestamp
-    interval = (params[:interval] or 3600000).to_i  # default 1 hour
-
-    results = @backend.histogram(params[:q], field, interval, async=false)
-
-    p :results => results
-    if results.error?
-      status 500
-      body({ "error" => results.error_message }.to_json)
-      next
-    end
-
-    begin
-      json = results.results.to_json
-      p :json => json
-      content_type :json
-      status 200
-      body json
-    rescue => e
-      p :exception => e
-      p e
-      raise e
-    end
-  end # get '/api/histogram'
-end # class LogStash::Web::Server
diff --git a/lib/logstash/web/controllers/search.rb b/lib/logstash/web/controllers/search.rb
deleted file mode 100644
index f6d8da498dd..00000000000
--- a/lib/logstash/web/controllers/search.rb
+++ /dev/null
@@ -1,60 +0,0 @@
-class LogStash::Web::Server < Sinatra::Base
-  get '/search' do
-    # TODO(sissel): Refactor this to use the api_search
-    result_callback = proc do |results|
-      status 500 if @error
-      @results = results
-
-      p :got => results
-
-      params[:format] ||= "html"
-      case params[:format]
-      when "html"
-        headers({"Content-Type" => "text/html" })
-        body haml :"search/results", :layout => !request.xhr?
-      when "text"
-        headers({"Content-Type" => "text/plain" })
-        body erb :"search/results.txt", :layout => false
-      when "txt"
-        headers({"Content-Type" => "text/plain" })
-        body erb :"search/results.txt", :layout => false
-      when "json"
-        headers({"Content-Type" => "text/plain" })
-        # TODO(sissel): issue/30 - needs refactoring here.
-        hits = @hits.collect { |h| h["_source"] }
-        response = {
-          "hits" => hits,
-        }
-
-        response["error"] = @error if @error
-        body response.to_json
-      end # case params[:format]
-    end # proc result_callback
-
-    # We'll still do a search query here even though most users
-    # have javascript enabled, we need to show the results in
-    # case a user doesn't have javascript.
-    if params[:q] and params[:q] != ""
-      query = LogStash::Search::Query.new(
-        :query_string => params[:q],
-        :offset => params[:offset],
-        :count => params[:count]
-      )
-
-      @backend.search(query) do |results|
-        p :got => results
-        begin
-          result_callback.call results
-        rescue => e
-          p :exception => e
-        end
-      end # @backend.search
-    else
-      results = LogStash::Search::Result.new(
-        :events => [],
-        :error_message => "No query given"
-      )
-      result_callback.call results
-    end
-  end # get '/search'
-end # class LogStash::Web::Server
diff --git a/lib/logstash/web/controllers/static_files.rb b/lib/logstash/web/controllers/static_files.rb
deleted file mode 100644
index fa2b9faff8e..00000000000
--- a/lib/logstash/web/controllers/static_files.rb
+++ /dev/null
@@ -1,38 +0,0 @@
-class LogStash::Web::Server < Sinatra::Base
-  # Mizuno can't serve static files from a jar
-  # https://github.com/matadon/mizuno/issues/9
-  #if __FILE__ =~ /^file:.+!.+$/
-    get '/js/*' do static_file end
-    get '/css/*' do static_file end
-    get '/media/*' do static_file end
-    get '/ws/*' do static_file end
-  #else
-    ## If here, we aren't running from a jar; safe to serve files
-    ## through the normal public handler.
-    #set :public, "#{File.dirname(__FILE__)}/public"
-  #end
-
-  def static_file
-    # request.path_info is the full path of the request.
-    path = File.join(File.dirname(__FILE__), "..", "public", *request.path_info.split("/"))
-    p :static => path
-    if File.exists?(path)
-      ext = path.split(".").last
-      case ext
-        when "js"; content_type "application/javascript"
-        when "css"; content_type "text/css"
-        when "jpg"; content_type "image/jpeg"
-        when "jpeg"; content_type "image/jpeg"
-        when "png"; content_type "image/png"
-        when "gif"; content_type "image/gif"
-      end
-
-      body File.new(path, "r").read
-    else
-      status 404
-      content_type "text/plain"
-      body "File not found: #{path}"
-    end
-  end # def static_file
-end # class LogStash::Web::Server
-
diff --git a/lib/logstash/web/helpers/require_param.rb b/lib/logstash/web/helpers/require_param.rb
deleted file mode 100644
index bfbd44c9973..00000000000
--- a/lib/logstash/web/helpers/require_param.rb
+++ /dev/null
@@ -1,17 +0,0 @@
-require "sinatra/base"
-
-module Sinatra
-  module RequireParam
-    def require_param(*fields)
-      missing = []
-      fields.each do |field|
-        if params[field].nil?
-          missing << field
-        end
-      end
-      return missing
-    end # def require_param
-  end # module RequireParam
-
-  helpers RequireParam
-end # module Sinatra
diff --git a/lib/logstash/web/public/css/smoothness/images/ui-bg_flat_0_aaaaaa_40x100.png b/lib/logstash/web/public/css/smoothness/images/ui-bg_flat_0_aaaaaa_40x100.png
deleted file mode 100644
index 5b5dab2ab7b..00000000000
Binary files a/lib/logstash/web/public/css/smoothness/images/ui-bg_flat_0_aaaaaa_40x100.png and /dev/null differ
diff --git a/lib/logstash/web/public/css/smoothness/images/ui-bg_flat_75_ffffff_40x100.png b/lib/logstash/web/public/css/smoothness/images/ui-bg_flat_75_ffffff_40x100.png
deleted file mode 100644
index ac8b229af95..00000000000
Binary files a/lib/logstash/web/public/css/smoothness/images/ui-bg_flat_75_ffffff_40x100.png and /dev/null differ
diff --git a/lib/logstash/web/public/css/smoothness/images/ui-bg_glass_55_fbf9ee_1x400.png b/lib/logstash/web/public/css/smoothness/images/ui-bg_glass_55_fbf9ee_1x400.png
deleted file mode 100644
index ad3d6346e00..00000000000
Binary files a/lib/logstash/web/public/css/smoothness/images/ui-bg_glass_55_fbf9ee_1x400.png and /dev/null differ
diff --git a/lib/logstash/web/public/css/smoothness/images/ui-bg_glass_65_ffffff_1x400.png b/lib/logstash/web/public/css/smoothness/images/ui-bg_glass_65_ffffff_1x400.png
deleted file mode 100644
index 42ccba269b6..00000000000
Binary files a/lib/logstash/web/public/css/smoothness/images/ui-bg_glass_65_ffffff_1x400.png and /dev/null differ
diff --git a/lib/logstash/web/public/css/smoothness/images/ui-bg_glass_75_dadada_1x400.png b/lib/logstash/web/public/css/smoothness/images/ui-bg_glass_75_dadada_1x400.png
deleted file mode 100644
index 5a46b47cb16..00000000000
Binary files a/lib/logstash/web/public/css/smoothness/images/ui-bg_glass_75_dadada_1x400.png and /dev/null differ
diff --git a/lib/logstash/web/public/css/smoothness/images/ui-bg_glass_75_e6e6e6_1x400.png b/lib/logstash/web/public/css/smoothness/images/ui-bg_glass_75_e6e6e6_1x400.png
deleted file mode 100644
index 86c2baa655e..00000000000
Binary files a/lib/logstash/web/public/css/smoothness/images/ui-bg_glass_75_e6e6e6_1x400.png and /dev/null differ
diff --git a/lib/logstash/web/public/css/smoothness/images/ui-bg_glass_95_fef1ec_1x400.png b/lib/logstash/web/public/css/smoothness/images/ui-bg_glass_95_fef1ec_1x400.png
deleted file mode 100644
index 4443fdc1a15..00000000000
Binary files a/lib/logstash/web/public/css/smoothness/images/ui-bg_glass_95_fef1ec_1x400.png and /dev/null differ
diff --git a/lib/logstash/web/public/css/smoothness/images/ui-bg_highlight-soft_75_cccccc_1x100.png b/lib/logstash/web/public/css/smoothness/images/ui-bg_highlight-soft_75_cccccc_1x100.png
deleted file mode 100644
index 7c9fa6c6edc..00000000000
Binary files a/lib/logstash/web/public/css/smoothness/images/ui-bg_highlight-soft_75_cccccc_1x100.png and /dev/null differ
diff --git a/lib/logstash/web/public/css/smoothness/images/ui-icons_222222_256x240.png b/lib/logstash/web/public/css/smoothness/images/ui-icons_222222_256x240.png
deleted file mode 100644
index b273ff111d2..00000000000
Binary files a/lib/logstash/web/public/css/smoothness/images/ui-icons_222222_256x240.png and /dev/null differ
diff --git a/lib/logstash/web/public/css/smoothness/images/ui-icons_2e83ff_256x240.png b/lib/logstash/web/public/css/smoothness/images/ui-icons_2e83ff_256x240.png
deleted file mode 100644
index 09d1cdc856c..00000000000
Binary files a/lib/logstash/web/public/css/smoothness/images/ui-icons_2e83ff_256x240.png and /dev/null differ
diff --git a/lib/logstash/web/public/css/smoothness/images/ui-icons_454545_256x240.png b/lib/logstash/web/public/css/smoothness/images/ui-icons_454545_256x240.png
deleted file mode 100644
index 59bd45b907c..00000000000
Binary files a/lib/logstash/web/public/css/smoothness/images/ui-icons_454545_256x240.png and /dev/null differ
diff --git a/lib/logstash/web/public/css/smoothness/images/ui-icons_888888_256x240.png b/lib/logstash/web/public/css/smoothness/images/ui-icons_888888_256x240.png
deleted file mode 100644
index 6d02426c114..00000000000
Binary files a/lib/logstash/web/public/css/smoothness/images/ui-icons_888888_256x240.png and /dev/null differ
diff --git a/lib/logstash/web/public/css/smoothness/images/ui-icons_cd0a0a_256x240.png b/lib/logstash/web/public/css/smoothness/images/ui-icons_cd0a0a_256x240.png
deleted file mode 100644
index 2ab019b73ec..00000000000
Binary files a/lib/logstash/web/public/css/smoothness/images/ui-icons_cd0a0a_256x240.png and /dev/null differ
diff --git a/lib/logstash/web/public/css/smoothness/jquery-ui-1.8.5.custom.css b/lib/logstash/web/public/css/smoothness/jquery-ui-1.8.5.custom.css
deleted file mode 100644
index 0cb22be829b..00000000000
--- a/lib/logstash/web/public/css/smoothness/jquery-ui-1.8.5.custom.css
+++ /dev/null
@@ -1,572 +0,0 @@
-/*
- * jQuery UI CSS Framework @VERSION
- *
- * Copyright 2010, AUTHORS.txt (http://jqueryui.com/about)
- * Dual licensed under the MIT or GPL Version 2 licenses.
- * http://jquery.org/license
- *
- * http://docs.jquery.com/UI/Theming/API
- */
-
-/* Layout helpers
-----------------------------------*/
-.ui-helper-hidden { display: none; }
-.ui-helper-hidden-accessible { position: absolute; left: -99999999px; }
-.ui-helper-reset { margin: 0; padding: 0; border: 0; outline: 0; line-height: 1.3; text-decoration: none; font-size: 100%; list-style: none; }
-.ui-helper-clearfix:after { content: "."; display: block; height: 0; clear: both; visibility: hidden; }
-.ui-helper-clearfix { display: inline-block; }
-/* required comment for clearfix to work in Opera \*/
-* html .ui-helper-clearfix { height:1%; }
-.ui-helper-clearfix { display:block; }
-/* end clearfix */
-.ui-helper-zfix { width: 100%; height: 100%; top: 0; left: 0; position: absolute; opacity: 0; filter:Alpha(Opacity=0); }
-
-
-/* Interaction Cues
-----------------------------------*/
-.ui-state-disabled { cursor: default !important; }
-
-
-/* Icons
-----------------------------------*/
-
-/* states and images */
-.ui-icon { display: block; text-indent: -99999px; overflow: hidden; background-repeat: no-repeat; }
-
-
-/* Misc visuals
-----------------------------------*/
-
-/* Overlays */
-.ui-widget-overlay { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }
-
-
-/*
- * jQuery UI CSS Framework @VERSION
- *
- * Copyright 2010, AUTHORS.txt (http://jqueryui.com/about)
- * Dual licensed under the MIT or GPL Version 2 licenses.
- * http://jquery.org/license
- *
- * http://docs.jquery.com/UI/Theming/API
- *
- * To view and modify this theme, visit http://jqueryui.com/themeroller/?ffDefault=Verdana,Arial,sans-serif&fwDefault=normal&fsDefault=1.1em&cornerRadius=4px&bgColorHeader=cccccc&bgTextureHeader=03_highlight_soft.png&bgImgOpacityHeader=75&borderColorHeader=aaaaaa&fcHeader=222222&iconColorHeader=222222&bgColorContent=ffffff&bgTextureContent=01_flat.png&bgImgOpacityContent=75&borderColorContent=aaaaaa&fcContent=222222&iconColorContent=222222&bgColorDefault=e6e6e6&bgTextureDefault=02_glass.png&bgImgOpacityDefault=75&borderColorDefault=d3d3d3&fcDefault=555555&iconColorDefault=888888&bgColorHover=dadada&bgTextureHover=02_glass.png&bgImgOpacityHover=75&borderColorHover=999999&fcHover=212121&iconColorHover=454545&bgColorActive=ffffff&bgTextureActive=02_glass.png&bgImgOpacityActive=65&borderColorActive=aaaaaa&fcActive=212121&iconColorActive=454545&bgColorHighlight=fbf9ee&bgTextureHighlight=02_glass.png&bgImgOpacityHighlight=55&borderColorHighlight=fcefa1&fcHighlight=363636&iconColorHighlight=2e83ff&bgColorError=fef1ec&bgTextureError=02_glass.png&bgImgOpacityError=95&borderColorError=cd0a0a&fcError=cd0a0a&iconColorError=cd0a0a&bgColorOverlay=aaaaaa&bgTextureOverlay=01_flat.png&bgImgOpacityOverlay=0&opacityOverlay=30&bgColorShadow=aaaaaa&bgTextureShadow=01_flat.png&bgImgOpacityShadow=0&opacityShadow=30&thicknessShadow=8px&offsetTopShadow=-8px&offsetLeftShadow=-8px&cornerRadiusShadow=8px
- */
-
-
-/* Component containers
-----------------------------------*/
-.ui-widget { font-family: Verdana,Arial,sans-serif; font-size: 1.1em; }
-.ui-widget .ui-widget { font-size: 1em; }
-.ui-widget input, .ui-widget select, .ui-widget textarea, .ui-widget button { font-family: Verdana,Arial,sans-serif; font-size: 1em; }
-.ui-widget-content { border: 1px solid #aaaaaa; background: #ffffff url(images/ui-bg_flat_75_ffffff_40x100.png) 50% 50% repeat-x; color: #222222; }
-.ui-widget-content a { color: #222222; }
-.ui-widget-header { border: 1px solid #aaaaaa; background: #cccccc url(images/ui-bg_highlight-soft_75_cccccc_1x100.png) 50% 50% repeat-x; color: #222222; font-weight: bold; }
-.ui-widget-header a { color: #222222; }
-
-/* Interaction states
-----------------------------------*/
-.ui-state-default, .ui-widget-content .ui-state-default, .ui-widget-header .ui-state-default { border: 1px solid #d3d3d3; background: #e6e6e6 url(images/ui-bg_glass_75_e6e6e6_1x400.png) 50% 50% repeat-x; font-weight: normal; color: #555555; }
-.ui-state-default a, .ui-state-default a:link, .ui-state-default a:visited { color: #555555; text-decoration: none; }
-.ui-state-hover, .ui-widget-content .ui-state-hover, .ui-widget-header .ui-state-hover, .ui-state-focus, .ui-widget-content .ui-state-focus, .ui-widget-header .ui-state-focus { border: 1px solid #999999; background: #dadada url(images/ui-bg_glass_75_dadada_1x400.png) 50% 50% repeat-x; font-weight: normal; color: #212121; }
-.ui-state-hover a, .ui-state-hover a:hover { color: #212121; text-decoration: none; }
-.ui-state-active, .ui-widget-content .ui-state-active, .ui-widget-header .ui-state-active { border: 1px solid #aaaaaa; background: #ffffff url(images/ui-bg_glass_65_ffffff_1x400.png) 50% 50% repeat-x; font-weight: normal; color: #212121; }
-.ui-state-active a, .ui-state-active a:link, .ui-state-active a:visited { color: #212121; text-decoration: none; }
-.ui-widget :active { outline: none; }
-
-/* Interaction Cues
-----------------------------------*/
-.ui-state-highlight, .ui-widget-content .ui-state-highlight, .ui-widget-header .ui-state-highlight  {border: 1px solid #fcefa1; background: #fbf9ee url(images/ui-bg_glass_55_fbf9ee_1x400.png) 50% 50% repeat-x; color: #363636; }
-.ui-state-highlight a, .ui-widget-content .ui-state-highlight a,.ui-widget-header .ui-state-highlight a { color: #363636; }
-.ui-state-error, .ui-widget-content .ui-state-error, .ui-widget-header .ui-state-error {border: 1px solid #cd0a0a; background: #fef1ec url(images/ui-bg_glass_95_fef1ec_1x400.png) 50% 50% repeat-x; color: #cd0a0a; }
-.ui-state-error a, .ui-widget-content .ui-state-error a, .ui-widget-header .ui-state-error a { color: #cd0a0a; }
-.ui-state-error-text, .ui-widget-content .ui-state-error-text, .ui-widget-header .ui-state-error-text { color: #cd0a0a; }
-.ui-priority-primary, .ui-widget-content .ui-priority-primary, .ui-widget-header .ui-priority-primary { font-weight: bold; }
-.ui-priority-secondary, .ui-widget-content .ui-priority-secondary,  .ui-widget-header .ui-priority-secondary { opacity: .7; filter:Alpha(Opacity=70); font-weight: normal; }
-.ui-state-disabled, .ui-widget-content .ui-state-disabled, .ui-widget-header .ui-state-disabled { opacity: .35; filter:Alpha(Opacity=35); background-image: none; }
-
-/* Icons
-----------------------------------*/
-
-/* states and images */
-.ui-icon { width: 16px; height: 16px; background-image: url(images/ui-icons_222222_256x240.png); }
-.ui-widget-content .ui-icon {background-image: url(images/ui-icons_222222_256x240.png); }
-.ui-widget-header .ui-icon {background-image: url(images/ui-icons_222222_256x240.png); }
-.ui-state-default .ui-icon { background-image: url(images/ui-icons_888888_256x240.png); }
-.ui-state-hover .ui-icon, .ui-state-focus .ui-icon {background-image: url(images/ui-icons_454545_256x240.png); }
-.ui-state-active .ui-icon {background-image: url(images/ui-icons_454545_256x240.png); }
-.ui-state-highlight .ui-icon {background-image: url(images/ui-icons_2e83ff_256x240.png); }
-.ui-state-error .ui-icon, .ui-state-error-text .ui-icon {background-image: url(images/ui-icons_cd0a0a_256x240.png); }
-
-/* positioning */
-.ui-icon-carat-1-n { background-position: 0 0; }
-.ui-icon-carat-1-ne { background-position: -16px 0; }
-.ui-icon-carat-1-e { background-position: -32px 0; }
-.ui-icon-carat-1-se { background-position: -48px 0; }
-.ui-icon-carat-1-s { background-position: -64px 0; }
-.ui-icon-carat-1-sw { background-position: -80px 0; }
-.ui-icon-carat-1-w { background-position: -96px 0; }
-.ui-icon-carat-1-nw { background-position: -112px 0; }
-.ui-icon-carat-2-n-s { background-position: -128px 0; }
-.ui-icon-carat-2-e-w { background-position: -144px 0; }
-.ui-icon-triangle-1-n { background-position: 0 -16px; }
-.ui-icon-triangle-1-ne { background-position: -16px -16px; }
-.ui-icon-triangle-1-e { background-position: -32px -16px; }
-.ui-icon-triangle-1-se { background-position: -48px -16px; }
-.ui-icon-triangle-1-s { background-position: -64px -16px; }
-.ui-icon-triangle-1-sw { background-position: -80px -16px; }
-.ui-icon-triangle-1-w { background-position: -96px -16px; }
-.ui-icon-triangle-1-nw { background-position: -112px -16px; }
-.ui-icon-triangle-2-n-s { background-position: -128px -16px; }
-.ui-icon-triangle-2-e-w { background-position: -144px -16px; }
-.ui-icon-arrow-1-n { background-position: 0 -32px; }
-.ui-icon-arrow-1-ne { background-position: -16px -32px; }
-.ui-icon-arrow-1-e { background-position: -32px -32px; }
-.ui-icon-arrow-1-se { background-position: -48px -32px; }
-.ui-icon-arrow-1-s { background-position: -64px -32px; }
-.ui-icon-arrow-1-sw { background-position: -80px -32px; }
-.ui-icon-arrow-1-w { background-position: -96px -32px; }
-.ui-icon-arrow-1-nw { background-position: -112px -32px; }
-.ui-icon-arrow-2-n-s { background-position: -128px -32px; }
-.ui-icon-arrow-2-ne-sw { background-position: -144px -32px; }
-.ui-icon-arrow-2-e-w { background-position: -160px -32px; }
-.ui-icon-arrow-2-se-nw { background-position: -176px -32px; }
-.ui-icon-arrowstop-1-n { background-position: -192px -32px; }
-.ui-icon-arrowstop-1-e { background-position: -208px -32px; }
-.ui-icon-arrowstop-1-s { background-position: -224px -32px; }
-.ui-icon-arrowstop-1-w { background-position: -240px -32px; }
-.ui-icon-arrowthick-1-n { background-position: 0 -48px; }
-.ui-icon-arrowthick-1-ne { background-position: -16px -48px; }
-.ui-icon-arrowthick-1-e { background-position: -32px -48px; }
-.ui-icon-arrowthick-1-se { background-position: -48px -48px; }
-.ui-icon-arrowthick-1-s { background-position: -64px -48px; }
-.ui-icon-arrowthick-1-sw { background-position: -80px -48px; }
-.ui-icon-arrowthick-1-w { background-position: -96px -48px; }
-.ui-icon-arrowthick-1-nw { background-position: -112px -48px; }
-.ui-icon-arrowthick-2-n-s { background-position: -128px -48px; }
-.ui-icon-arrowthick-2-ne-sw { background-position: -144px -48px; }
-.ui-icon-arrowthick-2-e-w { background-position: -160px -48px; }
-.ui-icon-arrowthick-2-se-nw { background-position: -176px -48px; }
-.ui-icon-arrowthickstop-1-n { background-position: -192px -48px; }
-.ui-icon-arrowthickstop-1-e { background-position: -208px -48px; }
-.ui-icon-arrowthickstop-1-s { background-position: -224px -48px; }
-.ui-icon-arrowthickstop-1-w { background-position: -240px -48px; }
-.ui-icon-arrowreturnthick-1-w { background-position: 0 -64px; }
-.ui-icon-arrowreturnthick-1-n { background-position: -16px -64px; }
-.ui-icon-arrowreturnthick-1-e { background-position: -32px -64px; }
-.ui-icon-arrowreturnthick-1-s { background-position: -48px -64px; }
-.ui-icon-arrowreturn-1-w { background-position: -64px -64px; }
-.ui-icon-arrowreturn-1-n { background-position: -80px -64px; }
-.ui-icon-arrowreturn-1-e { background-position: -96px -64px; }
-.ui-icon-arrowreturn-1-s { background-position: -112px -64px; }
-.ui-icon-arrowrefresh-1-w { background-position: -128px -64px; }
-.ui-icon-arrowrefresh-1-n { background-position: -144px -64px; }
-.ui-icon-arrowrefresh-1-e { background-position: -160px -64px; }
-.ui-icon-arrowrefresh-1-s { background-position: -176px -64px; }
-.ui-icon-arrow-4 { background-position: 0 -80px; }
-.ui-icon-arrow-4-diag { background-position: -16px -80px; }
-.ui-icon-extlink { background-position: -32px -80px; }
-.ui-icon-newwin { background-position: -48px -80px; }
-.ui-icon-refresh { background-position: -64px -80px; }
-.ui-icon-shuffle { background-position: -80px -80px; }
-.ui-icon-transfer-e-w { background-position: -96px -80px; }
-.ui-icon-transferthick-e-w { background-position: -112px -80px; }
-.ui-icon-folder-collapsed { background-position: 0 -96px; }
-.ui-icon-folder-open { background-position: -16px -96px; }
-.ui-icon-document { background-position: -32px -96px; }
-.ui-icon-document-b { background-position: -48px -96px; }
-.ui-icon-note { background-position: -64px -96px; }
-.ui-icon-mail-closed { background-position: -80px -96px; }
-.ui-icon-mail-open { background-position: -96px -96px; }
-.ui-icon-suitcase { background-position: -112px -96px; }
-.ui-icon-comment { background-position: -128px -96px; }
-.ui-icon-person { background-position: -144px -96px; }
-.ui-icon-print { background-position: -160px -96px; }
-.ui-icon-trash { background-position: -176px -96px; }
-.ui-icon-locked { background-position: -192px -96px; }
-.ui-icon-unlocked { background-position: -208px -96px; }
-.ui-icon-bookmark { background-position: -224px -96px; }
-.ui-icon-tag { background-position: -240px -96px; }
-.ui-icon-home { background-position: 0 -112px; }
-.ui-icon-flag { background-position: -16px -112px; }
-.ui-icon-calendar { background-position: -32px -112px; }
-.ui-icon-cart { background-position: -48px -112px; }
-.ui-icon-pencil { background-position: -64px -112px; }
-.ui-icon-clock { background-position: -80px -112px; }
-.ui-icon-disk { background-position: -96px -112px; }
-.ui-icon-calculator { background-position: -112px -112px; }
-.ui-icon-zoomin { background-position: -128px -112px; }
-.ui-icon-zoomout { background-position: -144px -112px; }
-.ui-icon-search { background-position: -160px -112px; }
-.ui-icon-wrench { background-position: -176px -112px; }
-.ui-icon-gear { background-position: -192px -112px; }
-.ui-icon-heart { background-position: -208px -112px; }
-.ui-icon-star { background-position: -224px -112px; }
-.ui-icon-link { background-position: -240px -112px; }
-.ui-icon-cancel { background-position: 0 -128px; }
-.ui-icon-plus { background-position: -16px -128px; }
-.ui-icon-plusthick { background-position: -32px -128px; }
-.ui-icon-minus { background-position: -48px -128px; }
-.ui-icon-minusthick { background-position: -64px -128px; }
-.ui-icon-close { background-position: -80px -128px; }
-.ui-icon-closethick { background-position: -96px -128px; }
-.ui-icon-key { background-position: -112px -128px; }
-.ui-icon-lightbulb { background-position: -128px -128px; }
-.ui-icon-scissors { background-position: -144px -128px; }
-.ui-icon-clipboard { background-position: -160px -128px; }
-.ui-icon-copy { background-position: -176px -128px; }
-.ui-icon-contact { background-position: -192px -128px; }
-.ui-icon-image { background-position: -208px -128px; }
-.ui-icon-video { background-position: -224px -128px; }
-.ui-icon-script { background-position: -240px -128px; }
-.ui-icon-alert { background-position: 0 -144px; }
-.ui-icon-info { background-position: -16px -144px; }
-.ui-icon-notice { background-position: -32px -144px; }
-.ui-icon-help { background-position: -48px -144px; }
-.ui-icon-check { background-position: -64px -144px; }
-.ui-icon-bullet { background-position: -80px -144px; }
-.ui-icon-radio-off { background-position: -96px -144px; }
-.ui-icon-radio-on { background-position: -112px -144px; }
-.ui-icon-pin-w { background-position: -128px -144px; }
-.ui-icon-pin-s { background-position: -144px -144px; }
-.ui-icon-play { background-position: 0 -160px; }
-.ui-icon-pause { background-position: -16px -160px; }
-.ui-icon-seek-next { background-position: -32px -160px; }
-.ui-icon-seek-prev { background-position: -48px -160px; }
-.ui-icon-seek-end { background-position: -64px -160px; }
-.ui-icon-seek-start { background-position: -80px -160px; }
-/* ui-icon-seek-first is deprecated, use ui-icon-seek-start instead */
-.ui-icon-seek-first { background-position: -80px -160px; }
-.ui-icon-stop { background-position: -96px -160px; }
-.ui-icon-eject { background-position: -112px -160px; }
-.ui-icon-volume-off { background-position: -128px -160px; }
-.ui-icon-volume-on { background-position: -144px -160px; }
-.ui-icon-power { background-position: 0 -176px; }
-.ui-icon-signal-diag { background-position: -16px -176px; }
-.ui-icon-signal { background-position: -32px -176px; }
-.ui-icon-battery-0 { background-position: -48px -176px; }
-.ui-icon-battery-1 { background-position: -64px -176px; }
-.ui-icon-battery-2 { background-position: -80px -176px; }
-.ui-icon-battery-3 { background-position: -96px -176px; }
-.ui-icon-circle-plus { background-position: 0 -192px; }
-.ui-icon-circle-minus { background-position: -16px -192px; }
-.ui-icon-circle-close { background-position: -32px -192px; }
-.ui-icon-circle-triangle-e { background-position: -48px -192px; }
-.ui-icon-circle-triangle-s { background-position: -64px -192px; }
-.ui-icon-circle-triangle-w { background-position: -80px -192px; }
-.ui-icon-circle-triangle-n { background-position: -96px -192px; }
-.ui-icon-circle-arrow-e { background-position: -112px -192px; }
-.ui-icon-circle-arrow-s { background-position: -128px -192px; }
-.ui-icon-circle-arrow-w { background-position: -144px -192px; }
-.ui-icon-circle-arrow-n { background-position: -160px -192px; }
-.ui-icon-circle-zoomin { background-position: -176px -192px; }
-.ui-icon-circle-zoomout { background-position: -192px -192px; }
-.ui-icon-circle-check { background-position: -208px -192px; }
-.ui-icon-circlesmall-plus { background-position: 0 -208px; }
-.ui-icon-circlesmall-minus { background-position: -16px -208px; }
-.ui-icon-circlesmall-close { background-position: -32px -208px; }
-.ui-icon-squaresmall-plus { background-position: -48px -208px; }
-.ui-icon-squaresmall-minus { background-position: -64px -208px; }
-.ui-icon-squaresmall-close { background-position: -80px -208px; }
-.ui-icon-grip-dotted-vertical { background-position: 0 -224px; }
-.ui-icon-grip-dotted-horizontal { background-position: -16px -224px; }
-.ui-icon-grip-solid-vertical { background-position: -32px -224px; }
-.ui-icon-grip-solid-horizontal { background-position: -48px -224px; }
-.ui-icon-gripsmall-diagonal-se { background-position: -64px -224px; }
-.ui-icon-grip-diagonal-se { background-position: -80px -224px; }
-
-
-/* Misc visuals
-----------------------------------*/
-
-/* Corner radius */
-.ui-corner-tl { -moz-border-radius-topleft: 4px; -webkit-border-top-left-radius: 4px; border-top-left-radius: 4px; }
-.ui-corner-tr { -moz-border-radius-topright: 4px; -webkit-border-top-right-radius: 4px; border-top-right-radius: 4px; }
-.ui-corner-bl { -moz-border-radius-bottomleft: 4px; -webkit-border-bottom-left-radius: 4px; border-bottom-left-radius: 4px; }
-.ui-corner-br { -moz-border-radius-bottomright: 4px; -webkit-border-bottom-right-radius: 4px; border-bottom-right-radius: 4px; }
-.ui-corner-top { -moz-border-radius-topleft: 4px; -webkit-border-top-left-radius: 4px; border-top-left-radius: 4px; -moz-border-radius-topright: 4px; -webkit-border-top-right-radius: 4px; border-top-right-radius: 4px; }
-.ui-corner-bottom { -moz-border-radius-bottomleft: 4px; -webkit-border-bottom-left-radius: 4px; border-bottom-left-radius: 4px; -moz-border-radius-bottomright: 4px; -webkit-border-bottom-right-radius: 4px; border-bottom-right-radius: 4px; }
-.ui-corner-right {  -moz-border-radius-topright: 4px; -webkit-border-top-right-radius: 4px; border-top-right-radius: 4px; -moz-border-radius-bottomright: 4px; -webkit-border-bottom-right-radius: 4px; border-bottom-right-radius: 4px; }
-.ui-corner-left { -moz-border-radius-topleft: 4px; -webkit-border-top-left-radius: 4px; border-top-left-radius: 4px; -moz-border-radius-bottomleft: 4px; -webkit-border-bottom-left-radius: 4px; border-bottom-left-radius: 4px; }
-.ui-corner-all { -moz-border-radius: 4px; -webkit-border-radius: 4px; border-radius: 4px; }
-
-/* Overlays */
-.ui-widget-overlay { background: #aaaaaa url(images/ui-bg_flat_0_aaaaaa_40x100.png) 50% 50% repeat-x; opacity: .30;filter:Alpha(Opacity=30); }
-.ui-widget-shadow { margin: -8px 0 0 -8px; padding: 8px; background: #aaaaaa url(images/ui-bg_flat_0_aaaaaa_40x100.png) 50% 50% repeat-x; opacity: .30;filter:Alpha(Opacity=30); -moz-border-radius: 8px; -webkit-border-radius: 8px; border-radius: 8px; }/*
- * jQuery UI Resizable @VERSION
- *
- * Copyright 2010, AUTHORS.txt (http://jqueryui.com/about)
- * Dual licensed under the MIT or GPL Version 2 licenses.
- * http://jquery.org/license
- *
- * http://docs.jquery.com/UI/Resizable#theming
- */
-.ui-resizable { position: relative;}
-.ui-resizable-handle { position: absolute;font-size: 0.1px;z-index: 99999; display: block;}
-.ui-resizable-disabled .ui-resizable-handle, .ui-resizable-autohide .ui-resizable-handle { display: none; }
-.ui-resizable-n { cursor: n-resize; height: 7px; width: 100%; top: -5px; left: 0; }
-.ui-resizable-s { cursor: s-resize; height: 7px; width: 100%; bottom: -5px; left: 0; }
-.ui-resizable-e { cursor: e-resize; width: 7px; right: -5px; top: 0; height: 100%; }
-.ui-resizable-w { cursor: w-resize; width: 7px; left: -5px; top: 0; height: 100%; }
-.ui-resizable-se { cursor: se-resize; width: 12px; height: 12px; right: 1px; bottom: 1px; }
-.ui-resizable-sw { cursor: sw-resize; width: 9px; height: 9px; left: -5px; bottom: -5px; }
-.ui-resizable-nw { cursor: nw-resize; width: 9px; height: 9px; left: -5px; top: -5px; }
-.ui-resizable-ne { cursor: ne-resize; width: 9px; height: 9px; right: -5px; top: -5px;}/*
- * jQuery UI Selectable @VERSION
- *
- * Copyright 2010, AUTHORS.txt (http://jqueryui.com/about)
- * Dual licensed under the MIT or GPL Version 2 licenses.
- * http://jquery.org/license
- *
- * http://docs.jquery.com/UI/Selectable#theming
- */
-.ui-selectable-helper { position: absolute; z-index: 100; border:1px dotted black; }
-/*
- * jQuery UI Accordion @VERSION
- *
- * Copyright 2010, AUTHORS.txt (http://jqueryui.com/about)
- * Dual licensed under the MIT or GPL Version 2 licenses.
- * http://jquery.org/license
- *
- * http://docs.jquery.com/UI/Accordion#theming
- */
-/* IE/Win - Fix animation bug - #4615 */
-.ui-accordion { width: 100%; }
-.ui-accordion .ui-accordion-header { cursor: pointer; position: relative; margin-top: 1px; zoom: 1; }
-.ui-accordion .ui-accordion-li-fix { display: inline; }
-.ui-accordion .ui-accordion-header-active { border-bottom: 0 !important; }
-.ui-accordion .ui-accordion-header a { display: block; font-size: 1em; padding: .5em .5em .5em .7em; }
-.ui-accordion-icons .ui-accordion-header a { padding-left: 2.2em; }
-.ui-accordion .ui-accordion-header .ui-icon { position: absolute; left: .5em; top: 50%; margin-top: -8px; }
-.ui-accordion .ui-accordion-content { padding: 1em 2.2em; border-top: 0; margin-top: -2px; position: relative; top: 1px; margin-bottom: 2px; overflow: auto; display: none; zoom: 1; }
-.ui-accordion .ui-accordion-content-active { display: block; }/*
- * jQuery UI Autocomplete @VERSION
- *
- * Copyright 2010, AUTHORS.txt (http://jqueryui.com/about)
- * Dual licensed under the MIT or GPL Version 2 licenses.
- * http://jquery.org/license
- *
- * http://docs.jquery.com/UI/Autocomplete#theming
- */
-.ui-autocomplete { position: absolute; cursor: default; }	
-
-/* workarounds */
-* html .ui-autocomplete { width:1px; } /* without this, the menu expands to 100% in IE6 */
-
-/*
- * jQuery UI Menu @VERSION
- *
- * Copyright 2010, AUTHORS.txt (http://jqueryui.com/about)
- * Dual licensed under the MIT or GPL Version 2 licenses.
- * http://jquery.org/license
- *
- * http://docs.jquery.com/UI/Menu#theming
- */
-.ui-menu {
-	list-style:none;
-	padding: 2px;
-	margin: 0;
-	display:block;
-	float: left;
-}
-.ui-menu .ui-menu {
-	margin-top: -3px;
-}
-.ui-menu .ui-menu-item {
-	margin:0;
-	padding: 0;
-	zoom: 1;
-	float: left;
-	clear: left;
-	width: 100%;
-}
-.ui-menu .ui-menu-item a {
-	text-decoration:none;
-	display:block;
-	padding:.2em .4em;
-	line-height:1.5;
-	zoom:1;
-}
-.ui-menu .ui-menu-item a.ui-state-hover,
-.ui-menu .ui-menu-item a.ui-state-active {
-	font-weight: normal;
-	margin: -1px;
-}
-/*
- * jQuery UI Button @VERSION
- *
- * Copyright 2010, AUTHORS.txt (http://jqueryui.com/about)
- * Dual licensed under the MIT or GPL Version 2 licenses.
- * http://jquery.org/license
- *
- * http://docs.jquery.com/UI/Button#theming
- */
-.ui-button { display: inline-block; position: relative; padding: 0; margin-right: .1em; text-decoration: none !important; cursor: pointer; text-align: center; zoom: 1; overflow: visible; } /* the overflow property removes extra width in IE */
-.ui-button-icon-only { width: 2.2em; } /* to make room for the icon, a width needs to be set here */
-button.ui-button-icon-only { width: 2.4em; } /* button elements seem to need a little more width */
-.ui-button-icons-only { width: 3.4em; } 
-button.ui-button-icons-only { width: 3.7em; } 
-
-/*button text element */
-.ui-button .ui-button-text { display: block; line-height: 1.4;  }
-.ui-button-text-only .ui-button-text { padding: .4em 1em; }
-.ui-button-icon-only .ui-button-text, .ui-button-icons-only .ui-button-text { padding: .4em; text-indent: -9999999px; }
-.ui-button-text-icon-primary .ui-button-text, .ui-button-text-icons .ui-button-text { padding: .4em 1em .4em 2.1em; }
-.ui-button-text-icon-secondary .ui-button-text, .ui-button-text-icons .ui-button-text { padding: .4em 2.1em .4em 1em; }
-.ui-button-text-icons .ui-button-text { padding-left: 2.1em; padding-right: 2.1em; }
-/* no icon support for input elements, provide padding by default */
-input.ui-button { padding: .4em 1em; }
-
-/*button icon element(s) */
-.ui-button-icon-only .ui-icon, .ui-button-text-icon-primary .ui-icon, .ui-button-text-icon-secondary .ui-icon, .ui-button-text-icons .ui-icon, .ui-button-icons-only .ui-icon { position: absolute; top: 50%; margin-top: -8px; }
-.ui-button-icon-only .ui-icon { left: 50%; margin-left: -8px; }
-.ui-button-text-icon-primary .ui-button-icon-primary, .ui-button-text-icons .ui-button-icon-primary, .ui-button-icons-only .ui-button-icon-primary { left: .5em; }
-.ui-button-text-icon-secondary .ui-button-icon-secondary, .ui-button-text-icons .ui-button-icon-secondary, .ui-button-icons-only .ui-button-icon-secondary { right: .5em; }
-.ui-button-text-icons .ui-button-icon-secondary, .ui-button-icons-only .ui-button-icon-secondary { right: .5em; }
-
-/*button sets*/
-.ui-buttonset { margin-right: 7px; }
-.ui-buttonset .ui-button { margin-left: 0; margin-right: -.3em; }
-
-/* workarounds */
-button.ui-button::-moz-focus-inner { border: 0; padding: 0; } /* reset extra padding in Firefox */
-/*
- * jQuery UI Dialog @VERSION
- *
- * Copyright 2010, AUTHORS.txt (http://jqueryui.com/about)
- * Dual licensed under the MIT or GPL Version 2 licenses.
- * http://jquery.org/license
- *
- * http://docs.jquery.com/UI/Dialog#theming
- */
-.ui-dialog { position: absolute; padding: .2em; width: 300px; overflow: hidden; }
-.ui-dialog .ui-dialog-titlebar { padding: .5em 1em .3em; position: relative;  }
-.ui-dialog .ui-dialog-title { float: left; margin: .1em 16px .2em 0; } 
-.ui-dialog .ui-dialog-titlebar-close { position: absolute; right: .3em; top: 50%; width: 19px; margin: -10px 0 0 0; padding: 1px; height: 18px; }
-.ui-dialog .ui-dialog-titlebar-close span { display: block; margin: 1px; }
-.ui-dialog .ui-dialog-titlebar-close:hover, .ui-dialog .ui-dialog-titlebar-close:focus { padding: 0; }
-.ui-dialog .ui-dialog-content { position: relative; border: 0; padding: .5em 1em; background: none; overflow: auto; zoom: 1; }
-.ui-dialog .ui-dialog-buttonpane { text-align: left; border-width: 1px 0 0 0; background-image: none; margin: .5em 0 0 0; padding: .3em 1em .5em .4em; }
-.ui-dialog .ui-dialog-buttonpane .ui-dialog-buttonset { float: right; }
-.ui-dialog .ui-dialog-buttonpane button { margin: .5em .4em .5em 0; cursor: pointer; }
-.ui-dialog .ui-resizable-se { width: 14px; height: 14px; right: 3px; bottom: 3px; }
-.ui-draggable .ui-dialog-titlebar { cursor: move; }
-/*
- * jQuery UI Slider @VERSION
- *
- * Copyright 2010, AUTHORS.txt (http://jqueryui.com/about)
- * Dual licensed under the MIT or GPL Version 2 licenses.
- * http://jquery.org/license
- *
- * http://docs.jquery.com/UI/Slider#theming
- */
-.ui-slider { position: relative; text-align: left; }
-.ui-slider .ui-slider-handle { position: absolute; z-index: 2; width: 1.2em; height: 1.2em; cursor: default; }
-.ui-slider .ui-slider-range { position: absolute; z-index: 1; font-size: .7em; display: block; border: 0; background-position: 0 0; }
-
-.ui-slider-horizontal { height: .8em; }
-.ui-slider-horizontal .ui-slider-handle { top: -.3em; margin-left: -.6em; }
-.ui-slider-horizontal .ui-slider-range { top: 0; height: 100%; }
-.ui-slider-horizontal .ui-slider-range-min { left: 0; }
-.ui-slider-horizontal .ui-slider-range-max { right: 0; }
-
-.ui-slider-vertical { width: .8em; height: 100px; }
-.ui-slider-vertical .ui-slider-handle { left: -.3em; margin-left: 0; margin-bottom: -.6em; }
-.ui-slider-vertical .ui-slider-range { left: 0; width: 100%; }
-.ui-slider-vertical .ui-slider-range-min { bottom: 0; }
-.ui-slider-vertical .ui-slider-range-max { top: 0; }/*
- * jQuery UI Tabs @VERSION
- *
- * Copyright 2010, AUTHORS.txt (http://jqueryui.com/about)
- * Dual licensed under the MIT or GPL Version 2 licenses.
- * http://jquery.org/license
- *
- * http://docs.jquery.com/UI/Tabs#theming
- */
-.ui-tabs { position: relative; padding: .2em; zoom: 1; } /* position: relative prevents IE scroll bug (element with position: relative inside container with overflow: auto appear as "fixed") */
-.ui-tabs .ui-tabs-nav { margin: 0; padding: .2em .2em 0; }
-.ui-tabs .ui-tabs-nav li { list-style: none; float: left; position: relative; top: 1px; margin: 0 .2em 1px 0; border-bottom: 0 !important; padding: 0; white-space: nowrap; }
-.ui-tabs .ui-tabs-nav li a { float: left; padding: .5em 1em; text-decoration: none; }
-.ui-tabs .ui-tabs-nav li.ui-tabs-selected { margin-bottom: 0; padding-bottom: 1px; }
-.ui-tabs .ui-tabs-nav li.ui-tabs-selected a, .ui-tabs .ui-tabs-nav li.ui-state-disabled a, .ui-tabs .ui-tabs-nav li.ui-state-processing a { cursor: text; }
-.ui-tabs .ui-tabs-nav li a, .ui-tabs.ui-tabs-collapsible .ui-tabs-nav li.ui-tabs-selected a { cursor: pointer; } /* first selector in group seems obsolete, but required to overcome bug in Opera applying cursor: text overall if defined elsewhere... */
-.ui-tabs .ui-tabs-panel { display: block; border-width: 0; padding: 1em 1.4em; background: none; }
-.ui-tabs .ui-tabs-hide { display: none !important; }
-/*
- * jQuery UI Datepicker @VERSION
- *
- * Copyright 2010, AUTHORS.txt (http://jqueryui.com/about)
- * Dual licensed under the MIT or GPL Version 2 licenses.
- * http://jquery.org/license
- *
- * http://docs.jquery.com/UI/Datepicker#theming
- */
-.ui-datepicker { width: 17em; padding: .2em .2em 0; }
-.ui-datepicker .ui-datepicker-header { position:relative; padding:.2em 0; }
-.ui-datepicker .ui-datepicker-prev, .ui-datepicker .ui-datepicker-next { position:absolute; top: 2px; width: 1.8em; height: 1.8em; }
-.ui-datepicker .ui-datepicker-prev-hover, .ui-datepicker .ui-datepicker-next-hover { top: 1px; }
-.ui-datepicker .ui-datepicker-prev { left:2px; }
-.ui-datepicker .ui-datepicker-next { right:2px; }
-.ui-datepicker .ui-datepicker-prev-hover { left:1px; }
-.ui-datepicker .ui-datepicker-next-hover { right:1px; }
-.ui-datepicker .ui-datepicker-prev span, .ui-datepicker .ui-datepicker-next span { display: block; position: absolute; left: 50%; margin-left: -8px; top: 50%; margin-top: -8px;  }
-.ui-datepicker .ui-datepicker-title { margin: 0 2.3em; line-height: 1.8em; text-align: center; }
-.ui-datepicker .ui-datepicker-title select { font-size:1em; margin:1px 0; }
-.ui-datepicker select.ui-datepicker-month-year {width: 100%;}
-.ui-datepicker select.ui-datepicker-month, 
-.ui-datepicker select.ui-datepicker-year { width: 49%;}
-.ui-datepicker table {width: 100%; font-size: .9em; border-collapse: collapse; margin:0 0 .4em; }
-.ui-datepicker th { padding: .7em .3em; text-align: center; font-weight: bold; border: 0;  }
-.ui-datepicker td { border: 0; padding: 1px; }
-.ui-datepicker td span, .ui-datepicker td a { display: block; padding: .2em; text-align: right; text-decoration: none; }
-.ui-datepicker .ui-datepicker-buttonpane { background-image: none; margin: .7em 0 0 0; padding:0 .2em; border-left: 0; border-right: 0; border-bottom: 0; }
-.ui-datepicker .ui-datepicker-buttonpane button { float: right; margin: .5em .2em .4em; cursor: pointer; padding: .2em .6em .3em .6em; width:auto; overflow:visible; }
-.ui-datepicker .ui-datepicker-buttonpane button.ui-datepicker-current { float:left; }
-
-/* with multiple calendars */
-.ui-datepicker.ui-datepicker-multi { width:auto; }
-.ui-datepicker-multi .ui-datepicker-group { float:left; }
-.ui-datepicker-multi .ui-datepicker-group table { width:95%; margin:0 auto .4em; }
-.ui-datepicker-multi-2 .ui-datepicker-group { width:50%; }
-.ui-datepicker-multi-3 .ui-datepicker-group { width:33.3%; }
-.ui-datepicker-multi-4 .ui-datepicker-group { width:25%; }
-.ui-datepicker-multi .ui-datepicker-group-last .ui-datepicker-header { border-left-width:0; }
-.ui-datepicker-multi .ui-datepicker-group-middle .ui-datepicker-header { border-left-width:0; }
-.ui-datepicker-multi .ui-datepicker-buttonpane { clear:left; }
-.ui-datepicker-row-break { clear:both; width:100%; }
-
-/* RTL support */
-.ui-datepicker-rtl { direction: rtl; }
-.ui-datepicker-rtl .ui-datepicker-prev { right: 2px; left: auto; }
-.ui-datepicker-rtl .ui-datepicker-next { left: 2px; right: auto; }
-.ui-datepicker-rtl .ui-datepicker-prev:hover { right: 1px; left: auto; }
-.ui-datepicker-rtl .ui-datepicker-next:hover { left: 1px; right: auto; }
-.ui-datepicker-rtl .ui-datepicker-buttonpane { clear:right; }
-.ui-datepicker-rtl .ui-datepicker-buttonpane button { float: left; }
-.ui-datepicker-rtl .ui-datepicker-buttonpane button.ui-datepicker-current { float:right; }
-.ui-datepicker-rtl .ui-datepicker-group { float:right; }
-.ui-datepicker-rtl .ui-datepicker-group-last .ui-datepicker-header { border-right-width:0; border-left-width:1px; }
-.ui-datepicker-rtl .ui-datepicker-group-middle .ui-datepicker-header { border-right-width:0; border-left-width:1px; }
-
-/* IE6 IFRAME FIX (taken from datepicker 1.5.3 */
-.ui-datepicker-cover {
-    display: none; /*sorry for IE5*/
-    display/**/: block; /*sorry for IE5*/
-    position: absolute; /*must have*/
-    z-index: -1; /*must have*/
-    filter: mask(); /*must have*/
-    top: -4px; /*must have*/
-    left: -4px; /*must have*/
-    width: 200px; /*must have*/
-    height: 200px; /*must have*/
-}/*
- * jQuery UI Progressbar @VERSION
- *
- * Copyright 2010, AUTHORS.txt (http://jqueryui.com/about)
- * Dual licensed under the MIT or GPL Version 2 licenses.
- * http://jquery.org/license
- *
- * http://docs.jquery.com/UI/Progressbar#theming
- */
-.ui-progressbar { height:2em; text-align: left; }
-.ui-progressbar .ui-progressbar-value {margin: -1px; height:100%; }
\ No newline at end of file
diff --git a/lib/logstash/web/public/js/flot/API.txt b/lib/logstash/web/public/js/flot/API.txt
deleted file mode 100644
index bd0c6632e72..00000000000
--- a/lib/logstash/web/public/js/flot/API.txt
+++ /dev/null
@@ -1,1024 +0,0 @@
-Flot Reference
---------------
-
-Consider a call to the plot function:
-
-   var plot = $.plot(placeholder, data, options)
-
-The placeholder is a jQuery object or DOM element or jQuery expression
-that the plot will be put into. This placeholder needs to have its
-width and height set as explained in the README (go read that now if
-you haven't, it's short). The plot will modify some properties of the
-placeholder so it's recommended you simply pass in a div that you
-don't use for anything else. Make sure you check any fancy styling
-you apply to the div, e.g. background images have been reported to be a
-problem on IE 7.
-
-The format of the data is documented below, as is the available
-options. The "plot" object returned has some methods you can call.
-These are documented separately below.
-
-Note that in general Flot gives no guarantees if you change any of the
-objects you pass in to the plot function or get out of it since
-they're not necessarily deep-copied.
-
-
-Data Format
------------
-
-The data is an array of data series:
-
-  [ series1, series2, ... ]
-
-A series can either be raw data or an object with properties. The raw
-data format is an array of points:
-
-  [ [x1, y1], [x2, y2], ... ]
-
-E.g.
-
-  [ [1, 3], [2, 14.01], [3.5, 3.14] ]
-
-Note that to simplify the internal logic in Flot both the x and y
-values must be numbers (even if specifying time series, see below for
-how to do this). This is a common problem because you might retrieve
-data from the database and serialize them directly to JSON without
-noticing the wrong type. If you're getting mysterious errors, double
-check that you're inputting numbers and not strings.
-
-If a null is specified as a point or if one of the coordinates is null
-or couldn't be converted to a number, the point is ignored when
-drawing. As a special case, a null value for lines is interpreted as a
-line segment end, i.e. the points before and after the null value are
-not connected.
-
-Lines and points take two coordinates. For bars, you can specify a
-third coordinate which is the bottom of the bar (defaults to 0).
-
-The format of a single series object is as follows:
-
-  {
-    color: color or number
-    data: rawdata
-    label: string
-    lines: specific lines options
-    bars: specific bars options
-    points: specific points options
-    xaxis: 1 or 2
-    yaxis: 1 or 2
-    clickable: boolean
-    hoverable: boolean
-    shadowSize: number
-  }
-
-You don't have to specify any of them except the data, the rest are
-options that will get default values. Typically you'd only specify
-label and data, like this:
-
-  {
-    label: "y = 3",
-    data: [[0, 3], [10, 3]]
-  }
-
-The label is used for the legend, if you don't specify one, the series
-will not show up in the legend.
-
-If you don't specify color, the series will get a color from the
-auto-generated colors. The color is either a CSS color specification
-(like "rgb(255, 100, 123)") or an integer that specifies which of
-auto-generated colors to select, e.g. 0 will get color no. 0, etc.
-
-The latter is mostly useful if you let the user add and remove series,
-in which case you can hard-code the color index to prevent the colors
-from jumping around between the series.
-
-The "xaxis" and "yaxis" options specify which axis to use, specify 2
-to get the secondary axis (x axis at top or y axis to the right).
-E.g., you can use this to make a dual axis plot by specifying
-{ yaxis: 2 } for one data series.
-
-"clickable" and "hoverable" can be set to false to disable
-interactivity for specific series if interactivity is turned on in
-the plot, see below.
-
-The rest of the options are all documented below as they are the same
-as the default options passed in via the options parameter in the plot
-commmand. When you specify them for a specific data series, they will
-override the default options for the plot for that data series.
-
-Here's a complete example of a simple data specification:
-
-  [ { label: "Foo", data: [ [10, 1], [17, -14], [30, 5] ] },
-    { label: "Bar", data: [ [11, 13], [19, 11], [30, -7] ] } ]
-
-
-Plot Options
-------------
-
-All options are completely optional. They are documented individually
-below, to change them you just specify them in an object, e.g.
-
-  var options = {
-    series: {
-      lines: { show: true },
-      points: { show: true }
-    }
-  };
-
-  $.plot(placeholder, data, options);
-
-
-Customizing the legend
-======================
-
-  legend: {
-    show: boolean
-    labelFormatter: null or (fn: string, series object -> string)
-    labelBoxBorderColor: color
-    noColumns: number
-    position: "ne" or "nw" or "se" or "sw"
-    margin: number of pixels or [x margin, y margin]
-    backgroundColor: null or color
-    backgroundOpacity: number between 0 and 1
-    container: null or jQuery object/DOM element/jQuery expression
-  }
-
-The legend is generated as a table with the data series labels and
-small label boxes with the color of the series. If you want to format
-the labels in some way, e.g. make them to links, you can pass in a
-function for "labelFormatter". Here's an example that makes them
-clickable:
-
-  labelFormatter: function(label, series) {
-    // series is the series object for the label
-    return '<a href="#' + label + '">' + label + '</a>';
-  }
-
-"noColumns" is the number of columns to divide the legend table into.
-"position" specifies the overall placement of the legend within the
-plot (top-right, top-left, etc.) and margin the distance to the plot
-edge (this can be either a number or an array of two numbers like [x,
-y]). "backgroundColor" and "backgroundOpacity" specifies the
-background. The default is a partly transparent auto-detected
-background.
-
-If you want the legend to appear somewhere else in the DOM, you can
-specify "container" as a jQuery object/expression to put the legend
-table into. The "position" and "margin" etc. options will then be
-ignored. Note that Flot will overwrite the contents of the container.
-
-
-Customizing the axes
-====================
-
-  xaxis, yaxis, x2axis, y2axis: {
-    mode: null or "time"
-    min: null or number
-    max: null or number
-    autoscaleMargin: null or number
-    
-    labelWidth: null or number
-    labelHeight: null or number
-
-    transform: null or fn: number -> number
-    inverseTransform: null or fn: number -> number
-    
-    ticks: null or number or ticks array or (fn: range -> ticks array)
-    tickSize: number or array
-    minTickSize: number or array
-    tickFormatter: (fn: number, object -> string) or string
-    tickDecimals: null or number
-  }
-
-All axes have the same kind of options. The "mode" option
-determines how the data is interpreted, the default of null means as
-decimal numbers. Use "time" for time series data, see the next section.
-
-The options "min"/"max" are the precise minimum/maximum value on the
-scale. If you don't specify either of them, a value will automatically
-be chosen based on the minimum/maximum data values.
-
-The "autoscaleMargin" is a bit esoteric: it's the fraction of margin
-that the scaling algorithm will add to avoid that the outermost points
-ends up on the grid border. Note that this margin is only applied
-when a min or max value is not explicitly set. If a margin is
-specified, the plot will furthermore extend the axis end-point to the
-nearest whole tick. The default value is "null" for the x axis and
-0.02 for the y axis which seems appropriate for most cases.
-
-"labelWidth" and "labelHeight" specifies a fixed size of the tick
-labels in pixels. They're useful in case you need to align several
-plots.
-
-"transform" and "inverseTransform" are callbacks you can put in to
-change the way the data is drawn. You can design a function to
-compress or expand certain parts of the axis non-linearly, e.g.
-suppress weekends or compress far away points with a logarithm or some
-other means. When Flot draws the plot, each value is first put through
-the transform function. Here's an example, the x axis can be turned
-into a natural logarithm axis with the following code:
-
-  xaxis: {
-    transform: function (v) { return Math.log(v); },
-    inverseTransform: function (v) { return Math.exp(v); }
-  }
-
-Note that for finding extrema, Flot assumes that the transform
-function does not reorder values (monotonicity is assumed).
-
-The inverseTransform is simply the inverse of the transform function
-(so v == inverseTransform(transform(v)) for all relevant v). It is
-required for converting from canvas coordinates to data coordinates,
-e.g. for a mouse interaction where a certain pixel is clicked. If you
-don't use any interactive features of Flot, you may not need it.
-
-
-The rest of the options deal with the ticks.
-
-If you don't specify any ticks, a tick generator algorithm will make
-some for you. The algorithm has two passes. It first estimates how
-many ticks would be reasonable and uses this number to compute a nice
-round tick interval size. Then it generates the ticks.
-
-You can specify how many ticks the algorithm aims for by setting
-"ticks" to a number. The algorithm always tries to generate reasonably
-round tick values so even if you ask for three ticks, you might get
-five if that fits better with the rounding. If you don't want any
-ticks at all, set "ticks" to 0 or an empty array.
-
-Another option is to skip the rounding part and directly set the tick
-interval size with "tickSize". If you set it to 2, you'll get ticks at
-2, 4, 6, etc. Alternatively, you can specify that you just don't want
-ticks at a size less than a specific tick size with "minTickSize".
-Note that for time series, the format is an array like [2, "month"],
-see the next section.
-
-If you want to completely override the tick algorithm, you can specify
-an array for "ticks", either like this:
-
-  ticks: [0, 1.2, 2.4]
-
-Or like this where the labels are also customized:
-
-  ticks: [[0, "zero"], [1.2, "one mark"], [2.4, "two marks"]]
-
-You can mix the two if you like.
-  
-For extra flexibility you can specify a function as the "ticks"
-parameter. The function will be called with an object with the axis
-min and max and should return a ticks array. Here's a simplistic tick
-generator that spits out intervals of pi, suitable for use on the x
-axis for trigonometric functions:
-
-  function piTickGenerator(axis) {
-    var res = [], i = Math.floor(axis.min / Math.PI);
-    do {
-      var v = i * Math.PI;
-      res.push([v, i + "\u03c0"]);
-      ++i;
-    } while (v < axis.max);
-    
-    return res;
-  }
-
-
-You can control how the ticks look like with "tickDecimals", the
-number of decimals to display (default is auto-detected).
-
-Alternatively, for ultimate control over how ticks look like you can
-provide a function to "tickFormatter". The function is passed two
-parameters, the tick value and an "axis" object with information, and
-should return a string. The default formatter looks like this:
-
-  function formatter(val, axis) {
-    return val.toFixed(axis.tickDecimals);
-  }
-
-The axis object has "min" and "max" with the range of the axis,
-"tickDecimals" with the number of decimals to round the value to and
-"tickSize" with the size of the interval between ticks as calculated
-by the automatic axis scaling algorithm (or specified by you). Here's
-an example of a custom formatter:
-
-  function suffixFormatter(val, axis) {
-    if (val > 1000000)
-      return (val / 1000000).toFixed(axis.tickDecimals) + " MB";
-    else if (val > 1000)
-      return (val / 1000).toFixed(axis.tickDecimals) + " kB";
-    else
-      return val.toFixed(axis.tickDecimals) + " B";
-  }
-
-Time series data
-================
-
-Time series are a bit more difficult than scalar data because
-calendars don't follow a simple base 10 system. For many cases, Flot
-abstracts most of this away, but it can still be a bit difficult to
-get the data into Flot. So we'll first discuss the data format.
-
-The time series support in Flot is based on Javascript timestamps,
-i.e. everywhere a time value is expected or handed over, a Javascript
-timestamp number is used. This is a number, not a Date object. A
-Javascript timestamp is the number of milliseconds since January 1,
-1970 00:00:00 UTC. This is almost the same as Unix timestamps, except it's
-in milliseconds, so remember to multiply by 1000!
-
-You can see a timestamp like this
-
-  alert((new Date()).getTime())
-
-Normally you want the timestamps to be displayed according to a
-certain time zone, usually the time zone in which the data has been
-produced. However, Flot always displays timestamps according to UTC.
-It has to as the only alternative with core Javascript is to interpret
-the timestamps according to the time zone that the visitor is in,
-which means that the ticks will shift unpredictably with the time zone
-and daylight savings of each visitor.
-
-So given that there's no good support for custom time zones in
-Javascript, you'll have to take care of this server-side.
-
-The easiest way to think about it is to pretend that the data
-production time zone is UTC, even if it isn't. So if you have a
-datapoint at 2002-02-20 08:00, you can generate a timestamp for eight
-o'clock UTC even if it really happened eight o'clock UTC+0200.
-
-In PHP you can get an appropriate timestamp with
-'strtotime("2002-02-20 UTC") * 1000', in Python with
-'calendar.timegm(datetime_object.timetuple()) * 1000', in .NET with
-something like:
-
-  public static int GetJavascriptTimestamp(System.DateTime input)
-  {
-    System.TimeSpan span = new System.TimeSpan(System.DateTime.Parse("1/1/1970").Ticks);
-    System.DateTime time = input.Subtract(span);
-    return (long)(time.Ticks / 10000);
-  }
-
-Javascript also has some support for parsing date strings, so it is
-possible to generate the timestamps manually client-side.
-
-If you've already got the real UTC timestamp, it's too late to use the
-pretend trick described above. But you can fix up the timestamps by
-adding the time zone offset, e.g. for UTC+0200 you would add 2 hours
-to the UTC timestamp you got. Then it'll look right on the plot. Most
-programming environments have some means of getting the timezone
-offset for a specific date (note that you need to get the offset for
-each individual timestamp to account for daylight savings).
-
-Once you've gotten the timestamps into the data and specified "time"
-as the axis mode, Flot will automatically generate relevant ticks and
-format them. As always, you can tweak the ticks via the "ticks" option
-- just remember that the values should be timestamps (numbers), not
-Date objects.
-
-Tick generation and formatting can also be controlled separately
-through the following axis options:
-
-  minTickSize: array
-  timeformat: null or format string
-  monthNames: null or array of size 12 of strings
-  twelveHourClock: boolean
-
-Here "timeformat" is a format string to use. You might use it like
-this:
-
-  xaxis: {
-    mode: "time"
-    timeformat: "%y/%m/%d"
-  }
-  
-This will result in tick labels like "2000/12/24". The following
-specifiers are supported
-
-  %h: hours
-  %H: hours (left-padded with a zero)
-  %M: minutes (left-padded with a zero)
-  %S: seconds (left-padded with a zero)
-  %d: day of month (1-31)
-  %m: month (1-12)
-  %y: year (four digits)
-  %b: month name (customizable)
-  %p: am/pm, additionally switches %h/%H to 12 hour instead of 24
-  %P: AM/PM (uppercase version of %p)
-
-You can customize the month names with the "monthNames" option. For
-instance, for Danish you might specify:
-
-  monthNames: ["jan", "feb", "mar", "apr", "maj", "jun", "jul", "aug", "sep", "okt", "nov", "dec"]
-
-If you set "twelveHourClock" to true, the autogenerated timestamps
-will use 12 hour AM/PM timestamps instead of 24 hour.
-  
-The format string and month names are used by a very simple built-in
-format function that takes a date object, a format string (and
-optionally an array of month names) and returns the formatted string.
-If needed, you can access it as $.plot.formatDate(date, formatstring,
-monthNames) or even replace it with another more advanced function
-from a date library if you're feeling adventurous.
-
-If everything else fails, you can control the formatting by specifying
-a custom tick formatter function as usual. Here's a simple example
-which will format December 24 as 24/12:
-
-  tickFormatter: function (val, axis) {
-    var d = new Date(val);
-    return d.getUTCDate() + "/" + (d.getUTCMonth() + 1);
-  }
-
-Note that for the time mode "tickSize" and "minTickSize" are a bit
-special in that they are arrays on the form "[value, unit]" where unit
-is one of "second", "minute", "hour", "day", "month" and "year". So
-you can specify
-
-  minTickSize: [1, "month"]
-
-to get a tick interval size of at least 1 month and correspondingly,
-if axis.tickSize is [2, "day"] in the tick formatter, the ticks have
-been produced with two days in-between.
-
-
-
-Customizing the data series
-===========================
-
-  series: {
-    lines, points, bars: {
-      show: boolean
-      lineWidth: number
-      fill: boolean or number
-      fillColor: null or color/gradient
-    }
-
-    points: {
-      radius: number
-    }
-
-    bars: {
-      barWidth: number
-      align: "left" or "center"
-      horizontal: boolean
-    }
-
-    lines: {
-      steps: boolean
-    }
-
-    shadowSize: number
-  }
-  
-  colors: [ color1, color2, ... ]
-
-The options inside "series: {}" are copied to each of the series. So
-you can specify that all series should have bars by putting it in the
-global options, or override it for individual series by specifying
-bars in a particular the series object in the array of data.
-  
-The most important options are "lines", "points" and "bars" that
-specify whether and how lines, points and bars should be shown for
-each data series. In case you don't specify anything at all, Flot will
-default to showing lines (you can turn this off with
-lines: { show: false}). You can specify the various types
-independently of each other, and Flot will happily draw each of them
-in turn (this is probably only useful for lines and points), e.g.
-
-  var options = {
-    series: {
-      lines: { show: true, fill: true, fillColor: "rgba(255, 255, 255, 0.8)" },
-      points: { show: true, fill: false }
-    }
-  };
-
-"lineWidth" is the thickness of the line or outline in pixels. You can
-set it to 0 to prevent a line or outline from being drawn; this will
-also hide the shadow.
-
-"fill" is whether the shape should be filled. For lines, this produces
-area graphs. You can use "fillColor" to specify the color of the fill.
-If "fillColor" evaluates to false (default for everything except
-points which are filled with white), the fill color is auto-set to the
-color of the data series. You can adjust the opacity of the fill by
-setting fill to a number between 0 (fully transparent) and 1 (fully
-opaque).
-
-For bars, fillColor can be a gradient, see the gradient documentation
-below. "barWidth" is the width of the bars in units of the x axis (or
-the y axis if "horizontal" is true), contrary to most other measures
-that are specified in pixels. For instance, for time series the unit
-is milliseconds so 24 * 60 * 60 * 1000 produces bars with the width of
-a day. "align" specifies whether a bar should be left-aligned
-(default) or centered on top of the value it represents. When
-"horizontal" is on, the bars are drawn horizontally, i.e. from the y
-axis instead of the x axis; note that the bar end points are still
-defined in the same way so you'll probably want to swap the
-coordinates if you've been plotting vertical bars first.
-
-For lines, "steps" specifies whether two adjacent data points are
-connected with a straight (possibly diagonal) line or with first a
-horizontal and then a vertical line. Note that this transforms the
-data by adding extra points.
-
-"shadowSize" is the default size of shadows in pixels. Set it to 0 to
-remove shadows.
-
-The "colors" array specifies a default color theme to get colors for
-the data series from. You can specify as many colors as you like, like
-this:
-
-  colors: ["#d18b2c", "#dba255", "#919733"]
-
-If there are more data series than colors, Flot will try to generate
-extra colors by lightening and darkening colors in the theme.
-
-
-Customizing the grid
-====================
-
-  grid: {
-    show: boolean
-    aboveData: boolean
-    color: color
-    backgroundColor: color/gradient or null
-    tickColor: color
-    labelMargin: number
-    markings: array of markings or (fn: axes -> array of markings)
-    borderWidth: number
-    borderColor: color or null
-    clickable: boolean
-    hoverable: boolean
-    autoHighlight: boolean
-    mouseActiveRadius: number
-  }
-
-The grid is the thing with the axes and a number of ticks. "color" is
-the color of the grid itself whereas "backgroundColor" specifies the
-background color inside the grid area. The default value of null means
-that the background is transparent. You can also set a gradient, see
-the gradient documentation below.
-
-You can turn off the whole grid including tick labels by setting
-"show" to false. "aboveData" determines whether the grid is drawn on
-above the data or below (below is default).
-
-"tickColor" is the color of the ticks and "labelMargin" is the spacing
-between tick labels and the grid. Note that you can style the tick
-labels with CSS, e.g. to change the color. They have class "tickLabel".
-"borderWidth" is the width of the border around the plot. Set it to 0
-to disable the border. You can also set "borderColor" if you want the
-border to have a different color than the grid lines.
-
-"markings" is used to draw simple lines and rectangular areas in the
-background of the plot. You can either specify an array of ranges on
-the form { xaxis: { from, to }, yaxis: { from, to } } (secondary axis
-coordinates with x2axis/y2axis) or with a function that returns such
-an array given the axes for the plot in an object as the first
-parameter.
-
-You can set the color of markings by specifying "color" in the ranges
-object. Here's an example array:
-
-  markings: [ { xaxis: { from: 0, to: 2 }, yaxis: { from: 10, to: 10 }, color: "#bb0000" }, ... ]
-
-If you leave out one of the values, that value is assumed to go to the
-border of the plot. So for example if you only specify { xaxis: {
-from: 0, to: 2 } } it means an area that extends from the top to the
-bottom of the plot in the x range 0-2.
-
-A line is drawn if from and to are the same, e.g.
-
-  markings: [ { yaxis: { from: 1, to: 1 } }, ... ]
-
-would draw a line parallel to the x axis at y = 1. You can control the
-line width with "lineWidth" in the range object.
-
-An example function might look like this:
-
-  markings: function (axes) {
-    var markings = [];
-    for (var x = Math.floor(axes.xaxis.min); x < axes.xaxis.max; x += 2)
-      markings.push({ xaxis: { from: x, to: x + 1 } });
-    return markings;
-  }
-
-
-If you set "clickable" to true, the plot will listen for click events
-on the plot area and fire a "plotclick" event on the placeholder with
-a position and a nearby data item object as parameters. The coordinates
-are available both in the unit of the axes (not in pixels) and in
-global screen coordinates.
-
-Likewise, if you set "hoverable" to true, the plot will listen for
-mouse move events on the plot area and fire a "plothover" event with
-the same parameters as the "plotclick" event. If "autoHighlight" is
-true (the default), nearby data items are highlighted automatically.
-If needed, you can disable highlighting and control it yourself with
-the highlight/unhighlight plot methods described elsewhere.
-
-You can use "plotclick" and "plothover" events like this:
-
-    $.plot($("#placeholder"), [ d ], { grid: { clickable: true } });
-
-    $("#placeholder").bind("plotclick", function (event, pos, item) {
-        alert("You clicked at " + pos.x + ", " + pos.y);
-        // secondary axis coordinates if present are in pos.x2, pos.y2,
-        // if you need global screen coordinates, they are pos.pageX, pos.pageY
-
-        if (item) {
-          highlight(item.series, item.datapoint);
-          alert("You clicked a point!");
-        }
-    });
-
-The item object in this example is either null or a nearby object on the form:
-
-  item: {
-      datapoint: the point, e.g. [0, 2]
-      dataIndex: the index of the point in the data array
-      series: the series object
-      seriesIndex: the index of the series
-      pageX, pageY: the global screen coordinates of the point
-  }
-
-For instance, if you have specified the data like this 
-
-    $.plot($("#placeholder"), [ { label: "Foo", data: [[0, 10], [7, 3]] } ], ...);
-
-and the mouse is near the point (7, 3), "datapoint" is [7, 3],
-"dataIndex" will be 1, "series" is a normalized series object with
-among other things the "Foo" label in series.label and the color in
-series.color, and "seriesIndex" is 0. Note that plugins and options
-that transform the data can shift the indexes from what you specified
-in the original data array.
-
-If you use the above events to update some other information and want
-to clear out that info in case the mouse goes away, you'll probably
-also need to listen to "mouseout" events on the placeholder div.
-
-"mouseActiveRadius" specifies how far the mouse can be from an item
-and still activate it. If there are two or more points within this
-radius, Flot chooses the closest item. For bars, the top-most bar
-(from the latest specified data series) is chosen.
-
-If you want to disable interactivity for a specific data series, you
-can set "hoverable" and "clickable" to false in the options for that
-series, like this { data: [...], label: "Foo", clickable: false }.
-
-
-Specifying gradients
-====================
-
-A gradient is specified like this:
-
-  { colors: [ color1, color2, ... ] }
-
-For instance, you might specify a background on the grid going from
-black to gray like this:
-
-  grid: {
-    backgroundColor: { colors: ["#000", "#999"] }
-  }
-
-For the series you can specify the gradient as an object that
-specifies the scaling of the brightness and the opacity of the series
-color, e.g.
-
-  { colors: [{ opacity: 0.8 }, { brightness: 0.6, opacity: 0.8 } ] }
-
-where the first color simply has its alpha scaled, whereas the second
-is also darkened. For instance, for bars the following makes the bars
-gradually disappear, without outline:
-
-  bars: {
-      show: true,
-      lineWidth: 0,
-      fill: true,
-      fillColor: { colors: [ { opacity: 0.8 }, { opacity: 0.1 } ] }
-  }
-  
-Flot currently only supports vertical gradients drawn from top to
-bottom because that's what works with IE.
-
-
-Plot Methods
-------------
-
-The Plot object returned from the plot function has some methods you
-can call:
-
-  - highlight(series, datapoint)
-
-    Highlight a specific datapoint in the data series. You can either
-    specify the actual objects, e.g. if you got them from a
-    "plotclick" event, or you can specify the indices, e.g.
-    highlight(1, 3) to highlight the fourth point in the second series
-    (remember, zero-based indexing).
-
-  
-  - unhighlight(series, datapoint) or unhighlight()
-
-    Remove the highlighting of the point, same parameters as
-    highlight.
-
-    If you call unhighlight with no parameters, e.g. as
-    plot.unhighlight(), all current highlights are removed.
-
-
-  - setData(data)
-
-    You can use this to reset the data used. Note that axis scaling,
-    ticks, legend etc. will not be recomputed (use setupGrid() to do
-    that). You'll probably want to call draw() afterwards.
-
-    You can use this function to speed up redrawing a small plot if
-    you know that the axes won't change. Put in the new data with
-    setData(newdata), call draw(), and you're good to go. Note that
-    for large datasets, almost all the time is consumed in draw()
-    plotting the data so in this case don't bother.
-
-    
-  - setupGrid()
-
-    Recalculate and set axis scaling, ticks, legend etc.
-
-    Note that because of the drawing model of the canvas, this
-    function will immediately redraw (actually reinsert in the DOM)
-    the labels and the legend, but not the actual tick lines because
-    they're drawn on the canvas. You need to call draw() to get the
-    canvas redrawn.
-    
-  - draw()
-
-    Redraws the plot canvas.
-
-  - triggerRedrawOverlay()
-
-    Schedules an update of an overlay canvas used for drawing
-    interactive things like a selection and point highlights. This
-    is mostly useful for writing plugins. The redraw doesn't happen
-    immediately, instead a timer is set to catch multiple successive
-    redraws (e.g. from a mousemove).
-
-  - width()/height()
-
-    Gets the width and height of the plotting area inside the grid.
-    This is smaller than the canvas or placeholder dimensions as some
-    extra space is needed (e.g. for labels).
-
-  - offset()
-
-    Returns the offset of the plotting area inside the grid relative
-    to the document, useful for instance for calculating mouse
-    positions (event.pageX/Y minus this offset is the pixel position
-    inside the plot).
-
-  - pointOffset({ x: xpos, y: ypos })
-
-    Returns the calculated offset of the data point at (x, y) in data
-    space within the placeholder div. If you are working with dual axes, you
-    can specify the x and y axis references, e.g. 
-
-      o = pointOffset({ x: xpos, y: ypos, xaxis: 2, yaxis: 2 })
-      // o.left and o.top now contains the offset within the div
-  
-
-There are also some members that let you peek inside the internal
-workings of Flot which is useful in some cases. Note that if you change
-something in the objects returned, you're changing the objects used by
-Flot to keep track of its state, so be careful.
-
-  - getData()
-
-    Returns an array of the data series currently used in normalized
-    form with missing settings filled in according to the global
-    options. So for instance to find out what color Flot has assigned
-    to the data series, you could do this:
-
-      var series = plot.getData();
-      for (var i = 0; i < series.length; ++i)
-        alert(series[i].color);
-
-    A notable other interesting field besides color is datapoints
-    which has a field "points" with the normalized data points in a
-    flat array (the field "pointsize" is the increment in the flat
-    array to get to the next point so for a dataset consisting only of
-    (x,y) pairs it would be 2).
-
-  - getAxes()
-
-    Gets an object with the axes settings as { xaxis, yaxis, x2axis,
-    y2axis }.
-
-    Various things are stuffed inside an axis object, e.g. you could
-    use getAxes().xaxis.ticks to find out what the ticks are for the
-    xaxis. Two other useful attributes are p2c and c2p, functions for
-    transforming from data point space to the canvas plot space and
-    back. Both returns values that are offset with the plot offset.
- 
-  - getPlaceholder()
-
-    Returns placeholder that the plot was put into. This can be useful
-    for plugins for adding DOM elements or firing events.
-
-  - getCanvas()
-
-    Returns the canvas used for drawing in case you need to hack on it
-    yourself. You'll probably need to get the plot offset too.
-  
-  - getPlotOffset()
-
-    Gets the offset that the grid has within the canvas as an object
-    with distances from the canvas edges as "left", "right", "top",
-    "bottom". I.e., if you draw a circle on the canvas with the center
-    placed at (left, top), its center will be at the top-most, left
-    corner of the grid.
-
-  - getOptions()
-
-    Gets the options for the plot, in a normalized format with default
-    values filled in.
-    
-
-Hooks
-=====
-
-In addition to the public methods, the Plot object also has some hooks
-that can be used to modify the plotting process. You can install a
-callback function at various points in the process, the function then
-gets access to the internal data structures in Flot.
-
-Here's an overview of the phases Flot goes through:
-
-  1. Plugin initialization, parsing options
-  
-  2. Constructing the canvases used for drawing
-
-  3. Set data: parsing data specification, calculating colors,
-     copying raw data points into internal format,
-     normalizing them, finding max/min for axis auto-scaling
-
-  4. Grid setup: calculating axis spacing, ticks, inserting tick
-     labels, the legend
-
-  5. Draw: drawing the grid, drawing each of the series in turn
-
-  6. Setting up event handling for interactive features
-
-  7. Responding to events, if any
-
-Each hook is simply a function which is put in the appropriate array.
-You can add them through the "hooks" option, and they are also available
-after the plot is constructed as the "hooks" attribute on the returned
-plot object, e.g.
-
-  // define a simple draw hook
-  function hellohook(plot, canvascontext) { alert("hello!"); };
-
-  // pass it in, in an array since we might want to specify several
-  var plot = $.plot(placeholder, data, { hooks: { draw: [hellohook] } });
-
-  // we can now find it again in plot.hooks.draw[0] unless a plugin
-  // has added other hooks
-
-The available hooks are described below. All hook callbacks get the
-plot object as first parameter. You can find some examples of defined
-hooks in the plugins bundled with Flot.
-
- - processOptions  [phase 1]
-
-   function(plot, options)
-   
-   Called after Flot has parsed and merged options. Useful in the
-   instance where customizations beyond simple merging of default
-   values is needed. A plugin might use it to detect that it has been
-   enabled and then turn on or off other options.
-
- 
- - processRawData  [phase 3]
-
-   function(plot, series, data, datapoints)
- 
-   Called before Flot copies and normalizes the raw data for the given
-   series. If the function fills in datapoints.points with normalized
-   points and sets datapoints.pointsize to the size of the points,
-   Flot will skip the copying/normalization step for this series.
-   
-   In any case, you might be interested in setting datapoints.format,
-   an array of objects for specifying how a point is normalized and
-   how it interferes with axis scaling.
-
-   The default format array for points is something along the lines of:
-
-     [
-       { x: true, number: true, required: true },
-       { y: true, number: true, required: true }
-     ]
-
-   The first object means that for the first coordinate it should be
-   taken into account when scaling the x axis, that it must be a
-   number, and that it is required - so if it is null or cannot be
-   converted to a number, the whole point will be zeroed out with
-   nulls. Beyond these you can also specify "defaultValue", a value to
-   use if the coordinate is null. This is for instance handy for bars
-   where one can omit the third coordinate (the bottom of the bar)
-   which then defaults to 0.
-
-
- - processDatapoints  [phase 3]
-
-   function(plot, series, datapoints)
- 
-   Called after normalization of the given series but before finding
-   min/max of the data points. This hook is useful for implementing data
-   transformations. "datapoints" contains the normalized data points in
-   a flat array as datapoints.points with the size of a single point
-   given in datapoints.pointsize. Here's a simple transform that
-   multiplies all y coordinates by 2:
-
-     function multiply(plot, series, datapoints) {
-         var points = datapoints.points, ps = datapoints.pointsize;
-         for (var i = 0; i < points.length; i += ps)
-             points[i + 1] *= 2;
-     }
-
-   Note that you must leave datapoints in a good condition as Flot
-   doesn't check it or do any normalization on it afterwards.
-
-
- - draw  [phase 5]
-
-   function(plot, canvascontext)
- 
-   Hook for drawing on the canvas. Called after the grid is drawn
-   (unless it's disabled) and the series have been plotted (in case
-   any points, lines or bars have been turned on). For examples of how
-   to draw things, look at the source code.
-   
- 
- - bindEvents  [phase 6]
-
-   function(plot, eventHolder)
-
-   Called after Flot has setup its event handlers. Should set any
-   necessary event handlers on eventHolder, a jQuery object with the
-   canvas, e.g.
-
-     function (plot, eventHolder) {
-         eventHolder.mousedown(function (e) {
-             alert("You pressed the mouse at " + e.pageX + " " + e.pageY);
-         });
-     }
-
-   Interesting events include click, mousemove, mouseup/down. You can
-   use all jQuery events. Usually, the event handlers will update the
-   state by drawing something (add a drawOverlay hook and call
-   triggerRedrawOverlay) or firing an externally visible event for
-   user code. See the crosshair plugin for an example.
-     
-   Currently, eventHolder actually contains both the static canvas
-   used for the plot itself and the overlay canvas used for
-   interactive features because some versions of IE get the stacking
-   order wrong. The hook only gets one event, though (either for the
-   overlay or for the static canvas).
-
-
- - drawOverlay  [phase 7]
-
-   function (plot, canvascontext)
-
-   The drawOverlay hook is used for interactive things that need a
-   canvas to draw on. The model currently used by Flot works the way
-   that an extra overlay canvas is positioned on top of the static
-   canvas. This overlay is cleared and then completely redrawn
-   whenever something interesting happens. This hook is called when
-   the overlay canvas is to be redrawn.
-
-   "canvascontext" is the 2D context of the overlay canvas. You can
-   use this to draw things. You'll most likely need some of the
-   metrics computed by Flot, e.g. plot.width()/plot.height(). See the
-   crosshair plugin for an example.
-
-
-   
-Plugins
--------
-
-Plugins extend the functionality of Flot. To use a plugin, simply
-include its Javascript file after Flot in the HTML page.
-
-If you're worried about download size/latency, you can concatenate all
-the plugins you use, and Flot itself for that matter, into one big file
-(make sure you get the order right), then optionally run it through a
-Javascript minifier such as YUI Compressor.
-
-Here's a brief explanation of how the plugin plumbings work:
-
-Each plugin registers itself in the global array $.plot.plugins. When
-you make a new plot object with $.plot, Flot goes through this array
-calling the "init" function of each plugin and merging default options
-from its "option" attribute. The init function gets a reference to the
-plot object created and uses this to register hooks and add new public
-methods if needed.
-
-See the PLUGINS.txt file for details on how to write a plugin. As the
-above description hints, it's actually pretty easy.
diff --git a/lib/logstash/web/public/js/flot/FAQ.txt b/lib/logstash/web/public/js/flot/FAQ.txt
deleted file mode 100644
index ee481246984..00000000000
--- a/lib/logstash/web/public/js/flot/FAQ.txt
+++ /dev/null
@@ -1,71 +0,0 @@
-Frequently asked questions
---------------------------
-
-Q: How much data can Flot cope with?
-
-A: Flot will happily draw everything you send to it so the answer
-depends on the browser. The excanvas emulation used for IE (built with
-VML) makes IE by far the slowest browser so be sure to test with that
-if IE users are in your target group.
-
-1000 points is not a problem, but as soon as you start having more
-points than the pixel width, you should probably start thinking about
-downsampling/aggregation as this is near the resolution limit of the
-chart anyway. If you downsample server-side, you also save bandwidth.
-
-
-Q: Flot isn't working when I'm using JSON data as source!
-
-A: Actually, Flot loves JSON data, you just got the format wrong.
-Double check that you're not inputting strings instead of numbers,
-like [["0", "-2.13"], ["5", "4.3"]]. This is most common mistake, and
-the error might not show up immediately because Javascript can do some
-conversion automatically.
-
-
-Q: Can I export the graph?
-
-A: This is a limitation of the canvas technology. There's a hook in
-the canvas object for getting an image out, but you won't get the tick
-labels. And it's not likely to be supported by IE. At this point, your
-best bet is probably taking a screenshot, e.g. with PrtScn.
-
-
-Q: The bars are all tiny in time mode?
-
-A: It's not really possible to determine the bar width automatically.
-So you have to set the width with the barWidth option which is NOT in
-pixels, but in the units of the x axis (or the y axis for horizontal
-bars). For time mode that's milliseconds so the default value of 1
-makes the bars 1 millisecond wide.
-
-
-Q: Can I use Flot with libraries like Mootools or Prototype?
-
-A: Yes, Flot supports it out of the box and it's easy! Just use jQuery
-instead of $, e.g. call jQuery.plot instead of $.plot and use
-jQuery(something) instead of $(something). As a convenience, you can
-put in a DOM element for the graph placeholder where the examples and
-the API documentation are using jQuery objects.
-
-Depending on how you include jQuery, you may have to add one line of
-code to prevent jQuery from overwriting functions from the other
-libraries, see the documentation in jQuery ("Using jQuery with other
-libraries") for details.
-
-
-Q: Flot doesn't work with [widget framework xyz]!
-
-A: The problem is most likely within the framework, or your use of the
-framework.
-
-The only non-standard thing used by Flot is the canvas tag; otherwise
-it is simply a series of absolute positioned divs within the
-placeholder tag you put in. If this is not working, it's probably
-because the framework you're using is doing something weird with the
-DOM. As a last resort, you might try replotting and see if it helps.
-
-If you find there's a specific thing we can do to Flot to help, feel
-free to submit a bug report. Otherwise, you're welcome to ask for help
-on the mailing list, but please don't submit a bug report to Flot -
-try the framework instead.
diff --git a/lib/logstash/web/public/js/flot/LICENSE.txt b/lib/logstash/web/public/js/flot/LICENSE.txt
deleted file mode 100644
index 07d5b2094d1..00000000000
--- a/lib/logstash/web/public/js/flot/LICENSE.txt
+++ /dev/null
@@ -1,22 +0,0 @@
-Copyright (c) 2007-2009 IOLA and Ole Laursen
-
-Permission is hereby granted, free of charge, to any person
-obtaining a copy of this software and associated documentation
-files (the "Software"), to deal in the Software without
-restriction, including without limitation the rights to use,
-copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the
-Software is furnished to do so, subject to the following
-conditions:
-
-The above copyright notice and this permission notice shall be
-included in all copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
-EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
-OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
-NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
-HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
-WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
-FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
-OTHER DEALINGS IN THE SOFTWARE.
diff --git a/lib/logstash/web/public/js/flot/Makefile b/lib/logstash/web/public/js/flot/Makefile
deleted file mode 100644
index f90a9695561..00000000000
--- a/lib/logstash/web/public/js/flot/Makefile
+++ /dev/null
@@ -1,15 +0,0 @@
-# Makefile for generating minified files
-
-YUICOMPRESSOR_PATH=../yuicompressor-2.3.5.jar
-
-# if you need another compressor path, just copy the above line to a
-# file called Makefile.local, customize it and you're good to go
--include Makefile.local
-
-.PHONY: all
-
-# we cheat and process all .js files instead of listing them
-all: $(patsubst %.js,%.min.js,$(filter-out %.min.js,$(wildcard *.js)))
-
-%.min.js: %.js
-	java -jar $(YUICOMPRESSOR_PATH) $< -o $@
diff --git a/lib/logstash/web/public/js/flot/NEWS.txt b/lib/logstash/web/public/js/flot/NEWS.txt
deleted file mode 100644
index 53281c50fac..00000000000
--- a/lib/logstash/web/public/js/flot/NEWS.txt
+++ /dev/null
@@ -1,340 +0,0 @@
-Flot 0.6
---------
-
-API changes:
-
-1. Selection support has been moved to a plugin. Thus if you're
-passing selection: { mode: something }, you MUST include the file
-jquery.flot.selection.js after jquery.flot.js. This reduces the size
-of base Flot and makes it easier to customize the selection as well as
-improving code clarity. The change is based on patch from andershol.
-
-2. In the global options specified in the $.plot command,
-"lines", "points", "bars" and "shadowSize" have been moved to a
-sub-object called "series", i.e.
-
-  $.plot(placeholder, data, { lines: { show: true }})
-
-should be changed to
-
-  $.plot(placeholder, data, { series: { lines: { show: true }}})
-
-All future series-specific options will go into this sub-object to
-simplify plugin writing. Backward-compatibility code is in place, so
-old code should not break.
-
-3. "plothover" no longer provides the original data point, but instead
-a normalized one, since there may be no corresponding original point.
-
-4. Due to a bug in previous versions of jQuery, you now need at least
-jQuery 1.2.6. But if you can, try jQuery 1.3.2 as it got some
-improvements in event handling speed.
-
-
-Changes:
-
-- Added support for disabling interactivity for specific data series
-  (request from Ronald Schouten and Steve Upton).
-
-- Flot now calls $() on the placeholder and optional legend container
-  passed in so you can specify DOM elements or CSS expressions to make
-  it easier to use Flot with libraries like Prototype or Mootools or
-  through raw JSON from Ajax responses.
-
-- A new "plotselecting" event is now emitted while the user is making
-  a selection.
-
-- The "plothover" event is now emitted immediately instead of at most
-  10 times per second, you'll have to put in a setTimeout yourself if
-  you're doing something really expensive on this event.
-
-- The built-in date formatter can now be accessed as
-  $.plot.formatDate(...) (suggestion by Matt Manela) and even
-  replaced.
-
-- Added "borderColor" option to the grid (patch from Amaury Chamayou
-  and patch from Mike R. Williamson).
-
-- Added support for gradient backgrounds for the grid, take a look at
-  the "setting options" example (based on patch from Amaury Chamayou,
-  issue 90).
-
-- Gradient bars (suggestion by stefpet).
-  
-- Added a "plotunselected" event which is triggered when the selection
-  is removed, see "selection" example (suggestion by Meda Ugo);
-
-- The option legend.margin can now specify horizontal and vertical
-  margins independently (suggestion by someone who's annoyed).
-
-- Data passed into Flot is now copied to a new canonical format to
-  enable further processing before it hits the drawing routines. As a
-  side-effect, this should make Flot more robust in the face of bad
-  data (and fixes issue 112).
-
-- Step-wise charting: line charts have a new option "steps" that when
-  set to true connects the points with horizontal/vertical steps
-  instead of diagonal lines.
-
-- The legend labelFormatter now passes the series in addition to just
-  the label (suggestion by Vincent Lemeltier).
-
-- Horizontal bars (based on patch by Jason LeBrun).
-
-- Support for partial bars by specifying a third coordinate, i.e. they
-  don't have to start from the axis. This can be used to make stacked
-  bars.
-
-- New option to disable the (grid.show).
-
-- Added pointOffset method for converting a point in data space to an
-  offset within the placeholder.
-  
-- Plugin system: register an init method in the $.flot.plugins array
-  to get started, see PLUGINS.txt for details on how to write plugins
-  (it's easy). There are also some extra methods to enable access to
-  internal state.
-
-- Hooks: you can register functions that are called while Flot is
-  crunching the data and doing the plot. This can be used to modify
-  Flot without changing the source, useful for writing plugins. Some
-  hooks are defined, more are likely to come.
-  
-- Threshold plugin: you can set a threshold and a color, and the data
-  points below that threshold will then get the color. Useful for
-  marking data below 0, for instance.
-
-- Stack plugin: you can specify a stack key for each series to have
-  them summed. This is useful for drawing additive/cumulative graphs
-  with bars and (currently unfilled) lines.
-
-- Crosshairs plugin: trace the mouse position on the axes, enable with
-  crosshair: { mode: "x"} (see the new tracking example for a use).
-
-- Image plugin: plot prerendered images.
-
-- Navigation plugin for panning and zooming a plot.
-
-- More configurable grid.
-
-- Axis transformation support, useful for non-linear plots, e.g. log
-  axes and compressed time axes (like omitting weekends).
-
-- Support for twelve-hour date formatting (patch by Forrest Aldridge).
-
-- The color parsing code in Flot has been cleaned up and split out so
-  it's now available as a separate jQuery plugin. It's included inline
-  in the Flot source to make dependency managing easier. This also
-  makes it really easy to use the color helpers in Flot plugins.
-
-Bug fixes:
-
-- Fixed two corner-case bugs when drawing filled curves (report and
-  analysis by Joshua Varner).
-- Fix auto-adjustment code when setting min to 0 for an axis where the
-  dataset is completely flat on that axis (report by chovy).
-- Fixed a bug with passing in data from getData to setData when the
-  secondary axes are used (issue 65, reported by nperelman).
-- Fixed so that it is possible to turn lines off when no other chart
-  type is shown (based on problem reported by Glenn Vanderburg), and
-  fixed so that setting lineWidth to 0 also hides the shadow (based on
-  problem reported by Sergio Nunes).
-- Updated mousemove position expression to the latest from jQuery (bug
-  reported by meyuchas).
-- Use CSS borders instead of background in legend (fix printing issue 25
-  and 45).
-- Explicitly convert axis min/max to numbers.
-- Fixed a bug with drawing marking lines with different colors
-  (reported by Khurram).
-- Fixed a bug with returning y2 values in the selection event (fix
-  by exists, issue 75).
-- Only set position relative on placeholder if it hasn't already a
-  position different from static (reported by kyberneticist, issue 95).
-- Don't round markings to prevent sub-pixel problems (reported by Dan
-  Lipsitt).
-- Make the grid border act similarly to a regular CSS border, i.e.
-  prevent it from overlapping the plot itself. This also fixes a
-  problem with anti-aliasing when the width is 1 pixel (reported by
-  Anthony Ettinger).
-- Imported version 3 of excanvas and fixed two issues with the newer
-  version. Hopefully, this will make Flot work with IE8 (nudge by
-  Fabien Menager, further analysis by Booink, issue 133).
-- Changed the shadow code for lines to hopefully look a bit better
-  with vertical lines.
-- Round tick positions to avoid possible problems with fractions
-  (suggestion by Fred, issue 130).
-- Made the heuristic for determining how many ticks to aim for a bit
-  smarter.
-- Fix for uneven axis margins (report and patch by Paul Kienzle) and
-  snapping to ticks (concurrent report and patch by lifthrasiir).
-- Fixed bug with slicing in findNearbyItems (patch by zollman).
-- Make heuristic for x axis label widths more dynamic (patch by
-  rickinhethuis).
-- Make sure points on top take precedence when finding nearby points
-  when hovering (reported by didroe, issue 224).
-
-Flot 0.5
---------
-
-Backwards API change summary: Timestamps are now in UTC. Also
-"selected" event -> becomes "plotselected" with new data, the
-parameters for setSelection are now different (but backwards
-compatibility hooks are in place), coloredAreas becomes markings with
-a new interface (but backwards compatibility hooks are in place).
-
-
-Interactivity: added a new "plothover" event and this and the
-"plotclick" event now returns the closest data item (based on patch by
-/david, patch by Mark Byers for bar support). See the revamped
-"interacting with the data" example for some hints on what you can do.
-
-Highlighting: you can now highlight points and datapoints are
-autohighlighted when you hover over them (if hovering is turned on).
-
-Support for dual axis has been added (based on patch by someone who's
-annoyed and /david). For each data series you can specify which axes
-it belongs to, and there are two more axes, x2axis and y2axis, to
-customize. This affects the "selected" event which has been renamed to
-"plotselected" and spews out { xaxis: { from: -10, to: 20 } ... },
-setSelection in which the parameters are on a new form (backwards
-compatible hooks are in place so old code shouldn't break) and
-markings (formerly coloredAreas).
-
-Timestamps in time mode are now displayed according to
-UTC instead of the time zone of the visitor. This affects the way the
-timestamps should be input; you'll probably have to offset the
-timestamps according to your local time zone. It also affects any
-custom date handling code (which basically now should use the
-equivalent UTC date mehods, e.g. .setUTCMonth() instead of
-.setMonth().
-
-Added support for specifying the size of tick labels (axis.labelWidth,
-axis.labelHeight). Useful for specifying a max label size to keep
-multiple plots aligned.
-
-Markings, previously coloredAreas, are now specified as ranges on the
-axes, like { xaxis: { from: 0, to: 10 }}. Furthermore with markings
-you can now draw horizontal/vertical lines by setting from and to to
-the same coordinate (idea from line support patch by by Ryan Funduk).
-
-The "fill" option can now be a number that specifies the opacity of
-the fill.
-
-You can now specify a coordinate as null (like [2, null]) and Flot
-will take the other coordinate into account when scaling the axes
-(based on patch by joebno).
-
-New option for bars "align". Set it to "center" to center the bars on
-the value they represent.
-
-setSelection now takes a second parameter which you can use to prevent
-the method from firing the "plotselected" handler. 
-
-Using the "container" option in legend now overwrites the container
-element instead of just appending to it (fixes infinite legend bug,
-reported by several people, fix by Brad Dewey).
-
-Fixed a bug in calculating spacing around the plot (reported by
-timothytoe). Fixed a bug in finding max values for all-negative data
-sets. Prevent the possibility of eternal looping in tick calculations.
-Fixed a bug when borderWidth is set to 0 (reported by
-Rob/sanchothefat). Fixed a bug with drawing bars extending below 0
-(reported by James Hewitt, patch by Ryan Funduk). Fixed a
-bug with line widths of bars (reported by MikeM). Fixed a bug with
-'nw' and 'sw' legend positions. Improved the handling of axis
-auto-scaling with bars. Fixed a bug with multi-line x-axis tick
-labels (reported by Luca Ciano). IE-fix help by Savage Zhang.
-
-
-Flot 0.4
---------
-
-API changes: deprecated axis.noTicks in favor of just specifying the
-number as axis.ticks. So "xaxis: { noTicks: 10 }" becomes
-"xaxis: { ticks: 10 }"
-
-Time series support. Specify axis.mode: "time", put in Javascript
-timestamps as data, and Flot will automatically spit out sensible
-ticks. Take a look at the two new examples. The format can be
-customized with axis.timeformat and axis.monthNames, or if that fails
-with axis.tickFormatter.
-
-Support for colored background areas via grid.coloredAreas. Specify an
-array of { x1, y1, x2, y2 } objects or a function that returns these
-given { xmin, xmax, ymin, ymax }.
-
-More members on the plot object (report by Chris Davies and others).
-"getData" for inspecting the assigned settings on data series (e.g.
-color) and "setData", "setupGrid" and "draw" for updating the contents
-without a total replot.
-
-The default number of ticks to aim for is now dependent on the size of
-the plot in pixels. Support for customizing tick interval sizes
-directly with axis.minTickSize and axis.tickSize.
-
-Cleaned up the automatic axis scaling algorithm and fixed how it
-interacts with ticks. Also fixed a couple of tick-related corner case
-bugs (one reported by mainstreetmark, another reported by timothytoe).
-
-The option axis.tickFormatter now takes a function with two
-parameters, the second parameter is an optional object with
-information about the axis. It has min, max, tickDecimals, tickSize.
-
-Added support for segmented lines (based on patch from Michael
-MacDonald) and for ignoring null and bad values (suggestion from Nick
-Konidaris and joshwaihi). 
-
-Added support for changing the border width (joebno and safoo).
-Label colors can be changed via CSS by selecting the tickLabel class.
-
-Fixed a bug in handling single-item bar series (reported by Emil
-Filipov). Fixed erratic behaviour when interacting with the plot
-with IE 7 (reported by Lau Bech Lauritzen). Prevent IE/Safari text
-selection when selecting stuff on the canvas.
-
-
-
-Flot 0.3
---------
-
-This is mostly a quick-fix release because jquery.js wasn't included
-in the previous zip/tarball.
-
-Support clicking on the plot. Turn it on with grid: { clickable: true },
-then you get a "plotclick" event on the graph placeholder with the
-position in units of the plot.
-
-Fixed a bug in dealing with data where min = max, thanks to Michael
-Messinides.
-
-Include jquery.js in the zip/tarball.
-
-
-Flot 0.2
---------
-
-Added support for putting a background behind the default legend. The
-default is the partly transparent background color. Added
-backgroundColor and backgroundOpacity to the legend options to control
-this.
-
-The ticks options can now be a callback function that takes one
-parameter, an object with the attributes min and max. The function
-should return a ticks array.
-
-Added labelFormatter option in legend, useful for turning the legend
-labels into links.
-
-Fixed a couple of bugs.
-
-The API should now be fully documented.
-
-Patch from Guy Fraser to make parts of the code smaller.
-
-API changes: Moved labelMargin option to grid from x/yaxis.
-
-
-Flot 0.1
---------
-
-First public release.
diff --git a/lib/logstash/web/public/js/flot/PLUGINS.txt b/lib/logstash/web/public/js/flot/PLUGINS.txt
deleted file mode 100644
index 00bf2e56894..00000000000
--- a/lib/logstash/web/public/js/flot/PLUGINS.txt
+++ /dev/null
@@ -1,105 +0,0 @@
-Writing plugins
----------------
-
-To make a new plugin, create an init function and a set of options (if
-needed), stuff it into an object and put it in the $.plot.plugins
-array. For example:
-
-  function myCoolPluginInit(plot) { plot.coolstring = "Hello!" };
-  var myCoolOptions = { coolstuff: { show: true } }
-  $.plot.plugins.push({ init: myCoolPluginInit, options: myCoolOptions });
-
-  // now when $.plot is called, the returned object will have the
-  // attribute "coolstring"
-
-Now, given that the plugin might run in many different places, it's
-a good idea to avoid leaking names. We can avoid this by wrapping the
-above lines in an anonymous function which we call immediately, like
-this: (function () { inner code ... })(). To make it even more robust
-in case $ is not bound to jQuery but some other Javascript library, we
-can write it as
-
-  (function ($) {
-    // plugin definition
-    // ...
-  })(jQuery);
-
-Here is a simple debug plugin which alerts each of the series in the
-plot. It has a single option that control whether it is enabled and
-how much info to output:
-
-  (function ($) {
-    function init(plot) {
-      var debugLevel = 1;
-    
-      function checkDebugEnabled(plot, options) {
-        if (options.debug) {
-          debugLevel = options.debug;
-            
-          plot.hooks.processDatapoints.push(alertSeries);
-        }
-      }
-
-      function alertSeries(plot, series, datapoints) {
-        var msg = "series " + series.label;
-        if (debugLevel > 1)
-          msg += " with " + series.data.length + " points";
-        alert(msg);
-      }
-    
-      plot.hooks.processOptions.push(checkDebugEnabled);
-    }
-
-    var options = { debug: 0 };
-    
-    $.plot.plugins.push({
-        init: init,
-        options: options,
-        name: "simpledebug",
-        version: "0.1"
-    });
-  })(jQuery);
-
-We also define "name" and "version". It's not used by Flot, but might
-be helpful for other plugins in resolving dependencies.
-  
-Put the above in a file named "jquery.flot.debug.js", include it in an
-HTML page and then it can be used with:
-
-  $.plot($("#placeholder"), [...], { debug: 2 });
-
-This simple plugin illustrates a couple of points:
-
- - It uses the anonymous function trick to avoid name pollution.
- - It can be enabled/disabled through an option.
- - Variables in the init function can be used to store plot-specific
-   state between the hooks.
-
- 
-Options guidelines
-==================
-   
-Plugins should always support appropriate options to enable/disable
-them because the plugin user may have several plots on the same page
-where only one should use the plugin.
-
-If the plugin needs series-specific options, you can put them in
-"series" in the options object, e.g.
-
-  var options = {
-    series: {
-      downsample: {
-        algorithm: null,
-        maxpoints: 1000
-      }
-    }
-  }
-
-Then they will be copied by Flot into each series, providing the
-defaults in case the plugin user doesn't specify any. Again, in most
-cases it's probably a good idea if the plugin is turned off rather
-than on per default, just like most of the powerful features in Flot.
-
-Think hard and long about naming the options. These names are going to
-be public API, and code is going to depend on them if the plugin is
-successful.
diff --git a/lib/logstash/web/public/js/flot/README.txt b/lib/logstash/web/public/js/flot/README.txt
deleted file mode 100644
index 5f962fb6711..00000000000
--- a/lib/logstash/web/public/js/flot/README.txt
+++ /dev/null
@@ -1,81 +0,0 @@
-About
------
-
-Flot is a Javascript plotting library for jQuery. Read more at the
-website:
-
-  http://code.google.com/p/flot/
-
-Take a look at the examples linked from above, they should give a good
-impression of what Flot can do and the source code of the examples is
-probably the fastest way to learn how to use Flot.
-  
-
-Installation
-------------
-
-Just include the Javascript file after you've included jQuery.
-
-Note that you need to get a version of Excanvas (e.g. the one bundled
-with Flot) which is canvas emulation on Internet Explorer. You can
-include the excanvas script like this:
-
-  <!--[if IE]><script language="javascript" type="text/javascript" src="excanvas.pack.js"></script><![endif]-->
-
-If it's not working on your development IE 6.0, check that it has
-support for VML which excanvas is relying on. It appears that some
-stripped down versions used for test environments on virtual machines
-lack the VML support.
-  
-Also note that you need at least jQuery 1.2.6 (but at least jQuery
-1.3.2 is recommended for interactive charts because of performance
-improvements in event handling).
-
-
-Basic usage
------------
-
-Create a placeholder div to put the graph in:
-
-   <div id="placeholder"></div>
-
-You need to set the width and height of this div, otherwise the plot
-library doesn't know how to scale the graph. You can do it inline like
-this:
-
-   <div id="placeholder" style="width:600px;height:300px"></div>
-
-You can also do it with an external stylesheet. Make sure that the
-placeholder isn't within something with a display:none CSS property -
-in that case, Flot has trouble measuring label dimensions which
-results in garbled looks and might have trouble measuring the
-placeholder dimensions which is fatal (it'll throw an exception).
-
-Then when the div is ready in the DOM, which is usually on document
-ready, run the plot function:
-
-  $.plot($("#placeholder"), data, options);
-
-Here, data is an array of data series and options is an object with
-settings if you want to customize the plot. Take a look at the
-examples for some ideas of what to put in or look at the reference
-in the file "API.txt". Here's a quick example that'll draw a line from
-(0, 0) to (1, 1):
-
-  $.plot($("#placeholder"), [ [[0, 0], [1, 1]] ], { yaxis: { max: 1 } });
-
-The plot function immediately draws the chart and then returns a plot
-object with a couple of methods.
-
-
-What's with the name?
----------------------
-
-First: it's pronounced with a short o, like "plot". Not like "flawed".
-
-So "Flot" rhymes with "plot".
-
-And if you look up "flot" in a Danish-to-English dictionary, some up
-the words that come up are "good-looking", "attractive", "stylish",
-"smart", "impressive", "extravagant". One of the main goals with Flot
-is pretty looks.
diff --git a/lib/logstash/web/public/js/flot/examples/ajax.html b/lib/logstash/web/public/js/flot/examples/ajax.html
deleted file mode 100644
index 385a834af43..00000000000
--- a/lib/logstash/web/public/js/flot/examples/ajax.html
+++ /dev/null
@@ -1,143 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
- </head>
-    <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:600px;height:300px;"></div>
-
-    <p>Example of loading data dynamically with AJAX. Percentage change in GDP (source: <a href="http://epp.eurostat.ec.europa.eu/tgm/table.do?tab=table&init=1&plugin=1&language=en&pcode=tsieb020">Eurostat</a>). Click the buttons below.</p>
-
-    <p>The data is fetched over HTTP, in this case directly from text
-    files. Usually the URL would point to some web server handler
-    (e.g. a PHP page or Java/.NET/Python/Ruby on Rails handler) that
-    extracts it from a database and serializes it to JSON.</p>
-
-    <p>
-      <input class="fetchSeries" type="button" value="First dataset"> -
-      <a href="data-eu-gdp-growth.json">data</a> -
-      <span></span>
-    </p>
-
-    <p>
-      <input class="fetchSeries" type="button" value="Second dataset"> -
-      <a href="data-japan-gdp-growth.json">data</a> -
-      <span></span>
-    </p>
-
-    <p>
-      <input class="fetchSeries" type="button" value="Third dataset"> -
-      <a href="data-usa-gdp-growth.json">data</a> -
-      <span></span>
-    </p>
-
-    <p>If you combine AJAX with setTimeout, you can poll the server
-       for new data.</p>
-
-    <p>
-      <input class="dataUpdate" type="button" value="Poll for data">
-    </p>
-
-<script id="source" language="javascript" type="text/javascript">
-$(function () {
-    var options = {
-        lines: { show: true },
-        points: { show: true },
-        xaxis: { tickDecimals: 0, tickSize: 1 }
-    };
-    var data = [];
-    var placeholder = $("#placeholder");
-    
-    $.plot(placeholder, data, options);
-
-    
-    // fetch one series, adding to what we got
-    var alreadyFetched = {};
-    
-    $("input.fetchSeries").click(function () {
-        var button = $(this);
-        
-        // find the URL in the link right next to us 
-        var dataurl = button.siblings('a').attr('href');
-
-        // then fetch the data with jQuery
-        function onDataReceived(series) {
-            // extract the first coordinate pair so you can see that
-            // data is now an ordinary Javascript object
-            var firstcoordinate = '(' + series.data[0][0] + ', ' + series.data[0][1] + ')';
-
-            button.siblings('span').text('Fetched ' + series.label + ', first point: ' + firstcoordinate);
-
-            // let's add it to our current data
-            if (!alreadyFetched[series.label]) {
-                alreadyFetched[series.label] = true;
-                data.push(series);
-            }
-            
-            // and plot all we got
-            $.plot(placeholder, data, options);
-         }
-        
-        $.ajax({
-            url: dataurl,
-            method: 'GET',
-            dataType: 'json',
-            success: onDataReceived
-        });
-    });
-
-
-    // initiate a recurring data update
-    $("input.dataUpdate").click(function () {
-        // reset data
-        data = [];
-        alreadyFetched = {};
-        
-        $.plot(placeholder, data, options);
-
-        var iteration = 0;
-        
-        function fetchData() {
-            ++iteration;
-
-            function onDataReceived(series) {
-                // we get all the data in one go, if we only got partial
-                // data, we could merge it with what we already got
-                data = [ series ];
-                
-                $.plot($("#placeholder"), data, options);
-            }
-        
-            $.ajax({
-                // usually, we'll just call the same URL, a script
-                // connected to a database, but in this case we only
-                // have static example files so we need to modify the
-                // URL
-                url: "data-eu-gdp-growth-" + iteration + ".json",
-                method: 'GET',
-                dataType: 'json',
-                success: onDataReceived
-            });
-            
-            if (iteration < 5)
-                setTimeout(fetchData, 1000);
-            else {
-                data = [];
-                alreadyFetched = {};
-            }
-        }
-
-        setTimeout(fetchData, 1000);
-    });
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/annotating.html b/lib/logstash/web/public/js/flot/examples/annotating.html
deleted file mode 100644
index 9d99ea40f94..00000000000
--- a/lib/logstash/web/public/js/flot/examples/annotating.html
+++ /dev/null
@@ -1,75 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
- </head>
-    <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:600px;height:300px;"></div>
-
-    <p>Flot has support for simple background decorations such as
-    lines and rectangles. They can be useful for marking up certain
-    areas. You can easily add any HTML you need with standard DOM
-    manipulation, e.g. for labels. For drawing custom shapes there is
-    also direct access to the canvas.</p>
-
-<script id="source" language="javascript" type="text/javascript">
-$(function () {
-    // generate a dataset
-    var d1 = [];
-    for (var i = 0; i < 20; ++i)
-        d1.push([i, Math.sin(i)]);
-    
-    var data = [{ data: d1, label: "Pressure", color: "#333" }];
-
-    // setup background areas
-    var markings = [
-        { color: '#f6f6f6', yaxis: { from: 1 } },
-        { color: '#f6f6f6', yaxis: { to: -1 } },
-        { color: '#000', lineWidth: 1, xaxis: { from: 2, to: 2 } },
-        { color: '#000', lineWidth: 1, xaxis: { from: 8, to: 8 } }
-    ];
-    
-    var placeholder = $("#placeholder");
-    
-    // plot it
-    var plot = $.plot(placeholder, data, {
-        bars: { show: true, barWidth: 0.5, fill: 0.9 },
-        xaxis: { ticks: [], autoscaleMargin: 0.02 },
-        yaxis: { min: -2, max: 2 },
-        grid: { markings: markings }
-    });
-
-    // add labels
-    var o;
-
-    o = plot.pointOffset({ x: 2, y: -1.2});
-    // we just append it to the placeholder which Flot already uses
-    // for positioning
-    placeholder.append('<div style="position:absolute;left:' + (o.left + 4) + 'px;top:' + o.top + 'px;color:#666;font-size:smaller">Warming up</div>');
-
-    o = plot.pointOffset({ x: 8, y: -1.2});
-    placeholder.append('<div style="position:absolute;left:' + (o.left + 4) + 'px;top:' + o.top + 'px;color:#666;font-size:smaller">Actual measurements</div>');
-
-    // draw a little arrow on top of the last label to demonstrate
-    // canvas drawing
-    var ctx = plot.getCanvas().getContext("2d");
-    ctx.beginPath();
-    o.left += 4;
-    ctx.moveTo(o.left, o.top);
-    ctx.lineTo(o.left, o.top - 10);
-    ctx.lineTo(o.left + 10, o.top - 5);
-    ctx.lineTo(o.left, o.top);
-    ctx.fillStyle = "#000";
-    ctx.fill();
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/arrow-down.gif b/lib/logstash/web/public/js/flot/examples/arrow-down.gif
deleted file mode 100644
index e239d11aa65..00000000000
Binary files a/lib/logstash/web/public/js/flot/examples/arrow-down.gif and /dev/null differ
diff --git a/lib/logstash/web/public/js/flot/examples/arrow-left.gif b/lib/logstash/web/public/js/flot/examples/arrow-left.gif
deleted file mode 100644
index 93ffd5a9e0d..00000000000
Binary files a/lib/logstash/web/public/js/flot/examples/arrow-left.gif and /dev/null differ
diff --git a/lib/logstash/web/public/js/flot/examples/arrow-right.gif b/lib/logstash/web/public/js/flot/examples/arrow-right.gif
deleted file mode 100644
index 5fd053085c1..00000000000
Binary files a/lib/logstash/web/public/js/flot/examples/arrow-right.gif and /dev/null differ
diff --git a/lib/logstash/web/public/js/flot/examples/arrow-up.gif b/lib/logstash/web/public/js/flot/examples/arrow-up.gif
deleted file mode 100644
index 7d196267ebf..00000000000
Binary files a/lib/logstash/web/public/js/flot/examples/arrow-up.gif and /dev/null differ
diff --git a/lib/logstash/web/public/js/flot/examples/basic.html b/lib/logstash/web/public/js/flot/examples/basic.html
deleted file mode 100644
index fde8def4fce..00000000000
--- a/lib/logstash/web/public/js/flot/examples/basic.html
+++ /dev/null
@@ -1,38 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
- </head>
-    <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:600px;height:300px;"></div>
-
-    <p>Simple example. You don't need to specify much to get an
-       attractive look. Put in a placeholder, make sure you set its
-       dimensions (otherwise the plot library will barf) and call the
-       plot function with the data. The axes are automatically
-       scaled.</p>
-
-<script id="source" language="javascript" type="text/javascript">
-$(function () {
-    var d1 = [];
-    for (var i = 0; i < 14; i += 0.5)
-        d1.push([i, Math.sin(i)]);
-
-    var d2 = [[0, 3], [4, 8], [8, 5], [9, 13]];
-
-    // a null signifies separate line segments
-    var d3 = [[0, 12], [7, 12], null, [7, 2.5], [12, 2.5]];
-    
-    $.plot($("#placeholder"), [ d1, d2, d3 ]);
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth-1.json b/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth-1.json
deleted file mode 100644
index 4372bf56c66..00000000000
--- a/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth-1.json
+++ /dev/null
@@ -1,4 +0,0 @@
-{
-    label: 'Europe (EU27)',
-    data: [[1999, 3.0], [2000, 3.9]]
-}
diff --git a/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth-2.json b/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth-2.json
deleted file mode 100644
index 619988246b1..00000000000
--- a/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth-2.json
+++ /dev/null
@@ -1,4 +0,0 @@
-{
-    label: 'Europe (EU27)',
-    data: [[1999, 3.0], [2000, 3.9], [2001, 2.0], [2002, 1.2]]
-}
diff --git a/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth-3.json b/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth-3.json
deleted file mode 100644
index 607f17880ae..00000000000
--- a/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth-3.json
+++ /dev/null
@@ -1,4 +0,0 @@
-{
-    label: 'Europe (EU27)',
-    data: [[1999, 3.0], [2000, 3.9], [2001, 2.0], [2002, 1.2], [2003, 1.3], [2004, 2.5]]
-}
diff --git a/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth-4.json b/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth-4.json
deleted file mode 100644
index df60fa94ac1..00000000000
--- a/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth-4.json
+++ /dev/null
@@ -1,4 +0,0 @@
-{
-    label: 'Europe (EU27)',
-    data: [[1999, 3.0], [2000, 3.9], [2001, 2.0], [2002, 1.2], [2003, 1.3], [2004, 2.5], [2005, 2.0], [2006, 3.1]]
-}
diff --git a/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth-5.json b/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth-5.json
deleted file mode 100644
index e722bcc1748..00000000000
--- a/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth-5.json
+++ /dev/null
@@ -1,4 +0,0 @@
-{
-    label: 'Europe (EU27)',
-    data: [[1999, 3.0], [2000, 3.9], [2001, 2.0], [2002, 1.2], [2003, 1.3], [2004, 2.5], [2005, 2.0], [2006, 3.1], [2007, 2.9], [2008, 0.9]]
-}
diff --git a/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth.json b/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth.json
deleted file mode 100644
index e722bcc1748..00000000000
--- a/lib/logstash/web/public/js/flot/examples/data-eu-gdp-growth.json
+++ /dev/null
@@ -1,4 +0,0 @@
-{
-    label: 'Europe (EU27)',
-    data: [[1999, 3.0], [2000, 3.9], [2001, 2.0], [2002, 1.2], [2003, 1.3], [2004, 2.5], [2005, 2.0], [2006, 3.1], [2007, 2.9], [2008, 0.9]]
-}
diff --git a/lib/logstash/web/public/js/flot/examples/data-japan-gdp-growth.json b/lib/logstash/web/public/js/flot/examples/data-japan-gdp-growth.json
deleted file mode 100644
index 09aae77aae2..00000000000
--- a/lib/logstash/web/public/js/flot/examples/data-japan-gdp-growth.json
+++ /dev/null
@@ -1,4 +0,0 @@
-{
-    label: 'Japan',
-    data: [[1999, -0.1], [2000, 2.9], [2001, 0.2], [2002, 0.3], [2003, 1.4], [2004, 2.7], [2005, 1.9], [2006, 2.0], [2007, 2.3], [2008, -0.7]]
-}
diff --git a/lib/logstash/web/public/js/flot/examples/data-usa-gdp-growth.json b/lib/logstash/web/public/js/flot/examples/data-usa-gdp-growth.json
deleted file mode 100644
index 33fd4d32e08..00000000000
--- a/lib/logstash/web/public/js/flot/examples/data-usa-gdp-growth.json
+++ /dev/null
@@ -1,4 +0,0 @@
-{
-    label: 'USA',
-    data: [[1999, 4.4], [2000, 3.7], [2001, 0.8], [2002, 1.6], [2003, 2.5], [2004, 3.6], [2005, 2.9], [2006, 2.8], [2007, 2.0], [2008, 1.1]]
-}
diff --git a/lib/logstash/web/public/js/flot/examples/dual-axis.html b/lib/logstash/web/public/js/flot/examples/dual-axis.html
deleted file mode 100644
index 093505dd6bc..00000000000
--- a/lib/logstash/web/public/js/flot/examples/dual-axis.html
+++ /dev/null
@@ -1,39 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
- </head>
-    <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:600px;height:300px;"></div>
-
-    <p>Dual axis support showing the raw oil price in US $/barrel of
-    crude oil (left axis) vs. the exchange rate from US $ to ‚Ç¨ (right
-    axis).</p>
-
-    <p>As illustrated, you can put in secondary y and x axes if you
-    need to. For each data series, simply specify the axis number.</p>
-
-<script id="source" language="javascript" type="text/javascript">
-$(function () {
-    var oilprices = [[1167692400000,61.05], [1167778800000,58.32], [1167865200000,57.35], [1167951600000,56.31], [1168210800000,55.55], [1168297200000,55.64], [1168383600000,54.02], [1168470000000,51.88], [1168556400000,52.99], [1168815600000,52.99], [1168902000000,51.21], [1168988400000,52.24], [1169074800000,50.48], [1169161200000,51.99], [1169420400000,51.13], [1169506800000,55.04], [1169593200000,55.37], [1169679600000,54.23], [1169766000000,55.42], [1170025200000,54.01], [1170111600000,56.97], [1170198000000,58.14], [1170284400000,58.14], [1170370800000,59.02], [1170630000000,58.74], [1170716400000,58.88], [1170802800000,57.71], [1170889200000,59.71], [1170975600000,59.89], [1171234800000,57.81], [1171321200000,59.06], [1171407600000,58.00], [1171494000000,57.99], [1171580400000,59.39], [1171839600000,59.39], [1171926000000,58.07], [1172012400000,60.07], [1172098800000,61.14], [1172444400000,61.39], [1172530800000,61.46], [1172617200000,61.79], [1172703600000,62.00], [1172790000000,60.07], [1173135600000,60.69], [1173222000000,61.82], [1173308400000,60.05], [1173654000000,58.91], [1173740400000,57.93], [1173826800000,58.16], [1173913200000,57.55], [1173999600000,57.11], [1174258800000,56.59], [1174345200000,59.61], [1174518000000,61.69], [1174604400000,62.28], [1174860000000,62.91], [1174946400000,62.93], [1175032800000,64.03], [1175119200000,66.03], [1175205600000,65.87], [1175464800000,64.64], [1175637600000,64.38], [1175724000000,64.28], [1175810400000,64.28], [1176069600000,61.51], [1176156000000,61.89], [1176242400000,62.01], [1176328800000,63.85], [1176415200000,63.63], [1176674400000,63.61], [1176760800000,63.10], [1176847200000,63.13], [1176933600000,61.83], [1177020000000,63.38], [1177279200000,64.58], [1177452000000,65.84], [1177538400000,65.06], [1177624800000,66.46], [1177884000000,64.40], [1178056800000,63.68], [1178143200000,63.19], [1178229600000,61.93], [1178488800000,61.47], [1178575200000,61.55], [1178748000000,61.81], [1178834400000,62.37], [1179093600000,62.46], [1179180000000,63.17], [1179266400000,62.55], [1179352800000,64.94], [1179698400000,66.27], [1179784800000,65.50], [1179871200000,65.77], [1179957600000,64.18], [1180044000000,65.20], [1180389600000,63.15], [1180476000000,63.49], [1180562400000,65.08], [1180908000000,66.30], [1180994400000,65.96], [1181167200000,66.93], [1181253600000,65.98], [1181599200000,65.35], [1181685600000,66.26], [1181858400000,68.00], [1182117600000,69.09], [1182204000000,69.10], [1182290400000,68.19], [1182376800000,68.19], [1182463200000,69.14], [1182722400000,68.19], [1182808800000,67.77], [1182895200000,68.97], [1182981600000,69.57], [1183068000000,70.68], [1183327200000,71.09], [1183413600000,70.92], [1183586400000,71.81], [1183672800000,72.81], [1183932000000,72.19], [1184018400000,72.56], [1184191200000,72.50], [1184277600000,74.15], [1184623200000,75.05], [1184796000000,75.92], [1184882400000,75.57], [1185141600000,74.89], [1185228000000,73.56], [1185314400000,75.57], [1185400800000,74.95], [1185487200000,76.83], [1185832800000,78.21], [1185919200000,76.53], [1186005600000,76.86], [1186092000000,76.00], [1186437600000,71.59], [1186696800000,71.47], [1186956000000,71.62], [1187042400000,71.00], [1187301600000,71.98], [1187560800000,71.12], [1187647200000,69.47], [1187733600000,69.26], [1187820000000,69.83], [1187906400000,71.09], [1188165600000,71.73], [1188338400000,73.36], [1188511200000,74.04], [1188856800000,76.30], [1189116000000,77.49], [1189461600000,78.23], [1189548000000,79.91], [1189634400000,80.09], [1189720800000,79.10], [1189980000000,80.57], [1190066400000,81.93], [1190239200000,83.32], [1190325600000,81.62], [1190584800000,80.95], [1190671200000,79.53], [1190757600000,80.30], [1190844000000,82.88], [1190930400000,81.66], [1191189600000,80.24], [1191276000000,80.05], [1191362400000,79.94], [1191448800000,81.44], [1191535200000,81.22], [1191794400000,79.02], [1191880800000,80.26], [1191967200000,80.30], [1192053600000,83.08], [1192140000000,83.69], [1192399200000,86.13], [1192485600000,87.61], [1192572000000,87.40], [1192658400000,89.47], [1192744800000,88.60], [1193004000000,87.56], [1193090400000,87.56], [1193176800000,87.10], [1193263200000,91.86], [1193612400000,93.53], [1193698800000,94.53], [1193871600000,95.93], [1194217200000,93.98], [1194303600000,96.37], [1194476400000,95.46], [1194562800000,96.32], [1195081200000,93.43], [1195167600000,95.10], [1195426800000,94.64], [1195513200000,95.10], [1196031600000,97.70], [1196118000000,94.42], [1196204400000,90.62], [1196290800000,91.01], [1196377200000,88.71], [1196636400000,88.32], [1196809200000,90.23], [1196982000000,88.28], [1197241200000,87.86], [1197327600000,90.02], [1197414000000,92.25], [1197586800000,90.63], [1197846000000,90.63], [1197932400000,90.49], [1198018800000,91.24], [1198105200000,91.06], [1198191600000,90.49], [1198710000000,96.62], [1198796400000,96.00], [1199142000000,99.62], [1199314800000,99.18], [1199401200000,95.09], [1199660400000,96.33], [1199833200000,95.67], [1200351600000,91.90], [1200438000000,90.84], [1200524400000,90.13], [1200610800000,90.57], [1200956400000,89.21], [1201042800000,86.99], [1201129200000,89.85], [1201474800000,90.99], [1201561200000,91.64], [1201647600000,92.33], [1201734000000,91.75], [1202079600000,90.02], [1202166000000,88.41], [1202252400000,87.14], [1202338800000,88.11], [1202425200000,91.77], [1202770800000,92.78], [1202857200000,93.27], [1202943600000,95.46], [1203030000000,95.46], [1203289200000,101.74], [1203462000000,98.81], [1203894000000,100.88], [1204066800000,99.64], [1204153200000,102.59], [1204239600000,101.84], [1204498800000,99.52], [1204585200000,99.52], [1204671600000,104.52], [1204758000000,105.47], [1204844400000,105.15], [1205103600000,108.75], [1205276400000,109.92], [1205362800000,110.33], [1205449200000,110.21], [1205708400000,105.68], [1205967600000,101.84], [1206313200000,100.86], [1206399600000,101.22], [1206486000000,105.90], [1206572400000,107.58], [1206658800000,105.62], [1206914400000,101.58], [1207000800000,100.98], [1207173600000,103.83], [1207260000000,106.23], [1207605600000,108.50], [1207778400000,110.11], [1207864800000,110.14], [1208210400000,113.79], [1208296800000,114.93], [1208383200000,114.86], [1208728800000,117.48], [1208815200000,118.30], [1208988000000,116.06], [1209074400000,118.52], [1209333600000,118.75], [1209420000000,113.46], [1209592800000,112.52], [1210024800000,121.84], [1210111200000,123.53], [1210197600000,123.69], [1210543200000,124.23], [1210629600000,125.80], [1210716000000,126.29], [1211148000000,127.05], [1211320800000,129.07], [1211493600000,132.19], [1211839200000,128.85], [1212357600000,127.76], [1212703200000,138.54], [1212962400000,136.80], [1213135200000,136.38], [1213308000000,134.86], [1213653600000,134.01], [1213740000000,136.68], [1213912800000,135.65], [1214172000000,134.62], [1214258400000,134.62], [1214344800000,134.62], [1214431200000,139.64], [1214517600000,140.21], [1214776800000,140.00], [1214863200000,140.97], [1214949600000,143.57], [1215036000000,145.29], [1215381600000,141.37], [1215468000000,136.04], [1215727200000,146.40], [1215986400000,145.18], [1216072800000,138.74], [1216159200000,134.60], [1216245600000,129.29], [1216332000000,130.65], [1216677600000,127.95], [1216850400000,127.95], [1217282400000,122.19], [1217455200000,124.08], [1217541600000,125.10], [1217800800000,121.41], [1217887200000,119.17], [1217973600000,118.58], [1218060000000,120.02], [1218405600000,114.45], [1218492000000,113.01], [1218578400000,116.00], [1218751200000,113.77], [1219010400000,112.87], [1219096800000,114.53], [1219269600000,114.98], [1219356000000,114.98], [1219701600000,116.27], [1219788000000,118.15], [1219874400000,115.59], [1219960800000,115.46], [1220306400000,109.71], [1220392800000,109.35], [1220565600000,106.23], [1220824800000,106.34]];
-    var exchangerates = [[1167606000000,0.7580], [1167692400000,0.7580], [1167778800000,0.75470], [1167865200000,0.75490], [1167951600000,0.76130], [1168038000000,0.76550], [1168124400000,0.76930], [1168210800000,0.76940], [1168297200000,0.76880], [1168383600000,0.76780], [1168470000000,0.77080], [1168556400000,0.77270], [1168642800000,0.77490], [1168729200000,0.77410], [1168815600000,0.77410], [1168902000000,0.77320], [1168988400000,0.77270], [1169074800000,0.77370], [1169161200000,0.77240], [1169247600000,0.77120], [1169334000000,0.7720], [1169420400000,0.77210], [1169506800000,0.77170], [1169593200000,0.77040], [1169679600000,0.7690], [1169766000000,0.77110], [1169852400000,0.7740], [1169938800000,0.77450], [1170025200000,0.77450], [1170111600000,0.7740], [1170198000000,0.77160], [1170284400000,0.77130], [1170370800000,0.76780], [1170457200000,0.76880], [1170543600000,0.77180], [1170630000000,0.77180], [1170716400000,0.77280], [1170802800000,0.77290], [1170889200000,0.76980], [1170975600000,0.76850], [1171062000000,0.76810], [1171148400000,0.7690], [1171234800000,0.7690], [1171321200000,0.76980], [1171407600000,0.76990], [1171494000000,0.76510], [1171580400000,0.76130], [1171666800000,0.76160], [1171753200000,0.76140], [1171839600000,0.76140], [1171926000000,0.76070], [1172012400000,0.76020], [1172098800000,0.76110], [1172185200000,0.76220], [1172271600000,0.76150], [1172358000000,0.75980], [1172444400000,0.75980], [1172530800000,0.75920], [1172617200000,0.75730], [1172703600000,0.75660], [1172790000000,0.75670], [1172876400000,0.75910], [1172962800000,0.75820], [1173049200000,0.75850], [1173135600000,0.76130], [1173222000000,0.76310], [1173308400000,0.76150], [1173394800000,0.760], [1173481200000,0.76130], [1173567600000,0.76270], [1173654000000,0.76270], [1173740400000,0.76080], [1173826800000,0.75830], [1173913200000,0.75750], [1173999600000,0.75620], [1174086000000,0.7520], [1174172400000,0.75120], [1174258800000,0.75120], [1174345200000,0.75170], [1174431600000,0.7520], [1174518000000,0.75110], [1174604400000,0.7480], [1174690800000,0.75090], [1174777200000,0.75310], [1174860000000,0.75310], [1174946400000,0.75270], [1175032800000,0.74980], [1175119200000,0.74930], [1175205600000,0.75040], [1175292000000,0.750], [1175378400000,0.74910], [1175464800000,0.74910], [1175551200000,0.74850], [1175637600000,0.74840], [1175724000000,0.74920], [1175810400000,0.74710], [1175896800000,0.74590], [1175983200000,0.74770], [1176069600000,0.74770], [1176156000000,0.74830], [1176242400000,0.74580], [1176328800000,0.74480], [1176415200000,0.7430], [1176501600000,0.73990], [1176588000000,0.73950], [1176674400000,0.73950], [1176760800000,0.73780], [1176847200000,0.73820], [1176933600000,0.73620], [1177020000000,0.73550], [1177106400000,0.73480], [1177192800000,0.73610], [1177279200000,0.73610], [1177365600000,0.73650], [1177452000000,0.73620], [1177538400000,0.73310], [1177624800000,0.73390], [1177711200000,0.73440], [1177797600000,0.73270], [1177884000000,0.73270], [1177970400000,0.73360], [1178056800000,0.73330], [1178143200000,0.73590], [1178229600000,0.73590], [1178316000000,0.73720], [1178402400000,0.7360], [1178488800000,0.7360], [1178575200000,0.7350], [1178661600000,0.73650], [1178748000000,0.73840], [1178834400000,0.73950], [1178920800000,0.74130], [1179007200000,0.73970], [1179093600000,0.73960], [1179180000000,0.73850], [1179266400000,0.73780], [1179352800000,0.73660], [1179439200000,0.740], [1179525600000,0.74110], [1179612000000,0.74060], [1179698400000,0.74050], [1179784800000,0.74140], [1179871200000,0.74310], [1179957600000,0.74310], [1180044000000,0.74380], [1180130400000,0.74430], [1180216800000,0.74430], [1180303200000,0.74430], [1180389600000,0.74340], [1180476000000,0.74290], [1180562400000,0.74420], [1180648800000,0.7440], [1180735200000,0.74390], [1180821600000,0.74370], [1180908000000,0.74370], [1180994400000,0.74290], [1181080800000,0.74030], [1181167200000,0.73990], [1181253600000,0.74180], [1181340000000,0.74680], [1181426400000,0.7480], [1181512800000,0.7480], [1181599200000,0.7490], [1181685600000,0.74940], [1181772000000,0.75220], [1181858400000,0.75150], [1181944800000,0.75020], [1182031200000,0.74720], [1182117600000,0.74720], [1182204000000,0.74620], [1182290400000,0.74550], [1182376800000,0.74490], [1182463200000,0.74670], [1182549600000,0.74580], [1182636000000,0.74270], [1182722400000,0.74270], [1182808800000,0.7430], [1182895200000,0.74290], [1182981600000,0.7440], [1183068000000,0.7430], [1183154400000,0.74220], [1183240800000,0.73880], [1183327200000,0.73880], [1183413600000,0.73690], [1183500000000,0.73450], [1183586400000,0.73450], [1183672800000,0.73450], [1183759200000,0.73520], [1183845600000,0.73410], [1183932000000,0.73410], [1184018400000,0.7340], [1184104800000,0.73240], [1184191200000,0.72720], [1184277600000,0.72640], [1184364000000,0.72550], [1184450400000,0.72580], [1184536800000,0.72580], [1184623200000,0.72560], [1184709600000,0.72570], [1184796000000,0.72470], [1184882400000,0.72430], [1184968800000,0.72440], [1185055200000,0.72350], [1185141600000,0.72350], [1185228000000,0.72350], [1185314400000,0.72350], [1185400800000,0.72620], [1185487200000,0.72880], [1185573600000,0.73010], [1185660000000,0.73370], [1185746400000,0.73370], [1185832800000,0.73240], [1185919200000,0.72970], [1186005600000,0.73170], [1186092000000,0.73150], [1186178400000,0.72880], [1186264800000,0.72630], [1186351200000,0.72630], [1186437600000,0.72420], [1186524000000,0.72530], [1186610400000,0.72640], [1186696800000,0.7270], [1186783200000,0.73120], [1186869600000,0.73050], [1186956000000,0.73050], [1187042400000,0.73180], [1187128800000,0.73580], [1187215200000,0.74090], [1187301600000,0.74540], [1187388000000,0.74370], [1187474400000,0.74240], [1187560800000,0.74240], [1187647200000,0.74150], [1187733600000,0.74190], [1187820000000,0.74140], [1187906400000,0.73770], [1187992800000,0.73550], [1188079200000,0.73150], [1188165600000,0.73150], [1188252000000,0.7320], [1188338400000,0.73320], [1188424800000,0.73460], [1188511200000,0.73280], [1188597600000,0.73230], [1188684000000,0.7340], [1188770400000,0.7340], [1188856800000,0.73360], [1188943200000,0.73510], [1189029600000,0.73460], [1189116000000,0.73210], [1189202400000,0.72940], [1189288800000,0.72660], [1189375200000,0.72660], [1189461600000,0.72540], [1189548000000,0.72420], [1189634400000,0.72130], [1189720800000,0.71970], [1189807200000,0.72090], [1189893600000,0.7210], [1189980000000,0.7210], [1190066400000,0.7210], [1190152800000,0.72090], [1190239200000,0.71590], [1190325600000,0.71330], [1190412000000,0.71050], [1190498400000,0.70990], [1190584800000,0.70990], [1190671200000,0.70930], [1190757600000,0.70930], [1190844000000,0.70760], [1190930400000,0.7070], [1191016800000,0.70490], [1191103200000,0.70120], [1191189600000,0.70110], [1191276000000,0.70190], [1191362400000,0.70460], [1191448800000,0.70630], [1191535200000,0.70890], [1191621600000,0.70770], [1191708000000,0.70770], [1191794400000,0.70770], [1191880800000,0.70910], [1191967200000,0.71180], [1192053600000,0.70790], [1192140000000,0.70530], [1192226400000,0.7050], [1192312800000,0.70550], [1192399200000,0.70550], [1192485600000,0.70450], [1192572000000,0.70510], [1192658400000,0.70510], [1192744800000,0.70170], [1192831200000,0.70], [1192917600000,0.69950], [1193004000000,0.69940], [1193090400000,0.70140], [1193176800000,0.70360], [1193263200000,0.70210], [1193349600000,0.70020], [1193436000000,0.69670], [1193522400000,0.6950], [1193612400000,0.6950], [1193698800000,0.69390], [1193785200000,0.6940], [1193871600000,0.69220], [1193958000000,0.69190], [1194044400000,0.69140], [1194130800000,0.68940], [1194217200000,0.68910], [1194303600000,0.69040], [1194390000000,0.6890], [1194476400000,0.68340], [1194562800000,0.68230], [1194649200000,0.68070], [1194735600000,0.68150], [1194822000000,0.68150], [1194908400000,0.68470], [1194994800000,0.68590], [1195081200000,0.68220], [1195167600000,0.68270], [1195254000000,0.68370], [1195340400000,0.68230], [1195426800000,0.68220], [1195513200000,0.68220], [1195599600000,0.67920], [1195686000000,0.67460], [1195772400000,0.67350], [1195858800000,0.67310], [1195945200000,0.67420], [1196031600000,0.67440], [1196118000000,0.67390], [1196204400000,0.67310], [1196290800000,0.67610], [1196377200000,0.67610], [1196463600000,0.67850], [1196550000000,0.68180], [1196636400000,0.68360], [1196722800000,0.68230], [1196809200000,0.68050], [1196895600000,0.67930], [1196982000000,0.68490], [1197068400000,0.68330], [1197154800000,0.68250], [1197241200000,0.68250], [1197327600000,0.68160], [1197414000000,0.67990], [1197500400000,0.68130], [1197586800000,0.68090], [1197673200000,0.68680], [1197759600000,0.69330], [1197846000000,0.69330], [1197932400000,0.69450], [1198018800000,0.69440], [1198105200000,0.69460], [1198191600000,0.69640], [1198278000000,0.69650], [1198364400000,0.69560], [1198450800000,0.69560], [1198537200000,0.6950], [1198623600000,0.69480], [1198710000000,0.69280], [1198796400000,0.68870], [1198882800000,0.68240], [1198969200000,0.67940], [1199055600000,0.67940], [1199142000000,0.68030], [1199228400000,0.68550], [1199314800000,0.68240], [1199401200000,0.67910], [1199487600000,0.67830], [1199574000000,0.67850], [1199660400000,0.67850], [1199746800000,0.67970], [1199833200000,0.680], [1199919600000,0.68030], [1200006000000,0.68050], [1200092400000,0.6760], [1200178800000,0.6770], [1200265200000,0.6770], [1200351600000,0.67360], [1200438000000,0.67260], [1200524400000,0.67640], [1200610800000,0.68210], [1200697200000,0.68310], [1200783600000,0.68420], [1200870000000,0.68420], [1200956400000,0.68870], [1201042800000,0.69030], [1201129200000,0.68480], [1201215600000,0.68240], [1201302000000,0.67880], [1201388400000,0.68140], [1201474800000,0.68140], [1201561200000,0.67970], [1201647600000,0.67690], [1201734000000,0.67650], [1201820400000,0.67330], [1201906800000,0.67290], [1201993200000,0.67580], [1202079600000,0.67580], [1202166000000,0.6750], [1202252400000,0.6780], [1202338800000,0.68330], [1202425200000,0.68560], [1202511600000,0.69030], [1202598000000,0.68960], [1202684400000,0.68960], [1202770800000,0.68820], [1202857200000,0.68790], [1202943600000,0.68620], [1203030000000,0.68520], [1203116400000,0.68230], [1203202800000,0.68130], [1203289200000,0.68130], [1203375600000,0.68220], [1203462000000,0.68020], [1203548400000,0.68020], [1203634800000,0.67840], [1203721200000,0.67480], [1203807600000,0.67470], [1203894000000,0.67470], [1203980400000,0.67480], [1204066800000,0.67330], [1204153200000,0.6650], [1204239600000,0.66110], [1204326000000,0.65830], [1204412400000,0.6590], [1204498800000,0.6590], [1204585200000,0.65810], [1204671600000,0.65780], [1204758000000,0.65740], [1204844400000,0.65320], [1204930800000,0.65020], [1205017200000,0.65140], [1205103600000,0.65140], [1205190000000,0.65070], [1205276400000,0.6510], [1205362800000,0.64890], [1205449200000,0.64240], [1205535600000,0.64060], [1205622000000,0.63820], [1205708400000,0.63820], [1205794800000,0.63410], [1205881200000,0.63440], [1205967600000,0.63780], [1206054000000,0.64390], [1206140400000,0.64780], [1206226800000,0.64810], [1206313200000,0.64810], [1206399600000,0.64940], [1206486000000,0.64380], [1206572400000,0.63770], [1206658800000,0.63290], [1206745200000,0.63360], [1206831600000,0.63330], [1206914400000,0.63330], [1207000800000,0.6330], [1207087200000,0.63710], [1207173600000,0.64030], [1207260000000,0.63960], [1207346400000,0.63640], [1207432800000,0.63560], [1207519200000,0.63560], [1207605600000,0.63680], [1207692000000,0.63570], [1207778400000,0.63540], [1207864800000,0.6320], [1207951200000,0.63320], [1208037600000,0.63280], [1208124000000,0.63310], [1208210400000,0.63420], [1208296800000,0.63210], [1208383200000,0.63020], [1208469600000,0.62780], [1208556000000,0.63080], [1208642400000,0.63240], [1208728800000,0.63240], [1208815200000,0.63070], [1208901600000,0.62770], [1208988000000,0.62690], [1209074400000,0.63350], [1209160800000,0.63920], [1209247200000,0.640], [1209333600000,0.64010], [1209420000000,0.63960], [1209506400000,0.64070], [1209592800000,0.64230], [1209679200000,0.64290], [1209765600000,0.64720], [1209852000000,0.64850], [1209938400000,0.64860], [1210024800000,0.64670], [1210111200000,0.64440], [1210197600000,0.64670], [1210284000000,0.65090], [1210370400000,0.64780], [1210456800000,0.64610], [1210543200000,0.64610], [1210629600000,0.64680], [1210716000000,0.64490], [1210802400000,0.6470], [1210888800000,0.64610], [1210975200000,0.64520], [1211061600000,0.64220], [1211148000000,0.64220], [1211234400000,0.64250], [1211320800000,0.64140], [1211407200000,0.63660], [1211493600000,0.63460], [1211580000000,0.6350], [1211666400000,0.63460], [1211752800000,0.63460], [1211839200000,0.63430], [1211925600000,0.63460], [1212012000000,0.63790], [1212098400000,0.64160], [1212184800000,0.64420], [1212271200000,0.64310], [1212357600000,0.64310], [1212444000000,0.64350], [1212530400000,0.6440], [1212616800000,0.64730], [1212703200000,0.64690], [1212789600000,0.63860], [1212876000000,0.63560], [1212962400000,0.6340], [1213048800000,0.63460], [1213135200000,0.6430], [1213221600000,0.64520], [1213308000000,0.64670], [1213394400000,0.65060], [1213480800000,0.65040], [1213567200000,0.65030], [1213653600000,0.64810], [1213740000000,0.64510], [1213826400000,0.6450], [1213912800000,0.64410], [1213999200000,0.64140], [1214085600000,0.64090], [1214172000000,0.64090], [1214258400000,0.64280], [1214344800000,0.64310], [1214431200000,0.64180], [1214517600000,0.63710], [1214604000000,0.63490], [1214690400000,0.63330], [1214776800000,0.63340], [1214863200000,0.63380], [1214949600000,0.63420], [1215036000000,0.6320], [1215122400000,0.63180], [1215208800000,0.6370], [1215295200000,0.63680], [1215381600000,0.63680], [1215468000000,0.63830], [1215554400000,0.63710], [1215640800000,0.63710], [1215727200000,0.63550], [1215813600000,0.6320], [1215900000000,0.62770], [1215986400000,0.62760], [1216072800000,0.62910], [1216159200000,0.62740], [1216245600000,0.62930], [1216332000000,0.63110], [1216418400000,0.6310], [1216504800000,0.63120], [1216591200000,0.63120], [1216677600000,0.63040], [1216764000000,0.62940], [1216850400000,0.63480], [1216936800000,0.63780], [1217023200000,0.63680], [1217109600000,0.63680], [1217196000000,0.63680], [1217282400000,0.6360], [1217368800000,0.6370], [1217455200000,0.64180], [1217541600000,0.64110], [1217628000000,0.64350], [1217714400000,0.64270], [1217800800000,0.64270], [1217887200000,0.64190], [1217973600000,0.64460], [1218060000000,0.64680], [1218146400000,0.64870], [1218232800000,0.65940], [1218319200000,0.66660], [1218405600000,0.66660], [1218492000000,0.66780], [1218578400000,0.67120], [1218664800000,0.67050], [1218751200000,0.67180], [1218837600000,0.67840], [1218924000000,0.68110], [1219010400000,0.68110], [1219096800000,0.67940], [1219183200000,0.68040], [1219269600000,0.67810], [1219356000000,0.67560], [1219442400000,0.67350], [1219528800000,0.67630], [1219615200000,0.67620], [1219701600000,0.67770], [1219788000000,0.68150], [1219874400000,0.68020], [1219960800000,0.6780], [1220047200000,0.67960], [1220133600000,0.68170], [1220220000000,0.68170], [1220306400000,0.68320], [1220392800000,0.68770], [1220479200000,0.69120], [1220565600000,0.69140], [1220652000000,0.70090], [1220738400000,0.70120], [1220824800000,0.7010], [1220911200000,0.70050]];
-
-    $.plot($("#placeholder"),
-           [ { data: oilprices, label: "Oil price ($)" },
-             { data: exchangerates, label: "USD/EUR exchange rate", yaxis: 2 }],
-           { 
-             xaxis: { mode: 'time' },
-             yaxis: { min: 0 },
-             y2axis: { tickFormatter: function (v, axis) { return v.toFixed(axis.tickDecimals) +"‚Ç¨" }},
-             legend: { position: 'sw' } });
-});
-</script>
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/graph-types.html b/lib/logstash/web/public/js/flot/examples/graph-types.html
deleted file mode 100644
index b3c38186c7f..00000000000
--- a/lib/logstash/web/public/js/flot/examples/graph-types.html
+++ /dev/null
@@ -1,75 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
- </head>
-    <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:600px;height:300px"></div>
-
-    <p>Flot supports lines, points, filled areas, bars and any
-    combinations of these, in the same plot and even on the same data
-    series.</p>
-
-<script id="source" language="javascript" type="text/javascript">
-$(function () {
-    var d1 = [];
-    for (var i = 0; i < 14; i += 0.5)
-        d1.push([i, Math.sin(i)]);
-
-    var d2 = [[0, 3], [4, 8], [8, 5], [9, 13]];
-
-    var d3 = [];
-    for (var i = 0; i < 14; i += 0.5)
-        d3.push([i, Math.cos(i)]);
-
-    var d4 = [];
-    for (var i = 0; i < 14; i += 0.1)
-        d4.push([i, Math.sqrt(i * 10)]);
-    
-    var d5 = [];
-    for (var i = 0; i < 14; i += 0.5)
-        d5.push([i, Math.sqrt(i)]);
-
-    var d6 = [];
-    for (var i = 0; i < 14; i += 0.5 + Math.random())
-        d6.push([i, Math.sqrt(2*i + Math.sin(i) + 5)]);
-                        
-    $.plot($("#placeholder"), [
-        {
-            data: d1,
-            lines: { show: true, fill: true }
-        },
-        {
-            data: d2,
-            bars: { show: true }
-        },
-        {
-            data: d3,
-            points: { show: true }
-        },
-        {
-            data: d4,
-            lines: { show: true }
-        },
-        {
-            data: d5,
-            lines: { show: true },
-            points: { show: true }
-        },
-        {
-            data: d6,
-            lines: { show: true, steps: true }
-        }
-    ]);
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/hs-2004-27-a-large_web.jpg b/lib/logstash/web/public/js/flot/examples/hs-2004-27-a-large_web.jpg
deleted file mode 100644
index a1d5c058375..00000000000
Binary files a/lib/logstash/web/public/js/flot/examples/hs-2004-27-a-large_web.jpg and /dev/null differ
diff --git a/lib/logstash/web/public/js/flot/examples/image.html b/lib/logstash/web/public/js/flot/examples/image.html
deleted file mode 100644
index 57189d2d5ca..00000000000
--- a/lib/logstash/web/public/js/flot/examples/image.html
+++ /dev/null
@@ -1,45 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.image.js"></script>
- </head>
- <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:400px;height:400px;"></div>
-
-    <p>The Cat's Eye Nebula (<a href="http://hubblesite.org/gallery/album/nebula/pr2004027a/">picture from Hubble</a>).</p>
-    
-    <p>With the image plugin, you can plot images. This is for example
-    useful for getting ticks on complex prerendered visualizations.
-    Instead of inputting data points, you put in the images and where
-    their two opposite corners are supposed to be in plot space.</p>
-
-    <p>Images represent a little further complication because you need
-    to make sure they are loaded before you can use them (Flot skips
-    incomplete images). The plugin comes with a couple of helpers
-    for doing that.</p>
-
-<script id="source" language="javascript" type="text/javascript">
-$(function () {
-    var data = [ [ ["hs-2004-27-a-large_web.jpg", -10, -10, 10, 10] ] ];
-    var options = {
-            series: { images: { show: true } },
-            xaxis: { min: -8, max: 4 },
-            yaxis: { min: -8, max: 4 }
-    };
-
-    $.plot.image.loadDataImages(data, options, function () {
-        $.plot($("#placeholder"), data, options);
-    });
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/index.html b/lib/logstash/web/public/js/flot/examples/index.html
deleted file mode 100644
index 789f941b4fe..00000000000
--- a/lib/logstash/web/public/js/flot/examples/index.html
+++ /dev/null
@@ -1,43 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
- </head>
- <body>
-    <h1>Flot Examples</h1>
-
-    <p>Here are some examples for <a href="http://code.google.com/p/flot/">Flot</a>, the Javascript charting library for jQuery:</p>
-
-    <ul>
-      <li><a href="basic.html">Basic example</a></li>
-      <li><a href="graph-types.html">Different graph types</a></li>
-      <li><a href="setting-options.html">Setting various options</a> and <a href="annotating.html">annotating a chart</a></li>
-      <li><a href="ajax.html">Updating graphs with AJAX</a></li>
-    </ul>
-
-    <p>Being interactive:</p>
-    
-    <ul>
-      <li><a href="turning-series.html">Turning series on/off</a></li>
-      <li><a href="selection.html">Rectangular selection support and zooming</a> and <a href="zooming.html">zooming with overview</a></li> (both with selection plugin)
-      <li><a href="interacting.html">Interacting with the data points</a></li>
-      <li><a href="navigate.html">Panning and zooming</a> (with navigation plugin)</li>
-    </ul>
-
-    <p>Some more esoteric features:</p>
-    
-    <ul>
-      <li><a href="time.html">Plotting time series</a> and <a href="visitors.html">visitors per day with zooming and weekends</a> (with selection plugin)</li>
-      <li><a href="dual-axis.html">Dual axis support</a></li>
-      <li><a href="thresholding.html">Thresholding the data</a> (with threshold plugin)</li>
-      <li><a href="stacking.html">Stacked charts</a> (with stacking plugin)</li>
-      <li><a href="tracking.html">Tracking curves with crosshair</a> (with crosshair plugin)</li>
-      <li><a href="image.html">Plotting prerendered images</a> (with image plugin)</li>
-    </ul>
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/interacting.html b/lib/logstash/web/public/js/flot/examples/interacting.html
deleted file mode 100644
index fbf03904418..00000000000
--- a/lib/logstash/web/public/js/flot/examples/interacting.html
+++ /dev/null
@@ -1,93 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
- </head>
-    <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:600px;height:300px"></div>
-
-    <p>One of the goals of Flot is to support user interactions. Try
-    pointing and clicking on the points.</p>
-
-    <p id="hoverdata">Mouse hovers at
-    (<span id="x">0</span>, <span id="y">0</span>). <span id="clickdata"></span></p>
-
-    <p>A tooltip is easy to build with a bit of jQuery code and the
-    data returned from the plot.</p>
-
-    <p><input id="enableTooltip" type="checkbox">Enable tooltip</p>
-
-<script id="source" language="javascript" type="text/javascript">
-$(function () {
-    var sin = [], cos = [];
-    for (var i = 0; i < 14; i += 0.5) {
-        sin.push([i, Math.sin(i)]);
-        cos.push([i, Math.cos(i)]);
-    }
-
-    var plot = $.plot($("#placeholder"),
-           [ { data: sin, label: "sin(x)"}, { data: cos, label: "cos(x)" } ], {
-               series: {
-                   lines: { show: true },
-                   points: { show: true }
-               },
-               grid: { hoverable: true, clickable: true },
-               yaxis: { min: -1.2, max: 1.2 }
-             });
-
-    function showTooltip(x, y, contents) {
-        $('<div id="tooltip">' + contents + '</div>').css( {
-            position: 'absolute',
-            display: 'none',
-            top: y + 5,
-            left: x + 5,
-            border: '1px solid #fdd',
-            padding: '2px',
-            'background-color': '#fee',
-            opacity: 0.80
-        }).appendTo("body").fadeIn(200);
-    }
-
-    var previousPoint = null;
-    $("#placeholder").bind("plothover", function (event, pos, item) {
-        $("#x").text(pos.x.toFixed(2));
-        $("#y").text(pos.y.toFixed(2));
-
-        if ($("#enableTooltip:checked").length > 0) {
-            if (item) {
-                if (previousPoint != item.datapoint) {
-                    previousPoint = item.datapoint;
-                    
-                    $("#tooltip").remove();
-                    var x = item.datapoint[0].toFixed(2),
-                        y = item.datapoint[1].toFixed(2);
-                    
-                    showTooltip(item.pageX, item.pageY,
-                                item.series.label + " of " + x + " = " + y);
-                }
-            }
-            else {
-                $("#tooltip").remove();
-                previousPoint = null;            
-            }
-        }
-    });
-
-    $("#placeholder").bind("plotclick", function (event, pos, item) {
-        if (item) {
-            $("#clickdata").text("You clicked point " + item.dataIndex + " in " + item.series.label + ".");
-            plot.highlight(item.series, item.datapoint);
-        }
-    });
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/layout.css b/lib/logstash/web/public/js/flot/examples/layout.css
deleted file mode 100644
index 7ef7dd4c5f0..00000000000
--- a/lib/logstash/web/public/js/flot/examples/layout.css
+++ /dev/null
@@ -1,6 +0,0 @@
-body {
-  font-family: sans-serif;
-  font-size: 16px;
-  margin: 50px;
-  max-width: 800px;
-}
diff --git a/lib/logstash/web/public/js/flot/examples/navigate.html b/lib/logstash/web/public/js/flot/examples/navigate.html
deleted file mode 100644
index 78eff55dc90..00000000000
--- a/lib/logstash/web/public/js/flot/examples/navigate.html
+++ /dev/null
@@ -1,118 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.navigate.js"></script>
-    <style>
-    #placeholder .button {
-        position: absolute;
-        cursor: pointer;
-    }
-    #placeholder div.button {
-        font-size: smaller;
-        color: #999;
-        background-color: #eee;
-        padding: 2px;
-    }
-    .message {
-        padding-left: 50px;
-        font-size: smaller;
-    }
-    </style>
- </head>
- <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:600px;height:300px;"></div>
-
-    <p class="message"></p>
-
-    <p>With the navigate plugin it is easy to add panning and zooming.
-    Drag to pan, double click to zoom (or use the mouse scrollwheel).</p>
-
-    <p>The plugin fires events (useful for synchronizing several
-    plots) and adds a couple of public methods so you can easily build
-    a little user interface around it, like the little buttons at the
-    top right in the plot.</p>
-    
-
-<script id="source" language="javascript" type="text/javascript">
-$(function () {
-    // generate data set from a parametric function with a fractal
-    // look
-    function sumf(f, t, m) {
-        var res = 0;
-        for (var i = 1; i < m; ++i)
-            res += f(i * i * t) / (i * i);
-        return res;
-    }
-    
-    var d1 = [];
-    for (var t = 0; t <= 2 * Math.PI; t += 0.01)
-        d1.push([sumf(Math.cos, t, 10), sumf(Math.sin, t, 10)]);
-    var data = [ d1 ];
-
-    
-    var placeholder = $("#placeholder");
-    var options = {
-        series: { lines: { show: true }, shadowSize: 0 },
-        xaxis: { zoomRange: [0.1, 10], panRange: [-10, 10] },
-        yaxis: { zoomRange: [0.1, 10], panRange: [-10, 10] },
-        zoom: {
-            interactive: true
-        },
-        pan: {
-            interactive: true
-        }
-    };
-
-    var plot = $.plot(placeholder, data, options);
-
-    // show pan/zoom messages to illustrate events 
-    placeholder.bind('plotpan', function (event, plot) {
-        var axes = plot.getAxes();
-        $(".message").html("Panning to x: "  + axes.xaxis.min.toFixed(2)
-                           + " &ndash; " + axes.xaxis.max.toFixed(2)
-                           + " and y: " + axes.yaxis.min.toFixed(2)
-                           + " &ndash; " + axes.yaxis.max.toFixed(2));
-    });
-
-    placeholder.bind('plotzoom', function (event, plot) {
-        var axes = plot.getAxes();
-        $(".message").html("Zooming to x: "  + axes.xaxis.min.toFixed(2)
-                           + " &ndash; " + axes.xaxis.max.toFixed(2)
-                           + " and y: " + axes.yaxis.min.toFixed(2)
-                           + " &ndash; " + axes.yaxis.max.toFixed(2));
-    });
-
-    // add zoom out button 
-    $('<div class="button" style="right:20px;top:20px">zoom out</div>').appendTo(placeholder).click(function (e) {
-        e.preventDefault();
-        plot.zoomOut();
-    });
-
-    // and add panning buttons
-    
-    // little helper for taking the repetitive work out of placing
-    // panning arrows
-    function addArrow(dir, right, top, offset) {
-        $('<img class="button" src="arrow-' + dir + '.gif" style="right:' + right + 'px;top:' + top + 'px">').appendTo(placeholder).click(function (e) {
-            e.preventDefault();
-            plot.pan(offset);
-        });
-    }
-
-    addArrow('left', 55, 60, { left: -100 });
-    addArrow('right', 25, 60, { left: 100 });
-    addArrow('up', 40, 45, { top: -100 });
-    addArrow('down', 40, 75, { top: 100 });
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/selection.html b/lib/logstash/web/public/js/flot/examples/selection.html
deleted file mode 100644
index 8b67a2b6b9b..00000000000
--- a/lib/logstash/web/public/js/flot/examples/selection.html
+++ /dev/null
@@ -1,114 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.selection.js"></script>
- </head>
-    <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:600px;height:300px"></div>
-
-    <p>1000 kg. CO<sub>2</sub> emissions per year per capita for various countries (source: <a href="http://en.wikipedia.org/wiki/List_of_countries_by_carbon_dioxide_emissions_per_capita">Wikipedia</a>).</p>
-
-    <p>Flot supports selections through the selection plugin.
-       You can enable rectangular selection
-       or one-dimensional selection if the user should only be able to
-       select on one axis. Try left-click and drag on the plot above
-       where selection on the x axis is enabled.</p>
-
-    <p>You selected: <span id="selection"></span></p>
-
-    <p>The plot command returns a plot object you can use to control
-       the selection. Click the buttons below.</p>
-
-    <p><input id="clearSelection" type="button" value="Clear selection" />
-       <input id="setSelection" type="button" value="Select year 1994" /></p>
-
-    <p>Selections are really useful for zooming. Just replot the
-       chart with min and max values for the axes set to the values
-       in the "plotselected" event triggered. Enable the checkbox
-       below and select a region again.</p>
-
-    <p><input id="zoom" type="checkbox">Zoom to selection.</input></p>
-
-<script id="source" language="javascript" type="text/javascript">
-$(function () {
-    var data = [
-        {
-            label: "United States",
-            data: [[1990, 18.9], [1991, 18.7], [1992, 18.4], [1993, 19.3], [1994, 19.5], [1995, 19.3], [1996, 19.4], [1997, 20.2], [1998, 19.8], [1999, 19.9], [2000, 20.4], [2001, 20.1], [2002, 20.0], [2003, 19.8], [2004, 20.4]]
-        },
-        {
-            label: "Russia", 
-            data: [[1992, 13.4], [1993, 12.2], [1994, 10.6], [1995, 10.2], [1996, 10.1], [1997, 9.7], [1998, 9.5], [1999, 9.7], [2000, 9.9], [2001, 9.9], [2002, 9.9], [2003, 10.3], [2004, 10.5]]
-        },
-        {
-            label: "United Kingdom",
-            data: [[1990, 10.0], [1991, 11.3], [1992, 9.9], [1993, 9.6], [1994, 9.5], [1995, 9.5], [1996, 9.9], [1997, 9.3], [1998, 9.2], [1999, 9.2], [2000, 9.5], [2001, 9.6], [2002, 9.3], [2003, 9.4], [2004, 9.79]]
-        },
-        {
-            label: "Germany",
-            data: [[1990, 12.4], [1991, 11.2], [1992, 10.8], [1993, 10.5], [1994, 10.4], [1995, 10.2], [1996, 10.5], [1997, 10.2], [1998, 10.1], [1999, 9.6], [2000, 9.7], [2001, 10.0], [2002, 9.7], [2003, 9.8], [2004, 9.79]]
-        },
-        {
-            label: "Denmark",
- 	    data: [[1990, 9.7], [1991, 12.1], [1992, 10.3], [1993, 11.3], [1994, 11.7], [1995, 10.6], [1996, 12.8], [1997, 10.8], [1998, 10.3], [1999, 9.4], [2000, 8.7], [2001, 9.0], [2002, 8.9], [2003, 10.1], [2004, 9.80]]
-        },
-        {
-            label: "Sweden",
-            data: [[1990, 5.8], [1991, 6.0], [1992, 5.9], [1993, 5.5], [1994, 5.7], [1995, 5.3], [1996, 6.1], [1997, 5.4], [1998, 5.4], [1999, 5.1], [2000, 5.2], [2001, 5.4], [2002, 6.2], [2003, 5.9], [2004, 5.89]]
-        },
-        {
-            label: "Norway",
-            data: [[1990, 8.3], [1991, 8.3], [1992, 7.8], [1993, 8.3], [1994, 8.4], [1995, 5.9], [1996, 6.4], [1997, 6.7], [1998, 6.9], [1999, 7.6], [2000, 7.4], [2001, 8.1], [2002, 12.5], [2003, 9.9], [2004, 19.0]]
-        }
-    ];
-
-    var options = {
-        series: {
-            lines: { show: true },
-            points: { show: true }
-        },
-        legend: { noColumns: 2 },
-        xaxis: { tickDecimals: 0 },
-        yaxis: { min: 0 },
-        selection: { mode: "x" }
-    };
-
-    var placeholder = $("#placeholder");
-
-    placeholder.bind("plotselected", function (event, ranges) {
-        $("#selection").text(ranges.xaxis.from.toFixed(1) + " to " + ranges.xaxis.to.toFixed(1));
-
-        var zoom = $("#zoom").attr("checked");
-        if (zoom)
-            plot = $.plot(placeholder, data,
-                          $.extend(true, {}, options, {
-                              xaxis: { min: ranges.xaxis.from, max: ranges.xaxis.to }
-                          }));
-    });
-
-    placeholder.bind("plotunselected", function (event) {
-        $("#selection").text("");
-    });
-    
-    var plot = $.plot(placeholder, data, options);
-
-    $("#clearSelection").click(function () {
-        plot.clearSelection();
-    });
-
-    $("#setSelection").click(function () {
-        plot.setSelection({ x1: 1994, x2: 1995 });
-    });
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/setting-options.html b/lib/logstash/web/public/js/flot/examples/setting-options.html
deleted file mode 100644
index 6eb6ee9aa0a..00000000000
--- a/lib/logstash/web/public/js/flot/examples/setting-options.html
+++ /dev/null
@@ -1,65 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
- </head>
-    <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:600px;height:300px"></div>
-
-    <p>There are plenty of options you can set to control the precise
-    looks of your plot. You can control the axes, the legend, the
-    default graph type, the look of grid, etc.</p>
-
-    <p>The idea is that Flot goes to great lengths to provide <b>sensible
-    defaults</b> which you can then customize as needed for your
-    particular application. If you've found a use case where the
-    defaults can be improved, please don't hesitate to give your
-    feedback.</p>
-
-<script id="source" language="javascript" type="text/javascript">
-$(function () {
-    var d1 = [];
-    for (var i = 0; i < Math.PI * 2; i += 0.25)
-        d1.push([i, Math.sin(i)]);
-    
-    var d2 = [];
-    for (var i = 0; i < Math.PI * 2; i += 0.25)
-        d2.push([i, Math.cos(i)]);
-
-    var d3 = [];
-    for (var i = 0; i < Math.PI * 2; i += 0.1)
-        d3.push([i, Math.tan(i)]);
-    
-    $.plot($("#placeholder"), [
-        { label: "sin(x)",  data: d1},
-        { label: "cos(x)",  data: d2},
-        { label: "tan(x)",  data: d3}
-    ], {
-        series: {
-            lines: { show: true },
-            points: { show: true }
-        },
-        xaxis: {
-            ticks: [0, [Math.PI/2, "\u03c0/2"], [Math.PI, "\u03c0"], [Math.PI * 3/2, "3\u03c0/2"], [Math.PI * 2, "2\u03c0"]]
-        },
-        yaxis: {
-            ticks: 10,
-            min: -2,
-            max: 2
-        },
-        grid: {
-            backgroundColor: { colors: ["#fff", "#eee"] }
-        }
-    });
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/stacking.html b/lib/logstash/web/public/js/flot/examples/stacking.html
deleted file mode 100644
index 62e0c7b0d98..00000000000
--- a/lib/logstash/web/public/js/flot/examples/stacking.html
+++ /dev/null
@@ -1,77 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.stack.js"></script>
- </head>
-    <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:600px;height:300px;"></div>
-
-    <p>With the stack plugin, you can have Flot stack the
-    series. This is useful if you wish to display both a total and the
-    constituents it is made of. The only requirement is that you provide
-    the input sorted on x.</p>
-
-    <p class="stackControls">
-    <input type="button" value="With stacking">
-    <input type="button" value="Without stacking">
-    </p>
-
-    <p class="graphControls">
-    <input type="button" value="Bars">
-    <input type="button" value="Lines">
-    <input type="button" value="Lines with steps">
-    </p>
-
-<script id="source">
-$(function () {
-    var d1 = [];
-    for (var i = 0; i <= 10; i += 1)
-        d1.push([i, parseInt(Math.random() * 30)]);
-
-    var d2 = [];
-    for (var i = 0; i <= 10; i += 1)
-        d2.push([i, parseInt(Math.random() * 30)]);
-
-    var d3 = [];
-    for (var i = 0; i <= 10; i += 1)
-        d3.push([i, parseInt(Math.random() * 30)]);
-
-    var stack = 0, bars = true, lines = false, steps = false;
-    
-    function plotWithOptions() {
-        $.plot($("#placeholder"), [ d1, d2, d3 ], {
-            series: {
-                stack: stack,
-                lines: { show: lines, steps: steps },
-                bars: { show: bars, barWidth: 0.6 }
-            }
-        });
-    }
-
-    plotWithOptions();
-    
-    $(".stackControls input").click(function (e) {
-        e.preventDefault();
-        stack = $(this).val() == "With stacking" ? true : null;
-        plotWithOptions();
-    });
-    $(".graphControls input").click(function (e) {
-        e.preventDefault();
-        bars = $(this).val().indexOf("Bars") != -1;
-        lines = $(this).val().indexOf("Lines") != -1;
-        steps = $(this).val().indexOf("steps") != -1;
-        plotWithOptions();
-    });
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/thresholding.html b/lib/logstash/web/public/js/flot/examples/thresholding.html
deleted file mode 100644
index 10b5b2a65b7..00000000000
--- a/lib/logstash/web/public/js/flot/examples/thresholding.html
+++ /dev/null
@@ -1,54 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.threshold.js"></script>
- </head>
-    <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:600px;height:300px;"></div>
-
-    <p>With the threshold plugin, you can apply a specific color to
-    the part of a data series below a threshold. This is can be useful
-    for highlighting negative values, e.g. when displaying net results
-    or what's in stock.</p>
-
-    <p class="controls">
-    <input type="button" value="Threshold at 5">
-    <input type="button" value="Threshold at 0">
-    <input type="button" value="Threshold at -2.5">
-    </p>
-
-<script id="source" language="javascript" type="text/javascript">
-$(function () {
-    var d1 = [];
-    for (var i = 0; i <= 60; i += 1)
-        d1.push([i, parseInt(Math.random() * 30 - 10)]);
-
-    function plotWithOptions(t) {
-        $.plot($("#placeholder"), [ {
-            data: d1,
-            color: "rgb(30, 180, 20)",
-            threshold: { below: t, color: "rgb(200, 20, 30)" },
-            lines: { steps: true }
-        } ]);
-    }
-
-    plotWithOptions(0);
-    
-    $(".controls input").click(function (e) {
-        e.preventDefault();
-        var t = parseFloat($(this).val().replace('Threshold at ', ''));
-        plotWithOptions(t);
-    });
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/time.html b/lib/logstash/web/public/js/flot/examples/time.html
deleted file mode 100644
index 5f43b88e4a0..00000000000
--- a/lib/logstash/web/public/js/flot/examples/time.html
+++ /dev/null
@@ -1,71 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
- </head>
-    <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:600px;height:300px;"></div>
-
-    <p>Monthly mean atmospheric CO<sub>2</sub> in PPM at Mauna Loa, Hawaii (source: <a href="http://www.esrl.noaa.gov/gmd/ccgg/trends/">NOAA/ESRL</a>).</p>
-
-    <p>If you tell Flot that an axis represents time, the data will
-      be interpreted as timestamps and the ticks adjusted and
-      formatted accordingly.</p>
-
-    <p>Zoom to: <button id="whole">Whole period</button>
-      <button id="nineties">1990-2000</button>
-      <button id="ninetynine">1999</button></p>
-
-    <p>The timestamps must be specified as Javascript timestamps, as
-      milliseconds since January 1, 1970 00:00. This is like Unix
-      timestamps, but in milliseconds instead of seconds (remember to
-      multiply with 1000!).</p>
-
-    <p>As an extra caveat, the timestamps are interpreted according to
-      UTC to avoid having the graph shift with each visitor's local
-      time zone. So you might have to add your local time zone offset
-      to the timestamps or simply pretend that the data was produced
-      in UTC instead of your local time zone.</p>
-
-<script id="source">
-$(function () {
-    var d = [[-373597200000, 315.71], [-370918800000, 317.45], [-368326800000, 317.50], [-363056400000, 315.86], [-360378000000, 314.93], [-357699600000, 313.19], [-352429200000, 313.34], [-349837200000, 314.67], [-347158800000, 315.58], [-344480400000, 316.47], [-342061200000, 316.65], [-339382800000, 317.71], [-336790800000, 318.29], [-334112400000, 318.16], [-331520400000, 316.55], [-328842000000, 314.80], [-326163600000, 313.84], [-323571600000, 313.34], [-320893200000, 314.81], [-318301200000, 315.59], [-315622800000, 316.43], [-312944400000, 316.97], [-310438800000, 317.58], [-307760400000, 319.03], [-305168400000, 320.03], [-302490000000, 319.59], [-299898000000, 318.18], [-297219600000, 315.91], [-294541200000, 314.16], [-291949200000, 313.83], [-289270800000, 315.00], [-286678800000, 316.19], [-284000400000, 316.89], [-281322000000, 317.70], [-278902800000, 318.54], [-276224400000, 319.48], [-273632400000, 320.58], [-270954000000, 319.78], [-268362000000, 318.58], [-265683600000, 316.79], [-263005200000, 314.99], [-260413200000, 315.31], [-257734800000, 316.10], [-255142800000, 317.01], [-252464400000, 317.94], [-249786000000, 318.56], [-247366800000, 319.69], [-244688400000, 320.58], [-242096400000, 321.01], [-239418000000, 320.61], [-236826000000, 319.61], [-234147600000, 317.40], [-231469200000, 316.26], [-228877200000, 315.42], [-226198800000, 316.69], [-223606800000, 317.69], [-220928400000, 318.74], [-218250000000, 319.08], [-215830800000, 319.86], [-213152400000, 321.39], [-210560400000, 322.24], [-207882000000, 321.47], [-205290000000, 319.74], [-202611600000, 317.77], [-199933200000, 316.21], [-197341200000, 315.99], [-194662800000, 317.07], [-192070800000, 318.36], [-189392400000, 319.57], [-178938000000, 322.23], [-176259600000, 321.89], [-173667600000, 320.44], [-170989200000, 318.70], [-168310800000, 316.70], [-165718800000, 316.87], [-163040400000, 317.68], [-160448400000, 318.71], [-157770000000, 319.44], [-155091600000, 320.44], [-152672400000, 320.89], [-149994000000, 322.13], [-147402000000, 322.16], [-144723600000, 321.87], [-142131600000, 321.21], [-139453200000, 318.87], [-136774800000, 317.81], [-134182800000, 317.30], [-131504400000, 318.87], [-128912400000, 319.42], [-126234000000, 320.62], [-123555600000, 321.59], [-121136400000, 322.39], [-118458000000, 323.70], [-115866000000, 324.07], [-113187600000, 323.75], [-110595600000, 322.40], [-107917200000, 320.37], [-105238800000, 318.64], [-102646800000, 318.10], [-99968400000, 319.79], [-97376400000, 321.03], [-94698000000, 322.33], [-92019600000, 322.50], [-89600400000, 323.04], [-86922000000, 324.42], [-84330000000, 325.00], [-81651600000, 324.09], [-79059600000, 322.55], [-76381200000, 320.92], [-73702800000, 319.26], [-71110800000, 319.39], [-68432400000, 320.72], [-65840400000, 321.96], [-63162000000, 322.57], [-60483600000, 323.15], [-57978000000, 323.89], [-55299600000, 325.02], [-52707600000, 325.57], [-50029200000, 325.36], [-47437200000, 324.14], [-44758800000, 322.11], [-42080400000, 320.33], [-39488400000, 320.25], [-36810000000, 321.32], [-34218000000, 322.90], [-31539600000, 324.00], [-28861200000, 324.42], [-26442000000, 325.64], [-23763600000, 326.66], [-21171600000, 327.38], [-18493200000, 326.70], [-15901200000, 325.89], [-13222800000, 323.67], [-10544400000, 322.38], [-7952400000, 321.78], [-5274000000, 322.85], [-2682000000, 324.12], [-3600000, 325.06], [2674800000, 325.98], [5094000000, 326.93], [7772400000, 328.13], [10364400000, 328.07], [13042800000, 327.66], [15634800000, 326.35], [18313200000, 324.69], [20991600000, 323.10], [23583600000, 323.07], [26262000000, 324.01], [28854000000, 325.13], [31532400000, 326.17], [34210800000, 326.68], [36630000000, 327.18], [39308400000, 327.78], [41900400000, 328.92], [44578800000, 328.57], [47170800000, 327.37], [49849200000, 325.43], [52527600000, 323.36], [55119600000, 323.56], [57798000000, 324.80], [60390000000, 326.01], [63068400000, 326.77], [65746800000, 327.63], [68252400000, 327.75], [70930800000, 329.72], [73522800000, 330.07], [76201200000, 329.09], [78793200000, 328.05], [81471600000, 326.32], [84150000000, 324.84], [86742000000, 325.20], [89420400000, 326.50], [92012400000, 327.55], [94690800000, 328.54], [97369200000, 329.56], [99788400000, 330.30], [102466800000, 331.50], [105058800000, 332.48], [107737200000, 332.07], [110329200000, 330.87], [113007600000, 329.31], [115686000000, 327.51], [118278000000, 327.18], [120956400000, 328.16], [123548400000, 328.64], [126226800000, 329.35], [128905200000, 330.71], [131324400000, 331.48], [134002800000, 332.65], [136594800000, 333.16], [139273200000, 332.06], [141865200000, 330.99], [144543600000, 329.17], [147222000000, 327.41], [149814000000, 327.20], [152492400000, 328.33], [155084400000, 329.50], [157762800000, 330.68], [160441200000, 331.41], [162860400000, 331.85], [165538800000, 333.29], [168130800000, 333.91], [170809200000, 333.40], [173401200000, 331.78], [176079600000, 329.88], [178758000000, 328.57], [181350000000, 328.46], [184028400000, 329.26], [189298800000, 331.71], [191977200000, 332.76], [194482800000, 333.48], [197161200000, 334.78], [199753200000, 334.78], [202431600000, 334.17], [205023600000, 332.78], [207702000000, 330.64], [210380400000, 328.95], [212972400000, 328.77], [215650800000, 330.23], [218242800000, 331.69], [220921200000, 332.70], [223599600000, 333.24], [226018800000, 334.96], [228697200000, 336.04], [231289200000, 336.82], [233967600000, 336.13], [236559600000, 334.73], [239238000000, 332.52], [241916400000, 331.19], [244508400000, 331.19], [247186800000, 332.35], [249778800000, 333.47], [252457200000, 335.11], [255135600000, 335.26], [257554800000, 336.60], [260233200000, 337.77], [262825200000, 338.00], [265503600000, 337.99], [268095600000, 336.48], [270774000000, 334.37], [273452400000, 332.27], [276044400000, 332.41], [278722800000, 333.76], [281314800000, 334.83], [283993200000, 336.21], [286671600000, 336.64], [289090800000, 338.12], [291769200000, 339.02], [294361200000, 339.02], [297039600000, 339.20], [299631600000, 337.58], [302310000000, 335.55], [304988400000, 333.89], [307580400000, 334.14], [310258800000, 335.26], [312850800000, 336.71], [315529200000, 337.81], [318207600000, 338.29], [320713200000, 340.04], [323391600000, 340.86], [325980000000, 341.47], [328658400000, 341.26], [331250400000, 339.29], [333928800000, 337.60], [336607200000, 336.12], [339202800000, 336.08], [341881200000, 337.22], [344473200000, 338.34], [347151600000, 339.36], [349830000000, 340.51], [352249200000, 341.57], [354924000000, 342.56], [357516000000, 343.01], [360194400000, 342.47], [362786400000, 340.71], [365464800000, 338.52], [368143200000, 336.96], [370738800000, 337.13], [373417200000, 338.58], [376009200000, 339.89], [378687600000, 340.93], [381366000000, 341.69], [383785200000, 342.69], [389052000000, 344.30], [391730400000, 343.43], [394322400000, 341.88], [397000800000, 339.89], [399679200000, 337.95], [402274800000, 338.10], [404953200000, 339.27], [407545200000, 340.67], [410223600000, 341.42], [412902000000, 342.68], [415321200000, 343.46], [417996000000, 345.10], [420588000000, 345.76], [423266400000, 345.36], [425858400000, 343.91], [428536800000, 342.05], [431215200000, 340.00], [433810800000, 340.12], [436489200000, 341.33], [439081200000, 342.94], [441759600000, 343.87], [444438000000, 344.60], [446943600000, 345.20], [452210400000, 347.36], [454888800000, 346.74], [457480800000, 345.41], [460159200000, 343.01], [462837600000, 341.23], [465433200000, 341.52], [468111600000, 342.86], [470703600000, 344.41], [473382000000, 345.09], [476060400000, 345.89], [478479600000, 347.49], [481154400000, 348.00], [483746400000, 348.75], [486424800000, 348.19], [489016800000, 346.54], [491695200000, 344.63], [494373600000, 343.03], [496969200000, 342.92], [499647600000, 344.24], [502239600000, 345.62], [504918000000, 346.43], [507596400000, 346.94], [510015600000, 347.88], [512690400000, 349.57], [515282400000, 350.35], [517960800000, 349.72], [520552800000, 347.78], [523231200000, 345.86], [525909600000, 344.84], [528505200000, 344.32], [531183600000, 345.67], [533775600000, 346.88], [536454000000, 348.19], [539132400000, 348.55], [541551600000, 349.52], [544226400000, 351.12], [546818400000, 351.84], [549496800000, 351.49], [552088800000, 349.82], [554767200000, 347.63], [557445600000, 346.38], [560041200000, 346.49], [562719600000, 347.75], [565311600000, 349.03], [567990000000, 350.20], [570668400000, 351.61], [573174000000, 352.22], [575848800000, 353.53], [578440800000, 354.14], [581119200000, 353.62], [583711200000, 352.53], [586389600000, 350.41], [589068000000, 348.84], [591663600000, 348.94], [594342000000, 350.04], [596934000000, 351.29], [599612400000, 352.72], [602290800000, 353.10], [604710000000, 353.65], [607384800000, 355.43], [609976800000, 355.70], [612655200000, 355.11], [615247200000, 353.79], [617925600000, 351.42], [620604000000, 349.81], [623199600000, 350.11], [625878000000, 351.26], [628470000000, 352.63], [631148400000, 353.64], [633826800000, 354.72], [636246000000, 355.49], [638920800000, 356.09], [641512800000, 357.08], [644191200000, 356.11], [646783200000, 354.70], [649461600000, 352.68], [652140000000, 351.05], [654735600000, 351.36], [657414000000, 352.81], [660006000000, 354.22], [662684400000, 354.85], [665362800000, 355.66], [667782000000, 357.04], [670456800000, 358.40], [673048800000, 359.00], [675727200000, 357.99], [678319200000, 356.00], [680997600000, 353.78], [683676000000, 352.20], [686271600000, 352.22], [688950000000, 353.70], [691542000000, 354.98], [694220400000, 356.09], [696898800000, 356.85], [699404400000, 357.73], [702079200000, 358.91], [704671200000, 359.45], [707349600000, 359.19], [709941600000, 356.72], [712620000000, 354.79], [715298400000, 352.79], [717894000000, 353.20], [720572400000, 354.15], [723164400000, 355.39], [725842800000, 356.77], [728521200000, 357.17], [730940400000, 358.26], [733615200000, 359.16], [736207200000, 360.07], [738885600000, 359.41], [741477600000, 357.44], [744156000000, 355.30], [746834400000, 353.87], [749430000000, 354.04], [752108400000, 355.27], [754700400000, 356.70], [757378800000, 358.00], [760057200000, 358.81], [762476400000, 359.68], [765151200000, 361.13], [767743200000, 361.48], [770421600000, 360.60], [773013600000, 359.20], [775692000000, 357.23], [778370400000, 355.42], [780966000000, 355.89], [783644400000, 357.41], [786236400000, 358.74], [788914800000, 359.73], [791593200000, 360.61], [794012400000, 361.58], [796687200000, 363.05], [799279200000, 363.62], [801957600000, 363.03], [804549600000, 361.55], [807228000000, 358.94], [809906400000, 357.93], [812502000000, 357.80], [815180400000, 359.22], [817772400000, 360.44], [820450800000, 361.83], [823129200000, 362.95], [825634800000, 363.91], [828309600000, 364.28], [830901600000, 364.94], [833580000000, 364.70], [836172000000, 363.31], [838850400000, 361.15], [841528800000, 359.40], [844120800000, 359.34], [846802800000, 360.62], [849394800000, 361.96], [852073200000, 362.81], [854751600000, 363.87], [857170800000, 364.25], [859845600000, 366.02], [862437600000, 366.46], [865116000000, 365.32], [867708000000, 364.07], [870386400000, 361.95], [873064800000, 360.06], [875656800000, 360.49], [878338800000, 362.19], [880930800000, 364.12], [883609200000, 364.99], [886287600000, 365.82], [888706800000, 366.95], [891381600000, 368.42], [893973600000, 369.33], [896652000000, 368.78], [899244000000, 367.59], [901922400000, 365.84], [904600800000, 363.83], [907192800000, 364.18], [909874800000, 365.34], [912466800000, 366.93], [915145200000, 367.94], [917823600000, 368.82], [920242800000, 369.46], [922917600000, 370.77], [925509600000, 370.66], [928188000000, 370.10], [930780000000, 369.08], [933458400000, 366.66], [936136800000, 364.60], [938728800000, 365.17], [941410800000, 366.51], [944002800000, 367.89], [946681200000, 369.04], [949359600000, 369.35], [951865200000, 370.38], [954540000000, 371.63], [957132000000, 371.32], [959810400000, 371.53], [962402400000, 369.75], [965080800000, 368.23], [967759200000, 366.87], [970351200000, 366.94], [973033200000, 368.27], [975625200000, 369.64], [978303600000, 370.46], [980982000000, 371.44], [983401200000, 372.37], [986076000000, 373.33], [988668000000, 373.77], [991346400000, 373.09], [993938400000, 371.51], [996616800000, 369.55], [999295200000, 368.12], [1001887200000, 368.38], [1004569200000, 369.66], [1007161200000, 371.11], [1009839600000, 372.36], [1012518000000, 373.09], [1014937200000, 373.81], [1017612000000, 374.93], [1020204000000, 375.58], [1022882400000, 375.44], [1025474400000, 373.86], [1028152800000, 371.77], [1030831200000, 370.73], [1033423200000, 370.50], [1036105200000, 372.18], [1038697200000, 373.70], [1041375600000, 374.92], [1044054000000, 375.62], [1046473200000, 376.51], [1049148000000, 377.75], [1051740000000, 378.54], [1054418400000, 378.20], [1057010400000, 376.68], [1059688800000, 374.43], [1062367200000, 373.11], [1064959200000, 373.10], [1067641200000, 374.77], [1070233200000, 375.97], [1072911600000, 377.03], [1075590000000, 377.87], [1078095600000, 378.88], [1080770400000, 380.42], [1083362400000, 380.62], [1086040800000, 379.70], [1088632800000, 377.43], [1091311200000, 376.32], [1093989600000, 374.19], [1096581600000, 374.47], [1099263600000, 376.15], [1101855600000, 377.51], [1104534000000, 378.43], [1107212400000, 379.70], [1109631600000, 380.92], [1112306400000, 382.18], [1114898400000, 382.45], [1117576800000, 382.14], [1120168800000, 380.60], [1122847200000, 378.64], [1125525600000, 376.73], [1128117600000, 376.84], [1130799600000, 378.29], [1133391600000, 380.06], [1136070000000, 381.40], [1138748400000, 382.20], [1141167600000, 382.66], [1143842400000, 384.69], [1146434400000, 384.94], [1149112800000, 384.01], [1151704800000, 382.14], [1154383200000, 380.31], [1157061600000, 378.81], [1159653600000, 379.03], [1162335600000, 380.17], [1164927600000, 381.85], [1167606000000, 382.94], [1170284400000, 383.86], [1172703600000, 384.49], [1175378400000, 386.37], [1177970400000, 386.54], [1180648800000, 385.98], [1183240800000, 384.36], [1185919200000, 381.85], [1188597600000, 380.74], [1191189600000, 381.15], [1193871600000, 382.38], [1196463600000, 383.94], [1199142000000, 385.44]]; 
-
-    $.plot($("#placeholder"), [d], { xaxis: { mode: "time" } });
-
-    $("#whole").click(function () {
-        $.plot($("#placeholder"), [d], { xaxis: { mode: "time" } });
-    });
-
-    $("#nineties").click(function () {
-        $.plot($("#placeholder"), [d], {
-            xaxis: {
-                mode: "time",
-                min: (new Date("1990/01/01")).getTime(),
-                max: (new Date("2000/01/01")).getTime()
-            }
-        });
-    });
-
-    $("#ninetynine").click(function () {
-        $.plot($("#placeholder"), [d], {
-            xaxis: {
-                mode: "time",
-                minTickSize: [1, "month"],
-                min: (new Date("1999/01/01")).getTime(),
-                max: (new Date("2000/01/01")).getTime()
-            }
-        });
-    });
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/tracking.html b/lib/logstash/web/public/js/flot/examples/tracking.html
deleted file mode 100644
index a0ad77d3006..00000000000
--- a/lib/logstash/web/public/js/flot/examples/tracking.html
+++ /dev/null
@@ -1,95 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.crosshair.js"></script>
- </head>
-    <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:600px;height:300px"></div>
-
-    <p>You can add crosshairs that'll track the mouse position, either
-    on both axes or as here on only one.</p>
-
-    <p>If you combine it with listening on hover events, you can use
-    it to track the intersection on the curves by interpolating
-    the data points (look at the legend).</p>
-
-    <p id="hoverdata"></p>
-
-<script id="source" language="javascript" type="text/javascript">
-var plot;
-$(function () {
-    var sin = [], cos = [];
-    for (var i = 0; i < 14; i += 0.1) {
-        sin.push([i, Math.sin(i)]);
-        cos.push([i, Math.cos(i)]);
-    }
-
-    plot = $.plot($("#placeholder"),
-                      [ { data: sin, label: "sin(x) = -0.00"},
-                        { data: cos, label: "cos(x) = -0.00" } ], {
-                            series: {
-                                lines: { show: true }
-                            },
-                            crosshair: { mode: "x" },
-                            grid: { hoverable: true, autoHighlight: false },
-                            yaxis: { min: -1.2, max: 1.2 }
-                        });
-    var legends = $("#placeholder .legendLabel");
-    legends.each(function () {
-        // fix the widths so they don't jump around
-        $(this).css('width', $(this).width());
-    });
-
-    var updateLegendTimeout = null;
-    var latestPosition = null;
-    
-    function updateLegend() {
-        updateLegendTimeout = null;
-        
-        var pos = latestPosition;
-        
-        var axes = plot.getAxes();
-        if (pos.x < axes.xaxis.min || pos.x > axes.xaxis.max ||
-            pos.y < axes.yaxis.min || pos.y > axes.yaxis.max)
-            return;
-
-        var i, j, dataset = plot.getData();
-        for (i = 0; i < dataset.length; ++i) {
-            var series = dataset[i];
-
-            // find the nearest points, x-wise
-            for (j = 0; j < series.data.length; ++j)
-                if (series.data[j][0] > pos.x)
-                    break;
-            
-            // now interpolate
-            var y, p1 = series.data[j - 1], p2 = series.data[j];
-            if (p1 == null)
-                y = p2[1];
-            else if (p2 == null)
-                y = p1[1];
-            else
-                y = p1[1] + (p2[1] - p1[1]) * (pos.x - p1[0]) / (p2[0] - p1[0]);
-
-            legends.eq(i).text(series.label.replace(/=.*/, "= " + y.toFixed(2)));
-        }
-    }
-    
-    $("#placeholder").bind("plothover",  function (event, pos, item) {
-        latestPosition = pos;
-        if (!updateLegendTimeout)
-            updateLegendTimeout = setTimeout(updateLegend, 50);
-    });
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/turning-series.html b/lib/logstash/web/public/js/flot/examples/turning-series.html
deleted file mode 100644
index f72fe62cc73..00000000000
--- a/lib/logstash/web/public/js/flot/examples/turning-series.html
+++ /dev/null
@@ -1,98 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
- </head>
-    <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:600px;height:300px;"></div>
-
-    <p>Here is an example with real data: military budgets for
-        various countries in constant (2005) million US dollars (source: <a href="http://www.sipri.org/">SIPRI</a>).</p>
-
-    <p>Since all data is available client-side, it's pretty easy to
-       make the plot interactive. Try turning countries on/off with the
-       checkboxes below.</p>
-
-    <p id="choices">Show:</p>
-
-<script id="source" language="javascript" type="text/javascript">
-$(function () {
-    var datasets = {
-        "usa": {
-            label: "USA",
-            data: [[1988, 483994], [1989, 479060], [1990, 457648], [1991, 401949], [1992, 424705], [1993, 402375], [1994, 377867], [1995, 357382], [1996, 337946], [1997, 336185], [1998, 328611], [1999, 329421], [2000, 342172], [2001, 344932], [2002, 387303], [2003, 440813], [2004, 480451], [2005, 504638], [2006, 528692]]
-        },        
-        "russia": {
-            label: "Russia",
-            data: [[1988, 218000], [1989, 203000], [1990, 171000], [1992, 42500], [1993, 37600], [1994, 36600], [1995, 21700], [1996, 19200], [1997, 21300], [1998, 13600], [1999, 14000], [2000, 19100], [2001, 21300], [2002, 23600], [2003, 25100], [2004, 26100], [2005, 31100], [2006, 34700]]
-        },
-        "uk": {
-            label: "UK",
-            data: [[1988, 62982], [1989, 62027], [1990, 60696], [1991, 62348], [1992, 58560], [1993, 56393], [1994, 54579], [1995, 50818], [1996, 50554], [1997, 48276], [1998, 47691], [1999, 47529], [2000, 47778], [2001, 48760], [2002, 50949], [2003, 57452], [2004, 60234], [2005, 60076], [2006, 59213]]
-        },
-        "germany": {
-            label: "Germany",
-            data: [[1988, 55627], [1989, 55475], [1990, 58464], [1991, 55134], [1992, 52436], [1993, 47139], [1994, 43962], [1995, 43238], [1996, 42395], [1997, 40854], [1998, 40993], [1999, 41822], [2000, 41147], [2001, 40474], [2002, 40604], [2003, 40044], [2004, 38816], [2005, 38060], [2006, 36984]]
-        },
-        "denmark": {
-            label: "Denmark",
-            data: [[1988, 3813], [1989, 3719], [1990, 3722], [1991, 3789], [1992, 3720], [1993, 3730], [1994, 3636], [1995, 3598], [1996, 3610], [1997, 3655], [1998, 3695], [1999, 3673], [2000, 3553], [2001, 3774], [2002, 3728], [2003, 3618], [2004, 3638], [2005, 3467], [2006, 3770]]
-        },
-        "sweden": {
-            label: "Sweden",
-            data: [[1988, 6402], [1989, 6474], [1990, 6605], [1991, 6209], [1992, 6035], [1993, 6020], [1994, 6000], [1995, 6018], [1996, 3958], [1997, 5780], [1998, 5954], [1999, 6178], [2000, 6411], [2001, 5993], [2002, 5833], [2003, 5791], [2004, 5450], [2005, 5521], [2006, 5271]]
-        },
-        "norway": {
-            label: "Norway",
-            data: [[1988, 4382], [1989, 4498], [1990, 4535], [1991, 4398], [1992, 4766], [1993, 4441], [1994, 4670], [1995, 4217], [1996, 4275], [1997, 4203], [1998, 4482], [1999, 4506], [2000, 4358], [2001, 4385], [2002, 5269], [2003, 5066], [2004, 5194], [2005, 4887], [2006, 4891]]
-        }
-    };
-
-    // hard-code color indices to prevent them from shifting as
-    // countries are turned on/off
-    var i = 0;
-    $.each(datasets, function(key, val) {
-        val.color = i;
-        ++i;
-    });
-    
-    // insert checkboxes 
-    var choiceContainer = $("#choices");
-    $.each(datasets, function(key, val) {
-        choiceContainer.append('<br/><input type="checkbox" name="' + key +
-                               '" checked="checked" id="id' + key + '">' +
-                               '<label for="id' + key + '">'
-                                + val.label + '</label>');
-    });
-    choiceContainer.find("input").click(plotAccordingToChoices);
-
-    
-    function plotAccordingToChoices() {
-        var data = [];
-
-        choiceContainer.find("input:checked").each(function () {
-            var key = $(this).attr("name");
-            if (key && datasets[key])
-                data.push(datasets[key]);
-        });
-
-        if (data.length > 0)
-            $.plot($("#placeholder"), data, {
-                yaxis: { min: 0 },
-                xaxis: { tickDecimals: 0 }
-            });
-    }
-
-    plotAccordingToChoices();
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/visitors.html b/lib/logstash/web/public/js/flot/examples/visitors.html
deleted file mode 100644
index 2b0aade1be8..00000000000
--- a/lib/logstash/web/public/js/flot/examples/visitors.html
+++ /dev/null
@@ -1,90 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.selection.js"></script>
- </head>
-    <body>
-    <h1>Flot Examples</h1>
-
-    <div id="placeholder" style="width:600px;height:300px;"></div>
-
-    <p>Visitors per day to the Flot homepage. Weekends are colored. Try zooming.
-      The plot below shows an overview.</p>
-
-    <div id="overview" style="margin-left:50px;margin-top:20px;width:400px;height:50px"></div>
-
-<script id="source">
-$(function () {
-    var d = [[1196463600000, 0], [1196550000000, 0], [1196636400000, 0], [1196722800000, 77], [1196809200000, 3636], [1196895600000, 3575], [1196982000000, 2736], [1197068400000, 1086], [1197154800000, 676], [1197241200000, 1205], [1197327600000, 906], [1197414000000, 710], [1197500400000, 639], [1197586800000, 540], [1197673200000, 435], [1197759600000, 301], [1197846000000, 575], [1197932400000, 481], [1198018800000, 591], [1198105200000, 608], [1198191600000, 459], [1198278000000, 234], [1198364400000, 1352], [1198450800000, 686], [1198537200000, 279], [1198623600000, 449], [1198710000000, 468], [1198796400000, 392], [1198882800000, 282], [1198969200000, 208], [1199055600000, 229], [1199142000000, 177], [1199228400000, 374], [1199314800000, 436], [1199401200000, 404], [1199487600000, 253], [1199574000000, 218], [1199660400000, 476], [1199746800000, 462], [1199833200000, 448], [1199919600000, 442], [1200006000000, 403], [1200092400000, 204], [1200178800000, 194], [1200265200000, 327], [1200351600000, 374], [1200438000000, 507], [1200524400000, 546], [1200610800000, 482], [1200697200000, 283], [1200783600000, 221], [1200870000000, 483], [1200956400000, 523], [1201042800000, 528], [1201129200000, 483], [1201215600000, 452], [1201302000000, 270], [1201388400000, 222], [1201474800000, 439], [1201561200000, 559], [1201647600000, 521], [1201734000000, 477], [1201820400000, 442], [1201906800000, 252], [1201993200000, 236], [1202079600000, 525], [1202166000000, 477], [1202252400000, 386], [1202338800000, 409], [1202425200000, 408], [1202511600000, 237], [1202598000000, 193], [1202684400000, 357], [1202770800000, 414], [1202857200000, 393], [1202943600000, 353], [1203030000000, 364], [1203116400000, 215], [1203202800000, 214], [1203289200000, 356], [1203375600000, 399], [1203462000000, 334], [1203548400000, 348], [1203634800000, 243], [1203721200000, 126], [1203807600000, 157], [1203894000000, 288]];
-
-    // first correct the timestamps - they are recorded as the daily
-    // midnights in UTC+0100, but Flot always displays dates in UTC
-    // so we have to add one hour to hit the midnights in the plot
-    for (var i = 0; i < d.length; ++i)
-      d[i][0] += 60 * 60 * 1000;
-
-    // helper for returning the weekends in a period
-    function weekendAreas(axes) {
-        var markings = [];
-        var d = new Date(axes.xaxis.min);
-        // go to the first Saturday
-        d.setUTCDate(d.getUTCDate() - ((d.getUTCDay() + 1) % 7))
-        d.setUTCSeconds(0);
-        d.setUTCMinutes(0);
-        d.setUTCHours(0);
-        var i = d.getTime();
-        do {
-            // when we don't set yaxis, the rectangle automatically
-            // extends to infinity upwards and downwards
-            markings.push({ xaxis: { from: i, to: i + 2 * 24 * 60 * 60 * 1000 } });
-            i += 7 * 24 * 60 * 60 * 1000;
-        } while (i < axes.xaxis.max);
-
-        return markings;
-    }
-    
-    var options = {
-        xaxis: { mode: "time" },
-        selection: { mode: "x" },
-        grid: { markings: weekendAreas }
-    };
-    
-    var plot = $.plot($("#placeholder"), [d], options);
-    
-    var overview = $.plot($("#overview"), [d], {
-        series: {
-            lines: { show: true, lineWidth: 1 },
-            shadowSize: 0
-        },
-        xaxis: { ticks: [], mode: "time" },
-        yaxis: { ticks: [], min: 0, autoscaleMargin: 0.1 },
-        selection: { mode: "x" }
-    });
-
-    // now connect the two
-    
-    $("#placeholder").bind("plotselected", function (event, ranges) {
-        // do the zooming
-        plot = $.plot($("#placeholder"), [d],
-                      $.extend(true, {}, options, {
-                          xaxis: { min: ranges.xaxis.from, max: ranges.xaxis.to }
-                      }));
-
-        // don't fire event on the overview to prevent eternal loop
-        overview.setSelection(ranges, true);
-    });
-    
-    $("#overview").bind("plotselected", function (event, ranges) {
-        plot.setSelection(ranges);
-    });
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/examples/zooming.html b/lib/logstash/web/public/js/flot/examples/zooming.html
deleted file mode 100644
index b4859120456..00000000000
--- a/lib/logstash/web/public/js/flot/examples/zooming.html
+++ /dev/null
@@ -1,98 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<html>
- <head>
-    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-    <title>Flot Examples</title>
-    <link href="layout.css" rel="stylesheet" type="text/css"></link>
-    <!--[if IE]><script language="javascript" type="text/javascript" src="../excanvas.min.js"></script><![endif]-->
-    <script language="javascript" type="text/javascript" src="../jquery.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.js"></script>
-    <script language="javascript" type="text/javascript" src="../jquery.flot.selection.js"></script>
- </head>
-    <body>
-    <h1>Flot Examples</h1>
-
-    <div style="float:left">
-      <div id="placeholder" style="width:500px;height:300px"></div>
-    </div>
-    
-    <div id="miniature" style="float:left;margin-left:20px;margin-top:50px">
-      <div id="overview" style="width:166px;height:100px"></div>
-
-      <p id="overviewLegend" style="margin-left:10px"></p>
-    </div>
-
-    <p style="clear:left"> The selection support makes 
-      pretty advanced zooming schemes possible. With a few lines of code,
-      the small overview plot to the right has been connected to the large
-      plot. Try selecting a rectangle on either of them.</p>
-
-<script id="source">
-$(function () {
-    // setup plot
-    function getData(x1, x2) {
-        var d = [];
-        for (var i = 0; i <= 100; ++i) {
-            var x = x1 + i * (x2 - x1) / 100;
-            d.push([x, Math.sin(x * Math.sin(x))]);
-        }
-
-        return [
-            { label: "sin(x sin(x))", data: d }
-        ];
-    }
-
-    var options = {
-        legend: { show: false },
-        series: {
-            lines: { show: true },
-            points: { show: true }
-        },
-        yaxis: { ticks: 10 },
-        selection: { mode: "xy" }
-    };
-
-    var startData = getData(0, 3 * Math.PI);
-    
-    var plot = $.plot($("#placeholder"), startData, options);
-
-    // setup overview
-    var overview = $.plot($("#overview"), startData, {
-        legend: { show: true, container: $("#overviewLegend") },
-        series: {
-            lines: { show: true, lineWidth: 1 },
-            shadowSize: 0
-        },
-        xaxis: { ticks: 4 },
-        yaxis: { ticks: 3, min: -2, max: 2 },
-        grid: { color: "#999" },
-        selection: { mode: "xy" }
-    });
-
-    // now connect the two
-    
-    $("#placeholder").bind("plotselected", function (event, ranges) {
-        // clamp the zooming to prevent eternal zoom
-        if (ranges.xaxis.to - ranges.xaxis.from < 0.00001)
-            ranges.xaxis.to = ranges.xaxis.from + 0.00001;
-        if (ranges.yaxis.to - ranges.yaxis.from < 0.00001)
-            ranges.yaxis.to = ranges.yaxis.from + 0.00001;
-        
-        // do the zooming
-        plot = $.plot($("#placeholder"), getData(ranges.xaxis.from, ranges.xaxis.to),
-                      $.extend(true, {}, options, {
-                          xaxis: { min: ranges.xaxis.from, max: ranges.xaxis.to },
-                          yaxis: { min: ranges.yaxis.from, max: ranges.yaxis.to }
-                      }));
-        
-        // don't fire event on the overview to prevent eternal loop
-        overview.setSelection(ranges, true);
-    });
-    $("#overview").bind("plotselected", function (event, ranges) {
-        plot.setSelection(ranges);
-    });
-});
-</script>
-
- </body>
-</html>
diff --git a/lib/logstash/web/public/js/flot/excanvas.js b/lib/logstash/web/public/js/flot/excanvas.js
deleted file mode 100644
index c40d6f7014d..00000000000
--- a/lib/logstash/web/public/js/flot/excanvas.js
+++ /dev/null
@@ -1,1427 +0,0 @@
-// Copyright 2006 Google Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//   http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-
-// Known Issues:
-//
-// * Patterns only support repeat.
-// * Radial gradient are not implemented. The VML version of these look very
-//   different from the canvas one.
-// * Clipping paths are not implemented.
-// * Coordsize. The width and height attribute have higher priority than the
-//   width and height style values which isn't correct.
-// * Painting mode isn't implemented.
-// * Canvas width/height should is using content-box by default. IE in
-//   Quirks mode will draw the canvas using border-box. Either change your
-//   doctype to HTML5
-//   (http://www.whatwg.org/specs/web-apps/current-work/#the-doctype)
-//   or use Box Sizing Behavior from WebFX
-//   (http://webfx.eae.net/dhtml/boxsizing/boxsizing.html)
-// * Non uniform scaling does not correctly scale strokes.
-// * Filling very large shapes (above 5000 points) is buggy.
-// * Optimize. There is always room for speed improvements.
-
-// Only add this code if we do not already have a canvas implementation
-if (!document.createElement('canvas').getContext) {
-
-(function() {
-
-  // alias some functions to make (compiled) code shorter
-  var m = Math;
-  var mr = m.round;
-  var ms = m.sin;
-  var mc = m.cos;
-  var abs = m.abs;
-  var sqrt = m.sqrt;
-
-  // this is used for sub pixel precision
-  var Z = 10;
-  var Z2 = Z / 2;
-
-  /**
-   * This funtion is assigned to the <canvas> elements as element.getContext().
-   * @this {HTMLElement}
-   * @return {CanvasRenderingContext2D_}
-   */
-  function getContext() {
-    return this.context_ ||
-        (this.context_ = new CanvasRenderingContext2D_(this));
-  }
-
-  var slice = Array.prototype.slice;
-
-  /**
-   * Binds a function to an object. The returned function will always use the
-   * passed in {@code obj} as {@code this}.
-   *
-   * Example:
-   *
-   *   g = bind(f, obj, a, b)
-   *   g(c, d) // will do f.call(obj, a, b, c, d)
-   *
-   * @param {Function} f The function to bind the object to
-   * @param {Object} obj The object that should act as this when the function
-   *     is called
-   * @param {*} var_args Rest arguments that will be used as the initial
-   *     arguments when the function is called
-   * @return {Function} A new function that has bound this
-   */
-  function bind(f, obj, var_args) {
-    var a = slice.call(arguments, 2);
-    return function() {
-      return f.apply(obj, a.concat(slice.call(arguments)));
-    };
-  }
-
-  function encodeHtmlAttribute(s) {
-    return String(s).replace(/&/g, '&amp;').replace(/"/g, '&quot;');
-  }
-
-  function addNamespacesAndStylesheet(doc) {
-    // create xmlns
-    if (!doc.namespaces['g_vml_']) {
-      doc.namespaces.add('g_vml_', 'urn:schemas-microsoft-com:vml',
-                         '#default#VML');
-
-    }
-    if (!doc.namespaces['g_o_']) {
-      doc.namespaces.add('g_o_', 'urn:schemas-microsoft-com:office:office',
-                         '#default#VML');
-    }
-
-    // Setup default CSS.  Only add one style sheet per document
-    if (!doc.styleSheets['ex_canvas_']) {
-      var ss = doc.createStyleSheet();
-      ss.owningElement.id = 'ex_canvas_';
-      ss.cssText = 'canvas{display:inline-block;overflow:hidden;' +
-          // default size is 300x150 in Gecko and Opera
-          'text-align:left;width:300px;height:150px}';
-    }
-  }
-
-  // Add namespaces and stylesheet at startup.
-  addNamespacesAndStylesheet(document);
-
-  var G_vmlCanvasManager_ = {
-    init: function(opt_doc) {
-      if (/MSIE/.test(navigator.userAgent) && !window.opera) {
-        var doc = opt_doc || document;
-        // Create a dummy element so that IE will allow canvas elements to be
-        // recognized.
-        doc.createElement('canvas');
-        doc.attachEvent('onreadystatechange', bind(this.init_, this, doc));
-      }
-    },
-
-    init_: function(doc) {
-      // find all canvas elements
-      var els = doc.getElementsByTagName('canvas');
-      for (var i = 0; i < els.length; i++) {
-        this.initElement(els[i]);
-      }
-    },
-
-    /**
-     * Public initializes a canvas element so that it can be used as canvas
-     * element from now on. This is called automatically before the page is
-     * loaded but if you are creating elements using createElement you need to
-     * make sure this is called on the element.
-     * @param {HTMLElement} el The canvas element to initialize.
-     * @return {HTMLElement} the element that was created.
-     */
-    initElement: function(el) {
-      if (!el.getContext) {
-        el.getContext = getContext;
-
-        // Add namespaces and stylesheet to document of the element.
-        addNamespacesAndStylesheet(el.ownerDocument);
-
-        // Remove fallback content. There is no way to hide text nodes so we
-        // just remove all childNodes. We could hide all elements and remove
-        // text nodes but who really cares about the fallback content.
-        el.innerHTML = '';
-
-        // do not use inline function because that will leak memory
-        el.attachEvent('onpropertychange', onPropertyChange);
-        el.attachEvent('onresize', onResize);
-
-        var attrs = el.attributes;
-        if (attrs.width && attrs.width.specified) {
-          // TODO: use runtimeStyle and coordsize
-          // el.getContext().setWidth_(attrs.width.nodeValue);
-          el.style.width = attrs.width.nodeValue + 'px';
-        } else {
-          el.width = el.clientWidth;
-        }
-        if (attrs.height && attrs.height.specified) {
-          // TODO: use runtimeStyle and coordsize
-          // el.getContext().setHeight_(attrs.height.nodeValue);
-          el.style.height = attrs.height.nodeValue + 'px';
-        } else {
-          el.height = el.clientHeight;
-        }
-        //el.getContext().setCoordsize_()
-      }
-      return el;
-    }
-  };
-
-  function onPropertyChange(e) {
-    var el = e.srcElement;
-
-    switch (e.propertyName) {
-      case 'width':
-        el.getContext().clearRect();
-        el.style.width = el.attributes.width.nodeValue + 'px';
-        // In IE8 this does not trigger onresize.
-        el.firstChild.style.width =  el.clientWidth + 'px';
-        break;
-      case 'height':
-        el.getContext().clearRect();
-        el.style.height = el.attributes.height.nodeValue + 'px';
-        el.firstChild.style.height = el.clientHeight + 'px';
-        break;
-    }
-  }
-
-  function onResize(e) {
-    var el = e.srcElement;
-    if (el.firstChild) {
-      el.firstChild.style.width =  el.clientWidth + 'px';
-      el.firstChild.style.height = el.clientHeight + 'px';
-    }
-  }
-
-  G_vmlCanvasManager_.init();
-
-  // precompute "00" to "FF"
-  var decToHex = [];
-  for (var i = 0; i < 16; i++) {
-    for (var j = 0; j < 16; j++) {
-      decToHex[i * 16 + j] = i.toString(16) + j.toString(16);
-    }
-  }
-
-  function createMatrixIdentity() {
-    return [
-      [1, 0, 0],
-      [0, 1, 0],
-      [0, 0, 1]
-    ];
-  }
-
-  function matrixMultiply(m1, m2) {
-    var result = createMatrixIdentity();
-
-    for (var x = 0; x < 3; x++) {
-      for (var y = 0; y < 3; y++) {
-        var sum = 0;
-
-        for (var z = 0; z < 3; z++) {
-          sum += m1[x][z] * m2[z][y];
-        }
-
-        result[x][y] = sum;
-      }
-    }
-    return result;
-  }
-
-  function copyState(o1, o2) {
-    o2.fillStyle     = o1.fillStyle;
-    o2.lineCap       = o1.lineCap;
-    o2.lineJoin      = o1.lineJoin;
-    o2.lineWidth     = o1.lineWidth;
-    o2.miterLimit    = o1.miterLimit;
-    o2.shadowBlur    = o1.shadowBlur;
-    o2.shadowColor   = o1.shadowColor;
-    o2.shadowOffsetX = o1.shadowOffsetX;
-    o2.shadowOffsetY = o1.shadowOffsetY;
-    o2.strokeStyle   = o1.strokeStyle;
-    o2.globalAlpha   = o1.globalAlpha;
-    o2.font          = o1.font;
-    o2.textAlign     = o1.textAlign;
-    o2.textBaseline  = o1.textBaseline;
-    o2.arcScaleX_    = o1.arcScaleX_;
-    o2.arcScaleY_    = o1.arcScaleY_;
-    o2.lineScale_    = o1.lineScale_;
-  }
-
-  var colorData = {
-    aliceblue: '#F0F8FF',
-    antiquewhite: '#FAEBD7',
-    aquamarine: '#7FFFD4',
-    azure: '#F0FFFF',
-    beige: '#F5F5DC',
-    bisque: '#FFE4C4',
-    black: '#000000',
-    blanchedalmond: '#FFEBCD',
-    blueviolet: '#8A2BE2',
-    brown: '#A52A2A',
-    burlywood: '#DEB887',
-    cadetblue: '#5F9EA0',
-    chartreuse: '#7FFF00',
-    chocolate: '#D2691E',
-    coral: '#FF7F50',
-    cornflowerblue: '#6495ED',
-    cornsilk: '#FFF8DC',
-    crimson: '#DC143C',
-    cyan: '#00FFFF',
-    darkblue: '#00008B',
-    darkcyan: '#008B8B',
-    darkgoldenrod: '#B8860B',
-    darkgray: '#A9A9A9',
-    darkgreen: '#006400',
-    darkgrey: '#A9A9A9',
-    darkkhaki: '#BDB76B',
-    darkmagenta: '#8B008B',
-    darkolivegreen: '#556B2F',
-    darkorange: '#FF8C00',
-    darkorchid: '#9932CC',
-    darkred: '#8B0000',
-    darksalmon: '#E9967A',
-    darkseagreen: '#8FBC8F',
-    darkslateblue: '#483D8B',
-    darkslategray: '#2F4F4F',
-    darkslategrey: '#2F4F4F',
-    darkturquoise: '#00CED1',
-    darkviolet: '#9400D3',
-    deeppink: '#FF1493',
-    deepskyblue: '#00BFFF',
-    dimgray: '#696969',
-    dimgrey: '#696969',
-    dodgerblue: '#1E90FF',
-    firebrick: '#B22222',
-    floralwhite: '#FFFAF0',
-    forestgreen: '#228B22',
-    gainsboro: '#DCDCDC',
-    ghostwhite: '#F8F8FF',
-    gold: '#FFD700',
-    goldenrod: '#DAA520',
-    grey: '#808080',
-    greenyellow: '#ADFF2F',
-    honeydew: '#F0FFF0',
-    hotpink: '#FF69B4',
-    indianred: '#CD5C5C',
-    indigo: '#4B0082',
-    ivory: '#FFFFF0',
-    khaki: '#F0E68C',
-    lavender: '#E6E6FA',
-    lavenderblush: '#FFF0F5',
-    lawngreen: '#7CFC00',
-    lemonchiffon: '#FFFACD',
-    lightblue: '#ADD8E6',
-    lightcoral: '#F08080',
-    lightcyan: '#E0FFFF',
-    lightgoldenrodyellow: '#FAFAD2',
-    lightgreen: '#90EE90',
-    lightgrey: '#D3D3D3',
-    lightpink: '#FFB6C1',
-    lightsalmon: '#FFA07A',
-    lightseagreen: '#20B2AA',
-    lightskyblue: '#87CEFA',
-    lightslategray: '#778899',
-    lightslategrey: '#778899',
-    lightsteelblue: '#B0C4DE',
-    lightyellow: '#FFFFE0',
-    limegreen: '#32CD32',
-    linen: '#FAF0E6',
-    magenta: '#FF00FF',
-    mediumaquamarine: '#66CDAA',
-    mediumblue: '#0000CD',
-    mediumorchid: '#BA55D3',
-    mediumpurple: '#9370DB',
-    mediumseagreen: '#3CB371',
-    mediumslateblue: '#7B68EE',
-    mediumspringgreen: '#00FA9A',
-    mediumturquoise: '#48D1CC',
-    mediumvioletred: '#C71585',
-    midnightblue: '#191970',
-    mintcream: '#F5FFFA',
-    mistyrose: '#FFE4E1',
-    moccasin: '#FFE4B5',
-    navajowhite: '#FFDEAD',
-    oldlace: '#FDF5E6',
-    olivedrab: '#6B8E23',
-    orange: '#FFA500',
-    orangered: '#FF4500',
-    orchid: '#DA70D6',
-    palegoldenrod: '#EEE8AA',
-    palegreen: '#98FB98',
-    paleturquoise: '#AFEEEE',
-    palevioletred: '#DB7093',
-    papayawhip: '#FFEFD5',
-    peachpuff: '#FFDAB9',
-    peru: '#CD853F',
-    pink: '#FFC0CB',
-    plum: '#DDA0DD',
-    powderblue: '#B0E0E6',
-    rosybrown: '#BC8F8F',
-    royalblue: '#4169E1',
-    saddlebrown: '#8B4513',
-    salmon: '#FA8072',
-    sandybrown: '#F4A460',
-    seagreen: '#2E8B57',
-    seashell: '#FFF5EE',
-    sienna: '#A0522D',
-    skyblue: '#87CEEB',
-    slateblue: '#6A5ACD',
-    slategray: '#708090',
-    slategrey: '#708090',
-    snow: '#FFFAFA',
-    springgreen: '#00FF7F',
-    steelblue: '#4682B4',
-    tan: '#D2B48C',
-    thistle: '#D8BFD8',
-    tomato: '#FF6347',
-    turquoise: '#40E0D0',
-    violet: '#EE82EE',
-    wheat: '#F5DEB3',
-    whitesmoke: '#F5F5F5',
-    yellowgreen: '#9ACD32'
-  };
-
-
-  function getRgbHslContent(styleString) {
-    var start = styleString.indexOf('(', 3);
-    var end = styleString.indexOf(')', start + 1);
-    var parts = styleString.substring(start + 1, end).split(',');
-    // add alpha if needed
-    if (parts.length == 4 && styleString.substr(3, 1) == 'a') {
-      alpha = Number(parts[3]);
-    } else {
-      parts[3] = 1;
-    }
-    return parts;
-  }
-
-  function percent(s) {
-    return parseFloat(s) / 100;
-  }
-
-  function clamp(v, min, max) {
-    return Math.min(max, Math.max(min, v));
-  }
-
-  function hslToRgb(parts){
-    var r, g, b;
-    h = parseFloat(parts[0]) / 360 % 360;
-    if (h < 0)
-      h++;
-    s = clamp(percent(parts[1]), 0, 1);
-    l = clamp(percent(parts[2]), 0, 1);
-    if (s == 0) {
-      r = g = b = l; // achromatic
-    } else {
-      var q = l < 0.5 ? l * (1 + s) : l + s - l * s;
-      var p = 2 * l - q;
-      r = hueToRgb(p, q, h + 1 / 3);
-      g = hueToRgb(p, q, h);
-      b = hueToRgb(p, q, h - 1 / 3);
-    }
-
-    return '#' + decToHex[Math.floor(r * 255)] +
-        decToHex[Math.floor(g * 255)] +
-        decToHex[Math.floor(b * 255)];
-  }
-
-  function hueToRgb(m1, m2, h) {
-    if (h < 0)
-      h++;
-    if (h > 1)
-      h--;
-
-    if (6 * h < 1)
-      return m1 + (m2 - m1) * 6 * h;
-    else if (2 * h < 1)
-      return m2;
-    else if (3 * h < 2)
-      return m1 + (m2 - m1) * (2 / 3 - h) * 6;
-    else
-      return m1;
-  }
-
-  function processStyle(styleString) {
-    var str, alpha = 1;
-
-    styleString = String(styleString);
-    if (styleString.charAt(0) == '#') {
-      str = styleString;
-    } else if (/^rgb/.test(styleString)) {
-      var parts = getRgbHslContent(styleString);
-      var str = '#', n;
-      for (var i = 0; i < 3; i++) {
-        if (parts[i].indexOf('%') != -1) {
-          n = Math.floor(percent(parts[i]) * 255);
-        } else {
-          n = Number(parts[i]);
-        }
-        str += decToHex[clamp(n, 0, 255)];
-      }
-      alpha = parts[3];
-    } else if (/^hsl/.test(styleString)) {
-      var parts = getRgbHslContent(styleString);
-      str = hslToRgb(parts);
-      alpha = parts[3];
-    } else {
-      str = colorData[styleString] || styleString;
-    }
-    return {color: str, alpha: alpha};
-  }
-
-  var DEFAULT_STYLE = {
-    style: 'normal',
-    variant: 'normal',
-    weight: 'normal',
-    size: 10,
-    family: 'sans-serif'
-  };
-
-  // Internal text style cache
-  var fontStyleCache = {};
-
-  function processFontStyle(styleString) {
-    if (fontStyleCache[styleString]) {
-      return fontStyleCache[styleString];
-    }
-
-    var el = document.createElement('div');
-    var style = el.style;
-    try {
-      style.font = styleString;
-    } catch (ex) {
-      // Ignore failures to set to invalid font.
-    }
-
-    return fontStyleCache[styleString] = {
-      style: style.fontStyle || DEFAULT_STYLE.style,
-      variant: style.fontVariant || DEFAULT_STYLE.variant,
-      weight: style.fontWeight || DEFAULT_STYLE.weight,
-      size: style.fontSize || DEFAULT_STYLE.size,
-      family: style.fontFamily || DEFAULT_STYLE.family
-    };
-  }
-
-  function getComputedStyle(style, element) {
-    var computedStyle = {};
-
-    for (var p in style) {
-      computedStyle[p] = style[p];
-    }
-
-    // Compute the size
-    var canvasFontSize = parseFloat(element.currentStyle.fontSize),
-        fontSize = parseFloat(style.size);
-
-    if (typeof style.size == 'number') {
-      computedStyle.size = style.size;
-    } else if (style.size.indexOf('px') != -1) {
-      computedStyle.size = fontSize;
-    } else if (style.size.indexOf('em') != -1) {
-      computedStyle.size = canvasFontSize * fontSize;
-    } else if(style.size.indexOf('%') != -1) {
-      computedStyle.size = (canvasFontSize / 100) * fontSize;
-    } else if (style.size.indexOf('pt') != -1) {
-      computedStyle.size = fontSize / .75;
-    } else {
-      computedStyle.size = canvasFontSize;
-    }
-
-    // Different scaling between normal text and VML text. This was found using
-    // trial and error to get the same size as non VML text.
-    computedStyle.size *= 0.981;
-
-    return computedStyle;
-  }
-
-  function buildStyle(style) {
-    return style.style + ' ' + style.variant + ' ' + style.weight + ' ' +
-        style.size + 'px ' + style.family;
-  }
-
-  function processLineCap(lineCap) {
-    switch (lineCap) {
-      case 'butt':
-        return 'flat';
-      case 'round':
-        return 'round';
-      case 'square':
-      default:
-        return 'square';
-    }
-  }
-
-  /**
-   * This class implements CanvasRenderingContext2D interface as described by
-   * the WHATWG.
-   * @param {HTMLElement} surfaceElement The element that the 2D context should
-   * be associated with
-   */
-  function CanvasRenderingContext2D_(surfaceElement) {
-    this.m_ = createMatrixIdentity();
-
-    this.mStack_ = [];
-    this.aStack_ = [];
-    this.currentPath_ = [];
-
-    // Canvas context properties
-    this.strokeStyle = '#000';
-    this.fillStyle = '#000';
-
-    this.lineWidth = 1;
-    this.lineJoin = 'miter';
-    this.lineCap = 'butt';
-    this.miterLimit = Z * 1;
-    this.globalAlpha = 1;
-    this.font = '10px sans-serif';
-    this.textAlign = 'left';
-    this.textBaseline = 'alphabetic';
-    this.canvas = surfaceElement;
-
-    var el = surfaceElement.ownerDocument.createElement('div');
-    el.style.width =  surfaceElement.clientWidth + 'px';
-    el.style.height = surfaceElement.clientHeight + 'px';
-    el.style.overflow = 'hidden';
-    el.style.position = 'absolute';
-    surfaceElement.appendChild(el);
-
-    this.element_ = el;
-    this.arcScaleX_ = 1;
-    this.arcScaleY_ = 1;
-    this.lineScale_ = 1;
-  }
-
-  var contextPrototype = CanvasRenderingContext2D_.prototype;
-  contextPrototype.clearRect = function() {
-    if (this.textMeasureEl_) {
-      this.textMeasureEl_.removeNode(true);
-      this.textMeasureEl_ = null;
-    }
-    this.element_.innerHTML = '';
-  };
-
-  contextPrototype.beginPath = function() {
-    // TODO: Branch current matrix so that save/restore has no effect
-    //       as per safari docs.
-    this.currentPath_ = [];
-  };
-
-  contextPrototype.moveTo = function(aX, aY) {
-    var p = this.getCoords_(aX, aY);
-    this.currentPath_.push({type: 'moveTo', x: p.x, y: p.y});
-    this.currentX_ = p.x;
-    this.currentY_ = p.y;
-  };
-
-  contextPrototype.lineTo = function(aX, aY) {
-    var p = this.getCoords_(aX, aY);
-    this.currentPath_.push({type: 'lineTo', x: p.x, y: p.y});
-
-    this.currentX_ = p.x;
-    this.currentY_ = p.y;
-  };
-
-  contextPrototype.bezierCurveTo = function(aCP1x, aCP1y,
-                                            aCP2x, aCP2y,
-                                            aX, aY) {
-    var p = this.getCoords_(aX, aY);
-    var cp1 = this.getCoords_(aCP1x, aCP1y);
-    var cp2 = this.getCoords_(aCP2x, aCP2y);
-    bezierCurveTo(this, cp1, cp2, p);
-  };
-
-  // Helper function that takes the already fixed cordinates.
-  function bezierCurveTo(self, cp1, cp2, p) {
-    self.currentPath_.push({
-      type: 'bezierCurveTo',
-      cp1x: cp1.x,
-      cp1y: cp1.y,
-      cp2x: cp2.x,
-      cp2y: cp2.y,
-      x: p.x,
-      y: p.y
-    });
-    self.currentX_ = p.x;
-    self.currentY_ = p.y;
-  }
-
-  contextPrototype.quadraticCurveTo = function(aCPx, aCPy, aX, aY) {
-    // the following is lifted almost directly from
-    // http://developer.mozilla.org/en/docs/Canvas_tutorial:Drawing_shapes
-
-    var cp = this.getCoords_(aCPx, aCPy);
-    var p = this.getCoords_(aX, aY);
-
-    var cp1 = {
-      x: this.currentX_ + 2.0 / 3.0 * (cp.x - this.currentX_),
-      y: this.currentY_ + 2.0 / 3.0 * (cp.y - this.currentY_)
-    };
-    var cp2 = {
-      x: cp1.x + (p.x - this.currentX_) / 3.0,
-      y: cp1.y + (p.y - this.currentY_) / 3.0
-    };
-
-    bezierCurveTo(this, cp1, cp2, p);
-  };
-
-  contextPrototype.arc = function(aX, aY, aRadius,
-                                  aStartAngle, aEndAngle, aClockwise) {
-    aRadius *= Z;
-    var arcType = aClockwise ? 'at' : 'wa';
-
-    var xStart = aX + mc(aStartAngle) * aRadius - Z2;
-    var yStart = aY + ms(aStartAngle) * aRadius - Z2;
-
-    var xEnd = aX + mc(aEndAngle) * aRadius - Z2;
-    var yEnd = aY + ms(aEndAngle) * aRadius - Z2;
-
-    // IE won't render arches drawn counter clockwise if xStart == xEnd.
-    if (xStart == xEnd && !aClockwise) {
-      xStart += 0.125; // Offset xStart by 1/80 of a pixel. Use something
-                       // that can be represented in binary
-    }
-
-    var p = this.getCoords_(aX, aY);
-    var pStart = this.getCoords_(xStart, yStart);
-    var pEnd = this.getCoords_(xEnd, yEnd);
-
-    this.currentPath_.push({type: arcType,
-                           x: p.x,
-                           y: p.y,
-                           radius: aRadius,
-                           xStart: pStart.x,
-                           yStart: pStart.y,
-                           xEnd: pEnd.x,
-                           yEnd: pEnd.y});
-
-  };
-
-  contextPrototype.rect = function(aX, aY, aWidth, aHeight) {
-    this.moveTo(aX, aY);
-    this.lineTo(aX + aWidth, aY);
-    this.lineTo(aX + aWidth, aY + aHeight);
-    this.lineTo(aX, aY + aHeight);
-    this.closePath();
-  };
-
-  contextPrototype.strokeRect = function(aX, aY, aWidth, aHeight) {
-    var oldPath = this.currentPath_;
-    this.beginPath();
-
-    this.moveTo(aX, aY);
-    this.lineTo(aX + aWidth, aY);
-    this.lineTo(aX + aWidth, aY + aHeight);
-    this.lineTo(aX, aY + aHeight);
-    this.closePath();
-    this.stroke();
-
-    this.currentPath_ = oldPath;
-  };
-
-  contextPrototype.fillRect = function(aX, aY, aWidth, aHeight) {
-    var oldPath = this.currentPath_;
-    this.beginPath();
-
-    this.moveTo(aX, aY);
-    this.lineTo(aX + aWidth, aY);
-    this.lineTo(aX + aWidth, aY + aHeight);
-    this.lineTo(aX, aY + aHeight);
-    this.closePath();
-    this.fill();
-
-    this.currentPath_ = oldPath;
-  };
-
-  contextPrototype.createLinearGradient = function(aX0, aY0, aX1, aY1) {
-    var gradient = new CanvasGradient_('gradient');
-    gradient.x0_ = aX0;
-    gradient.y0_ = aY0;
-    gradient.x1_ = aX1;
-    gradient.y1_ = aY1;
-    return gradient;
-  };
-
-  contextPrototype.createRadialGradient = function(aX0, aY0, aR0,
-                                                   aX1, aY1, aR1) {
-    var gradient = new CanvasGradient_('gradientradial');
-    gradient.x0_ = aX0;
-    gradient.y0_ = aY0;
-    gradient.r0_ = aR0;
-    gradient.x1_ = aX1;
-    gradient.y1_ = aY1;
-    gradient.r1_ = aR1;
-    return gradient;
-  };
-
-  contextPrototype.drawImage = function(image, var_args) {
-    var dx, dy, dw, dh, sx, sy, sw, sh;
-
-    // to find the original width we overide the width and height
-    var oldRuntimeWidth = image.runtimeStyle.width;
-    var oldRuntimeHeight = image.runtimeStyle.height;
-    image.runtimeStyle.width = 'auto';
-    image.runtimeStyle.height = 'auto';
-
-    // get the original size
-    var w = image.width;
-    var h = image.height;
-
-    // and remove overides
-    image.runtimeStyle.width = oldRuntimeWidth;
-    image.runtimeStyle.height = oldRuntimeHeight;
-
-    if (arguments.length == 3) {
-      dx = arguments[1];
-      dy = arguments[2];
-      sx = sy = 0;
-      sw = dw = w;
-      sh = dh = h;
-    } else if (arguments.length == 5) {
-      dx = arguments[1];
-      dy = arguments[2];
-      dw = arguments[3];
-      dh = arguments[4];
-      sx = sy = 0;
-      sw = w;
-      sh = h;
-    } else if (arguments.length == 9) {
-      sx = arguments[1];
-      sy = arguments[2];
-      sw = arguments[3];
-      sh = arguments[4];
-      dx = arguments[5];
-      dy = arguments[6];
-      dw = arguments[7];
-      dh = arguments[8];
-    } else {
-      throw Error('Invalid number of arguments');
-    }
-
-    var d = this.getCoords_(dx, dy);
-
-    var w2 = sw / 2;
-    var h2 = sh / 2;
-
-    var vmlStr = [];
-
-    var W = 10;
-    var H = 10;
-
-    // For some reason that I've now forgotten, using divs didn't work
-    vmlStr.push(' <g_vml_:group',
-                ' coordsize="', Z * W, ',', Z * H, '"',
-                ' coordorigin="0,0"' ,
-                ' style="width:', W, 'px;height:', H, 'px;position:absolute;');
-
-    // If filters are necessary (rotation exists), create them
-    // filters are bog-slow, so only create them if abbsolutely necessary
-    // The following check doesn't account for skews (which don't exist
-    // in the canvas spec (yet) anyway.
-
-    if (this.m_[0][0] != 1 || this.m_[0][1] ||
-        this.m_[1][1] != 1 || this.m_[1][0]) {
-      var filter = [];
-
-      // Note the 12/21 reversal
-      filter.push('M11=', this.m_[0][0], ',',
-                  'M12=', this.m_[1][0], ',',
-                  'M21=', this.m_[0][1], ',',
-                  'M22=', this.m_[1][1], ',',
-                  'Dx=', mr(d.x / Z), ',',
-                  'Dy=', mr(d.y / Z), '');
-
-      // Bounding box calculation (need to minimize displayed area so that
-      // filters don't waste time on unused pixels.
-      var max = d;
-      var c2 = this.getCoords_(dx + dw, dy);
-      var c3 = this.getCoords_(dx, dy + dh);
-      var c4 = this.getCoords_(dx + dw, dy + dh);
-
-      max.x = m.max(max.x, c2.x, c3.x, c4.x);
-      max.y = m.max(max.y, c2.y, c3.y, c4.y);
-
-      vmlStr.push('padding:0 ', mr(max.x / Z), 'px ', mr(max.y / Z),
-                  'px 0;filter:progid:DXImageTransform.Microsoft.Matrix(',
-                  filter.join(''), ", sizingmethod='clip');");
-
-    } else {
-      vmlStr.push('top:', mr(d.y / Z), 'px;left:', mr(d.x / Z), 'px;');
-    }
-
-    vmlStr.push(' ">' ,
-                '<g_vml_:image src="', image.src, '"',
-                ' style="width:', Z * dw, 'px;',
-                ' height:', Z * dh, 'px"',
-                ' cropleft="', sx / w, '"',
-                ' croptop="', sy / h, '"',
-                ' cropright="', (w - sx - sw) / w, '"',
-                ' cropbottom="', (h - sy - sh) / h, '"',
-                ' />',
-                '</g_vml_:group>');
-
-    this.element_.insertAdjacentHTML('BeforeEnd', vmlStr.join(''));
-  };
-
-  contextPrototype.stroke = function(aFill) {
-    var W = 10;
-    var H = 10;
-    // Divide the shape into chunks if it's too long because IE has a limit
-    // somewhere for how long a VML shape can be. This simple division does
-    // not work with fills, only strokes, unfortunately.
-    var chunkSize = 5000;
-
-    var min = {x: null, y: null};
-    var max = {x: null, y: null};
-
-    for (var j = 0; j < this.currentPath_.length; j += chunkSize) {
-      var lineStr = [];
-      var lineOpen = false;
-
-      lineStr.push('<g_vml_:shape',
-                   ' filled="', !!aFill, '"',
-                   ' style="position:absolute;width:', W, 'px;height:', H, 'px;"',
-                   ' coordorigin="0,0"',
-                   ' coordsize="', Z * W, ',', Z * H, '"',
-                   ' stroked="', !aFill, '"',
-                   ' path="');
-
-      var newSeq = false;
-
-      for (var i = j; i < Math.min(j + chunkSize, this.currentPath_.length); i++) {
-        if (i % chunkSize == 0 && i > 0) { // move into position for next chunk
-          lineStr.push(' m ', mr(this.currentPath_[i-1].x), ',', mr(this.currentPath_[i-1].y));
-        }
-
-        var p = this.currentPath_[i];
-        var c;
-
-        switch (p.type) {
-          case 'moveTo':
-            c = p;
-            lineStr.push(' m ', mr(p.x), ',', mr(p.y));
-            break;
-          case 'lineTo':
-            lineStr.push(' l ', mr(p.x), ',', mr(p.y));
-            break;
-          case 'close':
-            lineStr.push(' x ');
-            p = null;
-            break;
-          case 'bezierCurveTo':
-            lineStr.push(' c ',
-                         mr(p.cp1x), ',', mr(p.cp1y), ',',
-                         mr(p.cp2x), ',', mr(p.cp2y), ',',
-                         mr(p.x), ',', mr(p.y));
-            break;
-          case 'at':
-          case 'wa':
-            lineStr.push(' ', p.type, ' ',
-                         mr(p.x - this.arcScaleX_ * p.radius), ',',
-                         mr(p.y - this.arcScaleY_ * p.radius), ' ',
-                         mr(p.x + this.arcScaleX_ * p.radius), ',',
-                         mr(p.y + this.arcScaleY_ * p.radius), ' ',
-                         mr(p.xStart), ',', mr(p.yStart), ' ',
-                         mr(p.xEnd), ',', mr(p.yEnd));
-            break;
-        }
-  
-  
-        // TODO: Following is broken for curves due to
-        //       move to proper paths.
-  
-        // Figure out dimensions so we can do gradient fills
-        // properly
-        if (p) {
-          if (min.x == null || p.x < min.x) {
-            min.x = p.x;
-          }
-          if (max.x == null || p.x > max.x) {
-            max.x = p.x;
-          }
-          if (min.y == null || p.y < min.y) {
-            min.y = p.y;
-          }
-          if (max.y == null || p.y > max.y) {
-            max.y = p.y;
-          }
-        }
-      }
-      lineStr.push(' ">');
-  
-      if (!aFill) {
-        appendStroke(this, lineStr);
-      } else {
-        appendFill(this, lineStr, min, max);
-      }
-  
-      lineStr.push('</g_vml_:shape>');
-  
-      this.element_.insertAdjacentHTML('beforeEnd', lineStr.join(''));
-    }
-  };
-
-  function appendStroke(ctx, lineStr) {
-    var a = processStyle(ctx.strokeStyle);
-    var color = a.color;
-    var opacity = a.alpha * ctx.globalAlpha;
-    var lineWidth = ctx.lineScale_ * ctx.lineWidth;
-
-    // VML cannot correctly render a line if the width is less than 1px.
-    // In that case, we dilute the color to make the line look thinner.
-    if (lineWidth < 1) {
-      opacity *= lineWidth;
-    }
-
-    lineStr.push(
-      '<g_vml_:stroke',
-      ' opacity="', opacity, '"',
-      ' joinstyle="', ctx.lineJoin, '"',
-      ' miterlimit="', ctx.miterLimit, '"',
-      ' endcap="', processLineCap(ctx.lineCap), '"',
-      ' weight="', lineWidth, 'px"',
-      ' color="', color, '" />'
-    );
-  }
-
-  function appendFill(ctx, lineStr, min, max) {
-    var fillStyle = ctx.fillStyle;
-    var arcScaleX = ctx.arcScaleX_;
-    var arcScaleY = ctx.arcScaleY_;
-    var width = max.x - min.x;
-    var height = max.y - min.y;
-    if (fillStyle instanceof CanvasGradient_) {
-      // TODO: Gradients transformed with the transformation matrix.
-      var angle = 0;
-      var focus = {x: 0, y: 0};
-
-      // additional offset
-      var shift = 0;
-      // scale factor for offset
-      var expansion = 1;
-
-      if (fillStyle.type_ == 'gradient') {
-        var x0 = fillStyle.x0_ / arcScaleX;
-        var y0 = fillStyle.y0_ / arcScaleY;
-        var x1 = fillStyle.x1_ / arcScaleX;
-        var y1 = fillStyle.y1_ / arcScaleY;
-        var p0 = ctx.getCoords_(x0, y0);
-        var p1 = ctx.getCoords_(x1, y1);
-        var dx = p1.x - p0.x;
-        var dy = p1.y - p0.y;
-        angle = Math.atan2(dx, dy) * 180 / Math.PI;
-
-        // The angle should be a non-negative number.
-        if (angle < 0) {
-          angle += 360;
-        }
-
-        // Very small angles produce an unexpected result because they are
-        // converted to a scientific notation string.
-        if (angle < 1e-6) {
-          angle = 0;
-        }
-      } else {
-        var p0 = ctx.getCoords_(fillStyle.x0_, fillStyle.y0_);
-        focus = {
-          x: (p0.x - min.x) / width,
-          y: (p0.y - min.y) / height
-        };
-
-        width  /= arcScaleX * Z;
-        height /= arcScaleY * Z;
-        var dimension = m.max(width, height);
-        shift = 2 * fillStyle.r0_ / dimension;
-        expansion = 2 * fillStyle.r1_ / dimension - shift;
-      }
-
-      // We need to sort the color stops in ascending order by offset,
-      // otherwise IE won't interpret it correctly.
-      var stops = fillStyle.colors_;
-      stops.sort(function(cs1, cs2) {
-        return cs1.offset - cs2.offset;
-      });
-
-      var length = stops.length;
-      var color1 = stops[0].color;
-      var color2 = stops[length - 1].color;
-      var opacity1 = stops[0].alpha * ctx.globalAlpha;
-      var opacity2 = stops[length - 1].alpha * ctx.globalAlpha;
-
-      var colors = [];
-      for (var i = 0; i < length; i++) {
-        var stop = stops[i];
-        colors.push(stop.offset * expansion + shift + ' ' + stop.color);
-      }
-
-      // When colors attribute is used, the meanings of opacity and o:opacity2
-      // are reversed.
-      lineStr.push('<g_vml_:fill type="', fillStyle.type_, '"',
-                   ' method="none" focus="100%"',
-                   ' color="', color1, '"',
-                   ' color2="', color2, '"',
-                   ' colors="', colors.join(','), '"',
-                   ' opacity="', opacity2, '"',
-                   ' g_o_:opacity2="', opacity1, '"',
-                   ' angle="', angle, '"',
-                   ' focusposition="', focus.x, ',', focus.y, '" />');
-    } else if (fillStyle instanceof CanvasPattern_) {
-      if (width && height) {
-        var deltaLeft = -min.x;
-        var deltaTop = -min.y;
-        lineStr.push('<g_vml_:fill',
-                     ' position="',
-                     deltaLeft / width * arcScaleX * arcScaleX, ',',
-                     deltaTop / height * arcScaleY * arcScaleY, '"',
-                     ' type="tile"',
-                     // TODO: Figure out the correct size to fit the scale.
-                     //' size="', w, 'px ', h, 'px"',
-                     ' src="', fillStyle.src_, '" />');
-       }
-    } else {
-      var a = processStyle(ctx.fillStyle);
-      var color = a.color;
-      var opacity = a.alpha * ctx.globalAlpha;
-      lineStr.push('<g_vml_:fill color="', color, '" opacity="', opacity,
-                   '" />');
-    }
-  }
-
-  contextPrototype.fill = function() {
-    this.stroke(true);
-  };
-
-  contextPrototype.closePath = function() {
-    this.currentPath_.push({type: 'close'});
-  };
-
-  /**
-   * @private
-   */
-  contextPrototype.getCoords_ = function(aX, aY) {
-    var m = this.m_;
-    return {
-      x: Z * (aX * m[0][0] + aY * m[1][0] + m[2][0]) - Z2,
-      y: Z * (aX * m[0][1] + aY * m[1][1] + m[2][1]) - Z2
-    };
-  };
-
-  contextPrototype.save = function() {
-    var o = {};
-    copyState(this, o);
-    this.aStack_.push(o);
-    this.mStack_.push(this.m_);
-    this.m_ = matrixMultiply(createMatrixIdentity(), this.m_);
-  };
-
-  contextPrototype.restore = function() {
-    if (this.aStack_.length) {
-      copyState(this.aStack_.pop(), this);
-      this.m_ = this.mStack_.pop();
-    }
-  };
-
-  function matrixIsFinite(m) {
-    return isFinite(m[0][0]) && isFinite(m[0][1]) &&
-        isFinite(m[1][0]) && isFinite(m[1][1]) &&
-        isFinite(m[2][0]) && isFinite(m[2][1]);
-  }
-
-  function setM(ctx, m, updateLineScale) {
-    if (!matrixIsFinite(m)) {
-      return;
-    }
-    ctx.m_ = m;
-
-    if (updateLineScale) {
-      // Get the line scale.
-      // Determinant of this.m_ means how much the area is enlarged by the
-      // transformation. So its square root can be used as a scale factor
-      // for width.
-      var det = m[0][0] * m[1][1] - m[0][1] * m[1][0];
-      ctx.lineScale_ = sqrt(abs(det));
-    }
-  }
-
-  contextPrototype.translate = function(aX, aY) {
-    var m1 = [
-      [1,  0,  0],
-      [0,  1,  0],
-      [aX, aY, 1]
-    ];
-
-    setM(this, matrixMultiply(m1, this.m_), false);
-  };
-
-  contextPrototype.rotate = function(aRot) {
-    var c = mc(aRot);
-    var s = ms(aRot);
-
-    var m1 = [
-      [c,  s, 0],
-      [-s, c, 0],
-      [0,  0, 1]
-    ];
-
-    setM(this, matrixMultiply(m1, this.m_), false);
-  };
-
-  contextPrototype.scale = function(aX, aY) {
-    this.arcScaleX_ *= aX;
-    this.arcScaleY_ *= aY;
-    var m1 = [
-      [aX, 0,  0],
-      [0,  aY, 0],
-      [0,  0,  1]
-    ];
-
-    setM(this, matrixMultiply(m1, this.m_), true);
-  };
-
-  contextPrototype.transform = function(m11, m12, m21, m22, dx, dy) {
-    var m1 = [
-      [m11, m12, 0],
-      [m21, m22, 0],
-      [dx,  dy,  1]
-    ];
-
-    setM(this, matrixMultiply(m1, this.m_), true);
-  };
-
-  contextPrototype.setTransform = function(m11, m12, m21, m22, dx, dy) {
-    var m = [
-      [m11, m12, 0],
-      [m21, m22, 0],
-      [dx,  dy,  1]
-    ];
-
-    setM(this, m, true);
-  };
-
-  /**
-   * The text drawing function.
-   * The maxWidth argument isn't taken in account, since no browser supports
-   * it yet.
-   */
-  contextPrototype.drawText_ = function(text, x, y, maxWidth, stroke) {
-    var m = this.m_,
-        delta = 1000,
-        left = 0,
-        right = delta,
-        offset = {x: 0, y: 0},
-        lineStr = [];
-
-    var fontStyle = getComputedStyle(processFontStyle(this.font),
-                                     this.element_);
-
-    var fontStyleString = buildStyle(fontStyle);
-
-    var elementStyle = this.element_.currentStyle;
-    var textAlign = this.textAlign.toLowerCase();
-    switch (textAlign) {
-      case 'left':
-      case 'center':
-      case 'right':
-        break;
-      case 'end':
-        textAlign = elementStyle.direction == 'ltr' ? 'right' : 'left';
-        break;
-      case 'start':
-        textAlign = elementStyle.direction == 'rtl' ? 'right' : 'left';
-        break;
-      default:
-        textAlign = 'left';
-    }
-
-    // 1.75 is an arbitrary number, as there is no info about the text baseline
-    switch (this.textBaseline) {
-      case 'hanging':
-      case 'top':
-        offset.y = fontStyle.size / 1.75;
-        break;
-      case 'middle':
-        break;
-      default:
-      case null:
-      case 'alphabetic':
-      case 'ideographic':
-      case 'bottom':
-        offset.y = -fontStyle.size / 2.25;
-        break;
-    }
-
-    switch(textAlign) {
-      case 'right':
-        left = delta;
-        right = 0.05;
-        break;
-      case 'center':
-        left = right = delta / 2;
-        break;
-    }
-
-    var d = this.getCoords_(x + offset.x, y + offset.y);
-
-    lineStr.push('<g_vml_:line from="', -left ,' 0" to="', right ,' 0.05" ',
-                 ' coordsize="100 100" coordorigin="0 0"',
-                 ' filled="', !stroke, '" stroked="', !!stroke,
-                 '" style="position:absolute;width:1px;height:1px;">');
-
-    if (stroke) {
-      appendStroke(this, lineStr);
-    } else {
-      // TODO: Fix the min and max params.
-      appendFill(this, lineStr, {x: -left, y: 0},
-                 {x: right, y: fontStyle.size});
-    }
-
-    var skewM = m[0][0].toFixed(3) + ',' + m[1][0].toFixed(3) + ',' +
-                m[0][1].toFixed(3) + ',' + m[1][1].toFixed(3) + ',0,0';
-
-    var skewOffset = mr(d.x / Z) + ',' + mr(d.y / Z);
-
-    lineStr.push('<g_vml_:skew on="t" matrix="', skewM ,'" ',
-                 ' offset="', skewOffset, '" origin="', left ,' 0" />',
-                 '<g_vml_:path textpathok="true" />',
-                 '<g_vml_:textpath on="true" string="',
-                 encodeHtmlAttribute(text),
-                 '" style="v-text-align:', textAlign,
-                 ';font:', encodeHtmlAttribute(fontStyleString),
-                 '" /></g_vml_:line>');
-
-    this.element_.insertAdjacentHTML('beforeEnd', lineStr.join(''));
-  };
-
-  contextPrototype.fillText = function(text, x, y, maxWidth) {
-    this.drawText_(text, x, y, maxWidth, false);
-  };
-
-  contextPrototype.strokeText = function(text, x, y, maxWidth) {
-    this.drawText_(text, x, y, maxWidth, true);
-  };
-
-  contextPrototype.measureText = function(text) {
-    if (!this.textMeasureEl_) {
-      var s = '<span style="position:absolute;' +
-          'top:-20000px;left:0;padding:0;margin:0;border:none;' +
-          'white-space:pre;"></span>';
-      this.element_.insertAdjacentHTML('beforeEnd', s);
-      this.textMeasureEl_ = this.element_.lastChild;
-    }
-    var doc = this.element_.ownerDocument;
-    this.textMeasureEl_.innerHTML = '';
-    this.textMeasureEl_.style.font = this.font;
-    // Don't use innerHTML or innerText because they allow markup/whitespace.
-    this.textMeasureEl_.appendChild(doc.createTextNode(text));
-    return {width: this.textMeasureEl_.offsetWidth};
-  };
-
-  /******** STUBS ********/
-  contextPrototype.clip = function() {
-    // TODO: Implement
-  };
-
-  contextPrototype.arcTo = function() {
-    // TODO: Implement
-  };
-
-  contextPrototype.createPattern = function(image, repetition) {
-    return new CanvasPattern_(image, repetition);
-  };
-
-  // Gradient / Pattern Stubs
-  function CanvasGradient_(aType) {
-    this.type_ = aType;
-    this.x0_ = 0;
-    this.y0_ = 0;
-    this.r0_ = 0;
-    this.x1_ = 0;
-    this.y1_ = 0;
-    this.r1_ = 0;
-    this.colors_ = [];
-  }
-
-  CanvasGradient_.prototype.addColorStop = function(aOffset, aColor) {
-    aColor = processStyle(aColor);
-    this.colors_.push({offset: aOffset,
-                       color: aColor.color,
-                       alpha: aColor.alpha});
-  };
-
-  function CanvasPattern_(image, repetition) {
-    assertImageIsValid(image);
-    switch (repetition) {
-      case 'repeat':
-      case null:
-      case '':
-        this.repetition_ = 'repeat';
-        break
-      case 'repeat-x':
-      case 'repeat-y':
-      case 'no-repeat':
-        this.repetition_ = repetition;
-        break;
-      default:
-        throwException('SYNTAX_ERR');
-    }
-
-    this.src_ = image.src;
-    this.width_ = image.width;
-    this.height_ = image.height;
-  }
-
-  function throwException(s) {
-    throw new DOMException_(s);
-  }
-
-  function assertImageIsValid(img) {
-    if (!img || img.nodeType != 1 || img.tagName != 'IMG') {
-      throwException('TYPE_MISMATCH_ERR');
-    }
-    if (img.readyState != 'complete') {
-      throwException('INVALID_STATE_ERR');
-    }
-  }
-
-  function DOMException_(s) {
-    this.code = this[s];
-    this.message = s +': DOM Exception ' + this.code;
-  }
-  var p = DOMException_.prototype = new Error;
-  p.INDEX_SIZE_ERR = 1;
-  p.DOMSTRING_SIZE_ERR = 2;
-  p.HIERARCHY_REQUEST_ERR = 3;
-  p.WRONG_DOCUMENT_ERR = 4;
-  p.INVALID_CHARACTER_ERR = 5;
-  p.NO_DATA_ALLOWED_ERR = 6;
-  p.NO_MODIFICATION_ALLOWED_ERR = 7;
-  p.NOT_FOUND_ERR = 8;
-  p.NOT_SUPPORTED_ERR = 9;
-  p.INUSE_ATTRIBUTE_ERR = 10;
-  p.INVALID_STATE_ERR = 11;
-  p.SYNTAX_ERR = 12;
-  p.INVALID_MODIFICATION_ERR = 13;
-  p.NAMESPACE_ERR = 14;
-  p.INVALID_ACCESS_ERR = 15;
-  p.VALIDATION_ERR = 16;
-  p.TYPE_MISMATCH_ERR = 17;
-
-  // set up externs
-  G_vmlCanvasManager = G_vmlCanvasManager_;
-  CanvasRenderingContext2D = CanvasRenderingContext2D_;
-  CanvasGradient = CanvasGradient_;
-  CanvasPattern = CanvasPattern_;
-  DOMException = DOMException_;
-})();
-
-} // if
diff --git a/lib/logstash/web/public/js/flot/excanvas.min.js b/lib/logstash/web/public/js/flot/excanvas.min.js
deleted file mode 100644
index 12c74f7bea8..00000000000
--- a/lib/logstash/web/public/js/flot/excanvas.min.js
+++ /dev/null
@@ -1 +0,0 @@
-if(!document.createElement("canvas").getContext){(function(){var z=Math;var K=z.round;var J=z.sin;var U=z.cos;var b=z.abs;var k=z.sqrt;var D=10;var F=D/2;function T(){return this.context_||(this.context_=new W(this))}var O=Array.prototype.slice;function G(i,j,m){var Z=O.call(arguments,2);return function(){return i.apply(j,Z.concat(O.call(arguments)))}}function AD(Z){return String(Z).replace(/&/g,"&amp;").replace(/"/g,"&quot;")}function r(i){if(!i.namespaces.g_vml_){i.namespaces.add("g_vml_","urn:schemas-microsoft-com:vml","#default#VML")}if(!i.namespaces.g_o_){i.namespaces.add("g_o_","urn:schemas-microsoft-com:office:office","#default#VML")}if(!i.styleSheets.ex_canvas_){var Z=i.createStyleSheet();Z.owningElement.id="ex_canvas_";Z.cssText="canvas{display:inline-block;overflow:hidden;text-align:left;width:300px;height:150px}"}}r(document);var E={init:function(Z){if(/MSIE/.test(navigator.userAgent)&&!window.opera){var i=Z||document;i.createElement("canvas");i.attachEvent("onreadystatechange",G(this.init_,this,i))}},init_:function(m){var j=m.getElementsByTagName("canvas");for(var Z=0;Z<j.length;Z++){this.initElement(j[Z])}},initElement:function(i){if(!i.getContext){i.getContext=T;r(i.ownerDocument);i.innerHTML="";i.attachEvent("onpropertychange",S);i.attachEvent("onresize",w);var Z=i.attributes;if(Z.width&&Z.width.specified){i.style.width=Z.width.nodeValue+"px"}else{i.width=i.clientWidth}if(Z.height&&Z.height.specified){i.style.height=Z.height.nodeValue+"px"}else{i.height=i.clientHeight}}return i}};function S(i){var Z=i.srcElement;switch(i.propertyName){case"width":Z.getContext().clearRect();Z.style.width=Z.attributes.width.nodeValue+"px";Z.firstChild.style.width=Z.clientWidth+"px";break;case"height":Z.getContext().clearRect();Z.style.height=Z.attributes.height.nodeValue+"px";Z.firstChild.style.height=Z.clientHeight+"px";break}}function w(i){var Z=i.srcElement;if(Z.firstChild){Z.firstChild.style.width=Z.clientWidth+"px";Z.firstChild.style.height=Z.clientHeight+"px"}}E.init();var I=[];for(var AC=0;AC<16;AC++){for(var AB=0;AB<16;AB++){I[AC*16+AB]=AC.toString(16)+AB.toString(16)}}function V(){return[[1,0,0],[0,1,0],[0,0,1]]}function d(m,j){var i=V();for(var Z=0;Z<3;Z++){for(var AF=0;AF<3;AF++){var p=0;for(var AE=0;AE<3;AE++){p+=m[Z][AE]*j[AE][AF]}i[Z][AF]=p}}return i}function Q(i,Z){Z.fillStyle=i.fillStyle;Z.lineCap=i.lineCap;Z.lineJoin=i.lineJoin;Z.lineWidth=i.lineWidth;Z.miterLimit=i.miterLimit;Z.shadowBlur=i.shadowBlur;Z.shadowColor=i.shadowColor;Z.shadowOffsetX=i.shadowOffsetX;Z.shadowOffsetY=i.shadowOffsetY;Z.strokeStyle=i.strokeStyle;Z.globalAlpha=i.globalAlpha;Z.font=i.font;Z.textAlign=i.textAlign;Z.textBaseline=i.textBaseline;Z.arcScaleX_=i.arcScaleX_;Z.arcScaleY_=i.arcScaleY_;Z.lineScale_=i.lineScale_}var B={aliceblue:"#F0F8FF",antiquewhite:"#FAEBD7",aquamarine:"#7FFFD4",azure:"#F0FFFF",beige:"#F5F5DC",bisque:"#FFE4C4",black:"#000000",blanchedalmond:"#FFEBCD",blueviolet:"#8A2BE2",brown:"#A52A2A",burlywood:"#DEB887",cadetblue:"#5F9EA0",chartreuse:"#7FFF00",chocolate:"#D2691E",coral:"#FF7F50",cornflowerblue:"#6495ED",cornsilk:"#FFF8DC",crimson:"#DC143C",cyan:"#00FFFF",darkblue:"#00008B",darkcyan:"#008B8B",darkgoldenrod:"#B8860B",darkgray:"#A9A9A9",darkgreen:"#006400",darkgrey:"#A9A9A9",darkkhaki:"#BDB76B",darkmagenta:"#8B008B",darkolivegreen:"#556B2F",darkorange:"#FF8C00",darkorchid:"#9932CC",darkred:"#8B0000",darksalmon:"#E9967A",darkseagreen:"#8FBC8F",darkslateblue:"#483D8B",darkslategray:"#2F4F4F",darkslategrey:"#2F4F4F",darkturquoise:"#00CED1",darkviolet:"#9400D3",deeppink:"#FF1493",deepskyblue:"#00BFFF",dimgray:"#696969",dimgrey:"#696969",dodgerblue:"#1E90FF",firebrick:"#B22222",floralwhite:"#FFFAF0",forestgreen:"#228B22",gainsboro:"#DCDCDC",ghostwhite:"#F8F8FF",gold:"#FFD700",goldenrod:"#DAA520",grey:"#808080",greenyellow:"#ADFF2F",honeydew:"#F0FFF0",hotpink:"#FF69B4",indianred:"#CD5C5C",indigo:"#4B0082",ivory:"#FFFFF0",khaki:"#F0E68C",lavender:"#E6E6FA",lavenderblush:"#FFF0F5",lawngreen:"#7CFC00",lemonchiffon:"#FFFACD",lightblue:"#ADD8E6",lightcoral:"#F08080",lightcyan:"#E0FFFF",lightgoldenrodyellow:"#FAFAD2",lightgreen:"#90EE90",lightgrey:"#D3D3D3",lightpink:"#FFB6C1",lightsalmon:"#FFA07A",lightseagreen:"#20B2AA",lightskyblue:"#87CEFA",lightslategray:"#778899",lightslategrey:"#778899",lightsteelblue:"#B0C4DE",lightyellow:"#FFFFE0",limegreen:"#32CD32",linen:"#FAF0E6",magenta:"#FF00FF",mediumaquamarine:"#66CDAA",mediumblue:"#0000CD",mediumorchid:"#BA55D3",mediumpurple:"#9370DB",mediumseagreen:"#3CB371",mediumslateblue:"#7B68EE",mediumspringgreen:"#00FA9A",mediumturquoise:"#48D1CC",mediumvioletred:"#C71585",midnightblue:"#191970",mintcream:"#F5FFFA",mistyrose:"#FFE4E1",moccasin:"#FFE4B5",navajowhite:"#FFDEAD",oldlace:"#FDF5E6",olivedrab:"#6B8E23",orange:"#FFA500",orangered:"#FF4500",orchid:"#DA70D6",palegoldenrod:"#EEE8AA",palegreen:"#98FB98",paleturquoise:"#AFEEEE",palevioletred:"#DB7093",papayawhip:"#FFEFD5",peachpuff:"#FFDAB9",peru:"#CD853F",pink:"#FFC0CB",plum:"#DDA0DD",powderblue:"#B0E0E6",rosybrown:"#BC8F8F",royalblue:"#4169E1",saddlebrown:"#8B4513",salmon:"#FA8072",sandybrown:"#F4A460",seagreen:"#2E8B57",seashell:"#FFF5EE",sienna:"#A0522D",skyblue:"#87CEEB",slateblue:"#6A5ACD",slategray:"#708090",slategrey:"#708090",snow:"#FFFAFA",springgreen:"#00FF7F",steelblue:"#4682B4",tan:"#D2B48C",thistle:"#D8BFD8",tomato:"#FF6347",turquoise:"#40E0D0",violet:"#EE82EE",wheat:"#F5DEB3",whitesmoke:"#F5F5F5",yellowgreen:"#9ACD32"};function g(i){var m=i.indexOf("(",3);var Z=i.indexOf(")",m+1);var j=i.substring(m+1,Z).split(",");if(j.length==4&&i.substr(3,1)=="a"){alpha=Number(j[3])}else{j[3]=1}return j}function C(Z){return parseFloat(Z)/100}function N(i,j,Z){return Math.min(Z,Math.max(j,i))}function c(AF){var j,i,Z;h=parseFloat(AF[0])/360%360;if(h<0){h++}s=N(C(AF[1]),0,1);l=N(C(AF[2]),0,1);if(s==0){j=i=Z=l}else{var m=l<0.5?l*(1+s):l+s-l*s;var AE=2*l-m;j=A(AE,m,h+1/3);i=A(AE,m,h);Z=A(AE,m,h-1/3)}return"#"+I[Math.floor(j*255)]+I[Math.floor(i*255)]+I[Math.floor(Z*255)]}function A(i,Z,j){if(j<0){j++}if(j>1){j--}if(6*j<1){return i+(Z-i)*6*j}else{if(2*j<1){return Z}else{if(3*j<2){return i+(Z-i)*(2/3-j)*6}else{return i}}}}function Y(Z){var AE,p=1;Z=String(Z);if(Z.charAt(0)=="#"){AE=Z}else{if(/^rgb/.test(Z)){var m=g(Z);var AE="#",AF;for(var j=0;j<3;j++){if(m[j].indexOf("%")!=-1){AF=Math.floor(C(m[j])*255)}else{AF=Number(m[j])}AE+=I[N(AF,0,255)]}p=m[3]}else{if(/^hsl/.test(Z)){var m=g(Z);AE=c(m);p=m[3]}else{AE=B[Z]||Z}}}return{color:AE,alpha:p}}var L={style:"normal",variant:"normal",weight:"normal",size:10,family:"sans-serif"};var f={};function X(Z){if(f[Z]){return f[Z]}var m=document.createElement("div");var j=m.style;try{j.font=Z}catch(i){}return f[Z]={style:j.fontStyle||L.style,variant:j.fontVariant||L.variant,weight:j.fontWeight||L.weight,size:j.fontSize||L.size,family:j.fontFamily||L.family}}function P(j,i){var Z={};for(var AF in j){Z[AF]=j[AF]}var AE=parseFloat(i.currentStyle.fontSize),m=parseFloat(j.size);if(typeof j.size=="number"){Z.size=j.size}else{if(j.size.indexOf("px")!=-1){Z.size=m}else{if(j.size.indexOf("em")!=-1){Z.size=AE*m}else{if(j.size.indexOf("%")!=-1){Z.size=(AE/100)*m}else{if(j.size.indexOf("pt")!=-1){Z.size=m/0.75}else{Z.size=AE}}}}}Z.size*=0.981;return Z}function AA(Z){return Z.style+" "+Z.variant+" "+Z.weight+" "+Z.size+"px "+Z.family}function t(Z){switch(Z){case"butt":return"flat";case"round":return"round";case"square":default:return"square"}}function W(i){this.m_=V();this.mStack_=[];this.aStack_=[];this.currentPath_=[];this.strokeStyle="#000";this.fillStyle="#000";this.lineWidth=1;this.lineJoin="miter";this.lineCap="butt";this.miterLimit=D*1;this.globalAlpha=1;this.font="10px sans-serif";this.textAlign="left";this.textBaseline="alphabetic";this.canvas=i;var Z=i.ownerDocument.createElement("div");Z.style.width=i.clientWidth+"px";Z.style.height=i.clientHeight+"px";Z.style.overflow="hidden";Z.style.position="absolute";i.appendChild(Z);this.element_=Z;this.arcScaleX_=1;this.arcScaleY_=1;this.lineScale_=1}var M=W.prototype;M.clearRect=function(){if(this.textMeasureEl_){this.textMeasureEl_.removeNode(true);this.textMeasureEl_=null}this.element_.innerHTML=""};M.beginPath=function(){this.currentPath_=[]};M.moveTo=function(i,Z){var j=this.getCoords_(i,Z);this.currentPath_.push({type:"moveTo",x:j.x,y:j.y});this.currentX_=j.x;this.currentY_=j.y};M.lineTo=function(i,Z){var j=this.getCoords_(i,Z);this.currentPath_.push({type:"lineTo",x:j.x,y:j.y});this.currentX_=j.x;this.currentY_=j.y};M.bezierCurveTo=function(j,i,AI,AH,AG,AE){var Z=this.getCoords_(AG,AE);var AF=this.getCoords_(j,i);var m=this.getCoords_(AI,AH);e(this,AF,m,Z)};function e(Z,m,j,i){Z.currentPath_.push({type:"bezierCurveTo",cp1x:m.x,cp1y:m.y,cp2x:j.x,cp2y:j.y,x:i.x,y:i.y});Z.currentX_=i.x;Z.currentY_=i.y}M.quadraticCurveTo=function(AG,j,i,Z){var AF=this.getCoords_(AG,j);var AE=this.getCoords_(i,Z);var AH={x:this.currentX_+2/3*(AF.x-this.currentX_),y:this.currentY_+2/3*(AF.y-this.currentY_)};var m={x:AH.x+(AE.x-this.currentX_)/3,y:AH.y+(AE.y-this.currentY_)/3};e(this,AH,m,AE)};M.arc=function(AJ,AH,AI,AE,i,j){AI*=D;var AN=j?"at":"wa";var AK=AJ+U(AE)*AI-F;var AM=AH+J(AE)*AI-F;var Z=AJ+U(i)*AI-F;var AL=AH+J(i)*AI-F;if(AK==Z&&!j){AK+=0.125}var m=this.getCoords_(AJ,AH);var AG=this.getCoords_(AK,AM);var AF=this.getCoords_(Z,AL);this.currentPath_.push({type:AN,x:m.x,y:m.y,radius:AI,xStart:AG.x,yStart:AG.y,xEnd:AF.x,yEnd:AF.y})};M.rect=function(j,i,Z,m){this.moveTo(j,i);this.lineTo(j+Z,i);this.lineTo(j+Z,i+m);this.lineTo(j,i+m);this.closePath()};M.strokeRect=function(j,i,Z,m){var p=this.currentPath_;this.beginPath();this.moveTo(j,i);this.lineTo(j+Z,i);this.lineTo(j+Z,i+m);this.lineTo(j,i+m);this.closePath();this.stroke();this.currentPath_=p};M.fillRect=function(j,i,Z,m){var p=this.currentPath_;this.beginPath();this.moveTo(j,i);this.lineTo(j+Z,i);this.lineTo(j+Z,i+m);this.lineTo(j,i+m);this.closePath();this.fill();this.currentPath_=p};M.createLinearGradient=function(i,m,Z,j){var p=new v("gradient");p.x0_=i;p.y0_=m;p.x1_=Z;p.y1_=j;return p};M.createRadialGradient=function(m,AE,j,i,p,Z){var AF=new v("gradientradial");AF.x0_=m;AF.y0_=AE;AF.r0_=j;AF.x1_=i;AF.y1_=p;AF.r1_=Z;return AF};M.drawImage=function(AO,j){var AH,AF,AJ,AV,AM,AK,AQ,AX;var AI=AO.runtimeStyle.width;var AN=AO.runtimeStyle.height;AO.runtimeStyle.width="auto";AO.runtimeStyle.height="auto";var AG=AO.width;var AT=AO.height;AO.runtimeStyle.width=AI;AO.runtimeStyle.height=AN;if(arguments.length==3){AH=arguments[1];AF=arguments[2];AM=AK=0;AQ=AJ=AG;AX=AV=AT}else{if(arguments.length==5){AH=arguments[1];AF=arguments[2];AJ=arguments[3];AV=arguments[4];AM=AK=0;AQ=AG;AX=AT}else{if(arguments.length==9){AM=arguments[1];AK=arguments[2];AQ=arguments[3];AX=arguments[4];AH=arguments[5];AF=arguments[6];AJ=arguments[7];AV=arguments[8]}else{throw Error("Invalid number of arguments")}}}var AW=this.getCoords_(AH,AF);var m=AQ/2;var i=AX/2;var AU=[];var Z=10;var AE=10;AU.push(" <g_vml_:group",' coordsize="',D*Z,",",D*AE,'"',' coordorigin="0,0"',' style="width:',Z,"px;height:",AE,"px;position:absolute;");if(this.m_[0][0]!=1||this.m_[0][1]||this.m_[1][1]!=1||this.m_[1][0]){var p=[];p.push("M11=",this.m_[0][0],",","M12=",this.m_[1][0],",","M21=",this.m_[0][1],",","M22=",this.m_[1][1],",","Dx=",K(AW.x/D),",","Dy=",K(AW.y/D),"");var AS=AW;var AR=this.getCoords_(AH+AJ,AF);var AP=this.getCoords_(AH,AF+AV);var AL=this.getCoords_(AH+AJ,AF+AV);AS.x=z.max(AS.x,AR.x,AP.x,AL.x);AS.y=z.max(AS.y,AR.y,AP.y,AL.y);AU.push("padding:0 ",K(AS.x/D),"px ",K(AS.y/D),"px 0;filter:progid:DXImageTransform.Microsoft.Matrix(",p.join(""),", sizingmethod='clip');")}else{AU.push("top:",K(AW.y/D),"px;left:",K(AW.x/D),"px;")}AU.push(' ">','<g_vml_:image src="',AO.src,'"',' style="width:',D*AJ,"px;"," height:",D*AV,'px"',' cropleft="',AM/AG,'"',' croptop="',AK/AT,'"',' cropright="',(AG-AM-AQ)/AG,'"',' cropbottom="',(AT-AK-AX)/AT,'"'," />","</g_vml_:group>");this.element_.insertAdjacentHTML("BeforeEnd",AU.join(""))};M.stroke=function(AM){var m=10;var AN=10;var AE=5000;var AG={x:null,y:null};var AL={x:null,y:null};for(var AH=0;AH<this.currentPath_.length;AH+=AE){var AK=[];var AF=false;AK.push("<g_vml_:shape",' filled="',!!AM,'"',' style="position:absolute;width:',m,"px;height:",AN,'px;"',' coordorigin="0,0"',' coordsize="',D*m,",",D*AN,'"',' stroked="',!AM,'"',' path="');var AO=false;for(var AI=AH;AI<Math.min(AH+AE,this.currentPath_.length);AI++){if(AI%AE==0&&AI>0){AK.push(" m ",K(this.currentPath_[AI-1].x),",",K(this.currentPath_[AI-1].y))}var Z=this.currentPath_[AI];var AJ;switch(Z.type){case"moveTo":AJ=Z;AK.push(" m ",K(Z.x),",",K(Z.y));break;case"lineTo":AK.push(" l ",K(Z.x),",",K(Z.y));break;case"close":AK.push(" x ");Z=null;break;case"bezierCurveTo":AK.push(" c ",K(Z.cp1x),",",K(Z.cp1y),",",K(Z.cp2x),",",K(Z.cp2y),",",K(Z.x),",",K(Z.y));break;case"at":case"wa":AK.push(" ",Z.type," ",K(Z.x-this.arcScaleX_*Z.radius),",",K(Z.y-this.arcScaleY_*Z.radius)," ",K(Z.x+this.arcScaleX_*Z.radius),",",K(Z.y+this.arcScaleY_*Z.radius)," ",K(Z.xStart),",",K(Z.yStart)," ",K(Z.xEnd),",",K(Z.yEnd));break}if(Z){if(AG.x==null||Z.x<AG.x){AG.x=Z.x}if(AL.x==null||Z.x>AL.x){AL.x=Z.x}if(AG.y==null||Z.y<AG.y){AG.y=Z.y}if(AL.y==null||Z.y>AL.y){AL.y=Z.y}}}AK.push(' ">');if(!AM){R(this,AK)}else{a(this,AK,AG,AL)}AK.push("</g_vml_:shape>");this.element_.insertAdjacentHTML("beforeEnd",AK.join(""))}};function R(j,AE){var i=Y(j.strokeStyle);var m=i.color;var p=i.alpha*j.globalAlpha;var Z=j.lineScale_*j.lineWidth;if(Z<1){p*=Z}AE.push("<g_vml_:stroke",' opacity="',p,'"',' joinstyle="',j.lineJoin,'"',' miterlimit="',j.miterLimit,'"',' endcap="',t(j.lineCap),'"',' weight="',Z,'px"',' color="',m,'" />')}function a(AO,AG,Ah,AP){var AH=AO.fillStyle;var AY=AO.arcScaleX_;var AX=AO.arcScaleY_;var Z=AP.x-Ah.x;var m=AP.y-Ah.y;if(AH instanceof v){var AL=0;var Ac={x:0,y:0};var AU=0;var AK=1;if(AH.type_=="gradient"){var AJ=AH.x0_/AY;var j=AH.y0_/AX;var AI=AH.x1_/AY;var Aj=AH.y1_/AX;var Ag=AO.getCoords_(AJ,j);var Af=AO.getCoords_(AI,Aj);var AE=Af.x-Ag.x;var p=Af.y-Ag.y;AL=Math.atan2(AE,p)*180/Math.PI;if(AL<0){AL+=360}if(AL<0.000001){AL=0}}else{var Ag=AO.getCoords_(AH.x0_,AH.y0_);Ac={x:(Ag.x-Ah.x)/Z,y:(Ag.y-Ah.y)/m};Z/=AY*D;m/=AX*D;var Aa=z.max(Z,m);AU=2*AH.r0_/Aa;AK=2*AH.r1_/Aa-AU}var AS=AH.colors_;AS.sort(function(Ak,i){return Ak.offset-i.offset});var AN=AS.length;var AR=AS[0].color;var AQ=AS[AN-1].color;var AW=AS[0].alpha*AO.globalAlpha;var AV=AS[AN-1].alpha*AO.globalAlpha;var Ab=[];for(var Ae=0;Ae<AN;Ae++){var AM=AS[Ae];Ab.push(AM.offset*AK+AU+" "+AM.color)}AG.push('<g_vml_:fill type="',AH.type_,'"',' method="none" focus="100%"',' color="',AR,'"',' color2="',AQ,'"',' colors="',Ab.join(","),'"',' opacity="',AV,'"',' g_o_:opacity2="',AW,'"',' angle="',AL,'"',' focusposition="',Ac.x,",",Ac.y,'" />')}else{if(AH instanceof u){if(Z&&m){var AF=-Ah.x;var AZ=-Ah.y;AG.push("<g_vml_:fill",' position="',AF/Z*AY*AY,",",AZ/m*AX*AX,'"',' type="tile"',' src="',AH.src_,'" />')}}else{var Ai=Y(AO.fillStyle);var AT=Ai.color;var Ad=Ai.alpha*AO.globalAlpha;AG.push('<g_vml_:fill color="',AT,'" opacity="',Ad,'" />')}}}M.fill=function(){this.stroke(true)};M.closePath=function(){this.currentPath_.push({type:"close"})};M.getCoords_=function(j,i){var Z=this.m_;return{x:D*(j*Z[0][0]+i*Z[1][0]+Z[2][0])-F,y:D*(j*Z[0][1]+i*Z[1][1]+Z[2][1])-F}};M.save=function(){var Z={};Q(this,Z);this.aStack_.push(Z);this.mStack_.push(this.m_);this.m_=d(V(),this.m_)};M.restore=function(){if(this.aStack_.length){Q(this.aStack_.pop(),this);this.m_=this.mStack_.pop()}};function H(Z){return isFinite(Z[0][0])&&isFinite(Z[0][1])&&isFinite(Z[1][0])&&isFinite(Z[1][1])&&isFinite(Z[2][0])&&isFinite(Z[2][1])}function y(i,Z,j){if(!H(Z)){return }i.m_=Z;if(j){var p=Z[0][0]*Z[1][1]-Z[0][1]*Z[1][0];i.lineScale_=k(b(p))}}M.translate=function(j,i){var Z=[[1,0,0],[0,1,0],[j,i,1]];y(this,d(Z,this.m_),false)};M.rotate=function(i){var m=U(i);var j=J(i);var Z=[[m,j,0],[-j,m,0],[0,0,1]];y(this,d(Z,this.m_),false)};M.scale=function(j,i){this.arcScaleX_*=j;this.arcScaleY_*=i;var Z=[[j,0,0],[0,i,0],[0,0,1]];y(this,d(Z,this.m_),true)};M.transform=function(p,m,AF,AE,i,Z){var j=[[p,m,0],[AF,AE,0],[i,Z,1]];y(this,d(j,this.m_),true)};M.setTransform=function(AE,p,AG,AF,j,i){var Z=[[AE,p,0],[AG,AF,0],[j,i,1]];y(this,Z,true)};M.drawText_=function(AK,AI,AH,AN,AG){var AM=this.m_,AQ=1000,i=0,AP=AQ,AF={x:0,y:0},AE=[];var Z=P(X(this.font),this.element_);var j=AA(Z);var AR=this.element_.currentStyle;var p=this.textAlign.toLowerCase();switch(p){case"left":case"center":case"right":break;case"end":p=AR.direction=="ltr"?"right":"left";break;case"start":p=AR.direction=="rtl"?"right":"left";break;default:p="left"}switch(this.textBaseline){case"hanging":case"top":AF.y=Z.size/1.75;break;case"middle":break;default:case null:case"alphabetic":case"ideographic":case"bottom":AF.y=-Z.size/2.25;break}switch(p){case"right":i=AQ;AP=0.05;break;case"center":i=AP=AQ/2;break}var AO=this.getCoords_(AI+AF.x,AH+AF.y);AE.push('<g_vml_:line from="',-i,' 0" to="',AP,' 0.05" ',' coordsize="100 100" coordorigin="0 0"',' filled="',!AG,'" stroked="',!!AG,'" style="position:absolute;width:1px;height:1px;">');if(AG){R(this,AE)}else{a(this,AE,{x:-i,y:0},{x:AP,y:Z.size})}var AL=AM[0][0].toFixed(3)+","+AM[1][0].toFixed(3)+","+AM[0][1].toFixed(3)+","+AM[1][1].toFixed(3)+",0,0";var AJ=K(AO.x/D)+","+K(AO.y/D);AE.push('<g_vml_:skew on="t" matrix="',AL,'" ',' offset="',AJ,'" origin="',i,' 0" />','<g_vml_:path textpathok="true" />','<g_vml_:textpath on="true" string="',AD(AK),'" style="v-text-align:',p,";font:",AD(j),'" /></g_vml_:line>');this.element_.insertAdjacentHTML("beforeEnd",AE.join(""))};M.fillText=function(j,Z,m,i){this.drawText_(j,Z,m,i,false)};M.strokeText=function(j,Z,m,i){this.drawText_(j,Z,m,i,true)};M.measureText=function(j){if(!this.textMeasureEl_){var Z='<span style="position:absolute;top:-20000px;left:0;padding:0;margin:0;border:none;white-space:pre;"></span>';this.element_.insertAdjacentHTML("beforeEnd",Z);this.textMeasureEl_=this.element_.lastChild}var i=this.element_.ownerDocument;this.textMeasureEl_.innerHTML="";this.textMeasureEl_.style.font=this.font;this.textMeasureEl_.appendChild(i.createTextNode(j));return{width:this.textMeasureEl_.offsetWidth}};M.clip=function(){};M.arcTo=function(){};M.createPattern=function(i,Z){return new u(i,Z)};function v(Z){this.type_=Z;this.x0_=0;this.y0_=0;this.r0_=0;this.x1_=0;this.y1_=0;this.r1_=0;this.colors_=[]}v.prototype.addColorStop=function(i,Z){Z=Y(Z);this.colors_.push({offset:i,color:Z.color,alpha:Z.alpha})};function u(i,Z){q(i);switch(Z){case"repeat":case null:case"":this.repetition_="repeat";break;case"repeat-x":case"repeat-y":case"no-repeat":this.repetition_=Z;break;default:n("SYNTAX_ERR")}this.src_=i.src;this.width_=i.width;this.height_=i.height}function n(Z){throw new o(Z)}function q(Z){if(!Z||Z.nodeType!=1||Z.tagName!="IMG"){n("TYPE_MISMATCH_ERR")}if(Z.readyState!="complete"){n("INVALID_STATE_ERR")}}function o(Z){this.code=this[Z];this.message=Z+": DOM Exception "+this.code}var x=o.prototype=new Error;x.INDEX_SIZE_ERR=1;x.DOMSTRING_SIZE_ERR=2;x.HIERARCHY_REQUEST_ERR=3;x.WRONG_DOCUMENT_ERR=4;x.INVALID_CHARACTER_ERR=5;x.NO_DATA_ALLOWED_ERR=6;x.NO_MODIFICATION_ALLOWED_ERR=7;x.NOT_FOUND_ERR=8;x.NOT_SUPPORTED_ERR=9;x.INUSE_ATTRIBUTE_ERR=10;x.INVALID_STATE_ERR=11;x.SYNTAX_ERR=12;x.INVALID_MODIFICATION_ERR=13;x.NAMESPACE_ERR=14;x.INVALID_ACCESS_ERR=15;x.VALIDATION_ERR=16;x.TYPE_MISMATCH_ERR=17;G_vmlCanvasManager=E;CanvasRenderingContext2D=W;CanvasGradient=v;CanvasPattern=u;DOMException=o})()};
\ No newline at end of file
diff --git a/lib/logstash/web/public/js/flot/jquery.colorhelpers.js b/lib/logstash/web/public/js/flot/jquery.colorhelpers.js
deleted file mode 100644
index fa44961217a..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.colorhelpers.js
+++ /dev/null
@@ -1,174 +0,0 @@
-/* Plugin for jQuery for working with colors.
- * 
- * Version 1.0.
- * 
- * Inspiration from jQuery color animation plugin by John Resig.
- *
- * Released under the MIT license by Ole Laursen, October 2009.
- *
- * Examples:
- *
- *   $.color.parse("#fff").scale('rgb', 0.25).add('a', -0.5).toString()
- *   var c = $.color.extract($("#mydiv"), 'background-color');
- *   console.log(c.r, c.g, c.b, c.a);
- *   $.color.make(100, 50, 25, 0.4).toString() // returns "rgba(100,50,25,0.4)"
- *
- * Note that .scale() and .add() work in-place instead of returning
- * new objects.
- */ 
-
-(function() {
-    jQuery.color = {};
-
-    // construct color object with some convenient chainable helpers
-    jQuery.color.make = function (r, g, b, a) {
-        var o = {};
-        o.r = r || 0;
-        o.g = g || 0;
-        o.b = b || 0;
-        o.a = a != null ? a : 1;
-
-        o.add = function (c, d) {
-            for (var i = 0; i < c.length; ++i)
-                o[c.charAt(i)] += d;
-            return o.normalize();
-        };
-        
-        o.scale = function (c, f) {
-            for (var i = 0; i < c.length; ++i)
-                o[c.charAt(i)] *= f;
-            return o.normalize();
-        };
-        
-        o.toString = function () {
-            if (o.a >= 1.0) {
-                return "rgb("+[o.r, o.g, o.b].join(",")+")";
-            } else {
-                return "rgba("+[o.r, o.g, o.b, o.a].join(",")+")";
-            }
-        };
-
-        o.normalize = function () {
-            function clamp(min, value, max) {
-                return value < min ? min: (value > max ? max: value);
-            }
-            
-            o.r = clamp(0, parseInt(o.r), 255);
-            o.g = clamp(0, parseInt(o.g), 255);
-            o.b = clamp(0, parseInt(o.b), 255);
-            o.a = clamp(0, o.a, 1);
-            return o;
-        };
-
-        o.clone = function () {
-            return jQuery.color.make(o.r, o.b, o.g, o.a);
-        };
-
-        return o.normalize();
-    }
-
-    // extract CSS color property from element, going up in the DOM
-    // if it's "transparent"
-    jQuery.color.extract = function (elem, css) {
-        var c;
-        do {
-            c = elem.css(css).toLowerCase();
-            // keep going until we find an element that has color, or
-            // we hit the body
-            if (c != '' && c != 'transparent')
-                break;
-            elem = elem.parent();
-        } while (!jQuery.nodeName(elem.get(0), "body"));
-
-        // catch Safari's way of signalling transparent
-        if (c == "rgba(0, 0, 0, 0)")
-            c = "transparent";
-        
-        return jQuery.color.parse(c);
-    }
-    
-    // parse CSS color string (like "rgb(10, 32, 43)" or "#fff"),
-    // returns color object
-    jQuery.color.parse = function (str) {
-        var res, m = jQuery.color.make;
-
-        // Look for rgb(num,num,num)
-        if (res = /rgb\(\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*\)/.exec(str))
-            return m(parseInt(res[1], 10), parseInt(res[2], 10), parseInt(res[3], 10));
-        
-        // Look for rgba(num,num,num,num)
-        if (res = /rgba\(\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*,\s*([0-9]+(?:\.[0-9]+)?)\s*\)/.exec(str))
-            return m(parseInt(res[1], 10), parseInt(res[2], 10), parseInt(res[3], 10), parseFloat(res[4]));
-            
-        // Look for rgb(num%,num%,num%)
-        if (res = /rgb\(\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*\)/.exec(str))
-            return m(parseFloat(res[1])*2.55, parseFloat(res[2])*2.55, parseFloat(res[3])*2.55);
-
-        // Look for rgba(num%,num%,num%,num)
-        if (res = /rgba\(\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\s*\)/.exec(str))
-            return m(parseFloat(res[1])*2.55, parseFloat(res[2])*2.55, parseFloat(res[3])*2.55, parseFloat(res[4]));
-        
-        // Look for #a0b1c2
-        if (res = /#([a-fA-F0-9]{2})([a-fA-F0-9]{2})([a-fA-F0-9]{2})/.exec(str))
-            return m(parseInt(res[1], 16), parseInt(res[2], 16), parseInt(res[3], 16));
-
-        // Look for #fff
-        if (res = /#([a-fA-F0-9])([a-fA-F0-9])([a-fA-F0-9])/.exec(str))
-            return m(parseInt(res[1]+res[1], 16), parseInt(res[2]+res[2], 16), parseInt(res[3]+res[3], 16));
-
-        // Otherwise, we're most likely dealing with a named color
-        var name = jQuery.trim(str).toLowerCase();
-        if (name == "transparent")
-            return m(255, 255, 255, 0);
-        else {
-            res = lookupColors[name];
-            return m(res[0], res[1], res[2]);
-        }
-    }
-    
-    var lookupColors = {
-        aqua:[0,255,255],
-        azure:[240,255,255],
-        beige:[245,245,220],
-        black:[0,0,0],
-        blue:[0,0,255],
-        brown:[165,42,42],
-        cyan:[0,255,255],
-        darkblue:[0,0,139],
-        darkcyan:[0,139,139],
-        darkgrey:[169,169,169],
-        darkgreen:[0,100,0],
-        darkkhaki:[189,183,107],
-        darkmagenta:[139,0,139],
-        darkolivegreen:[85,107,47],
-        darkorange:[255,140,0],
-        darkorchid:[153,50,204],
-        darkred:[139,0,0],
-        darksalmon:[233,150,122],
-        darkviolet:[148,0,211],
-        fuchsia:[255,0,255],
-        gold:[255,215,0],
-        green:[0,128,0],
-        indigo:[75,0,130],
-        khaki:[240,230,140],
-        lightblue:[173,216,230],
-        lightcyan:[224,255,255],
-        lightgreen:[144,238,144],
-        lightgrey:[211,211,211],
-        lightpink:[255,182,193],
-        lightyellow:[255,255,224],
-        lime:[0,255,0],
-        magenta:[255,0,255],
-        maroon:[128,0,0],
-        navy:[0,0,128],
-        olive:[128,128,0],
-        orange:[255,165,0],
-        pink:[255,192,203],
-        purple:[128,0,128],
-        violet:[128,0,128],
-        red:[255,0,0],
-        silver:[192,192,192],
-        white:[255,255,255],
-        yellow:[255,255,0]
-    };    
-})();
diff --git a/lib/logstash/web/public/js/flot/jquery.colorhelpers.min.js b/lib/logstash/web/public/js/flot/jquery.colorhelpers.min.js
deleted file mode 100644
index fafe905ce09..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.colorhelpers.min.js
+++ /dev/null
@@ -1 +0,0 @@
-(function(){jQuery.color={};jQuery.color.make=function(E,D,B,C){var F={};F.r=E||0;F.g=D||0;F.b=B||0;F.a=C!=null?C:1;F.add=function(I,H){for(var G=0;G<I.length;++G){F[I.charAt(G)]+=H}return F.normalize()};F.scale=function(I,H){for(var G=0;G<I.length;++G){F[I.charAt(G)]*=H}return F.normalize()};F.toString=function(){if(F.a>=1){return"rgb("+[F.r,F.g,F.b].join(",")+")"}else{return"rgba("+[F.r,F.g,F.b,F.a].join(",")+")"}};F.normalize=function(){function G(I,J,H){return J<I?I:(J>H?H:J)}F.r=G(0,parseInt(F.r),255);F.g=G(0,parseInt(F.g),255);F.b=G(0,parseInt(F.b),255);F.a=G(0,F.a,1);return F};F.clone=function(){return jQuery.color.make(F.r,F.b,F.g,F.a)};return F.normalize()};jQuery.color.extract=function(C,B){var D;do{D=C.css(B).toLowerCase();if(D!=""&&D!="transparent"){break}C=C.parent()}while(!jQuery.nodeName(C.get(0),"body"));if(D=="rgba(0, 0, 0, 0)"){D="transparent"}return jQuery.color.parse(D)};jQuery.color.parse=function(E){var D,B=jQuery.color.make;if(D=/rgb\(\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*\)/.exec(E)){return B(parseInt(D[1],10),parseInt(D[2],10),parseInt(D[3],10))}if(D=/rgba\(\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*,\s*([0-9]+(?:\.[0-9]+)?)\s*\)/.exec(E)){return B(parseInt(D[1],10),parseInt(D[2],10),parseInt(D[3],10),parseFloat(D[4]))}if(D=/rgb\(\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*\)/.exec(E)){return B(parseFloat(D[1])*2.55,parseFloat(D[2])*2.55,parseFloat(D[3])*2.55)}if(D=/rgba\(\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\s*\)/.exec(E)){return B(parseFloat(D[1])*2.55,parseFloat(D[2])*2.55,parseFloat(D[3])*2.55,parseFloat(D[4]))}if(D=/#([a-fA-F0-9]{2})([a-fA-F0-9]{2})([a-fA-F0-9]{2})/.exec(E)){return B(parseInt(D[1],16),parseInt(D[2],16),parseInt(D[3],16))}if(D=/#([a-fA-F0-9])([a-fA-F0-9])([a-fA-F0-9])/.exec(E)){return B(parseInt(D[1]+D[1],16),parseInt(D[2]+D[2],16),parseInt(D[3]+D[3],16))}var C=jQuery.trim(E).toLowerCase();if(C=="transparent"){return B(255,255,255,0)}else{D=A[C];return B(D[0],D[1],D[2])}};var A={aqua:[0,255,255],azure:[240,255,255],beige:[245,245,220],black:[0,0,0],blue:[0,0,255],brown:[165,42,42],cyan:[0,255,255],darkblue:[0,0,139],darkcyan:[0,139,139],darkgrey:[169,169,169],darkgreen:[0,100,0],darkkhaki:[189,183,107],darkmagenta:[139,0,139],darkolivegreen:[85,107,47],darkorange:[255,140,0],darkorchid:[153,50,204],darkred:[139,0,0],darksalmon:[233,150,122],darkviolet:[148,0,211],fuchsia:[255,0,255],gold:[255,215,0],green:[0,128,0],indigo:[75,0,130],khaki:[240,230,140],lightblue:[173,216,230],lightcyan:[224,255,255],lightgreen:[144,238,144],lightgrey:[211,211,211],lightpink:[255,182,193],lightyellow:[255,255,224],lime:[0,255,0],magenta:[255,0,255],maroon:[128,0,0],navy:[0,0,128],olive:[128,128,0],orange:[255,165,0],pink:[255,192,203],purple:[128,0,128],violet:[128,0,128],red:[255,0,0],silver:[192,192,192],white:[255,255,255],yellow:[255,255,0]}})();
\ No newline at end of file
diff --git a/lib/logstash/web/public/js/flot/jquery.flot.crosshair.js b/lib/logstash/web/public/js/flot/jquery.flot.crosshair.js
deleted file mode 100644
index 11be113fd27..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.flot.crosshair.js
+++ /dev/null
@@ -1,156 +0,0 @@
-/*
-Flot plugin for showing a crosshair, thin lines, when the mouse hovers
-over the plot.
-
-  crosshair: {
-    mode: null or "x" or "y" or "xy"
-    color: color
-    lineWidth: number
-  }
-
-Set the mode to one of "x", "y" or "xy". The "x" mode enables a
-vertical crosshair that lets you trace the values on the x axis, "y"
-enables a horizontal crosshair and "xy" enables them both. "color" is
-the color of the crosshair (default is "rgba(170, 0, 0, 0.80)"),
-"lineWidth" is the width of the drawn lines (default is 1).
-
-The plugin also adds four public methods:
-
-  - setCrosshair(pos)
-
-    Set the position of the crosshair. Note that this is cleared if
-    the user moves the mouse. "pos" should be on the form { x: xpos,
-    y: ypos } (or x2 and y2 if you're using the secondary axes), which
-    is coincidentally the same format as what you get from a "plothover"
-    event. If "pos" is null, the crosshair is cleared.
-
-  - clearCrosshair()
-
-    Clear the crosshair.
-
-  - lockCrosshair(pos)
-
-    Cause the crosshair to lock to the current location, no longer
-    updating if the user moves the mouse. Optionally supply a position
-    (passed on to setCrosshair()) to move it to.
-
-    Example usage:
-      var myFlot = $.plot( $("#graph"), ..., { crosshair: { mode: "x" } } };
-      $("#graph").bind("plothover", function (evt, position, item) {
-        if (item) {
-          // Lock the crosshair to the data point being hovered
-          myFlot.lockCrosshair({ x: item.datapoint[0], y: item.datapoint[1] });
-        }
-        else {
-          // Return normal crosshair operation
-          myFlot.unlockCrosshair();
-        }
-      });
-
-  - unlockCrosshair()
-
-    Free the crosshair to move again after locking it.
-*/
-
-(function ($) {
-    var options = {
-        crosshair: {
-            mode: null, // one of null, "x", "y" or "xy",
-            color: "rgba(170, 0, 0, 0.80)",
-            lineWidth: 1
-        }
-    };
-    
-    function init(plot) {
-        // position of crosshair in pixels
-        var crosshair = { x: -1, y: -1, locked: false };
-
-        plot.setCrosshair = function setCrosshair(pos) {
-            if (!pos)
-                crosshair.x = -1;
-            else {
-                var axes = plot.getAxes();
-                
-                crosshair.x = Math.max(0, Math.min(pos.x != null ? axes.xaxis.p2c(pos.x) : axes.x2axis.p2c(pos.x2), plot.width()));
-                crosshair.y = Math.max(0, Math.min(pos.y != null ? axes.yaxis.p2c(pos.y) : axes.y2axis.p2c(pos.y2), plot.height()));
-            }
-            
-            plot.triggerRedrawOverlay();
-        };
-        
-        plot.clearCrosshair = plot.setCrosshair; // passes null for pos
-        
-        plot.lockCrosshair = function lockCrosshair(pos) {
-            if (pos)
-                plot.setCrosshair(pos);
-            crosshair.locked = true;
-        }
-
-        plot.unlockCrosshair = function unlockCrosshair() {
-            crosshair.locked = false;
-        }
-
-        plot.hooks.bindEvents.push(function (plot, eventHolder) {
-            if (!plot.getOptions().crosshair.mode)
-                return;
-
-            eventHolder.mouseout(function () {
-                if (crosshair.x != -1) {
-                    crosshair.x = -1;
-                    plot.triggerRedrawOverlay();
-                }
-            });
-            
-            eventHolder.mousemove(function (e) {
-                if (plot.getSelection && plot.getSelection()) {
-                    crosshair.x = -1; // hide the crosshair while selecting
-                    return;
-                }
-                
-                if (crosshair.locked)
-                    return;
-                
-                var offset = plot.offset();
-                crosshair.x = Math.max(0, Math.min(e.pageX - offset.left, plot.width()));
-                crosshair.y = Math.max(0, Math.min(e.pageY - offset.top, plot.height()));
-                plot.triggerRedrawOverlay();
-            });
-        });
-
-        plot.hooks.drawOverlay.push(function (plot, ctx) {
-            var c = plot.getOptions().crosshair;
-            if (!c.mode)
-                return;
-
-            var plotOffset = plot.getPlotOffset();
-            
-            ctx.save();
-            ctx.translate(plotOffset.left, plotOffset.top);
-
-            if (crosshair.x != -1) {
-                ctx.strokeStyle = c.color;
-                ctx.lineWidth = c.lineWidth;
-                ctx.lineJoin = "round";
-
-                ctx.beginPath();
-                if (c.mode.indexOf("x") != -1) {
-                    ctx.moveTo(crosshair.x, 0);
-                    ctx.lineTo(crosshair.x, plot.height());
-                }
-                if (c.mode.indexOf("y") != -1) {
-                    ctx.moveTo(0, crosshair.y);
-                    ctx.lineTo(plot.width(), crosshair.y);
-                }
-                ctx.stroke();
-            }
-            ctx.restore();
-        });
-    }
-    
-    $.plot.plugins.push({
-        init: init,
-        options: options,
-        name: 'crosshair',
-        version: '1.0'
-    });
-})(jQuery);
diff --git a/lib/logstash/web/public/js/flot/jquery.flot.crosshair.min.js b/lib/logstash/web/public/js/flot/jquery.flot.crosshair.min.js
deleted file mode 100644
index ce689b19eb9..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.flot.crosshair.min.js
+++ /dev/null
@@ -1 +0,0 @@
-(function(B){var A={crosshair:{mode:null,color:"rgba(170, 0, 0, 0.80)",lineWidth:1}};function C(G){var H={x:-1,y:-1,locked:false};G.setCrosshair=function D(J){if(!J){H.x=-1}else{var I=G.getAxes();H.x=Math.max(0,Math.min(J.x!=null?I.xaxis.p2c(J.x):I.x2axis.p2c(J.x2),G.width()));H.y=Math.max(0,Math.min(J.y!=null?I.yaxis.p2c(J.y):I.y2axis.p2c(J.y2),G.height()))}G.triggerRedrawOverlay()};G.clearCrosshair=G.setCrosshair;G.lockCrosshair=function E(I){if(I){G.setCrosshair(I)}H.locked=true};G.unlockCrosshair=function F(){H.locked=false};G.hooks.bindEvents.push(function(J,I){if(!J.getOptions().crosshair.mode){return }I.mouseout(function(){if(H.x!=-1){H.x=-1;J.triggerRedrawOverlay()}});I.mousemove(function(K){if(J.getSelection&&J.getSelection()){H.x=-1;return }if(H.locked){return }var L=J.offset();H.x=Math.max(0,Math.min(K.pageX-L.left,J.width()));H.y=Math.max(0,Math.min(K.pageY-L.top,J.height()));J.triggerRedrawOverlay()})});G.hooks.drawOverlay.push(function(K,I){var L=K.getOptions().crosshair;if(!L.mode){return }var J=K.getPlotOffset();I.save();I.translate(J.left,J.top);if(H.x!=-1){I.strokeStyle=L.color;I.lineWidth=L.lineWidth;I.lineJoin="round";I.beginPath();if(L.mode.indexOf("x")!=-1){I.moveTo(H.x,0);I.lineTo(H.x,K.height())}if(L.mode.indexOf("y")!=-1){I.moveTo(0,H.y);I.lineTo(K.width(),H.y)}I.stroke()}I.restore()})}B.plot.plugins.push({init:C,options:A,name:"crosshair",version:"1.0"})})(jQuery);
\ No newline at end of file
diff --git a/lib/logstash/web/public/js/flot/jquery.flot.image.js b/lib/logstash/web/public/js/flot/jquery.flot.image.js
deleted file mode 100644
index 90babf625be..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.flot.image.js
+++ /dev/null
@@ -1,237 +0,0 @@
-/*
-Flot plugin for plotting images, e.g. useful for putting ticks on a
-prerendered complex visualization.
-
-The data syntax is [[image, x1, y1, x2, y2], ...] where (x1, y1) and
-(x2, y2) are where you intend the two opposite corners of the image to
-end up in the plot. Image must be a fully loaded Javascript image (you
-can make one with new Image()). If the image is not complete, it's
-skipped when plotting.
-
-There are two helpers included for retrieving images. The easiest work
-the way that you put in URLs instead of images in the data (like
-["myimage.png", 0, 0, 10, 10]), then call $.plot.image.loadData(data,
-options, callback) where data and options are the same as you pass in
-to $.plot. This loads the images, replaces the URLs in the data with
-the corresponding images and calls "callback" when all images are
-loaded (or failed loading). In the callback, you can then call $.plot
-with the data set. See the included example.
-
-A more low-level helper, $.plot.image.load(urls, callback) is also
-included. Given a list of URLs, it calls callback with an object
-mapping from URL to Image object when all images are loaded or have
-failed loading.
-
-Options for the plugin are
-
-  series: {
-      images: {
-          show: boolean
-          anchor: "corner" or "center"
-          alpha: [0,1]
-      }
-  }
-
-which can be specified for a specific series
-
-  $.plot($("#placeholder"), [{ data: [ ... ], images: { ... } ])
-
-Note that because the data format is different from usual data points,
-you can't use images with anything else in a specific data series.
-
-Setting "anchor" to "center" causes the pixels in the image to be
-anchored at the corner pixel centers inside of at the pixel corners,
-effectively letting half a pixel stick out to each side in the plot.
-
-
-A possible future direction could be support for tiling for large
-images (like Google Maps).
-
-*/
-
-(function ($) {
-    var options = {
-        series: {
-            images: {
-                show: false,
-                alpha: 1,
-                anchor: "corner" // or "center"
-            }
-        }
-    };
-
-    $.plot.image = {};
-
-    $.plot.image.loadDataImages = function (series, options, callback) {
-        var urls = [], points = [];
-
-        var defaultShow = options.series.images.show;
-        
-        $.each(series, function (i, s) {
-            if (!(defaultShow || s.images.show))
-                return;
-            
-            if (s.data)
-                s = s.data;
-
-            $.each(s, function (i, p) {
-                if (typeof p[0] == "string") {
-                    urls.push(p[0]);
-                    points.push(p);
-                }
-            });
-        });
-
-        $.plot.image.load(urls, function (loadedImages) {
-            $.each(points, function (i, p) {
-                var url = p[0];
-                if (loadedImages[url])
-                    p[0] = loadedImages[url];
-            });
-
-            callback();
-        });
-    }
-    
-    $.plot.image.load = function (urls, callback) {
-        var missing = urls.length, loaded = {};
-        if (missing == 0)
-            callback({});
-
-        $.each(urls, function (i, url) {
-            var handler = function () {
-                --missing;
-                
-                loaded[url] = this;
-                
-                if (missing == 0)
-                    callback(loaded);
-            };
-
-            $('<img />').load(handler).error(handler).attr('src', url);
-        });
-    }
-    
-    function draw(plot, ctx) {
-        var plotOffset = plot.getPlotOffset();
-        
-        $.each(plot.getData(), function (i, series) {
-            var points = series.datapoints.points,
-                ps = series.datapoints.pointsize;
-            
-            for (var i = 0; i < points.length; i += ps) {
-                var img = points[i],
-                    x1 = points[i + 1], y1 = points[i + 2],
-                    x2 = points[i + 3], y2 = points[i + 4],
-                    xaxis = series.xaxis, yaxis = series.yaxis,
-                    tmp;
-
-                // actually we should check img.complete, but it
-                // appears to be a somewhat unreliable indicator in
-                // IE6 (false even after load event)
-                if (!img || img.width <= 0 || img.height <= 0)
-                    continue;
-
-                if (x1 > x2) {
-                    tmp = x2;
-                    x2 = x1;
-                    x1 = tmp;
-                }
-                if (y1 > y2) {
-                    tmp = y2;
-                    y2 = y1;
-                    y1 = tmp;
-                }
-                
-                // if the anchor is at the center of the pixel, expand the 
-                // image by 1/2 pixel in each direction
-                if (series.images.anchor == "center") {
-                    tmp = 0.5 * (x2-x1) / (img.width - 1);
-                    x1 -= tmp;
-                    x2 += tmp;
-                    tmp = 0.5 * (y2-y1) / (img.height - 1);
-                    y1 -= tmp;
-                    y2 += tmp;
-                }
-                
-                // clip
-                if (x1 == x2 || y1 == y2 ||
-                    x1 >= xaxis.max || x2 <= xaxis.min ||
-                    y1 >= yaxis.max || y2 <= yaxis.min)
-                    continue;
-
-                var sx1 = 0, sy1 = 0, sx2 = img.width, sy2 = img.height;
-                if (x1 < xaxis.min) {
-                    sx1 += (sx2 - sx1) * (xaxis.min - x1) / (x2 - x1);
-                    x1 = xaxis.min;
-                }
-
-                if (x2 > xaxis.max) {
-                    sx2 += (sx2 - sx1) * (xaxis.max - x2) / (x2 - x1);
-                    x2 = xaxis.max;
-                }
-
-                if (y1 < yaxis.min) {
-                    sy2 += (sy1 - sy2) * (yaxis.min - y1) / (y2 - y1);
-                    y1 = yaxis.min;
-                }
-
-                if (y2 > yaxis.max) {
-                    sy1 += (sy1 - sy2) * (yaxis.max - y2) / (y2 - y1);
-                    y2 = yaxis.max;
-                }
-                
-                x1 = xaxis.p2c(x1);
-                x2 = xaxis.p2c(x2);
-                y1 = yaxis.p2c(y1);
-                y2 = yaxis.p2c(y2);
-                
-                // the transformation may have swapped us
-                if (x1 > x2) {
-                    tmp = x2;
-                    x2 = x1;
-                    x1 = tmp;
-                }
-                if (y1 > y2) {
-                    tmp = y2;
-                    y2 = y1;
-                    y1 = tmp;
-                }
-
-                tmp = ctx.globalAlpha;
-                ctx.globalAlpha *= series.images.alpha;
-                ctx.drawImage(img,
-                              sx1, sy1, sx2 - sx1, sy2 - sy1,
-                              x1 + plotOffset.left, y1 + plotOffset.top,
-                              x2 - x1, y2 - y1);
-                ctx.globalAlpha = tmp;
-            }
-        });
-    }
-
-    function processRawData(plot, series, data, datapoints) {
-        if (!series.images.show)
-            return;
-
-        // format is Image, x1, y1, x2, y2 (opposite corners)
-        datapoints.format = [
-            { required: true },
-            { x: true, number: true, required: true },
-            { y: true, number: true, required: true },
-            { x: true, number: true, required: true },
-            { y: true, number: true, required: true }
-        ];
-    }
-    
-    function init(plot) {
-        plot.hooks.processRawData.push(processRawData);
-        plot.hooks.draw.push(draw);
-    }
-    
-    $.plot.plugins.push({
-        init: init,
-        options: options,
-        name: 'image',
-        version: '1.1'
-    });
-})(jQuery);
diff --git a/lib/logstash/web/public/js/flot/jquery.flot.image.min.js b/lib/logstash/web/public/js/flot/jquery.flot.image.min.js
deleted file mode 100644
index eb16cb1ae9d..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.flot.image.min.js
+++ /dev/null
@@ -1 +0,0 @@
-(function(D){var B={series:{images:{show:false,alpha:1,anchor:"corner"}}};D.plot.image={};D.plot.image.loadDataImages=function(G,F,K){var J=[],H=[];var I=F.series.images.show;D.each(G,function(L,M){if(!(I||M.images.show)){return }if(M.data){M=M.data}D.each(M,function(N,O){if(typeof O[0]=="string"){J.push(O[0]);H.push(O)}})});D.plot.image.load(J,function(L){D.each(H,function(N,O){var M=O[0];if(L[M]){O[0]=L[M]}});K()})};D.plot.image.load=function(H,I){var G=H.length,F={};if(G==0){I({})}D.each(H,function(K,J){var L=function(){--G;F[J]=this;if(G==0){I(F)}};D("<img />").load(L).error(L).attr("src",J)})};function A(H,F){var G=H.getPlotOffset();D.each(H.getData(),function(O,P){var X=P.datapoints.points,I=P.datapoints.pointsize;for(var O=0;O<X.length;O+=I){var Q=X[O],M=X[O+1],V=X[O+2],K=X[O+3],T=X[O+4],W=P.xaxis,S=P.yaxis,N;if(!Q||Q.width<=0||Q.height<=0){continue}if(M>K){N=K;K=M;M=N}if(V>T){N=T;T=V;V=N}if(P.images.anchor=="center"){N=0.5*(K-M)/(Q.width-1);M-=N;K+=N;N=0.5*(T-V)/(Q.height-1);V-=N;T+=N}if(M==K||V==T||M>=W.max||K<=W.min||V>=S.max||T<=S.min){continue}var L=0,U=0,J=Q.width,R=Q.height;if(M<W.min){L+=(J-L)*(W.min-M)/(K-M);M=W.min}if(K>W.max){J+=(J-L)*(W.max-K)/(K-M);K=W.max}if(V<S.min){R+=(U-R)*(S.min-V)/(T-V);V=S.min}if(T>S.max){U+=(U-R)*(S.max-T)/(T-V);T=S.max}M=W.p2c(M);K=W.p2c(K);V=S.p2c(V);T=S.p2c(T);if(M>K){N=K;K=M;M=N}if(V>T){N=T;T=V;V=N}N=F.globalAlpha;F.globalAlpha*=P.images.alpha;F.drawImage(Q,L,U,J-L,R-U,M+G.left,V+G.top,K-M,T-V);F.globalAlpha=N}})}function C(I,F,G,H){if(!F.images.show){return }H.format=[{required:true},{x:true,number:true,required:true},{y:true,number:true,required:true},{x:true,number:true,required:true},{y:true,number:true,required:true}]}function E(F){F.hooks.processRawData.push(C);F.hooks.draw.push(A)}D.plot.plugins.push({init:E,options:B,name:"image",version:"1.1"})})(jQuery);
\ No newline at end of file
diff --git a/lib/logstash/web/public/js/flot/jquery.flot.js b/lib/logstash/web/public/js/flot/jquery.flot.js
deleted file mode 100644
index 6534a4685c9..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.flot.js
+++ /dev/null
@@ -1,2119 +0,0 @@
-/* Javascript plotting library for jQuery, v. 0.6.
- *
- * Released under the MIT license by IOLA, December 2007.
- *
- */
-
-// first an inline dependency, jquery.colorhelpers.js, we inline it here
-// for convenience
-
-/* Plugin for jQuery for working with colors.
- * 
- * Version 1.0.
- * 
- * Inspiration from jQuery color animation plugin by John Resig.
- *
- * Released under the MIT license by Ole Laursen, October 2009.
- *
- * Examples:
- *
- *   $.color.parse("#fff").scale('rgb', 0.25).add('a', -0.5).toString()
- *   var c = $.color.extract($("#mydiv"), 'background-color');
- *   console.log(c.r, c.g, c.b, c.a);
- *   $.color.make(100, 50, 25, 0.4).toString() // returns "rgba(100,50,25,0.4)"
- *
- * Note that .scale() and .add() work in-place instead of returning
- * new objects.
- */ 
-(function(){jQuery.color={};jQuery.color.make=function(E,D,B,C){var F={};F.r=E||0;F.g=D||0;F.b=B||0;F.a=C!=null?C:1;F.add=function(I,H){for(var G=0;G<I.length;++G){F[I.charAt(G)]+=H}return F.normalize()};F.scale=function(I,H){for(var G=0;G<I.length;++G){F[I.charAt(G)]*=H}return F.normalize()};F.toString=function(){if(F.a>=1){return"rgb("+[F.r,F.g,F.b].join(",")+")"}else{return"rgba("+[F.r,F.g,F.b,F.a].join(",")+")"}};F.normalize=function(){function G(I,J,H){return J<I?I:(J>H?H:J)}F.r=G(0,parseInt(F.r),255);F.g=G(0,parseInt(F.g),255);F.b=G(0,parseInt(F.b),255);F.a=G(0,F.a,1);return F};F.clone=function(){return jQuery.color.make(F.r,F.b,F.g,F.a)};return F.normalize()};jQuery.color.extract=function(C,B){var D;do{D=C.css(B).toLowerCase();if(D!=""&&D!="transparent"){break}C=C.parent()}while(!jQuery.nodeName(C.get(0),"body"));if(D=="rgba(0, 0, 0, 0)"){D="transparent"}return jQuery.color.parse(D)};jQuery.color.parse=function(E){var D,B=jQuery.color.make;if(D=/rgb\(\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*\)/.exec(E)){return B(parseInt(D[1],10),parseInt(D[2],10),parseInt(D[3],10))}if(D=/rgba\(\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*,\s*([0-9]+(?:\.[0-9]+)?)\s*\)/.exec(E)){return B(parseInt(D[1],10),parseInt(D[2],10),parseInt(D[3],10),parseFloat(D[4]))}if(D=/rgb\(\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*\)/.exec(E)){return B(parseFloat(D[1])*2.55,parseFloat(D[2])*2.55,parseFloat(D[3])*2.55)}if(D=/rgba\(\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\s*\)/.exec(E)){return B(parseFloat(D[1])*2.55,parseFloat(D[2])*2.55,parseFloat(D[3])*2.55,parseFloat(D[4]))}if(D=/#([a-fA-F0-9]{2})([a-fA-F0-9]{2})([a-fA-F0-9]{2})/.exec(E)){return B(parseInt(D[1],16),parseInt(D[2],16),parseInt(D[3],16))}if(D=/#([a-fA-F0-9])([a-fA-F0-9])([a-fA-F0-9])/.exec(E)){return B(parseInt(D[1]+D[1],16),parseInt(D[2]+D[2],16),parseInt(D[3]+D[3],16))}var C=jQuery.trim(E).toLowerCase();if(C=="transparent"){return B(255,255,255,0)}else{D=A[C];return B(D[0],D[1],D[2])}};var A={aqua:[0,255,255],azure:[240,255,255],beige:[245,245,220],black:[0,0,0],blue:[0,0,255],brown:[165,42,42],cyan:[0,255,255],darkblue:[0,0,139],darkcyan:[0,139,139],darkgrey:[169,169,169],darkgreen:[0,100,0],darkkhaki:[189,183,107],darkmagenta:[139,0,139],darkolivegreen:[85,107,47],darkorange:[255,140,0],darkorchid:[153,50,204],darkred:[139,0,0],darksalmon:[233,150,122],darkviolet:[148,0,211],fuchsia:[255,0,255],gold:[255,215,0],green:[0,128,0],indigo:[75,0,130],khaki:[240,230,140],lightblue:[173,216,230],lightcyan:[224,255,255],lightgreen:[144,238,144],lightgrey:[211,211,211],lightpink:[255,182,193],lightyellow:[255,255,224],lime:[0,255,0],magenta:[255,0,255],maroon:[128,0,0],navy:[0,0,128],olive:[128,128,0],orange:[255,165,0],pink:[255,192,203],purple:[128,0,128],violet:[128,0,128],red:[255,0,0],silver:[192,192,192],white:[255,255,255],yellow:[255,255,0]}})();
-
-// the actual Flot code
-(function($) {
-    function Plot(placeholder, data_, options_, plugins) {
-        // data is on the form:
-        //   [ series1, series2 ... ]
-        // where series is either just the data as [ [x1, y1], [x2, y2], ... ]
-        // or { data: [ [x1, y1], [x2, y2], ... ], label: "some label", ... }
-        
-        var series = [],
-            options = {
-                // the color theme used for graphs
-                colors: ["#edc240", "#afd8f8", "#cb4b4b", "#4da74d", "#9440ed"],
-                legend: {
-                    show: true,
-                    noColumns: 1, // number of colums in legend table
-                    labelFormatter: null, // fn: string -> string
-                    labelBoxBorderColor: "#ccc", // border color for the little label boxes
-                    container: null, // container (as jQuery object) to put legend in, null means default on top of graph
-                    position: "ne", // position of default legend container within plot
-                    margin: 5, // distance from grid edge to default legend container within plot
-                    backgroundColor: null, // null means auto-detect
-                    backgroundOpacity: 0.85 // set to 0 to avoid background
-                },
-                xaxis: {
-                    mode: null, // null or "time"
-                    transform: null, // null or f: number -> number to transform axis
-                    inverseTransform: null, // if transform is set, this should be the inverse function
-                    min: null, // min. value to show, null means set automatically
-                    max: null, // max. value to show, null means set automatically
-                    autoscaleMargin: null, // margin in % to add if auto-setting min/max
-                    ticks: null, // either [1, 3] or [[1, "a"], 3] or (fn: axis info -> ticks) or app. number of ticks for auto-ticks
-                    tickFormatter: null, // fn: number -> string
-                    labelWidth: null, // size of tick labels in pixels
-                    labelHeight: null,
-                    
-                    // mode specific options
-                    tickDecimals: null, // no. of decimals, null means auto
-                    tickSize: null, // number or [number, "unit"]
-                    minTickSize: null, // number or [number, "unit"]
-                    monthNames: null, // list of names of months
-                    timeformat: null, // format string to use
-                    twelveHourClock: false // 12 or 24 time in time mode
-                },
-                yaxis: {
-                    autoscaleMargin: 0.02
-                },
-                x2axis: {
-                    autoscaleMargin: null
-                },
-                y2axis: {
-                    autoscaleMargin: 0.02
-                },
-                series: {
-                    points: {
-                        show: false,
-                        radius: 3,
-                        lineWidth: 2, // in pixels
-                        fill: true,
-                        fillColor: "#ffffff"
-                    },
-                    lines: {
-                        // we don't put in show: false so we can see
-                        // whether lines were actively disabled 
-                        lineWidth: 2, // in pixels
-                        fill: false,
-                        fillColor: null,
-                        steps: false
-                    },
-                    bars: {
-                        show: false,
-                        lineWidth: 2, // in pixels
-                        barWidth: 1, // in units of the x axis
-                        fill: true,
-                        fillColor: null,
-                        align: "left", // or "center" 
-                        horizontal: false // when horizontal, left is now top
-                    },
-                    shadowSize: 3
-                },
-                grid: {
-                    show: true,
-                    aboveData: false,
-                    color: "#545454", // primary color used for outline and labels
-                    backgroundColor: null, // null for transparent, else color
-                    tickColor: "rgba(0,0,0,0.15)", // color used for the ticks
-                    labelMargin: 5, // in pixels
-                    borderWidth: 2, // in pixels
-                    borderColor: null, // set if different from the grid color
-                    markings: null, // array of ranges or fn: axes -> array of ranges
-                    markingsColor: "#f4f4f4",
-                    markingsLineWidth: 2,
-                    // interactive stuff
-                    clickable: false,
-                    hoverable: false,
-                    autoHighlight: true, // highlight in case mouse is near
-                    mouseActiveRadius: 10 // how far the mouse can be away to activate an item
-                },
-                hooks: {}
-            },
-        canvas = null,      // the canvas for the plot itself
-        overlay = null,     // canvas for interactive stuff on top of plot
-        eventHolder = null, // jQuery object that events should be bound to
-        ctx = null, octx = null,
-        axes = { xaxis: {}, yaxis: {}, x2axis: {}, y2axis: {} },
-        plotOffset = { left: 0, right: 0, top: 0, bottom: 0},
-        canvasWidth = 0, canvasHeight = 0,
-        plotWidth = 0, plotHeight = 0,
-        hooks = {
-            processOptions: [],
-            processRawData: [],
-            processDatapoints: [],
-            draw: [],
-            bindEvents: [],
-            drawOverlay: []
-        },
-        plot = this;
-
-        // public functions
-        plot.setData = setData;
-        plot.setupGrid = setupGrid;
-        plot.draw = draw;
-        plot.getPlaceholder = function() { return placeholder; };
-        plot.getCanvas = function() { return canvas; };
-        plot.getPlotOffset = function() { return plotOffset; };
-        plot.width = function () { return plotWidth; };
-        plot.height = function () { return plotHeight; };
-        plot.offset = function () {
-            var o = eventHolder.offset();
-            o.left += plotOffset.left;
-            o.top += plotOffset.top;
-            return o;
-        };
-        plot.getData = function() { return series; };
-        plot.getAxes = function() { return axes; };
-        plot.getOptions = function() { return options; };
-        plot.highlight = highlight;
-        plot.unhighlight = unhighlight;
-        plot.triggerRedrawOverlay = triggerRedrawOverlay;
-        plot.pointOffset = function(point) {
-            return { left: parseInt(axisSpecToRealAxis(point, "xaxis").p2c(+point.x) + plotOffset.left),
-                     top: parseInt(axisSpecToRealAxis(point, "yaxis").p2c(+point.y) + plotOffset.top) };
-        };
-        
-
-        // public attributes
-        plot.hooks = hooks;
-        
-        // initialize
-        initPlugins(plot);
-        parseOptions(options_);
-        constructCanvas();
-        setData(data_);
-        setupGrid();
-        draw();
-        bindEvents();
-
-
-        function executeHooks(hook, args) {
-            args = [plot].concat(args);
-            for (var i = 0; i < hook.length; ++i)
-                hook[i].apply(this, args);
-        }
-
-        function initPlugins() {
-            for (var i = 0; i < plugins.length; ++i) {
-                var p = plugins[i];
-                p.init(plot);
-                if (p.options)
-                    $.extend(true, options, p.options);
-            }
-        }
-        
-        function parseOptions(opts) {
-            $.extend(true, options, opts);
-            if (options.grid.borderColor == null)
-                options.grid.borderColor = options.grid.color;
-            // backwards compatibility, to be removed in future
-            if (options.xaxis.noTicks && options.xaxis.ticks == null)
-                options.xaxis.ticks = options.xaxis.noTicks;
-            if (options.yaxis.noTicks && options.yaxis.ticks == null)
-                options.yaxis.ticks = options.yaxis.noTicks;
-            if (options.grid.coloredAreas)
-                options.grid.markings = options.grid.coloredAreas;
-            if (options.grid.coloredAreasColor)
-                options.grid.markingsColor = options.grid.coloredAreasColor;
-            if (options.lines)
-                $.extend(true, options.series.lines, options.lines);
-            if (options.points)
-                $.extend(true, options.series.points, options.points);
-            if (options.bars)
-                $.extend(true, options.series.bars, options.bars);
-            if (options.shadowSize)
-                options.series.shadowSize = options.shadowSize;
-
-            for (var n in hooks)
-                if (options.hooks[n] && options.hooks[n].length)
-                    hooks[n] = hooks[n].concat(options.hooks[n]);
-
-            executeHooks(hooks.processOptions, [options]);
-        }
-
-        function setData(d) {
-            series = parseData(d);
-            fillInSeriesOptions();
-            processData();
-        }
-        
-        function parseData(d) {
-            var res = [];
-            for (var i = 0; i < d.length; ++i) {
-                var s = $.extend(true, {}, options.series);
-
-                if (d[i].data) {
-                    s.data = d[i].data; // move the data instead of deep-copy
-                    delete d[i].data;
-
-                    $.extend(true, s, d[i]);
-
-                    d[i].data = s.data;
-                }
-                else
-                    s.data = d[i];
-                res.push(s);
-            }
-
-            return res;
-        }
-        
-        function axisSpecToRealAxis(obj, attr) {
-            var a = obj[attr];
-            if (!a || a == 1)
-                return axes[attr];
-            if (typeof a == "number")
-                return axes[attr.charAt(0) + a + attr.slice(1)];
-            return a; // assume it's OK
-        }
-        
-        function fillInSeriesOptions() {
-            var i;
-            
-            // collect what we already got of colors
-            var neededColors = series.length,
-                usedColors = [],
-                assignedColors = [];
-            for (i = 0; i < series.length; ++i) {
-                var sc = series[i].color;
-                if (sc != null) {
-                    --neededColors;
-                    if (typeof sc == "number")
-                        assignedColors.push(sc);
-                    else
-                        usedColors.push($.color.parse(series[i].color));
-                }
-            }
-            
-            // we might need to generate more colors if higher indices
-            // are assigned
-            for (i = 0; i < assignedColors.length; ++i) {
-                neededColors = Math.max(neededColors, assignedColors[i] + 1);
-            }
-
-            // produce colors as needed
-            var colors = [], variation = 0;
-            i = 0;
-            while (colors.length < neededColors) {
-                var c;
-                if (options.colors.length == i) // check degenerate case
-                    c = $.color.make(100, 100, 100);
-                else
-                    c = $.color.parse(options.colors[i]);
-
-                // vary color if needed
-                var sign = variation % 2 == 1 ? -1 : 1;
-                c.scale('rgb', 1 + sign * Math.ceil(variation / 2) * 0.2)
-
-                // FIXME: if we're getting to close to something else,
-                // we should probably skip this one
-                colors.push(c);
-                
-                ++i;
-                if (i >= options.colors.length) {
-                    i = 0;
-                    ++variation;
-                }
-            }
-
-            // fill in the options
-            var colori = 0, s;
-            for (i = 0; i < series.length; ++i) {
-                s = series[i];
-                
-                // assign colors
-                if (s.color == null) {
-                    s.color = colors[colori].toString();
-                    ++colori;
-                }
-                else if (typeof s.color == "number")
-                    s.color = colors[s.color].toString();
-
-                // turn on lines automatically in case nothing is set
-                if (s.lines.show == null) {
-                    var v, show = true;
-                    for (v in s)
-                        if (s[v].show) {
-                            show = false;
-                            break;
-                        }
-                    if (show)
-                        s.lines.show = true;
-                }
-
-                // setup axes
-                s.xaxis = axisSpecToRealAxis(s, "xaxis");
-                s.yaxis = axisSpecToRealAxis(s, "yaxis");
-            }
-        }
-        
-        function processData() {
-            var topSentry = Number.POSITIVE_INFINITY,
-                bottomSentry = Number.NEGATIVE_INFINITY,
-                i, j, k, m, length,
-                s, points, ps, x, y, axis, val, f, p;
-
-            for (axis in axes) {
-                axes[axis].datamin = topSentry;
-                axes[axis].datamax = bottomSentry;
-                axes[axis].used = false;
-            }
-
-            function updateAxis(axis, min, max) {
-                if (min < axis.datamin)
-                    axis.datamin = min;
-                if (max > axis.datamax)
-                    axis.datamax = max;
-            }
-
-            for (i = 0; i < series.length; ++i) {
-                s = series[i];
-                s.datapoints = { points: [] };
-                
-                executeHooks(hooks.processRawData, [ s, s.data, s.datapoints ]);
-            }
-            
-            // first pass: clean and copy data
-            for (i = 0; i < series.length; ++i) {
-                s = series[i];
-
-                var data = s.data, format = s.datapoints.format;
-
-                if (!format) {
-                    format = [];
-                    // find out how to copy
-                    format.push({ x: true, number: true, required: true });
-                    format.push({ y: true, number: true, required: true });
-
-                    if (s.bars.show)
-                        format.push({ y: true, number: true, required: false, defaultValue: 0 });
-                    
-                    s.datapoints.format = format;
-                }
-
-                if (s.datapoints.pointsize != null)
-                    continue; // already filled in
-
-                if (s.datapoints.pointsize == null)
-                    s.datapoints.pointsize = format.length;
-                
-                ps = s.datapoints.pointsize;
-                points = s.datapoints.points;
-
-                insertSteps = s.lines.show && s.lines.steps;
-                s.xaxis.used = s.yaxis.used = true;
-                
-                for (j = k = 0; j < data.length; ++j, k += ps) {
-                    p = data[j];
-
-                    var nullify = p == null;
-                    if (!nullify) {
-                        for (m = 0; m < ps; ++m) {
-                            val = p[m];
-                            f = format[m];
-
-                            if (f) {
-                                if (f.number && val != null) {
-                                    val = +val; // convert to number
-                                    if (isNaN(val))
-                                        val = null;
-                                }
-
-                                if (val == null) {
-                                    if (f.required)
-                                        nullify = true;
-                                    
-                                    if (f.defaultValue != null)
-                                        val = f.defaultValue;
-                                }
-                            }
-                            
-                            points[k + m] = val;
-                        }
-                    }
-                    
-                    if (nullify) {
-                        for (m = 0; m < ps; ++m) {
-                            val = points[k + m];
-                            if (val != null) {
-                                f = format[m];
-                                // extract min/max info
-                                if (f.x)
-                                    updateAxis(s.xaxis, val, val);
-                                if (f.y)
-                                    updateAxis(s.yaxis, val, val);
-                            }
-                            points[k + m] = null;
-                        }
-                    }
-                    else {
-                        // a little bit of line specific stuff that
-                        // perhaps shouldn't be here, but lacking
-                        // better means...
-                        if (insertSteps && k > 0
-                            && points[k - ps] != null
-                            && points[k - ps] != points[k]
-                            && points[k - ps + 1] != points[k + 1]) {
-                            // copy the point to make room for a middle point
-                            for (m = 0; m < ps; ++m)
-                                points[k + ps + m] = points[k + m];
-
-                            // middle point has same y
-                            points[k + 1] = points[k - ps + 1];
-
-                            // we've added a point, better reflect that
-                            k += ps;
-                        }
-                    }
-                }
-            }
-
-            // give the hooks a chance to run
-            for (i = 0; i < series.length; ++i) {
-                s = series[i];
-                
-                executeHooks(hooks.processDatapoints, [ s, s.datapoints]);
-            }
-
-            // second pass: find datamax/datamin for auto-scaling
-            for (i = 0; i < series.length; ++i) {
-                s = series[i];
-                points = s.datapoints.points,
-                ps = s.datapoints.pointsize;
-
-                var xmin = topSentry, ymin = topSentry,
-                    xmax = bottomSentry, ymax = bottomSentry;
-                
-                for (j = 0; j < points.length; j += ps) {
-                    if (points[j] == null)
-                        continue;
-
-                    for (m = 0; m < ps; ++m) {
-                        val = points[j + m];
-                        f = format[m];
-                        if (!f)
-                            continue;
-                        
-                        if (f.x) {
-                            if (val < xmin)
-                                xmin = val;
-                            if (val > xmax)
-                                xmax = val;
-                        }
-                        if (f.y) {
-                            if (val < ymin)
-                                ymin = val;
-                            if (val > ymax)
-                                ymax = val;
-                        }
-                    }
-                }
-                
-                if (s.bars.show) {
-                    // make sure we got room for the bar on the dancing floor
-                    var delta = s.bars.align == "left" ? 0 : -s.bars.barWidth/2;
-                    if (s.bars.horizontal) {
-                        ymin += delta;
-                        ymax += delta + s.bars.barWidth;
-                    }
-                    else {
-                        xmin += delta;
-                        xmax += delta + s.bars.barWidth;
-                    }
-                }
-                
-                updateAxis(s.xaxis, xmin, xmax);
-                updateAxis(s.yaxis, ymin, ymax);
-            }
-
-            for (axis in axes) {
-                if (axes[axis].datamin == topSentry)
-                    axes[axis].datamin = null;
-                if (axes[axis].datamax == bottomSentry)
-                    axes[axis].datamax = null;
-            }
-        }
-
-        function constructCanvas() {
-            function makeCanvas(width, height) {
-                var c = document.createElement('canvas');
-                c.width = width;
-                c.height = height;
-                if ($.browser.msie) // excanvas hack
-                    c = window.G_vmlCanvasManager.initElement(c);
-                return c;
-            }
-            
-            canvasWidth = placeholder.width();
-            canvasHeight = placeholder.height();
-            placeholder.html(""); // clear placeholder
-            if (placeholder.css("position") == 'static')
-                placeholder.css("position", "relative"); // for positioning labels and overlay
-
-            if (canvasWidth <= 0 || canvasHeight <= 0)
-                throw "Invalid dimensions for plot, width = " + canvasWidth + ", height = " + canvasHeight;
-
-            if ($.browser.msie) // excanvas hack
-                window.G_vmlCanvasManager.init_(document); // make sure everything is setup
-            
-            // the canvas
-            canvas = $(makeCanvas(canvasWidth, canvasHeight)).appendTo(placeholder).get(0);
-            ctx = canvas.getContext("2d");
-
-            // overlay canvas for interactive features
-            overlay = $(makeCanvas(canvasWidth, canvasHeight)).css({ position: 'absolute', left: 0, top: 0 }).appendTo(placeholder).get(0);
-            octx = overlay.getContext("2d");
-            octx.stroke();
-        }
-
-        function bindEvents() {
-            // we include the canvas in the event holder too, because IE 7
-            // sometimes has trouble with the stacking order
-            eventHolder = $([overlay, canvas]);
-
-            // bind events
-            if (options.grid.hoverable)
-                eventHolder.mousemove(onMouseMove);
-
-            if (options.grid.clickable)
-                eventHolder.click(onClick);
-
-            executeHooks(hooks.bindEvents, [eventHolder]);
-        }
-
-        function setupGrid() {
-            function setTransformationHelpers(axis, o) {
-                function identity(x) { return x; }
-                
-                var s, m, t = o.transform || identity,
-                    it = o.inverseTransform;
-                    
-                // add transformation helpers
-                if (axis == axes.xaxis || axis == axes.x2axis) {
-                    // precompute how much the axis is scaling a point
-                    // in canvas space
-                    s = axis.scale = plotWidth / (t(axis.max) - t(axis.min));
-                    m = t(axis.min);
-
-                    // data point to canvas coordinate
-                    if (t == identity) // slight optimization
-                        axis.p2c = function (p) { return (p - m) * s; };
-                    else
-                        axis.p2c = function (p) { return (t(p) - m) * s; };
-                    // canvas coordinate to data point
-                    if (!it)
-                        axis.c2p = function (c) { return m + c / s; };
-                    else
-                        axis.c2p = function (c) { return it(m + c / s); };
-                }
-                else {
-                    s = axis.scale = plotHeight / (t(axis.max) - t(axis.min));
-                    m = t(axis.max);
-                    
-                    if (t == identity)
-                        axis.p2c = function (p) { return (m - p) * s; };
-                    else
-                        axis.p2c = function (p) { return (m - t(p)) * s; };
-                    if (!it)
-                        axis.c2p = function (c) { return m - c / s; };
-                    else
-                        axis.c2p = function (c) { return it(m - c / s); };
-                }
-            }
-
-            function measureLabels(axis, axisOptions) {
-                var i, labels = [], l;
-                
-                axis.labelWidth = axisOptions.labelWidth;
-                axis.labelHeight = axisOptions.labelHeight;
-
-                if (axis == axes.xaxis || axis == axes.x2axis) {
-                    // to avoid measuring the widths of the labels, we
-                    // construct fixed-size boxes and put the labels inside
-                    // them, we don't need the exact figures and the
-                    // fixed-size box content is easy to center
-                    if (axis.labelWidth == null)
-                        axis.labelWidth = canvasWidth / (axis.ticks.length > 0 ? axis.ticks.length : 1);
-
-                    // measure x label heights
-                    if (axis.labelHeight == null) {
-                        labels = [];
-                        for (i = 0; i < axis.ticks.length; ++i) {
-                            l = axis.ticks[i].label;
-                            if (l)
-                                labels.push('<div class="tickLabel" style="float:left;width:' + axis.labelWidth + 'px">' + l + '</div>');
-                        }
-                        
-                        if (labels.length > 0) {
-                            var dummyDiv = $('<div style="position:absolute;top:-10000px;width:10000px;font-size:smaller">'
-                                             + labels.join("") + '<div style="clear:left"></div></div>').appendTo(placeholder);
-                            axis.labelHeight = dummyDiv.height();
-                            dummyDiv.remove();
-                        }
-                    }
-                }
-                else if (axis.labelWidth == null || axis.labelHeight == null) {
-                    // calculate y label dimensions
-                    for (i = 0; i < axis.ticks.length; ++i) {
-                        l = axis.ticks[i].label;
-                        if (l)
-                            labels.push('<div class="tickLabel">' + l + '</div>');
-                    }
-                    
-                    if (labels.length > 0) {
-                        var dummyDiv = $('<div style="position:absolute;top:-10000px;font-size:smaller">'
-                                         + labels.join("") + '</div>').appendTo(placeholder);
-                        if (axis.labelWidth == null)
-                            axis.labelWidth = dummyDiv.width();
-                        if (axis.labelHeight == null)
-                            axis.labelHeight = dummyDiv.find("div").height();
-                        dummyDiv.remove();
-                    }
-                    
-                }
-
-                if (axis.labelWidth == null)
-                    axis.labelWidth = 0;
-                if (axis.labelHeight == null)
-                    axis.labelHeight = 0;
-            }
-            
-            function setGridSpacing() {
-                // get the most space needed around the grid for things
-                // that may stick out
-                var maxOutset = options.grid.borderWidth;
-                for (i = 0; i < series.length; ++i)
-                    maxOutset = Math.max(maxOutset, 2 * (series[i].points.radius + series[i].points.lineWidth/2));
-                
-                plotOffset.left = plotOffset.right = plotOffset.top = plotOffset.bottom = maxOutset;
-                
-                var margin = options.grid.labelMargin + options.grid.borderWidth;
-                
-                if (axes.xaxis.labelHeight > 0)
-                    plotOffset.bottom = Math.max(maxOutset, axes.xaxis.labelHeight + margin);
-                if (axes.yaxis.labelWidth > 0)
-                    plotOffset.left = Math.max(maxOutset, axes.yaxis.labelWidth + margin);
-                if (axes.x2axis.labelHeight > 0)
-                    plotOffset.top = Math.max(maxOutset, axes.x2axis.labelHeight + margin);
-                if (axes.y2axis.labelWidth > 0)
-                    plotOffset.right = Math.max(maxOutset, axes.y2axis.labelWidth + margin);
-            
-                plotWidth = canvasWidth - plotOffset.left - plotOffset.right;
-                plotHeight = canvasHeight - plotOffset.bottom - plotOffset.top;
-            }
-            
-            var axis;
-            for (axis in axes)
-                setRange(axes[axis], options[axis]);
-            
-            if (options.grid.show) {
-                for (axis in axes) {
-                    prepareTickGeneration(axes[axis], options[axis]);
-                    setTicks(axes[axis], options[axis]);
-                    measureLabels(axes[axis], options[axis]);
-                }
-
-                setGridSpacing();
-            }
-            else {
-                plotOffset.left = plotOffset.right = plotOffset.top = plotOffset.bottom = 0;
-                plotWidth = canvasWidth;
-                plotHeight = canvasHeight;
-            }
-            
-            for (axis in axes)
-                setTransformationHelpers(axes[axis], options[axis]);
-
-            if (options.grid.show)
-                insertLabels();
-            
-            insertLegend();
-        }
-        
-        function setRange(axis, axisOptions) {
-            var min = +(axisOptions.min != null ? axisOptions.min : axis.datamin),
-                max = +(axisOptions.max != null ? axisOptions.max : axis.datamax),
-                delta = max - min;
-
-            if (delta == 0.0) {
-                // degenerate case
-                var widen = max == 0 ? 1 : 0.01;
-
-                if (axisOptions.min == null)
-                    min -= widen;
-                // alway widen max if we couldn't widen min to ensure we
-                // don't fall into min == max which doesn't work
-                if (axisOptions.max == null || axisOptions.min != null)
-                    max += widen;
-            }
-            else {
-                // consider autoscaling
-                var margin = axisOptions.autoscaleMargin;
-                if (margin != null) {
-                    if (axisOptions.min == null) {
-                        min -= delta * margin;
-                        // make sure we don't go below zero if all values
-                        // are positive
-                        if (min < 0 && axis.datamin != null && axis.datamin >= 0)
-                            min = 0;
-                    }
-                    if (axisOptions.max == null) {
-                        max += delta * margin;
-                        if (max > 0 && axis.datamax != null && axis.datamax <= 0)
-                            max = 0;
-                    }
-                }
-            }
-            axis.min = min;
-            axis.max = max;
-        }
-
-        function prepareTickGeneration(axis, axisOptions) {
-            // estimate number of ticks
-            var noTicks;
-            if (typeof axisOptions.ticks == "number" && axisOptions.ticks > 0)
-                noTicks = axisOptions.ticks;
-            else if (axis == axes.xaxis || axis == axes.x2axis)
-                 // heuristic based on the model a*sqrt(x) fitted to
-                 // some reasonable data points
-                noTicks = 0.3 * Math.sqrt(canvasWidth);
-            else
-                noTicks = 0.3 * Math.sqrt(canvasHeight);
-            
-            var delta = (axis.max - axis.min) / noTicks,
-                size, generator, unit, formatter, i, magn, norm;
-
-            if (axisOptions.mode == "time") {
-                // pretty handling of time
-                
-                // map of app. size of time units in milliseconds
-                var timeUnitSize = {
-                    "second": 1000,
-                    "minute": 60 * 1000,
-                    "hour": 60 * 60 * 1000,
-                    "day": 24 * 60 * 60 * 1000,
-                    "month": 30 * 24 * 60 * 60 * 1000,
-                    "year": 365.2425 * 24 * 60 * 60 * 1000
-                };
-
-
-                // the allowed tick sizes, after 1 year we use
-                // an integer algorithm
-                var spec = [
-                    [1, "second"], [2, "second"], [5, "second"], [10, "second"],
-                    [30, "second"], 
-                    [1, "minute"], [2, "minute"], [5, "minute"], [10, "minute"],
-                    [30, "minute"], 
-                    [1, "hour"], [2, "hour"], [4, "hour"],
-                    [8, "hour"], [12, "hour"],
-                    [1, "day"], [2, "day"], [3, "day"],
-                    [0.25, "month"], [0.5, "month"], [1, "month"],
-                    [2, "month"], [3, "month"], [6, "month"],
-                    [1, "year"]
-                ];
-
-                var minSize = 0;
-                if (axisOptions.minTickSize != null) {
-                    if (typeof axisOptions.tickSize == "number")
-                        minSize = axisOptions.tickSize;
-                    else
-                        minSize = axisOptions.minTickSize[0] * timeUnitSize[axisOptions.minTickSize[1]];
-                }
-
-                for (i = 0; i < spec.length - 1; ++i)
-                    if (delta < (spec[i][0] * timeUnitSize[spec[i][1]]
-                                 + spec[i + 1][0] * timeUnitSize[spec[i + 1][1]]) / 2
-                       && spec[i][0] * timeUnitSize[spec[i][1]] >= minSize)
-                        break;
-                size = spec[i][0];
-                unit = spec[i][1];
-                
-                // special-case the possibility of several years
-                if (unit == "year") {
-                    magn = Math.pow(10, Math.floor(Math.log(delta / timeUnitSize.year) / Math.LN10));
-                    norm = (delta / timeUnitSize.year) / magn;
-                    if (norm < 1.5)
-                        size = 1;
-                    else if (norm < 3)
-                        size = 2;
-                    else if (norm < 7.5)
-                        size = 5;
-                    else
-                        size = 10;
-
-                    size *= magn;
-                }
-
-                if (axisOptions.tickSize) {
-                    size = axisOptions.tickSize[0];
-                    unit = axisOptions.tickSize[1];
-                }
-                
-                generator = function(axis) {
-                    var ticks = [],
-                        tickSize = axis.tickSize[0], unit = axis.tickSize[1],
-                        d = new Date(axis.min);
-                    
-                    var step = tickSize * timeUnitSize[unit];
-
-                    if (unit == "second")
-                        d.setUTCSeconds(floorInBase(d.getUTCSeconds(), tickSize));
-                    if (unit == "minute")
-                        d.setUTCMinutes(floorInBase(d.getUTCMinutes(), tickSize));
-                    if (unit == "hour")
-                        d.setUTCHours(floorInBase(d.getUTCHours(), tickSize));
-                    if (unit == "month")
-                        d.setUTCMonth(floorInBase(d.getUTCMonth(), tickSize));
-                    if (unit == "year")
-                        d.setUTCFullYear(floorInBase(d.getUTCFullYear(), tickSize));
-                    
-                    // reset smaller components
-                    d.setUTCMilliseconds(0);
-                    if (step >= timeUnitSize.minute)
-                        d.setUTCSeconds(0);
-                    if (step >= timeUnitSize.hour)
-                        d.setUTCMinutes(0);
-                    if (step >= timeUnitSize.day)
-                        d.setUTCHours(0);
-                    if (step >= timeUnitSize.day * 4)
-                        d.setUTCDate(1);
-                    if (step >= timeUnitSize.year)
-                        d.setUTCMonth(0);
-
-
-                    var carry = 0, v = Number.NaN, prev;
-                    do {
-                        prev = v;
-                        v = d.getTime();
-                        ticks.push({ v: v, label: axis.tickFormatter(v, axis) });
-                        if (unit == "month") {
-                            if (tickSize < 1) {
-                                // a bit complicated - we'll divide the month
-                                // up but we need to take care of fractions
-                                // so we don't end up in the middle of a day
-                                d.setUTCDate(1);
-                                var start = d.getTime();
-                                d.setUTCMonth(d.getUTCMonth() + 1);
-                                var end = d.getTime();
-                                d.setTime(v + carry * timeUnitSize.hour + (end - start) * tickSize);
-                                carry = d.getUTCHours();
-                                d.setUTCHours(0);
-                            }
-                            else
-                                d.setUTCMonth(d.getUTCMonth() + tickSize);
-                        }
-                        else if (unit == "year") {
-                            d.setUTCFullYear(d.getUTCFullYear() + tickSize);
-                        }
-                        else
-                            d.setTime(v + step);
-                    } while (v < axis.max && v != prev);
-
-                    return ticks;
-                };
-
-                formatter = function (v, axis) {
-                    var d = new Date(v);
-
-                    // first check global format
-                    if (axisOptions.timeformat != null)
-                        return $.plot.formatDate(d, axisOptions.timeformat, axisOptions.monthNames);
-                    
-                    var t = axis.tickSize[0] * timeUnitSize[axis.tickSize[1]];
-                    var span = axis.max - axis.min;
-                    var suffix = (axisOptions.twelveHourClock) ? " %p" : "";
-                    
-                    if (t < timeUnitSize.minute)
-                        fmt = "%h:%M:%S" + suffix;
-                    else if (t < timeUnitSize.day) {
-                        if (span < 2 * timeUnitSize.day)
-                            fmt = "%h:%M" + suffix;
-                        else
-                            fmt = "%b %d %h:%M" + suffix;
-                    }
-                    else if (t < timeUnitSize.month)
-                        fmt = "%b %d";
-                    else if (t < timeUnitSize.year) {
-                        if (span < timeUnitSize.year)
-                            fmt = "%b";
-                        else
-                            fmt = "%b %y";
-                    }
-                    else
-                        fmt = "%y";
-                    
-                    return $.plot.formatDate(d, fmt, axisOptions.monthNames);
-                };
-            }
-            else {
-                // pretty rounding of base-10 numbers
-                var maxDec = axisOptions.tickDecimals;
-                var dec = -Math.floor(Math.log(delta) / Math.LN10);
-                if (maxDec != null && dec > maxDec)
-                    dec = maxDec;
-
-                magn = Math.pow(10, -dec);
-                norm = delta / magn; // norm is between 1.0 and 10.0
-                
-                if (norm < 1.5)
-                    size = 1;
-                else if (norm < 3) {
-                    size = 2;
-                    // special case for 2.5, requires an extra decimal
-                    if (norm > 2.25 && (maxDec == null || dec + 1 <= maxDec)) {
-                        size = 2.5;
-                        ++dec;
-                    }
-                }
-                else if (norm < 7.5)
-                    size = 5;
-                else
-                    size = 10;
-
-                size *= magn;
-                
-                if (axisOptions.minTickSize != null && size < axisOptions.minTickSize)
-                    size = axisOptions.minTickSize;
-
-                if (axisOptions.tickSize != null)
-                    size = axisOptions.tickSize;
-
-                axis.tickDecimals = Math.max(0, (maxDec != null) ? maxDec : dec);
-
-                generator = function (axis) {
-                    var ticks = [];
-
-                    // spew out all possible ticks
-                    var start = floorInBase(axis.min, axis.tickSize),
-                        i = 0, v = Number.NaN, prev;
-                    do {
-                        prev = v;
-                        v = start + i * axis.tickSize;
-                        ticks.push({ v: v, label: axis.tickFormatter(v, axis) });
-                        ++i;
-                    } while (v < axis.max && v != prev);
-                    return ticks;
-                };
-
-                formatter = function (v, axis) {
-                    return v.toFixed(axis.tickDecimals);
-                };
-            }
-
-            axis.tickSize = unit ? [size, unit] : size;
-            axis.tickGenerator = generator;
-            if ($.isFunction(axisOptions.tickFormatter))
-                axis.tickFormatter = function (v, axis) { return "" + axisOptions.tickFormatter(v, axis); };
-            else
-                axis.tickFormatter = formatter;
-        }
-        
-        function setTicks(axis, axisOptions) {
-            axis.ticks = [];
-
-            if (!axis.used)
-                return;
-            
-            if (axisOptions.ticks == null)
-                axis.ticks = axis.tickGenerator(axis);
-            else if (typeof axisOptions.ticks == "number") {
-                if (axisOptions.ticks > 0)
-                    axis.ticks = axis.tickGenerator(axis);
-            }
-            else if (axisOptions.ticks) {
-                var ticks = axisOptions.ticks;
-
-                if ($.isFunction(ticks))
-                    // generate the ticks
-                    ticks = ticks({ min: axis.min, max: axis.max });
-                
-                // clean up the user-supplied ticks, copy them over
-                var i, v;
-                for (i = 0; i < ticks.length; ++i) {
-                    var label = null;
-                    var t = ticks[i];
-                    if (typeof t == "object") {
-                        v = t[0];
-                        if (t.length > 1)
-                            label = t[1];
-                    }
-                    else
-                        v = t;
-                    if (label == null)
-                        label = axis.tickFormatter(v, axis);
-                    axis.ticks[i] = { v: v, label: label };
-                }
-            }
-
-            if (axisOptions.autoscaleMargin != null && axis.ticks.length > 0) {
-                // snap to ticks
-                if (axisOptions.min == null)
-                    axis.min = Math.min(axis.min, axis.ticks[0].v);
-                if (axisOptions.max == null && axis.ticks.length > 1)
-                    axis.max = Math.max(axis.max, axis.ticks[axis.ticks.length - 1].v);
-            }
-        }
-      
-        function draw() {
-            ctx.clearRect(0, 0, canvasWidth, canvasHeight);
-
-            var grid = options.grid;
-            
-            if (grid.show && !grid.aboveData)
-                drawGrid();
-
-            for (var i = 0; i < series.length; ++i)
-                drawSeries(series[i]);
-
-            executeHooks(hooks.draw, [ctx]);
-            
-            if (grid.show && grid.aboveData)
-                drawGrid();
-        }
-
-        function extractRange(ranges, coord) {
-            var firstAxis = coord + "axis",
-                secondaryAxis = coord + "2axis",
-                axis, from, to, reverse;
-
-            if (ranges[firstAxis]) {
-                axis = axes[firstAxis];
-                from = ranges[firstAxis].from;
-                to = ranges[firstAxis].to;
-            }
-            else if (ranges[secondaryAxis]) {
-                axis = axes[secondaryAxis];
-                from = ranges[secondaryAxis].from;
-                to = ranges[secondaryAxis].to;
-            }
-            else {
-                // backwards-compat stuff - to be removed in future
-                axis = axes[firstAxis];
-                from = ranges[coord + "1"];
-                to = ranges[coord + "2"];
-            }
-
-            // auto-reverse as an added bonus
-            if (from != null && to != null && from > to)
-                return { from: to, to: from, axis: axis };
-            
-            return { from: from, to: to, axis: axis };
-        }
-        
-        function drawGrid() {
-            var i;
-            
-            ctx.save();
-            ctx.translate(plotOffset.left, plotOffset.top);
-
-            // draw background, if any
-            if (options.grid.backgroundColor) {
-                ctx.fillStyle = getColorOrGradient(options.grid.backgroundColor, plotHeight, 0, "rgba(255, 255, 255, 0)");
-                ctx.fillRect(0, 0, plotWidth, plotHeight);
-            }
-
-            // draw markings
-            var markings = options.grid.markings;
-            if (markings) {
-                if ($.isFunction(markings))
-                    // xmin etc. are backwards-compatible, to be removed in future
-                    markings = markings({ xmin: axes.xaxis.min, xmax: axes.xaxis.max, ymin: axes.yaxis.min, ymax: axes.yaxis.max, xaxis: axes.xaxis, yaxis: axes.yaxis, x2axis: axes.x2axis, y2axis: axes.y2axis });
-
-                for (i = 0; i < markings.length; ++i) {
-                    var m = markings[i],
-                        xrange = extractRange(m, "x"),
-                        yrange = extractRange(m, "y");
-
-                    // fill in missing
-                    if (xrange.from == null)
-                        xrange.from = xrange.axis.min;
-                    if (xrange.to == null)
-                        xrange.to = xrange.axis.max;
-                    if (yrange.from == null)
-                        yrange.from = yrange.axis.min;
-                    if (yrange.to == null)
-                        yrange.to = yrange.axis.max;
-
-                    // clip
-                    if (xrange.to < xrange.axis.min || xrange.from > xrange.axis.max ||
-                        yrange.to < yrange.axis.min || yrange.from > yrange.axis.max)
-                        continue;
-
-                    xrange.from = Math.max(xrange.from, xrange.axis.min);
-                    xrange.to = Math.min(xrange.to, xrange.axis.max);
-                    yrange.from = Math.max(yrange.from, yrange.axis.min);
-                    yrange.to = Math.min(yrange.to, yrange.axis.max);
-
-                    if (xrange.from == xrange.to && yrange.from == yrange.to)
-                        continue;
-
-                    // then draw
-                    xrange.from = xrange.axis.p2c(xrange.from);
-                    xrange.to = xrange.axis.p2c(xrange.to);
-                    yrange.from = yrange.axis.p2c(yrange.from);
-                    yrange.to = yrange.axis.p2c(yrange.to);
-                    
-                    if (xrange.from == xrange.to || yrange.from == yrange.to) {
-                        // draw line
-                        ctx.beginPath();
-                        ctx.strokeStyle = m.color || options.grid.markingsColor;
-                        ctx.lineWidth = m.lineWidth || options.grid.markingsLineWidth;
-                        //ctx.moveTo(Math.floor(xrange.from), yrange.from);
-                        //ctx.lineTo(Math.floor(xrange.to), yrange.to);
-                        ctx.moveTo(xrange.from, yrange.from);
-                        ctx.lineTo(xrange.to, yrange.to);
-                        ctx.stroke();
-                    }
-                    else {
-                        // fill area
-                        ctx.fillStyle = m.color || options.grid.markingsColor;
-                        ctx.fillRect(xrange.from, yrange.to,
-                                     xrange.to - xrange.from,
-                                     yrange.from - yrange.to);
-                    }
-                }
-            }
-            
-            // draw the inner grid
-            ctx.lineWidth = 1;
-            ctx.strokeStyle = options.grid.tickColor;
-            ctx.beginPath();
-            var v, axis = axes.xaxis;
-            for (i = 0; i < axis.ticks.length; ++i) {
-                v = axis.ticks[i].v;
-                if (v <= axis.min || v >= axes.xaxis.max)
-                    continue;   // skip those lying on the axes
-
-                ctx.moveTo(Math.floor(axis.p2c(v)) + ctx.lineWidth/2, 0);
-                ctx.lineTo(Math.floor(axis.p2c(v)) + ctx.lineWidth/2, plotHeight);
-            }
-
-            axis = axes.yaxis;
-            for (i = 0; i < axis.ticks.length; ++i) {
-                v = axis.ticks[i].v;
-                if (v <= axis.min || v >= axis.max)
-                    continue;
-
-                ctx.moveTo(0, Math.floor(axis.p2c(v)) + ctx.lineWidth/2);
-                ctx.lineTo(plotWidth, Math.floor(axis.p2c(v)) + ctx.lineWidth/2);
-            }
-
-            axis = axes.x2axis;
-            for (i = 0; i < axis.ticks.length; ++i) {
-                v = axis.ticks[i].v;
-                if (v <= axis.min || v >= axis.max)
-                    continue;
-    
-                ctx.moveTo(Math.floor(axis.p2c(v)) + ctx.lineWidth/2, -5);
-                ctx.lineTo(Math.floor(axis.p2c(v)) + ctx.lineWidth/2, 5);
-            }
-
-            axis = axes.y2axis;
-            for (i = 0; i < axis.ticks.length; ++i) {
-                v = axis.ticks[i].v;
-                if (v <= axis.min || v >= axis.max)
-                    continue;
-
-                ctx.moveTo(plotWidth-5, Math.floor(axis.p2c(v)) + ctx.lineWidth/2);
-                ctx.lineTo(plotWidth+5, Math.floor(axis.p2c(v)) + ctx.lineWidth/2);
-            }
-            
-            ctx.stroke();
-            
-            if (options.grid.borderWidth) {
-                // draw border
-                var bw = options.grid.borderWidth;
-                ctx.lineWidth = bw;
-                ctx.strokeStyle = options.grid.borderColor;
-                ctx.strokeRect(-bw/2, -bw/2, plotWidth + bw, plotHeight + bw);
-            }
-
-            ctx.restore();
-        }
-
-        function insertLabels() {
-            placeholder.find(".tickLabels").remove();
-            
-            var html = ['<div class="tickLabels" style="font-size:smaller;color:' + options.grid.color + '">'];
-
-            function addLabels(axis, labelGenerator) {
-                for (var i = 0; i < axis.ticks.length; ++i) {
-                    var tick = axis.ticks[i];
-                    if (!tick.label || tick.v < axis.min || tick.v > axis.max)
-                        continue;
-                    html.push(labelGenerator(tick, axis));
-                }
-            }
-
-            var margin = options.grid.labelMargin + options.grid.borderWidth;
-            
-            addLabels(axes.xaxis, function (tick, axis) {
-                return '<div style="position:absolute;top:' + (plotOffset.top + plotHeight + margin) + 'px;left:' + Math.round(plotOffset.left + axis.p2c(tick.v) - axis.labelWidth/2) + 'px;width:' + axis.labelWidth + 'px;text-align:center" class="tickLabel">' + tick.label + "</div>";
-            });
-            
-            
-            addLabels(axes.yaxis, function (tick, axis) {
-                return '<div style="position:absolute;top:' + Math.round(plotOffset.top + axis.p2c(tick.v) - axis.labelHeight/2) + 'px;right:' + (plotOffset.right + plotWidth + margin) + 'px;width:' + axis.labelWidth + 'px;text-align:right" class="tickLabel">' + tick.label + "</div>";
-            });
-            
-            addLabels(axes.x2axis, function (tick, axis) {
-                return '<div style="position:absolute;bottom:' + (plotOffset.bottom + plotHeight + margin) + 'px;left:' + Math.round(plotOffset.left + axis.p2c(tick.v) - axis.labelWidth/2) + 'px;width:' + axis.labelWidth + 'px;text-align:center" class="tickLabel">' + tick.label + "</div>";
-            });
-            
-            addLabels(axes.y2axis, function (tick, axis) {
-                return '<div style="position:absolute;top:' + Math.round(plotOffset.top + axis.p2c(tick.v) - axis.labelHeight/2) + 'px;left:' + (plotOffset.left + plotWidth + margin) +'px;width:' + axis.labelWidth + 'px;text-align:left" class="tickLabel">' + tick.label + "</div>";
-            });
-
-            html.push('</div>');
-            
-            placeholder.append(html.join(""));
-        }
-
-        function drawSeries(series) {
-            if (series.lines.show)
-                drawSeriesLines(series);
-            if (series.bars.show)
-                drawSeriesBars(series);
-            if (series.points.show)
-                drawSeriesPoints(series);
-        }
-        
-        function drawSeriesLines(series) {
-            function plotLine(datapoints, xoffset, yoffset, axisx, axisy) {
-                var points = datapoints.points,
-                    ps = datapoints.pointsize,
-                    prevx = null, prevy = null;
-                
-                ctx.beginPath();
-                for (var i = ps; i < points.length; i += ps) {
-                    var x1 = points[i - ps], y1 = points[i - ps + 1],
-                        x2 = points[i], y2 = points[i + 1];
-                    
-                    if (x1 == null || x2 == null)
-                        continue;
-
-                    // clip with ymin
-                    if (y1 <= y2 && y1 < axisy.min) {
-                        if (y2 < axisy.min)
-                            continue;   // line segment is outside
-                        // compute new intersection point
-                        x1 = (axisy.min - y1) / (y2 - y1) * (x2 - x1) + x1;
-                        y1 = axisy.min;
-                    }
-                    else if (y2 <= y1 && y2 < axisy.min) {
-                        if (y1 < axisy.min)
-                            continue;
-                        x2 = (axisy.min - y1) / (y2 - y1) * (x2 - x1) + x1;
-                        y2 = axisy.min;
-                    }
-
-                    // clip with ymax
-                    if (y1 >= y2 && y1 > axisy.max) {
-                        if (y2 > axisy.max)
-                            continue;
-                        x1 = (axisy.max - y1) / (y2 - y1) * (x2 - x1) + x1;
-                        y1 = axisy.max;
-                    }
-                    else if (y2 >= y1 && y2 > axisy.max) {
-                        if (y1 > axisy.max)
-                            continue;
-                        x2 = (axisy.max - y1) / (y2 - y1) * (x2 - x1) + x1;
-                        y2 = axisy.max;
-                    }
-
-                    // clip with xmin
-                    if (x1 <= x2 && x1 < axisx.min) {
-                        if (x2 < axisx.min)
-                            continue;
-                        y1 = (axisx.min - x1) / (x2 - x1) * (y2 - y1) + y1;
-                        x1 = axisx.min;
-                    }
-                    else if (x2 <= x1 && x2 < axisx.min) {
-                        if (x1 < axisx.min)
-                            continue;
-                        y2 = (axisx.min - x1) / (x2 - x1) * (y2 - y1) + y1;
-                        x2 = axisx.min;
-                    }
-
-                    // clip with xmax
-                    if (x1 >= x2 && x1 > axisx.max) {
-                        if (x2 > axisx.max)
-                            continue;
-                        y1 = (axisx.max - x1) / (x2 - x1) * (y2 - y1) + y1;
-                        x1 = axisx.max;
-                    }
-                    else if (x2 >= x1 && x2 > axisx.max) {
-                        if (x1 > axisx.max)
-                            continue;
-                        y2 = (axisx.max - x1) / (x2 - x1) * (y2 - y1) + y1;
-                        x2 = axisx.max;
-                    }
-
-                    if (x1 != prevx || y1 != prevy)
-                        ctx.moveTo(axisx.p2c(x1) + xoffset, axisy.p2c(y1) + yoffset);
-                    
-                    prevx = x2;
-                    prevy = y2;
-                    ctx.lineTo(axisx.p2c(x2) + xoffset, axisy.p2c(y2) + yoffset);
-                }
-                ctx.stroke();
-            }
-
-            function plotLineArea(datapoints, axisx, axisy) {
-                var points = datapoints.points,
-                    ps = datapoints.pointsize,
-                    bottom = Math.min(Math.max(0, axisy.min), axisy.max),
-                    top, lastX = 0, areaOpen = false;
-                
-                for (var i = ps; i < points.length; i += ps) {
-                    var x1 = points[i - ps], y1 = points[i - ps + 1],
-                        x2 = points[i], y2 = points[i + 1];
-                    
-                    if (areaOpen && x1 != null && x2 == null) {
-                        // close area
-                        ctx.lineTo(axisx.p2c(lastX), axisy.p2c(bottom));
-                        ctx.fill();
-                        areaOpen = false;
-                        continue;
-                    }
-
-                    if (x1 == null || x2 == null)
-                        continue;
-
-                    // clip x values
-                    
-                    // clip with xmin
-                    if (x1 <= x2 && x1 < axisx.min) {
-                        if (x2 < axisx.min)
-                            continue;
-                        y1 = (axisx.min - x1) / (x2 - x1) * (y2 - y1) + y1;
-                        x1 = axisx.min;
-                    }
-                    else if (x2 <= x1 && x2 < axisx.min) {
-                        if (x1 < axisx.min)
-                            continue;
-                        y2 = (axisx.min - x1) / (x2 - x1) * (y2 - y1) + y1;
-                        x2 = axisx.min;
-                    }
-
-                    // clip with xmax
-                    if (x1 >= x2 && x1 > axisx.max) {
-                        if (x2 > axisx.max)
-                            continue;
-                        y1 = (axisx.max - x1) / (x2 - x1) * (y2 - y1) + y1;
-                        x1 = axisx.max;
-                    }
-                    else if (x2 >= x1 && x2 > axisx.max) {
-                        if (x1 > axisx.max)
-                            continue;
-                        y2 = (axisx.max - x1) / (x2 - x1) * (y2 - y1) + y1;
-                        x2 = axisx.max;
-                    }
-
-                    if (!areaOpen) {
-                        // open area
-                        ctx.beginPath();
-                        ctx.moveTo(axisx.p2c(x1), axisy.p2c(bottom));
-                        areaOpen = true;
-                    }
-                    
-                    // now first check the case where both is outside
-                    if (y1 >= axisy.max && y2 >= axisy.max) {
-                        ctx.lineTo(axisx.p2c(x1), axisy.p2c(axisy.max));
-                        ctx.lineTo(axisx.p2c(x2), axisy.p2c(axisy.max));
-                        lastX = x2;
-                        continue;
-                    }
-                    else if (y1 <= axisy.min && y2 <= axisy.min) {
-                        ctx.lineTo(axisx.p2c(x1), axisy.p2c(axisy.min));
-                        ctx.lineTo(axisx.p2c(x2), axisy.p2c(axisy.min));
-                        lastX = x2;
-                        continue;
-                    }
-                    
-                    // else it's a bit more complicated, there might
-                    // be two rectangles and two triangles we need to fill
-                    // in; to find these keep track of the current x values
-                    var x1old = x1, x2old = x2;
-
-                    // and clip the y values, without shortcutting
-                    
-                    // clip with ymin
-                    if (y1 <= y2 && y1 < axisy.min && y2 >= axisy.min) {
-                        x1 = (axisy.min - y1) / (y2 - y1) * (x2 - x1) + x1;
-                        y1 = axisy.min;
-                    }
-                    else if (y2 <= y1 && y2 < axisy.min && y1 >= axisy.min) {
-                        x2 = (axisy.min - y1) / (y2 - y1) * (x2 - x1) + x1;
-                        y2 = axisy.min;
-                    }
-
-                    // clip with ymax
-                    if (y1 >= y2 && y1 > axisy.max && y2 <= axisy.max) {
-                        x1 = (axisy.max - y1) / (y2 - y1) * (x2 - x1) + x1;
-                        y1 = axisy.max;
-                    }
-                    else if (y2 >= y1 && y2 > axisy.max && y1 <= axisy.max) {
-                        x2 = (axisy.max - y1) / (y2 - y1) * (x2 - x1) + x1;
-                        y2 = axisy.max;
-                    }
-
-
-                    // if the x value was changed we got a rectangle
-                    // to fill
-                    if (x1 != x1old) {
-                        if (y1 <= axisy.min)
-                            top = axisy.min;
-                        else
-                            top = axisy.max;
-                        
-                        ctx.lineTo(axisx.p2c(x1old), axisy.p2c(top));
-                        ctx.lineTo(axisx.p2c(x1), axisy.p2c(top));
-                    }
-                    
-                    // fill the triangles
-                    ctx.lineTo(axisx.p2c(x1), axisy.p2c(y1));
-                    ctx.lineTo(axisx.p2c(x2), axisy.p2c(y2));
-
-                    // fill the other rectangle if it's there
-                    if (x2 != x2old) {
-                        if (y2 <= axisy.min)
-                            top = axisy.min;
-                        else
-                            top = axisy.max;
-                        
-                        ctx.lineTo(axisx.p2c(x2), axisy.p2c(top));
-                        ctx.lineTo(axisx.p2c(x2old), axisy.p2c(top));
-                    }
-
-                    lastX = Math.max(x2, x2old);
-                }
-
-                if (areaOpen) {
-                    ctx.lineTo(axisx.p2c(lastX), axisy.p2c(bottom));
-                    ctx.fill();
-                }
-            }
-            
-            ctx.save();
-            ctx.translate(plotOffset.left, plotOffset.top);
-            ctx.lineJoin = "round";
-
-            var lw = series.lines.lineWidth,
-                sw = series.shadowSize;
-            // FIXME: consider another form of shadow when filling is turned on
-            if (lw > 0 && sw > 0) {
-                // draw shadow as a thick and thin line with transparency
-                ctx.lineWidth = sw;
-                ctx.strokeStyle = "rgba(0,0,0,0.1)";
-                // position shadow at angle from the mid of line
-                var angle = Math.PI/18;
-                plotLine(series.datapoints, Math.sin(angle) * (lw/2 + sw/2), Math.cos(angle) * (lw/2 + sw/2), series.xaxis, series.yaxis);
-                ctx.lineWidth = sw/2;
-                plotLine(series.datapoints, Math.sin(angle) * (lw/2 + sw/4), Math.cos(angle) * (lw/2 + sw/4), series.xaxis, series.yaxis);
-            }
-
-            ctx.lineWidth = lw;
-            ctx.strokeStyle = series.color;
-            var fillStyle = getFillStyle(series.lines, series.color, 0, plotHeight);
-            if (fillStyle) {
-                ctx.fillStyle = fillStyle;
-                plotLineArea(series.datapoints, series.xaxis, series.yaxis);
-            }
-
-            if (lw > 0)
-                plotLine(series.datapoints, 0, 0, series.xaxis, series.yaxis);
-            ctx.restore();
-        }
-
-        function drawSeriesPoints(series) {
-            function plotPoints(datapoints, radius, fillStyle, offset, circumference, axisx, axisy) {
-                var points = datapoints.points, ps = datapoints.pointsize;
-                
-                for (var i = 0; i < points.length; i += ps) {
-                    var x = points[i], y = points[i + 1];
-                    if (x == null || x < axisx.min || x > axisx.max || y < axisy.min || y > axisy.max)
-                        continue;
-                    
-                    ctx.beginPath();
-                    ctx.arc(axisx.p2c(x), axisy.p2c(y) + offset, radius, 0, circumference, false);
-                    if (fillStyle) {
-                        ctx.fillStyle = fillStyle;
-                        ctx.fill();
-                    }
-                    ctx.stroke();
-                }
-            }
-            
-            ctx.save();
-            ctx.translate(plotOffset.left, plotOffset.top);
-
-            var lw = series.lines.lineWidth,
-                sw = series.shadowSize,
-                radius = series.points.radius;
-            if (lw > 0 && sw > 0) {
-                // draw shadow in two steps
-                var w = sw / 2;
-                ctx.lineWidth = w;
-                ctx.strokeStyle = "rgba(0,0,0,0.1)";
-                plotPoints(series.datapoints, radius, null, w + w/2, Math.PI,
-                           series.xaxis, series.yaxis);
-
-                ctx.strokeStyle = "rgba(0,0,0,0.2)";
-                plotPoints(series.datapoints, radius, null, w/2, Math.PI,
-                           series.xaxis, series.yaxis);
-            }
-
-            ctx.lineWidth = lw;
-            ctx.strokeStyle = series.color;
-            plotPoints(series.datapoints, radius,
-                       getFillStyle(series.points, series.color), 0, 2 * Math.PI,
-                       series.xaxis, series.yaxis);
-            ctx.restore();
-        }
-
-        function drawBar(x, y, b, barLeft, barRight, offset, fillStyleCallback, axisx, axisy, c, horizontal) {
-            var left, right, bottom, top,
-                drawLeft, drawRight, drawTop, drawBottom,
-                tmp;
-
-            if (horizontal) {
-                drawBottom = drawRight = drawTop = true;
-                drawLeft = false;
-                left = b;
-                right = x;
-                top = y + barLeft;
-                bottom = y + barRight;
-
-                // account for negative bars
-                if (right < left) {
-                    tmp = right;
-                    right = left;
-                    left = tmp;
-                    drawLeft = true;
-                    drawRight = false;
-                }
-            }
-            else {
-                drawLeft = drawRight = drawTop = true;
-                drawBottom = false;
-                left = x + barLeft;
-                right = x + barRight;
-                bottom = b;
-                top = y;
-
-                // account for negative bars
-                if (top < bottom) {
-                    tmp = top;
-                    top = bottom;
-                    bottom = tmp;
-                    drawBottom = true;
-                    drawTop = false;
-                }
-            }
-           
-            // clip
-            if (right < axisx.min || left > axisx.max ||
-                top < axisy.min || bottom > axisy.max)
-                return;
-            
-            if (left < axisx.min) {
-                left = axisx.min;
-                drawLeft = false;
-            }
-
-            if (right > axisx.max) {
-                right = axisx.max;
-                drawRight = false;
-            }
-
-            if (bottom < axisy.min) {
-                bottom = axisy.min;
-                drawBottom = false;
-            }
-            
-            if (top > axisy.max) {
-                top = axisy.max;
-                drawTop = false;
-            }
-
-            left = axisx.p2c(left);
-            bottom = axisy.p2c(bottom);
-            right = axisx.p2c(right);
-            top = axisy.p2c(top);
-            
-            // fill the bar
-            if (fillStyleCallback) {
-                c.beginPath();
-                c.moveTo(left, bottom);
-                c.lineTo(left, top);
-                c.lineTo(right, top);
-                c.lineTo(right, bottom);
-                c.fillStyle = fillStyleCallback(bottom, top);
-                c.fill();
-            }
-
-            // draw outline
-            if (drawLeft || drawRight || drawTop || drawBottom) {
-                c.beginPath();
-
-                // FIXME: inline moveTo is buggy with excanvas
-                c.moveTo(left, bottom + offset);
-                if (drawLeft)
-                    c.lineTo(left, top + offset);
-                else
-                    c.moveTo(left, top + offset);
-                if (drawTop)
-                    c.lineTo(right, top + offset);
-                else
-                    c.moveTo(right, top + offset);
-                if (drawRight)
-                    c.lineTo(right, bottom + offset);
-                else
-                    c.moveTo(right, bottom + offset);
-                if (drawBottom)
-                    c.lineTo(left, bottom + offset);
-                else
-                    c.moveTo(left, bottom + offset);
-                c.stroke();
-            }
-        }
-        
-        function drawSeriesBars(series) {
-            function plotBars(datapoints, barLeft, barRight, offset, fillStyleCallback, axisx, axisy) {
-                var points = datapoints.points, ps = datapoints.pointsize;
-                
-                for (var i = 0; i < points.length; i += ps) {
-                    if (points[i] == null)
-                        continue;
-                    drawBar(points[i], points[i + 1], points[i + 2], barLeft, barRight, offset, fillStyleCallback, axisx, axisy, ctx, series.bars.horizontal);
-                }
-            }
-
-            ctx.save();
-            ctx.translate(plotOffset.left, plotOffset.top);
-
-            // FIXME: figure out a way to add shadows (for instance along the right edge)
-            ctx.lineWidth = series.bars.lineWidth;
-            ctx.strokeStyle = series.color;
-            var barLeft = series.bars.align == "left" ? 0 : -series.bars.barWidth/2;
-            var fillStyleCallback = series.bars.fill ? function (bottom, top) { return getFillStyle(series.bars, series.color, bottom, top); } : null;
-            plotBars(series.datapoints, barLeft, barLeft + series.bars.barWidth, 0, fillStyleCallback, series.xaxis, series.yaxis);
-            ctx.restore();
-        }
-
-        function getFillStyle(filloptions, seriesColor, bottom, top) {
-            var fill = filloptions.fill;
-            if (!fill)
-                return null;
-
-            if (filloptions.fillColor)
-                return getColorOrGradient(filloptions.fillColor, bottom, top, seriesColor);
-            
-            var c = $.color.parse(seriesColor);
-            c.a = typeof fill == "number" ? fill : 0.4;
-            c.normalize();
-            return c.toString();
-        }
-        
-        function insertLegend() {
-            placeholder.find(".legend").remove();
-
-            if (!options.legend.show)
-                return;
-            
-            var fragments = [], rowStarted = false,
-                lf = options.legend.labelFormatter, s, label;
-            for (i = 0; i < series.length; ++i) {
-                s = series[i];
-                label = s.label;
-                if (!label)
-                    continue;
-                
-                if (i % options.legend.noColumns == 0) {
-                    if (rowStarted)
-                        fragments.push('</tr>');
-                    fragments.push('<tr>');
-                    rowStarted = true;
-                }
-
-                if (lf)
-                    label = lf(label, s);
-                
-                fragments.push(
-                    '<td class="legendColorBox"><div style="border:1px solid ' + options.legend.labelBoxBorderColor + ';padding:1px"><div style="width:4px;height:0;border:5px solid ' + s.color + ';overflow:hidden"></div></div></td>' +
-                    '<td class="legendLabel">' + label + '</td>');
-            }
-            if (rowStarted)
-                fragments.push('</tr>');
-            
-            if (fragments.length == 0)
-                return;
-
-            var table = '<table style="font-size:smaller;color:' + options.grid.color + '">' + fragments.join("") + '</table>';
-            if (options.legend.container != null)
-                $(options.legend.container).html(table);
-            else {
-                var pos = "",
-                    p = options.legend.position,
-                    m = options.legend.margin;
-                if (m[0] == null)
-                    m = [m, m];
-                if (p.charAt(0) == "n")
-                    pos += 'top:' + (m[1] + plotOffset.top) + 'px;';
-                else if (p.charAt(0) == "s")
-                    pos += 'bottom:' + (m[1] + plotOffset.bottom) + 'px;';
-                if (p.charAt(1) == "e")
-                    pos += 'right:' + (m[0] + plotOffset.right) + 'px;';
-                else if (p.charAt(1) == "w")
-                    pos += 'left:' + (m[0] + plotOffset.left) + 'px;';
-                var legend = $('<div class="legend">' + table.replace('style="', 'style="position:absolute;' + pos +';') + '</div>').appendTo(placeholder);
-                if (options.legend.backgroundOpacity != 0.0) {
-                    // put in the transparent background
-                    // separately to avoid blended labels and
-                    // label boxes
-                    var c = options.legend.backgroundColor;
-                    if (c == null) {
-                        c = options.grid.backgroundColor;
-                        if (c && typeof c == "string")
-                            c = $.color.parse(c);
-                        else
-                            c = $.color.extract(legend, 'background-color');
-                        c.a = 1;
-                        c = c.toString();
-                    }
-                    var div = legend.children();
-                    $('<div style="position:absolute;width:' + div.width() + 'px;height:' + div.height() + 'px;' + pos +'background-color:' + c + ';"> </div>').prependTo(legend).css('opacity', options.legend.backgroundOpacity);
-                }
-            }
-        }
-
-
-        // interactive features
-        
-        var highlights = [],
-            redrawTimeout = null;
-        
-        // returns the data item the mouse is over, or null if none is found
-        function findNearbyItem(mouseX, mouseY, seriesFilter) {
-            var maxDistance = options.grid.mouseActiveRadius,
-                smallestDistance = maxDistance * maxDistance + 1,
-                item = null, foundPoint = false, i, j;
-
-            for (i = 0; i < series.length; ++i) {
-                if (!seriesFilter(series[i]))
-                    continue;
-                
-                var s = series[i],
-                    axisx = s.xaxis,
-                    axisy = s.yaxis,
-                    points = s.datapoints.points,
-                    ps = s.datapoints.pointsize,
-                    mx = axisx.c2p(mouseX), // precompute some stuff to make the loop faster
-                    my = axisy.c2p(mouseY),
-                    maxx = maxDistance / axisx.scale,
-                    maxy = maxDistance / axisy.scale;
-
-                if (s.lines.show || s.points.show) {
-                    for (j = 0; j < points.length; j += ps) {
-                        var x = points[j], y = points[j + 1];
-                        if (x == null)
-                            continue;
-                        
-                        // For points and lines, the cursor must be within a
-                        // certain distance to the data point
-                        if (x - mx > maxx || x - mx < -maxx ||
-                            y - my > maxy || y - my < -maxy)
-                            continue;
-
-                        // We have to calculate distances in pixels, not in
-                        // data units, because the scales of the axes may be different
-                        var dx = Math.abs(axisx.p2c(x) - mouseX),
-                            dy = Math.abs(axisy.p2c(y) - mouseY),
-                            dist = dx * dx + dy * dy; // we save the sqrt
-
-                        // use <= to ensure last point takes precedence
-                        // (last generally means on top of)
-                        if (dist <= smallestDistance) {
-                            smallestDistance = dist;
-                            item = [i, j / ps];
-                        }
-                    }
-                }
-                    
-                if (s.bars.show && !item) { // no other point can be nearby
-                    var barLeft = s.bars.align == "left" ? 0 : -s.bars.barWidth/2,
-                        barRight = barLeft + s.bars.barWidth;
-                    
-                    for (j = 0; j < points.length; j += ps) {
-                        var x = points[j], y = points[j + 1], b = points[j + 2];
-                        if (x == null)
-                            continue;
-  
-                        // for a bar graph, the cursor must be inside the bar
-                        if (series[i].bars.horizontal ? 
-                            (mx <= Math.max(b, x) && mx >= Math.min(b, x) && 
-                             my >= y + barLeft && my <= y + barRight) :
-                            (mx >= x + barLeft && mx <= x + barRight &&
-                             my >= Math.min(b, y) && my <= Math.max(b, y)))
-                                item = [i, j / ps];
-                    }
-                }
-            }
-
-            if (item) {
-                i = item[0];
-                j = item[1];
-                ps = series[i].datapoints.pointsize;
-                
-                return { datapoint: series[i].datapoints.points.slice(j * ps, (j + 1) * ps),
-                         dataIndex: j,
-                         series: series[i],
-                         seriesIndex: i };
-            }
-            
-            return null;
-        }
-
-        function onMouseMove(e) {
-            if (options.grid.hoverable)
-                triggerClickHoverEvent("plothover", e,
-                                       function (s) { return s["hoverable"] != false; });
-        }
-        
-        function onClick(e) {
-            triggerClickHoverEvent("plotclick", e,
-                                   function (s) { return s["clickable"] != false; });
-        }
-
-        // trigger click or hover event (they send the same parameters
-        // so we share their code)
-        function triggerClickHoverEvent(eventname, event, seriesFilter) {
-            var offset = eventHolder.offset(),
-                pos = { pageX: event.pageX, pageY: event.pageY },
-                canvasX = event.pageX - offset.left - plotOffset.left,
-                canvasY = event.pageY - offset.top - plotOffset.top;
-
-            if (axes.xaxis.used)
-                pos.x = axes.xaxis.c2p(canvasX);
-            if (axes.yaxis.used)
-                pos.y = axes.yaxis.c2p(canvasY);
-            if (axes.x2axis.used)
-                pos.x2 = axes.x2axis.c2p(canvasX);
-            if (axes.y2axis.used)
-                pos.y2 = axes.y2axis.c2p(canvasY);
-
-            var item = findNearbyItem(canvasX, canvasY, seriesFilter);
-
-            if (item) {
-                // fill in mouse pos for any listeners out there
-                item.pageX = parseInt(item.series.xaxis.p2c(item.datapoint[0]) + offset.left + plotOffset.left);
-                item.pageY = parseInt(item.series.yaxis.p2c(item.datapoint[1]) + offset.top + plotOffset.top);
-            }
-
-            if (options.grid.autoHighlight) {
-                // clear auto-highlights
-                for (var i = 0; i < highlights.length; ++i) {
-                    var h = highlights[i];
-                    if (h.auto == eventname &&
-                        !(item && h.series == item.series && h.point == item.datapoint))
-                        unhighlight(h.series, h.point);
-                }
-                
-                if (item)
-                    highlight(item.series, item.datapoint, eventname);
-            }
-            
-            placeholder.trigger(eventname, [ pos, item ]);
-        }
-
-        function triggerRedrawOverlay() {
-            if (!redrawTimeout)
-                redrawTimeout = setTimeout(drawOverlay, 30);
-        }
-
-        function drawOverlay() {
-            redrawTimeout = null;
-
-            // draw highlights
-            octx.save();
-            octx.clearRect(0, 0, canvasWidth, canvasHeight);
-            octx.translate(plotOffset.left, plotOffset.top);
-            
-            var i, hi;
-            for (i = 0; i < highlights.length; ++i) {
-                hi = highlights[i];
-
-                if (hi.series.bars.show)
-                    drawBarHighlight(hi.series, hi.point);
-                else
-                    drawPointHighlight(hi.series, hi.point);
-            }
-            octx.restore();
-            
-            executeHooks(hooks.drawOverlay, [octx]);
-        }
-        
-        function highlight(s, point, auto) {
-            if (typeof s == "number")
-                s = series[s];
-
-            if (typeof point == "number")
-                point = s.data[point];
-
-            var i = indexOfHighlight(s, point);
-            if (i == -1) {
-                highlights.push({ series: s, point: point, auto: auto });
-
-                triggerRedrawOverlay();
-            }
-            else if (!auto)
-                highlights[i].auto = false;
-        }
-            
-        function unhighlight(s, point) {
-            if (s == null && point == null) {
-                highlights = [];
-                triggerRedrawOverlay();
-            }
-            
-            if (typeof s == "number")
-                s = series[s];
-
-            if (typeof point == "number")
-                point = s.data[point];
-
-            var i = indexOfHighlight(s, point);
-            if (i != -1) {
-                highlights.splice(i, 1);
-
-                triggerRedrawOverlay();
-            }
-        }
-        
-        function indexOfHighlight(s, p) {
-            for (var i = 0; i < highlights.length; ++i) {
-                var h = highlights[i];
-                if (h.series == s && h.point[0] == p[0]
-                    && h.point[1] == p[1])
-                    return i;
-            }
-            return -1;
-        }
-        
-        function drawPointHighlight(series, point) {
-            var x = point[0], y = point[1],
-                axisx = series.xaxis, axisy = series.yaxis;
-            
-            if (x < axisx.min || x > axisx.max || y < axisy.min || y > axisy.max)
-                return;
-            
-            var pointRadius = series.points.radius + series.points.lineWidth / 2;
-            octx.lineWidth = pointRadius;
-            octx.strokeStyle = $.color.parse(series.color).scale('a', 0.5).toString();
-            var radius = 1.5 * pointRadius;
-            octx.beginPath();
-            octx.arc(axisx.p2c(x), axisy.p2c(y), radius, 0, 2 * Math.PI, false);
-            octx.stroke();
-        }
-
-        function drawBarHighlight(series, point) {
-            octx.lineWidth = series.bars.lineWidth;
-            octx.strokeStyle = $.color.parse(series.color).scale('a', 0.5).toString();
-            var fillStyle = $.color.parse(series.color).scale('a', 0.5).toString();
-            var barLeft = series.bars.align == "left" ? 0 : -series.bars.barWidth/2;
-            drawBar(point[0], point[1], point[2] || 0, barLeft, barLeft + series.bars.barWidth,
-                    0, function () { return fillStyle; }, series.xaxis, series.yaxis, octx, series.bars.horizontal);
-        }
-
-        function getColorOrGradient(spec, bottom, top, defaultColor) {
-            if (typeof spec == "string")
-                return spec;
-            else {
-                // assume this is a gradient spec; IE currently only
-                // supports a simple vertical gradient properly, so that's
-                // what we support too
-                var gradient = ctx.createLinearGradient(0, top, 0, bottom);
-                
-                for (var i = 0, l = spec.colors.length; i < l; ++i) {
-                    var c = spec.colors[i];
-                    if (typeof c != "string") {
-                        c = $.color.parse(defaultColor).scale('rgb', c.brightness);
-                        c.a *= c.opacity;
-                        c = c.toString();
-                    }
-                    gradient.addColorStop(i / (l - 1), c);
-                }
-                
-                return gradient;
-            }
-        }
-    }
-
-    $.plot = function(placeholder, data, options) {
-        var plot = new Plot($(placeholder), data, options, $.plot.plugins);
-        /*var t0 = new Date();
-        var t1 = new Date();
-        var tstr = "time used (msecs): " + (t1.getTime() - t0.getTime())
-        if (window.console)
-            console.log(tstr);
-        else
-            alert(tstr);*/
-        return plot;
-    };
-
-    $.plot.plugins = [];
-
-    // returns a string with the date d formatted according to fmt
-    $.plot.formatDate = function(d, fmt, monthNames) {
-        var leftPad = function(n) {
-            n = "" + n;
-            return n.length == 1 ? "0" + n : n;
-        };
-        
-        var r = [];
-        var escape = false;
-        var hours = d.getUTCHours();
-        var isAM = hours < 12;
-        if (monthNames == null)
-            monthNames = ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"];
-
-        if (fmt.search(/%p|%P/) != -1) {
-            if (hours > 12) {
-                hours = hours - 12;
-            } else if (hours == 0) {
-                hours = 12;
-            }
-        }
-        for (var i = 0; i < fmt.length; ++i) {
-            var c = fmt.charAt(i);
-            
-            if (escape) {
-                switch (c) {
-                case 'h': c = "" + hours; break;
-                case 'H': c = leftPad(hours); break;
-                case 'M': c = leftPad(d.getUTCMinutes()); break;
-                case 'S': c = leftPad(d.getUTCSeconds()); break;
-                case 'd': c = "" + d.getUTCDate(); break;
-                case 'm': c = "" + (d.getUTCMonth() + 1); break;
-                case 'y': c = "" + d.getUTCFullYear(); break;
-                case 'b': c = "" + monthNames[d.getUTCMonth()]; break;
-                case 'p': c = (isAM) ? ("" + "am") : ("" + "pm"); break;
-                case 'P': c = (isAM) ? ("" + "AM") : ("" + "PM"); break;
-                }
-                r.push(c);
-                escape = false;
-            }
-            else {
-                if (c == "%")
-                    escape = true;
-                else
-                    r.push(c);
-            }
-        }
-        return r.join("");
-    };
-    
-    // round to nearby lower multiple of base
-    function floorInBase(n, base) {
-        return base * Math.floor(n / base);
-    }
-    
-})(jQuery);
diff --git a/lib/logstash/web/public/js/flot/jquery.flot.min.js b/lib/logstash/web/public/js/flot/jquery.flot.min.js
deleted file mode 100644
index 31f465b8309..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.flot.min.js
+++ /dev/null
@@ -1 +0,0 @@
-(function(){jQuery.color={};jQuery.color.make=function(G,H,J,I){var A={};A.r=G||0;A.g=H||0;A.b=J||0;A.a=I!=null?I:1;A.add=function(C,D){for(var E=0;E<C.length;++E){A[C.charAt(E)]+=D}return A.normalize()};A.scale=function(C,D){for(var E=0;E<C.length;++E){A[C.charAt(E)]*=D}return A.normalize()};A.toString=function(){if(A.a>=1){return"rgb("+[A.r,A.g,A.b].join(",")+")"}else{return"rgba("+[A.r,A.g,A.b,A.a].join(",")+")"}};A.normalize=function(){function C(E,D,F){return D<E?E:(D>F?F:D)}A.r=C(0,parseInt(A.r),255);A.g=C(0,parseInt(A.g),255);A.b=C(0,parseInt(A.b),255);A.a=C(0,A.a,1);return A};A.clone=function(){return jQuery.color.make(A.r,A.b,A.g,A.a)};return A.normalize()};jQuery.color.extract=function(E,F){var A;do{A=E.css(F).toLowerCase();if(A!=""&&A!="transparent"){break}E=E.parent()}while(!jQuery.nodeName(E.get(0),"body"));if(A=="rgba(0, 0, 0, 0)"){A="transparent"}return jQuery.color.parse(A)};jQuery.color.parse=function(A){var F,H=jQuery.color.make;if(F=/rgb\(\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*\)/.exec(A)){return H(parseInt(F[1],10),parseInt(F[2],10),parseInt(F[3],10))}if(F=/rgba\(\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*,\s*([0-9]{1,3})\s*,\s*([0-9]+(?:\.[0-9]+)?)\s*\)/.exec(A)){return H(parseInt(F[1],10),parseInt(F[2],10),parseInt(F[3],10),parseFloat(F[4]))}if(F=/rgb\(\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*\)/.exec(A)){return H(parseFloat(F[1])*2.55,parseFloat(F[2])*2.55,parseFloat(F[3])*2.55)}if(F=/rgba\(\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\%\s*,\s*([0-9]+(?:\.[0-9]+)?)\s*\)/.exec(A)){return H(parseFloat(F[1])*2.55,parseFloat(F[2])*2.55,parseFloat(F[3])*2.55,parseFloat(F[4]))}if(F=/#([a-fA-F0-9]{2})([a-fA-F0-9]{2})([a-fA-F0-9]{2})/.exec(A)){return H(parseInt(F[1],16),parseInt(F[2],16),parseInt(F[3],16))}if(F=/#([a-fA-F0-9])([a-fA-F0-9])([a-fA-F0-9])/.exec(A)){return H(parseInt(F[1]+F[1],16),parseInt(F[2]+F[2],16),parseInt(F[3]+F[3],16))}var G=jQuery.trim(A).toLowerCase();if(G=="transparent"){return H(255,255,255,0)}else{F=B[G];return H(F[0],F[1],F[2])}};var B={aqua:[0,255,255],azure:[240,255,255],beige:[245,245,220],black:[0,0,0],blue:[0,0,255],brown:[165,42,42],cyan:[0,255,255],darkblue:[0,0,139],darkcyan:[0,139,139],darkgrey:[169,169,169],darkgreen:[0,100,0],darkkhaki:[189,183,107],darkmagenta:[139,0,139],darkolivegreen:[85,107,47],darkorange:[255,140,0],darkorchid:[153,50,204],darkred:[139,0,0],darksalmon:[233,150,122],darkviolet:[148,0,211],fuchsia:[255,0,255],gold:[255,215,0],green:[0,128,0],indigo:[75,0,130],khaki:[240,230,140],lightblue:[173,216,230],lightcyan:[224,255,255],lightgreen:[144,238,144],lightgrey:[211,211,211],lightpink:[255,182,193],lightyellow:[255,255,224],lime:[0,255,0],magenta:[255,0,255],maroon:[128,0,0],navy:[0,0,128],olive:[128,128,0],orange:[255,165,0],pink:[255,192,203],purple:[128,0,128],violet:[128,0,128],red:[255,0,0],silver:[192,192,192],white:[255,255,255],yellow:[255,255,0]}})();(function(C){function B(l,W,X,E){var O=[],g={colors:["#edc240","#afd8f8","#cb4b4b","#4da74d","#9440ed"],legend:{show:true,noColumns:1,labelFormatter:null,labelBoxBorderColor:"#ccc",container:null,position:"ne",margin:5,backgroundColor:null,backgroundOpacity:0.85},xaxis:{mode:null,transform:null,inverseTransform:null,min:null,max:null,autoscaleMargin:null,ticks:null,tickFormatter:null,labelWidth:null,labelHeight:null,tickDecimals:null,tickSize:null,minTickSize:null,monthNames:null,timeformat:null,twelveHourClock:false},yaxis:{autoscaleMargin:0.02},x2axis:{autoscaleMargin:null},y2axis:{autoscaleMargin:0.02},series:{points:{show:false,radius:3,lineWidth:2,fill:true,fillColor:"#ffffff"},lines:{lineWidth:2,fill:false,fillColor:null,steps:false},bars:{show:false,lineWidth:2,barWidth:1,fill:true,fillColor:null,align:"left",horizontal:false},shadowSize:3},grid:{show:true,aboveData:false,color:"#545454",backgroundColor:null,tickColor:"rgba(0,0,0,0.15)",labelMargin:5,borderWidth:2,borderColor:null,markings:null,markingsColor:"#f4f4f4",markingsLineWidth:2,clickable:false,hoverable:false,autoHighlight:true,mouseActiveRadius:10},hooks:{}},P=null,AC=null,AD=null,Y=null,AJ=null,s={xaxis:{},yaxis:{},x2axis:{},y2axis:{}},e={left:0,right:0,top:0,bottom:0},y=0,Q=0,I=0,t=0,L={processOptions:[],processRawData:[],processDatapoints:[],draw:[],bindEvents:[],drawOverlay:[]},G=this;G.setData=f;G.setupGrid=k;G.draw=AH;G.getPlaceholder=function(){return l};G.getCanvas=function(){return P};G.getPlotOffset=function(){return e};G.width=function(){return I};G.height=function(){return t};G.offset=function(){var AK=AD.offset();AK.left+=e.left;AK.top+=e.top;return AK};G.getData=function(){return O};G.getAxes=function(){return s};G.getOptions=function(){return g};G.highlight=AE;G.unhighlight=x;G.triggerRedrawOverlay=q;G.pointOffset=function(AK){return{left:parseInt(T(AK,"xaxis").p2c(+AK.x)+e.left),top:parseInt(T(AK,"yaxis").p2c(+AK.y)+e.top)}};G.hooks=L;b(G);r(X);c();f(W);k();AH();AG();function Z(AM,AK){AK=[G].concat(AK);for(var AL=0;AL<AM.length;++AL){AM[AL].apply(this,AK)}}function b(){for(var AK=0;AK<E.length;++AK){var AL=E[AK];AL.init(G);if(AL.options){C.extend(true,g,AL.options)}}}function r(AK){C.extend(true,g,AK);if(g.grid.borderColor==null){g.grid.borderColor=g.grid.color}if(g.xaxis.noTicks&&g.xaxis.ticks==null){g.xaxis.ticks=g.xaxis.noTicks}if(g.yaxis.noTicks&&g.yaxis.ticks==null){g.yaxis.ticks=g.yaxis.noTicks}if(g.grid.coloredAreas){g.grid.markings=g.grid.coloredAreas}if(g.grid.coloredAreasColor){g.grid.markingsColor=g.grid.coloredAreasColor}if(g.lines){C.extend(true,g.series.lines,g.lines)}if(g.points){C.extend(true,g.series.points,g.points)}if(g.bars){C.extend(true,g.series.bars,g.bars)}if(g.shadowSize){g.series.shadowSize=g.shadowSize}for(var AL in L){if(g.hooks[AL]&&g.hooks[AL].length){L[AL]=L[AL].concat(g.hooks[AL])}}Z(L.processOptions,[g])}function f(AK){O=M(AK);U();m()}function M(AN){var AL=[];for(var AK=0;AK<AN.length;++AK){var AM=C.extend(true,{},g.series);if(AN[AK].data){AM.data=AN[AK].data;delete AN[AK].data;C.extend(true,AM,AN[AK]);AN[AK].data=AM.data}else{AM.data=AN[AK]}AL.push(AM)}return AL}function T(AM,AK){var AL=AM[AK];if(!AL||AL==1){return s[AK]}if(typeof AL=="number"){return s[AK.charAt(0)+AL+AK.slice(1)]}return AL}function U(){var AP;var AV=O.length,AK=[],AN=[];for(AP=0;AP<O.length;++AP){var AS=O[AP].color;if(AS!=null){--AV;if(typeof AS=="number"){AN.push(AS)}else{AK.push(C.color.parse(O[AP].color))}}}for(AP=0;AP<AN.length;++AP){AV=Math.max(AV,AN[AP]+1)}var AL=[],AO=0;AP=0;while(AL.length<AV){var AR;if(g.colors.length==AP){AR=C.color.make(100,100,100)}else{AR=C.color.parse(g.colors[AP])}var AM=AO%2==1?-1:1;AR.scale("rgb",1+AM*Math.ceil(AO/2)*0.2);AL.push(AR);++AP;if(AP>=g.colors.length){AP=0;++AO}}var AQ=0,AW;for(AP=0;AP<O.length;++AP){AW=O[AP];if(AW.color==null){AW.color=AL[AQ].toString();++AQ}else{if(typeof AW.color=="number"){AW.color=AL[AW.color].toString()}}if(AW.lines.show==null){var AU,AT=true;for(AU in AW){if(AW[AU].show){AT=false;break}}if(AT){AW.lines.show=true}}AW.xaxis=T(AW,"xaxis");AW.yaxis=T(AW,"yaxis")}}function m(){var AW=Number.POSITIVE_INFINITY,AQ=Number.NEGATIVE_INFINITY,Ac,Aa,AZ,AV,AL,AR,Ab,AX,AP,AO,AK,Ai,Af,AT;for(AK in s){s[AK].datamin=AW;s[AK].datamax=AQ;s[AK].used=false}function AN(Al,Ak,Aj){if(Ak<Al.datamin){Al.datamin=Ak}if(Aj>Al.datamax){Al.datamax=Aj}}for(Ac=0;Ac<O.length;++Ac){AR=O[Ac];AR.datapoints={points:[]};Z(L.processRawData,[AR,AR.data,AR.datapoints])}for(Ac=0;Ac<O.length;++Ac){AR=O[Ac];var Ah=AR.data,Ae=AR.datapoints.format;if(!Ae){Ae=[];Ae.push({x:true,number:true,required:true});Ae.push({y:true,number:true,required:true});if(AR.bars.show){Ae.push({y:true,number:true,required:false,defaultValue:0})}AR.datapoints.format=Ae}if(AR.datapoints.pointsize!=null){continue}if(AR.datapoints.pointsize==null){AR.datapoints.pointsize=Ae.length}AX=AR.datapoints.pointsize;Ab=AR.datapoints.points;insertSteps=AR.lines.show&&AR.lines.steps;AR.xaxis.used=AR.yaxis.used=true;for(Aa=AZ=0;Aa<Ah.length;++Aa,AZ+=AX){AT=Ah[Aa];var AM=AT==null;if(!AM){for(AV=0;AV<AX;++AV){Ai=AT[AV];Af=Ae[AV];if(Af){if(Af.number&&Ai!=null){Ai=+Ai;if(isNaN(Ai)){Ai=null}}if(Ai==null){if(Af.required){AM=true}if(Af.defaultValue!=null){Ai=Af.defaultValue}}}Ab[AZ+AV]=Ai}}if(AM){for(AV=0;AV<AX;++AV){Ai=Ab[AZ+AV];if(Ai!=null){Af=Ae[AV];if(Af.x){AN(AR.xaxis,Ai,Ai)}if(Af.y){AN(AR.yaxis,Ai,Ai)}}Ab[AZ+AV]=null}}else{if(insertSteps&&AZ>0&&Ab[AZ-AX]!=null&&Ab[AZ-AX]!=Ab[AZ]&&Ab[AZ-AX+1]!=Ab[AZ+1]){for(AV=0;AV<AX;++AV){Ab[AZ+AX+AV]=Ab[AZ+AV]}Ab[AZ+1]=Ab[AZ-AX+1];AZ+=AX}}}}for(Ac=0;Ac<O.length;++Ac){AR=O[Ac];Z(L.processDatapoints,[AR,AR.datapoints])}for(Ac=0;Ac<O.length;++Ac){AR=O[Ac];Ab=AR.datapoints.points,AX=AR.datapoints.pointsize;var AS=AW,AY=AW,AU=AQ,Ad=AQ;for(Aa=0;Aa<Ab.length;Aa+=AX){if(Ab[Aa]==null){continue}for(AV=0;AV<AX;++AV){Ai=Ab[Aa+AV];Af=Ae[AV];if(!Af){continue}if(Af.x){if(Ai<AS){AS=Ai}if(Ai>AU){AU=Ai}}if(Af.y){if(Ai<AY){AY=Ai}if(Ai>Ad){Ad=Ai}}}}if(AR.bars.show){var Ag=AR.bars.align=="left"?0:-AR.bars.barWidth/2;if(AR.bars.horizontal){AY+=Ag;Ad+=Ag+AR.bars.barWidth}else{AS+=Ag;AU+=Ag+AR.bars.barWidth}}AN(AR.xaxis,AS,AU);AN(AR.yaxis,AY,Ad)}for(AK in s){if(s[AK].datamin==AW){s[AK].datamin=null}if(s[AK].datamax==AQ){s[AK].datamax=null}}}function c(){function AK(AM,AL){var AN=document.createElement("canvas");AN.width=AM;AN.height=AL;if(C.browser.msie){AN=window.G_vmlCanvasManager.initElement(AN)}return AN}y=l.width();Q=l.height();l.html("");if(l.css("position")=="static"){l.css("position","relative")}if(y<=0||Q<=0){throw"Invalid dimensions for plot, width = "+y+", height = "+Q}if(C.browser.msie){window.G_vmlCanvasManager.init_(document)}P=C(AK(y,Q)).appendTo(l).get(0);Y=P.getContext("2d");AC=C(AK(y,Q)).css({position:"absolute",left:0,top:0}).appendTo(l).get(0);AJ=AC.getContext("2d");AJ.stroke()}function AG(){AD=C([AC,P]);if(g.grid.hoverable){AD.mousemove(D)}if(g.grid.clickable){AD.click(d)}Z(L.bindEvents,[AD])}function k(){function AL(AT,AU){function AP(AV){return AV}var AS,AO,AQ=AU.transform||AP,AR=AU.inverseTransform;if(AT==s.xaxis||AT==s.x2axis){AS=AT.scale=I/(AQ(AT.max)-AQ(AT.min));AO=AQ(AT.min);if(AQ==AP){AT.p2c=function(AV){return(AV-AO)*AS}}else{AT.p2c=function(AV){return(AQ(AV)-AO)*AS}}if(!AR){AT.c2p=function(AV){return AO+AV/AS}}else{AT.c2p=function(AV){return AR(AO+AV/AS)}}}else{AS=AT.scale=t/(AQ(AT.max)-AQ(AT.min));AO=AQ(AT.max);if(AQ==AP){AT.p2c=function(AV){return(AO-AV)*AS}}else{AT.p2c=function(AV){return(AO-AQ(AV))*AS}}if(!AR){AT.c2p=function(AV){return AO-AV/AS}}else{AT.c2p=function(AV){return AR(AO-AV/AS)}}}}function AN(AR,AT){var AQ,AS=[],AP;AR.labelWidth=AT.labelWidth;AR.labelHeight=AT.labelHeight;if(AR==s.xaxis||AR==s.x2axis){if(AR.labelWidth==null){AR.labelWidth=y/(AR.ticks.length>0?AR.ticks.length:1)}if(AR.labelHeight==null){AS=[];for(AQ=0;AQ<AR.ticks.length;++AQ){AP=AR.ticks[AQ].label;if(AP){AS.push('<div class="tickLabel" style="float:left;width:'+AR.labelWidth+'px">'+AP+"</div>")}}if(AS.length>0){var AO=C('<div style="position:absolute;top:-10000px;width:10000px;font-size:smaller">'+AS.join("")+'<div style="clear:left"></div></div>').appendTo(l);AR.labelHeight=AO.height();AO.remove()}}}else{if(AR.labelWidth==null||AR.labelHeight==null){for(AQ=0;AQ<AR.ticks.length;++AQ){AP=AR.ticks[AQ].label;if(AP){AS.push('<div class="tickLabel">'+AP+"</div>")}}if(AS.length>0){var AO=C('<div style="position:absolute;top:-10000px;font-size:smaller">'+AS.join("")+"</div>").appendTo(l);if(AR.labelWidth==null){AR.labelWidth=AO.width()}if(AR.labelHeight==null){AR.labelHeight=AO.find("div").height()}AO.remove()}}}if(AR.labelWidth==null){AR.labelWidth=0}if(AR.labelHeight==null){AR.labelHeight=0}}function AM(){var AP=g.grid.borderWidth;for(i=0;i<O.length;++i){AP=Math.max(AP,2*(O[i].points.radius+O[i].points.lineWidth/2))}e.left=e.right=e.top=e.bottom=AP;var AO=g.grid.labelMargin+g.grid.borderWidth;if(s.xaxis.labelHeight>0){e.bottom=Math.max(AP,s.xaxis.labelHeight+AO)}if(s.yaxis.labelWidth>0){e.left=Math.max(AP,s.yaxis.labelWidth+AO)}if(s.x2axis.labelHeight>0){e.top=Math.max(AP,s.x2axis.labelHeight+AO)}if(s.y2axis.labelWidth>0){e.right=Math.max(AP,s.y2axis.labelWidth+AO)}I=y-e.left-e.right;t=Q-e.bottom-e.top}var AK;for(AK in s){K(s[AK],g[AK])}if(g.grid.show){for(AK in s){F(s[AK],g[AK]);p(s[AK],g[AK]);AN(s[AK],g[AK])}AM()}else{e.left=e.right=e.top=e.bottom=0;I=y;t=Q}for(AK in s){AL(s[AK],g[AK])}if(g.grid.show){h()}AI()}function K(AN,AQ){var AM=+(AQ.min!=null?AQ.min:AN.datamin),AK=+(AQ.max!=null?AQ.max:AN.datamax),AP=AK-AM;if(AP==0){var AL=AK==0?1:0.01;if(AQ.min==null){AM-=AL}if(AQ.max==null||AQ.min!=null){AK+=AL}}else{var AO=AQ.autoscaleMargin;if(AO!=null){if(AQ.min==null){AM-=AP*AO;if(AM<0&&AN.datamin!=null&&AN.datamin>=0){AM=0}}if(AQ.max==null){AK+=AP*AO;if(AK>0&&AN.datamax!=null&&AN.datamax<=0){AK=0}}}}AN.min=AM;AN.max=AK}function F(AP,AS){var AO;if(typeof AS.ticks=="number"&&AS.ticks>0){AO=AS.ticks}else{if(AP==s.xaxis||AP==s.x2axis){AO=0.3*Math.sqrt(y)}else{AO=0.3*Math.sqrt(Q)}}var AX=(AP.max-AP.min)/AO,AZ,AT,AV,AW,AR,AM,AL;if(AS.mode=="time"){var AU={second:1000,minute:60*1000,hour:60*60*1000,day:24*60*60*1000,month:30*24*60*60*1000,year:365.2425*24*60*60*1000};var AY=[[1,"second"],[2,"second"],[5,"second"],[10,"second"],[30,"second"],[1,"minute"],[2,"minute"],[5,"minute"],[10,"minute"],[30,"minute"],[1,"hour"],[2,"hour"],[4,"hour"],[8,"hour"],[12,"hour"],[1,"day"],[2,"day"],[3,"day"],[0.25,"month"],[0.5,"month"],[1,"month"],[2,"month"],[3,"month"],[6,"month"],[1,"year"]];var AN=0;if(AS.minTickSize!=null){if(typeof AS.tickSize=="number"){AN=AS.tickSize}else{AN=AS.minTickSize[0]*AU[AS.minTickSize[1]]}}for(AR=0;AR<AY.length-1;++AR){if(AX<(AY[AR][0]*AU[AY[AR][1]]+AY[AR+1][0]*AU[AY[AR+1][1]])/2&&AY[AR][0]*AU[AY[AR][1]]>=AN){break}}AZ=AY[AR][0];AV=AY[AR][1];if(AV=="year"){AM=Math.pow(10,Math.floor(Math.log(AX/AU.year)/Math.LN10));AL=(AX/AU.year)/AM;if(AL<1.5){AZ=1}else{if(AL<3){AZ=2}else{if(AL<7.5){AZ=5}else{AZ=10}}}AZ*=AM}if(AS.tickSize){AZ=AS.tickSize[0];AV=AS.tickSize[1]}AT=function(Ac){var Ah=[],Af=Ac.tickSize[0],Ai=Ac.tickSize[1],Ag=new Date(Ac.min);var Ab=Af*AU[Ai];if(Ai=="second"){Ag.setUTCSeconds(A(Ag.getUTCSeconds(),Af))}if(Ai=="minute"){Ag.setUTCMinutes(A(Ag.getUTCMinutes(),Af))}if(Ai=="hour"){Ag.setUTCHours(A(Ag.getUTCHours(),Af))}if(Ai=="month"){Ag.setUTCMonth(A(Ag.getUTCMonth(),Af))}if(Ai=="year"){Ag.setUTCFullYear(A(Ag.getUTCFullYear(),Af))}Ag.setUTCMilliseconds(0);if(Ab>=AU.minute){Ag.setUTCSeconds(0)}if(Ab>=AU.hour){Ag.setUTCMinutes(0)}if(Ab>=AU.day){Ag.setUTCHours(0)}if(Ab>=AU.day*4){Ag.setUTCDate(1)}if(Ab>=AU.year){Ag.setUTCMonth(0)}var Ak=0,Aj=Number.NaN,Ad;do{Ad=Aj;Aj=Ag.getTime();Ah.push({v:Aj,label:Ac.tickFormatter(Aj,Ac)});if(Ai=="month"){if(Af<1){Ag.setUTCDate(1);var Aa=Ag.getTime();Ag.setUTCMonth(Ag.getUTCMonth()+1);var Ae=Ag.getTime();Ag.setTime(Aj+Ak*AU.hour+(Ae-Aa)*Af);Ak=Ag.getUTCHours();Ag.setUTCHours(0)}else{Ag.setUTCMonth(Ag.getUTCMonth()+Af)}}else{if(Ai=="year"){Ag.setUTCFullYear(Ag.getUTCFullYear()+Af)}else{Ag.setTime(Aj+Ab)}}}while(Aj<Ac.max&&Aj!=Ad);return Ah};AW=function(Aa,Ad){var Af=new Date(Aa);if(AS.timeformat!=null){return C.plot.formatDate(Af,AS.timeformat,AS.monthNames)}var Ab=Ad.tickSize[0]*AU[Ad.tickSize[1]];var Ac=Ad.max-Ad.min;var Ae=(AS.twelveHourClock)?" %p":"";if(Ab<AU.minute){fmt="%h:%M:%S"+Ae}else{if(Ab<AU.day){if(Ac<2*AU.day){fmt="%h:%M"+Ae}else{fmt="%b %d %h:%M"+Ae}}else{if(Ab<AU.month){fmt="%b %d"}else{if(Ab<AU.year){if(Ac<AU.year){fmt="%b"}else{fmt="%b %y"}}else{fmt="%y"}}}}return C.plot.formatDate(Af,fmt,AS.monthNames)}}else{var AK=AS.tickDecimals;var AQ=-Math.floor(Math.log(AX)/Math.LN10);if(AK!=null&&AQ>AK){AQ=AK}AM=Math.pow(10,-AQ);AL=AX/AM;if(AL<1.5){AZ=1}else{if(AL<3){AZ=2;if(AL>2.25&&(AK==null||AQ+1<=AK)){AZ=2.5;++AQ}}else{if(AL<7.5){AZ=5}else{AZ=10}}}AZ*=AM;if(AS.minTickSize!=null&&AZ<AS.minTickSize){AZ=AS.minTickSize}if(AS.tickSize!=null){AZ=AS.tickSize}AP.tickDecimals=Math.max(0,(AK!=null)?AK:AQ);AT=function(Ac){var Ae=[];var Af=A(Ac.min,Ac.tickSize),Ab=0,Aa=Number.NaN,Ad;do{Ad=Aa;Aa=Af+Ab*Ac.tickSize;Ae.push({v:Aa,label:Ac.tickFormatter(Aa,Ac)});++Ab}while(Aa<Ac.max&&Aa!=Ad);return Ae};AW=function(Aa,Ab){return Aa.toFixed(Ab.tickDecimals)}}AP.tickSize=AV?[AZ,AV]:AZ;AP.tickGenerator=AT;if(C.isFunction(AS.tickFormatter)){AP.tickFormatter=function(Aa,Ab){return""+AS.tickFormatter(Aa,Ab)}}else{AP.tickFormatter=AW}}function p(AO,AQ){AO.ticks=[];if(!AO.used){return }if(AQ.ticks==null){AO.ticks=AO.tickGenerator(AO)}else{if(typeof AQ.ticks=="number"){if(AQ.ticks>0){AO.ticks=AO.tickGenerator(AO)}}else{if(AQ.ticks){var AP=AQ.ticks;if(C.isFunction(AP)){AP=AP({min:AO.min,max:AO.max})}var AN,AK;for(AN=0;AN<AP.length;++AN){var AL=null;var AM=AP[AN];if(typeof AM=="object"){AK=AM[0];if(AM.length>1){AL=AM[1]}}else{AK=AM}if(AL==null){AL=AO.tickFormatter(AK,AO)}AO.ticks[AN]={v:AK,label:AL}}}}}if(AQ.autoscaleMargin!=null&&AO.ticks.length>0){if(AQ.min==null){AO.min=Math.min(AO.min,AO.ticks[0].v)}if(AQ.max==null&&AO.ticks.length>1){AO.max=Math.max(AO.max,AO.ticks[AO.ticks.length-1].v)}}}function AH(){Y.clearRect(0,0,y,Q);var AL=g.grid;if(AL.show&&!AL.aboveData){S()}for(var AK=0;AK<O.length;++AK){AA(O[AK])}Z(L.draw,[Y]);if(AL.show&&AL.aboveData){S()}}function N(AL,AR){var AO=AR+"axis",AK=AR+"2axis",AN,AQ,AP,AM;if(AL[AO]){AN=s[AO];AQ=AL[AO].from;AP=AL[AO].to}else{if(AL[AK]){AN=s[AK];AQ=AL[AK].from;AP=AL[AK].to}else{AN=s[AO];AQ=AL[AR+"1"];AP=AL[AR+"2"]}}if(AQ!=null&&AP!=null&&AQ>AP){return{from:AP,to:AQ,axis:AN}}return{from:AQ,to:AP,axis:AN}}function S(){var AO;Y.save();Y.translate(e.left,e.top);if(g.grid.backgroundColor){Y.fillStyle=R(g.grid.backgroundColor,t,0,"rgba(255, 255, 255, 0)");Y.fillRect(0,0,I,t)}var AL=g.grid.markings;if(AL){if(C.isFunction(AL)){AL=AL({xmin:s.xaxis.min,xmax:s.xaxis.max,ymin:s.yaxis.min,ymax:s.yaxis.max,xaxis:s.xaxis,yaxis:s.yaxis,x2axis:s.x2axis,y2axis:s.y2axis})}for(AO=0;AO<AL.length;++AO){var AK=AL[AO],AQ=N(AK,"x"),AN=N(AK,"y");if(AQ.from==null){AQ.from=AQ.axis.min}if(AQ.to==null){AQ.to=AQ.axis.max}if(AN.from==null){AN.from=AN.axis.min}if(AN.to==null){AN.to=AN.axis.max}if(AQ.to<AQ.axis.min||AQ.from>AQ.axis.max||AN.to<AN.axis.min||AN.from>AN.axis.max){continue}AQ.from=Math.max(AQ.from,AQ.axis.min);AQ.to=Math.min(AQ.to,AQ.axis.max);AN.from=Math.max(AN.from,AN.axis.min);AN.to=Math.min(AN.to,AN.axis.max);if(AQ.from==AQ.to&&AN.from==AN.to){continue}AQ.from=AQ.axis.p2c(AQ.from);AQ.to=AQ.axis.p2c(AQ.to);AN.from=AN.axis.p2c(AN.from);AN.to=AN.axis.p2c(AN.to);if(AQ.from==AQ.to||AN.from==AN.to){Y.beginPath();Y.strokeStyle=AK.color||g.grid.markingsColor;Y.lineWidth=AK.lineWidth||g.grid.markingsLineWidth;Y.moveTo(AQ.from,AN.from);Y.lineTo(AQ.to,AN.to);Y.stroke()}else{Y.fillStyle=AK.color||g.grid.markingsColor;Y.fillRect(AQ.from,AN.to,AQ.to-AQ.from,AN.from-AN.to)}}}Y.lineWidth=1;Y.strokeStyle=g.grid.tickColor;Y.beginPath();var AM,AP=s.xaxis;for(AO=0;AO<AP.ticks.length;++AO){AM=AP.ticks[AO].v;if(AM<=AP.min||AM>=s.xaxis.max){continue}Y.moveTo(Math.floor(AP.p2c(AM))+Y.lineWidth/2,0);Y.lineTo(Math.floor(AP.p2c(AM))+Y.lineWidth/2,t)}AP=s.yaxis;for(AO=0;AO<AP.ticks.length;++AO){AM=AP.ticks[AO].v;if(AM<=AP.min||AM>=AP.max){continue}Y.moveTo(0,Math.floor(AP.p2c(AM))+Y.lineWidth/2);Y.lineTo(I,Math.floor(AP.p2c(AM))+Y.lineWidth/2)}AP=s.x2axis;for(AO=0;AO<AP.ticks.length;++AO){AM=AP.ticks[AO].v;if(AM<=AP.min||AM>=AP.max){continue}Y.moveTo(Math.floor(AP.p2c(AM))+Y.lineWidth/2,-5);Y.lineTo(Math.floor(AP.p2c(AM))+Y.lineWidth/2,5)}AP=s.y2axis;for(AO=0;AO<AP.ticks.length;++AO){AM=AP.ticks[AO].v;if(AM<=AP.min||AM>=AP.max){continue}Y.moveTo(I-5,Math.floor(AP.p2c(AM))+Y.lineWidth/2);Y.lineTo(I+5,Math.floor(AP.p2c(AM))+Y.lineWidth/2)}Y.stroke();if(g.grid.borderWidth){var AR=g.grid.borderWidth;Y.lineWidth=AR;Y.strokeStyle=g.grid.borderColor;Y.strokeRect(-AR/2,-AR/2,I+AR,t+AR)}Y.restore()}function h(){l.find(".tickLabels").remove();var AK=['<div class="tickLabels" style="font-size:smaller;color:'+g.grid.color+'">'];function AM(AP,AQ){for(var AO=0;AO<AP.ticks.length;++AO){var AN=AP.ticks[AO];if(!AN.label||AN.v<AP.min||AN.v>AP.max){continue}AK.push(AQ(AN,AP))}}var AL=g.grid.labelMargin+g.grid.borderWidth;AM(s.xaxis,function(AN,AO){return'<div style="position:absolute;top:'+(e.top+t+AL)+"px;left:"+Math.round(e.left+AO.p2c(AN.v)-AO.labelWidth/2)+"px;width:"+AO.labelWidth+'px;text-align:center" class="tickLabel">'+AN.label+"</div>"});AM(s.yaxis,function(AN,AO){return'<div style="position:absolute;top:'+Math.round(e.top+AO.p2c(AN.v)-AO.labelHeight/2)+"px;right:"+(e.right+I+AL)+"px;width:"+AO.labelWidth+'px;text-align:right" class="tickLabel">'+AN.label+"</div>"});AM(s.x2axis,function(AN,AO){return'<div style="position:absolute;bottom:'+(e.bottom+t+AL)+"px;left:"+Math.round(e.left+AO.p2c(AN.v)-AO.labelWidth/2)+"px;width:"+AO.labelWidth+'px;text-align:center" class="tickLabel">'+AN.label+"</div>"});AM(s.y2axis,function(AN,AO){return'<div style="position:absolute;top:'+Math.round(e.top+AO.p2c(AN.v)-AO.labelHeight/2)+"px;left:"+(e.left+I+AL)+"px;width:"+AO.labelWidth+'px;text-align:left" class="tickLabel">'+AN.label+"</div>"});AK.push("</div>");l.append(AK.join(""))}function AA(AK){if(AK.lines.show){a(AK)}if(AK.bars.show){n(AK)}if(AK.points.show){o(AK)}}function a(AN){function AM(AY,AZ,AR,Ad,Ac){var Ae=AY.points,AS=AY.pointsize,AW=null,AV=null;Y.beginPath();for(var AX=AS;AX<Ae.length;AX+=AS){var AU=Ae[AX-AS],Ab=Ae[AX-AS+1],AT=Ae[AX],Aa=Ae[AX+1];if(AU==null||AT==null){continue}if(Ab<=Aa&&Ab<Ac.min){if(Aa<Ac.min){continue}AU=(Ac.min-Ab)/(Aa-Ab)*(AT-AU)+AU;Ab=Ac.min}else{if(Aa<=Ab&&Aa<Ac.min){if(Ab<Ac.min){continue}AT=(Ac.min-Ab)/(Aa-Ab)*(AT-AU)+AU;Aa=Ac.min}}if(Ab>=Aa&&Ab>Ac.max){if(Aa>Ac.max){continue}AU=(Ac.max-Ab)/(Aa-Ab)*(AT-AU)+AU;Ab=Ac.max}else{if(Aa>=Ab&&Aa>Ac.max){if(Ab>Ac.max){continue}AT=(Ac.max-Ab)/(Aa-Ab)*(AT-AU)+AU;Aa=Ac.max}}if(AU<=AT&&AU<Ad.min){if(AT<Ad.min){continue}Ab=(Ad.min-AU)/(AT-AU)*(Aa-Ab)+Ab;AU=Ad.min}else{if(AT<=AU&&AT<Ad.min){if(AU<Ad.min){continue}Aa=(Ad.min-AU)/(AT-AU)*(Aa-Ab)+Ab;AT=Ad.min}}if(AU>=AT&&AU>Ad.max){if(AT>Ad.max){continue}Ab=(Ad.max-AU)/(AT-AU)*(Aa-Ab)+Ab;AU=Ad.max}else{if(AT>=AU&&AT>Ad.max){if(AU>Ad.max){continue}Aa=(Ad.max-AU)/(AT-AU)*(Aa-Ab)+Ab;AT=Ad.max}}if(AU!=AW||Ab!=AV){Y.moveTo(Ad.p2c(AU)+AZ,Ac.p2c(Ab)+AR)}AW=AT;AV=Aa;Y.lineTo(Ad.p2c(AT)+AZ,Ac.p2c(Aa)+AR)}Y.stroke()}function AO(AX,Ae,Ac){var Af=AX.points,AR=AX.pointsize,AS=Math.min(Math.max(0,Ac.min),Ac.max),Aa,AV=0,Ad=false;for(var AW=AR;AW<Af.length;AW+=AR){var AU=Af[AW-AR],Ab=Af[AW-AR+1],AT=Af[AW],AZ=Af[AW+1];if(Ad&&AU!=null&&AT==null){Y.lineTo(Ae.p2c(AV),Ac.p2c(AS));Y.fill();Ad=false;continue}if(AU==null||AT==null){continue}if(AU<=AT&&AU<Ae.min){if(AT<Ae.min){continue}Ab=(Ae.min-AU)/(AT-AU)*(AZ-Ab)+Ab;AU=Ae.min}else{if(AT<=AU&&AT<Ae.min){if(AU<Ae.min){continue}AZ=(Ae.min-AU)/(AT-AU)*(AZ-Ab)+Ab;AT=Ae.min}}if(AU>=AT&&AU>Ae.max){if(AT>Ae.max){continue}Ab=(Ae.max-AU)/(AT-AU)*(AZ-Ab)+Ab;AU=Ae.max}else{if(AT>=AU&&AT>Ae.max){if(AU>Ae.max){continue}AZ=(Ae.max-AU)/(AT-AU)*(AZ-Ab)+Ab;AT=Ae.max}}if(!Ad){Y.beginPath();Y.moveTo(Ae.p2c(AU),Ac.p2c(AS));Ad=true}if(Ab>=Ac.max&&AZ>=Ac.max){Y.lineTo(Ae.p2c(AU),Ac.p2c(Ac.max));Y.lineTo(Ae.p2c(AT),Ac.p2c(Ac.max));AV=AT;continue}else{if(Ab<=Ac.min&&AZ<=Ac.min){Y.lineTo(Ae.p2c(AU),Ac.p2c(Ac.min));Y.lineTo(Ae.p2c(AT),Ac.p2c(Ac.min));AV=AT;continue}}var Ag=AU,AY=AT;if(Ab<=AZ&&Ab<Ac.min&&AZ>=Ac.min){AU=(Ac.min-Ab)/(AZ-Ab)*(AT-AU)+AU;Ab=Ac.min}else{if(AZ<=Ab&&AZ<Ac.min&&Ab>=Ac.min){AT=(Ac.min-Ab)/(AZ-Ab)*(AT-AU)+AU;AZ=Ac.min}}if(Ab>=AZ&&Ab>Ac.max&&AZ<=Ac.max){AU=(Ac.max-Ab)/(AZ-Ab)*(AT-AU)+AU;Ab=Ac.max}else{if(AZ>=Ab&&AZ>Ac.max&&Ab<=Ac.max){AT=(Ac.max-Ab)/(AZ-Ab)*(AT-AU)+AU;AZ=Ac.max}}if(AU!=Ag){if(Ab<=Ac.min){Aa=Ac.min}else{Aa=Ac.max}Y.lineTo(Ae.p2c(Ag),Ac.p2c(Aa));Y.lineTo(Ae.p2c(AU),Ac.p2c(Aa))}Y.lineTo(Ae.p2c(AU),Ac.p2c(Ab));Y.lineTo(Ae.p2c(AT),Ac.p2c(AZ));if(AT!=AY){if(AZ<=Ac.min){Aa=Ac.min}else{Aa=Ac.max}Y.lineTo(Ae.p2c(AT),Ac.p2c(Aa));Y.lineTo(Ae.p2c(AY),Ac.p2c(Aa))}AV=Math.max(AT,AY)}if(Ad){Y.lineTo(Ae.p2c(AV),Ac.p2c(AS));Y.fill()}}Y.save();Y.translate(e.left,e.top);Y.lineJoin="round";var AP=AN.lines.lineWidth,AK=AN.shadowSize;if(AP>0&&AK>0){Y.lineWidth=AK;Y.strokeStyle="rgba(0,0,0,0.1)";var AQ=Math.PI/18;AM(AN.datapoints,Math.sin(AQ)*(AP/2+AK/2),Math.cos(AQ)*(AP/2+AK/2),AN.xaxis,AN.yaxis);Y.lineWidth=AK/2;AM(AN.datapoints,Math.sin(AQ)*(AP/2+AK/4),Math.cos(AQ)*(AP/2+AK/4),AN.xaxis,AN.yaxis)}Y.lineWidth=AP;Y.strokeStyle=AN.color;var AL=V(AN.lines,AN.color,0,t);if(AL){Y.fillStyle=AL;AO(AN.datapoints,AN.xaxis,AN.yaxis)}if(AP>0){AM(AN.datapoints,0,0,AN.xaxis,AN.yaxis)}Y.restore()}function o(AN){function AP(AU,AT,Ab,AR,AV,AZ,AY){var Aa=AU.points,AQ=AU.pointsize;for(var AS=0;AS<Aa.length;AS+=AQ){var AX=Aa[AS],AW=Aa[AS+1];if(AX==null||AX<AZ.min||AX>AZ.max||AW<AY.min||AW>AY.max){continue}Y.beginPath();Y.arc(AZ.p2c(AX),AY.p2c(AW)+AR,AT,0,AV,false);if(Ab){Y.fillStyle=Ab;Y.fill()}Y.stroke()}}Y.save();Y.translate(e.left,e.top);var AO=AN.lines.lineWidth,AL=AN.shadowSize,AK=AN.points.radius;if(AO>0&&AL>0){var AM=AL/2;Y.lineWidth=AM;Y.strokeStyle="rgba(0,0,0,0.1)";AP(AN.datapoints,AK,null,AM+AM/2,Math.PI,AN.xaxis,AN.yaxis);Y.strokeStyle="rgba(0,0,0,0.2)";AP(AN.datapoints,AK,null,AM/2,Math.PI,AN.xaxis,AN.yaxis)}Y.lineWidth=AO;Y.strokeStyle=AN.color;AP(AN.datapoints,AK,V(AN.points,AN.color),0,2*Math.PI,AN.xaxis,AN.yaxis);Y.restore()}function AB(AV,AU,Ad,AQ,AY,AN,AL,AT,AS,Ac,AZ){var AM,Ab,AR,AX,AO,AK,AW,AP,Aa;if(AZ){AP=AK=AW=true;AO=false;AM=Ad;Ab=AV;AX=AU+AQ;AR=AU+AY;if(Ab<AM){Aa=Ab;Ab=AM;AM=Aa;AO=true;AK=false}}else{AO=AK=AW=true;AP=false;AM=AV+AQ;Ab=AV+AY;AR=Ad;AX=AU;if(AX<AR){Aa=AX;AX=AR;AR=Aa;AP=true;AW=false}}if(Ab<AT.min||AM>AT.max||AX<AS.min||AR>AS.max){return }if(AM<AT.min){AM=AT.min;AO=false}if(Ab>AT.max){Ab=AT.max;AK=false}if(AR<AS.min){AR=AS.min;AP=false}if(AX>AS.max){AX=AS.max;AW=false}AM=AT.p2c(AM);AR=AS.p2c(AR);Ab=AT.p2c(Ab);AX=AS.p2c(AX);if(AL){Ac.beginPath();Ac.moveTo(AM,AR);Ac.lineTo(AM,AX);Ac.lineTo(Ab,AX);Ac.lineTo(Ab,AR);Ac.fillStyle=AL(AR,AX);Ac.fill()}if(AO||AK||AW||AP){Ac.beginPath();Ac.moveTo(AM,AR+AN);if(AO){Ac.lineTo(AM,AX+AN)}else{Ac.moveTo(AM,AX+AN)}if(AW){Ac.lineTo(Ab,AX+AN)}else{Ac.moveTo(Ab,AX+AN)}if(AK){Ac.lineTo(Ab,AR+AN)}else{Ac.moveTo(Ab,AR+AN)}if(AP){Ac.lineTo(AM,AR+AN)}else{Ac.moveTo(AM,AR+AN)}Ac.stroke()}}function n(AM){function AL(AS,AR,AU,AP,AT,AW,AV){var AX=AS.points,AO=AS.pointsize;for(var AQ=0;AQ<AX.length;AQ+=AO){if(AX[AQ]==null){continue}AB(AX[AQ],AX[AQ+1],AX[AQ+2],AR,AU,AP,AT,AW,AV,Y,AM.bars.horizontal)}}Y.save();Y.translate(e.left,e.top);Y.lineWidth=AM.bars.lineWidth;Y.strokeStyle=AM.color;var AK=AM.bars.align=="left"?0:-AM.bars.barWidth/2;var AN=AM.bars.fill?function(AO,AP){return V(AM.bars,AM.color,AO,AP)}:null;AL(AM.datapoints,AK,AK+AM.bars.barWidth,0,AN,AM.xaxis,AM.yaxis);Y.restore()}function V(AM,AK,AL,AO){var AN=AM.fill;if(!AN){return null}if(AM.fillColor){return R(AM.fillColor,AL,AO,AK)}var AP=C.color.parse(AK);AP.a=typeof AN=="number"?AN:0.4;AP.normalize();return AP.toString()}function AI(){l.find(".legend").remove();if(!g.legend.show){return }var AP=[],AN=false,AV=g.legend.labelFormatter,AU,AR;for(i=0;i<O.length;++i){AU=O[i];AR=AU.label;if(!AR){continue}if(i%g.legend.noColumns==0){if(AN){AP.push("</tr>")}AP.push("<tr>");AN=true}if(AV){AR=AV(AR,AU)}AP.push('<td class="legendColorBox"><div style="border:1px solid '+g.legend.labelBoxBorderColor+';padding:1px"><div style="width:4px;height:0;border:5px solid '+AU.color+';overflow:hidden"></div></div></td><td class="legendLabel">'+AR+"</td>")}if(AN){AP.push("</tr>")}if(AP.length==0){return }var AT='<table style="font-size:smaller;color:'+g.grid.color+'">'+AP.join("")+"</table>";if(g.legend.container!=null){C(g.legend.container).html(AT)}else{var AQ="",AL=g.legend.position,AM=g.legend.margin;if(AM[0]==null){AM=[AM,AM]}if(AL.charAt(0)=="n"){AQ+="top:"+(AM[1]+e.top)+"px;"}else{if(AL.charAt(0)=="s"){AQ+="bottom:"+(AM[1]+e.bottom)+"px;"}}if(AL.charAt(1)=="e"){AQ+="right:"+(AM[0]+e.right)+"px;"}else{if(AL.charAt(1)=="w"){AQ+="left:"+(AM[0]+e.left)+"px;"}}var AS=C('<div class="legend">'+AT.replace('style="','style="position:absolute;'+AQ+";")+"</div>").appendTo(l);if(g.legend.backgroundOpacity!=0){var AO=g.legend.backgroundColor;if(AO==null){AO=g.grid.backgroundColor;if(AO&&typeof AO=="string"){AO=C.color.parse(AO)}else{AO=C.color.extract(AS,"background-color")}AO.a=1;AO=AO.toString()}var AK=AS.children();C('<div style="position:absolute;width:'+AK.width()+"px;height:"+AK.height()+"px;"+AQ+"background-color:"+AO+';"> </div>').prependTo(AS).css("opacity",g.legend.backgroundOpacity)}}}var w=[],J=null;function AF(AR,AP,AM){var AX=g.grid.mouseActiveRadius,Aj=AX*AX+1,Ah=null,Aa=false,Af,Ad;for(Af=0;Af<O.length;++Af){if(!AM(O[Af])){continue}var AY=O[Af],AQ=AY.xaxis,AO=AY.yaxis,Ae=AY.datapoints.points,Ac=AY.datapoints.pointsize,AZ=AQ.c2p(AR),AW=AO.c2p(AP),AL=AX/AQ.scale,AK=AX/AO.scale;if(AY.lines.show||AY.points.show){for(Ad=0;Ad<Ae.length;Ad+=Ac){var AT=Ae[Ad],AS=Ae[Ad+1];if(AT==null){continue}if(AT-AZ>AL||AT-AZ<-AL||AS-AW>AK||AS-AW<-AK){continue}var AV=Math.abs(AQ.p2c(AT)-AR),AU=Math.abs(AO.p2c(AS)-AP),Ab=AV*AV+AU*AU;if(Ab<=Aj){Aj=Ab;Ah=[Af,Ad/Ac]}}}if(AY.bars.show&&!Ah){var AN=AY.bars.align=="left"?0:-AY.bars.barWidth/2,Ag=AN+AY.bars.barWidth;for(Ad=0;Ad<Ae.length;Ad+=Ac){var AT=Ae[Ad],AS=Ae[Ad+1],Ai=Ae[Ad+2];if(AT==null){continue}if(O[Af].bars.horizontal?(AZ<=Math.max(Ai,AT)&&AZ>=Math.min(Ai,AT)&&AW>=AS+AN&&AW<=AS+Ag):(AZ>=AT+AN&&AZ<=AT+Ag&&AW>=Math.min(Ai,AS)&&AW<=Math.max(Ai,AS))){Ah=[Af,Ad/Ac]}}}}if(Ah){Af=Ah[0];Ad=Ah[1];Ac=O[Af].datapoints.pointsize;return{datapoint:O[Af].datapoints.points.slice(Ad*Ac,(Ad+1)*Ac),dataIndex:Ad,series:O[Af],seriesIndex:Af}}return null}function D(AK){if(g.grid.hoverable){H("plothover",AK,function(AL){return AL.hoverable!=false})}}function d(AK){H("plotclick",AK,function(AL){return AL.clickable!=false})}function H(AL,AK,AM){var AN=AD.offset(),AS={pageX:AK.pageX,pageY:AK.pageY},AQ=AK.pageX-AN.left-e.left,AO=AK.pageY-AN.top-e.top;if(s.xaxis.used){AS.x=s.xaxis.c2p(AQ)}if(s.yaxis.used){AS.y=s.yaxis.c2p(AO)}if(s.x2axis.used){AS.x2=s.x2axis.c2p(AQ)}if(s.y2axis.used){AS.y2=s.y2axis.c2p(AO)}var AT=AF(AQ,AO,AM);if(AT){AT.pageX=parseInt(AT.series.xaxis.p2c(AT.datapoint[0])+AN.left+e.left);AT.pageY=parseInt(AT.series.yaxis.p2c(AT.datapoint[1])+AN.top+e.top)}if(g.grid.autoHighlight){for(var AP=0;AP<w.length;++AP){var AR=w[AP];if(AR.auto==AL&&!(AT&&AR.series==AT.series&&AR.point==AT.datapoint)){x(AR.series,AR.point)}}if(AT){AE(AT.series,AT.datapoint,AL)}}l.trigger(AL,[AS,AT])}function q(){if(!J){J=setTimeout(v,30)}}function v(){J=null;AJ.save();AJ.clearRect(0,0,y,Q);AJ.translate(e.left,e.top);var AL,AK;for(AL=0;AL<w.length;++AL){AK=w[AL];if(AK.series.bars.show){z(AK.series,AK.point)}else{u(AK.series,AK.point)}}AJ.restore();Z(L.drawOverlay,[AJ])}function AE(AM,AK,AN){if(typeof AM=="number"){AM=O[AM]}if(typeof AK=="number"){AK=AM.data[AK]}var AL=j(AM,AK);if(AL==-1){w.push({series:AM,point:AK,auto:AN});q()}else{if(!AN){w[AL].auto=false}}}function x(AM,AK){if(AM==null&&AK==null){w=[];q()}if(typeof AM=="number"){AM=O[AM]}if(typeof AK=="number"){AK=AM.data[AK]}var AL=j(AM,AK);if(AL!=-1){w.splice(AL,1);q()}}function j(AM,AN){for(var AK=0;AK<w.length;++AK){var AL=w[AK];if(AL.series==AM&&AL.point[0]==AN[0]&&AL.point[1]==AN[1]){return AK}}return -1}function u(AN,AM){var AL=AM[0],AR=AM[1],AQ=AN.xaxis,AP=AN.yaxis;if(AL<AQ.min||AL>AQ.max||AR<AP.min||AR>AP.max){return }var AO=AN.points.radius+AN.points.lineWidth/2;AJ.lineWidth=AO;AJ.strokeStyle=C.color.parse(AN.color).scale("a",0.5).toString();var AK=1.5*AO;AJ.beginPath();AJ.arc(AQ.p2c(AL),AP.p2c(AR),AK,0,2*Math.PI,false);AJ.stroke()}function z(AN,AK){AJ.lineWidth=AN.bars.lineWidth;AJ.strokeStyle=C.color.parse(AN.color).scale("a",0.5).toString();var AM=C.color.parse(AN.color).scale("a",0.5).toString();var AL=AN.bars.align=="left"?0:-AN.bars.barWidth/2;AB(AK[0],AK[1],AK[2]||0,AL,AL+AN.bars.barWidth,0,function(){return AM},AN.xaxis,AN.yaxis,AJ,AN.bars.horizontal)}function R(AM,AL,AQ,AO){if(typeof AM=="string"){return AM}else{var AP=Y.createLinearGradient(0,AQ,0,AL);for(var AN=0,AK=AM.colors.length;AN<AK;++AN){var AR=AM.colors[AN];if(typeof AR!="string"){AR=C.color.parse(AO).scale("rgb",AR.brightness);AR.a*=AR.opacity;AR=AR.toString()}AP.addColorStop(AN/(AK-1),AR)}return AP}}}C.plot=function(G,E,D){var F=new B(C(G),E,D,C.plot.plugins);return F};C.plot.plugins=[];C.plot.formatDate=function(H,E,G){var L=function(N){N=""+N;return N.length==1?"0"+N:N};var D=[];var M=false;var K=H.getUTCHours();var I=K<12;if(G==null){G=["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"]}if(E.search(/%p|%P/)!=-1){if(K>12){K=K-12}else{if(K==0){K=12}}}for(var F=0;F<E.length;++F){var J=E.charAt(F);if(M){switch(J){case"h":J=""+K;break;case"H":J=L(K);break;case"M":J=L(H.getUTCMinutes());break;case"S":J=L(H.getUTCSeconds());break;case"d":J=""+H.getUTCDate();break;case"m":J=""+(H.getUTCMonth()+1);break;case"y":J=""+H.getUTCFullYear();break;case"b":J=""+G[H.getUTCMonth()];break;case"p":J=(I)?("am"):("pm");break;case"P":J=(I)?("AM"):("PM");break}D.push(J);M=false}else{if(J=="%"){M=true}else{D.push(J)}}}return D.join("")};function A(E,D){return D*Math.floor(E/D)}})(jQuery);
\ No newline at end of file
diff --git a/lib/logstash/web/public/js/flot/jquery.flot.navigate.js b/lib/logstash/web/public/js/flot/jquery.flot.navigate.js
deleted file mode 100644
index e6f88341088..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.flot.navigate.js
+++ /dev/null
@@ -1,272 +0,0 @@
-/*
-Flot plugin for adding panning and zooming capabilities to a plot.
-
-The default behaviour is double click and scrollwheel up/down to zoom
-in, drag to pan. The plugin defines plot.zoom({ center }),
-plot.zoomOut() and plot.pan(offset) so you easily can add custom
-controls. It also fires a "plotpan" and "plotzoom" event when
-something happens, useful for synchronizing plots.
-
-Example usage:
-
-  plot = $.plot(...);
-  
-  // zoom default amount in on the pixel (100, 200) 
-  plot.zoom({ center: { left: 10, top: 20 } });
-
-  // zoom out again
-  plot.zoomOut({ center: { left: 10, top: 20 } });
-
-  // pan 100 pixels to the left and 20 down
-  plot.pan({ left: -100, top: 20 })
-
-
-Options:
-
-  zoom: {
-    interactive: false
-    trigger: "dblclick" // or "click" for single click
-    amount: 1.5         // 2 = 200% (zoom in), 0.5 = 50% (zoom out)
-  }
-  
-  pan: {
-    interactive: false
-  }
-
-  xaxis, yaxis, x2axis, y2axis: {
-    zoomRange: null  // or [number, number] (min range, max range)
-    panRange: null   // or [number, number] (min, max)
-  }
-  
-"interactive" enables the built-in drag/click behaviour. "amount" is
-the amount to zoom the viewport relative to the current range, so 1 is
-100% (i.e. no change), 1.5 is 150% (zoom in), 0.7 is 70% (zoom out).
-
-"zoomRange" is the interval in which zooming can happen, e.g. with
-zoomRange: [1, 100] the zoom will never scale the axis so that the
-difference between min and max is smaller than 1 or larger than 100.
-You can set either of them to null to ignore.
-
-"panRange" confines the panning to stay within a range, e.g. with
-panRange: [-10, 20] panning stops at -10 in one end and at 20 in the
-other. Either can be null.
-*/
-
-
-// First two dependencies, jquery.event.drag.js and
-// jquery.mousewheel.js, we put them inline here to save people the
-// effort of downloading them.
-
-/*
-jquery.event.drag.js ~ v1.5 ~ Copyright (c) 2008, Three Dub Media (http://threedubmedia.com)  
-Licensed under the MIT License ~ http://threedubmedia.googlecode.com/files/MIT-LICENSE.txt
-*/
-(function(E){E.fn.drag=function(L,K,J){if(K){this.bind("dragstart",L)}if(J){this.bind("dragend",J)}return !L?this.trigger("drag"):this.bind("drag",K?K:L)};var A=E.event,B=A.special,F=B.drag={not:":input",distance:0,which:1,dragging:false,setup:function(J){J=E.extend({distance:F.distance,which:F.which,not:F.not},J||{});J.distance=I(J.distance);A.add(this,"mousedown",H,J);if(this.attachEvent){this.attachEvent("ondragstart",D)}},teardown:function(){A.remove(this,"mousedown",H);if(this===F.dragging){F.dragging=F.proxy=false}G(this,true);if(this.detachEvent){this.detachEvent("ondragstart",D)}}};B.dragstart=B.dragend={setup:function(){},teardown:function(){}};function H(L){var K=this,J,M=L.data||{};if(M.elem){K=L.dragTarget=M.elem;L.dragProxy=F.proxy||K;L.cursorOffsetX=M.pageX-M.left;L.cursorOffsetY=M.pageY-M.top;L.offsetX=L.pageX-L.cursorOffsetX;L.offsetY=L.pageY-L.cursorOffsetY}else{if(F.dragging||(M.which>0&&L.which!=M.which)||E(L.target).is(M.not)){return }}switch(L.type){case"mousedown":E.extend(M,E(K).offset(),{elem:K,target:L.target,pageX:L.pageX,pageY:L.pageY});A.add(document,"mousemove mouseup",H,M);G(K,false);F.dragging=null;return false;case !F.dragging&&"mousemove":if(I(L.pageX-M.pageX)+I(L.pageY-M.pageY)<M.distance){break}L.target=M.target;J=C(L,"dragstart",K);if(J!==false){F.dragging=K;F.proxy=L.dragProxy=E(J||K)[0]}case"mousemove":if(F.dragging){J=C(L,"drag",K);if(B.drop){B.drop.allowed=(J!==false);B.drop.handler(L)}if(J!==false){break}L.type="mouseup"}case"mouseup":A.remove(document,"mousemove mouseup",H);if(F.dragging){if(B.drop){B.drop.handler(L)}C(L,"dragend",K)}G(K,true);F.dragging=F.proxy=M.elem=false;break}return true}function C(M,K,L){M.type=K;var J=E.event.handle.call(L,M);return J===false?false:J||M.result}function I(J){return Math.pow(J,2)}function D(){return(F.dragging===false)}function G(K,J){if(!K){return }K.unselectable=J?"off":"on";K.onselectstart=function(){return J};if(K.style){K.style.MozUserSelect=J?"":"none"}}})(jQuery);
-
-
-/* jquery.mousewheel.min.js
- * Copyright (c) 2009 Brandon Aaron (http://brandonaaron.net)
- * Dual licensed under the MIT (http://www.opensource.org/licenses/mit-license.php)
- * and GPL (http://www.opensource.org/licenses/gpl-license.php) licenses.
- * Thanks to: http://adomas.org/javascript-mouse-wheel/ for some pointers.
- * Thanks to: Mathias Bank(http://www.mathias-bank.de) for a scope bug fix.
- *
- * Version: 3.0.2
- * 
- * Requires: 1.2.2+
- */
-(function(c){var a=["DOMMouseScroll","mousewheel"];c.event.special.mousewheel={setup:function(){if(this.addEventListener){for(var d=a.length;d;){this.addEventListener(a[--d],b,false)}}else{this.onmousewheel=b}},teardown:function(){if(this.removeEventListener){for(var d=a.length;d;){this.removeEventListener(a[--d],b,false)}}else{this.onmousewheel=null}}};c.fn.extend({mousewheel:function(d){return d?this.bind("mousewheel",d):this.trigger("mousewheel")},unmousewheel:function(d){return this.unbind("mousewheel",d)}});function b(f){var d=[].slice.call(arguments,1),g=0,e=true;f=c.event.fix(f||window.event);f.type="mousewheel";if(f.wheelDelta){g=f.wheelDelta/120}if(f.detail){g=-f.detail/3}d.unshift(f,g);return c.event.handle.apply(this,d)}})(jQuery);
-
-
-
-
-(function ($) {
-    var options = {
-        xaxis: {
-            zoomRange: null, // or [number, number] (min range, max range)
-            panRange: null // or [number, number] (min, max)
-        },
-        zoom: {
-            interactive: false,
-            trigger: "dblclick", // or "click" for single click
-            amount: 1.5 // how much to zoom relative to current position, 2 = 200% (zoom in), 0.5 = 50% (zoom out)
-        },
-        pan: {
-            interactive: false
-        }
-    };
-
-    function init(plot) {
-        function bindEvents(plot, eventHolder) {
-            var o = plot.getOptions();
-            if (o.zoom.interactive) {
-                function clickHandler(e, zoomOut) {
-                    var c = plot.offset();
-                    c.left = e.pageX - c.left;
-                    c.top = e.pageY - c.top;
-                    if (zoomOut)
-                        plot.zoomOut({ center: c });
-                    else
-                        plot.zoom({ center: c });
-                }
-                
-                eventHolder[o.zoom.trigger](clickHandler);
-
-                eventHolder.mousewheel(function (e, delta) {
-                    clickHandler(e, delta < 0);
-                    return false;
-                });
-            }
-            if (o.pan.interactive) {
-                var prevCursor = 'default', pageX = 0, pageY = 0;
-                
-                eventHolder.bind("dragstart", { distance: 10 }, function (e) {
-                    if (e.which != 1)  // only accept left-click
-                        return false;
-                    eventHolderCursor = eventHolder.css('cursor');
-                    eventHolder.css('cursor', 'move');
-                    pageX = e.pageX;
-                    pageY = e.pageY;
-                });
-                eventHolder.bind("drag", function (e) {
-                    // unused at the moment, but we need it here to
-                    // trigger the dragstart/dragend events
-                });
-                eventHolder.bind("dragend", function (e) {
-                    eventHolder.css('cursor', prevCursor);
-                    plot.pan({ left: pageX - e.pageX,
-                               top: pageY - e.pageY });
-                });
-            }
-        }
-
-        plot.zoomOut = function (args) {
-            if (!args)
-                args = {};
-            
-            if (!args.amount)
-                args.amount = plot.getOptions().zoom.amount
-
-            args.amount = 1 / args.amount;
-            plot.zoom(args);
-        }
-        
-        plot.zoom = function (args) {
-            if (!args)
-                args = {};
-            
-            var axes = plot.getAxes(),
-                options = plot.getOptions(),
-                c = args.center,
-                amount = args.amount ? args.amount : options.zoom.amount,
-                w = plot.width(), h = plot.height();
-
-            if (!c)
-                c = { left: w / 2, top: h / 2 };
-                
-            var xf = c.left / w,
-                x1 = c.left - xf * w / amount,
-                x2 = c.left + (1 - xf) * w / amount,
-                yf = c.top / h,
-                y1 = c.top - yf * h / amount,
-                y2 = c.top + (1 - yf) * h / amount;
-
-            function scaleAxis(min, max, name) {
-                var axis = axes[name],
-                    axisOptions = options[name];
-                
-                if (!axis.used)
-                    return;
-                    
-                min = axis.c2p(min);
-                max = axis.c2p(max);
-                if (max < min) { // make sure min < max
-                    var tmp = min
-                    min = max;
-                    max = tmp;
-                }
-
-                var range = max - min, zr = axisOptions.zoomRange;
-                if (zr &&
-                    ((zr[0] != null && range < zr[0]) ||
-                     (zr[1] != null && range > zr[1])))
-                    return;
-            
-                axisOptions.min = min;
-                axisOptions.max = max;
-            }
-
-            scaleAxis(x1, x2, 'xaxis');
-            scaleAxis(x1, x2, 'x2axis');
-            scaleAxis(y1, y2, 'yaxis');
-            scaleAxis(y1, y2, 'y2axis');
-            
-            plot.setupGrid();
-            plot.draw();
-            
-            if (!args.preventEvent)
-                plot.getPlaceholder().trigger("plotzoom", [ plot ]);
-        }
-
-        plot.pan = function (args) {
-            var l = +args.left, t = +args.top,
-                axes = plot.getAxes(), options = plot.getOptions();
-
-            if (isNaN(l))
-                l = 0;
-            if (isNaN(t))
-                t = 0;
-
-            function panAxis(delta, name) {
-                var axis = axes[name],
-                    axisOptions = options[name],
-                    min, max;
-                
-                if (!axis.used)
-                    return;
-
-                min = axis.c2p(axis.p2c(axis.min) + delta),
-                max = axis.c2p(axis.p2c(axis.max) + delta);
-
-                var pr = axisOptions.panRange;
-                if (pr) {
-                    // check whether we hit the wall
-                    if (pr[0] != null && pr[0] > min) {
-                        delta = pr[0] - min;
-                        min += delta;
-                        max += delta;
-                    }
-                    
-                    if (pr[1] != null && pr[1] < max) {
-                        delta = pr[1] - max;
-                        min += delta;
-                        max += delta;
-                    }
-                }
-                
-                axisOptions.min = min;
-                axisOptions.max = max;
-            }
-
-            panAxis(l, 'xaxis');
-            panAxis(l, 'x2axis');
-            panAxis(t, 'yaxis');
-            panAxis(t, 'y2axis');
-            
-            plot.setupGrid();
-            plot.draw();
-            
-            if (!args.preventEvent)
-                plot.getPlaceholder().trigger("plotpan", [ plot ]);
-        }
-        
-        plot.hooks.bindEvents.push(bindEvents);
-    }
-    
-    $.plot.plugins.push({
-        init: init,
-        options: options,
-        name: 'navigate',
-        version: '1.1'
-    });
-})(jQuery);
diff --git a/lib/logstash/web/public/js/flot/jquery.flot.navigate.min.js b/lib/logstash/web/public/js/flot/jquery.flot.navigate.min.js
deleted file mode 100644
index fb7814e9b98..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.flot.navigate.min.js
+++ /dev/null
@@ -1 +0,0 @@
-(function(R){R.fn.drag=function(A,B,C){if(B){this.bind("dragstart",A)}if(C){this.bind("dragend",C)}return !A?this.trigger("drag"):this.bind("drag",B?B:A)};var M=R.event,L=M.special,Q=L.drag={not:":input",distance:0,which:1,dragging:false,setup:function(A){A=R.extend({distance:Q.distance,which:Q.which,not:Q.not},A||{});A.distance=N(A.distance);M.add(this,"mousedown",O,A);if(this.attachEvent){this.attachEvent("ondragstart",J)}},teardown:function(){M.remove(this,"mousedown",O);if(this===Q.dragging){Q.dragging=Q.proxy=false}P(this,true);if(this.detachEvent){this.detachEvent("ondragstart",J)}}};L.dragstart=L.dragend={setup:function(){},teardown:function(){}};function O(A){var B=this,C,D=A.data||{};if(D.elem){B=A.dragTarget=D.elem;A.dragProxy=Q.proxy||B;A.cursorOffsetX=D.pageX-D.left;A.cursorOffsetY=D.pageY-D.top;A.offsetX=A.pageX-A.cursorOffsetX;A.offsetY=A.pageY-A.cursorOffsetY}else{if(Q.dragging||(D.which>0&&A.which!=D.which)||R(A.target).is(D.not)){return }}switch(A.type){case"mousedown":R.extend(D,R(B).offset(),{elem:B,target:A.target,pageX:A.pageX,pageY:A.pageY});M.add(document,"mousemove mouseup",O,D);P(B,false);Q.dragging=null;return false;case !Q.dragging&&"mousemove":if(N(A.pageX-D.pageX)+N(A.pageY-D.pageY)<D.distance){break}A.target=D.target;C=K(A,"dragstart",B);if(C!==false){Q.dragging=B;Q.proxy=A.dragProxy=R(C||B)[0]}case"mousemove":if(Q.dragging){C=K(A,"drag",B);if(L.drop){L.drop.allowed=(C!==false);L.drop.handler(A)}if(C!==false){break}A.type="mouseup"}case"mouseup":M.remove(document,"mousemove mouseup",O);if(Q.dragging){if(L.drop){L.drop.handler(A)}K(A,"dragend",B)}P(B,true);Q.dragging=Q.proxy=D.elem=false;break}return true}function K(D,B,A){D.type=B;var C=R.event.handle.call(A,D);return C===false?false:C||D.result}function N(A){return Math.pow(A,2)}function J(){return(Q.dragging===false)}function P(A,B){if(!A){return }A.unselectable=B?"off":"on";A.onselectstart=function(){return B};if(A.style){A.style.MozUserSelect=B?"":"none"}}})(jQuery);(function(C){var B=["DOMMouseScroll","mousewheel"];C.event.special.mousewheel={setup:function(){if(this.addEventListener){for(var D=B.length;D;){this.addEventListener(B[--D],A,false)}}else{this.onmousewheel=A}},teardown:function(){if(this.removeEventListener){for(var D=B.length;D;){this.removeEventListener(B[--D],A,false)}}else{this.onmousewheel=null}}};C.fn.extend({mousewheel:function(D){return D?this.bind("mousewheel",D):this.trigger("mousewheel")},unmousewheel:function(D){return this.unbind("mousewheel",D)}});function A(E){var G=[].slice.call(arguments,1),D=0,F=true;E=C.event.fix(E||window.event);E.type="mousewheel";if(E.wheelDelta){D=E.wheelDelta/120}if(E.detail){D=-E.detail/3}G.unshift(E,D);return C.event.handle.apply(this,G)}})(jQuery);(function(B){var A={xaxis:{zoomRange:null,panRange:null},zoom:{interactive:false,trigger:"dblclick",amount:1.5},pan:{interactive:false}};function C(D){function E(J,F){var K=J.getOptions();if(K.zoom.interactive){function L(N,M){var O=J.offset();O.left=N.pageX-O.left;O.top=N.pageY-O.top;if(M){J.zoomOut({center:O})}else{J.zoom({center:O})}}F[K.zoom.trigger](L);F.mousewheel(function(M,N){L(M,N<0);return false})}if(K.pan.interactive){var I="default",H=0,G=0;F.bind("dragstart",{distance:10},function(M){if(M.which!=1){return false}eventHolderCursor=F.css("cursor");F.css("cursor","move");H=M.pageX;G=M.pageY});F.bind("drag",function(M){});F.bind("dragend",function(M){F.css("cursor",I);J.pan({left:H-M.pageX,top:G-M.pageY})})}}D.zoomOut=function(F){if(!F){F={}}if(!F.amount){F.amount=D.getOptions().zoom.amount}F.amount=1/F.amount;D.zoom(F)};D.zoom=function(M){if(!M){M={}}var L=D.getAxes(),S=D.getOptions(),N=M.center,J=M.amount?M.amount:S.zoom.amount,R=D.width(),I=D.height();if(!N){N={left:R/2,top:I/2}}var Q=N.left/R,G=N.left-Q*R/J,F=N.left+(1-Q)*R/J,H=N.top/I,P=N.top-H*I/J,O=N.top+(1-H)*I/J;function K(X,T,V){var Y=L[V],a=S[V];if(!Y.used){return }X=Y.c2p(X);T=Y.c2p(T);if(T<X){var W=X;X=T;T=W}var U=T-X,Z=a.zoomRange;if(Z&&((Z[0]!=null&&U<Z[0])||(Z[1]!=null&&U>Z[1]))){return }a.min=X;a.max=T}K(G,F,"xaxis");K(G,F,"x2axis");K(P,O,"yaxis");K(P,O,"y2axis");D.setupGrid();D.draw();if(!M.preventEvent){D.getPlaceholder().trigger("plotzoom",[D])}};D.pan=function(I){var F=+I.left,J=+I.top,K=D.getAxes(),H=D.getOptions();if(isNaN(F)){F=0}if(isNaN(J)){J=0}function G(R,M){var O=K[M],Q=H[M],N,L;if(!O.used){return }N=O.c2p(O.p2c(O.min)+R),L=O.c2p(O.p2c(O.max)+R);var P=Q.panRange;if(P){if(P[0]!=null&&P[0]>N){R=P[0]-N;N+=R;L+=R}if(P[1]!=null&&P[1]<L){R=P[1]-L;N+=R;L+=R}}Q.min=N;Q.max=L}G(F,"xaxis");G(F,"x2axis");G(J,"yaxis");G(J,"y2axis");D.setupGrid();D.draw();if(!I.preventEvent){D.getPlaceholder().trigger("plotpan",[D])}};D.hooks.bindEvents.push(E)}B.plot.plugins.push({init:C,options:A,name:"navigate",version:"1.1"})})(jQuery);
\ No newline at end of file
diff --git a/lib/logstash/web/public/js/flot/jquery.flot.selection.js b/lib/logstash/web/public/js/flot/jquery.flot.selection.js
deleted file mode 100644
index da81c92363e..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.flot.selection.js
+++ /dev/null
@@ -1,299 +0,0 @@
-/*
-Flot plugin for selecting regions.
-
-The plugin defines the following options:
-
-  selection: {
-    mode: null or "x" or "y" or "xy",
-    color: color
-  }
-
-You enable selection support by setting the mode to one of "x", "y" or
-"xy". In "x" mode, the user will only be able to specify the x range,
-similarly for "y" mode. For "xy", the selection becomes a rectangle
-where both ranges can be specified. "color" is color of the selection.
-
-When selection support is enabled, a "plotselected" event will be emitted
-on the DOM element you passed into the plot function. The event
-handler gets one extra parameter with the ranges selected on the axes,
-like this:
-
-  placeholder.bind("plotselected", function(event, ranges) {
-    alert("You selected " + ranges.xaxis.from + " to " + ranges.xaxis.to)
-    // similar for yaxis, secondary axes are in x2axis
-    // and y2axis if present
-  });
-
-The "plotselected" event is only fired when the user has finished
-making the selection. A "plotselecting" event is fired during the
-process with the same parameters as the "plotselected" event, in case
-you want to know what's happening while it's happening,
-
-A "plotunselected" event with no arguments is emitted when the user
-clicks the mouse to remove the selection.
-
-The plugin allso adds the following methods to the plot object:
-
-- setSelection(ranges, preventEvent)
-
-  Set the selection rectangle. The passed in ranges is on the same
-  form as returned in the "plotselected" event. If the selection
-  mode is "x", you should put in either an xaxis (or x2axis) object,
-  if the mode is "y" you need to put in an yaxis (or y2axis) object
-  and both xaxis/x2axis and yaxis/y2axis if the selection mode is
-  "xy", like this:
-
-    setSelection({ xaxis: { from: 0, to: 10 }, yaxis: { from: 40, to: 60 } });
-
-  setSelection will trigger the "plotselected" event when called. If
-  you don't want that to happen, e.g. if you're inside a
-  "plotselected" handler, pass true as the second parameter.
-  
-- clearSelection(preventEvent)
-
-  Clear the selection rectangle. Pass in true to avoid getting a
-  "plotunselected" event.
-
-- getSelection()
-
-  Returns the current selection in the same format as the
-  "plotselected" event. If there's currently no selection, the
-  function returns null.
-
-*/
-
-(function ($) {
-    function init(plot) {
-        var selection = {
-                first: { x: -1, y: -1}, second: { x: -1, y: -1},
-                show: false,
-                active: false
-            };
-
-        // FIXME: The drag handling implemented here should be
-        // abstracted out, there's some similar code from a library in
-        // the navigation plugin, this should be massaged a bit to fit
-        // the Flot cases here better and reused. Doing this would
-        // make this plugin much slimmer.
-        var savedhandlers = {};
-
-        function onMouseMove(e) {
-            if (selection.active) {
-                plot.getPlaceholder().trigger("plotselecting", [ getSelection() ]);
-
-                updateSelection(e);
-            }
-        }
-
-        function onMouseDown(e) {
-            if (e.which != 1)  // only accept left-click
-                return;
-            
-            // cancel out any text selections
-            document.body.focus();
-
-            // prevent text selection and drag in old-school browsers
-            if (document.onselectstart !== undefined && savedhandlers.onselectstart == null) {
-                savedhandlers.onselectstart = document.onselectstart;
-                document.onselectstart = function () { return false; };
-            }
-            if (document.ondrag !== undefined && savedhandlers.ondrag == null) {
-                savedhandlers.ondrag = document.ondrag;
-                document.ondrag = function () { return false; };
-            }
-
-            setSelectionPos(selection.first, e);
-
-            selection.active = true;
-            
-            $(document).one("mouseup", onMouseUp);
-        }
-
-        function onMouseUp(e) {
-            // revert drag stuff for old-school browsers
-            if (document.onselectstart !== undefined)
-                document.onselectstart = savedhandlers.onselectstart;
-            if (document.ondrag !== undefined)
-                document.ondrag = savedhandlers.ondrag;
-
-            // no more draggy-dee-drag
-            selection.active = false;
-            updateSelection(e);
-
-            if (selectionIsSane())
-                triggerSelectedEvent();
-            else {
-                // this counts as a clear
-                plot.getPlaceholder().trigger("plotunselected", [ ]);
-                plot.getPlaceholder().trigger("plotselecting", [ null ]);
-            }
-
-            return false;
-        }
-
-        function getSelection() {
-            if (!selectionIsSane())
-                return null;
-
-            var x1 = Math.min(selection.first.x, selection.second.x),
-                x2 = Math.max(selection.first.x, selection.second.x),
-                y1 = Math.max(selection.first.y, selection.second.y),
-                y2 = Math.min(selection.first.y, selection.second.y);
-
-            var r = {};
-            var axes = plot.getAxes();
-            if (axes.xaxis.used)
-                r.xaxis = { from: axes.xaxis.c2p(x1), to: axes.xaxis.c2p(x2) };
-            if (axes.x2axis.used)
-                r.x2axis = { from: axes.x2axis.c2p(x1), to: axes.x2axis.c2p(x2) };
-            if (axes.yaxis.used)
-                r.yaxis = { from: axes.yaxis.c2p(y1), to: axes.yaxis.c2p(y2) };
-            if (axes.y2axis.used)
-                r.y2axis = { from: axes.y2axis.c2p(y1), to: axes.y2axis.c2p(y2) };
-            return r;
-        }
-
-        function triggerSelectedEvent() {
-            var r = getSelection();
-
-            plot.getPlaceholder().trigger("plotselected", [ r ]);
-
-            // backwards-compat stuff, to be removed in future
-            var axes = plot.getAxes();
-            if (axes.xaxis.used && axes.yaxis.used)
-                plot.getPlaceholder().trigger("selected", [ { x1: r.xaxis.from, y1: r.yaxis.from, x2: r.xaxis.to, y2: r.yaxis.to } ]);
-        }
-
-        function clamp(min, value, max) {
-            return value < min? min: (value > max? max: value);
-        }
-
-        function setSelectionPos(pos, e) {
-            var o = plot.getOptions();
-            var offset = plot.getPlaceholder().offset();
-            var plotOffset = plot.getPlotOffset();
-            pos.x = clamp(0, e.pageX - offset.left - plotOffset.left, plot.width());
-            pos.y = clamp(0, e.pageY - offset.top - plotOffset.top, plot.height());
-
-            if (o.selection.mode == "y")
-                pos.x = pos == selection.first? 0: plot.width();
-
-            if (o.selection.mode == "x")
-                pos.y = pos == selection.first? 0: plot.height();
-        }
-
-        function updateSelection(pos) {
-            if (pos.pageX == null)
-                return;
-
-            setSelectionPos(selection.second, pos);
-            if (selectionIsSane()) {
-                selection.show = true;
-                plot.triggerRedrawOverlay();
-            }
-            else
-                clearSelection(true);
-        }
-
-        function clearSelection(preventEvent) {
-            if (selection.show) {
-                selection.show = false;
-                plot.triggerRedrawOverlay();
-                if (!preventEvent)
-                    plot.getPlaceholder().trigger("plotunselected", [ ]);
-            }
-        }
-
-        function setSelection(ranges, preventEvent) {
-            var axis, range, axes = plot.getAxes();
-            var o = plot.getOptions();
-
-            if (o.selection.mode == "y") {
-                selection.first.x = 0;
-                selection.second.x = plot.width();
-            }
-            else {
-                axis = ranges["xaxis"]? axes["xaxis"]: (ranges["x2axis"]? axes["x2axis"]: axes["xaxis"]);
-                range = ranges["xaxis"] || ranges["x2axis"] || { from:ranges["x1"], to:ranges["x2"] }
-                selection.first.x = axis.p2c(Math.min(range.from, range.to));
-                selection.second.x = axis.p2c(Math.max(range.from, range.to));
-            }
-
-            if (o.selection.mode == "x") {
-                selection.first.y = 0;
-                selection.second.y = plot.height();
-            }
-            else {
-                axis = ranges["yaxis"]? axes["yaxis"]: (ranges["y2axis"]? axes["y2axis"]: axes["yaxis"]);
-                range = ranges["yaxis"] || ranges["y2axis"] || { from:ranges["y1"], to:ranges["y2"] }
-                selection.first.y = axis.p2c(Math.min(range.from, range.to));
-                selection.second.y = axis.p2c(Math.max(range.from, range.to));
-            }
-
-            selection.show = true;
-            plot.triggerRedrawOverlay();
-            if (!preventEvent)
-                triggerSelectedEvent();
-        }
-
-        function selectionIsSane() {
-            var minSize = 5;
-            return Math.abs(selection.second.x - selection.first.x) >= minSize &&
-                Math.abs(selection.second.y - selection.first.y) >= minSize;
-        }
-
-        plot.clearSelection = clearSelection;
-        plot.setSelection = setSelection;
-        plot.getSelection = getSelection;
-
-        plot.hooks.bindEvents.push(function(plot, eventHolder) {
-            var o = plot.getOptions();
-            if (o.selection.mode != null)
-                eventHolder.mousemove(onMouseMove);
-
-            if (o.selection.mode != null)
-                eventHolder.mousedown(onMouseDown);
-        });
-
-
-        plot.hooks.drawOverlay.push(function (plot, ctx) {
-            // draw selection
-            if (selection.show && selectionIsSane()) {
-                var plotOffset = plot.getPlotOffset();
-                var o = plot.getOptions();
-
-                ctx.save();
-                ctx.translate(plotOffset.left, plotOffset.top);
-
-                var c = $.color.parse(o.selection.color);
-
-                ctx.strokeStyle = c.scale('a', 0.8).toString();
-                ctx.lineWidth = 1;
-                ctx.lineJoin = "round";
-                ctx.fillStyle = c.scale('a', 0.4).toString();
-
-                var x = Math.min(selection.first.x, selection.second.x),
-                    y = Math.min(selection.first.y, selection.second.y),
-                    w = Math.abs(selection.second.x - selection.first.x),
-                    h = Math.abs(selection.second.y - selection.first.y);
-
-                ctx.fillRect(x, y, w, h);
-                ctx.strokeRect(x, y, w, h);
-
-                ctx.restore();
-            }
-        });
-    }
-
-    $.plot.plugins.push({
-        init: init,
-        options: {
-            selection: {
-                mode: null, // one of null, "x", "y" or "xy"
-                color: "#e8cfac"
-            }
-        },
-        name: 'selection',
-        version: '1.0'
-    });
-})(jQuery);
diff --git a/lib/logstash/web/public/js/flot/jquery.flot.selection.min.js b/lib/logstash/web/public/js/flot/jquery.flot.selection.min.js
deleted file mode 100644
index 2260e8cdc21..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.flot.selection.min.js
+++ /dev/null
@@ -1 +0,0 @@
-(function(A){function B(J){var O={first:{x:-1,y:-1},second:{x:-1,y:-1},show:false,active:false};var L={};function D(Q){if(O.active){J.getPlaceholder().trigger("plotselecting",[F()]);K(Q)}}function M(Q){if(Q.which!=1){return }document.body.focus();if(document.onselectstart!==undefined&&L.onselectstart==null){L.onselectstart=document.onselectstart;document.onselectstart=function(){return false}}if(document.ondrag!==undefined&&L.ondrag==null){L.ondrag=document.ondrag;document.ondrag=function(){return false}}C(O.first,Q);O.active=true;A(document).one("mouseup",I)}function I(Q){if(document.onselectstart!==undefined){document.onselectstart=L.onselectstart}if(document.ondrag!==undefined){document.ondrag=L.ondrag}O.active=false;K(Q);if(E()){H()}else{J.getPlaceholder().trigger("plotunselected",[]);J.getPlaceholder().trigger("plotselecting",[null])}return false}function F(){if(!E()){return null}var R=Math.min(O.first.x,O.second.x),Q=Math.max(O.first.x,O.second.x),T=Math.max(O.first.y,O.second.y),S=Math.min(O.first.y,O.second.y);var U={};var V=J.getAxes();if(V.xaxis.used){U.xaxis={from:V.xaxis.c2p(R),to:V.xaxis.c2p(Q)}}if(V.x2axis.used){U.x2axis={from:V.x2axis.c2p(R),to:V.x2axis.c2p(Q)}}if(V.yaxis.used){U.yaxis={from:V.yaxis.c2p(T),to:V.yaxis.c2p(S)}}if(V.y2axis.used){U.y2axis={from:V.y2axis.c2p(T),to:V.y2axis.c2p(S)}}return U}function H(){var Q=F();J.getPlaceholder().trigger("plotselected",[Q]);var R=J.getAxes();if(R.xaxis.used&&R.yaxis.used){J.getPlaceholder().trigger("selected",[{x1:Q.xaxis.from,y1:Q.yaxis.from,x2:Q.xaxis.to,y2:Q.yaxis.to}])}}function G(R,S,Q){return S<R?R:(S>Q?Q:S)}function C(U,R){var T=J.getOptions();var S=J.getPlaceholder().offset();var Q=J.getPlotOffset();U.x=G(0,R.pageX-S.left-Q.left,J.width());U.y=G(0,R.pageY-S.top-Q.top,J.height());if(T.selection.mode=="y"){U.x=U==O.first?0:J.width()}if(T.selection.mode=="x"){U.y=U==O.first?0:J.height()}}function K(Q){if(Q.pageX==null){return }C(O.second,Q);if(E()){O.show=true;J.triggerRedrawOverlay()}else{P(true)}}function P(Q){if(O.show){O.show=false;J.triggerRedrawOverlay();if(!Q){J.getPlaceholder().trigger("plotunselected",[])}}}function N(R,Q){var T,S,U=J.getAxes();var V=J.getOptions();if(V.selection.mode=="y"){O.first.x=0;O.second.x=J.width()}else{T=R.xaxis?U.xaxis:(R.x2axis?U.x2axis:U.xaxis);S=R.xaxis||R.x2axis||{from:R.x1,to:R.x2};O.first.x=T.p2c(Math.min(S.from,S.to));O.second.x=T.p2c(Math.max(S.from,S.to))}if(V.selection.mode=="x"){O.first.y=0;O.second.y=J.height()}else{T=R.yaxis?U.yaxis:(R.y2axis?U.y2axis:U.yaxis);S=R.yaxis||R.y2axis||{from:R.y1,to:R.y2};O.first.y=T.p2c(Math.min(S.from,S.to));O.second.y=T.p2c(Math.max(S.from,S.to))}O.show=true;J.triggerRedrawOverlay();if(!Q){H()}}function E(){var Q=5;return Math.abs(O.second.x-O.first.x)>=Q&&Math.abs(O.second.y-O.first.y)>=Q}J.clearSelection=P;J.setSelection=N;J.getSelection=F;J.hooks.bindEvents.push(function(R,Q){var S=R.getOptions();if(S.selection.mode!=null){Q.mousemove(D)}if(S.selection.mode!=null){Q.mousedown(M)}});J.hooks.drawOverlay.push(function(T,Y){if(O.show&&E()){var R=T.getPlotOffset();var Q=T.getOptions();Y.save();Y.translate(R.left,R.top);var U=A.color.parse(Q.selection.color);Y.strokeStyle=U.scale("a",0.8).toString();Y.lineWidth=1;Y.lineJoin="round";Y.fillStyle=U.scale("a",0.4).toString();var W=Math.min(O.first.x,O.second.x),V=Math.min(O.first.y,O.second.y),X=Math.abs(O.second.x-O.first.x),S=Math.abs(O.second.y-O.first.y);Y.fillRect(W,V,X,S);Y.strokeRect(W,V,X,S);Y.restore()}})}A.plot.plugins.push({init:B,options:{selection:{mode:null,color:"#e8cfac"}},name:"selection",version:"1.0"})})(jQuery);
\ No newline at end of file
diff --git a/lib/logstash/web/public/js/flot/jquery.flot.stack.js b/lib/logstash/web/public/js/flot/jquery.flot.stack.js
deleted file mode 100644
index 4dbd29f1981..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.flot.stack.js
+++ /dev/null
@@ -1,152 +0,0 @@
-/*
-Flot plugin for stacking data sets, i.e. putting them on top of each
-other, for accumulative graphs. Note that the plugin assumes the data
-is sorted on x. Also note that stacking a mix of positive and negative
-values in most instances doesn't make sense (so it looks weird).
-
-Two or more series are stacked when their "stack" attribute is set to
-the same key (which can be any number or string or just "true"). To
-specify the default stack, you can set
-
-  series: {
-    stack: null or true or key (number/string)
-  }
-
-or specify it for a specific series
-
-  $.plot($("#placeholder"), [{ data: [ ... ], stack: true ])
-  
-The stacking order is determined by the order of the data series in
-the array (later series end up on top of the previous).
-
-Internally, the plugin modifies the datapoints in each series, adding
-an offset to the y value. For line series, extra data points are
-inserted through interpolation. For bar charts, the second y value is
-also adjusted.
-*/
-
-(function ($) {
-    var options = {
-        series: { stack: null } // or number/string
-    };
-    
-    function init(plot) {
-        function findMatchingSeries(s, allseries) {
-            var res = null
-            for (var i = 0; i < allseries.length; ++i) {
-                if (s == allseries[i])
-                    break;
-                
-                if (allseries[i].stack == s.stack)
-                    res = allseries[i];
-            }
-            
-            return res;
-        }
-        
-        function stackData(plot, s, datapoints) {
-            if (s.stack == null)
-                return;
-
-            var other = findMatchingSeries(s, plot.getData());
-            if (!other)
-                return;
-            
-            var ps = datapoints.pointsize,
-                points = datapoints.points,
-                otherps = other.datapoints.pointsize,
-                otherpoints = other.datapoints.points,
-                newpoints = [],
-                px, py, intery, qx, qy, bottom,
-                withlines = s.lines.show, withbars = s.bars.show,
-                withsteps = withlines && s.lines.steps,
-                i = 0, j = 0, l;
-
-            while (true) {
-                if (i >= points.length)
-                    break;
-
-                l = newpoints.length;
-
-                if (j >= otherpoints.length
-                    || otherpoints[j] == null
-                    || points[i] == null) {
-                    // degenerate cases
-                    for (m = 0; m < ps; ++m)
-                        newpoints.push(points[i + m]);
-                    i += ps;
-                }
-                else {
-                    // cases where we actually got two points
-                    px = points[i];
-                    py = points[i + 1];
-                    qx = otherpoints[j];
-                    qy = otherpoints[j + 1];
-                    bottom = 0;
-
-                    if (px == qx) {
-                        for (m = 0; m < ps; ++m)
-                            newpoints.push(points[i + m]);
-
-                        newpoints[l + 1] += qy;
-                        bottom = qy;
-                        
-                        i += ps;
-                        j += otherps;
-                    }
-                    else if (px > qx) {
-                        // we got past point below, might need to
-                        // insert interpolated extra point
-                        if (withlines && i > 0 && points[i - ps] != null) {
-                            intery = py + (points[i - ps + 1] - py) * (qx - px) / (points[i - ps] - px);
-                            newpoints.push(qx);
-                            newpoints.push(intery + qy)
-                            for (m = 2; m < ps; ++m)
-                                newpoints.push(points[i + m]);
-                            bottom = qy; 
-                        }
-
-                        j += otherps;
-                    }
-                    else {
-                        for (m = 0; m < ps; ++m)
-                            newpoints.push(points[i + m]);
-                        
-                        // we might be able to interpolate a point below,
-                        // this can give us a better y
-                        if (withlines && j > 0 && otherpoints[j - ps] != null)
-                            bottom = qy + (otherpoints[j - ps + 1] - qy) * (px - qx) / (otherpoints[j - ps] - qx);
-
-                        newpoints[l + 1] += bottom;
-                        
-                        i += ps;
-                    }
-                    
-                    if (l != newpoints.length && withbars)
-                        newpoints[l + 2] += bottom;
-                }
-
-                // maintain the line steps invariant
-                if (withsteps && l != newpoints.length && l > 0
-                    && newpoints[l] != null
-                    && newpoints[l] != newpoints[l - ps]
-                    && newpoints[l + 1] != newpoints[l - ps + 1]) {
-                    for (m = 0; m < ps; ++m)
-                        newpoints[l + ps + m] = newpoints[l + m];
-                    newpoints[l + 1] = newpoints[l - ps + 1];
-                }
-            }
-            
-            datapoints.points = newpoints;
-        }
-        
-        plot.hooks.processDatapoints.push(stackData);
-    }
-    
-    $.plot.plugins.push({
-        init: init,
-        options: options,
-        name: 'stack',
-        version: '1.0'
-    });
-})(jQuery);
diff --git a/lib/logstash/web/public/js/flot/jquery.flot.stack.min.js b/lib/logstash/web/public/js/flot/jquery.flot.stack.min.js
deleted file mode 100644
index b5b8943b2a5..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.flot.stack.min.js
+++ /dev/null
@@ -1 +0,0 @@
-(function(B){var A={series:{stack:null}};function C(F){function D(J,I){var H=null;for(var G=0;G<I.length;++G){if(J==I[G]){break}if(I[G].stack==J.stack){H=I[G]}}return H}function E(W,P,G){if(P.stack==null){return }var L=D(P,W.getData());if(!L){return }var T=G.pointsize,Y=G.points,H=L.datapoints.pointsize,S=L.datapoints.points,N=[],R,Q,I,a,Z,M,O=P.lines.show,K=P.bars.show,J=O&&P.lines.steps,X=0,V=0,U;while(true){if(X>=Y.length){break}U=N.length;if(V>=S.length||S[V]==null||Y[X]==null){for(m=0;m<T;++m){N.push(Y[X+m])}X+=T}else{R=Y[X];Q=Y[X+1];a=S[V];Z=S[V+1];M=0;if(R==a){for(m=0;m<T;++m){N.push(Y[X+m])}N[U+1]+=Z;M=Z;X+=T;V+=H}else{if(R>a){if(O&&X>0&&Y[X-T]!=null){I=Q+(Y[X-T+1]-Q)*(a-R)/(Y[X-T]-R);N.push(a);N.push(I+Z);for(m=2;m<T;++m){N.push(Y[X+m])}M=Z}V+=H}else{for(m=0;m<T;++m){N.push(Y[X+m])}if(O&&V>0&&S[V-T]!=null){M=Z+(S[V-T+1]-Z)*(R-a)/(S[V-T]-a)}N[U+1]+=M;X+=T}}if(U!=N.length&&K){N[U+2]+=M}}if(J&&U!=N.length&&U>0&&N[U]!=null&&N[U]!=N[U-T]&&N[U+1]!=N[U-T+1]){for(m=0;m<T;++m){N[U+T+m]=N[U+m]}N[U+1]=N[U-T+1]}}G.points=N}F.hooks.processDatapoints.push(E)}B.plot.plugins.push({init:C,options:A,name:"stack",version:"1.0"})})(jQuery);
\ No newline at end of file
diff --git a/lib/logstash/web/public/js/flot/jquery.flot.threshold.js b/lib/logstash/web/public/js/flot/jquery.flot.threshold.js
deleted file mode 100644
index 0b2e7ac82a7..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.flot.threshold.js
+++ /dev/null
@@ -1,103 +0,0 @@
-/*
-Flot plugin for thresholding data. Controlled through the option
-"threshold" in either the global series options
-
-  series: {
-    threshold: {
-      below: number
-      color: colorspec
-    }
-  }
-
-or in a specific series
-
-  $.plot($("#placeholder"), [{ data: [ ... ], threshold: { ... }}])
-
-The data points below "below" are drawn with the specified color. This
-makes it easy to mark points below 0, e.g. for budget data.
-
-Internally, the plugin works by splitting the data into two series,
-above and below the threshold. The extra series below the threshold
-will have its label cleared and the special "originSeries" attribute
-set to the original series. You may need to check for this in hover
-events.
-*/
-
-(function ($) {
-    var options = {
-        series: { threshold: null } // or { below: number, color: color spec}
-    };
-    
-    function init(plot) {
-        function thresholdData(plot, s, datapoints) {
-            if (!s.threshold)
-                return;
-            
-            var ps = datapoints.pointsize, i, x, y, p, prevp,
-                thresholded = $.extend({}, s); // note: shallow copy
-
-            thresholded.datapoints = { points: [], pointsize: ps };
-            thresholded.label = null;
-            thresholded.color = s.threshold.color;
-            thresholded.threshold = null;
-            thresholded.originSeries = s;
-            thresholded.data = [];
-
-            var below = s.threshold.below,
-                origpoints = datapoints.points,
-                addCrossingPoints = s.lines.show;
-
-            threspoints = [];
-            newpoints = [];
-
-            for (i = 0; i < origpoints.length; i += ps) {
-                x = origpoints[i]
-                y = origpoints[i + 1];
-
-                prevp = p;
-                if (y < below)
-                    p = threspoints;
-                else
-                    p = newpoints;
-
-                if (addCrossingPoints && prevp != p && x != null
-                    && i > 0 && origpoints[i - ps] != null) {
-                    var interx = (x - origpoints[i - ps]) / (y - origpoints[i - ps + 1]) * (below - y) + x;
-                    prevp.push(interx);
-                    prevp.push(below);
-                    for (m = 2; m < ps; ++m)
-                        prevp.push(origpoints[i + m]);
-                    
-                    p.push(null); // start new segment
-                    p.push(null);
-                    for (m = 2; m < ps; ++m)
-                        p.push(origpoints[i + m]);
-                    p.push(interx);
-                    p.push(below);
-                    for (m = 2; m < ps; ++m)
-                        p.push(origpoints[i + m]);
-                }
-
-                p.push(x);
-                p.push(y);
-            }
-
-            datapoints.points = newpoints;
-            thresholded.datapoints.points = threspoints;
-            
-            if (thresholded.datapoints.points.length > 0)
-                plot.getData().push(thresholded);
-                
-            // FIXME: there are probably some edge cases left in bars
-        }
-        
-        plot.hooks.processDatapoints.push(thresholdData);
-    }
-    
-    $.plot.plugins.push({
-        init: init,
-        options: options,
-        name: 'threshold',
-        version: '1.0'
-    });
-})(jQuery);
diff --git a/lib/logstash/web/public/js/flot/jquery.flot.threshold.min.js b/lib/logstash/web/public/js/flot/jquery.flot.threshold.min.js
deleted file mode 100644
index d8b79dfc93c..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.flot.threshold.min.js
+++ /dev/null
@@ -1 +0,0 @@
-(function(B){var A={series:{threshold:null}};function C(D){function E(L,S,M){if(!S.threshold){return }var F=M.pointsize,I,O,N,G,K,H=B.extend({},S);H.datapoints={points:[],pointsize:F};H.label=null;H.color=S.threshold.color;H.threshold=null;H.originSeries=S;H.data=[];var P=S.threshold.below,Q=M.points,R=S.lines.show;threspoints=[];newpoints=[];for(I=0;I<Q.length;I+=F){O=Q[I];N=Q[I+1];K=G;if(N<P){G=threspoints}else{G=newpoints}if(R&&K!=G&&O!=null&&I>0&&Q[I-F]!=null){var J=(O-Q[I-F])/(N-Q[I-F+1])*(P-N)+O;K.push(J);K.push(P);for(m=2;m<F;++m){K.push(Q[I+m])}G.push(null);G.push(null);for(m=2;m<F;++m){G.push(Q[I+m])}G.push(J);G.push(P);for(m=2;m<F;++m){G.push(Q[I+m])}}G.push(O);G.push(N)}M.points=newpoints;H.datapoints.points=threspoints;if(H.datapoints.points.length>0){L.getData().push(H)}}D.hooks.processDatapoints.push(E)}B.plot.plugins.push({init:C,options:A,name:"threshold",version:"1.0"})})(jQuery);
\ No newline at end of file
diff --git a/lib/logstash/web/public/js/flot/jquery.js b/lib/logstash/web/public/js/flot/jquery.js
deleted file mode 100644
index 926357433e3..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.js
+++ /dev/null
@@ -1,4376 +0,0 @@
-/*!
- * jQuery JavaScript Library v1.3.2
- * http://jquery.com/
- *
- * Copyright (c) 2009 John Resig
- * Dual licensed under the MIT and GPL licenses.
- * http://docs.jquery.com/License
- *
- * Date: 2009-02-19 17:34:21 -0500 (Thu, 19 Feb 2009)
- * Revision: 6246
- */
-(function(){
-
-var 
-	// Will speed up references to window, and allows munging its name.
-	window = this,
-	// Will speed up references to undefined, and allows munging its name.
-	undefined,
-	// Map over jQuery in case of overwrite
-	_jQuery = window.jQuery,
-	// Map over the $ in case of overwrite
-	_$ = window.$,
-
-	jQuery = window.jQuery = window.$ = function( selector, context ) {
-		// The jQuery object is actually just the init constructor 'enhanced'
-		return new jQuery.fn.init( selector, context );
-	},
-
-	// A simple way to check for HTML strings or ID strings
-	// (both of which we optimize for)
-	quickExpr = /^[^<]*(<(.|\s)+>)[^>]*$|^#([\w-]+)$/,
-	// Is it a simple selector
-	isSimple = /^.[^:#\[\.,]*$/;
-
-jQuery.fn = jQuery.prototype = {
-	init: function( selector, context ) {
-		// Make sure that a selection was provided
-		selector = selector || document;
-
-		// Handle $(DOMElement)
-		if ( selector.nodeType ) {
-			this[0] = selector;
-			this.length = 1;
-			this.context = selector;
-			return this;
-		}
-		// Handle HTML strings
-		if ( typeof selector === "string" ) {
-			// Are we dealing with HTML string or an ID?
-			var match = quickExpr.exec( selector );
-
-			// Verify a match, and that no context was specified for #id
-			if ( match && (match[1] || !context) ) {
-
-				// HANDLE: $(html) -> $(array)
-				if ( match[1] )
-					selector = jQuery.clean( [ match[1] ], context );
-
-				// HANDLE: $("#id")
-				else {
-					var elem = document.getElementById( match[3] );
-
-					// Handle the case where IE and Opera return items
-					// by name instead of ID
-					if ( elem && elem.id != match[3] )
-						return jQuery().find( selector );
-
-					// Otherwise, we inject the element directly into the jQuery object
-					var ret = jQuery( elem || [] );
-					ret.context = document;
-					ret.selector = selector;
-					return ret;
-				}
-
-			// HANDLE: $(expr, [context])
-			// (which is just equivalent to: $(content).find(expr)
-			} else
-				return jQuery( context ).find( selector );
-
-		// HANDLE: $(function)
-		// Shortcut for document ready
-		} else if ( jQuery.isFunction( selector ) )
-			return jQuery( document ).ready( selector );
-
-		// Make sure that old selector state is passed along
-		if ( selector.selector && selector.context ) {
-			this.selector = selector.selector;
-			this.context = selector.context;
-		}
-
-		return this.setArray(jQuery.isArray( selector ) ?
-			selector :
-			jQuery.makeArray(selector));
-	},
-
-	// Start with an empty selector
-	selector: "",
-
-	// The current version of jQuery being used
-	jquery: "1.3.2",
-
-	// The number of elements contained in the matched element set
-	size: function() {
-		return this.length;
-	},
-
-	// Get the Nth element in the matched element set OR
-	// Get the whole matched element set as a clean array
-	get: function( num ) {
-		return num === undefined ?
-
-			// Return a 'clean' array
-			Array.prototype.slice.call( this ) :
-
-			// Return just the object
-			this[ num ];
-	},
-
-	// Take an array of elements and push it onto the stack
-	// (returning the new matched element set)
-	pushStack: function( elems, name, selector ) {
-		// Build a new jQuery matched element set
-		var ret = jQuery( elems );
-
-		// Add the old object onto the stack (as a reference)
-		ret.prevObject = this;
-
-		ret.context = this.context;
-
-		if ( name === "find" )
-			ret.selector = this.selector + (this.selector ? " " : "") + selector;
-		else if ( name )
-			ret.selector = this.selector + "." + name + "(" + selector + ")";
-
-		// Return the newly-formed element set
-		return ret;
-	},
-
-	// Force the current matched set of elements to become
-	// the specified array of elements (destroying the stack in the process)
-	// You should use pushStack() in order to do this, but maintain the stack
-	setArray: function( elems ) {
-		// Resetting the length to 0, then using the native Array push
-		// is a super-fast way to populate an object with array-like properties
-		this.length = 0;
-		Array.prototype.push.apply( this, elems );
-
-		return this;
-	},
-
-	// Execute a callback for every element in the matched set.
-	// (You can seed the arguments with an array of args, but this is
-	// only used internally.)
-	each: function( callback, args ) {
-		return jQuery.each( this, callback, args );
-	},
-
-	// Determine the position of an element within
-	// the matched set of elements
-	index: function( elem ) {
-		// Locate the position of the desired element
-		return jQuery.inArray(
-			// If it receives a jQuery object, the first element is used
-			elem && elem.jquery ? elem[0] : elem
-		, this );
-	},
-
-	attr: function( name, value, type ) {
-		var options = name;
-
-		// Look for the case where we're accessing a style value
-		if ( typeof name === "string" )
-			if ( value === undefined )
-				return this[0] && jQuery[ type || "attr" ]( this[0], name );
-
-			else {
-				options = {};
-				options[ name ] = value;
-			}
-
-		// Check to see if we're setting style values
-		return this.each(function(i){
-			// Set all the styles
-			for ( name in options )
-				jQuery.attr(
-					type ?
-						this.style :
-						this,
-					name, jQuery.prop( this, options[ name ], type, i, name )
-				);
-		});
-	},
-
-	css: function( key, value ) {
-		// ignore negative width and height values
-		if ( (key == 'width' || key == 'height') && parseFloat(value) < 0 )
-			value = undefined;
-		return this.attr( key, value, "curCSS" );
-	},
-
-	text: function( text ) {
-		if ( typeof text !== "object" && text != null )
-			return this.empty().append( (this[0] && this[0].ownerDocument || document).createTextNode( text ) );
-
-		var ret = "";
-
-		jQuery.each( text || this, function(){
-			jQuery.each( this.childNodes, function(){
-				if ( this.nodeType != 8 )
-					ret += this.nodeType != 1 ?
-						this.nodeValue :
-						jQuery.fn.text( [ this ] );
-			});
-		});
-
-		return ret;
-	},
-
-	wrapAll: function( html ) {
-		if ( this[0] ) {
-			// The elements to wrap the target around
-			var wrap = jQuery( html, this[0].ownerDocument ).clone();
-
-			if ( this[0].parentNode )
-				wrap.insertBefore( this[0] );
-
-			wrap.map(function(){
-				var elem = this;
-
-				while ( elem.firstChild )
-					elem = elem.firstChild;
-
-				return elem;
-			}).append(this);
-		}
-
-		return this;
-	},
-
-	wrapInner: function( html ) {
-		return this.each(function(){
-			jQuery( this ).contents().wrapAll( html );
-		});
-	},
-
-	wrap: function( html ) {
-		return this.each(function(){
-			jQuery( this ).wrapAll( html );
-		});
-	},
-
-	append: function() {
-		return this.domManip(arguments, true, function(elem){
-			if (this.nodeType == 1)
-				this.appendChild( elem );
-		});
-	},
-
-	prepend: function() {
-		return this.domManip(arguments, true, function(elem){
-			if (this.nodeType == 1)
-				this.insertBefore( elem, this.firstChild );
-		});
-	},
-
-	before: function() {
-		return this.domManip(arguments, false, function(elem){
-			this.parentNode.insertBefore( elem, this );
-		});
-	},
-
-	after: function() {
-		return this.domManip(arguments, false, function(elem){
-			this.parentNode.insertBefore( elem, this.nextSibling );
-		});
-	},
-
-	end: function() {
-		return this.prevObject || jQuery( [] );
-	},
-
-	// For internal use only.
-	// Behaves like an Array's method, not like a jQuery method.
-	push: [].push,
-	sort: [].sort,
-	splice: [].splice,
-
-	find: function( selector ) {
-		if ( this.length === 1 ) {
-			var ret = this.pushStack( [], "find", selector );
-			ret.length = 0;
-			jQuery.find( selector, this[0], ret );
-			return ret;
-		} else {
-			return this.pushStack( jQuery.unique(jQuery.map(this, function(elem){
-				return jQuery.find( selector, elem );
-			})), "find", selector );
-		}
-	},
-
-	clone: function( events ) {
-		// Do the clone
-		var ret = this.map(function(){
-			if ( !jQuery.support.noCloneEvent && !jQuery.isXMLDoc(this) ) {
-				// IE copies events bound via attachEvent when
-				// using cloneNode. Calling detachEvent on the
-				// clone will also remove the events from the orignal
-				// In order to get around this, we use innerHTML.
-				// Unfortunately, this means some modifications to
-				// attributes in IE that are actually only stored
-				// as properties will not be copied (such as the
-				// the name attribute on an input).
-				var html = this.outerHTML;
-				if ( !html ) {
-					var div = this.ownerDocument.createElement("div");
-					div.appendChild( this.cloneNode(true) );
-					html = div.innerHTML;
-				}
-
-				return jQuery.clean([html.replace(/ jQuery\d+="(?:\d+|null)"/g, "").replace(/^\s*/, "")])[0];
-			} else
-				return this.cloneNode(true);
-		});
-
-		// Copy the events from the original to the clone
-		if ( events === true ) {
-			var orig = this.find("*").andSelf(), i = 0;
-
-			ret.find("*").andSelf().each(function(){
-				if ( this.nodeName !== orig[i].nodeName )
-					return;
-
-				var events = jQuery.data( orig[i], "events" );
-
-				for ( var type in events ) {
-					for ( var handler in events[ type ] ) {
-						jQuery.event.add( this, type, events[ type ][ handler ], events[ type ][ handler ].data );
-					}
-				}
-
-				i++;
-			});
-		}
-
-		// Return the cloned set
-		return ret;
-	},
-
-	filter: function( selector ) {
-		return this.pushStack(
-			jQuery.isFunction( selector ) &&
-			jQuery.grep(this, function(elem, i){
-				return selector.call( elem, i );
-			}) ||
-
-			jQuery.multiFilter( selector, jQuery.grep(this, function(elem){
-				return elem.nodeType === 1;
-			}) ), "filter", selector );
-	},
-
-	closest: function( selector ) {
-		var pos = jQuery.expr.match.POS.test( selector ) ? jQuery(selector) : null,
-			closer = 0;
-
-		return this.map(function(){
-			var cur = this;
-			while ( cur && cur.ownerDocument ) {
-				if ( pos ? pos.index(cur) > -1 : jQuery(cur).is(selector) ) {
-					jQuery.data(cur, "closest", closer);
-					return cur;
-				}
-				cur = cur.parentNode;
-				closer++;
-			}
-		});
-	},
-
-	not: function( selector ) {
-		if ( typeof selector === "string" )
-			// test special case where just one selector is passed in
-			if ( isSimple.test( selector ) )
-				return this.pushStack( jQuery.multiFilter( selector, this, true ), "not", selector );
-			else
-				selector = jQuery.multiFilter( selector, this );
-
-		var isArrayLike = selector.length && selector[selector.length - 1] !== undefined && !selector.nodeType;
-		return this.filter(function() {
-			return isArrayLike ? jQuery.inArray( this, selector ) < 0 : this != selector;
-		});
-	},
-
-	add: function( selector ) {
-		return this.pushStack( jQuery.unique( jQuery.merge(
-			this.get(),
-			typeof selector === "string" ?
-				jQuery( selector ) :
-				jQuery.makeArray( selector )
-		)));
-	},
-
-	is: function( selector ) {
-		return !!selector && jQuery.multiFilter( selector, this ).length > 0;
-	},
-
-	hasClass: function( selector ) {
-		return !!selector && this.is( "." + selector );
-	},
-
-	val: function( value ) {
-		if ( value === undefined ) {			
-			var elem = this[0];
-
-			if ( elem ) {
-				if( jQuery.nodeName( elem, 'option' ) )
-					return (elem.attributes.value || {}).specified ? elem.value : elem.text;
-				
-				// We need to handle select boxes special
-				if ( jQuery.nodeName( elem, "select" ) ) {
-					var index = elem.selectedIndex,
-						values = [],
-						options = elem.options,
-						one = elem.type == "select-one";
-
-					// Nothing was selected
-					if ( index < 0 )
-						return null;
-
-					// Loop through all the selected options
-					for ( var i = one ? index : 0, max = one ? index + 1 : options.length; i < max; i++ ) {
-						var option = options[ i ];
-
-						if ( option.selected ) {
-							// Get the specifc value for the option
-							value = jQuery(option).val();
-
-							// We don't need an array for one selects
-							if ( one )
-								return value;
-
-							// Multi-Selects return an array
-							values.push( value );
-						}
-					}
-
-					return values;				
-				}
-
-				// Everything else, we just grab the value
-				return (elem.value || "").replace(/\r/g, "");
-
-			}
-
-			return undefined;
-		}
-
-		if ( typeof value === "number" )
-			value += '';
-
-		return this.each(function(){
-			if ( this.nodeType != 1 )
-				return;
-
-			if ( jQuery.isArray(value) && /radio|checkbox/.test( this.type ) )
-				this.checked = (jQuery.inArray(this.value, value) >= 0 ||
-					jQuery.inArray(this.name, value) >= 0);
-
-			else if ( jQuery.nodeName( this, "select" ) ) {
-				var values = jQuery.makeArray(value);
-
-				jQuery( "option", this ).each(function(){
-					this.selected = (jQuery.inArray( this.value, values ) >= 0 ||
-						jQuery.inArray( this.text, values ) >= 0);
-				});
-
-				if ( !values.length )
-					this.selectedIndex = -1;
-
-			} else
-				this.value = value;
-		});
-	},
-
-	html: function( value ) {
-		return value === undefined ?
-			(this[0] ?
-				this[0].innerHTML.replace(/ jQuery\d+="(?:\d+|null)"/g, "") :
-				null) :
-			this.empty().append( value );
-	},
-
-	replaceWith: function( value ) {
-		return this.after( value ).remove();
-	},
-
-	eq: function( i ) {
-		return this.slice( i, +i + 1 );
-	},
-
-	slice: function() {
-		return this.pushStack( Array.prototype.slice.apply( this, arguments ),
-			"slice", Array.prototype.slice.call(arguments).join(",") );
-	},
-
-	map: function( callback ) {
-		return this.pushStack( jQuery.map(this, function(elem, i){
-			return callback.call( elem, i, elem );
-		}));
-	},
-
-	andSelf: function() {
-		return this.add( this.prevObject );
-	},
-
-	domManip: function( args, table, callback ) {
-		if ( this[0] ) {
-			var fragment = (this[0].ownerDocument || this[0]).createDocumentFragment(),
-				scripts = jQuery.clean( args, (this[0].ownerDocument || this[0]), fragment ),
-				first = fragment.firstChild;
-
-			if ( first )
-				for ( var i = 0, l = this.length; i < l; i++ )
-					callback.call( root(this[i], first), this.length > 1 || i > 0 ?
-							fragment.cloneNode(true) : fragment );
-		
-			if ( scripts )
-				jQuery.each( scripts, evalScript );
-		}
-
-		return this;
-		
-		function root( elem, cur ) {
-			return table && jQuery.nodeName(elem, "table") && jQuery.nodeName(cur, "tr") ?
-				(elem.getElementsByTagName("tbody")[0] ||
-				elem.appendChild(elem.ownerDocument.createElement("tbody"))) :
-				elem;
-		}
-	}
-};
-
-// Give the init function the jQuery prototype for later instantiation
-jQuery.fn.init.prototype = jQuery.fn;
-
-function evalScript( i, elem ) {
-	if ( elem.src )
-		jQuery.ajax({
-			url: elem.src,
-			async: false,
-			dataType: "script"
-		});
-
-	else
-		jQuery.globalEval( elem.text || elem.textContent || elem.innerHTML || "" );
-
-	if ( elem.parentNode )
-		elem.parentNode.removeChild( elem );
-}
-
-function now(){
-	return +new Date;
-}
-
-jQuery.extend = jQuery.fn.extend = function() {
-	// copy reference to target object
-	var target = arguments[0] || {}, i = 1, length = arguments.length, deep = false, options;
-
-	// Handle a deep copy situation
-	if ( typeof target === "boolean" ) {
-		deep = target;
-		target = arguments[1] || {};
-		// skip the boolean and the target
-		i = 2;
-	}
-
-	// Handle case when target is a string or something (possible in deep copy)
-	if ( typeof target !== "object" && !jQuery.isFunction(target) )
-		target = {};
-
-	// extend jQuery itself if only one argument is passed
-	if ( length == i ) {
-		target = this;
-		--i;
-	}
-
-	for ( ; i < length; i++ )
-		// Only deal with non-null/undefined values
-		if ( (options = arguments[ i ]) != null )
-			// Extend the base object
-			for ( var name in options ) {
-				var src = target[ name ], copy = options[ name ];
-
-				// Prevent never-ending loop
-				if ( target === copy )
-					continue;
-
-				// Recurse if we're merging object values
-				if ( deep && copy && typeof copy === "object" && !copy.nodeType )
-					target[ name ] = jQuery.extend( deep, 
-						// Never move original objects, clone them
-						src || ( copy.length != null ? [ ] : { } )
-					, copy );
-
-				// Don't bring in undefined values
-				else if ( copy !== undefined )
-					target[ name ] = copy;
-
-			}
-
-	// Return the modified object
-	return target;
-};
-
-// exclude the following css properties to add px
-var	exclude = /z-?index|font-?weight|opacity|zoom|line-?height/i,
-	// cache defaultView
-	defaultView = document.defaultView || {},
-	toString = Object.prototype.toString;
-
-jQuery.extend({
-	noConflict: function( deep ) {
-		window.$ = _$;
-
-		if ( deep )
-			window.jQuery = _jQuery;
-
-		return jQuery;
-	},
-
-	// See test/unit/core.js for details concerning isFunction.
-	// Since version 1.3, DOM methods and functions like alert
-	// aren't supported. They return false on IE (#2968).
-	isFunction: function( obj ) {
-		return toString.call(obj) === "[object Function]";
-	},
-
-	isArray: function( obj ) {
-		return toString.call(obj) === "[object Array]";
-	},
-
-	// check if an element is in a (or is an) XML document
-	isXMLDoc: function( elem ) {
-		return elem.nodeType === 9 && elem.documentElement.nodeName !== "HTML" ||
-			!!elem.ownerDocument && jQuery.isXMLDoc( elem.ownerDocument );
-	},
-
-	// Evalulates a script in a global context
-	globalEval: function( data ) {
-		if ( data && /\S/.test(data) ) {
-			// Inspired by code by Andrea Giammarchi
-			// http://webreflection.blogspot.com/2007/08/global-scope-evaluation-and-dom.html
-			var head = document.getElementsByTagName("head")[0] || document.documentElement,
-				script = document.createElement("script");
-
-			script.type = "text/javascript";
-			if ( jQuery.support.scriptEval )
-				script.appendChild( document.createTextNode( data ) );
-			else
-				script.text = data;
-
-			// Use insertBefore instead of appendChild  to circumvent an IE6 bug.
-			// This arises when a base node is used (#2709).
-			head.insertBefore( script, head.firstChild );
-			head.removeChild( script );
-		}
-	},
-
-	nodeName: function( elem, name ) {
-		return elem.nodeName && elem.nodeName.toUpperCase() == name.toUpperCase();
-	},
-
-	// args is for internal usage only
-	each: function( object, callback, args ) {
-		var name, i = 0, length = object.length;
-
-		if ( args ) {
-			if ( length === undefined ) {
-				for ( name in object )
-					if ( callback.apply( object[ name ], args ) === false )
-						break;
-			} else
-				for ( ; i < length; )
-					if ( callback.apply( object[ i++ ], args ) === false )
-						break;
-
-		// A special, fast, case for the most common use of each
-		} else {
-			if ( length === undefined ) {
-				for ( name in object )
-					if ( callback.call( object[ name ], name, object[ name ] ) === false )
-						break;
-			} else
-				for ( var value = object[0];
-					i < length && callback.call( value, i, value ) !== false; value = object[++i] ){}
-		}
-
-		return object;
-	},
-
-	prop: function( elem, value, type, i, name ) {
-		// Handle executable functions
-		if ( jQuery.isFunction( value ) )
-			value = value.call( elem, i );
-
-		// Handle passing in a number to a CSS property
-		return typeof value === "number" && type == "curCSS" && !exclude.test( name ) ?
-			value + "px" :
-			value;
-	},
-
-	className: {
-		// internal only, use addClass("class")
-		add: function( elem, classNames ) {
-			jQuery.each((classNames || "").split(/\s+/), function(i, className){
-				if ( elem.nodeType == 1 && !jQuery.className.has( elem.className, className ) )
-					elem.className += (elem.className ? " " : "") + className;
-			});
-		},
-
-		// internal only, use removeClass("class")
-		remove: function( elem, classNames ) {
-			if (elem.nodeType == 1)
-				elem.className = classNames !== undefined ?
-					jQuery.grep(elem.className.split(/\s+/), function(className){
-						return !jQuery.className.has( classNames, className );
-					}).join(" ") :
-					"";
-		},
-
-		// internal only, use hasClass("class")
-		has: function( elem, className ) {
-			return elem && jQuery.inArray( className, (elem.className || elem).toString().split(/\s+/) ) > -1;
-		}
-	},
-
-	// A method for quickly swapping in/out CSS properties to get correct calculations
-	swap: function( elem, options, callback ) {
-		var old = {};
-		// Remember the old values, and insert the new ones
-		for ( var name in options ) {
-			old[ name ] = elem.style[ name ];
-			elem.style[ name ] = options[ name ];
-		}
-
-		callback.call( elem );
-
-		// Revert the old values
-		for ( var name in options )
-			elem.style[ name ] = old[ name ];
-	},
-
-	css: function( elem, name, force, extra ) {
-		if ( name == "width" || name == "height" ) {
-			var val, props = { position: "absolute", visibility: "hidden", display:"block" }, which = name == "width" ? [ "Left", "Right" ] : [ "Top", "Bottom" ];
-
-			function getWH() {
-				val = name == "width" ? elem.offsetWidth : elem.offsetHeight;
-
-				if ( extra === "border" )
-					return;
-
-				jQuery.each( which, function() {
-					if ( !extra )
-						val -= parseFloat(jQuery.curCSS( elem, "padding" + this, true)) || 0;
-					if ( extra === "margin" )
-						val += parseFloat(jQuery.curCSS( elem, "margin" + this, true)) || 0;
-					else
-						val -= parseFloat(jQuery.curCSS( elem, "border" + this + "Width", true)) || 0;
-				});
-			}
-
-			if ( elem.offsetWidth !== 0 )
-				getWH();
-			else
-				jQuery.swap( elem, props, getWH );
-
-			return Math.max(0, Math.round(val));
-		}
-
-		return jQuery.curCSS( elem, name, force );
-	},
-
-	curCSS: function( elem, name, force ) {
-		var ret, style = elem.style;
-
-		// We need to handle opacity special in IE
-		if ( name == "opacity" && !jQuery.support.opacity ) {
-			ret = jQuery.attr( style, "opacity" );
-
-			return ret == "" ?
-				"1" :
-				ret;
-		}
-
-		// Make sure we're using the right name for getting the float value
-		if ( name.match( /float/i ) )
-			name = styleFloat;
-
-		if ( !force && style && style[ name ] )
-			ret = style[ name ];
-
-		else if ( defaultView.getComputedStyle ) {
-
-			// Only "float" is needed here
-			if ( name.match( /float/i ) )
-				name = "float";
-
-			name = name.replace( /([A-Z])/g, "-$1" ).toLowerCase();
-
-			var computedStyle = defaultView.getComputedStyle( elem, null );
-
-			if ( computedStyle )
-				ret = computedStyle.getPropertyValue( name );
-
-			// We should always get a number back from opacity
-			if ( name == "opacity" && ret == "" )
-				ret = "1";
-
-		} else if ( elem.currentStyle ) {
-			var camelCase = name.replace(/\-(\w)/g, function(all, letter){
-				return letter.toUpperCase();
-			});
-
-			ret = elem.currentStyle[ name ] || elem.currentStyle[ camelCase ];
-
-			// From the awesome hack by Dean Edwards
-			// http://erik.eae.net/archives/2007/07/27/18.54.15/#comment-102291
-
-			// If we're not dealing with a regular pixel number
-			// but a number that has a weird ending, we need to convert it to pixels
-			if ( !/^\d+(px)?$/i.test( ret ) && /^\d/.test( ret ) ) {
-				// Remember the original values
-				var left = style.left, rsLeft = elem.runtimeStyle.left;
-
-				// Put in the new values to get a computed value out
-				elem.runtimeStyle.left = elem.currentStyle.left;
-				style.left = ret || 0;
-				ret = style.pixelLeft + "px";
-
-				// Revert the changed values
-				style.left = left;
-				elem.runtimeStyle.left = rsLeft;
-			}
-		}
-
-		return ret;
-	},
-
-	clean: function( elems, context, fragment ) {
-		context = context || document;
-
-		// !context.createElement fails in IE with an error but returns typeof 'object'
-		if ( typeof context.createElement === "undefined" )
-			context = context.ownerDocument || context[0] && context[0].ownerDocument || document;
-
-		// If a single string is passed in and it's a single tag
-		// just do a createElement and skip the rest
-		if ( !fragment && elems.length === 1 && typeof elems[0] === "string" ) {
-			var match = /^<(\w+)\s*\/?>$/.exec(elems[0]);
-			if ( match )
-				return [ context.createElement( match[1] ) ];
-		}
-
-		var ret = [], scripts = [], div = context.createElement("div");
-
-		jQuery.each(elems, function(i, elem){
-			if ( typeof elem === "number" )
-				elem += '';
-
-			if ( !elem )
-				return;
-
-			// Convert html string into DOM nodes
-			if ( typeof elem === "string" ) {
-				// Fix "XHTML"-style tags in all browsers
-				elem = elem.replace(/(<(\w+)[^>]*?)\/>/g, function(all, front, tag){
-					return tag.match(/^(abbr|br|col|img|input|link|meta|param|hr|area|embed)$/i) ?
-						all :
-						front + "></" + tag + ">";
-				});
-
-				// Trim whitespace, otherwise indexOf won't work as expected
-				var tags = elem.replace(/^\s+/, "").substring(0, 10).toLowerCase();
-
-				var wrap =
-					// option or optgroup
-					!tags.indexOf("<opt") &&
-					[ 1, "<select multiple='multiple'>", "</select>" ] ||
-
-					!tags.indexOf("<leg") &&
-					[ 1, "<fieldset>", "</fieldset>" ] ||
-
-					tags.match(/^<(thead|tbody|tfoot|colg|cap)/) &&
-					[ 1, "<table>", "</table>" ] ||
-
-					!tags.indexOf("<tr") &&
-					[ 2, "<table><tbody>", "</tbody></table>" ] ||
-
-				 	// <thead> matched above
-					(!tags.indexOf("<td") || !tags.indexOf("<th")) &&
-					[ 3, "<table><tbody><tr>", "</tr></tbody></table>" ] ||
-
-					!tags.indexOf("<col") &&
-					[ 2, "<table><tbody></tbody><colgroup>", "</colgroup></table>" ] ||
-
-					// IE can't serialize <link> and <script> tags normally
-					!jQuery.support.htmlSerialize &&
-					[ 1, "div<div>", "</div>" ] ||
-
-					[ 0, "", "" ];
-
-				// Go to html and back, then peel off extra wrappers
-				div.innerHTML = wrap[1] + elem + wrap[2];
-
-				// Move to the right depth
-				while ( wrap[0]-- )
-					div = div.lastChild;
-
-				// Remove IE's autoinserted <tbody> from table fragments
-				if ( !jQuery.support.tbody ) {
-
-					// String was a <table>, *may* have spurious <tbody>
-					var hasBody = /<tbody/i.test(elem),
-						tbody = !tags.indexOf("<table") && !hasBody ?
-							div.firstChild && div.firstChild.childNodes :
-
-						// String was a bare <thead> or <tfoot>
-						wrap[1] == "<table>" && !hasBody ?
-							div.childNodes :
-							[];
-
-					for ( var j = tbody.length - 1; j >= 0 ; --j )
-						if ( jQuery.nodeName( tbody[ j ], "tbody" ) && !tbody[ j ].childNodes.length )
-							tbody[ j ].parentNode.removeChild( tbody[ j ] );
-
-					}
-
-				// IE completely kills leading whitespace when innerHTML is used
-				if ( !jQuery.support.leadingWhitespace && /^\s/.test( elem ) )
-					div.insertBefore( context.createTextNode( elem.match(/^\s*/)[0] ), div.firstChild );
-				
-				elem = jQuery.makeArray( div.childNodes );
-			}
-
-			if ( elem.nodeType )
-				ret.push( elem );
-			else
-				ret = jQuery.merge( ret, elem );
-
-		});
-
-		if ( fragment ) {
-			for ( var i = 0; ret[i]; i++ ) {
-				if ( jQuery.nodeName( ret[i], "script" ) && (!ret[i].type || ret[i].type.toLowerCase() === "text/javascript") ) {
-					scripts.push( ret[i].parentNode ? ret[i].parentNode.removeChild( ret[i] ) : ret[i] );
-				} else {
-					if ( ret[i].nodeType === 1 )
-						ret.splice.apply( ret, [i + 1, 0].concat(jQuery.makeArray(ret[i].getElementsByTagName("script"))) );
-					fragment.appendChild( ret[i] );
-				}
-			}
-			
-			return scripts;
-		}
-
-		return ret;
-	},
-
-	attr: function( elem, name, value ) {
-		// don't set attributes on text and comment nodes
-		if (!elem || elem.nodeType == 3 || elem.nodeType == 8)
-			return undefined;
-
-		var notxml = !jQuery.isXMLDoc( elem ),
-			// Whether we are setting (or getting)
-			set = value !== undefined;
-
-		// Try to normalize/fix the name
-		name = notxml && jQuery.props[ name ] || name;
-
-		// Only do all the following if this is a node (faster for style)
-		// IE elem.getAttribute passes even for style
-		if ( elem.tagName ) {
-
-			// These attributes require special treatment
-			var special = /href|src|style/.test( name );
-
-			// Safari mis-reports the default selected property of a hidden option
-			// Accessing the parent's selectedIndex property fixes it
-			if ( name == "selected" && elem.parentNode )
-				elem.parentNode.selectedIndex;
-
-			// If applicable, access the attribute via the DOM 0 way
-			if ( name in elem && notxml && !special ) {
-				if ( set ){
-					// We can't allow the type property to be changed (since it causes problems in IE)
-					if ( name == "type" && jQuery.nodeName( elem, "input" ) && elem.parentNode )
-						throw "type property can't be changed";
-
-					elem[ name ] = value;
-				}
-
-				// browsers index elements by id/name on forms, give priority to attributes.
-				if( jQuery.nodeName( elem, "form" ) && elem.getAttributeNode(name) )
-					return elem.getAttributeNode( name ).nodeValue;
-
-				// elem.tabIndex doesn't always return the correct value when it hasn't been explicitly set
-				// http://fluidproject.org/blog/2008/01/09/getting-setting-and-removing-tabindex-values-with-javascript/
-				if ( name == "tabIndex" ) {
-					var attributeNode = elem.getAttributeNode( "tabIndex" );
-					return attributeNode && attributeNode.specified
-						? attributeNode.value
-						: elem.nodeName.match(/(button|input|object|select|textarea)/i)
-							? 0
-							: elem.nodeName.match(/^(a|area)$/i) && elem.href
-								? 0
-								: undefined;
-				}
-
-				return elem[ name ];
-			}
-
-			if ( !jQuery.support.style && notxml &&  name == "style" )
-				return jQuery.attr( elem.style, "cssText", value );
-
-			if ( set )
-				// convert the value to a string (all browsers do this but IE) see #1070
-				elem.setAttribute( name, "" + value );
-
-			var attr = !jQuery.support.hrefNormalized && notxml && special
-					// Some attributes require a special call on IE
-					? elem.getAttribute( name, 2 )
-					: elem.getAttribute( name );
-
-			// Non-existent attributes return null, we normalize to undefined
-			return attr === null ? undefined : attr;
-		}
-
-		// elem is actually elem.style ... set the style
-
-		// IE uses filters for opacity
-		if ( !jQuery.support.opacity && name == "opacity" ) {
-			if ( set ) {
-				// IE has trouble with opacity if it does not have layout
-				// Force it by setting the zoom level
-				elem.zoom = 1;
-
-				// Set the alpha filter to set the opacity
-				elem.filter = (elem.filter || "").replace( /alpha\([^)]*\)/, "" ) +
-					(parseInt( value ) + '' == "NaN" ? "" : "alpha(opacity=" + value * 100 + ")");
-			}
-
-			return elem.filter && elem.filter.indexOf("opacity=") >= 0 ?
-				(parseFloat( elem.filter.match(/opacity=([^)]*)/)[1] ) / 100) + '':
-				"";
-		}
-
-		name = name.replace(/-([a-z])/ig, function(all, letter){
-			return letter.toUpperCase();
-		});
-
-		if ( set )
-			elem[ name ] = value;
-
-		return elem[ name ];
-	},
-
-	trim: function( text ) {
-		return (text || "").replace( /^\s+|\s+$/g, "" );
-	},
-
-	makeArray: function( array ) {
-		var ret = [];
-
-		if( array != null ){
-			var i = array.length;
-			// The window, strings (and functions) also have 'length'
-			if( i == null || typeof array === "string" || jQuery.isFunction(array) || array.setInterval )
-				ret[0] = array;
-			else
-				while( i )
-					ret[--i] = array[i];
-		}
-
-		return ret;
-	},
-
-	inArray: function( elem, array ) {
-		for ( var i = 0, length = array.length; i < length; i++ )
-		// Use === because on IE, window == document
-			if ( array[ i ] === elem )
-				return i;
-
-		return -1;
-	},
-
-	merge: function( first, second ) {
-		// We have to loop this way because IE & Opera overwrite the length
-		// expando of getElementsByTagName
-		var i = 0, elem, pos = first.length;
-		// Also, we need to make sure that the correct elements are being returned
-		// (IE returns comment nodes in a '*' query)
-		if ( !jQuery.support.getAll ) {
-			while ( (elem = second[ i++ ]) != null )
-				if ( elem.nodeType != 8 )
-					first[ pos++ ] = elem;
-
-		} else
-			while ( (elem = second[ i++ ]) != null )
-				first[ pos++ ] = elem;
-
-		return first;
-	},
-
-	unique: function( array ) {
-		var ret = [], done = {};
-
-		try {
-
-			for ( var i = 0, length = array.length; i < length; i++ ) {
-				var id = jQuery.data( array[ i ] );
-
-				if ( !done[ id ] ) {
-					done[ id ] = true;
-					ret.push( array[ i ] );
-				}
-			}
-
-		} catch( e ) {
-			ret = array;
-		}
-
-		return ret;
-	},
-
-	grep: function( elems, callback, inv ) {
-		var ret = [];
-
-		// Go through the array, only saving the items
-		// that pass the validator function
-		for ( var i = 0, length = elems.length; i < length; i++ )
-			if ( !inv != !callback( elems[ i ], i ) )
-				ret.push( elems[ i ] );
-
-		return ret;
-	},
-
-	map: function( elems, callback ) {
-		var ret = [];
-
-		// Go through the array, translating each of the items to their
-		// new value (or values).
-		for ( var i = 0, length = elems.length; i < length; i++ ) {
-			var value = callback( elems[ i ], i );
-
-			if ( value != null )
-				ret[ ret.length ] = value;
-		}
-
-		return ret.concat.apply( [], ret );
-	}
-});
-
-// Use of jQuery.browser is deprecated.
-// It's included for backwards compatibility and plugins,
-// although they should work to migrate away.
-
-var userAgent = navigator.userAgent.toLowerCase();
-
-// Figure out what browser is being used
-jQuery.browser = {
-	version: (userAgent.match( /.+(?:rv|it|ra|ie)[\/: ]([\d.]+)/ ) || [0,'0'])[1],
-	safari: /webkit/.test( userAgent ),
-	opera: /opera/.test( userAgent ),
-	msie: /msie/.test( userAgent ) && !/opera/.test( userAgent ),
-	mozilla: /mozilla/.test( userAgent ) && !/(compatible|webkit)/.test( userAgent )
-};
-
-jQuery.each({
-	parent: function(elem){return elem.parentNode;},
-	parents: function(elem){return jQuery.dir(elem,"parentNode");},
-	next: function(elem){return jQuery.nth(elem,2,"nextSibling");},
-	prev: function(elem){return jQuery.nth(elem,2,"previousSibling");},
-	nextAll: function(elem){return jQuery.dir(elem,"nextSibling");},
-	prevAll: function(elem){return jQuery.dir(elem,"previousSibling");},
-	siblings: function(elem){return jQuery.sibling(elem.parentNode.firstChild,elem);},
-	children: function(elem){return jQuery.sibling(elem.firstChild);},
-	contents: function(elem){return jQuery.nodeName(elem,"iframe")?elem.contentDocument||elem.contentWindow.document:jQuery.makeArray(elem.childNodes);}
-}, function(name, fn){
-	jQuery.fn[ name ] = function( selector ) {
-		var ret = jQuery.map( this, fn );
-
-		if ( selector && typeof selector == "string" )
-			ret = jQuery.multiFilter( selector, ret );
-
-		return this.pushStack( jQuery.unique( ret ), name, selector );
-	};
-});
-
-jQuery.each({
-	appendTo: "append",
-	prependTo: "prepend",
-	insertBefore: "before",
-	insertAfter: "after",
-	replaceAll: "replaceWith"
-}, function(name, original){
-	jQuery.fn[ name ] = function( selector ) {
-		var ret = [], insert = jQuery( selector );
-
-		for ( var i = 0, l = insert.length; i < l; i++ ) {
-			var elems = (i > 0 ? this.clone(true) : this).get();
-			jQuery.fn[ original ].apply( jQuery(insert[i]), elems );
-			ret = ret.concat( elems );
-		}
-
-		return this.pushStack( ret, name, selector );
-	};
-});
-
-jQuery.each({
-	removeAttr: function( name ) {
-		jQuery.attr( this, name, "" );
-		if (this.nodeType == 1)
-			this.removeAttribute( name );
-	},
-
-	addClass: function( classNames ) {
-		jQuery.className.add( this, classNames );
-	},
-
-	removeClass: function( classNames ) {
-		jQuery.className.remove( this, classNames );
-	},
-
-	toggleClass: function( classNames, state ) {
-		if( typeof state !== "boolean" )
-			state = !jQuery.className.has( this, classNames );
-		jQuery.className[ state ? "add" : "remove" ]( this, classNames );
-	},
-
-	remove: function( selector ) {
-		if ( !selector || jQuery.filter( selector, [ this ] ).length ) {
-			// Prevent memory leaks
-			jQuery( "*", this ).add([this]).each(function(){
-				jQuery.event.remove(this);
-				jQuery.removeData(this);
-			});
-			if (this.parentNode)
-				this.parentNode.removeChild( this );
-		}
-	},
-
-	empty: function() {
-		// Remove element nodes and prevent memory leaks
-		jQuery(this).children().remove();
-
-		// Remove any remaining nodes
-		while ( this.firstChild )
-			this.removeChild( this.firstChild );
-	}
-}, function(name, fn){
-	jQuery.fn[ name ] = function(){
-		return this.each( fn, arguments );
-	};
-});
-
-// Helper function used by the dimensions and offset modules
-function num(elem, prop) {
-	return elem[0] && parseInt( jQuery.curCSS(elem[0], prop, true), 10 ) || 0;
-}
-var expando = "jQuery" + now(), uuid = 0, windowData = {};
-
-jQuery.extend({
-	cache: {},
-
-	data: function( elem, name, data ) {
-		elem = elem == window ?
-			windowData :
-			elem;
-
-		var id = elem[ expando ];
-
-		// Compute a unique ID for the element
-		if ( !id )
-			id = elem[ expando ] = ++uuid;
-
-		// Only generate the data cache if we're
-		// trying to access or manipulate it
-		if ( name && !jQuery.cache[ id ] )
-			jQuery.cache[ id ] = {};
-
-		// Prevent overriding the named cache with undefined values
-		if ( data !== undefined )
-			jQuery.cache[ id ][ name ] = data;
-
-		// Return the named cache data, or the ID for the element
-		return name ?
-			jQuery.cache[ id ][ name ] :
-			id;
-	},
-
-	removeData: function( elem, name ) {
-		elem = elem == window ?
-			windowData :
-			elem;
-
-		var id = elem[ expando ];
-
-		// If we want to remove a specific section of the element's data
-		if ( name ) {
-			if ( jQuery.cache[ id ] ) {
-				// Remove the section of cache data
-				delete jQuery.cache[ id ][ name ];
-
-				// If we've removed all the data, remove the element's cache
-				name = "";
-
-				for ( name in jQuery.cache[ id ] )
-					break;
-
-				if ( !name )
-					jQuery.removeData( elem );
-			}
-
-		// Otherwise, we want to remove all of the element's data
-		} else {
-			// Clean up the element expando
-			try {
-				delete elem[ expando ];
-			} catch(e){
-				// IE has trouble directly removing the expando
-				// but it's ok with using removeAttribute
-				if ( elem.removeAttribute )
-					elem.removeAttribute( expando );
-			}
-
-			// Completely remove the data cache
-			delete jQuery.cache[ id ];
-		}
-	},
-	queue: function( elem, type, data ) {
-		if ( elem ){
-	
-			type = (type || "fx") + "queue";
-	
-			var q = jQuery.data( elem, type );
-	
-			if ( !q || jQuery.isArray(data) )
-				q = jQuery.data( elem, type, jQuery.makeArray(data) );
-			else if( data )
-				q.push( data );
-	
-		}
-		return q;
-	},
-
-	dequeue: function( elem, type ){
-		var queue = jQuery.queue( elem, type ),
-			fn = queue.shift();
-		
-		if( !type || type === "fx" )
-			fn = queue[0];
-			
-		if( fn !== undefined )
-			fn.call(elem);
-	}
-});
-
-jQuery.fn.extend({
-	data: function( key, value ){
-		var parts = key.split(".");
-		parts[1] = parts[1] ? "." + parts[1] : "";
-
-		if ( value === undefined ) {
-			var data = this.triggerHandler("getData" + parts[1] + "!", [parts[0]]);
-
-			if ( data === undefined && this.length )
-				data = jQuery.data( this[0], key );
-
-			return data === undefined && parts[1] ?
-				this.data( parts[0] ) :
-				data;
-		} else
-			return this.trigger("setData" + parts[1] + "!", [parts[0], value]).each(function(){
-				jQuery.data( this, key, value );
-			});
-	},
-
-	removeData: function( key ){
-		return this.each(function(){
-			jQuery.removeData( this, key );
-		});
-	},
-	queue: function(type, data){
-		if ( typeof type !== "string" ) {
-			data = type;
-			type = "fx";
-		}
-
-		if ( data === undefined )
-			return jQuery.queue( this[0], type );
-
-		return this.each(function(){
-			var queue = jQuery.queue( this, type, data );
-			
-			 if( type == "fx" && queue.length == 1 )
-				queue[0].call(this);
-		});
-	},
-	dequeue: function(type){
-		return this.each(function(){
-			jQuery.dequeue( this, type );
-		});
-	}
-});/*!
- * Sizzle CSS Selector Engine - v0.9.3
- *  Copyright 2009, The Dojo Foundation
- *  Released under the MIT, BSD, and GPL Licenses.
- *  More information: http://sizzlejs.com/
- */
-(function(){
-
-var chunker = /((?:\((?:\([^()]+\)|[^()]+)+\)|\[(?:\[[^[\]]*\]|['"][^'"]*['"]|[^[\]'"]+)+\]|\\.|[^ >+~,(\[\\]+)+|[>+~])(\s*,\s*)?/g,
-	done = 0,
-	toString = Object.prototype.toString;
-
-var Sizzle = function(selector, context, results, seed) {
-	results = results || [];
-	context = context || document;
-
-	if ( context.nodeType !== 1 && context.nodeType !== 9 )
-		return [];
-	
-	if ( !selector || typeof selector !== "string" ) {
-		return results;
-	}
-
-	var parts = [], m, set, checkSet, check, mode, extra, prune = true;
-	
-	// Reset the position of the chunker regexp (start from head)
-	chunker.lastIndex = 0;
-	
-	while ( (m = chunker.exec(selector)) !== null ) {
-		parts.push( m[1] );
-		
-		if ( m[2] ) {
-			extra = RegExp.rightContext;
-			break;
-		}
-	}
-
-	if ( parts.length > 1 && origPOS.exec( selector ) ) {
-		if ( parts.length === 2 && Expr.relative[ parts[0] ] ) {
-			set = posProcess( parts[0] + parts[1], context );
-		} else {
-			set = Expr.relative[ parts[0] ] ?
-				[ context ] :
-				Sizzle( parts.shift(), context );
-
-			while ( parts.length ) {
-				selector = parts.shift();
-
-				if ( Expr.relative[ selector ] )
-					selector += parts.shift();
-
-				set = posProcess( selector, set );
-			}
-		}
-	} else {
-		var ret = seed ?
-			{ expr: parts.pop(), set: makeArray(seed) } :
-			Sizzle.find( parts.pop(), parts.length === 1 && context.parentNode ? context.parentNode : context, isXML(context) );
-		set = Sizzle.filter( ret.expr, ret.set );
-
-		if ( parts.length > 0 ) {
-			checkSet = makeArray(set);
-		} else {
-			prune = false;
-		}
-
-		while ( parts.length ) {
-			var cur = parts.pop(), pop = cur;
-
-			if ( !Expr.relative[ cur ] ) {
-				cur = "";
-			} else {
-				pop = parts.pop();
-			}
-
-			if ( pop == null ) {
-				pop = context;
-			}
-
-			Expr.relative[ cur ]( checkSet, pop, isXML(context) );
-		}
-	}
-
-	if ( !checkSet ) {
-		checkSet = set;
-	}
-
-	if ( !checkSet ) {
-		throw "Syntax error, unrecognized expression: " + (cur || selector);
-	}
-
-	if ( toString.call(checkSet) === "[object Array]" ) {
-		if ( !prune ) {
-			results.push.apply( results, checkSet );
-		} else if ( context.nodeType === 1 ) {
-			for ( var i = 0; checkSet[i] != null; i++ ) {
-				if ( checkSet[i] && (checkSet[i] === true || checkSet[i].nodeType === 1 && contains(context, checkSet[i])) ) {
-					results.push( set[i] );
-				}
-			}
-		} else {
-			for ( var i = 0; checkSet[i] != null; i++ ) {
-				if ( checkSet[i] && checkSet[i].nodeType === 1 ) {
-					results.push( set[i] );
-				}
-			}
-		}
-	} else {
-		makeArray( checkSet, results );
-	}
-
-	if ( extra ) {
-		Sizzle( extra, context, results, seed );
-
-		if ( sortOrder ) {
-			hasDuplicate = false;
-			results.sort(sortOrder);
-
-			if ( hasDuplicate ) {
-				for ( var i = 1; i < results.length; i++ ) {
-					if ( results[i] === results[i-1] ) {
-						results.splice(i--, 1);
-					}
-				}
-			}
-		}
-	}
-
-	return results;
-};
-
-Sizzle.matches = function(expr, set){
-	return Sizzle(expr, null, null, set);
-};
-
-Sizzle.find = function(expr, context, isXML){
-	var set, match;
-
-	if ( !expr ) {
-		return [];
-	}
-
-	for ( var i = 0, l = Expr.order.length; i < l; i++ ) {
-		var type = Expr.order[i], match;
-		
-		if ( (match = Expr.match[ type ].exec( expr )) ) {
-			var left = RegExp.leftContext;
-
-			if ( left.substr( left.length - 1 ) !== "\\" ) {
-				match[1] = (match[1] || "").replace(/\\/g, "");
-				set = Expr.find[ type ]( match, context, isXML );
-				if ( set != null ) {
-					expr = expr.replace( Expr.match[ type ], "" );
-					break;
-				}
-			}
-		}
-	}
-
-	if ( !set ) {
-		set = context.getElementsByTagName("*");
-	}
-
-	return {set: set, expr: expr};
-};
-
-Sizzle.filter = function(expr, set, inplace, not){
-	var old = expr, result = [], curLoop = set, match, anyFound,
-		isXMLFilter = set && set[0] && isXML(set[0]);
-
-	while ( expr && set.length ) {
-		for ( var type in Expr.filter ) {
-			if ( (match = Expr.match[ type ].exec( expr )) != null ) {
-				var filter = Expr.filter[ type ], found, item;
-				anyFound = false;
-
-				if ( curLoop == result ) {
-					result = [];
-				}
-
-				if ( Expr.preFilter[ type ] ) {
-					match = Expr.preFilter[ type ]( match, curLoop, inplace, result, not, isXMLFilter );
-
-					if ( !match ) {
-						anyFound = found = true;
-					} else if ( match === true ) {
-						continue;
-					}
-				}
-
-				if ( match ) {
-					for ( var i = 0; (item = curLoop[i]) != null; i++ ) {
-						if ( item ) {
-							found = filter( item, match, i, curLoop );
-							var pass = not ^ !!found;
-
-							if ( inplace && found != null ) {
-								if ( pass ) {
-									anyFound = true;
-								} else {
-									curLoop[i] = false;
-								}
-							} else if ( pass ) {
-								result.push( item );
-								anyFound = true;
-							}
-						}
-					}
-				}
-
-				if ( found !== undefined ) {
-					if ( !inplace ) {
-						curLoop = result;
-					}
-
-					expr = expr.replace( Expr.match[ type ], "" );
-
-					if ( !anyFound ) {
-						return [];
-					}
-
-					break;
-				}
-			}
-		}
-
-		// Improper expression
-		if ( expr == old ) {
-			if ( anyFound == null ) {
-				throw "Syntax error, unrecognized expression: " + expr;
-			} else {
-				break;
-			}
-		}
-
-		old = expr;
-	}
-
-	return curLoop;
-};
-
-var Expr = Sizzle.selectors = {
-	order: [ "ID", "NAME", "TAG" ],
-	match: {
-		ID: /#((?:[\w\u00c0-\uFFFF_-]|\\.)+)/,
-		CLASS: /\.((?:[\w\u00c0-\uFFFF_-]|\\.)+)/,
-		NAME: /\[name=['"]*((?:[\w\u00c0-\uFFFF_-]|\\.)+)['"]*\]/,
-		ATTR: /\[\s*((?:[\w\u00c0-\uFFFF_-]|\\.)+)\s*(?:(\S?=)\s*(['"]*)(.*?)\3|)\s*\]/,
-		TAG: /^((?:[\w\u00c0-\uFFFF\*_-]|\\.)+)/,
-		CHILD: /:(only|nth|last|first)-child(?:\((even|odd|[\dn+-]*)\))?/,
-		POS: /:(nth|eq|gt|lt|first|last|even|odd)(?:\((\d*)\))?(?=[^-]|$)/,
-		PSEUDO: /:((?:[\w\u00c0-\uFFFF_-]|\\.)+)(?:\((['"]*)((?:\([^\)]+\)|[^\2\(\)]*)+)\2\))?/
-	},
-	attrMap: {
-		"class": "className",
-		"for": "htmlFor"
-	},
-	attrHandle: {
-		href: function(elem){
-			return elem.getAttribute("href");
-		}
-	},
-	relative: {
-		"+": function(checkSet, part, isXML){
-			var isPartStr = typeof part === "string",
-				isTag = isPartStr && !/\W/.test(part),
-				isPartStrNotTag = isPartStr && !isTag;
-
-			if ( isTag && !isXML ) {
-				part = part.toUpperCase();
-			}
-
-			for ( var i = 0, l = checkSet.length, elem; i < l; i++ ) {
-				if ( (elem = checkSet[i]) ) {
-					while ( (elem = elem.previousSibling) && elem.nodeType !== 1 ) {}
-
-					checkSet[i] = isPartStrNotTag || elem && elem.nodeName === part ?
-						elem || false :
-						elem === part;
-				}
-			}
-
-			if ( isPartStrNotTag ) {
-				Sizzle.filter( part, checkSet, true );
-			}
-		},
-		">": function(checkSet, part, isXML){
-			var isPartStr = typeof part === "string";
-
-			if ( isPartStr && !/\W/.test(part) ) {
-				part = isXML ? part : part.toUpperCase();
-
-				for ( var i = 0, l = checkSet.length; i < l; i++ ) {
-					var elem = checkSet[i];
-					if ( elem ) {
-						var parent = elem.parentNode;
-						checkSet[i] = parent.nodeName === part ? parent : false;
-					}
-				}
-			} else {
-				for ( var i = 0, l = checkSet.length; i < l; i++ ) {
-					var elem = checkSet[i];
-					if ( elem ) {
-						checkSet[i] = isPartStr ?
-							elem.parentNode :
-							elem.parentNode === part;
-					}
-				}
-
-				if ( isPartStr ) {
-					Sizzle.filter( part, checkSet, true );
-				}
-			}
-		},
-		"": function(checkSet, part, isXML){
-			var doneName = done++, checkFn = dirCheck;
-
-			if ( !part.match(/\W/) ) {
-				var nodeCheck = part = isXML ? part : part.toUpperCase();
-				checkFn = dirNodeCheck;
-			}
-
-			checkFn("parentNode", part, doneName, checkSet, nodeCheck, isXML);
-		},
-		"~": function(checkSet, part, isXML){
-			var doneName = done++, checkFn = dirCheck;
-
-			if ( typeof part === "string" && !part.match(/\W/) ) {
-				var nodeCheck = part = isXML ? part : part.toUpperCase();
-				checkFn = dirNodeCheck;
-			}
-
-			checkFn("previousSibling", part, doneName, checkSet, nodeCheck, isXML);
-		}
-	},
-	find: {
-		ID: function(match, context, isXML){
-			if ( typeof context.getElementById !== "undefined" && !isXML ) {
-				var m = context.getElementById(match[1]);
-				return m ? [m] : [];
-			}
-		},
-		NAME: function(match, context, isXML){
-			if ( typeof context.getElementsByName !== "undefined" ) {
-				var ret = [], results = context.getElementsByName(match[1]);
-
-				for ( var i = 0, l = results.length; i < l; i++ ) {
-					if ( results[i].getAttribute("name") === match[1] ) {
-						ret.push( results[i] );
-					}
-				}
-
-				return ret.length === 0 ? null : ret;
-			}
-		},
-		TAG: function(match, context){
-			return context.getElementsByTagName(match[1]);
-		}
-	},
-	preFilter: {
-		CLASS: function(match, curLoop, inplace, result, not, isXML){
-			match = " " + match[1].replace(/\\/g, "") + " ";
-
-			if ( isXML ) {
-				return match;
-			}
-
-			for ( var i = 0, elem; (elem = curLoop[i]) != null; i++ ) {
-				if ( elem ) {
-					if ( not ^ (elem.className && (" " + elem.className + " ").indexOf(match) >= 0) ) {
-						if ( !inplace )
-							result.push( elem );
-					} else if ( inplace ) {
-						curLoop[i] = false;
-					}
-				}
-			}
-
-			return false;
-		},
-		ID: function(match){
-			return match[1].replace(/\\/g, "");
-		},
-		TAG: function(match, curLoop){
-			for ( var i = 0; curLoop[i] === false; i++ ){}
-			return curLoop[i] && isXML(curLoop[i]) ? match[1] : match[1].toUpperCase();
-		},
-		CHILD: function(match){
-			if ( match[1] == "nth" ) {
-				// parse equations like 'even', 'odd', '5', '2n', '3n+2', '4n-1', '-n+6'
-				var test = /(-?)(\d*)n((?:\+|-)?\d*)/.exec(
-					match[2] == "even" && "2n" || match[2] == "odd" && "2n+1" ||
-					!/\D/.test( match[2] ) && "0n+" + match[2] || match[2]);
-
-				// calculate the numbers (first)n+(last) including if they are negative
-				match[2] = (test[1] + (test[2] || 1)) - 0;
-				match[3] = test[3] - 0;
-			}
-
-			// TODO: Move to normal caching system
-			match[0] = done++;
-
-			return match;
-		},
-		ATTR: function(match, curLoop, inplace, result, not, isXML){
-			var name = match[1].replace(/\\/g, "");
-			
-			if ( !isXML && Expr.attrMap[name] ) {
-				match[1] = Expr.attrMap[name];
-			}
-
-			if ( match[2] === "~=" ) {
-				match[4] = " " + match[4] + " ";
-			}
-
-			return match;
-		},
-		PSEUDO: function(match, curLoop, inplace, result, not){
-			if ( match[1] === "not" ) {
-				// If we're dealing with a complex expression, or a simple one
-				if ( match[3].match(chunker).length > 1 || /^\w/.test(match[3]) ) {
-					match[3] = Sizzle(match[3], null, null, curLoop);
-				} else {
-					var ret = Sizzle.filter(match[3], curLoop, inplace, true ^ not);
-					if ( !inplace ) {
-						result.push.apply( result, ret );
-					}
-					return false;
-				}
-			} else if ( Expr.match.POS.test( match[0] ) || Expr.match.CHILD.test( match[0] ) ) {
-				return true;
-			}
-			
-			return match;
-		},
-		POS: function(match){
-			match.unshift( true );
-			return match;
-		}
-	},
-	filters: {
-		enabled: function(elem){
-			return elem.disabled === false && elem.type !== "hidden";
-		},
-		disabled: function(elem){
-			return elem.disabled === true;
-		},
-		checked: function(elem){
-			return elem.checked === true;
-		},
-		selected: function(elem){
-			// Accessing this property makes selected-by-default
-			// options in Safari work properly
-			elem.parentNode.selectedIndex;
-			return elem.selected === true;
-		},
-		parent: function(elem){
-			return !!elem.firstChild;
-		},
-		empty: function(elem){
-			return !elem.firstChild;
-		},
-		has: function(elem, i, match){
-			return !!Sizzle( match[3], elem ).length;
-		},
-		header: function(elem){
-			return /h\d/i.test( elem.nodeName );
-		},
-		text: function(elem){
-			return "text" === elem.type;
-		},
-		radio: function(elem){
-			return "radio" === elem.type;
-		},
-		checkbox: function(elem){
-			return "checkbox" === elem.type;
-		},
-		file: function(elem){
-			return "file" === elem.type;
-		},
-		password: function(elem){
-			return "password" === elem.type;
-		},
-		submit: function(elem){
-			return "submit" === elem.type;
-		},
-		image: function(elem){
-			return "image" === elem.type;
-		},
-		reset: function(elem){
-			return "reset" === elem.type;
-		},
-		button: function(elem){
-			return "button" === elem.type || elem.nodeName.toUpperCase() === "BUTTON";
-		},
-		input: function(elem){
-			return /input|select|textarea|button/i.test(elem.nodeName);
-		}
-	},
-	setFilters: {
-		first: function(elem, i){
-			return i === 0;
-		},
-		last: function(elem, i, match, array){
-			return i === array.length - 1;
-		},
-		even: function(elem, i){
-			return i % 2 === 0;
-		},
-		odd: function(elem, i){
-			return i % 2 === 1;
-		},
-		lt: function(elem, i, match){
-			return i < match[3] - 0;
-		},
-		gt: function(elem, i, match){
-			return i > match[3] - 0;
-		},
-		nth: function(elem, i, match){
-			return match[3] - 0 == i;
-		},
-		eq: function(elem, i, match){
-			return match[3] - 0 == i;
-		}
-	},
-	filter: {
-		PSEUDO: function(elem, match, i, array){
-			var name = match[1], filter = Expr.filters[ name ];
-
-			if ( filter ) {
-				return filter( elem, i, match, array );
-			} else if ( name === "contains" ) {
-				return (elem.textContent || elem.innerText || "").indexOf(match[3]) >= 0;
-			} else if ( name === "not" ) {
-				var not = match[3];
-
-				for ( var i = 0, l = not.length; i < l; i++ ) {
-					if ( not[i] === elem ) {
-						return false;
-					}
-				}
-
-				return true;
-			}
-		},
-		CHILD: function(elem, match){
-			var type = match[1], node = elem;
-			switch (type) {
-				case 'only':
-				case 'first':
-					while (node = node.previousSibling)  {
-						if ( node.nodeType === 1 ) return false;
-					}
-					if ( type == 'first') return true;
-					node = elem;
-				case 'last':
-					while (node = node.nextSibling)  {
-						if ( node.nodeType === 1 ) return false;
-					}
-					return true;
-				case 'nth':
-					var first = match[2], last = match[3];
-
-					if ( first == 1 && last == 0 ) {
-						return true;
-					}
-					
-					var doneName = match[0],
-						parent = elem.parentNode;
-	
-					if ( parent && (parent.sizcache !== doneName || !elem.nodeIndex) ) {
-						var count = 0;
-						for ( node = parent.firstChild; node; node = node.nextSibling ) {
-							if ( node.nodeType === 1 ) {
-								node.nodeIndex = ++count;
-							}
-						} 
-						parent.sizcache = doneName;
-					}
-					
-					var diff = elem.nodeIndex - last;
-					if ( first == 0 ) {
-						return diff == 0;
-					} else {
-						return ( diff % first == 0 && diff / first >= 0 );
-					}
-			}
-		},
-		ID: function(elem, match){
-			return elem.nodeType === 1 && elem.getAttribute("id") === match;
-		},
-		TAG: function(elem, match){
-			return (match === "*" && elem.nodeType === 1) || elem.nodeName === match;
-		},
-		CLASS: function(elem, match){
-			return (" " + (elem.className || elem.getAttribute("class")) + " ")
-				.indexOf( match ) > -1;
-		},
-		ATTR: function(elem, match){
-			var name = match[1],
-				result = Expr.attrHandle[ name ] ?
-					Expr.attrHandle[ name ]( elem ) :
-					elem[ name ] != null ?
-						elem[ name ] :
-						elem.getAttribute( name ),
-				value = result + "",
-				type = match[2],
-				check = match[4];
-
-			return result == null ?
-				type === "!=" :
-				type === "=" ?
-				value === check :
-				type === "*=" ?
-				value.indexOf(check) >= 0 :
-				type === "~=" ?
-				(" " + value + " ").indexOf(check) >= 0 :
-				!check ?
-				value && result !== false :
-				type === "!=" ?
-				value != check :
-				type === "^=" ?
-				value.indexOf(check) === 0 :
-				type === "$=" ?
-				value.substr(value.length - check.length) === check :
-				type === "|=" ?
-				value === check || value.substr(0, check.length + 1) === check + "-" :
-				false;
-		},
-		POS: function(elem, match, i, array){
-			var name = match[2], filter = Expr.setFilters[ name ];
-
-			if ( filter ) {
-				return filter( elem, i, match, array );
-			}
-		}
-	}
-};
-
-var origPOS = Expr.match.POS;
-
-for ( var type in Expr.match ) {
-	Expr.match[ type ] = RegExp( Expr.match[ type ].source + /(?![^\[]*\])(?![^\(]*\))/.source );
-}
-
-var makeArray = function(array, results) {
-	array = Array.prototype.slice.call( array );
-
-	if ( results ) {
-		results.push.apply( results, array );
-		return results;
-	}
-	
-	return array;
-};
-
-// Perform a simple check to determine if the browser is capable of
-// converting a NodeList to an array using builtin methods.
-try {
-	Array.prototype.slice.call( document.documentElement.childNodes );
-
-// Provide a fallback method if it does not work
-} catch(e){
-	makeArray = function(array, results) {
-		var ret = results || [];
-
-		if ( toString.call(array) === "[object Array]" ) {
-			Array.prototype.push.apply( ret, array );
-		} else {
-			if ( typeof array.length === "number" ) {
-				for ( var i = 0, l = array.length; i < l; i++ ) {
-					ret.push( array[i] );
-				}
-			} else {
-				for ( var i = 0; array[i]; i++ ) {
-					ret.push( array[i] );
-				}
-			}
-		}
-
-		return ret;
-	};
-}
-
-var sortOrder;
-
-if ( document.documentElement.compareDocumentPosition ) {
-	sortOrder = function( a, b ) {
-		var ret = a.compareDocumentPosition(b) & 4 ? -1 : a === b ? 0 : 1;
-		if ( ret === 0 ) {
-			hasDuplicate = true;
-		}
-		return ret;
-	};
-} else if ( "sourceIndex" in document.documentElement ) {
-	sortOrder = function( a, b ) {
-		var ret = a.sourceIndex - b.sourceIndex;
-		if ( ret === 0 ) {
-			hasDuplicate = true;
-		}
-		return ret;
-	};
-} else if ( document.createRange ) {
-	sortOrder = function( a, b ) {
-		var aRange = a.ownerDocument.createRange(), bRange = b.ownerDocument.createRange();
-		aRange.selectNode(a);
-		aRange.collapse(true);
-		bRange.selectNode(b);
-		bRange.collapse(true);
-		var ret = aRange.compareBoundaryPoints(Range.START_TO_END, bRange);
-		if ( ret === 0 ) {
-			hasDuplicate = true;
-		}
-		return ret;
-	};
-}
-
-// Check to see if the browser returns elements by name when
-// querying by getElementById (and provide a workaround)
-(function(){
-	// We're going to inject a fake input element with a specified name
-	var form = document.createElement("form"),
-		id = "script" + (new Date).getTime();
-	form.innerHTML = "<input name='" + id + "'/>";
-
-	// Inject it into the root element, check its status, and remove it quickly
-	var root = document.documentElement;
-	root.insertBefore( form, root.firstChild );
-
-	// The workaround has to do additional checks after a getElementById
-	// Which slows things down for other browsers (hence the branching)
-	if ( !!document.getElementById( id ) ) {
-		Expr.find.ID = function(match, context, isXML){
-			if ( typeof context.getElementById !== "undefined" && !isXML ) {
-				var m = context.getElementById(match[1]);
-				return m ? m.id === match[1] || typeof m.getAttributeNode !== "undefined" && m.getAttributeNode("id").nodeValue === match[1] ? [m] : undefined : [];
-			}
-		};
-
-		Expr.filter.ID = function(elem, match){
-			var node = typeof elem.getAttributeNode !== "undefined" && elem.getAttributeNode("id");
-			return elem.nodeType === 1 && node && node.nodeValue === match;
-		};
-	}
-
-	root.removeChild( form );
-})();
-
-(function(){
-	// Check to see if the browser returns only elements
-	// when doing getElementsByTagName("*")
-
-	// Create a fake element
-	var div = document.createElement("div");
-	div.appendChild( document.createComment("") );
-
-	// Make sure no comments are found
-	if ( div.getElementsByTagName("*").length > 0 ) {
-		Expr.find.TAG = function(match, context){
-			var results = context.getElementsByTagName(match[1]);
-
-			// Filter out possible comments
-			if ( match[1] === "*" ) {
-				var tmp = [];
-
-				for ( var i = 0; results[i]; i++ ) {
-					if ( results[i].nodeType === 1 ) {
-						tmp.push( results[i] );
-					}
-				}
-
-				results = tmp;
-			}
-
-			return results;
-		};
-	}
-
-	// Check to see if an attribute returns normalized href attributes
-	div.innerHTML = "<a href='#'></a>";
-	if ( div.firstChild && typeof div.firstChild.getAttribute !== "undefined" &&
-			div.firstChild.getAttribute("href") !== "#" ) {
-		Expr.attrHandle.href = function(elem){
-			return elem.getAttribute("href", 2);
-		};
-	}
-})();
-
-if ( document.querySelectorAll ) (function(){
-	var oldSizzle = Sizzle, div = document.createElement("div");
-	div.innerHTML = "<p class='TEST'></p>";
-
-	// Safari can't handle uppercase or unicode characters when
-	// in quirks mode.
-	if ( div.querySelectorAll && div.querySelectorAll(".TEST").length === 0 ) {
-		return;
-	}
-	
-	Sizzle = function(query, context, extra, seed){
-		context = context || document;
-
-		// Only use querySelectorAll on non-XML documents
-		// (ID selectors don't work in non-HTML documents)
-		if ( !seed && context.nodeType === 9 && !isXML(context) ) {
-			try {
-				return makeArray( context.querySelectorAll(query), extra );
-			} catch(e){}
-		}
-		
-		return oldSizzle(query, context, extra, seed);
-	};
-
-	Sizzle.find = oldSizzle.find;
-	Sizzle.filter = oldSizzle.filter;
-	Sizzle.selectors = oldSizzle.selectors;
-	Sizzle.matches = oldSizzle.matches;
-})();
-
-if ( document.getElementsByClassName && document.documentElement.getElementsByClassName ) (function(){
-	var div = document.createElement("div");
-	div.innerHTML = "<div class='test e'></div><div class='test'></div>";
-
-	// Opera can't find a second classname (in 9.6)
-	if ( div.getElementsByClassName("e").length === 0 )
-		return;
-
-	// Safari caches class attributes, doesn't catch changes (in 3.2)
-	div.lastChild.className = "e";
-
-	if ( div.getElementsByClassName("e").length === 1 )
-		return;
-
-	Expr.order.splice(1, 0, "CLASS");
-	Expr.find.CLASS = function(match, context, isXML) {
-		if ( typeof context.getElementsByClassName !== "undefined" && !isXML ) {
-			return context.getElementsByClassName(match[1]);
-		}
-	};
-})();
-
-function dirNodeCheck( dir, cur, doneName, checkSet, nodeCheck, isXML ) {
-	var sibDir = dir == "previousSibling" && !isXML;
-	for ( var i = 0, l = checkSet.length; i < l; i++ ) {
-		var elem = checkSet[i];
-		if ( elem ) {
-			if ( sibDir && elem.nodeType === 1 ){
-				elem.sizcache = doneName;
-				elem.sizset = i;
-			}
-			elem = elem[dir];
-			var match = false;
-
-			while ( elem ) {
-				if ( elem.sizcache === doneName ) {
-					match = checkSet[elem.sizset];
-					break;
-				}
-
-				if ( elem.nodeType === 1 && !isXML ){
-					elem.sizcache = doneName;
-					elem.sizset = i;
-				}
-
-				if ( elem.nodeName === cur ) {
-					match = elem;
-					break;
-				}
-
-				elem = elem[dir];
-			}
-
-			checkSet[i] = match;
-		}
-	}
-}
-
-function dirCheck( dir, cur, doneName, checkSet, nodeCheck, isXML ) {
-	var sibDir = dir == "previousSibling" && !isXML;
-	for ( var i = 0, l = checkSet.length; i < l; i++ ) {
-		var elem = checkSet[i];
-		if ( elem ) {
-			if ( sibDir && elem.nodeType === 1 ) {
-				elem.sizcache = doneName;
-				elem.sizset = i;
-			}
-			elem = elem[dir];
-			var match = false;
-
-			while ( elem ) {
-				if ( elem.sizcache === doneName ) {
-					match = checkSet[elem.sizset];
-					break;
-				}
-
-				if ( elem.nodeType === 1 ) {
-					if ( !isXML ) {
-						elem.sizcache = doneName;
-						elem.sizset = i;
-					}
-					if ( typeof cur !== "string" ) {
-						if ( elem === cur ) {
-							match = true;
-							break;
-						}
-
-					} else if ( Sizzle.filter( cur, [elem] ).length > 0 ) {
-						match = elem;
-						break;
-					}
-				}
-
-				elem = elem[dir];
-			}
-
-			checkSet[i] = match;
-		}
-	}
-}
-
-var contains = document.compareDocumentPosition ?  function(a, b){
-	return a.compareDocumentPosition(b) & 16;
-} : function(a, b){
-	return a !== b && (a.contains ? a.contains(b) : true);
-};
-
-var isXML = function(elem){
-	return elem.nodeType === 9 && elem.documentElement.nodeName !== "HTML" ||
-		!!elem.ownerDocument && isXML( elem.ownerDocument );
-};
-
-var posProcess = function(selector, context){
-	var tmpSet = [], later = "", match,
-		root = context.nodeType ? [context] : context;
-
-	// Position selectors must be done after the filter
-	// And so must :not(positional) so we move all PSEUDOs to the end
-	while ( (match = Expr.match.PSEUDO.exec( selector )) ) {
-		later += match[0];
-		selector = selector.replace( Expr.match.PSEUDO, "" );
-	}
-
-	selector = Expr.relative[selector] ? selector + "*" : selector;
-
-	for ( var i = 0, l = root.length; i < l; i++ ) {
-		Sizzle( selector, root[i], tmpSet );
-	}
-
-	return Sizzle.filter( later, tmpSet );
-};
-
-// EXPOSE
-jQuery.find = Sizzle;
-jQuery.filter = Sizzle.filter;
-jQuery.expr = Sizzle.selectors;
-jQuery.expr[":"] = jQuery.expr.filters;
-
-Sizzle.selectors.filters.hidden = function(elem){
-	return elem.offsetWidth === 0 || elem.offsetHeight === 0;
-};
-
-Sizzle.selectors.filters.visible = function(elem){
-	return elem.offsetWidth > 0 || elem.offsetHeight > 0;
-};
-
-Sizzle.selectors.filters.animated = function(elem){
-	return jQuery.grep(jQuery.timers, function(fn){
-		return elem === fn.elem;
-	}).length;
-};
-
-jQuery.multiFilter = function( expr, elems, not ) {
-	if ( not ) {
-		expr = ":not(" + expr + ")";
-	}
-
-	return Sizzle.matches(expr, elems);
-};
-
-jQuery.dir = function( elem, dir ){
-	var matched = [], cur = elem[dir];
-	while ( cur && cur != document ) {
-		if ( cur.nodeType == 1 )
-			matched.push( cur );
-		cur = cur[dir];
-	}
-	return matched;
-};
-
-jQuery.nth = function(cur, result, dir, elem){
-	result = result || 1;
-	var num = 0;
-
-	for ( ; cur; cur = cur[dir] )
-		if ( cur.nodeType == 1 && ++num == result )
-			break;
-
-	return cur;
-};
-
-jQuery.sibling = function(n, elem){
-	var r = [];
-
-	for ( ; n; n = n.nextSibling ) {
-		if ( n.nodeType == 1 && n != elem )
-			r.push( n );
-	}
-
-	return r;
-};
-
-return;
-
-window.Sizzle = Sizzle;
-
-})();
-/*
- * A number of helper functions used for managing events.
- * Many of the ideas behind this code originated from
- * Dean Edwards' addEvent library.
- */
-jQuery.event = {
-
-	// Bind an event to an element
-	// Original by Dean Edwards
-	add: function(elem, types, handler, data) {
-		if ( elem.nodeType == 3 || elem.nodeType == 8 )
-			return;
-
-		// For whatever reason, IE has trouble passing the window object
-		// around, causing it to be cloned in the process
-		if ( elem.setInterval && elem != window )
-			elem = window;
-
-		// Make sure that the function being executed has a unique ID
-		if ( !handler.guid )
-			handler.guid = this.guid++;
-
-		// if data is passed, bind to handler
-		if ( data !== undefined ) {
-			// Create temporary function pointer to original handler
-			var fn = handler;
-
-			// Create unique handler function, wrapped around original handler
-			handler = this.proxy( fn );
-
-			// Store data in unique handler
-			handler.data = data;
-		}
-
-		// Init the element's event structure
-		var events = jQuery.data(elem, "events") || jQuery.data(elem, "events", {}),
-			handle = jQuery.data(elem, "handle") || jQuery.data(elem, "handle", function(){
-				// Handle the second event of a trigger and when
-				// an event is called after a page has unloaded
-				return typeof jQuery !== "undefined" && !jQuery.event.triggered ?
-					jQuery.event.handle.apply(arguments.callee.elem, arguments) :
-					undefined;
-			});
-		// Add elem as a property of the handle function
-		// This is to prevent a memory leak with non-native
-		// event in IE.
-		handle.elem = elem;
-
-		// Handle multiple events separated by a space
-		// jQuery(...).bind("mouseover mouseout", fn);
-		jQuery.each(types.split(/\s+/), function(index, type) {
-			// Namespaced event handlers
-			var namespaces = type.split(".");
-			type = namespaces.shift();
-			handler.type = namespaces.slice().sort().join(".");
-
-			// Get the current list of functions bound to this event
-			var handlers = events[type];
-			
-			if ( jQuery.event.specialAll[type] )
-				jQuery.event.specialAll[type].setup.call(elem, data, namespaces);
-
-			// Init the event handler queue
-			if (!handlers) {
-				handlers = events[type] = {};
-
-				// Check for a special event handler
-				// Only use addEventListener/attachEvent if the special
-				// events handler returns false
-				if ( !jQuery.event.special[type] || jQuery.event.special[type].setup.call(elem, data, namespaces) === false ) {
-					// Bind the global event handler to the element
-					if (elem.addEventListener)
-						elem.addEventListener(type, handle, false);
-					else if (elem.attachEvent)
-						elem.attachEvent("on" + type, handle);
-				}
-			}
-
-			// Add the function to the element's handler list
-			handlers[handler.guid] = handler;
-
-			// Keep track of which events have been used, for global triggering
-			jQuery.event.global[type] = true;
-		});
-
-		// Nullify elem to prevent memory leaks in IE
-		elem = null;
-	},
-
-	guid: 1,
-	global: {},
-
-	// Detach an event or set of events from an element
-	remove: function(elem, types, handler) {
-		// don't do events on text and comment nodes
-		if ( elem.nodeType == 3 || elem.nodeType == 8 )
-			return;
-
-		var events = jQuery.data(elem, "events"), ret, index;
-
-		if ( events ) {
-			// Unbind all events for the element
-			if ( types === undefined || (typeof types === "string" && types.charAt(0) == ".") )
-				for ( var type in events )
-					this.remove( elem, type + (types || "") );
-			else {
-				// types is actually an event object here
-				if ( types.type ) {
-					handler = types.handler;
-					types = types.type;
-				}
-
-				// Handle multiple events seperated by a space
-				// jQuery(...).unbind("mouseover mouseout", fn);
-				jQuery.each(types.split(/\s+/), function(index, type){
-					// Namespaced event handlers
-					var namespaces = type.split(".");
-					type = namespaces.shift();
-					var namespace = RegExp("(^|\\.)" + namespaces.slice().sort().join(".*\\.") + "(\\.|$)");
-
-					if ( events[type] ) {
-						// remove the given handler for the given type
-						if ( handler )
-							delete events[type][handler.guid];
-
-						// remove all handlers for the given type
-						else
-							for ( var handle in events[type] )
-								// Handle the removal of namespaced events
-								if ( namespace.test(events[type][handle].type) )
-									delete events[type][handle];
-									
-						if ( jQuery.event.specialAll[type] )
-							jQuery.event.specialAll[type].teardown.call(elem, namespaces);
-
-						// remove generic event handler if no more handlers exist
-						for ( ret in events[type] ) break;
-						if ( !ret ) {
-							if ( !jQuery.event.special[type] || jQuery.event.special[type].teardown.call(elem, namespaces) === false ) {
-								if (elem.removeEventListener)
-									elem.removeEventListener(type, jQuery.data(elem, "handle"), false);
-								else if (elem.detachEvent)
-									elem.detachEvent("on" + type, jQuery.data(elem, "handle"));
-							}
-							ret = null;
-							delete events[type];
-						}
-					}
-				});
-			}
-
-			// Remove the expando if it's no longer used
-			for ( ret in events ) break;
-			if ( !ret ) {
-				var handle = jQuery.data( elem, "handle" );
-				if ( handle ) handle.elem = null;
-				jQuery.removeData( elem, "events" );
-				jQuery.removeData( elem, "handle" );
-			}
-		}
-	},
-
-	// bubbling is internal
-	trigger: function( event, data, elem, bubbling ) {
-		// Event object or event type
-		var type = event.type || event;
-
-		if( !bubbling ){
-			event = typeof event === "object" ?
-				// jQuery.Event object
-				event[expando] ? event :
-				// Object literal
-				jQuery.extend( jQuery.Event(type), event ) :
-				// Just the event type (string)
-				jQuery.Event(type);
-
-			if ( type.indexOf("!") >= 0 ) {
-				event.type = type = type.slice(0, -1);
-				event.exclusive = true;
-			}
-
-			// Handle a global trigger
-			if ( !elem ) {
-				// Don't bubble custom events when global (to avoid too much overhead)
-				event.stopPropagation();
-				// Only trigger if we've ever bound an event for it
-				if ( this.global[type] )
-					jQuery.each( jQuery.cache, function(){
-						if ( this.events && this.events[type] )
-							jQuery.event.trigger( event, data, this.handle.elem );
-					});
-			}
-
-			// Handle triggering a single element
-
-			// don't do events on text and comment nodes
-			if ( !elem || elem.nodeType == 3 || elem.nodeType == 8 )
-				return undefined;
-			
-			// Clean up in case it is reused
-			event.result = undefined;
-			event.target = elem;
-			
-			// Clone the incoming data, if any
-			data = jQuery.makeArray(data);
-			data.unshift( event );
-		}
-
-		event.currentTarget = elem;
-
-		// Trigger the event, it is assumed that "handle" is a function
-		var handle = jQuery.data(elem, "handle");
-		if ( handle )
-			handle.apply( elem, data );
-
-		// Handle triggering native .onfoo handlers (and on links since we don't call .click() for links)
-		if ( (!elem[type] || (jQuery.nodeName(elem, 'a') && type == "click")) && elem["on"+type] && elem["on"+type].apply( elem, data ) === false )
-			event.result = false;
-
-		// Trigger the native events (except for clicks on links)
-		if ( !bubbling && elem[type] && !event.isDefaultPrevented() && !(jQuery.nodeName(elem, 'a') && type == "click") ) {
-			this.triggered = true;
-			try {
-				elem[ type ]();
-			// prevent IE from throwing an error for some hidden elements
-			} catch (e) {}
-		}
-
-		this.triggered = false;
-
-		if ( !event.isPropagationStopped() ) {
-			var parent = elem.parentNode || elem.ownerDocument;
-			if ( parent )
-				jQuery.event.trigger(event, data, parent, true);
-		}
-	},
-
-	handle: function(event) {
-		// returned undefined or false
-		var all, handlers;
-
-		event = arguments[0] = jQuery.event.fix( event || window.event );
-		event.currentTarget = this;
-		
-		// Namespaced event handlers
-		var namespaces = event.type.split(".");
-		event.type = namespaces.shift();
-
-		// Cache this now, all = true means, any handler
-		all = !namespaces.length && !event.exclusive;
-		
-		var namespace = RegExp("(^|\\.)" + namespaces.slice().sort().join(".*\\.") + "(\\.|$)");
-
-		handlers = ( jQuery.data(this, "events") || {} )[event.type];
-
-		for ( var j in handlers ) {
-			var handler = handlers[j];
-
-			// Filter the functions by class
-			if ( all || namespace.test(handler.type) ) {
-				// Pass in a reference to the handler function itself
-				// So that we can later remove it
-				event.handler = handler;
-				event.data = handler.data;
-
-				var ret = handler.apply(this, arguments);
-
-				if( ret !== undefined ){
-					event.result = ret;
-					if ( ret === false ) {
-						event.preventDefault();
-						event.stopPropagation();
-					}
-				}
-
-				if( event.isImmediatePropagationStopped() )
-					break;
-
-			}
-		}
-	},
-
-	props: "altKey attrChange attrName bubbles button cancelable charCode clientX clientY ctrlKey currentTarget data detail eventPhase fromElement handler keyCode metaKey newValue originalTarget pageX pageY prevValue relatedNode relatedTarget screenX screenY shiftKey srcElement target toElement view wheelDelta which".split(" "),
-
-	fix: function(event) {
-		if ( event[expando] )
-			return event;
-
-		// store a copy of the original event object
-		// and "clone" to set read-only properties
-		var originalEvent = event;
-		event = jQuery.Event( originalEvent );
-
-		for ( var i = this.props.length, prop; i; ){
-			prop = this.props[ --i ];
-			event[ prop ] = originalEvent[ prop ];
-		}
-
-		// Fix target property, if necessary
-		if ( !event.target )
-			event.target = event.srcElement || document; // Fixes #1925 where srcElement might not be defined either
-
-		// check if target is a textnode (safari)
-		if ( event.target.nodeType == 3 )
-			event.target = event.target.parentNode;
-
-		// Add relatedTarget, if necessary
-		if ( !event.relatedTarget && event.fromElement )
-			event.relatedTarget = event.fromElement == event.target ? event.toElement : event.fromElement;
-
-		// Calculate pageX/Y if missing and clientX/Y available
-		if ( event.pageX == null && event.clientX != null ) {
-			var doc = document.documentElement, body = document.body;
-			event.pageX = event.clientX + (doc && doc.scrollLeft || body && body.scrollLeft || 0) - (doc.clientLeft || 0);
-			event.pageY = event.clientY + (doc && doc.scrollTop || body && body.scrollTop || 0) - (doc.clientTop || 0);
-		}
-
-		// Add which for key events
-		if ( !event.which && ((event.charCode || event.charCode === 0) ? event.charCode : event.keyCode) )
-			event.which = event.charCode || event.keyCode;
-
-		// Add metaKey to non-Mac browsers (use ctrl for PC's and Meta for Macs)
-		if ( !event.metaKey && event.ctrlKey )
-			event.metaKey = event.ctrlKey;
-
-		// Add which for click: 1 == left; 2 == middle; 3 == right
-		// Note: button is not normalized, so don't use it
-		if ( !event.which && event.button )
-			event.which = (event.button & 1 ? 1 : ( event.button & 2 ? 3 : ( event.button & 4 ? 2 : 0 ) ));
-
-		return event;
-	},
-
-	proxy: function( fn, proxy ){
-		proxy = proxy || function(){ return fn.apply(this, arguments); };
-		// Set the guid of unique handler to the same of original handler, so it can be removed
-		proxy.guid = fn.guid = fn.guid || proxy.guid || this.guid++;
-		// So proxy can be declared as an argument
-		return proxy;
-	},
-
-	special: {
-		ready: {
-			// Make sure the ready event is setup
-			setup: bindReady,
-			teardown: function() {}
-		}
-	},
-	
-	specialAll: {
-		live: {
-			setup: function( selector, namespaces ){
-				jQuery.event.add( this, namespaces[0], liveHandler );
-			},
-			teardown:  function( namespaces ){
-				if ( namespaces.length ) {
-					var remove = 0, name = RegExp("(^|\\.)" + namespaces[0] + "(\\.|$)");
-					
-					jQuery.each( (jQuery.data(this, "events").live || {}), function(){
-						if ( name.test(this.type) )
-							remove++;
-					});
-					
-					if ( remove < 1 )
-						jQuery.event.remove( this, namespaces[0], liveHandler );
-				}
-			}
-		}
-	}
-};
-
-jQuery.Event = function( src ){
-	// Allow instantiation without the 'new' keyword
-	if( !this.preventDefault )
-		return new jQuery.Event(src);
-	
-	// Event object
-	if( src && src.type ){
-		this.originalEvent = src;
-		this.type = src.type;
-	// Event type
-	}else
-		this.type = src;
-
-	// timeStamp is buggy for some events on Firefox(#3843)
-	// So we won't rely on the native value
-	this.timeStamp = now();
-	
-	// Mark it as fixed
-	this[expando] = true;
-};
-
-function returnFalse(){
-	return false;
-}
-function returnTrue(){
-	return true;
-}
-
-// jQuery.Event is based on DOM3 Events as specified by the ECMAScript Language Binding
-// http://www.w3.org/TR/2003/WD-DOM-Level-3-Events-20030331/ecma-script-binding.html
-jQuery.Event.prototype = {
-	preventDefault: function() {
-		this.isDefaultPrevented = returnTrue;
-
-		var e = this.originalEvent;
-		if( !e )
-			return;
-		// if preventDefault exists run it on the original event
-		if (e.preventDefault)
-			e.preventDefault();
-		// otherwise set the returnValue property of the original event to false (IE)
-		e.returnValue = false;
-	},
-	stopPropagation: function() {
-		this.isPropagationStopped = returnTrue;
-
-		var e = this.originalEvent;
-		if( !e )
-			return;
-		// if stopPropagation exists run it on the original event
-		if (e.stopPropagation)
-			e.stopPropagation();
-		// otherwise set the cancelBubble property of the original event to true (IE)
-		e.cancelBubble = true;
-	},
-	stopImmediatePropagation:function(){
-		this.isImmediatePropagationStopped = returnTrue;
-		this.stopPropagation();
-	},
-	isDefaultPrevented: returnFalse,
-	isPropagationStopped: returnFalse,
-	isImmediatePropagationStopped: returnFalse
-};
-// Checks if an event happened on an element within another element
-// Used in jQuery.event.special.mouseenter and mouseleave handlers
-var withinElement = function(event) {
-	// Check if mouse(over|out) are still within the same parent element
-	var parent = event.relatedTarget;
-	// Traverse up the tree
-	while ( parent && parent != this )
-		try { parent = parent.parentNode; }
-		catch(e) { parent = this; }
-	
-	if( parent != this ){
-		// set the correct event type
-		event.type = event.data;
-		// handle event if we actually just moused on to a non sub-element
-		jQuery.event.handle.apply( this, arguments );
-	}
-};
-	
-jQuery.each({ 
-	mouseover: 'mouseenter', 
-	mouseout: 'mouseleave'
-}, function( orig, fix ){
-	jQuery.event.special[ fix ] = {
-		setup: function(){
-			jQuery.event.add( this, orig, withinElement, fix );
-		},
-		teardown: function(){
-			jQuery.event.remove( this, orig, withinElement );
-		}
-	};			   
-});
-
-jQuery.fn.extend({
-	bind: function( type, data, fn ) {
-		return type == "unload" ? this.one(type, data, fn) : this.each(function(){
-			jQuery.event.add( this, type, fn || data, fn && data );
-		});
-	},
-
-	one: function( type, data, fn ) {
-		var one = jQuery.event.proxy( fn || data, function(event) {
-			jQuery(this).unbind(event, one);
-			return (fn || data).apply( this, arguments );
-		});
-		return this.each(function(){
-			jQuery.event.add( this, type, one, fn && data);
-		});
-	},
-
-	unbind: function( type, fn ) {
-		return this.each(function(){
-			jQuery.event.remove( this, type, fn );
-		});
-	},
-
-	trigger: function( type, data ) {
-		return this.each(function(){
-			jQuery.event.trigger( type, data, this );
-		});
-	},
-
-	triggerHandler: function( type, data ) {
-		if( this[0] ){
-			var event = jQuery.Event(type);
-			event.preventDefault();
-			event.stopPropagation();
-			jQuery.event.trigger( event, data, this[0] );
-			return event.result;
-		}		
-	},
-
-	toggle: function( fn ) {
-		// Save reference to arguments for access in closure
-		var args = arguments, i = 1;
-
-		// link all the functions, so any of them can unbind this click handler
-		while( i < args.length )
-			jQuery.event.proxy( fn, args[i++] );
-
-		return this.click( jQuery.event.proxy( fn, function(event) {
-			// Figure out which function to execute
-			this.lastToggle = ( this.lastToggle || 0 ) % i;
-
-			// Make sure that clicks stop
-			event.preventDefault();
-
-			// and execute the function
-			return args[ this.lastToggle++ ].apply( this, arguments ) || false;
-		}));
-	},
-
-	hover: function(fnOver, fnOut) {
-		return this.mouseenter(fnOver).mouseleave(fnOut);
-	},
-
-	ready: function(fn) {
-		// Attach the listeners
-		bindReady();
-
-		// If the DOM is already ready
-		if ( jQuery.isReady )
-			// Execute the function immediately
-			fn.call( document, jQuery );
-
-		// Otherwise, remember the function for later
-		else
-			// Add the function to the wait list
-			jQuery.readyList.push( fn );
-
-		return this;
-	},
-	
-	live: function( type, fn ){
-		var proxy = jQuery.event.proxy( fn );
-		proxy.guid += this.selector + type;
-
-		jQuery(document).bind( liveConvert(type, this.selector), this.selector, proxy );
-
-		return this;
-	},
-	
-	die: function( type, fn ){
-		jQuery(document).unbind( liveConvert(type, this.selector), fn ? { guid: fn.guid + this.selector + type } : null );
-		return this;
-	}
-});
-
-function liveHandler( event ){
-	var check = RegExp("(^|\\.)" + event.type + "(\\.|$)"),
-		stop = true,
-		elems = [];
-
-	jQuery.each(jQuery.data(this, "events").live || [], function(i, fn){
-		if ( check.test(fn.type) ) {
-			var elem = jQuery(event.target).closest(fn.data)[0];
-			if ( elem )
-				elems.push({ elem: elem, fn: fn });
-		}
-	});
-
-	elems.sort(function(a,b) {
-		return jQuery.data(a.elem, "closest") - jQuery.data(b.elem, "closest");
-	});
-	
-	jQuery.each(elems, function(){
-		if ( this.fn.call(this.elem, event, this.fn.data) === false )
-			return (stop = false);
-	});
-
-	return stop;
-}
-
-function liveConvert(type, selector){
-	return ["live", type, selector.replace(/\./g, "`").replace(/ /g, "|")].join(".");
-}
-
-jQuery.extend({
-	isReady: false,
-	readyList: [],
-	// Handle when the DOM is ready
-	ready: function() {
-		// Make sure that the DOM is not already loaded
-		if ( !jQuery.isReady ) {
-			// Remember that the DOM is ready
-			jQuery.isReady = true;
-
-			// If there are functions bound, to execute
-			if ( jQuery.readyList ) {
-				// Execute all of them
-				jQuery.each( jQuery.readyList, function(){
-					this.call( document, jQuery );
-				});
-
-				// Reset the list of functions
-				jQuery.readyList = null;
-			}
-
-			// Trigger any bound ready events
-			jQuery(document).triggerHandler("ready");
-		}
-	}
-});
-
-var readyBound = false;
-
-function bindReady(){
-	if ( readyBound ) return;
-	readyBound = true;
-
-	// Mozilla, Opera and webkit nightlies currently support this event
-	if ( document.addEventListener ) {
-		// Use the handy event callback
-		document.addEventListener( "DOMContentLoaded", function(){
-			document.removeEventListener( "DOMContentLoaded", arguments.callee, false );
-			jQuery.ready();
-		}, false );
-
-	// If IE event model is used
-	} else if ( document.attachEvent ) {
-		// ensure firing before onload,
-		// maybe late but safe also for iframes
-		document.attachEvent("onreadystatechange", function(){
-			if ( document.readyState === "complete" ) {
-				document.detachEvent( "onreadystatechange", arguments.callee );
-				jQuery.ready();
-			}
-		});
-
-		// If IE and not an iframe
-		// continually check to see if the document is ready
-		if ( document.documentElement.doScroll && window == window.top ) (function(){
-			if ( jQuery.isReady ) return;
-
-			try {
-				// If IE is used, use the trick by Diego Perini
-				// http://javascript.nwbox.com/IEContentLoaded/
-				document.documentElement.doScroll("left");
-			} catch( error ) {
-				setTimeout( arguments.callee, 0 );
-				return;
-			}
-
-			// and execute any waiting functions
-			jQuery.ready();
-		})();
-	}
-
-	// A fallback to window.onload, that will always work
-	jQuery.event.add( window, "load", jQuery.ready );
-}
-
-jQuery.each( ("blur,focus,load,resize,scroll,unload,click,dblclick," +
-	"mousedown,mouseup,mousemove,mouseover,mouseout,mouseenter,mouseleave," +
-	"change,select,submit,keydown,keypress,keyup,error").split(","), function(i, name){
-
-	// Handle event binding
-	jQuery.fn[name] = function(fn){
-		return fn ? this.bind(name, fn) : this.trigger(name);
-	};
-});
-
-// Prevent memory leaks in IE
-// And prevent errors on refresh with events like mouseover in other browsers
-// Window isn't included so as not to unbind existing unload events
-jQuery( window ).bind( 'unload', function(){ 
-	for ( var id in jQuery.cache )
-		// Skip the window
-		if ( id != 1 && jQuery.cache[ id ].handle )
-			jQuery.event.remove( jQuery.cache[ id ].handle.elem );
-}); 
-(function(){
-
-	jQuery.support = {};
-
-	var root = document.documentElement,
-		script = document.createElement("script"),
-		div = document.createElement("div"),
-		id = "script" + (new Date).getTime();
-
-	div.style.display = "none";
-	div.innerHTML = '   <link/><table></table><a href="/a" style="color:red;float:left;opacity:.5;">a</a><select><option>text</option></select><object><param/></object>';
-
-	var all = div.getElementsByTagName("*"),
-		a = div.getElementsByTagName("a")[0];
-
-	// Can't get basic test support
-	if ( !all || !all.length || !a ) {
-		return;
-	}
-
-	jQuery.support = {
-		// IE strips leading whitespace when .innerHTML is used
-		leadingWhitespace: div.firstChild.nodeType == 3,
-		
-		// Make sure that tbody elements aren't automatically inserted
-		// IE will insert them into empty tables
-		tbody: !div.getElementsByTagName("tbody").length,
-		
-		// Make sure that you can get all elements in an <object> element
-		// IE 7 always returns no results
-		objectAll: !!div.getElementsByTagName("object")[0]
-			.getElementsByTagName("*").length,
-		
-		// Make sure that link elements get serialized correctly by innerHTML
-		// This requires a wrapper element in IE
-		htmlSerialize: !!div.getElementsByTagName("link").length,
-		
-		// Get the style information from getAttribute
-		// (IE uses .cssText insted)
-		style: /red/.test( a.getAttribute("style") ),
-		
-		// Make sure that URLs aren't manipulated
-		// (IE normalizes it by default)
-		hrefNormalized: a.getAttribute("href") === "/a",
-		
-		// Make sure that element opacity exists
-		// (IE uses filter instead)
-		opacity: a.style.opacity === "0.5",
-		
-		// Verify style float existence
-		// (IE uses styleFloat instead of cssFloat)
-		cssFloat: !!a.style.cssFloat,
-
-		// Will be defined later
-		scriptEval: false,
-		noCloneEvent: true,
-		boxModel: null
-	};
-	
-	script.type = "text/javascript";
-	try {
-		script.appendChild( document.createTextNode( "window." + id + "=1;" ) );
-	} catch(e){}
-
-	root.insertBefore( script, root.firstChild );
-	
-	// Make sure that the execution of code works by injecting a script
-	// tag with appendChild/createTextNode
-	// (IE doesn't support this, fails, and uses .text instead)
-	if ( window[ id ] ) {
-		jQuery.support.scriptEval = true;
-		delete window[ id ];
-	}
-
-	root.removeChild( script );
-
-	if ( div.attachEvent && div.fireEvent ) {
-		div.attachEvent("onclick", function(){
-			// Cloning a node shouldn't copy over any
-			// bound event handlers (IE does this)
-			jQuery.support.noCloneEvent = false;
-			div.detachEvent("onclick", arguments.callee);
-		});
-		div.cloneNode(true).fireEvent("onclick");
-	}
-
-	// Figure out if the W3C box model works as expected
-	// document.body must exist before we can do this
-	jQuery(function(){
-		var div = document.createElement("div");
-		div.style.width = div.style.paddingLeft = "1px";
-
-		document.body.appendChild( div );
-		jQuery.boxModel = jQuery.support.boxModel = div.offsetWidth === 2;
-		document.body.removeChild( div ).style.display = 'none';
-	});
-})();
-
-var styleFloat = jQuery.support.cssFloat ? "cssFloat" : "styleFloat";
-
-jQuery.props = {
-	"for": "htmlFor",
-	"class": "className",
-	"float": styleFloat,
-	cssFloat: styleFloat,
-	styleFloat: styleFloat,
-	readonly: "readOnly",
-	maxlength: "maxLength",
-	cellspacing: "cellSpacing",
-	rowspan: "rowSpan",
-	tabindex: "tabIndex"
-};
-jQuery.fn.extend({
-	// Keep a copy of the old load
-	_load: jQuery.fn.load,
-
-	load: function( url, params, callback ) {
-		if ( typeof url !== "string" )
-			return this._load( url );
-
-		var off = url.indexOf(" ");
-		if ( off >= 0 ) {
-			var selector = url.slice(off, url.length);
-			url = url.slice(0, off);
-		}
-
-		// Default to a GET request
-		var type = "GET";
-
-		// If the second parameter was provided
-		if ( params )
-			// If it's a function
-			if ( jQuery.isFunction( params ) ) {
-				// We assume that it's the callback
-				callback = params;
-				params = null;
-
-			// Otherwise, build a param string
-			} else if( typeof params === "object" ) {
-				params = jQuery.param( params );
-				type = "POST";
-			}
-
-		var self = this;
-
-		// Request the remote document
-		jQuery.ajax({
-			url: url,
-			type: type,
-			dataType: "html",
-			data: params,
-			complete: function(res, status){
-				// If successful, inject the HTML into all the matched elements
-				if ( status == "success" || status == "notmodified" )
-					// See if a selector was specified
-					self.html( selector ?
-						// Create a dummy div to hold the results
-						jQuery("<div/>")
-							// inject the contents of the document in, removing the scripts
-							// to avoid any 'Permission Denied' errors in IE
-							.append(res.responseText.replace(/<script(.|\s)*?\/script>/g, ""))
-
-							// Locate the specified elements
-							.find(selector) :
-
-						// If not, just inject the full result
-						res.responseText );
-
-				if( callback )
-					self.each( callback, [res.responseText, status, res] );
-			}
-		});
-		return this;
-	},
-
-	serialize: function() {
-		return jQuery.param(this.serializeArray());
-	},
-	serializeArray: function() {
-		return this.map(function(){
-			return this.elements ? jQuery.makeArray(this.elements) : this;
-		})
-		.filter(function(){
-			return this.name && !this.disabled &&
-				(this.checked || /select|textarea/i.test(this.nodeName) ||
-					/text|hidden|password|search/i.test(this.type));
-		})
-		.map(function(i, elem){
-			var val = jQuery(this).val();
-			return val == null ? null :
-				jQuery.isArray(val) ?
-					jQuery.map( val, function(val, i){
-						return {name: elem.name, value: val};
-					}) :
-					{name: elem.name, value: val};
-		}).get();
-	}
-});
-
-// Attach a bunch of functions for handling common AJAX events
-jQuery.each( "ajaxStart,ajaxStop,ajaxComplete,ajaxError,ajaxSuccess,ajaxSend".split(","), function(i,o){
-	jQuery.fn[o] = function(f){
-		return this.bind(o, f);
-	};
-});
-
-var jsc = now();
-
-jQuery.extend({
-  
-	get: function( url, data, callback, type ) {
-		// shift arguments if data argument was ommited
-		if ( jQuery.isFunction( data ) ) {
-			callback = data;
-			data = null;
-		}
-
-		return jQuery.ajax({
-			type: "GET",
-			url: url,
-			data: data,
-			success: callback,
-			dataType: type
-		});
-	},
-
-	getScript: function( url, callback ) {
-		return jQuery.get(url, null, callback, "script");
-	},
-
-	getJSON: function( url, data, callback ) {
-		return jQuery.get(url, data, callback, "json");
-	},
-
-	post: function( url, data, callback, type ) {
-		if ( jQuery.isFunction( data ) ) {
-			callback = data;
-			data = {};
-		}
-
-		return jQuery.ajax({
-			type: "POST",
-			url: url,
-			data: data,
-			success: callback,
-			dataType: type
-		});
-	},
-
-	ajaxSetup: function( settings ) {
-		jQuery.extend( jQuery.ajaxSettings, settings );
-	},
-
-	ajaxSettings: {
-		url: location.href,
-		global: true,
-		type: "GET",
-		contentType: "application/x-www-form-urlencoded",
-		processData: true,
-		async: true,
-		/*
-		timeout: 0,
-		data: null,
-		username: null,
-		password: null,
-		*/
-		// Create the request object; Microsoft failed to properly
-		// implement the XMLHttpRequest in IE7, so we use the ActiveXObject when it is available
-		// This function can be overriden by calling jQuery.ajaxSetup
-		xhr:function(){
-			return window.ActiveXObject ? new ActiveXObject("Microsoft.XMLHTTP") : new XMLHttpRequest();
-		},
-		accepts: {
-			xml: "application/xml, text/xml",
-			html: "text/html",
-			script: "text/javascript, application/javascript",
-			json: "application/json, text/javascript",
-			text: "text/plain",
-			_default: "*/*"
-		}
-	},
-
-	// Last-Modified header cache for next request
-	lastModified: {},
-
-	ajax: function( s ) {
-		// Extend the settings, but re-extend 's' so that it can be
-		// checked again later (in the test suite, specifically)
-		s = jQuery.extend(true, s, jQuery.extend(true, {}, jQuery.ajaxSettings, s));
-
-		var jsonp, jsre = /=\?(&|$)/g, status, data,
-			type = s.type.toUpperCase();
-
-		// convert data if not already a string
-		if ( s.data && s.processData && typeof s.data !== "string" )
-			s.data = jQuery.param(s.data);
-
-		// Handle JSONP Parameter Callbacks
-		if ( s.dataType == "jsonp" ) {
-			if ( type == "GET" ) {
-				if ( !s.url.match(jsre) )
-					s.url += (s.url.match(/\?/) ? "&" : "?") + (s.jsonp || "callback") + "=?";
-			} else if ( !s.data || !s.data.match(jsre) )
-				s.data = (s.data ? s.data + "&" : "") + (s.jsonp || "callback") + "=?";
-			s.dataType = "json";
-		}
-
-		// Build temporary JSONP function
-		if ( s.dataType == "json" && (s.data && s.data.match(jsre) || s.url.match(jsre)) ) {
-			jsonp = "jsonp" + jsc++;
-
-			// Replace the =? sequence both in the query string and the data
-			if ( s.data )
-				s.data = (s.data + "").replace(jsre, "=" + jsonp + "$1");
-			s.url = s.url.replace(jsre, "=" + jsonp + "$1");
-
-			// We need to make sure
-			// that a JSONP style response is executed properly
-			s.dataType = "script";
-
-			// Handle JSONP-style loading
-			window[ jsonp ] = function(tmp){
-				data = tmp;
-				success();
-				complete();
-				// Garbage collect
-				window[ jsonp ] = undefined;
-				try{ delete window[ jsonp ]; } catch(e){}
-				if ( head )
-					head.removeChild( script );
-			};
-		}
-
-		if ( s.dataType == "script" && s.cache == null )
-			s.cache = false;
-
-		if ( s.cache === false && type == "GET" ) {
-			var ts = now();
-			// try replacing _= if it is there
-			var ret = s.url.replace(/(\?|&)_=.*?(&|$)/, "$1_=" + ts + "$2");
-			// if nothing was replaced, add timestamp to the end
-			s.url = ret + ((ret == s.url) ? (s.url.match(/\?/) ? "&" : "?") + "_=" + ts : "");
-		}
-
-		// If data is available, append data to url for get requests
-		if ( s.data && type == "GET" ) {
-			s.url += (s.url.match(/\?/) ? "&" : "?") + s.data;
-
-			// IE likes to send both get and post data, prevent this
-			s.data = null;
-		}
-
-		// Watch for a new set of requests
-		if ( s.global && ! jQuery.active++ )
-			jQuery.event.trigger( "ajaxStart" );
-
-		// Matches an absolute URL, and saves the domain
-		var parts = /^(\w+:)?\/\/([^\/?#]+)/.exec( s.url );
-
-		// If we're requesting a remote document
-		// and trying to load JSON or Script with a GET
-		if ( s.dataType == "script" && type == "GET" && parts
-			&& ( parts[1] && parts[1] != location.protocol || parts[2] != location.host )){
-
-			var head = document.getElementsByTagName("head")[0];
-			var script = document.createElement("script");
-			script.src = s.url;
-			if (s.scriptCharset)
-				script.charset = s.scriptCharset;
-
-			// Handle Script loading
-			if ( !jsonp ) {
-				var done = false;
-
-				// Attach handlers for all browsers
-				script.onload = script.onreadystatechange = function(){
-					if ( !done && (!this.readyState ||
-							this.readyState == "loaded" || this.readyState == "complete") ) {
-						done = true;
-						success();
-						complete();
-
-						// Handle memory leak in IE
-						script.onload = script.onreadystatechange = null;
-						head.removeChild( script );
-					}
-				};
-			}
-
-			head.appendChild(script);
-
-			// We handle everything using the script element injection
-			return undefined;
-		}
-
-		var requestDone = false;
-
-		// Create the request object
-		var xhr = s.xhr();
-
-		// Open the socket
-		// Passing null username, generates a login popup on Opera (#2865)
-		if( s.username )
-			xhr.open(type, s.url, s.async, s.username, s.password);
-		else
-			xhr.open(type, s.url, s.async);
-
-		// Need an extra try/catch for cross domain requests in Firefox 3
-		try {
-			// Set the correct header, if data is being sent
-			if ( s.data )
-				xhr.setRequestHeader("Content-Type", s.contentType);
-
-			// Set the If-Modified-Since header, if ifModified mode.
-			if ( s.ifModified )
-				xhr.setRequestHeader("If-Modified-Since",
-					jQuery.lastModified[s.url] || "Thu, 01 Jan 1970 00:00:00 GMT" );
-
-			// Set header so the called script knows that it's an XMLHttpRequest
-			xhr.setRequestHeader("X-Requested-With", "XMLHttpRequest");
-
-			// Set the Accepts header for the server, depending on the dataType
-			xhr.setRequestHeader("Accept", s.dataType && s.accepts[ s.dataType ] ?
-				s.accepts[ s.dataType ] + ", */*" :
-				s.accepts._default );
-		} catch(e){}
-
-		// Allow custom headers/mimetypes and early abort
-		if ( s.beforeSend && s.beforeSend(xhr, s) === false ) {
-			// Handle the global AJAX counter
-			if ( s.global && ! --jQuery.active )
-				jQuery.event.trigger( "ajaxStop" );
-			// close opended socket
-			xhr.abort();
-			return false;
-		}
-
-		if ( s.global )
-			jQuery.event.trigger("ajaxSend", [xhr, s]);
-
-		// Wait for a response to come back
-		var onreadystatechange = function(isTimeout){
-			// The request was aborted, clear the interval and decrement jQuery.active
-			if (xhr.readyState == 0) {
-				if (ival) {
-					// clear poll interval
-					clearInterval(ival);
-					ival = null;
-					// Handle the global AJAX counter
-					if ( s.global && ! --jQuery.active )
-						jQuery.event.trigger( "ajaxStop" );
-				}
-			// The transfer is complete and the data is available, or the request timed out
-			} else if ( !requestDone && xhr && (xhr.readyState == 4 || isTimeout == "timeout") ) {
-				requestDone = true;
-
-				// clear poll interval
-				if (ival) {
-					clearInterval(ival);
-					ival = null;
-				}
-
-				status = isTimeout == "timeout" ? "timeout" :
-					!jQuery.httpSuccess( xhr ) ? "error" :
-					s.ifModified && jQuery.httpNotModified( xhr, s.url ) ? "notmodified" :
-					"success";
-
-				if ( status == "success" ) {
-					// Watch for, and catch, XML document parse errors
-					try {
-						// process the data (runs the xml through httpData regardless of callback)
-						data = jQuery.httpData( xhr, s.dataType, s );
-					} catch(e) {
-						status = "parsererror";
-					}
-				}
-
-				// Make sure that the request was successful or notmodified
-				if ( status == "success" ) {
-					// Cache Last-Modified header, if ifModified mode.
-					var modRes;
-					try {
-						modRes = xhr.getResponseHeader("Last-Modified");
-					} catch(e) {} // swallow exception thrown by FF if header is not available
-
-					if ( s.ifModified && modRes )
-						jQuery.lastModified[s.url] = modRes;
-
-					// JSONP handles its own success callback
-					if ( !jsonp )
-						success();
-				} else
-					jQuery.handleError(s, xhr, status);
-
-				// Fire the complete handlers
-				complete();
-
-				if ( isTimeout )
-					xhr.abort();
-
-				// Stop memory leaks
-				if ( s.async )
-					xhr = null;
-			}
-		};
-
-		if ( s.async ) {
-			// don't attach the handler to the request, just poll it instead
-			var ival = setInterval(onreadystatechange, 13);
-
-			// Timeout checker
-			if ( s.timeout > 0 )
-				setTimeout(function(){
-					// Check to see if the request is still happening
-					if ( xhr && !requestDone )
-						onreadystatechange( "timeout" );
-				}, s.timeout);
-		}
-
-		// Send the data
-		try {
-			xhr.send(s.data);
-		} catch(e) {
-			jQuery.handleError(s, xhr, null, e);
-		}
-
-		// firefox 1.5 doesn't fire statechange for sync requests
-		if ( !s.async )
-			onreadystatechange();
-
-		function success(){
-			// If a local callback was specified, fire it and pass it the data
-			if ( s.success )
-				s.success( data, status );
-
-			// Fire the global callback
-			if ( s.global )
-				jQuery.event.trigger( "ajaxSuccess", [xhr, s] );
-		}
-
-		function complete(){
-			// Process result
-			if ( s.complete )
-				s.complete(xhr, status);
-
-			// The request was completed
-			if ( s.global )
-				jQuery.event.trigger( "ajaxComplete", [xhr, s] );
-
-			// Handle the global AJAX counter
-			if ( s.global && ! --jQuery.active )
-				jQuery.event.trigger( "ajaxStop" );
-		}
-
-		// return XMLHttpRequest to allow aborting the request etc.
-		return xhr;
-	},
-
-	handleError: function( s, xhr, status, e ) {
-		// If a local callback was specified, fire it
-		if ( s.error ) s.error( xhr, status, e );
-
-		// Fire the global callback
-		if ( s.global )
-			jQuery.event.trigger( "ajaxError", [xhr, s, e] );
-	},
-
-	// Counter for holding the number of active queries
-	active: 0,
-
-	// Determines if an XMLHttpRequest was successful or not
-	httpSuccess: function( xhr ) {
-		try {
-			// IE error sometimes returns 1223 when it should be 204 so treat it as success, see #1450
-			return !xhr.status && location.protocol == "file:" ||
-				( xhr.status >= 200 && xhr.status < 300 ) || xhr.status == 304 || xhr.status == 1223;
-		} catch(e){}
-		return false;
-	},
-
-	// Determines if an XMLHttpRequest returns NotModified
-	httpNotModified: function( xhr, url ) {
-		try {
-			var xhrRes = xhr.getResponseHeader("Last-Modified");
-
-			// Firefox always returns 200. check Last-Modified date
-			return xhr.status == 304 || xhrRes == jQuery.lastModified[url];
-		} catch(e){}
-		return false;
-	},
-
-	httpData: function( xhr, type, s ) {
-		var ct = xhr.getResponseHeader("content-type"),
-			xml = type == "xml" || !type && ct && ct.indexOf("xml") >= 0,
-			data = xml ? xhr.responseXML : xhr.responseText;
-
-		if ( xml && data.documentElement.tagName == "parsererror" )
-			throw "parsererror";
-			
-		// Allow a pre-filtering function to sanitize the response
-		// s != null is checked to keep backwards compatibility
-		if( s && s.dataFilter )
-			data = s.dataFilter( data, type );
-
-		// The filter can actually parse the response
-		if( typeof data === "string" ){
-
-			// If the type is "script", eval it in global context
-			if ( type == "script" )
-				jQuery.globalEval( data );
-
-			// Get the JavaScript object, if JSON is used.
-			if ( type == "json" )
-				data = window["eval"]("(" + data + ")");
-		}
-		
-		return data;
-	},
-
-	// Serialize an array of form elements or a set of
-	// key/values into a query string
-	param: function( a ) {
-		var s = [ ];
-
-		function add( key, value ){
-			s[ s.length ] = encodeURIComponent(key) + '=' + encodeURIComponent(value);
-		};
-
-		// If an array was passed in, assume that it is an array
-		// of form elements
-		if ( jQuery.isArray(a) || a.jquery )
-			// Serialize the form elements
-			jQuery.each( a, function(){
-				add( this.name, this.value );
-			});
-
-		// Otherwise, assume that it's an object of key/value pairs
-		else
-			// Serialize the key/values
-			for ( var j in a )
-				// If the value is an array then the key names need to be repeated
-				if ( jQuery.isArray(a[j]) )
-					jQuery.each( a[j], function(){
-						add( j, this );
-					});
-				else
-					add( j, jQuery.isFunction(a[j]) ? a[j]() : a[j] );
-
-		// Return the resulting serialization
-		return s.join("&").replace(/%20/g, "+");
-	}
-
-});
-var elemdisplay = {},
-	timerId,
-	fxAttrs = [
-		// height animations
-		[ "height", "marginTop", "marginBottom", "paddingTop", "paddingBottom" ],
-		// width animations
-		[ "width", "marginLeft", "marginRight", "paddingLeft", "paddingRight" ],
-		// opacity animations
-		[ "opacity" ]
-	];
-
-function genFx( type, num ){
-	var obj = {};
-	jQuery.each( fxAttrs.concat.apply([], fxAttrs.slice(0,num)), function(){
-		obj[ this ] = type;
-	});
-	return obj;
-}
-
-jQuery.fn.extend({
-	show: function(speed,callback){
-		if ( speed ) {
-			return this.animate( genFx("show", 3), speed, callback);
-		} else {
-			for ( var i = 0, l = this.length; i < l; i++ ){
-				var old = jQuery.data(this[i], "olddisplay");
-				
-				this[i].style.display = old || "";
-				
-				if ( jQuery.css(this[i], "display") === "none" ) {
-					var tagName = this[i].tagName, display;
-					
-					if ( elemdisplay[ tagName ] ) {
-						display = elemdisplay[ tagName ];
-					} else {
-						var elem = jQuery("<" + tagName + " />").appendTo("body");
-						
-						display = elem.css("display");
-						if ( display === "none" )
-							display = "block";
-						
-						elem.remove();
-						
-						elemdisplay[ tagName ] = display;
-					}
-					
-					jQuery.data(this[i], "olddisplay", display);
-				}
-			}
-
-			// Set the display of the elements in a second loop
-			// to avoid the constant reflow
-			for ( var i = 0, l = this.length; i < l; i++ ){
-				this[i].style.display = jQuery.data(this[i], "olddisplay") || "";
-			}
-			
-			return this;
-		}
-	},
-
-	hide: function(speed,callback){
-		if ( speed ) {
-			return this.animate( genFx("hide", 3), speed, callback);
-		} else {
-			for ( var i = 0, l = this.length; i < l; i++ ){
-				var old = jQuery.data(this[i], "olddisplay");
-				if ( !old && old !== "none" )
-					jQuery.data(this[i], "olddisplay", jQuery.css(this[i], "display"));
-			}
-
-			// Set the display of the elements in a second loop
-			// to avoid the constant reflow
-			for ( var i = 0, l = this.length; i < l; i++ ){
-				this[i].style.display = "none";
-			}
-
-			return this;
-		}
-	},
-
-	// Save the old toggle function
-	_toggle: jQuery.fn.toggle,
-
-	toggle: function( fn, fn2 ){
-		var bool = typeof fn === "boolean";
-
-		return jQuery.isFunction(fn) && jQuery.isFunction(fn2) ?
-			this._toggle.apply( this, arguments ) :
-			fn == null || bool ?
-				this.each(function(){
-					var state = bool ? fn : jQuery(this).is(":hidden");
-					jQuery(this)[ state ? "show" : "hide" ]();
-				}) :
-				this.animate(genFx("toggle", 3), fn, fn2);
-	},
-
-	fadeTo: function(speed,to,callback){
-		return this.animate({opacity: to}, speed, callback);
-	},
-
-	animate: function( prop, speed, easing, callback ) {
-		var optall = jQuery.speed(speed, easing, callback);
-
-		return this[ optall.queue === false ? "each" : "queue" ](function(){
-		
-			var opt = jQuery.extend({}, optall), p,
-				hidden = this.nodeType == 1 && jQuery(this).is(":hidden"),
-				self = this;
-	
-			for ( p in prop ) {
-				if ( prop[p] == "hide" && hidden || prop[p] == "show" && !hidden )
-					return opt.complete.call(this);
-
-				if ( ( p == "height" || p == "width" ) && this.style ) {
-					// Store display property
-					opt.display = jQuery.css(this, "display");
-
-					// Make sure that nothing sneaks out
-					opt.overflow = this.style.overflow;
-				}
-			}
-
-			if ( opt.overflow != null )
-				this.style.overflow = "hidden";
-
-			opt.curAnim = jQuery.extend({}, prop);
-
-			jQuery.each( prop, function(name, val){
-				var e = new jQuery.fx( self, opt, name );
-
-				if ( /toggle|show|hide/.test(val) )
-					e[ val == "toggle" ? hidden ? "show" : "hide" : val ]( prop );
-				else {
-					var parts = val.toString().match(/^([+-]=)?([\d+-.]+)(.*)$/),
-						start = e.cur(true) || 0;
-
-					if ( parts ) {
-						var end = parseFloat(parts[2]),
-							unit = parts[3] || "px";
-
-						// We need to compute starting value
-						if ( unit != "px" ) {
-							self.style[ name ] = (end || 1) + unit;
-							start = ((end || 1) / e.cur(true)) * start;
-							self.style[ name ] = start + unit;
-						}
-
-						// If a +=/-= token was provided, we're doing a relative animation
-						if ( parts[1] )
-							end = ((parts[1] == "-=" ? -1 : 1) * end) + start;
-
-						e.custom( start, end, unit );
-					} else
-						e.custom( start, val, "" );
-				}
-			});
-
-			// For JS strict compliance
-			return true;
-		});
-	},
-
-	stop: function(clearQueue, gotoEnd){
-		var timers = jQuery.timers;
-
-		if (clearQueue)
-			this.queue([]);
-
-		this.each(function(){
-			// go in reverse order so anything added to the queue during the loop is ignored
-			for ( var i = timers.length - 1; i >= 0; i-- )
-				if ( timers[i].elem == this ) {
-					if (gotoEnd)
-						// force the next step to be the last
-						timers[i](true);
-					timers.splice(i, 1);
-				}
-		});
-
-		// start the next in the queue if the last step wasn't forced
-		if (!gotoEnd)
-			this.dequeue();
-
-		return this;
-	}
-
-});
-
-// Generate shortcuts for custom animations
-jQuery.each({
-	slideDown: genFx("show", 1),
-	slideUp: genFx("hide", 1),
-	slideToggle: genFx("toggle", 1),
-	fadeIn: { opacity: "show" },
-	fadeOut: { opacity: "hide" }
-}, function( name, props ){
-	jQuery.fn[ name ] = function( speed, callback ){
-		return this.animate( props, speed, callback );
-	};
-});
-
-jQuery.extend({
-
-	speed: function(speed, easing, fn) {
-		var opt = typeof speed === "object" ? speed : {
-			complete: fn || !fn && easing ||
-				jQuery.isFunction( speed ) && speed,
-			duration: speed,
-			easing: fn && easing || easing && !jQuery.isFunction(easing) && easing
-		};
-
-		opt.duration = jQuery.fx.off ? 0 : typeof opt.duration === "number" ? opt.duration :
-			jQuery.fx.speeds[opt.duration] || jQuery.fx.speeds._default;
-
-		// Queueing
-		opt.old = opt.complete;
-		opt.complete = function(){
-			if ( opt.queue !== false )
-				jQuery(this).dequeue();
-			if ( jQuery.isFunction( opt.old ) )
-				opt.old.call( this );
-		};
-
-		return opt;
-	},
-
-	easing: {
-		linear: function( p, n, firstNum, diff ) {
-			return firstNum + diff * p;
-		},
-		swing: function( p, n, firstNum, diff ) {
-			return ((-Math.cos(p*Math.PI)/2) + 0.5) * diff + firstNum;
-		}
-	},
-
-	timers: [],
-
-	fx: function( elem, options, prop ){
-		this.options = options;
-		this.elem = elem;
-		this.prop = prop;
-
-		if ( !options.orig )
-			options.orig = {};
-	}
-
-});
-
-jQuery.fx.prototype = {
-
-	// Simple function for setting a style value
-	update: function(){
-		if ( this.options.step )
-			this.options.step.call( this.elem, this.now, this );
-
-		(jQuery.fx.step[this.prop] || jQuery.fx.step._default)( this );
-
-		// Set display property to block for height/width animations
-		if ( ( this.prop == "height" || this.prop == "width" ) && this.elem.style )
-			this.elem.style.display = "block";
-	},
-
-	// Get the current size
-	cur: function(force){
-		if ( this.elem[this.prop] != null && (!this.elem.style || this.elem.style[this.prop] == null) )
-			return this.elem[ this.prop ];
-
-		var r = parseFloat(jQuery.css(this.elem, this.prop, force));
-		return r && r > -10000 ? r : parseFloat(jQuery.curCSS(this.elem, this.prop)) || 0;
-	},
-
-	// Start an animation from one number to another
-	custom: function(from, to, unit){
-		this.startTime = now();
-		this.start = from;
-		this.end = to;
-		this.unit = unit || this.unit || "px";
-		this.now = this.start;
-		this.pos = this.state = 0;
-
-		var self = this;
-		function t(gotoEnd){
-			return self.step(gotoEnd);
-		}
-
-		t.elem = this.elem;
-
-		if ( t() && jQuery.timers.push(t) && !timerId ) {
-			timerId = setInterval(function(){
-				var timers = jQuery.timers;
-
-				for ( var i = 0; i < timers.length; i++ )
-					if ( !timers[i]() )
-						timers.splice(i--, 1);
-
-				if ( !timers.length ) {
-					clearInterval( timerId );
-					timerId = undefined;
-				}
-			}, 13);
-		}
-	},
-
-	// Simple 'show' function
-	show: function(){
-		// Remember where we started, so that we can go back to it later
-		this.options.orig[this.prop] = jQuery.attr( this.elem.style, this.prop );
-		this.options.show = true;
-
-		// Begin the animation
-		// Make sure that we start at a small width/height to avoid any
-		// flash of content
-		this.custom(this.prop == "width" || this.prop == "height" ? 1 : 0, this.cur());
-
-		// Start by showing the element
-		jQuery(this.elem).show();
-	},
-
-	// Simple 'hide' function
-	hide: function(){
-		// Remember where we started, so that we can go back to it later
-		this.options.orig[this.prop] = jQuery.attr( this.elem.style, this.prop );
-		this.options.hide = true;
-
-		// Begin the animation
-		this.custom(this.cur(), 0);
-	},
-
-	// Each step of an animation
-	step: function(gotoEnd){
-		var t = now();
-
-		if ( gotoEnd || t >= this.options.duration + this.startTime ) {
-			this.now = this.end;
-			this.pos = this.state = 1;
-			this.update();
-
-			this.options.curAnim[ this.prop ] = true;
-
-			var done = true;
-			for ( var i in this.options.curAnim )
-				if ( this.options.curAnim[i] !== true )
-					done = false;
-
-			if ( done ) {
-				if ( this.options.display != null ) {
-					// Reset the overflow
-					this.elem.style.overflow = this.options.overflow;
-
-					// Reset the display
-					this.elem.style.display = this.options.display;
-					if ( jQuery.css(this.elem, "display") == "none" )
-						this.elem.style.display = "block";
-				}
-
-				// Hide the element if the "hide" operation was done
-				if ( this.options.hide )
-					jQuery(this.elem).hide();
-
-				// Reset the properties, if the item has been hidden or shown
-				if ( this.options.hide || this.options.show )
-					for ( var p in this.options.curAnim )
-						jQuery.attr(this.elem.style, p, this.options.orig[p]);
-					
-				// Execute the complete function
-				this.options.complete.call( this.elem );
-			}
-
-			return false;
-		} else {
-			var n = t - this.startTime;
-			this.state = n / this.options.duration;
-
-			// Perform the easing function, defaults to swing
-			this.pos = jQuery.easing[this.options.easing || (jQuery.easing.swing ? "swing" : "linear")](this.state, n, 0, 1, this.options.duration);
-			this.now = this.start + ((this.end - this.start) * this.pos);
-
-			// Perform the next step of the animation
-			this.update();
-		}
-
-		return true;
-	}
-
-};
-
-jQuery.extend( jQuery.fx, {
-	speeds:{
-		slow: 600,
- 		fast: 200,
- 		// Default speed
- 		_default: 400
-	},
-	step: {
-
-		opacity: function(fx){
-			jQuery.attr(fx.elem.style, "opacity", fx.now);
-		},
-
-		_default: function(fx){
-			if ( fx.elem.style && fx.elem.style[ fx.prop ] != null )
-				fx.elem.style[ fx.prop ] = fx.now + fx.unit;
-			else
-				fx.elem[ fx.prop ] = fx.now;
-		}
-	}
-});
-if ( document.documentElement["getBoundingClientRect"] )
-	jQuery.fn.offset = function() {
-		if ( !this[0] ) return { top: 0, left: 0 };
-		if ( this[0] === this[0].ownerDocument.body ) return jQuery.offset.bodyOffset( this[0] );
-		var box  = this[0].getBoundingClientRect(), doc = this[0].ownerDocument, body = doc.body, docElem = doc.documentElement,
-			clientTop = docElem.clientTop || body.clientTop || 0, clientLeft = docElem.clientLeft || body.clientLeft || 0,
-			top  = box.top  + (self.pageYOffset || jQuery.boxModel && docElem.scrollTop  || body.scrollTop ) - clientTop,
-			left = box.left + (self.pageXOffset || jQuery.boxModel && docElem.scrollLeft || body.scrollLeft) - clientLeft;
-		return { top: top, left: left };
-	};
-else 
-	jQuery.fn.offset = function() {
-		if ( !this[0] ) return { top: 0, left: 0 };
-		if ( this[0] === this[0].ownerDocument.body ) return jQuery.offset.bodyOffset( this[0] );
-		jQuery.offset.initialized || jQuery.offset.initialize();
-
-		var elem = this[0], offsetParent = elem.offsetParent, prevOffsetParent = elem,
-			doc = elem.ownerDocument, computedStyle, docElem = doc.documentElement,
-			body = doc.body, defaultView = doc.defaultView,
-			prevComputedStyle = defaultView.getComputedStyle(elem, null),
-			top = elem.offsetTop, left = elem.offsetLeft;
-
-		while ( (elem = elem.parentNode) && elem !== body && elem !== docElem ) {
-			computedStyle = defaultView.getComputedStyle(elem, null);
-			top -= elem.scrollTop, left -= elem.scrollLeft;
-			if ( elem === offsetParent ) {
-				top += elem.offsetTop, left += elem.offsetLeft;
-				if ( jQuery.offset.doesNotAddBorder && !(jQuery.offset.doesAddBorderForTableAndCells && /^t(able|d|h)$/i.test(elem.tagName)) )
-					top  += parseInt( computedStyle.borderTopWidth,  10) || 0,
-					left += parseInt( computedStyle.borderLeftWidth, 10) || 0;
-				prevOffsetParent = offsetParent, offsetParent = elem.offsetParent;
-			}
-			if ( jQuery.offset.subtractsBorderForOverflowNotVisible && computedStyle.overflow !== "visible" )
-				top  += parseInt( computedStyle.borderTopWidth,  10) || 0,
-				left += parseInt( computedStyle.borderLeftWidth, 10) || 0;
-			prevComputedStyle = computedStyle;
-		}
-
-		if ( prevComputedStyle.position === "relative" || prevComputedStyle.position === "static" )
-			top  += body.offsetTop,
-			left += body.offsetLeft;
-
-		if ( prevComputedStyle.position === "fixed" )
-			top  += Math.max(docElem.scrollTop, body.scrollTop),
-			left += Math.max(docElem.scrollLeft, body.scrollLeft);
-
-		return { top: top, left: left };
-	};
-
-jQuery.offset = {
-	initialize: function() {
-		if ( this.initialized ) return;
-		var body = document.body, container = document.createElement('div'), innerDiv, checkDiv, table, td, rules, prop, bodyMarginTop = body.style.marginTop,
-			html = '<div style="position:absolute;top:0;left:0;margin:0;border:5px solid #000;padding:0;width:1px;height:1px;"><div></div></div><table style="position:absolute;top:0;left:0;margin:0;border:5px solid #000;padding:0;width:1px;height:1px;" cellpadding="0" cellspacing="0"><tr><td></td></tr></table>';
-
-		rules = { position: 'absolute', top: 0, left: 0, margin: 0, border: 0, width: '1px', height: '1px', visibility: 'hidden' };
-		for ( prop in rules ) container.style[prop] = rules[prop];
-
-		container.innerHTML = html;
-		body.insertBefore(container, body.firstChild);
-		innerDiv = container.firstChild, checkDiv = innerDiv.firstChild, td = innerDiv.nextSibling.firstChild.firstChild;
-
-		this.doesNotAddBorder = (checkDiv.offsetTop !== 5);
-		this.doesAddBorderForTableAndCells = (td.offsetTop === 5);
-
-		innerDiv.style.overflow = 'hidden', innerDiv.style.position = 'relative';
-		this.subtractsBorderForOverflowNotVisible = (checkDiv.offsetTop === -5);
-
-		body.style.marginTop = '1px';
-		this.doesNotIncludeMarginInBodyOffset = (body.offsetTop === 0);
-		body.style.marginTop = bodyMarginTop;
-
-		body.removeChild(container);
-		this.initialized = true;
-	},
-
-	bodyOffset: function(body) {
-		jQuery.offset.initialized || jQuery.offset.initialize();
-		var top = body.offsetTop, left = body.offsetLeft;
-		if ( jQuery.offset.doesNotIncludeMarginInBodyOffset )
-			top  += parseInt( jQuery.curCSS(body, 'marginTop',  true), 10 ) || 0,
-			left += parseInt( jQuery.curCSS(body, 'marginLeft', true), 10 ) || 0;
-		return { top: top, left: left };
-	}
-};
-
-
-jQuery.fn.extend({
-	position: function() {
-		var left = 0, top = 0, results;
-
-		if ( this[0] ) {
-			// Get *real* offsetParent
-			var offsetParent = this.offsetParent(),
-
-			// Get correct offsets
-			offset       = this.offset(),
-			parentOffset = /^body|html$/i.test(offsetParent[0].tagName) ? { top: 0, left: 0 } : offsetParent.offset();
-
-			// Subtract element margins
-			// note: when an element has margin: auto the offsetLeft and marginLeft 
-			// are the same in Safari causing offset.left to incorrectly be 0
-			offset.top  -= num( this, 'marginTop'  );
-			offset.left -= num( this, 'marginLeft' );
-
-			// Add offsetParent borders
-			parentOffset.top  += num( offsetParent, 'borderTopWidth'  );
-			parentOffset.left += num( offsetParent, 'borderLeftWidth' );
-
-			// Subtract the two offsets
-			results = {
-				top:  offset.top  - parentOffset.top,
-				left: offset.left - parentOffset.left
-			};
-		}
-
-		return results;
-	},
-
-	offsetParent: function() {
-		var offsetParent = this[0].offsetParent || document.body;
-		while ( offsetParent && (!/^body|html$/i.test(offsetParent.tagName) && jQuery.css(offsetParent, 'position') == 'static') )
-			offsetParent = offsetParent.offsetParent;
-		return jQuery(offsetParent);
-	}
-});
-
-
-// Create scrollLeft and scrollTop methods
-jQuery.each( ['Left', 'Top'], function(i, name) {
-	var method = 'scroll' + name;
-	
-	jQuery.fn[ method ] = function(val) {
-		if (!this[0]) return null;
-
-		return val !== undefined ?
-
-			// Set the scroll offset
-			this.each(function() {
-				this == window || this == document ?
-					window.scrollTo(
-						!i ? val : jQuery(window).scrollLeft(),
-						 i ? val : jQuery(window).scrollTop()
-					) :
-					this[ method ] = val;
-			}) :
-
-			// Return the scroll offset
-			this[0] == window || this[0] == document ?
-				self[ i ? 'pageYOffset' : 'pageXOffset' ] ||
-					jQuery.boxModel && document.documentElement[ method ] ||
-					document.body[ method ] :
-				this[0][ method ];
-	};
-});
-// Create innerHeight, innerWidth, outerHeight and outerWidth methods
-jQuery.each([ "Height", "Width" ], function(i, name){
-
-	var tl = i ? "Left"  : "Top",  // top or left
-		br = i ? "Right" : "Bottom", // bottom or right
-		lower = name.toLowerCase();
-
-	// innerHeight and innerWidth
-	jQuery.fn["inner" + name] = function(){
-		return this[0] ?
-			jQuery.css( this[0], lower, false, "padding" ) :
-			null;
-	};
-
-	// outerHeight and outerWidth
-	jQuery.fn["outer" + name] = function(margin) {
-		return this[0] ?
-			jQuery.css( this[0], lower, false, margin ? "margin" : "border" ) :
-			null;
-	};
-	
-	var type = name.toLowerCase();
-
-	jQuery.fn[ type ] = function( size ) {
-		// Get window width or height
-		return this[0] == window ?
-			// Everyone else use document.documentElement or document.body depending on Quirks vs Standards mode
-			document.compatMode == "CSS1Compat" && document.documentElement[ "client" + name ] ||
-			document.body[ "client" + name ] :
-
-			// Get document width or height
-			this[0] == document ?
-				// Either scroll[Width/Height] or offset[Width/Height], whichever is greater
-				Math.max(
-					document.documentElement["client" + name],
-					document.body["scroll" + name], document.documentElement["scroll" + name],
-					document.body["offset" + name], document.documentElement["offset" + name]
-				) :
-
-				// Get or set width or height on the element
-				size === undefined ?
-					// Get width or height on the element
-					(this.length ? jQuery.css( this[0], type ) : null) :
-
-					// Set the width or height on the element (default to pixels if value is unitless)
-					this.css( type, typeof size === "string" ? size : size + "px" );
-	};
-
-});
-})();
diff --git a/lib/logstash/web/public/js/flot/jquery.min.js b/lib/logstash/web/public/js/flot/jquery.min.js
deleted file mode 100644
index d8cc8082037..00000000000
--- a/lib/logstash/web/public/js/flot/jquery.min.js
+++ /dev/null
@@ -1,19 +0,0 @@
-/*
- * jQuery JavaScript Library v1.3.2
- * http://jquery.com/
- *
- * Copyright (c) 2009 John Resig
- * Dual licensed under the MIT and GPL licenses.
- * http://docs.jquery.com/License
- *
- * Date: 2009-02-19 17:34:21 -0500 (Thu, 19 Feb 2009)
- * Revision: 6246
- */
-(function(){var window=this,undefined,_jQuery=window.jQuery,_$=window.$,jQuery=window.jQuery=window.$=function(selector,context){return new jQuery.fn.init(selector,context)},quickExpr=/^[^<]*(<(.|\s)+>)[^>]*$|^#([\w-]+)$/,isSimple=/^.[^:#\[\.,]*$/;jQuery.fn=jQuery.prototype={init:function(selector,context){selector=selector||document;if(selector.nodeType){this[0]=selector;this.length=1;this.context=selector;return this}if(typeof selector==="string"){var match=quickExpr.exec(selector);if(match&&(match[1]||!context)){if(match[1]){selector=jQuery.clean([match[1]],context)}else{var elem=document.getElementById(match[3]);if(elem&&elem.id!=match[3]){return jQuery().find(selector)}var ret=jQuery(elem||[]);ret.context=document;ret.selector=selector;return ret}}else{return jQuery(context).find(selector)}}else{if(jQuery.isFunction(selector)){return jQuery(document).ready(selector)}}if(selector.selector&&selector.context){this.selector=selector.selector;this.context=selector.context}return this.setArray(jQuery.isArray(selector)?selector:jQuery.makeArray(selector))},selector:"",jquery:"1.3.2",size:function(){return this.length},get:function(num){return num===undefined?Array.prototype.slice.call(this):this[num]},pushStack:function(elems,name,selector){var ret=jQuery(elems);ret.prevObject=this;ret.context=this.context;if(name==="find"){ret.selector=this.selector+(this.selector?" ":"")+selector}else{if(name){ret.selector=this.selector+"."+name+"("+selector+")"}}return ret},setArray:function(elems){this.length=0;Array.prototype.push.apply(this,elems);return this},each:function(callback,args){return jQuery.each(this,callback,args)},index:function(elem){return jQuery.inArray(elem&&elem.jquery?elem[0]:elem,this)},attr:function(name,value,type){var options=name;if(typeof name==="string"){if(value===undefined){return this[0]&&jQuery[type||"attr"](this[0],name)}else{options={};options[name]=value}}return this.each(function(i){for(name in options){jQuery.attr(type?this.style:this,name,jQuery.prop(this,options[name],type,i,name))}})},css:function(key,value){if((key=="width"||key=="height")&&parseFloat(value)<0){value=undefined}return this.attr(key,value,"curCSS")},text:function(text){if(typeof text!=="object"&&text!=null){return this.empty().append((this[0]&&this[0].ownerDocument||document).createTextNode(text))}var ret="";jQuery.each(text||this,function(){jQuery.each(this.childNodes,function(){if(this.nodeType!=8){ret+=this.nodeType!=1?this.nodeValue:jQuery.fn.text([this])}})});return ret},wrapAll:function(html){if(this[0]){var wrap=jQuery(html,this[0].ownerDocument).clone();if(this[0].parentNode){wrap.insertBefore(this[0])}wrap.map(function(){var elem=this;while(elem.firstChild){elem=elem.firstChild}return elem}).append(this)}return this},wrapInner:function(html){return this.each(function(){jQuery(this).contents().wrapAll(html)})},wrap:function(html){return this.each(function(){jQuery(this).wrapAll(html)})},append:function(){return this.domManip(arguments,true,function(elem){if(this.nodeType==1){this.appendChild(elem)}})},prepend:function(){return this.domManip(arguments,true,function(elem){if(this.nodeType==1){this.insertBefore(elem,this.firstChild)}})},before:function(){return this.domManip(arguments,false,function(elem){this.parentNode.insertBefore(elem,this)})},after:function(){return this.domManip(arguments,false,function(elem){this.parentNode.insertBefore(elem,this.nextSibling)})},end:function(){return this.prevObject||jQuery([])},push:[].push,sort:[].sort,splice:[].splice,find:function(selector){if(this.length===1){var ret=this.pushStack([],"find",selector);ret.length=0;jQuery.find(selector,this[0],ret);return ret}else{return this.pushStack(jQuery.unique(jQuery.map(this,function(elem){return jQuery.find(selector,elem)})),"find",selector)}},clone:function(events){var ret=this.map(function(){if(!jQuery.support.noCloneEvent&&!jQuery.isXMLDoc(this)){var html=this.outerHTML;if(!html){var div=this.ownerDocument.createElement("div");div.appendChild(this.cloneNode(true));html=div.innerHTML}return jQuery.clean([html.replace(/ jQuery\d+="(?:\d+|null)"/g,"").replace(/^\s*/,"")])[0]}else{return this.cloneNode(true)}});if(events===true){var orig=this.find("*").andSelf(),i=0;ret.find("*").andSelf().each(function(){if(this.nodeName!==orig[i].nodeName){return }var events=jQuery.data(orig[i],"events");for(var type in events){for(var handler in events[type]){jQuery.event.add(this,type,events[type][handler],events[type][handler].data)}}i++})}return ret},filter:function(selector){return this.pushStack(jQuery.isFunction(selector)&&jQuery.grep(this,function(elem,i){return selector.call(elem,i)})||jQuery.multiFilter(selector,jQuery.grep(this,function(elem){return elem.nodeType===1})),"filter",selector)},closest:function(selector){var pos=jQuery.expr.match.POS.test(selector)?jQuery(selector):null,closer=0;return this.map(function(){var cur=this;while(cur&&cur.ownerDocument){if(pos?pos.index(cur)>-1:jQuery(cur).is(selector)){jQuery.data(cur,"closest",closer);return cur}cur=cur.parentNode;closer++}})},not:function(selector){if(typeof selector==="string"){if(isSimple.test(selector)){return this.pushStack(jQuery.multiFilter(selector,this,true),"not",selector)}else{selector=jQuery.multiFilter(selector,this)}}var isArrayLike=selector.length&&selector[selector.length-1]!==undefined&&!selector.nodeType;return this.filter(function(){return isArrayLike?jQuery.inArray(this,selector)<0:this!=selector})},add:function(selector){return this.pushStack(jQuery.unique(jQuery.merge(this.get(),typeof selector==="string"?jQuery(selector):jQuery.makeArray(selector))))},is:function(selector){return !!selector&&jQuery.multiFilter(selector,this).length>0},hasClass:function(selector){return !!selector&&this.is("."+selector)},val:function(value){if(value===undefined){var elem=this[0];if(elem){if(jQuery.nodeName(elem,"option")){return(elem.attributes.value||{}).specified?elem.value:elem.text}if(jQuery.nodeName(elem,"select")){var index=elem.selectedIndex,values=[],options=elem.options,one=elem.type=="select-one";if(index<0){return null}for(var i=one?index:0,max=one?index+1:options.length;i<max;i++){var option=options[i];if(option.selected){value=jQuery(option).val();if(one){return value}values.push(value)}}return values}return(elem.value||"").replace(/\r/g,"")}return undefined}if(typeof value==="number"){value+=""}return this.each(function(){if(this.nodeType!=1){return }if(jQuery.isArray(value)&&/radio|checkbox/.test(this.type)){this.checked=(jQuery.inArray(this.value,value)>=0||jQuery.inArray(this.name,value)>=0)}else{if(jQuery.nodeName(this,"select")){var values=jQuery.makeArray(value);jQuery("option",this).each(function(){this.selected=(jQuery.inArray(this.value,values)>=0||jQuery.inArray(this.text,values)>=0)});if(!values.length){this.selectedIndex=-1}}else{this.value=value}}})},html:function(value){return value===undefined?(this[0]?this[0].innerHTML.replace(/ jQuery\d+="(?:\d+|null)"/g,""):null):this.empty().append(value)},replaceWith:function(value){return this.after(value).remove()},eq:function(i){return this.slice(i,+i+1)},slice:function(){return this.pushStack(Array.prototype.slice.apply(this,arguments),"slice",Array.prototype.slice.call(arguments).join(","))},map:function(callback){return this.pushStack(jQuery.map(this,function(elem,i){return callback.call(elem,i,elem)}))},andSelf:function(){return this.add(this.prevObject)},domManip:function(args,table,callback){if(this[0]){var fragment=(this[0].ownerDocument||this[0]).createDocumentFragment(),scripts=jQuery.clean(args,(this[0].ownerDocument||this[0]),fragment),first=fragment.firstChild;if(first){for(var i=0,l=this.length;i<l;i++){callback.call(root(this[i],first),this.length>1||i>0?fragment.cloneNode(true):fragment)}}if(scripts){jQuery.each(scripts,evalScript)}}return this;function root(elem,cur){return table&&jQuery.nodeName(elem,"table")&&jQuery.nodeName(cur,"tr")?(elem.getElementsByTagName("tbody")[0]||elem.appendChild(elem.ownerDocument.createElement("tbody"))):elem}}};jQuery.fn.init.prototype=jQuery.fn;function evalScript(i,elem){if(elem.src){jQuery.ajax({url:elem.src,async:false,dataType:"script"})}else{jQuery.globalEval(elem.text||elem.textContent||elem.innerHTML||"")}if(elem.parentNode){elem.parentNode.removeChild(elem)}}function now(){return +new Date}jQuery.extend=jQuery.fn.extend=function(){var target=arguments[0]||{},i=1,length=arguments.length,deep=false,options;if(typeof target==="boolean"){deep=target;target=arguments[1]||{};i=2}if(typeof target!=="object"&&!jQuery.isFunction(target)){target={}}if(length==i){target=this;--i}for(;i<length;i++){if((options=arguments[i])!=null){for(var name in options){var src=target[name],copy=options[name];if(target===copy){continue}if(deep&&copy&&typeof copy==="object"&&!copy.nodeType){target[name]=jQuery.extend(deep,src||(copy.length!=null?[]:{}),copy)}else{if(copy!==undefined){target[name]=copy}}}}}return target};var exclude=/z-?index|font-?weight|opacity|zoom|line-?height/i,defaultView=document.defaultView||{},toString=Object.prototype.toString;jQuery.extend({noConflict:function(deep){window.$=_$;if(deep){window.jQuery=_jQuery}return jQuery},isFunction:function(obj){return toString.call(obj)==="[object Function]"},isArray:function(obj){return toString.call(obj)==="[object Array]"},isXMLDoc:function(elem){return elem.nodeType===9&&elem.documentElement.nodeName!=="HTML"||!!elem.ownerDocument&&jQuery.isXMLDoc(elem.ownerDocument)},globalEval:function(data){if(data&&/\S/.test(data)){var head=document.getElementsByTagName("head")[0]||document.documentElement,script=document.createElement("script");script.type="text/javascript";if(jQuery.support.scriptEval){script.appendChild(document.createTextNode(data))}else{script.text=data}head.insertBefore(script,head.firstChild);head.removeChild(script)}},nodeName:function(elem,name){return elem.nodeName&&elem.nodeName.toUpperCase()==name.toUpperCase()},each:function(object,callback,args){var name,i=0,length=object.length;if(args){if(length===undefined){for(name in object){if(callback.apply(object[name],args)===false){break}}}else{for(;i<length;){if(callback.apply(object[i++],args)===false){break}}}}else{if(length===undefined){for(name in object){if(callback.call(object[name],name,object[name])===false){break}}}else{for(var value=object[0];i<length&&callback.call(value,i,value)!==false;value=object[++i]){}}}return object},prop:function(elem,value,type,i,name){if(jQuery.isFunction(value)){value=value.call(elem,i)}return typeof value==="number"&&type=="curCSS"&&!exclude.test(name)?value+"px":value},className:{add:function(elem,classNames){jQuery.each((classNames||"").split(/\s+/),function(i,className){if(elem.nodeType==1&&!jQuery.className.has(elem.className,className)){elem.className+=(elem.className?" ":"")+className}})},remove:function(elem,classNames){if(elem.nodeType==1){elem.className=classNames!==undefined?jQuery.grep(elem.className.split(/\s+/),function(className){return !jQuery.className.has(classNames,className)}).join(" "):""}},has:function(elem,className){return elem&&jQuery.inArray(className,(elem.className||elem).toString().split(/\s+/))>-1}},swap:function(elem,options,callback){var old={};for(var name in options){old[name]=elem.style[name];elem.style[name]=options[name]}callback.call(elem);for(var name in options){elem.style[name]=old[name]}},css:function(elem,name,force,extra){if(name=="width"||name=="height"){var val,props={position:"absolute",visibility:"hidden",display:"block"},which=name=="width"?["Left","Right"]:["Top","Bottom"];function getWH(){val=name=="width"?elem.offsetWidth:elem.offsetHeight;if(extra==="border"){return }jQuery.each(which,function(){if(!extra){val-=parseFloat(jQuery.curCSS(elem,"padding"+this,true))||0}if(extra==="margin"){val+=parseFloat(jQuery.curCSS(elem,"margin"+this,true))||0}else{val-=parseFloat(jQuery.curCSS(elem,"border"+this+"Width",true))||0}})}if(elem.offsetWidth!==0){getWH()}else{jQuery.swap(elem,props,getWH)}return Math.max(0,Math.round(val))}return jQuery.curCSS(elem,name,force)},curCSS:function(elem,name,force){var ret,style=elem.style;if(name=="opacity"&&!jQuery.support.opacity){ret=jQuery.attr(style,"opacity");return ret==""?"1":ret}if(name.match(/float/i)){name=styleFloat}if(!force&&style&&style[name]){ret=style[name]}else{if(defaultView.getComputedStyle){if(name.match(/float/i)){name="float"}name=name.replace(/([A-Z])/g,"-$1").toLowerCase();var computedStyle=defaultView.getComputedStyle(elem,null);if(computedStyle){ret=computedStyle.getPropertyValue(name)}if(name=="opacity"&&ret==""){ret="1"}}else{if(elem.currentStyle){var camelCase=name.replace(/\-(\w)/g,function(all,letter){return letter.toUpperCase()});ret=elem.currentStyle[name]||elem.currentStyle[camelCase];if(!/^\d+(px)?$/i.test(ret)&&/^\d/.test(ret)){var left=style.left,rsLeft=elem.runtimeStyle.left;elem.runtimeStyle.left=elem.currentStyle.left;style.left=ret||0;ret=style.pixelLeft+"px";style.left=left;elem.runtimeStyle.left=rsLeft}}}}return ret},clean:function(elems,context,fragment){context=context||document;if(typeof context.createElement==="undefined"){context=context.ownerDocument||context[0]&&context[0].ownerDocument||document}if(!fragment&&elems.length===1&&typeof elems[0]==="string"){var match=/^<(\w+)\s*\/?>$/.exec(elems[0]);if(match){return[context.createElement(match[1])]}}var ret=[],scripts=[],div=context.createElement("div");jQuery.each(elems,function(i,elem){if(typeof elem==="number"){elem+=""}if(!elem){return }if(typeof elem==="string"){elem=elem.replace(/(<(\w+)[^>]*?)\/>/g,function(all,front,tag){return tag.match(/^(abbr|br|col|img|input|link|meta|param|hr|area|embed)$/i)?all:front+"></"+tag+">"});var tags=elem.replace(/^\s+/,"").substring(0,10).toLowerCase();var wrap=!tags.indexOf("<opt")&&[1,"<select multiple='multiple'>","</select>"]||!tags.indexOf("<leg")&&[1,"<fieldset>","</fieldset>"]||tags.match(/^<(thead|tbody|tfoot|colg|cap)/)&&[1,"<table>","</table>"]||!tags.indexOf("<tr")&&[2,"<table><tbody>","</tbody></table>"]||(!tags.indexOf("<td")||!tags.indexOf("<th"))&&[3,"<table><tbody><tr>","</tr></tbody></table>"]||!tags.indexOf("<col")&&[2,"<table><tbody></tbody><colgroup>","</colgroup></table>"]||!jQuery.support.htmlSerialize&&[1,"div<div>","</div>"]||[0,"",""];div.innerHTML=wrap[1]+elem+wrap[2];while(wrap[0]--){div=div.lastChild}if(!jQuery.support.tbody){var hasBody=/<tbody/i.test(elem),tbody=!tags.indexOf("<table")&&!hasBody?div.firstChild&&div.firstChild.childNodes:wrap[1]=="<table>"&&!hasBody?div.childNodes:[];for(var j=tbody.length-1;j>=0;--j){if(jQuery.nodeName(tbody[j],"tbody")&&!tbody[j].childNodes.length){tbody[j].parentNode.removeChild(tbody[j])}}}if(!jQuery.support.leadingWhitespace&&/^\s/.test(elem)){div.insertBefore(context.createTextNode(elem.match(/^\s*/)[0]),div.firstChild)}elem=jQuery.makeArray(div.childNodes)}if(elem.nodeType){ret.push(elem)}else{ret=jQuery.merge(ret,elem)}});if(fragment){for(var i=0;ret[i];i++){if(jQuery.nodeName(ret[i],"script")&&(!ret[i].type||ret[i].type.toLowerCase()==="text/javascript")){scripts.push(ret[i].parentNode?ret[i].parentNode.removeChild(ret[i]):ret[i])}else{if(ret[i].nodeType===1){ret.splice.apply(ret,[i+1,0].concat(jQuery.makeArray(ret[i].getElementsByTagName("script"))))}fragment.appendChild(ret[i])}}return scripts}return ret},attr:function(elem,name,value){if(!elem||elem.nodeType==3||elem.nodeType==8){return undefined}var notxml=!jQuery.isXMLDoc(elem),set=value!==undefined;name=notxml&&jQuery.props[name]||name;if(elem.tagName){var special=/href|src|style/.test(name);if(name=="selected"&&elem.parentNode){elem.parentNode.selectedIndex}if(name in elem&&notxml&&!special){if(set){if(name=="type"&&jQuery.nodeName(elem,"input")&&elem.parentNode){throw"type property can't be changed"}elem[name]=value}if(jQuery.nodeName(elem,"form")&&elem.getAttributeNode(name)){return elem.getAttributeNode(name).nodeValue}if(name=="tabIndex"){var attributeNode=elem.getAttributeNode("tabIndex");return attributeNode&&attributeNode.specified?attributeNode.value:elem.nodeName.match(/(button|input|object|select|textarea)/i)?0:elem.nodeName.match(/^(a|area)$/i)&&elem.href?0:undefined}return elem[name]}if(!jQuery.support.style&&notxml&&name=="style"){return jQuery.attr(elem.style,"cssText",value)}if(set){elem.setAttribute(name,""+value)}var attr=!jQuery.support.hrefNormalized&&notxml&&special?elem.getAttribute(name,2):elem.getAttribute(name);return attr===null?undefined:attr}if(!jQuery.support.opacity&&name=="opacity"){if(set){elem.zoom=1;elem.filter=(elem.filter||"").replace(/alpha\([^)]*\)/,"")+(parseInt(value)+""=="NaN"?"":"alpha(opacity="+value*100+")")}return elem.filter&&elem.filter.indexOf("opacity=")>=0?(parseFloat(elem.filter.match(/opacity=([^)]*)/)[1])/100)+"":""}name=name.replace(/-([a-z])/ig,function(all,letter){return letter.toUpperCase()});if(set){elem[name]=value}return elem[name]},trim:function(text){return(text||"").replace(/^\s+|\s+$/g,"")},makeArray:function(array){var ret=[];if(array!=null){var i=array.length;if(i==null||typeof array==="string"||jQuery.isFunction(array)||array.setInterval){ret[0]=array}else{while(i){ret[--i]=array[i]}}}return ret},inArray:function(elem,array){for(var i=0,length=array.length;i<length;i++){if(array[i]===elem){return i}}return -1},merge:function(first,second){var i=0,elem,pos=first.length;if(!jQuery.support.getAll){while((elem=second[i++])!=null){if(elem.nodeType!=8){first[pos++]=elem}}}else{while((elem=second[i++])!=null){first[pos++]=elem}}return first},unique:function(array){var ret=[],done={};try{for(var i=0,length=array.length;i<length;i++){var id=jQuery.data(array[i]);if(!done[id]){done[id]=true;ret.push(array[i])}}}catch(e){ret=array}return ret},grep:function(elems,callback,inv){var ret=[];for(var i=0,length=elems.length;i<length;i++){if(!inv!=!callback(elems[i],i)){ret.push(elems[i])}}return ret},map:function(elems,callback){var ret=[];for(var i=0,length=elems.length;i<length;i++){var value=callback(elems[i],i);if(value!=null){ret[ret.length]=value}}return ret.concat.apply([],ret)}});var userAgent=navigator.userAgent.toLowerCase();jQuery.browser={version:(userAgent.match(/.+(?:rv|it|ra|ie)[\/: ]([\d.]+)/)||[0,"0"])[1],safari:/webkit/.test(userAgent),opera:/opera/.test(userAgent),msie:/msie/.test(userAgent)&&!/opera/.test(userAgent),mozilla:/mozilla/.test(userAgent)&&!/(compatible|webkit)/.test(userAgent)};jQuery.each({parent:function(elem){return elem.parentNode},parents:function(elem){return jQuery.dir(elem,"parentNode")},next:function(elem){return jQuery.nth(elem,2,"nextSibling")},prev:function(elem){return jQuery.nth(elem,2,"previousSibling")},nextAll:function(elem){return jQuery.dir(elem,"nextSibling")},prevAll:function(elem){return jQuery.dir(elem,"previousSibling")},siblings:function(elem){return jQuery.sibling(elem.parentNode.firstChild,elem)},children:function(elem){return jQuery.sibling(elem.firstChild)},contents:function(elem){return jQuery.nodeName(elem,"iframe")?elem.contentDocument||elem.contentWindow.document:jQuery.makeArray(elem.childNodes)}},function(name,fn){jQuery.fn[name]=function(selector){var ret=jQuery.map(this,fn);if(selector&&typeof selector=="string"){ret=jQuery.multiFilter(selector,ret)}return this.pushStack(jQuery.unique(ret),name,selector)}});jQuery.each({appendTo:"append",prependTo:"prepend",insertBefore:"before",insertAfter:"after",replaceAll:"replaceWith"},function(name,original){jQuery.fn[name]=function(selector){var ret=[],insert=jQuery(selector);for(var i=0,l=insert.length;i<l;i++){var elems=(i>0?this.clone(true):this).get();jQuery.fn[original].apply(jQuery(insert[i]),elems);ret=ret.concat(elems)}return this.pushStack(ret,name,selector)}});jQuery.each({removeAttr:function(name){jQuery.attr(this,name,"");if(this.nodeType==1){this.removeAttribute(name)}},addClass:function(classNames){jQuery.className.add(this,classNames)},removeClass:function(classNames){jQuery.className.remove(this,classNames)},toggleClass:function(classNames,state){if(typeof state!=="boolean"){state=!jQuery.className.has(this,classNames)}jQuery.className[state?"add":"remove"](this,classNames)},remove:function(selector){if(!selector||jQuery.filter(selector,[this]).length){jQuery("*",this).add([this]).each(function(){jQuery.event.remove(this);jQuery.removeData(this)});if(this.parentNode){this.parentNode.removeChild(this)}}},empty:function(){jQuery(this).children().remove();while(this.firstChild){this.removeChild(this.firstChild)}}},function(name,fn){jQuery.fn[name]=function(){return this.each(fn,arguments)}});function num(elem,prop){return elem[0]&&parseInt(jQuery.curCSS(elem[0],prop,true),10)||0}var expando="jQuery"+now(),uuid=0,windowData={};jQuery.extend({cache:{},data:function(elem,name,data){elem=elem==window?windowData:elem;var id=elem[expando];if(!id){id=elem[expando]=++uuid}if(name&&!jQuery.cache[id]){jQuery.cache[id]={}}if(data!==undefined){jQuery.cache[id][name]=data}return name?jQuery.cache[id][name]:id},removeData:function(elem,name){elem=elem==window?windowData:elem;var id=elem[expando];if(name){if(jQuery.cache[id]){delete jQuery.cache[id][name];name="";for(name in jQuery.cache[id]){break}if(!name){jQuery.removeData(elem)}}}else{try{delete elem[expando]}catch(e){if(elem.removeAttribute){elem.removeAttribute(expando)}}delete jQuery.cache[id]}},queue:function(elem,type,data){if(elem){type=(type||"fx")+"queue";var q=jQuery.data(elem,type);if(!q||jQuery.isArray(data)){q=jQuery.data(elem,type,jQuery.makeArray(data))}else{if(data){q.push(data)}}}return q},dequeue:function(elem,type){var queue=jQuery.queue(elem,type),fn=queue.shift();if(!type||type==="fx"){fn=queue[0]}if(fn!==undefined){fn.call(elem)}}});jQuery.fn.extend({data:function(key,value){var parts=key.split(".");parts[1]=parts[1]?"."+parts[1]:"";if(value===undefined){var data=this.triggerHandler("getData"+parts[1]+"!",[parts[0]]);if(data===undefined&&this.length){data=jQuery.data(this[0],key)}return data===undefined&&parts[1]?this.data(parts[0]):data}else{return this.trigger("setData"+parts[1]+"!",[parts[0],value]).each(function(){jQuery.data(this,key,value)})}},removeData:function(key){return this.each(function(){jQuery.removeData(this,key)})},queue:function(type,data){if(typeof type!=="string"){data=type;type="fx"}if(data===undefined){return jQuery.queue(this[0],type)}return this.each(function(){var queue=jQuery.queue(this,type,data);if(type=="fx"&&queue.length==1){queue[0].call(this)}})},dequeue:function(type){return this.each(function(){jQuery.dequeue(this,type)})}});
-/*
- * Sizzle CSS Selector Engine - v0.9.3
- *  Copyright 2009, The Dojo Foundation
- *  Released under the MIT, BSD, and GPL Licenses.
- *  More information: http://sizzlejs.com/
- */
-(function(){var chunker=/((?:\((?:\([^()]+\)|[^()]+)+\)|\[(?:\[[^[\]]*\]|['"][^'"]*['"]|[^[\]'"]+)+\]|\\.|[^ >+~,(\[\\]+)+|[>+~])(\s*,\s*)?/g,done=0,toString=Object.prototype.toString;var Sizzle=function(selector,context,results,seed){results=results||[];context=context||document;if(context.nodeType!==1&&context.nodeType!==9){return[]}if(!selector||typeof selector!=="string"){return results}var parts=[],m,set,checkSet,check,mode,extra,prune=true;chunker.lastIndex=0;while((m=chunker.exec(selector))!==null){parts.push(m[1]);if(m[2]){extra=RegExp.rightContext;break}}if(parts.length>1&&origPOS.exec(selector)){if(parts.length===2&&Expr.relative[parts[0]]){set=posProcess(parts[0]+parts[1],context)}else{set=Expr.relative[parts[0]]?[context]:Sizzle(parts.shift(),context);while(parts.length){selector=parts.shift();if(Expr.relative[selector]){selector+=parts.shift()}set=posProcess(selector,set)}}}else{var ret=seed?{expr:parts.pop(),set:makeArray(seed)}:Sizzle.find(parts.pop(),parts.length===1&&context.parentNode?context.parentNode:context,isXML(context));set=Sizzle.filter(ret.expr,ret.set);if(parts.length>0){checkSet=makeArray(set)}else{prune=false}while(parts.length){var cur=parts.pop(),pop=cur;if(!Expr.relative[cur]){cur=""}else{pop=parts.pop()}if(pop==null){pop=context}Expr.relative[cur](checkSet,pop,isXML(context))}}if(!checkSet){checkSet=set}if(!checkSet){throw"Syntax error, unrecognized expression: "+(cur||selector)}if(toString.call(checkSet)==="[object Array]"){if(!prune){results.push.apply(results,checkSet)}else{if(context.nodeType===1){for(var i=0;checkSet[i]!=null;i++){if(checkSet[i]&&(checkSet[i]===true||checkSet[i].nodeType===1&&contains(context,checkSet[i]))){results.push(set[i])}}}else{for(var i=0;checkSet[i]!=null;i++){if(checkSet[i]&&checkSet[i].nodeType===1){results.push(set[i])}}}}}else{makeArray(checkSet,results)}if(extra){Sizzle(extra,context,results,seed);if(sortOrder){hasDuplicate=false;results.sort(sortOrder);if(hasDuplicate){for(var i=1;i<results.length;i++){if(results[i]===results[i-1]){results.splice(i--,1)}}}}}return results};Sizzle.matches=function(expr,set){return Sizzle(expr,null,null,set)};Sizzle.find=function(expr,context,isXML){var set,match;if(!expr){return[]}for(var i=0,l=Expr.order.length;i<l;i++){var type=Expr.order[i],match;if((match=Expr.match[type].exec(expr))){var left=RegExp.leftContext;if(left.substr(left.length-1)!=="\\"){match[1]=(match[1]||"").replace(/\\/g,"");set=Expr.find[type](match,context,isXML);if(set!=null){expr=expr.replace(Expr.match[type],"");break}}}}if(!set){set=context.getElementsByTagName("*")}return{set:set,expr:expr}};Sizzle.filter=function(expr,set,inplace,not){var old=expr,result=[],curLoop=set,match,anyFound,isXMLFilter=set&&set[0]&&isXML(set[0]);while(expr&&set.length){for(var type in Expr.filter){if((match=Expr.match[type].exec(expr))!=null){var filter=Expr.filter[type],found,item;anyFound=false;if(curLoop==result){result=[]}if(Expr.preFilter[type]){match=Expr.preFilter[type](match,curLoop,inplace,result,not,isXMLFilter);if(!match){anyFound=found=true}else{if(match===true){continue}}}if(match){for(var i=0;(item=curLoop[i])!=null;i++){if(item){found=filter(item,match,i,curLoop);var pass=not^!!found;if(inplace&&found!=null){if(pass){anyFound=true}else{curLoop[i]=false}}else{if(pass){result.push(item);anyFound=true}}}}}if(found!==undefined){if(!inplace){curLoop=result}expr=expr.replace(Expr.match[type],"");if(!anyFound){return[]}break}}}if(expr==old){if(anyFound==null){throw"Syntax error, unrecognized expression: "+expr}else{break}}old=expr}return curLoop};var Expr=Sizzle.selectors={order:["ID","NAME","TAG"],match:{ID:/#((?:[\w\u00c0-\uFFFF_-]|\\.)+)/,CLASS:/\.((?:[\w\u00c0-\uFFFF_-]|\\.)+)/,NAME:/\[name=['"]*((?:[\w\u00c0-\uFFFF_-]|\\.)+)['"]*\]/,ATTR:/\[\s*((?:[\w\u00c0-\uFFFF_-]|\\.)+)\s*(?:(\S?=)\s*(['"]*)(.*?)\3|)\s*\]/,TAG:/^((?:[\w\u00c0-\uFFFF\*_-]|\\.)+)/,CHILD:/:(only|nth|last|first)-child(?:\((even|odd|[\dn+-]*)\))?/,POS:/:(nth|eq|gt|lt|first|last|even|odd)(?:\((\d*)\))?(?=[^-]|$)/,PSEUDO:/:((?:[\w\u00c0-\uFFFF_-]|\\.)+)(?:\((['"]*)((?:\([^\)]+\)|[^\2\(\)]*)+)\2\))?/},attrMap:{"class":"className","for":"htmlFor"},attrHandle:{href:function(elem){return elem.getAttribute("href")}},relative:{"+":function(checkSet,part,isXML){var isPartStr=typeof part==="string",isTag=isPartStr&&!/\W/.test(part),isPartStrNotTag=isPartStr&&!isTag;if(isTag&&!isXML){part=part.toUpperCase()}for(var i=0,l=checkSet.length,elem;i<l;i++){if((elem=checkSet[i])){while((elem=elem.previousSibling)&&elem.nodeType!==1){}checkSet[i]=isPartStrNotTag||elem&&elem.nodeName===part?elem||false:elem===part}}if(isPartStrNotTag){Sizzle.filter(part,checkSet,true)}},">":function(checkSet,part,isXML){var isPartStr=typeof part==="string";if(isPartStr&&!/\W/.test(part)){part=isXML?part:part.toUpperCase();for(var i=0,l=checkSet.length;i<l;i++){var elem=checkSet[i];if(elem){var parent=elem.parentNode;checkSet[i]=parent.nodeName===part?parent:false}}}else{for(var i=0,l=checkSet.length;i<l;i++){var elem=checkSet[i];if(elem){checkSet[i]=isPartStr?elem.parentNode:elem.parentNode===part}}if(isPartStr){Sizzle.filter(part,checkSet,true)}}},"":function(checkSet,part,isXML){var doneName=done++,checkFn=dirCheck;if(!part.match(/\W/)){var nodeCheck=part=isXML?part:part.toUpperCase();checkFn=dirNodeCheck}checkFn("parentNode",part,doneName,checkSet,nodeCheck,isXML)},"~":function(checkSet,part,isXML){var doneName=done++,checkFn=dirCheck;if(typeof part==="string"&&!part.match(/\W/)){var nodeCheck=part=isXML?part:part.toUpperCase();checkFn=dirNodeCheck}checkFn("previousSibling",part,doneName,checkSet,nodeCheck,isXML)}},find:{ID:function(match,context,isXML){if(typeof context.getElementById!=="undefined"&&!isXML){var m=context.getElementById(match[1]);return m?[m]:[]}},NAME:function(match,context,isXML){if(typeof context.getElementsByName!=="undefined"){var ret=[],results=context.getElementsByName(match[1]);for(var i=0,l=results.length;i<l;i++){if(results[i].getAttribute("name")===match[1]){ret.push(results[i])}}return ret.length===0?null:ret}},TAG:function(match,context){return context.getElementsByTagName(match[1])}},preFilter:{CLASS:function(match,curLoop,inplace,result,not,isXML){match=" "+match[1].replace(/\\/g,"")+" ";if(isXML){return match}for(var i=0,elem;(elem=curLoop[i])!=null;i++){if(elem){if(not^(elem.className&&(" "+elem.className+" ").indexOf(match)>=0)){if(!inplace){result.push(elem)}}else{if(inplace){curLoop[i]=false}}}}return false},ID:function(match){return match[1].replace(/\\/g,"")},TAG:function(match,curLoop){for(var i=0;curLoop[i]===false;i++){}return curLoop[i]&&isXML(curLoop[i])?match[1]:match[1].toUpperCase()},CHILD:function(match){if(match[1]=="nth"){var test=/(-?)(\d*)n((?:\+|-)?\d*)/.exec(match[2]=="even"&&"2n"||match[2]=="odd"&&"2n+1"||!/\D/.test(match[2])&&"0n+"+match[2]||match[2]);match[2]=(test[1]+(test[2]||1))-0;match[3]=test[3]-0}match[0]=done++;return match},ATTR:function(match,curLoop,inplace,result,not,isXML){var name=match[1].replace(/\\/g,"");if(!isXML&&Expr.attrMap[name]){match[1]=Expr.attrMap[name]}if(match[2]==="~="){match[4]=" "+match[4]+" "}return match},PSEUDO:function(match,curLoop,inplace,result,not){if(match[1]==="not"){if(match[3].match(chunker).length>1||/^\w/.test(match[3])){match[3]=Sizzle(match[3],null,null,curLoop)}else{var ret=Sizzle.filter(match[3],curLoop,inplace,true^not);if(!inplace){result.push.apply(result,ret)}return false}}else{if(Expr.match.POS.test(match[0])||Expr.match.CHILD.test(match[0])){return true}}return match},POS:function(match){match.unshift(true);return match}},filters:{enabled:function(elem){return elem.disabled===false&&elem.type!=="hidden"},disabled:function(elem){return elem.disabled===true},checked:function(elem){return elem.checked===true},selected:function(elem){elem.parentNode.selectedIndex;return elem.selected===true},parent:function(elem){return !!elem.firstChild},empty:function(elem){return !elem.firstChild},has:function(elem,i,match){return !!Sizzle(match[3],elem).length},header:function(elem){return/h\d/i.test(elem.nodeName)},text:function(elem){return"text"===elem.type},radio:function(elem){return"radio"===elem.type},checkbox:function(elem){return"checkbox"===elem.type},file:function(elem){return"file"===elem.type},password:function(elem){return"password"===elem.type},submit:function(elem){return"submit"===elem.type},image:function(elem){return"image"===elem.type},reset:function(elem){return"reset"===elem.type},button:function(elem){return"button"===elem.type||elem.nodeName.toUpperCase()==="BUTTON"},input:function(elem){return/input|select|textarea|button/i.test(elem.nodeName)}},setFilters:{first:function(elem,i){return i===0},last:function(elem,i,match,array){return i===array.length-1},even:function(elem,i){return i%2===0},odd:function(elem,i){return i%2===1},lt:function(elem,i,match){return i<match[3]-0},gt:function(elem,i,match){return i>match[3]-0},nth:function(elem,i,match){return match[3]-0==i},eq:function(elem,i,match){return match[3]-0==i}},filter:{PSEUDO:function(elem,match,i,array){var name=match[1],filter=Expr.filters[name];if(filter){return filter(elem,i,match,array)}else{if(name==="contains"){return(elem.textContent||elem.innerText||"").indexOf(match[3])>=0}else{if(name==="not"){var not=match[3];for(var i=0,l=not.length;i<l;i++){if(not[i]===elem){return false}}return true}}}},CHILD:function(elem,match){var type=match[1],node=elem;switch(type){case"only":case"first":while(node=node.previousSibling){if(node.nodeType===1){return false}}if(type=="first"){return true}node=elem;case"last":while(node=node.nextSibling){if(node.nodeType===1){return false}}return true;case"nth":var first=match[2],last=match[3];if(first==1&&last==0){return true}var doneName=match[0],parent=elem.parentNode;if(parent&&(parent.sizcache!==doneName||!elem.nodeIndex)){var count=0;for(node=parent.firstChild;node;node=node.nextSibling){if(node.nodeType===1){node.nodeIndex=++count}}parent.sizcache=doneName}var diff=elem.nodeIndex-last;if(first==0){return diff==0}else{return(diff%first==0&&diff/first>=0)}}},ID:function(elem,match){return elem.nodeType===1&&elem.getAttribute("id")===match},TAG:function(elem,match){return(match==="*"&&elem.nodeType===1)||elem.nodeName===match},CLASS:function(elem,match){return(" "+(elem.className||elem.getAttribute("class"))+" ").indexOf(match)>-1},ATTR:function(elem,match){var name=match[1],result=Expr.attrHandle[name]?Expr.attrHandle[name](elem):elem[name]!=null?elem[name]:elem.getAttribute(name),value=result+"",type=match[2],check=match[4];return result==null?type==="!=":type==="="?value===check:type==="*="?value.indexOf(check)>=0:type==="~="?(" "+value+" ").indexOf(check)>=0:!check?value&&result!==false:type==="!="?value!=check:type==="^="?value.indexOf(check)===0:type==="$="?value.substr(value.length-check.length)===check:type==="|="?value===check||value.substr(0,check.length+1)===check+"-":false},POS:function(elem,match,i,array){var name=match[2],filter=Expr.setFilters[name];if(filter){return filter(elem,i,match,array)}}}};var origPOS=Expr.match.POS;for(var type in Expr.match){Expr.match[type]=RegExp(Expr.match[type].source+/(?![^\[]*\])(?![^\(]*\))/.source)}var makeArray=function(array,results){array=Array.prototype.slice.call(array);if(results){results.push.apply(results,array);return results}return array};try{Array.prototype.slice.call(document.documentElement.childNodes)}catch(e){makeArray=function(array,results){var ret=results||[];if(toString.call(array)==="[object Array]"){Array.prototype.push.apply(ret,array)}else{if(typeof array.length==="number"){for(var i=0,l=array.length;i<l;i++){ret.push(array[i])}}else{for(var i=0;array[i];i++){ret.push(array[i])}}}return ret}}var sortOrder;if(document.documentElement.compareDocumentPosition){sortOrder=function(a,b){var ret=a.compareDocumentPosition(b)&4?-1:a===b?0:1;if(ret===0){hasDuplicate=true}return ret}}else{if("sourceIndex" in document.documentElement){sortOrder=function(a,b){var ret=a.sourceIndex-b.sourceIndex;if(ret===0){hasDuplicate=true}return ret}}else{if(document.createRange){sortOrder=function(a,b){var aRange=a.ownerDocument.createRange(),bRange=b.ownerDocument.createRange();aRange.selectNode(a);aRange.collapse(true);bRange.selectNode(b);bRange.collapse(true);var ret=aRange.compareBoundaryPoints(Range.START_TO_END,bRange);if(ret===0){hasDuplicate=true}return ret}}}}(function(){var form=document.createElement("form"),id="script"+(new Date).getTime();form.innerHTML="<input name='"+id+"'/>";var root=document.documentElement;root.insertBefore(form,root.firstChild);if(!!document.getElementById(id)){Expr.find.ID=function(match,context,isXML){if(typeof context.getElementById!=="undefined"&&!isXML){var m=context.getElementById(match[1]);return m?m.id===match[1]||typeof m.getAttributeNode!=="undefined"&&m.getAttributeNode("id").nodeValue===match[1]?[m]:undefined:[]}};Expr.filter.ID=function(elem,match){var node=typeof elem.getAttributeNode!=="undefined"&&elem.getAttributeNode("id");return elem.nodeType===1&&node&&node.nodeValue===match}}root.removeChild(form)})();(function(){var div=document.createElement("div");div.appendChild(document.createComment(""));if(div.getElementsByTagName("*").length>0){Expr.find.TAG=function(match,context){var results=context.getElementsByTagName(match[1]);if(match[1]==="*"){var tmp=[];for(var i=0;results[i];i++){if(results[i].nodeType===1){tmp.push(results[i])}}results=tmp}return results}}div.innerHTML="<a href='#'></a>";if(div.firstChild&&typeof div.firstChild.getAttribute!=="undefined"&&div.firstChild.getAttribute("href")!=="#"){Expr.attrHandle.href=function(elem){return elem.getAttribute("href",2)}}})();if(document.querySelectorAll){(function(){var oldSizzle=Sizzle,div=document.createElement("div");div.innerHTML="<p class='TEST'></p>";if(div.querySelectorAll&&div.querySelectorAll(".TEST").length===0){return }Sizzle=function(query,context,extra,seed){context=context||document;if(!seed&&context.nodeType===9&&!isXML(context)){try{return makeArray(context.querySelectorAll(query),extra)}catch(e){}}return oldSizzle(query,context,extra,seed)};Sizzle.find=oldSizzle.find;Sizzle.filter=oldSizzle.filter;Sizzle.selectors=oldSizzle.selectors;Sizzle.matches=oldSizzle.matches})()}if(document.getElementsByClassName&&document.documentElement.getElementsByClassName){(function(){var div=document.createElement("div");div.innerHTML="<div class='test e'></div><div class='test'></div>";if(div.getElementsByClassName("e").length===0){return }div.lastChild.className="e";if(div.getElementsByClassName("e").length===1){return }Expr.order.splice(1,0,"CLASS");Expr.find.CLASS=function(match,context,isXML){if(typeof context.getElementsByClassName!=="undefined"&&!isXML){return context.getElementsByClassName(match[1])}}})()}function dirNodeCheck(dir,cur,doneName,checkSet,nodeCheck,isXML){var sibDir=dir=="previousSibling"&&!isXML;for(var i=0,l=checkSet.length;i<l;i++){var elem=checkSet[i];if(elem){if(sibDir&&elem.nodeType===1){elem.sizcache=doneName;elem.sizset=i}elem=elem[dir];var match=false;while(elem){if(elem.sizcache===doneName){match=checkSet[elem.sizset];break}if(elem.nodeType===1&&!isXML){elem.sizcache=doneName;elem.sizset=i}if(elem.nodeName===cur){match=elem;break}elem=elem[dir]}checkSet[i]=match}}}function dirCheck(dir,cur,doneName,checkSet,nodeCheck,isXML){var sibDir=dir=="previousSibling"&&!isXML;for(var i=0,l=checkSet.length;i<l;i++){var elem=checkSet[i];if(elem){if(sibDir&&elem.nodeType===1){elem.sizcache=doneName;elem.sizset=i}elem=elem[dir];var match=false;while(elem){if(elem.sizcache===doneName){match=checkSet[elem.sizset];break}if(elem.nodeType===1){if(!isXML){elem.sizcache=doneName;elem.sizset=i}if(typeof cur!=="string"){if(elem===cur){match=true;break}}else{if(Sizzle.filter(cur,[elem]).length>0){match=elem;break}}}elem=elem[dir]}checkSet[i]=match}}}var contains=document.compareDocumentPosition?function(a,b){return a.compareDocumentPosition(b)&16}:function(a,b){return a!==b&&(a.contains?a.contains(b):true)};var isXML=function(elem){return elem.nodeType===9&&elem.documentElement.nodeName!=="HTML"||!!elem.ownerDocument&&isXML(elem.ownerDocument)};var posProcess=function(selector,context){var tmpSet=[],later="",match,root=context.nodeType?[context]:context;while((match=Expr.match.PSEUDO.exec(selector))){later+=match[0];selector=selector.replace(Expr.match.PSEUDO,"")}selector=Expr.relative[selector]?selector+"*":selector;for(var i=0,l=root.length;i<l;i++){Sizzle(selector,root[i],tmpSet)}return Sizzle.filter(later,tmpSet)};jQuery.find=Sizzle;jQuery.filter=Sizzle.filter;jQuery.expr=Sizzle.selectors;jQuery.expr[":"]=jQuery.expr.filters;Sizzle.selectors.filters.hidden=function(elem){return elem.offsetWidth===0||elem.offsetHeight===0};Sizzle.selectors.filters.visible=function(elem){return elem.offsetWidth>0||elem.offsetHeight>0};Sizzle.selectors.filters.animated=function(elem){return jQuery.grep(jQuery.timers,function(fn){return elem===fn.elem}).length};jQuery.multiFilter=function(expr,elems,not){if(not){expr=":not("+expr+")"}return Sizzle.matches(expr,elems)};jQuery.dir=function(elem,dir){var matched=[],cur=elem[dir];while(cur&&cur!=document){if(cur.nodeType==1){matched.push(cur)}cur=cur[dir]}return matched};jQuery.nth=function(cur,result,dir,elem){result=result||1;var num=0;for(;cur;cur=cur[dir]){if(cur.nodeType==1&&++num==result){break}}return cur};jQuery.sibling=function(n,elem){var r=[];for(;n;n=n.nextSibling){if(n.nodeType==1&&n!=elem){r.push(n)}}return r};return ;window.Sizzle=Sizzle})();jQuery.event={add:function(elem,types,handler,data){if(elem.nodeType==3||elem.nodeType==8){return }if(elem.setInterval&&elem!=window){elem=window}if(!handler.guid){handler.guid=this.guid++}if(data!==undefined){var fn=handler;handler=this.proxy(fn);handler.data=data}var events=jQuery.data(elem,"events")||jQuery.data(elem,"events",{}),handle=jQuery.data(elem,"handle")||jQuery.data(elem,"handle",function(){return typeof jQuery!=="undefined"&&!jQuery.event.triggered?jQuery.event.handle.apply(arguments.callee.elem,arguments):undefined});handle.elem=elem;jQuery.each(types.split(/\s+/),function(index,type){var namespaces=type.split(".");type=namespaces.shift();handler.type=namespaces.slice().sort().join(".");var handlers=events[type];if(jQuery.event.specialAll[type]){jQuery.event.specialAll[type].setup.call(elem,data,namespaces)}if(!handlers){handlers=events[type]={};if(!jQuery.event.special[type]||jQuery.event.special[type].setup.call(elem,data,namespaces)===false){if(elem.addEventListener){elem.addEventListener(type,handle,false)}else{if(elem.attachEvent){elem.attachEvent("on"+type,handle)}}}}handlers[handler.guid]=handler;jQuery.event.global[type]=true});elem=null},guid:1,global:{},remove:function(elem,types,handler){if(elem.nodeType==3||elem.nodeType==8){return }var events=jQuery.data(elem,"events"),ret,index;if(events){if(types===undefined||(typeof types==="string"&&types.charAt(0)==".")){for(var type in events){this.remove(elem,type+(types||""))}}else{if(types.type){handler=types.handler;types=types.type}jQuery.each(types.split(/\s+/),function(index,type){var namespaces=type.split(".");type=namespaces.shift();var namespace=RegExp("(^|\\.)"+namespaces.slice().sort().join(".*\\.")+"(\\.|$)");if(events[type]){if(handler){delete events[type][handler.guid]}else{for(var handle in events[type]){if(namespace.test(events[type][handle].type)){delete events[type][handle]}}}if(jQuery.event.specialAll[type]){jQuery.event.specialAll[type].teardown.call(elem,namespaces)}for(ret in events[type]){break}if(!ret){if(!jQuery.event.special[type]||jQuery.event.special[type].teardown.call(elem,namespaces)===false){if(elem.removeEventListener){elem.removeEventListener(type,jQuery.data(elem,"handle"),false)}else{if(elem.detachEvent){elem.detachEvent("on"+type,jQuery.data(elem,"handle"))}}}ret=null;delete events[type]}}})}for(ret in events){break}if(!ret){var handle=jQuery.data(elem,"handle");if(handle){handle.elem=null}jQuery.removeData(elem,"events");jQuery.removeData(elem,"handle")}}},trigger:function(event,data,elem,bubbling){var type=event.type||event;if(!bubbling){event=typeof event==="object"?event[expando]?event:jQuery.extend(jQuery.Event(type),event):jQuery.Event(type);if(type.indexOf("!")>=0){event.type=type=type.slice(0,-1);event.exclusive=true}if(!elem){event.stopPropagation();if(this.global[type]){jQuery.each(jQuery.cache,function(){if(this.events&&this.events[type]){jQuery.event.trigger(event,data,this.handle.elem)}})}}if(!elem||elem.nodeType==3||elem.nodeType==8){return undefined}event.result=undefined;event.target=elem;data=jQuery.makeArray(data);data.unshift(event)}event.currentTarget=elem;var handle=jQuery.data(elem,"handle");if(handle){handle.apply(elem,data)}if((!elem[type]||(jQuery.nodeName(elem,"a")&&type=="click"))&&elem["on"+type]&&elem["on"+type].apply(elem,data)===false){event.result=false}if(!bubbling&&elem[type]&&!event.isDefaultPrevented()&&!(jQuery.nodeName(elem,"a")&&type=="click")){this.triggered=true;try{elem[type]()}catch(e){}}this.triggered=false;if(!event.isPropagationStopped()){var parent=elem.parentNode||elem.ownerDocument;if(parent){jQuery.event.trigger(event,data,parent,true)}}},handle:function(event){var all,handlers;event=arguments[0]=jQuery.event.fix(event||window.event);event.currentTarget=this;var namespaces=event.type.split(".");event.type=namespaces.shift();all=!namespaces.length&&!event.exclusive;var namespace=RegExp("(^|\\.)"+namespaces.slice().sort().join(".*\\.")+"(\\.|$)");handlers=(jQuery.data(this,"events")||{})[event.type];for(var j in handlers){var handler=handlers[j];if(all||namespace.test(handler.type)){event.handler=handler;event.data=handler.data;var ret=handler.apply(this,arguments);if(ret!==undefined){event.result=ret;if(ret===false){event.preventDefault();event.stopPropagation()}}if(event.isImmediatePropagationStopped()){break}}}},props:"altKey attrChange attrName bubbles button cancelable charCode clientX clientY ctrlKey currentTarget data detail eventPhase fromElement handler keyCode metaKey newValue originalTarget pageX pageY prevValue relatedNode relatedTarget screenX screenY shiftKey srcElement target toElement view wheelDelta which".split(" "),fix:function(event){if(event[expando]){return event}var originalEvent=event;event=jQuery.Event(originalEvent);for(var i=this.props.length,prop;i;){prop=this.props[--i];event[prop]=originalEvent[prop]}if(!event.target){event.target=event.srcElement||document}if(event.target.nodeType==3){event.target=event.target.parentNode}if(!event.relatedTarget&&event.fromElement){event.relatedTarget=event.fromElement==event.target?event.toElement:event.fromElement}if(event.pageX==null&&event.clientX!=null){var doc=document.documentElement,body=document.body;event.pageX=event.clientX+(doc&&doc.scrollLeft||body&&body.scrollLeft||0)-(doc.clientLeft||0);event.pageY=event.clientY+(doc&&doc.scrollTop||body&&body.scrollTop||0)-(doc.clientTop||0)}if(!event.which&&((event.charCode||event.charCode===0)?event.charCode:event.keyCode)){event.which=event.charCode||event.keyCode}if(!event.metaKey&&event.ctrlKey){event.metaKey=event.ctrlKey}if(!event.which&&event.button){event.which=(event.button&1?1:(event.button&2?3:(event.button&4?2:0)))}return event},proxy:function(fn,proxy){proxy=proxy||function(){return fn.apply(this,arguments)};proxy.guid=fn.guid=fn.guid||proxy.guid||this.guid++;return proxy},special:{ready:{setup:bindReady,teardown:function(){}}},specialAll:{live:{setup:function(selector,namespaces){jQuery.event.add(this,namespaces[0],liveHandler)},teardown:function(namespaces){if(namespaces.length){var remove=0,name=RegExp("(^|\\.)"+namespaces[0]+"(\\.|$)");jQuery.each((jQuery.data(this,"events").live||{}),function(){if(name.test(this.type)){remove++}});if(remove<1){jQuery.event.remove(this,namespaces[0],liveHandler)}}}}}};jQuery.Event=function(src){if(!this.preventDefault){return new jQuery.Event(src)}if(src&&src.type){this.originalEvent=src;this.type=src.type}else{this.type=src}this.timeStamp=now();this[expando]=true};function returnFalse(){return false}function returnTrue(){return true}jQuery.Event.prototype={preventDefault:function(){this.isDefaultPrevented=returnTrue;var e=this.originalEvent;if(!e){return }if(e.preventDefault){e.preventDefault()}e.returnValue=false},stopPropagation:function(){this.isPropagationStopped=returnTrue;var e=this.originalEvent;if(!e){return }if(e.stopPropagation){e.stopPropagation()}e.cancelBubble=true},stopImmediatePropagation:function(){this.isImmediatePropagationStopped=returnTrue;this.stopPropagation()},isDefaultPrevented:returnFalse,isPropagationStopped:returnFalse,isImmediatePropagationStopped:returnFalse};var withinElement=function(event){var parent=event.relatedTarget;while(parent&&parent!=this){try{parent=parent.parentNode}catch(e){parent=this}}if(parent!=this){event.type=event.data;jQuery.event.handle.apply(this,arguments)}};jQuery.each({mouseover:"mouseenter",mouseout:"mouseleave"},function(orig,fix){jQuery.event.special[fix]={setup:function(){jQuery.event.add(this,orig,withinElement,fix)},teardown:function(){jQuery.event.remove(this,orig,withinElement)}}});jQuery.fn.extend({bind:function(type,data,fn){return type=="unload"?this.one(type,data,fn):this.each(function(){jQuery.event.add(this,type,fn||data,fn&&data)})},one:function(type,data,fn){var one=jQuery.event.proxy(fn||data,function(event){jQuery(this).unbind(event,one);return(fn||data).apply(this,arguments)});return this.each(function(){jQuery.event.add(this,type,one,fn&&data)})},unbind:function(type,fn){return this.each(function(){jQuery.event.remove(this,type,fn)})},trigger:function(type,data){return this.each(function(){jQuery.event.trigger(type,data,this)})},triggerHandler:function(type,data){if(this[0]){var event=jQuery.Event(type);event.preventDefault();event.stopPropagation();jQuery.event.trigger(event,data,this[0]);return event.result}},toggle:function(fn){var args=arguments,i=1;while(i<args.length){jQuery.event.proxy(fn,args[i++])}return this.click(jQuery.event.proxy(fn,function(event){this.lastToggle=(this.lastToggle||0)%i;event.preventDefault();return args[this.lastToggle++].apply(this,arguments)||false}))},hover:function(fnOver,fnOut){return this.mouseenter(fnOver).mouseleave(fnOut)},ready:function(fn){bindReady();if(jQuery.isReady){fn.call(document,jQuery)}else{jQuery.readyList.push(fn)}return this},live:function(type,fn){var proxy=jQuery.event.proxy(fn);proxy.guid+=this.selector+type;jQuery(document).bind(liveConvert(type,this.selector),this.selector,proxy);return this},die:function(type,fn){jQuery(document).unbind(liveConvert(type,this.selector),fn?{guid:fn.guid+this.selector+type}:null);return this}});function liveHandler(event){var check=RegExp("(^|\\.)"+event.type+"(\\.|$)"),stop=true,elems=[];jQuery.each(jQuery.data(this,"events").live||[],function(i,fn){if(check.test(fn.type)){var elem=jQuery(event.target).closest(fn.data)[0];if(elem){elems.push({elem:elem,fn:fn})}}});elems.sort(function(a,b){return jQuery.data(a.elem,"closest")-jQuery.data(b.elem,"closest")});jQuery.each(elems,function(){if(this.fn.call(this.elem,event,this.fn.data)===false){return(stop=false)}});return stop}function liveConvert(type,selector){return["live",type,selector.replace(/\./g,"`").replace(/ /g,"|")].join(".")}jQuery.extend({isReady:false,readyList:[],ready:function(){if(!jQuery.isReady){jQuery.isReady=true;if(jQuery.readyList){jQuery.each(jQuery.readyList,function(){this.call(document,jQuery)});jQuery.readyList=null}jQuery(document).triggerHandler("ready")}}});var readyBound=false;function bindReady(){if(readyBound){return }readyBound=true;if(document.addEventListener){document.addEventListener("DOMContentLoaded",function(){document.removeEventListener("DOMContentLoaded",arguments.callee,false);jQuery.ready()},false)}else{if(document.attachEvent){document.attachEvent("onreadystatechange",function(){if(document.readyState==="complete"){document.detachEvent("onreadystatechange",arguments.callee);jQuery.ready()}});if(document.documentElement.doScroll&&window==window.top){(function(){if(jQuery.isReady){return }try{document.documentElement.doScroll("left")}catch(error){setTimeout(arguments.callee,0);return }jQuery.ready()})()}}}jQuery.event.add(window,"load",jQuery.ready)}jQuery.each(("blur,focus,load,resize,scroll,unload,click,dblclick,mousedown,mouseup,mousemove,mouseover,mouseout,mouseenter,mouseleave,change,select,submit,keydown,keypress,keyup,error").split(","),function(i,name){jQuery.fn[name]=function(fn){return fn?this.bind(name,fn):this.trigger(name)}});jQuery(window).bind("unload",function(){for(var id in jQuery.cache){if(id!=1&&jQuery.cache[id].handle){jQuery.event.remove(jQuery.cache[id].handle.elem)}}});(function(){jQuery.support={};var root=document.documentElement,script=document.createElement("script"),div=document.createElement("div"),id="script"+(new Date).getTime();div.style.display="none";div.innerHTML='   <link/><table></table><a href="/a" style="color:red;float:left;opacity:.5;">a</a><select><option>text</option></select><object><param/></object>';var all=div.getElementsByTagName("*"),a=div.getElementsByTagName("a")[0];if(!all||!all.length||!a){return }jQuery.support={leadingWhitespace:div.firstChild.nodeType==3,tbody:!div.getElementsByTagName("tbody").length,objectAll:!!div.getElementsByTagName("object")[0].getElementsByTagName("*").length,htmlSerialize:!!div.getElementsByTagName("link").length,style:/red/.test(a.getAttribute("style")),hrefNormalized:a.getAttribute("href")==="/a",opacity:a.style.opacity==="0.5",cssFloat:!!a.style.cssFloat,scriptEval:false,noCloneEvent:true,boxModel:null};script.type="text/javascript";try{script.appendChild(document.createTextNode("window."+id+"=1;"))}catch(e){}root.insertBefore(script,root.firstChild);if(window[id]){jQuery.support.scriptEval=true;delete window[id]}root.removeChild(script);if(div.attachEvent&&div.fireEvent){div.attachEvent("onclick",function(){jQuery.support.noCloneEvent=false;div.detachEvent("onclick",arguments.callee)});div.cloneNode(true).fireEvent("onclick")}jQuery(function(){var div=document.createElement("div");div.style.width=div.style.paddingLeft="1px";document.body.appendChild(div);jQuery.boxModel=jQuery.support.boxModel=div.offsetWidth===2;document.body.removeChild(div).style.display="none"})})();var styleFloat=jQuery.support.cssFloat?"cssFloat":"styleFloat";jQuery.props={"for":"htmlFor","class":"className","float":styleFloat,cssFloat:styleFloat,styleFloat:styleFloat,readonly:"readOnly",maxlength:"maxLength",cellspacing:"cellSpacing",rowspan:"rowSpan",tabindex:"tabIndex"};jQuery.fn.extend({_load:jQuery.fn.load,load:function(url,params,callback){if(typeof url!=="string"){return this._load(url)}var off=url.indexOf(" ");if(off>=0){var selector=url.slice(off,url.length);url=url.slice(0,off)}var type="GET";if(params){if(jQuery.isFunction(params)){callback=params;params=null}else{if(typeof params==="object"){params=jQuery.param(params);type="POST"}}}var self=this;jQuery.ajax({url:url,type:type,dataType:"html",data:params,complete:function(res,status){if(status=="success"||status=="notmodified"){self.html(selector?jQuery("<div/>").append(res.responseText.replace(/<script(.|\s)*?\/script>/g,"")).find(selector):res.responseText)}if(callback){self.each(callback,[res.responseText,status,res])}}});return this},serialize:function(){return jQuery.param(this.serializeArray())},serializeArray:function(){return this.map(function(){return this.elements?jQuery.makeArray(this.elements):this}).filter(function(){return this.name&&!this.disabled&&(this.checked||/select|textarea/i.test(this.nodeName)||/text|hidden|password|search/i.test(this.type))}).map(function(i,elem){var val=jQuery(this).val();return val==null?null:jQuery.isArray(val)?jQuery.map(val,function(val,i){return{name:elem.name,value:val}}):{name:elem.name,value:val}}).get()}});jQuery.each("ajaxStart,ajaxStop,ajaxComplete,ajaxError,ajaxSuccess,ajaxSend".split(","),function(i,o){jQuery.fn[o]=function(f){return this.bind(o,f)}});var jsc=now();jQuery.extend({get:function(url,data,callback,type){if(jQuery.isFunction(data)){callback=data;data=null}return jQuery.ajax({type:"GET",url:url,data:data,success:callback,dataType:type})},getScript:function(url,callback){return jQuery.get(url,null,callback,"script")},getJSON:function(url,data,callback){return jQuery.get(url,data,callback,"json")},post:function(url,data,callback,type){if(jQuery.isFunction(data)){callback=data;data={}}return jQuery.ajax({type:"POST",url:url,data:data,success:callback,dataType:type})},ajaxSetup:function(settings){jQuery.extend(jQuery.ajaxSettings,settings)},ajaxSettings:{url:location.href,global:true,type:"GET",contentType:"application/x-www-form-urlencoded",processData:true,async:true,xhr:function(){return window.ActiveXObject?new ActiveXObject("Microsoft.XMLHTTP"):new XMLHttpRequest()},accepts:{xml:"application/xml, text/xml",html:"text/html",script:"text/javascript, application/javascript",json:"application/json, text/javascript",text:"text/plain",_default:"*/*"}},lastModified:{},ajax:function(s){s=jQuery.extend(true,s,jQuery.extend(true,{},jQuery.ajaxSettings,s));var jsonp,jsre=/=\?(&|$)/g,status,data,type=s.type.toUpperCase();if(s.data&&s.processData&&typeof s.data!=="string"){s.data=jQuery.param(s.data)}if(s.dataType=="jsonp"){if(type=="GET"){if(!s.url.match(jsre)){s.url+=(s.url.match(/\?/)?"&":"?")+(s.jsonp||"callback")+"=?"}}else{if(!s.data||!s.data.match(jsre)){s.data=(s.data?s.data+"&":"")+(s.jsonp||"callback")+"=?"}}s.dataType="json"}if(s.dataType=="json"&&(s.data&&s.data.match(jsre)||s.url.match(jsre))){jsonp="jsonp"+jsc++;if(s.data){s.data=(s.data+"").replace(jsre,"="+jsonp+"$1")}s.url=s.url.replace(jsre,"="+jsonp+"$1");s.dataType="script";window[jsonp]=function(tmp){data=tmp;success();complete();window[jsonp]=undefined;try{delete window[jsonp]}catch(e){}if(head){head.removeChild(script)}}}if(s.dataType=="script"&&s.cache==null){s.cache=false}if(s.cache===false&&type=="GET"){var ts=now();var ret=s.url.replace(/(\?|&)_=.*?(&|$)/,"$1_="+ts+"$2");s.url=ret+((ret==s.url)?(s.url.match(/\?/)?"&":"?")+"_="+ts:"")}if(s.data&&type=="GET"){s.url+=(s.url.match(/\?/)?"&":"?")+s.data;s.data=null}if(s.global&&!jQuery.active++){jQuery.event.trigger("ajaxStart")}var parts=/^(\w+:)?\/\/([^\/?#]+)/.exec(s.url);if(s.dataType=="script"&&type=="GET"&&parts&&(parts[1]&&parts[1]!=location.protocol||parts[2]!=location.host)){var head=document.getElementsByTagName("head")[0];var script=document.createElement("script");script.src=s.url;if(s.scriptCharset){script.charset=s.scriptCharset}if(!jsonp){var done=false;script.onload=script.onreadystatechange=function(){if(!done&&(!this.readyState||this.readyState=="loaded"||this.readyState=="complete")){done=true;success();complete();script.onload=script.onreadystatechange=null;head.removeChild(script)}}}head.appendChild(script);return undefined}var requestDone=false;var xhr=s.xhr();if(s.username){xhr.open(type,s.url,s.async,s.username,s.password)}else{xhr.open(type,s.url,s.async)}try{if(s.data){xhr.setRequestHeader("Content-Type",s.contentType)}if(s.ifModified){xhr.setRequestHeader("If-Modified-Since",jQuery.lastModified[s.url]||"Thu, 01 Jan 1970 00:00:00 GMT")}xhr.setRequestHeader("X-Requested-With","XMLHttpRequest");xhr.setRequestHeader("Accept",s.dataType&&s.accepts[s.dataType]?s.accepts[s.dataType]+", */*":s.accepts._default)}catch(e){}if(s.beforeSend&&s.beforeSend(xhr,s)===false){if(s.global&&!--jQuery.active){jQuery.event.trigger("ajaxStop")}xhr.abort();return false}if(s.global){jQuery.event.trigger("ajaxSend",[xhr,s])}var onreadystatechange=function(isTimeout){if(xhr.readyState==0){if(ival){clearInterval(ival);ival=null;if(s.global&&!--jQuery.active){jQuery.event.trigger("ajaxStop")}}}else{if(!requestDone&&xhr&&(xhr.readyState==4||isTimeout=="timeout")){requestDone=true;if(ival){clearInterval(ival);ival=null}status=isTimeout=="timeout"?"timeout":!jQuery.httpSuccess(xhr)?"error":s.ifModified&&jQuery.httpNotModified(xhr,s.url)?"notmodified":"success";if(status=="success"){try{data=jQuery.httpData(xhr,s.dataType,s)}catch(e){status="parsererror"}}if(status=="success"){var modRes;try{modRes=xhr.getResponseHeader("Last-Modified")}catch(e){}if(s.ifModified&&modRes){jQuery.lastModified[s.url]=modRes}if(!jsonp){success()}}else{jQuery.handleError(s,xhr,status)}complete();if(isTimeout){xhr.abort()}if(s.async){xhr=null}}}};if(s.async){var ival=setInterval(onreadystatechange,13);if(s.timeout>0){setTimeout(function(){if(xhr&&!requestDone){onreadystatechange("timeout")}},s.timeout)}}try{xhr.send(s.data)}catch(e){jQuery.handleError(s,xhr,null,e)}if(!s.async){onreadystatechange()}function success(){if(s.success){s.success(data,status)}if(s.global){jQuery.event.trigger("ajaxSuccess",[xhr,s])}}function complete(){if(s.complete){s.complete(xhr,status)}if(s.global){jQuery.event.trigger("ajaxComplete",[xhr,s])}if(s.global&&!--jQuery.active){jQuery.event.trigger("ajaxStop")}}return xhr},handleError:function(s,xhr,status,e){if(s.error){s.error(xhr,status,e)}if(s.global){jQuery.event.trigger("ajaxError",[xhr,s,e])}},active:0,httpSuccess:function(xhr){try{return !xhr.status&&location.protocol=="file:"||(xhr.status>=200&&xhr.status<300)||xhr.status==304||xhr.status==1223}catch(e){}return false},httpNotModified:function(xhr,url){try{var xhrRes=xhr.getResponseHeader("Last-Modified");return xhr.status==304||xhrRes==jQuery.lastModified[url]}catch(e){}return false},httpData:function(xhr,type,s){var ct=xhr.getResponseHeader("content-type"),xml=type=="xml"||!type&&ct&&ct.indexOf("xml")>=0,data=xml?xhr.responseXML:xhr.responseText;if(xml&&data.documentElement.tagName=="parsererror"){throw"parsererror"}if(s&&s.dataFilter){data=s.dataFilter(data,type)}if(typeof data==="string"){if(type=="script"){jQuery.globalEval(data)}if(type=="json"){data=window["eval"]("("+data+")")}}return data},param:function(a){var s=[];function add(key,value){s[s.length]=encodeURIComponent(key)+"="+encodeURIComponent(value)}if(jQuery.isArray(a)||a.jquery){jQuery.each(a,function(){add(this.name,this.value)})}else{for(var j in a){if(jQuery.isArray(a[j])){jQuery.each(a[j],function(){add(j,this)})}else{add(j,jQuery.isFunction(a[j])?a[j]():a[j])}}}return s.join("&").replace(/%20/g,"+")}});var elemdisplay={},timerId,fxAttrs=[["height","marginTop","marginBottom","paddingTop","paddingBottom"],["width","marginLeft","marginRight","paddingLeft","paddingRight"],["opacity"]];function genFx(type,num){var obj={};jQuery.each(fxAttrs.concat.apply([],fxAttrs.slice(0,num)),function(){obj[this]=type});return obj}jQuery.fn.extend({show:function(speed,callback){if(speed){return this.animate(genFx("show",3),speed,callback)}else{for(var i=0,l=this.length;i<l;i++){var old=jQuery.data(this[i],"olddisplay");this[i].style.display=old||"";if(jQuery.css(this[i],"display")==="none"){var tagName=this[i].tagName,display;if(elemdisplay[tagName]){display=elemdisplay[tagName]}else{var elem=jQuery("<"+tagName+" />").appendTo("body");display=elem.css("display");if(display==="none"){display="block"}elem.remove();elemdisplay[tagName]=display}jQuery.data(this[i],"olddisplay",display)}}for(var i=0,l=this.length;i<l;i++){this[i].style.display=jQuery.data(this[i],"olddisplay")||""}return this}},hide:function(speed,callback){if(speed){return this.animate(genFx("hide",3),speed,callback)}else{for(var i=0,l=this.length;i<l;i++){var old=jQuery.data(this[i],"olddisplay");if(!old&&old!=="none"){jQuery.data(this[i],"olddisplay",jQuery.css(this[i],"display"))}}for(var i=0,l=this.length;i<l;i++){this[i].style.display="none"}return this}},_toggle:jQuery.fn.toggle,toggle:function(fn,fn2){var bool=typeof fn==="boolean";return jQuery.isFunction(fn)&&jQuery.isFunction(fn2)?this._toggle.apply(this,arguments):fn==null||bool?this.each(function(){var state=bool?fn:jQuery(this).is(":hidden");jQuery(this)[state?"show":"hide"]()}):this.animate(genFx("toggle",3),fn,fn2)},fadeTo:function(speed,to,callback){return this.animate({opacity:to},speed,callback)},animate:function(prop,speed,easing,callback){var optall=jQuery.speed(speed,easing,callback);return this[optall.queue===false?"each":"queue"](function(){var opt=jQuery.extend({},optall),p,hidden=this.nodeType==1&&jQuery(this).is(":hidden"),self=this;for(p in prop){if(prop[p]=="hide"&&hidden||prop[p]=="show"&&!hidden){return opt.complete.call(this)}if((p=="height"||p=="width")&&this.style){opt.display=jQuery.css(this,"display");opt.overflow=this.style.overflow}}if(opt.overflow!=null){this.style.overflow="hidden"}opt.curAnim=jQuery.extend({},prop);jQuery.each(prop,function(name,val){var e=new jQuery.fx(self,opt,name);if(/toggle|show|hide/.test(val)){e[val=="toggle"?hidden?"show":"hide":val](prop)}else{var parts=val.toString().match(/^([+-]=)?([\d+-.]+)(.*)$/),start=e.cur(true)||0;if(parts){var end=parseFloat(parts[2]),unit=parts[3]||"px";if(unit!="px"){self.style[name]=(end||1)+unit;start=((end||1)/e.cur(true))*start;self.style[name]=start+unit}if(parts[1]){end=((parts[1]=="-="?-1:1)*end)+start}e.custom(start,end,unit)}else{e.custom(start,val,"")}}});return true})},stop:function(clearQueue,gotoEnd){var timers=jQuery.timers;if(clearQueue){this.queue([])}this.each(function(){for(var i=timers.length-1;i>=0;i--){if(timers[i].elem==this){if(gotoEnd){timers[i](true)}timers.splice(i,1)}}});if(!gotoEnd){this.dequeue()}return this}});jQuery.each({slideDown:genFx("show",1),slideUp:genFx("hide",1),slideToggle:genFx("toggle",1),fadeIn:{opacity:"show"},fadeOut:{opacity:"hide"}},function(name,props){jQuery.fn[name]=function(speed,callback){return this.animate(props,speed,callback)}});jQuery.extend({speed:function(speed,easing,fn){var opt=typeof speed==="object"?speed:{complete:fn||!fn&&easing||jQuery.isFunction(speed)&&speed,duration:speed,easing:fn&&easing||easing&&!jQuery.isFunction(easing)&&easing};opt.duration=jQuery.fx.off?0:typeof opt.duration==="number"?opt.duration:jQuery.fx.speeds[opt.duration]||jQuery.fx.speeds._default;opt.old=opt.complete;opt.complete=function(){if(opt.queue!==false){jQuery(this).dequeue()}if(jQuery.isFunction(opt.old)){opt.old.call(this)}};return opt},easing:{linear:function(p,n,firstNum,diff){return firstNum+diff*p},swing:function(p,n,firstNum,diff){return((-Math.cos(p*Math.PI)/2)+0.5)*diff+firstNum}},timers:[],fx:function(elem,options,prop){this.options=options;this.elem=elem;this.prop=prop;if(!options.orig){options.orig={}}}});jQuery.fx.prototype={update:function(){if(this.options.step){this.options.step.call(this.elem,this.now,this)}(jQuery.fx.step[this.prop]||jQuery.fx.step._default)(this);if((this.prop=="height"||this.prop=="width")&&this.elem.style){this.elem.style.display="block"}},cur:function(force){if(this.elem[this.prop]!=null&&(!this.elem.style||this.elem.style[this.prop]==null)){return this.elem[this.prop]}var r=parseFloat(jQuery.css(this.elem,this.prop,force));return r&&r>-10000?r:parseFloat(jQuery.curCSS(this.elem,this.prop))||0},custom:function(from,to,unit){this.startTime=now();this.start=from;this.end=to;this.unit=unit||this.unit||"px";this.now=this.start;this.pos=this.state=0;var self=this;function t(gotoEnd){return self.step(gotoEnd)}t.elem=this.elem;if(t()&&jQuery.timers.push(t)&&!timerId){timerId=setInterval(function(){var timers=jQuery.timers;for(var i=0;i<timers.length;i++){if(!timers[i]()){timers.splice(i--,1)}}if(!timers.length){clearInterval(timerId);timerId=undefined}},13)}},show:function(){this.options.orig[this.prop]=jQuery.attr(this.elem.style,this.prop);this.options.show=true;this.custom(this.prop=="width"||this.prop=="height"?1:0,this.cur());jQuery(this.elem).show()},hide:function(){this.options.orig[this.prop]=jQuery.attr(this.elem.style,this.prop);this.options.hide=true;this.custom(this.cur(),0)},step:function(gotoEnd){var t=now();if(gotoEnd||t>=this.options.duration+this.startTime){this.now=this.end;this.pos=this.state=1;this.update();this.options.curAnim[this.prop]=true;var done=true;for(var i in this.options.curAnim){if(this.options.curAnim[i]!==true){done=false}}if(done){if(this.options.display!=null){this.elem.style.overflow=this.options.overflow;this.elem.style.display=this.options.display;if(jQuery.css(this.elem,"display")=="none"){this.elem.style.display="block"}}if(this.options.hide){jQuery(this.elem).hide()}if(this.options.hide||this.options.show){for(var p in this.options.curAnim){jQuery.attr(this.elem.style,p,this.options.orig[p])}}this.options.complete.call(this.elem)}return false}else{var n=t-this.startTime;this.state=n/this.options.duration;this.pos=jQuery.easing[this.options.easing||(jQuery.easing.swing?"swing":"linear")](this.state,n,0,1,this.options.duration);this.now=this.start+((this.end-this.start)*this.pos);this.update()}return true}};jQuery.extend(jQuery.fx,{speeds:{slow:600,fast:200,_default:400},step:{opacity:function(fx){jQuery.attr(fx.elem.style,"opacity",fx.now)},_default:function(fx){if(fx.elem.style&&fx.elem.style[fx.prop]!=null){fx.elem.style[fx.prop]=fx.now+fx.unit}else{fx.elem[fx.prop]=fx.now}}}});if(document.documentElement.getBoundingClientRect){jQuery.fn.offset=function(){if(!this[0]){return{top:0,left:0}}if(this[0]===this[0].ownerDocument.body){return jQuery.offset.bodyOffset(this[0])}var box=this[0].getBoundingClientRect(),doc=this[0].ownerDocument,body=doc.body,docElem=doc.documentElement,clientTop=docElem.clientTop||body.clientTop||0,clientLeft=docElem.clientLeft||body.clientLeft||0,top=box.top+(self.pageYOffset||jQuery.boxModel&&docElem.scrollTop||body.scrollTop)-clientTop,left=box.left+(self.pageXOffset||jQuery.boxModel&&docElem.scrollLeft||body.scrollLeft)-clientLeft;return{top:top,left:left}}}else{jQuery.fn.offset=function(){if(!this[0]){return{top:0,left:0}}if(this[0]===this[0].ownerDocument.body){return jQuery.offset.bodyOffset(this[0])}jQuery.offset.initialized||jQuery.offset.initialize();var elem=this[0],offsetParent=elem.offsetParent,prevOffsetParent=elem,doc=elem.ownerDocument,computedStyle,docElem=doc.documentElement,body=doc.body,defaultView=doc.defaultView,prevComputedStyle=defaultView.getComputedStyle(elem,null),top=elem.offsetTop,left=elem.offsetLeft;while((elem=elem.parentNode)&&elem!==body&&elem!==docElem){computedStyle=defaultView.getComputedStyle(elem,null);top-=elem.scrollTop,left-=elem.scrollLeft;if(elem===offsetParent){top+=elem.offsetTop,left+=elem.offsetLeft;if(jQuery.offset.doesNotAddBorder&&!(jQuery.offset.doesAddBorderForTableAndCells&&/^t(able|d|h)$/i.test(elem.tagName))){top+=parseInt(computedStyle.borderTopWidth,10)||0,left+=parseInt(computedStyle.borderLeftWidth,10)||0}prevOffsetParent=offsetParent,offsetParent=elem.offsetParent}if(jQuery.offset.subtractsBorderForOverflowNotVisible&&computedStyle.overflow!=="visible"){top+=parseInt(computedStyle.borderTopWidth,10)||0,left+=parseInt(computedStyle.borderLeftWidth,10)||0}prevComputedStyle=computedStyle}if(prevComputedStyle.position==="relative"||prevComputedStyle.position==="static"){top+=body.offsetTop,left+=body.offsetLeft}if(prevComputedStyle.position==="fixed"){top+=Math.max(docElem.scrollTop,body.scrollTop),left+=Math.max(docElem.scrollLeft,body.scrollLeft)}return{top:top,left:left}}}jQuery.offset={initialize:function(){if(this.initialized){return }var body=document.body,container=document.createElement("div"),innerDiv,checkDiv,table,td,rules,prop,bodyMarginTop=body.style.marginTop,html='<div style="position:absolute;top:0;left:0;margin:0;border:5px solid #000;padding:0;width:1px;height:1px;"><div></div></div><table style="position:absolute;top:0;left:0;margin:0;border:5px solid #000;padding:0;width:1px;height:1px;" cellpadding="0" cellspacing="0"><tr><td></td></tr></table>';rules={position:"absolute",top:0,left:0,margin:0,border:0,width:"1px",height:"1px",visibility:"hidden"};for(prop in rules){container.style[prop]=rules[prop]}container.innerHTML=html;body.insertBefore(container,body.firstChild);innerDiv=container.firstChild,checkDiv=innerDiv.firstChild,td=innerDiv.nextSibling.firstChild.firstChild;this.doesNotAddBorder=(checkDiv.offsetTop!==5);this.doesAddBorderForTableAndCells=(td.offsetTop===5);innerDiv.style.overflow="hidden",innerDiv.style.position="relative";this.subtractsBorderForOverflowNotVisible=(checkDiv.offsetTop===-5);body.style.marginTop="1px";this.doesNotIncludeMarginInBodyOffset=(body.offsetTop===0);body.style.marginTop=bodyMarginTop;body.removeChild(container);this.initialized=true},bodyOffset:function(body){jQuery.offset.initialized||jQuery.offset.initialize();var top=body.offsetTop,left=body.offsetLeft;if(jQuery.offset.doesNotIncludeMarginInBodyOffset){top+=parseInt(jQuery.curCSS(body,"marginTop",true),10)||0,left+=parseInt(jQuery.curCSS(body,"marginLeft",true),10)||0}return{top:top,left:left}}};jQuery.fn.extend({position:function(){var left=0,top=0,results;if(this[0]){var offsetParent=this.offsetParent(),offset=this.offset(),parentOffset=/^body|html$/i.test(offsetParent[0].tagName)?{top:0,left:0}:offsetParent.offset();offset.top-=num(this,"marginTop");offset.left-=num(this,"marginLeft");parentOffset.top+=num(offsetParent,"borderTopWidth");parentOffset.left+=num(offsetParent,"borderLeftWidth");results={top:offset.top-parentOffset.top,left:offset.left-parentOffset.left}}return results},offsetParent:function(){var offsetParent=this[0].offsetParent||document.body;while(offsetParent&&(!/^body|html$/i.test(offsetParent.tagName)&&jQuery.css(offsetParent,"position")=="static")){offsetParent=offsetParent.offsetParent}return jQuery(offsetParent)}});jQuery.each(["Left","Top"],function(i,name){var method="scroll"+name;jQuery.fn[method]=function(val){if(!this[0]){return null}return val!==undefined?this.each(function(){this==window||this==document?window.scrollTo(!i?val:jQuery(window).scrollLeft(),i?val:jQuery(window).scrollTop()):this[method]=val}):this[0]==window||this[0]==document?self[i?"pageYOffset":"pageXOffset"]||jQuery.boxModel&&document.documentElement[method]||document.body[method]:this[0][method]}});jQuery.each(["Height","Width"],function(i,name){var tl=i?"Left":"Top",br=i?"Right":"Bottom",lower=name.toLowerCase();jQuery.fn["inner"+name]=function(){return this[0]?jQuery.css(this[0],lower,false,"padding"):null};jQuery.fn["outer"+name]=function(margin){return this[0]?jQuery.css(this[0],lower,false,margin?"margin":"border"):null};var type=name.toLowerCase();jQuery.fn[type]=function(size){return this[0]==window?document.compatMode=="CSS1Compat"&&document.documentElement["client"+name]||document.body["client"+name]:this[0]==document?Math.max(document.documentElement["client"+name],document.body["scroll"+name],document.documentElement["scroll"+name],document.body["offset"+name],document.documentElement["offset"+name]):size===undefined?(this.length?jQuery.css(this[0],type):null):this.css(type,typeof size==="string"?size:size+"px")}})})();
\ No newline at end of file
diff --git a/lib/logstash/web/public/js/jquery-hashchange-1.0.0.js b/lib/logstash/web/public/js/jquery-hashchange-1.0.0.js
deleted file mode 100644
index 3bb8b7f8ceb..00000000000
--- a/lib/logstash/web/public/js/jquery-hashchange-1.0.0.js
+++ /dev/null
@@ -1,121 +0,0 @@
-/**
- * jQuery hashchange 1.0.0
- * 
- * (based on jquery.history)
- *
- * Copyright (c) 2008 Chris Leishman (chrisleishman.com)
- * Dual licensed under the MIT (MIT-LICENSE.txt)
- * and GPL (GPL-LICENSE.txt) licenses.
- */
-(function($) {
-
-$.fn.extend({
-    hashchange: function(callback) { this.bind('hashchange', callback) },
-    openOnClick: function(href) {
-		if (href === undefined || href.length == 0)
-			href = '#';
-		return this.click(function(ev) {
-			if (href && href.charAt(0) == '#') {
-				// execute load in separate call stack
-				window.setTimeout(function() { $.locationHash(href) }, 0);
-			} else {
-				window.location(href);
-			}
-			ev.stopPropagation();
-			return false;
-		});
-    }
-});
-
-// IE 8 introduces the hashchange event natively - so nothing more to do
-if ($.browser.msie && document.documentMode && document.documentMode >= 8) {
-	$.extend({
-		locationHash: function(hash) {
-	        if (!hash) hash = '#';
-	        else if (hash.charAt(0) != '#') hash = '#' + hash;
-	        location.hash = hash;
-	    }
-	});
-	return;
-}
-
-var curHash;
-// hidden iframe for IE (earlier than 8)
-var iframe;
-
-$.extend({
-	locationHash: function(hash) {
-		if (curHash === undefined) return;
-
-		if (!hash) hash = '#';
-		else if (hash.charAt(0) != '#') hash = '#' + hash;
-		
-		location.hash = hash;
-		
-		if (curHash == hash) return;
-		curHash = hash;
-		
-		if ($.browser.msie) updateIEFrame(hash);
-		$.event.trigger('hashchange');
-	}
-});
-
-$(document).ready(function() {
-    curHash = location.hash;
-    if ($.browser.msie) {
-        // stop the callback firing twice during init if no hash present
-        if (curHash == '') curHash = '#';
-        // add hidden iframe for IE
-        iframe = $('<iframe />').hide().get(0);
-        $('body').prepend(iframe);
-        updateIEFrame(location.hash);
-        setInterval(checkHashIE, 100);
-    } else {
-        setInterval(checkHash, 100);
-    }
-});
-$(window).unload(function() { iframe = null });
-
-function checkHash() {
-    var hash = location.hash;
-    if (hash != curHash) {
-        curHash = hash;
-        $.event.trigger('hashchange');
-    }
-}
-
-if ($.browser.msie) {
-    // Attach a live handler for any anchor links
-    $('a[href^=#]').live('click', function() {
-        var hash = $(this).attr('href');
-        // Don't intercept the click if there is an existing anchor on the page
-        // that matches this hash
-        if ($(hash).length == 0 && $('a[name='+hash.slice(1)+']').length == 0) {
-            $.locationHash(hash);
-            return false;
-        }
-    });
-}
-
-function checkHashIE() {
-    // On IE, check for location.hash of iframe
-    var idoc = iframe.contentDocument || iframe.contentWindow.document;
-    var hash = idoc.location.hash;
-    if (hash == '') hash = '#';
-
-    if (hash != curHash) {
-        if (location.hash != hash) location.hash = hash;
-        curHash = hash;
-        $.event.trigger('hashchange');
-    }
-}
-
-function updateIEFrame(hash) {
-    if (hash == '#') hash = '';
-    var idoc = iframe.contentWindow.document;
-    idoc.open();
-    idoc.close();
-    if (idoc.location.hash != hash) idoc.location.hash = hash;
-}
-
-})(jQuery);
diff --git a/lib/logstash/web/public/js/jquery.livequery.js b/lib/logstash/web/public/js/jquery.livequery.js
deleted file mode 100644
index dde8ad8e322..00000000000
--- a/lib/logstash/web/public/js/jquery.livequery.js
+++ /dev/null
@@ -1,250 +0,0 @@
-/*! Copyright (c) 2008 Brandon Aaron (http://brandonaaron.net)
- * Dual licensed under the MIT (http://www.opensource.org/licenses/mit-license.php) 
- * and GPL (http://www.opensource.org/licenses/gpl-license.php) licenses.
- *
- * Version: 1.0.3
- * Requires jQuery 1.1.3+
- * Docs: http://docs.jquery.com/Plugins/livequery
- */
-
-(function($) {
-	
-$.extend($.fn, {
-	livequery: function(type, fn, fn2) {
-		var self = this, q;
-		
-		// Handle different call patterns
-		if ($.isFunction(type))
-			fn2 = fn, fn = type, type = undefined;
-			
-		// See if Live Query already exists
-		$.each( $.livequery.queries, function(i, query) {
-			if ( self.selector == query.selector && self.context == query.context &&
-				type == query.type && (!fn || fn.$lqguid == query.fn.$lqguid) && (!fn2 || fn2.$lqguid == query.fn2.$lqguid) )
-					// Found the query, exit the each loop
-					return (q = query) && false;
-		});
-		
-		// Create new Live Query if it wasn't found
-		q = q || new $.livequery(this.selector, this.context, type, fn, fn2);
-		
-		// Make sure it is running
-		q.stopped = false;
-		
-		// Run it immediately for the first time
-		q.run();
-		
-		// Contnue the chain
-		return this;
-	},
-	
-	expire: function(type, fn, fn2) {
-		var self = this;
-		
-		// Handle different call patterns
-		if ($.isFunction(type))
-			fn2 = fn, fn = type, type = undefined;
-			
-		// Find the Live Query based on arguments and stop it
-		$.each( $.livequery.queries, function(i, query) {
-			if ( self.selector == query.selector && self.context == query.context && 
-				(!type || type == query.type) && (!fn || fn.$lqguid == query.fn.$lqguid) && (!fn2 || fn2.$lqguid == query.fn2.$lqguid) && !this.stopped )
-					$.livequery.stop(query.id);
-		});
-		
-		// Continue the chain
-		return this;
-	}
-});
-
-$.livequery = function(selector, context, type, fn, fn2) {
-	this.selector = selector;
-	this.context  = context || document;
-	this.type     = type;
-	this.fn       = fn;
-	this.fn2      = fn2;
-	this.elements = [];
-	this.stopped  = false;
-	
-	// The id is the index of the Live Query in $.livequery.queries
-	this.id = $.livequery.queries.push(this)-1;
-	
-	// Mark the functions for matching later on
-	fn.$lqguid = fn.$lqguid || $.livequery.guid++;
-	if (fn2) fn2.$lqguid = fn2.$lqguid || $.livequery.guid++;
-	
-	// Return the Live Query
-	return this;
-};
-
-$.livequery.prototype = {
-	stop: function() {
-		var query = this;
-		
-		if ( this.type )
-			// Unbind all bound events
-			this.elements.unbind(this.type, this.fn);
-		else if (this.fn2)
-			// Call the second function for all matched elements
-			this.elements.each(function(i, el) {
-				query.fn2.apply(el);
-			});
-			
-		// Clear out matched elements
-		this.elements = [];
-		
-		// Stop the Live Query from running until restarted
-		this.stopped = true;
-	},
-	
-	run: function() {
-		// Short-circuit if stopped
-		if ( this.stopped ) return;
-		var query = this;
-		
-		var oEls = this.elements,
-			els  = $(this.selector, this.context),
-			nEls = els.not(oEls);
-		
-		// Set elements to the latest set of matched elements
-		this.elements = els;
-		
-		if (this.type) {
-			// Bind events to newly matched elements
-			nEls.bind(this.type, this.fn);
-			
-			// Unbind events to elements no longer matched
-			if (oEls.length > 0)
-				$.each(oEls, function(i, el) {
-					if ( $.inArray(el, els) < 0 )
-						$.event.remove(el, query.type, query.fn);
-				});
-		}
-		else {
-			// Call the first function for newly matched elements
-			nEls.each(function() {
-				query.fn.apply(this);
-			});
-			
-			// Call the second function for elements no longer matched
-			if ( this.fn2 && oEls.length > 0 )
-				$.each(oEls, function(i, el) {
-					if ( $.inArray(el, els) < 0 )
-						query.fn2.apply(el);
-				});
-		}
-	}
-};
-
-$.extend($.livequery, {
-	guid: 0,
-	queries: [],
-	queue: [],
-	running: false,
-	timeout: null,
-	
-	checkQueue: function() {
-		if ( $.livequery.running && $.livequery.queue.length ) {
-			var length = $.livequery.queue.length;
-			// Run each Live Query currently in the queue
-			while ( length-- )
-				$.livequery.queries[ $.livequery.queue.shift() ].run();
-		}
-	},
-	
-	pause: function() {
-		// Don't run anymore Live Queries until restarted
-		$.livequery.running = false;
-	},
-	
-	play: function() {
-		// Restart Live Queries
-		$.livequery.running = true;
-		// Request a run of the Live Queries
-		$.livequery.run();
-	},
-	
-	registerPlugin: function() {
-		$.each( arguments, function(i,n) {
-			// Short-circuit if the method doesn't exist
-			if (!$.fn[n]) return;
-			
-			// Save a reference to the original method
-			var old = $.fn[n];
-			
-			// Create a new method
-			$.fn[n] = function() {
-				// Call the original method
-				var r = old.apply(this, arguments);
-				
-				// Request a run of the Live Queries
-				$.livequery.run();
-				
-				// Return the original methods result
-				return r;
-			}
-		});
-	},
-	
-	run: function(id) {
-		if (id != undefined) {
-			// Put the particular Live Query in the queue if it doesn't already exist
-			if ( $.inArray(id, $.livequery.queue) < 0 )
-				$.livequery.queue.push( id );
-		}
-		else
-			// Put each Live Query in the queue if it doesn't already exist
-			$.each( $.livequery.queries, function(id) {
-				if ( $.inArray(id, $.livequery.queue) < 0 )
-					$.livequery.queue.push( id );
-			});
-		
-		// Clear timeout if it already exists
-		if ($.livequery.timeout) clearTimeout($.livequery.timeout);
-		// Create a timeout to check the queue and actually run the Live Queries
-		$.livequery.timeout = setTimeout($.livequery.checkQueue, 20);
-	},
-	
-	stop: function(id) {
-		if (id != undefined)
-			// Stop are particular Live Query
-			$.livequery.queries[ id ].stop();
-		else
-			// Stop all Live Queries
-			$.each( $.livequery.queries, function(id) {
-				$.livequery.queries[ id ].stop();
-			});
-	}
-});
-
-// Register core DOM manipulation methods
-$.livequery.registerPlugin('append', 'prepend', 'after', 'before', 'wrap', 'attr', 'removeAttr', 'addClass', 'removeClass', 'toggleClass', 'empty', 'remove');
-
-// Run Live Queries when the Document is ready
-$(function() { $.livequery.play(); });
-
-
-// Save a reference to the original init method
-var init = $.prototype.init;
-
-// Create a new init method that exposes two new properties: selector and context
-$.prototype.init = function(a,c) {
-	// Call the original init and save the result
-	var r = init.apply(this, arguments);
-	
-	// Copy over properties if they exist already
-	if (a && a.selector)
-		r.context = a.context, r.selector = a.selector;
-		
-	// Set properties
-	if ( typeof a == 'string' )
-		r.context = c || document, r.selector = a;
-	
-	// Return the result
-	return r;
-};
-
-// Give the init function the jQuery prototype for later instantiation (needed after Rev 4091)
-$.prototype.init.prototype = $.prototype;
-	
-})(jQuery);
\ No newline at end of file
diff --git a/lib/logstash/web/public/js/jquery.tmpl.min.js b/lib/logstash/web/public/js/jquery.tmpl.min.js
deleted file mode 100644
index f08e81dccaa..00000000000
--- a/lib/logstash/web/public/js/jquery.tmpl.min.js
+++ /dev/null
@@ -1 +0,0 @@
-(function(a){var r=a.fn.domManip,d="_tmplitem",q=/^[^<]*(<[\w\W]+>)[^>]*$|\{\{\! /,b={},f={},e,p={key:0,data:{}},h=0,c=0,l=[];function g(e,d,g,i){var c={data:i||(d?d.data:{}),_wrap:d?d._wrap:null,tmpl:null,parent:d||null,nodes:[],calls:u,nest:w,wrap:x,html:v,update:t};e&&a.extend(c,e,{nodes:[],parent:d});if(g){c.tmpl=g;c._ctnt=c._ctnt||c.tmpl(a,c);c.key=++h;(l.length?f:b)[h]=c}return c}a.each({appendTo:"append",prependTo:"prepend",insertBefore:"before",insertAfter:"after",replaceAll:"replaceWith"},function(f,d){a.fn[f]=function(n){var g=[],i=a(n),k,h,m,l,j=this.length===1&&this[0].parentNode;e=b||{};if(j&&j.nodeType===11&&j.childNodes.length===1&&i.length===1){i[d](this[0]);g=this}else{for(h=0,m=i.length;h<m;h++){c=h;k=(h>0?this.clone(true):this).get();a.fn[d].apply(a(i[h]),k);g=g.concat(k)}c=0;g=this.pushStack(g,f,i.selector)}l=e;e=null;a.tmpl.complete(l);return g}});a.fn.extend({tmpl:function(d,c,b){return a.tmpl(this[0],d,c,b)},tmplItem:function(){return a.tmplItem(this[0])},template:function(b){return a.template(b,this[0])},domManip:function(d,l,j){if(d[0]&&d[0].nodeType){var f=a.makeArray(arguments),g=d.length,i=0,h;while(i<g&&!(h=a.data(d[i++],"tmplItem")));if(g>1)f[0]=[a.makeArray(d)];if(h&&c)f[2]=function(b){a.tmpl.afterManip(this,b,j)};r.apply(this,f)}else r.apply(this,arguments);c=0;!e&&a.tmpl.complete(b);return this}});a.extend({tmpl:function(d,h,e,c){var j,k=!c;if(k){c=p;d=a.template[d]||a.template(null,d);f={}}else if(!d){d=c.tmpl;b[c.key]=c;c.nodes=[];c.wrapped&&n(c,c.wrapped);return a(i(c,null,c.tmpl(a,c)))}if(!d)return[];if(typeof h==="function")h=h.call(c||{});e&&e.wrapped&&n(e,e.wrapped);j=a.isArray(h)?a.map(h,function(a){return a?g(e,c,d,a):null}):[g(e,c,d,h)];return k?a(i(c,null,j)):j},tmplItem:function(b){var c;if(b instanceof a)b=b[0];while(b&&b.nodeType===1&&!(c=a.data(b,"tmplItem"))&&(b=b.parentNode));return c||p},template:function(c,b){if(b){if(typeof b==="string")b=o(b);else if(b instanceof a)b=b[0]||{};if(b.nodeType)b=a.data(b,"tmpl")||a.data(b,"tmpl",o(b.innerHTML));return typeof c==="string"?(a.template[c]=b):b}return c?typeof c!=="string"?a.template(null,c):a.template[c]||a.template(null,q.test(c)?c:a(c)):null},encode:function(a){return(""+a).split("<").join("&lt;").split(">").join("&gt;").split('"').join("&#34;").split("'").join("&#39;")}});a.extend(a.tmpl,{tag:{tmpl:{_default:{$2:"null"},open:"if($notnull_1){_=_.concat($item.nest($1,$2));}"},wrap:{_default:{$2:"null"},open:"$item.calls(_,$1,$2);_=[];",close:"call=$item.calls();_=call._.concat($item.wrap(call,_));"},each:{_default:{$2:"$index, $value"},open:"if($notnull_1){$.each($1a,function($2){with(this){",close:"}});}"},"if":{open:"if(($notnull_1) && $1a){",close:"}"},"else":{_default:{$1:"true"},open:"}else if(($notnull_1) && $1a){"},html:{open:"if($notnull_1){_.push($1a);}"},"=":{_default:{$1:"$data"},open:"if($notnull_1){_.push($.encode($1a));}"},"!":{open:""}},complete:function(){b={}},afterManip:function(f,b,d){var e=b.nodeType===11?a.makeArray(b.childNodes):b.nodeType===1?[b]:[];d.call(f,b);m(e);c++}});function i(e,g,f){var b,c=f?a.map(f,function(a){return typeof a==="string"?e.key?a.replace(/(<\w+)(?=[\s>])(?![^>]*_tmplitem)([^>]*)/g,"$1 "+d+'="'+e.key+'" $2'):a:i(a,e,a._ctnt)}):e;if(g)return c;c=c.join("");c.replace(/^\s*([^<\s][^<]*)?(<[\w\W]+>)([^>]*[^>\s])?\s*$/,function(f,c,e,d){b=a(e).get();m(b);if(c)b=j(c).concat(b);if(d)b=b.concat(j(d))});return b?b:j(c)}function j(c){var b=document.createElement("div");b.innerHTML=c;return a.makeArray(b.childNodes)}function o(b){return new Function("jQuery","$item","var $=jQuery,call,_=[],$data=$item.data;with($data){_.push('"+a.trim(b).replace(/([\\'])/g,"\\$1").replace(/[\r\t\n]/g," ").replace(/\$\{([^\}]*)\}/g,"{{= $1}}").replace(/\{\{(\/?)(\w+|.)(?:\(((?:[^\}]|\}(?!\}))*?)?\))?(?:\s+(.*?)?)?(\(((?:[^\}]|\}(?!\}))*?)\))?\s*\}\}/g,function(m,l,j,d,b,c,e){var i=a.tmpl.tag[j],h,f,g;if(!i)throw"Template command not found: "+j;h=i._default||[];if(c&&!/\w$/.test(b)){b+=c;c=""}if(b){b=k(b);e=e?","+k(e)+")":c?")":"";f=c?b.indexOf(".")>-1?b+c:"("+b+").call($item"+e:b;g=c?f:"(typeof("+b+")==='function'?("+b+").call($item):("+b+"))"}else g=f=h.$1||"null";d=k(d);return"');"+i[l?"close":"open"].split("$notnull_1").join(b?"typeof("+b+")!=='undefined' && ("+b+")!=null":"true").split("$1a").join(g).split("$1").join(f).split("$2").join(d?d.replace(/\s*([^\(]+)\s*(\((.*?)\))?/g,function(d,c,b,a){a=a?","+a+")":b?")":"";return a?"("+c+").call($item"+a:d}):h.$2||"")+"_.push('"})+"');}return _;")}function n(c,b){c._wrap=i(c,true,a.isArray(b)?b:[q.test(b)?b:a(b).html()]).join("")}function k(a){return a?a.replace(/\\'/g,"'").replace(/\\\\/g,"\\"):null}function s(b){var a=document.createElement("div");a.appendChild(b.cloneNode(true));return a.innerHTML}function m(o){var n="_"+c,k,j,l={},e,p,i;for(e=0,p=o.length;e<p;e++){if((k=o[e]).nodeType!==1)continue;j=k.getElementsByTagName("*");for(i=j.length-1;i>=0;i--)m(j[i]);m(k)}function m(j){var p,i=j,k,e,m;if(m=j.getAttribute(d)){while(i.parentNode&&(i=i.parentNode).nodeType===1&&!(p=i.getAttribute(d)));if(p!==m){i=i.parentNode?i.nodeType===11?0:i.getAttribute(d)||0:0;if(!(e=b[m])){e=f[m];e=g(e,b[i]||f[i],null,true);e.key=++h;b[h]=e}c&&o(m)}j.removeAttribute(d)}else if(c&&(e=a.data(j,"tmplItem"))){o(e.key);b[e.key]=e;i=a.data(j.parentNode,"tmplItem");i=i?i.key:0}if(e){k=e;while(k&&k.key!=i){k.nodes.push(j);k=k.parent}delete e._ctnt;delete e._wrap;a.data(j,"tmplItem",e)}function o(a){a=a+n;e=l[a]=l[a]||g(e,b[e.parent.key+n]||e.parent,null,true)}}}function u(a,d,c,b){if(!a)return l.pop();l.push({_:a,tmpl:d,item:this,data:c,options:b})}function w(d,c,b){return a.tmpl(a.template(d),c,b,this)}function x(b,d){var c=b.options||{};c.wrapped=d;return a.tmpl(a.template(b.tmpl),b.data,c,b.item)}function v(d,c){var b=this._wrap;return a.map(a(a.isArray(b)?b.join(""):b).filter(d||"*"),function(a){return c?a.innerText||a.textContent:a.outerHTML||s(a)})}function t(){var b=this.nodes;a.tmpl(null,null,null,this).insertBefore(b[0]);a(b).remove()}})(jQuery)
\ No newline at end of file
diff --git a/lib/logstash/web/public/js/logstash.js b/lib/logstash/web/public/js/logstash.js
deleted file mode 100644
index 028b53572f0..00000000000
--- a/lib/logstash/web/public/js/logstash.js
+++ /dev/null
@@ -1,354 +0,0 @@
-(function() {
-  // TODO(sissel): this code could use serious refactorlove.
-  // TODO(sissel): Write something that will use history.pushState and fall back
-  // to document.location.hash madness.
-  
-  if (typeof(window.console) === 'undefined') {
-    window.console = { 
-      log: function() { 
-        // no-op if we don't have a console logger.
-      }
-    };
-  }
-
-  var logstash = {
-    params: { 
-      offset: 0,
-      count: 50,
-    },
-
-    search: function(query, options) {
-      if (query == undefined || query == "") {
-        return;
-      }
-
-      /* Default options */
-      if (typeof(options) == 'undefined') {
-        options = { graph: true };
-      }
-
-      /* Check for history.pushState */
-
-      var display_query = query.replace("<", "&lt;").replace(">", "&gt;")
-      $("#querystatus, #results h1").html("Loading query '" + display_query + "' (offset:" + logstash.params.offset + ", count:" + logstash.params.count + ") <img class='throbber' src='/media/throbber.gif'>")
-      //console.log(logstash.params)
-      logstash.params.q = query;
-
-      /* Load the search results */
-      $("#results").load("/api/search?format=html", logstash.params);
-
-      if (options.graph != false) {
-        /* Load the default histogram graph */
-        logstash.params.interval = 3600000; /* 1 hour, default */
-        logstash.histogram();
-      } /* if options.graph != false */
-      $("#query").val(logstash.params.q);
-      logstash.hash = document.location.hash = escape(JSON.stringify(logstash.params));
-    }, /* search */
-
-    histogram: function(tries) {
-      if (typeof(tries) == 'undefined') {
-        /* GeoCities mode on the graph while waiting ...
-         * This won't likely survive 1.0, but it's fun for now... 
-         * TODO(sissel): Replace with a 'loading' or something icon. */
-
-        $("#visual").html("<center><img src='/media/throbber.gif'><center>");
-
-        tries = 4; /* default tries */
-      }
-
-      jQuery.getJSON("/api/histogram", logstash.params, function(histogram, text, jqxhr) {
-        /* Load the data into the graph */
-        var flot_data = [];
-        // histogram is an array of { "key": ..., "count": ... }
-        for (var i in histogram) {
-          flot_data.push([parseInt(histogram[i]["key"]), histogram[i]["count"]])
-        }
-
-        /* Try to be intelligent about how we choose the histogram interval.
-         * If there are too few data points, try a smaller interval.
-         * If there are too many data points, try a larger interval.
-         * Give up after a few tries and go with the last result. 
-         *
-         * This queries the backend several times, but should be reasonably
-         * speedy as this behaves roughly as a binary search. */
-
-        /* I originally used '2' for this value for binary search-like tuning
-         * on the graph interval, but it's not fast enough, so use 3 for now.
-         */
-        jumpsize = 3
-        if (tries > 0 && logstash.params.interval > 1000) {
-          if (flot_data.length < 6 && flot_data.length > 0) {
-            console.log("Histogram bucket " + logstash.params.interval + " has only " + flot_data.length + " data points, trying smaller...");
-            logstash.params.interval /= jumpsize;
-          } else if (flot_data.length > 50) {
-            console.log("Histogram bucket " + logstash.params.interval + " too many (" + flot_data.length + ") data points, trying larger interval...");
-            logstash.params.interval *= jumpsize;
-          }
-          logstash.histogram(tries - 1);
-          return;
-        } else {
-          /* Got a good interval, plot it */
-          logstash.plot(flot_data, logstash.params.interval);
-        }
-      });
-    },
-
-    parse_params: function(href) {
-      var query = href.replace(/^[^?]*\?/, "");
-      if (query == href) {
-        //console.log("No query params in link " + href);
-        /* No query params */
-        return {};
-      }
-
-      //console.log({ "query": query });
-      var param_list = query.split("&");
-      params = {};
-      //console.log({ "Parsed params": params });
-      for (var p in param_list) {
-        var a = param_list[p].split("=");
-        var key = a[0];
-        var value = a[1];
-        params[key] = unescape(value);
-      }
-      return params;
-    },
-
-    appendquery: function(query) {
-      var newquery = $("#query").val();
-      newquery += " " + query;
-      logstash.search(newquery.trim());
-    }, /* appendquery */
-
-    plot: function(data, interval) {
-      var target = $("#visual");
-      target.css("display", "block");
-      target.children().remove();
-      console.log("Plotting data")
-      console.log(data)
-
-      var plot = $.plot(target,
-        [ {  /* data */
-            data: data,
-            bars: { 
-              show: true,
-              barWidth: interval,
-            }
-        } ],
-        { /* options */
-          xaxis: { mode: "time" },
-          grid: { hoverable: true, clickable: true },
-        }
-      );
-
-      /* Remove any previous bindings on this plot
-       * If we don't do this, then all previously-registered events will fire
-       * for this plot when clicked, and that's bad. */
-      target.unbind("plotclick");
-      target.bind("plotclick", function(e, pos, item) {
-        if (item) {
-          /* The following commented code should not be necessary anymore since I
-           * believe I fixed the 'multiple plotclick' bug. */
-          //var now = (new Date()).getTime();
-          //if (now - logstash.last_plot_click < 50) { 
-            /* TODO(sissel): debug why this happens.
-             * For some reason, sometimes clicking on the plot will result in
-             * more than one click event being fired. I've seen up to 5 'clicks'
-             * being fired for the same event. Very strange.
-             *
-             * This hack should block extra/buggy clicks from being processed.
-             */ 
-            //console.log("Skipping extra click on plot?")
-            //return;
-          //}
-          //console.log("plotclick; last time: " + (now - logstash.last_plot_click));
-          //logstash.last_plot_click = now;
-          console.log(e, pos, item)
-          start = logstash.ms_to_iso8601(item.datapoint[0]);
-          end = logstash.ms_to_iso8601(item.datapoint[0] + interval);
-
-          /* Clicking on the graph means a new search, means
-           * we probably don't want to keep the old offset since
-           * the search results will change. */
-          logstash.params.offset = 0;
-          logstash.appendquery("@timestamp:[" + start + " TO " + end + "]");
-        }
-      });
-    }, /* plot */
-
-    ms_to_iso8601: function(milliseconds) {
-      /* From: 
-       * https://developer.mozilla.org/en/JavaScript/Reference/global_objects/date#Example.3a_ISO_8601_formatted_dates
-       */
-      var d = new Date(milliseconds);
-      function pad(n){return n<10 ? '0'+n : n}
-      return d.getUTCFullYear()+'-'
-        + pad(d.getUTCMonth()+1)+'-'
-        + pad(d.getUTCDate())+'T'
-        + pad(d.getUTCHours())+':'
-        + pad(d.getUTCMinutes())+':'
-        + pad(d.getUTCSeconds())+'Z'
-    },
-  }; /* logstash */
-
-  window.logstash = logstash;
-
-  $().ready(function() {
-    if (location.hash.length > 1) {
-      try {
-        logstash.params = JSON.parse(unescape(location.hash.substring(1)));
-      } catch (e) {
-        // Do nothing 
-      }
-      logstash.search(logstash.params.q);
-    } else {
-      /* No hash. See if there's a query param. */
-      var params = logstash.parse_params(location.href);
-      //console.log(params)
-      for (var p in params) {
-        logstash.params[p] = params[p];
-      }
-      logstash.search(logstash.params.q)
-    }
-
-    $(window).hashchange(function() {
-      /* Sometimes hashchange is called when we change the hash ourselves from
-       * javascript. We don't want that. */
-      if (logstash.hash === location.hash.substring(1)) {
-        console.log("Ignoring superfluous hashchange")
-        return;
-      }
-      logstash.params = JSON.parse(unescape(location.hash.substring(1)));
-      console.log("hashchange");
-      query = logstash.params.q
-      if (query != $("#query").val()) {
-        scroll(0, 0); 
-        logstash.search(query);
-      }
-    });
-
-    $("a.pager, a.querychanger").live("click", function() {
-      /* TODO(sissel): Allow 'control click' and 'middle click' to act normally */
-      var href = $(this).attr("href");
-      var params = logstash.parse_params(href);
-      for (var p in params) {
-        logstash.params[p] = params[p];
-      }
-      logstash.search(logstash.params.q, { graph: false })
-      return false;
-    });
-
-    var result_row_selector = "table.results tr.event";
-    $(result_row_selector).live("click", function() {
-      var data = $("td.message", this).data("full");
-      if (typeof(data) == "string") {
-        data = JSON.parse(data);
-      }
-
-      /* Apply template to the dialog */
-      var query = $("#query").val().replace(/^\s+|\s+$/g, "")
-      var sanitize = function(str) {
-        if (!/^".*"$/.test(str)) {
-          str = '"' + str + '"';
-        }
-        return escape(str);
-      };
-
-      var template = $.template("inspector",
-        "<li>" +
-          "<b>(${type}) ${field}</b>:" +
-          "{{each(idx, val) value}}" +
-            "<a href='/search?q=" + query + " ${escape(field)}:${$item.sanitize(val)}'" +
-            "   data-field='${escape(field)}' data-value='${$item.sanitize(val)}'>" +
-              "${val}" +
-            "</a>, " +
-          "{{/each}}" +
-        "</li>");
-
-      /* TODO(sissel): recurse through the data */
-      var fields = new Array();
-      for (var i in data["@fields"]) {
-        var value = data["@fields"][i]
-        if (/^[, ]*$/.test(value)) {
-          continue; /* Skip empty data fields */
-        }
-        if (!(value instanceof Array)) {
-          value = [value];
-        }
-        fields.push( { type: "field", field: i, value: value })
-      }
-
-      for (var i in data) {
-        if (i == "@fields") continue;
-        var value = data[i]
-        if (!(value instanceof Array)) {
-          value = [value];
-        }
-
-        if (i.charAt(0) == "@") { /* metadata */
-          fields.push( { type: "metadata", field: i, value: value });
-        } else { /* data */
-          if (/^[, ]*$/.test(value)) {
-            continue; /* Skip empty data fields */
-          }
-          fields.push( { type: "field", field: i, value: value })
-        }
-      }
-
-      //for (var i in data) {
-        //if (i == "_source") {
-          //continue; /* already processed this one */
-        //}
-        //value = data[i]
-        //if (!(value instanceof Array)) {
-          //value = [value];
-        //}
-        //fields.push( { type: "metadata", field: i, value: value })
-      //}
-
-      fields.sort(function(a, b) {
-        if (a.type+a.field < b.type+b.field) { return -1; }
-        if (a.type+a.field > b.type+b.field) { return 1; }
-        return 0;
-      });
-
-      $(result_row_selector).removeClass("selected")
-      $(this).addClass("selected");
-      var entry = this;
-      $("#inspector li").remove()
-      $("#inspector")
-        .append($.tmpl("inspector", fields, { "sanitize": sanitize }))
-        .dialog({ 
-          width: 400,
-          title: "Fields for this log" ,
-          closeOnEscape: true,
-          position: ["right", "top"],
-        });
-    });
-
-    $("#inspector li a").live("click", function(ev) {
-      var field = $(this).data("field");
-      var value = $(this).data("value");
-      var query = $("#query");
-      var newcondition = unescape(field) + ":" + unescape(value);
-
-      var newquery = query.val();
-      if (ev.shiftKey) {
-        // Shift-click will make a "and not" condition
-        query.val(newquery + " -" + newcondition)
-      } else {
-        query.val(newquery + " " + newcondition)
-      }
-      logstash.search(query.val())
-      return false;
-    });
-
-    $("#searchbutton").bind("click submit", function(ev) {
-      var query = $("#query").val().replace(/^\s+|\s+$/g, "")
-      /* Search now, we pressed the submit button */
-      logstash.search(query)
-      return false;
-    });
-  }); /* $().ready */
-})(); /* function scoping */
diff --git a/lib/logstash/web/public/media/construction.gif b/lib/logstash/web/public/media/construction.gif
deleted file mode 100644
index 421f85f15c9..00000000000
Binary files a/lib/logstash/web/public/media/construction.gif and /dev/null differ
diff --git a/lib/logstash/web/public/media/throbber.gif b/lib/logstash/web/public/media/throbber.gif
deleted file mode 100644
index 9e75bd4bae2..00000000000
Binary files a/lib/logstash/web/public/media/throbber.gif and /dev/null differ
diff --git a/lib/logstash/web/public/media/truckconstruction.gif b/lib/logstash/web/public/media/truckconstruction.gif
deleted file mode 100644
index 462796afa26..00000000000
Binary files a/lib/logstash/web/public/media/truckconstruction.gif and /dev/null differ
diff --git a/lib/logstash/web/public/ws/index.html b/lib/logstash/web/public/ws/index.html
deleted file mode 100644
index 755af7b577f..00000000000
--- a/lib/logstash/web/public/ws/index.html
+++ /dev/null
@@ -1,76 +0,0 @@
-<!DOCTYPE html>
-<html>
-  <head>
-    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.4.3/jquery.min.js"></script>
-    <script src="../js/jquery.tmpl.min.js"></script>
-    <style type="text/css">
-      #radiator {
-        margin-top: 1em;
-        padding-left:10px;
-        padding-right:10px;
-        font-size: 120%;
-        border-top: 1px solid black;
-        width: 100%;
-      }
-
-      #radiator .entry:nth-child(2n) {
-        background-color: #C4FFC6;
-      }
-
-      #radiator .timestamp {
-        white-space: nowrap;
-        text-align: left;
-        vertical-align: top;
-        font-family: monospace;
-        font-size: 75%;
-        padding-right: 1em;
-        width: 14em;
-        border-right: 1px solid grey;
-      }
-
-      #radiator .message {
-        font-family: monospace;
-        //text-indent: -2em;
-        //padding-left: 3em;
-      }
-    </style>
-  </head>
-  <body>
-    <h1> logstash information radiator </h1>
-
-    New log data will be streamed here in real-time as logstash
-    receives it.
-
-    <table id="radiator"></table>
-
-    <script id="message-template" type="text/x-jquery-tmpl"><![CDATA[
-      <tr>
-          <td class="timestamp">${$item.data["@timestamp"]}</td>
-          <td class="message">${$item.data["@message"]}</td>
-      </tr>
-    ]]></script>
-
-    <script>
-      $(document).ready(function() {
-        var ws = new WebSocket("ws://" + document.location.hostname + ":3232");
-        ws.onopen = function(event) {
-          //console.log(["WebSocket open", ws])
-        };
-        ws.onmessage = function(event) {
-          var data = JSON.parse(event.data); 
-          var el = $("#message-template").tmpl(data, { "message": data["@message"], "timestamp": data["@timestamp"] });
-            //.css("display", "none")
-            //.fadeIn()
-          el.addClass("message")
-            .appendTo($("#radiator"))
-            //.delay(10000)
-            //.hide(2000, function() {
-              //$(this).remove();
-            //});
-          setTimeout(function() { $(el).remove() }, 12000)
-            //.fadeOut(2000, function() {
-        };
-      });
-    </script>
-  </body>
-</html>
diff --git a/lib/logstash/web/server.rb b/lib/logstash/web/server.rb
deleted file mode 100755
index 76cf13cf451..00000000000
--- a/lib/logstash/web/server.rb
+++ /dev/null
@@ -1,151 +0,0 @@
-#!/usr/bin/env ruby
-# I don't want folks to have to learn to use yet another tool (rackup)
-# just to launch logstash-web. So let's work like a standard ruby
-# executable.
-##rackup -Ilib:../lib -s thin
-
-$:.unshift("%s/../lib" % File.dirname(__FILE__))
-$:.unshift(File.dirname(__FILE__))
-
-require "logstash/search/elasticsearch"
-require "logstash/search/query"
-require "logstash/namespace"
-require "logstash/web/helpers/require_param"
-require "json" # gem json
-require "rack" # gem rack
-require "mizuno" # gem mizuno
-require "sinatra/base" # gem sinatra
-
-
-class LogStash::Web::Server < Sinatra::Base
-  mime_type :html, "text/html"
-  mime_type :txt, "text/plain"
-  mime_type :json, "text/plain" # so browsers don't "download" when viewed
-  mime_type :javascript, "application/javascript"
-  mime_type :gif, "image/gif"
-  mime_type :jpg, "image/jpeg"
-  mime_type :png, "image/png"
-
-  require "logstash/web/controllers/api_v1"
-  require "logstash/web/controllers/static_files"
-  require "logstash/web/controllers/search"
-
-  #register Sinatra::Async
-  helpers Sinatra::RequireParam # logstash/web/helpers/require_param
-
-  set :haml, :format => :html5
-  set :logging, true
-  set :views, "#{File.dirname(__FILE__)}/views"
-
-  use Rack::CommonLogger
-  use Rack::ShowExceptions
-
-  # We could do 'use' here, but 'use' is for middleware and it seems difficult
-  # (intentionally?) to share between middlewares. We'd need to share the
-  # instances variables like @backend, etc.
-  #
-  # Load anything in controllers/
-  #Dir.glob(File.join(File.dirname(__FILE__), "controllers", "**", "*")).each do |path|
-    #puts "Loading #{path}"
-    ## TODO(sissel): This is pretty shitty.
-    #eval(File.new(path).read, binding, path)
-  #end
-
-  def initialize(settings={})
-    super
-    # TODO(sissel): Make this better.
-    backend_url = URI.parse(settings.backend_url)
-
-    case backend_url.scheme 
-      when "elasticsearch"
-        # if host is nil, it will 
-        # TODO(sissel): Support 'cluster' name?
-        @backend = LogStash::Search::ElasticSearch.new(
-          :host => backend_url.host,
-          :port => backend_url.port
-        )
-      when "twitter"
-        require "logstash/search/twitter"
-        @backend = LogStash::Search::Twitter.new(
-          :host => backend_url.host,
-          :port => backend_url.port
-        )
-    end # backend_url.scheme
-  end # def initialize
- 
-  get '/style.css' do
-    headers "Content-Type" => "text/css; charset=utf8"
-    body sass :style
-  end # /style.css
-
-  get '/' do
-    redirect "/search"
-  end # '/'
-
-  get '/*' do
-    status 404 if @error
-    body "Invalid path."
-  end # get /*
-end # class LogStash::Web::Server
-
-require "optparse"
-Settings = Struct.new(:daemonize, :logfile, :address, :port, :backend_url)
-settings = Settings.new
-
-settings.address = "0.0.0.0"
-settings.port = 9292
-settings.backend_url = "elasticsearch:///"
-
-progname = File.basename($0)
-
-opts = OptionParser.new do |opts|
-  opts.banner = "Usage: #{progname} [options]"
-
-  opts.on("-d", "--daemonize", "Daemonize (default is run in foreground).") do
-    settings.daemonize = true
-  end
-
-  opts.on("-l", "--log FILE", "Log to a given path. Default is stdout.") do |path|
-    settings.logfile = path
-  end
-
-  opts.on("-a", "--address ADDRESS", "Address on which to start webserver. Default is 0.0.0.0.") do |address|
-    settings.address = address
-  end
-
-  opts.on("-p", "--port PORT", "Port on which to start webserver. Default is 9292.") do |port|
-    settings.port = port.to_i
-  end
-
-  opts.on("-b", "--backend URL",
-          "The backend URL to use. Default is elasticserach:/// (assumes multicast discovery)") do |url|
-    settings.backend_url = url
-  end
-end
-
-opts.parse!
-
-if settings.daemonize
-  $stderr.puts "Daemonizing is not supported. (JRuby has no 'fork')"
-  exit(1)
-  #if Process.fork == nil
-    #Process.setsid
-  #else
-    #exit(0)
-  #end
-end
-
-if settings.logfile
-  logfile = File.open(settings.logfile, "w")
-  STDOUT.reopen(logfile)
-  STDERR.reopen(logfile)
-elsif settings.daemonize
-  # Write to /dev/null if
-  devnull = File.open("/dev/null", "w")
-  STDOUT.reopen(devnull)
-  STDERR.reopen(devnull)
-end
-
-Mizuno::HttpServer.run(
-  LogStash::Web::Server.new(settings),
-  :port => settings.port, :host => settings.address)
diff --git a/lib/logstash/web/views/header.haml b/lib/logstash/web/views/header.haml
deleted file mode 100644
index 29cc922ac6d..00000000000
--- a/lib/logstash/web/views/header.haml
+++ /dev/null
@@ -1,9 +0,0 @@
-.logo
-  logstash.
-.search
-  %form.search{ :action => "/search" }
-    %label{ :for => "q" } Query:
-    %input.query{ :id => "query", :type => "text", :name => "q", :value => params[:q],
-                  :size => 60 }
-    %input{ :id => "searchbutton", :type => "submit", :value => "Search" }
-    %input{ :id => "clearbutton", :type => "reset", :value => "Clear" }
diff --git a/lib/logstash/web/views/layout.haml b/lib/logstash/web/views/layout.haml
deleted file mode 100644
index 37e2060ab72..00000000000
--- a/lib/logstash/web/views/layout.haml
+++ /dev/null
@@ -1,21 +0,0 @@
-!!! 5
-%html
-  %head
-    %title= @title || "logstash" 
-    %link{ :rel => "stylesheet", :href => "/style.css", :type => "text/css" }
-    %link{ :rel => "stylesheet", :href => "/css/smoothness/jquery-ui-1.8.5.custom.css", :type => "text/css" }
-    %script{ :src => "https://ajax.googleapis.com/ajax/libs/jquery/1.5.0/jquery.min.js", 
-             :type => "text/javascript" }
-%body
-  #header
-    =haml :header, :layout => false
-  #content
-    =yield
-  #footer
-
-  %script{ :src => "https://ajax.googleapis.com/ajax/libs/jqueryui/1.8.5/jquery-ui.min.js", 
-           :type => "text/javascript" }
-  %script{ :src => "js/jquery.tmpl.min.js", :type => "text/javascript" }
-  %script{ :src => "js/jquery-hashchange-1.0.0.js", :type => "text/javascript" }
-  %script{ :src => "js/flot/jquery.flot.js", :type => "text/javascript" }
-  %script{ :src => "js/logstash.js", :type => "text/javascript" }
diff --git a/lib/logstash/web/views/main/index.haml b/lib/logstash/web/views/main/index.haml
deleted file mode 100644
index 0c04e4ddd1e..00000000000
--- a/lib/logstash/web/views/main/index.haml
+++ /dev/null
@@ -1,5 +0,0 @@
-%form.search{ :action => "/search" }
-  %label{ :for => "q" }
-  %input.query{ :type => "text", :name => "q" }
-  %input{ :type => "submit" }
-
diff --git a/lib/logstash/web/views/search/ajax.haml b/lib/logstash/web/views/search/ajax.haml
deleted file mode 100644
index ee43c649679..00000000000
--- a/lib/logstash/web/views/search/ajax.haml
+++ /dev/null
@@ -1,55 +0,0 @@
-#results
-  - if (params[:q].strip.length > 0 rescue false)
-    %h1
-      Search results for '#{params[:q]}'
-  - if @total and @result_start and @result_end
-    %small
-      %strong
-        Results #{@result_start} - #{@result_end} of #{@results.total}
-      |
-      - if @first_href
-        %a.pager{ :href => @first_href } first
-      - else
-        %span.unavailable first
-      |
-      - if @prev_href
-        %a.pager{ :href => @prev_href } 
-          prev
-      - else
-        %span.unavailable prev
-      |
-      - if @next_href
-        %a.pager{ :href => @next_href }
-          next
-      - else
-        %span.unavailable next
-      |
-      - if @last_href
-        %a.pager{ :href => @last_href }
-          last
-      - else
-        %span.unavailable last
-      |
-      %a.pager{ :href => @refresh_href }
-        refresh
-      |
-      %span#querytime= "(%.3f seconds)" % @results.duration
-  - if @results.events.length == 0 
-    - if !params[:q]
-      / We default to a '+2 days' in the future  to capture 'today at 00:00'
-      / plus tomorrow, inclusive, in case you are 23 hours behind the international
-      / dateline.
-      %h3#querystatus No query given. How about <a href="?q=* @timestamp:[#{(Time.now - 7*24*60*60).strftime("%Y-%m-%d")} TO #{(Time.now + 2*24*60*60).strftime("%Y-%m-%d")}]" class="querychanger">this?</a>
-    - else
-      %h3#querystatus No results for query '#{params[:q]}' 
-  - else
-    %table.results
-      %tr
-        %th timestamp
-        %th event
-      - @results.events.reverse.each do |event|
-        %tr.event
-          %td.timestamp&= event.timestamp
-          %td.message{ :"data-full" => event.to_json }
-            %a{:href => "#"}
-              %pre&= event.message
diff --git a/lib/logstash/web/views/search/error.haml b/lib/logstash/web/views/search/error.haml
deleted file mode 100644
index 3ca32eb715d..00000000000
--- a/lib/logstash/web/views/search/error.haml
+++ /dev/null
@@ -1,3 +0,0 @@
-#error
-  %h4 The query '#{params["q"]}' resulted the following error:
-  %pre&= @results.error_message
diff --git a/lib/logstash/web/views/search/error.txt.erb b/lib/logstash/web/views/search/error.txt.erb
deleted file mode 100644
index d8b06e44aaf..00000000000
--- a/lib/logstash/web/views/search/error.txt.erb
+++ /dev/null
@@ -1,4 +0,0 @@
-An error occured in query '<%= params[:q] %>'
-ERROR:
-
-<%= @results.error_message %>
diff --git a/lib/logstash/web/views/search/results.haml b/lib/logstash/web/views/search/results.haml
deleted file mode 100644
index 66c24225407..00000000000
--- a/lib/logstash/web/views/search/results.haml
+++ /dev/null
@@ -1,20 +0,0 @@
-- if @error
-  #error 
-    %strong A search error occurred:
-    =@error
-#ssquery{ :style => "display: none;", :"data-query" => params[:q] }
-#inspector{ :style => "display: none;" }
-  The following fields are known for the log you selected. Click on any link to
-  append it to your search. If you shift+click, the field will be added to the
-  search as an exclude rather than include.
-  %ul
-
-%i
-  You can click on any search result to see what kind of fields we know about
-  for that event. You can also click on the graph to zoom to that time period.
-  The query language is that of Lucene's string query (<a href="http://lucene.apache.org/java/2_4_0/queryparsersyntax.html">docs</a>).
-
-
-#visual
-
-=haml :"search/ajax", :layout => false
diff --git a/lib/logstash/web/views/search/results.txt.erb b/lib/logstash/web/views/search/results.txt.erb
deleted file mode 100644
index 00e30c79e50..00000000000
--- a/lib/logstash/web/views/search/results.txt.erb
+++ /dev/null
@@ -1,9 +0,0 @@
-<% 
-  # Sinatra currently doesn't do ERB with newline trimming, so we
-  # have to write this funky mishmosh on one line that is hard to read.
-if @results.error? %>Error: <%= @results.error_message%><% else
-  @results.events.each do |event|
-%><%= event.message || event.to_hash.to_json %>
-<%   end
-   end 
-%>
diff --git a/lib/logstash/web/views/style.sass b/lib/logstash/web/views/style.sass
deleted file mode 100644
index 974b61adacb..00000000000
--- a/lib/logstash/web/views/style.sass
+++ /dev/null
@@ -1,67 +0,0 @@
-$lightgrey: #d8d8d8
-$darkgrey: #adadad
-body
-  margin: 0
-  padding: 0
-#header
-  border-top: 4px solid black
-  border-bottom: 1px solid black
-  background-color: lightgreen
-  padding-left: 1em
-
-  .search
-    display: inline
-  .logo
-    font-size: 130%
-    font-weight: bold
-    float: right
-    padding-right: 20px
-#content
-  margin-left: 2em
-  margin-right: 2em
-  margin-top: 1em
-#content table.results
-  font-family: monospace
-#content td.message
-  vertical-align: top
-  padding: 1px
-  padding-bottom: 3px
-  pre
-    white-space: pre-wrap
-    margin: 0
-  a
-    text-decoration: none
-    color: black
-#content td.timestamp
-  white-space: nowrap
-  padding: 1px
-  //font-size: 85%
-  vertical-align: top
-#content tr.selected
-  background-color: #FCE69D !important
-#content tr.event:nth-child(2n)
-  background-color: #E3F6CE
-#content tr.event:nth-child(2n+1)
-  background-color: #F5FBEF
-#content tr.event:hover
-  background-color: lightgreen
-#error
-  background-color: pink
-  border: 1px solid red
-  padding: 3px
-  pre
-    white-space: pre-wrap
-#error h1
-  font-size: 130%
-  padding: 0
-  margin: 0
-#inspector
-  font-size: 70%
-#visual
-  width: 850px
-  height: 200px
-  display: none
-#results h1
-  font-size: 100%
-img.throbber
-  vertical-align: top
diff --git a/locales/en.yml b/locales/en.yml
new file mode 100644
index 00000000000..defe6e78d0d
--- /dev/null
+++ b/locales/en.yml
@@ -0,0 +1,164 @@
+# YAML notes
+#   |- means 'scalar block' useful for formatted text
+#   > means 'scalar block' but it chomps all newlines. Useful 
+#     for unformatted text.
+en:
+  oops: |-
+    The error reported is: 
+      %{error}
+  logstash:
+    pipeline:
+      worker-error: |-
+        A plugin had an unrecoverable error. Will restart this plugin.
+          Plugin: %{plugin}
+          Error: %{error}
+      worker-error-debug: |-
+        A plugin had an unrecoverable error. Will restart this plugin.
+          Plugin: %{plugin}
+          Error: %{error}
+          Exception: %{exception}
+          Stack: %{stacktrace}
+      plugin-loading-error: >-
+        Couldn't find any %{type} plugin named '%{name}'. Are you
+        sure this is correct? Trying to load the %{name} %{type} plugin
+        resulted in this error: %{error}
+      plugin-type-loading-error: >-
+        Could not find any plugin type named '%{type}'. Check for typos.
+        Valid plugin types are 'input' 'filter' and 'output'
+      output-worker-unsupported: >-
+        %{plugin} output plugin: setting 'workers => %{worker_count}' is not
+        supported by this plugin. I will continue working as if you had not set
+        this setting.
+      output-worker-unsupported-with-message: >-
+        %{plugin} output plugin: setting 'workers => %{worker_count}' is not
+        supported by this plugin. I will continue working as if you had not set
+        this setting.
+    plugin:
+      milestone:
+        "0": >-
+          Using milestone 0 %{type} plugin '%{name}'. This plugin isn't well
+          supported by the community and likely has no maintainer. For more
+          information on plugin milestones, see
+          http://logstash.net/docs/%{LOGSTASH_VERSION}/plugin-milestones
+        "1": >-
+          Using milestone 1 %{type} plugin '%{name}'. This plugin should work,
+          but would benefit from use by folks like you. Please let us know if you
+          find bugs or have suggestions on how to improve this plugin.  For more
+          information on plugin milestones, see
+          http://logstash.net/docs/%{LOGSTASH_VERSION}/plugin-milestones
+        "2": >-
+          Using milestone 2 %{type} plugin '%{name}'. This plugin should be
+          stable, but if you see strange behavior, please let us know!
+          For more information on plugin milestones, see
+          http://logstash.net/docs/%{LOGSTASH_VERSION}/plugin-milestones
+    agent:
+      sighup: >-
+        SIGHUP received.
+      missing-configuration: >-
+        No configuration file was specified. Perhaps you forgot to provide
+        the '-f yourlogstash.conf' flag?
+      error: >-
+        Error: %{error}
+      interrupted: >-
+        Interrupt received. Shutting down the pipeline.
+      configtest-flag-information: |-
+        You may be interested in the '--configtest' flag which you can
+        use to validate logstash's configuration before you choose
+        to restart a running system.
+      configuration:
+        file-not-found: |-
+          No config files found: %{path}
+          Can you make sure this path is a logstash config file?
+        setting_missing: |-
+          Missing a required setting for the %{plugin} %{type} plugin:
+
+            %{type} {
+              %{plugin} {
+                %{setting} => # SETTING MISSING
+                ...
+              }
+            }
+        setting_invalid: |-
+          Invalid setting for %{plugin} %{type} plugin:
+
+            %{type} {
+              %{plugin} {
+                # This setting must be a %{value_type}
+                # %{note}
+                %{setting} => %{value}
+                ...
+              }
+            }
+        invalid_plugin_settings: >-
+          Something is wrong with your configuration.
+        invalid_plugin_register: >-
+          Cannot register %{plugin} %{type} plugin.
+          The error reported is: 
+            %{error}
+        plugin_path_missing: >-
+          You specified a plugin path that does not exist: %{path}
+        no_plugins_found: |-
+          Could not find any plugins in "%{path}"
+          I tried to find files matching the following, but found none: 
+            %{plugin_glob}
+        log_file_failed: |-
+          Failed to open %{path} for writing: %{error}
+
+          This is often a permissions issue, or the wrong 
+          path was specified?
+      flag:
+        # Note: Wrap these at 45 chars so they display nicely when clamp emits
+        # them in an 80-character terminal
+        config: |+
+          Load the logstash config from a specific file
+          or directory.  If a directory is given, all
+          files in that directory will be concatenated
+          in lexicographical order and then parsed as a
+          single config file. You can also specify
+          wildcards (globs) and any matched files will
+          be loaded in the order described above.
+        config-string: |+
+          Use the given string as the configuration
+          data. Same syntax as the config file. If not
+          input is specified, then 'stdin { type =>
+          stdin }' is the default input. If no output
+          is specified, then 'stdout { debug => true
+          }}' is default output.
+        filterworkers: |+
+          Sets the number of filter workers to run.
+        watchdog-timeout: |+
+          Set the filter watchdog timeout (in seconds).
+          This timeout is used to detect stuck filters;
+          stuck filters usually symptoms of bugs.
+          When a filter takes longer than TIMEOUT
+          seconds, it will cause logstash to abort.
+        log: |+
+          Write logstash internal logs to the given
+          file. Without this flag, logstash will emit
+          logs to standard output.
+        verbosity: |+
+          Increase verbosity of logstash internal logs.
+          Specifying once will show 'informational'
+          logs. Specifying twice will show 'debug'
+          logs. This flag is deprecated. You should use
+          --verbose or --debug instead.
+        version: |+
+          Emit the version of logstash and its friends,
+          then exit.
+        pluginpath: |+
+          A path of where to find plugins. This flag
+          can be given multiple times to include
+          multiple paths. Plugins are expected to be
+          in a specific directory hierarchy:
+          'PATH/logstash/TYPE/NAME.rb' where TYPE is
+          'input' 'filter' or 'output' and NAME is the
+          name of the plugin.
+        quiet: |+
+          Quieter logstash logging. This causes only 
+          errors to be emitted.
+        verbose: |+
+          More verbose logging. This causes 'info' 
+          level logs to be emitted.
+        debug: |+
+          Most verbose logging. This causes 'debug'
+          level logs to be emitted.
diff --git a/logstash-event.gemspec b/logstash-event.gemspec
new file mode 100644
index 00000000000..41286036830
--- /dev/null
+++ b/logstash-event.gemspec
@@ -0,0 +1,29 @@
+# -*- encoding: utf-8 -*-
+Gem::Specification.new do |gem|
+  gem.authors       = ["Jordan Sissel"]
+  gem.email         = ["jls@semicomplete.com"]
+  gem.description   = %q{Library that contains the classes required to create LogStash events}
+  gem.summary       = %q{Library that contains the classes required to create LogStash events}
+  gem.homepage      = "https://github.com/logstash/logstash"
+  gem.license       = "Apache License (2.0)"
+
+  gem.files = %w{
+    lib/logstash-event.rb
+    lib/logstash/event.rb
+    lib/logstash/namespace.rb
+    lib/logstash/util/fieldreference.rb
+    lib/logstash/util.rb
+    spec/event.rb
+    LICENSE
+  }
+
+  gem.test_files    = []
+  gem.name          = "logstash-event"
+  gem.require_paths = ["lib"]
+  gem.version       = "1.2.02"
+  
+  gem.add_development_dependency "rspec"
+  gem.add_development_dependency "guard"
+  gem.add_development_dependency "guard-rspec"
+  gem.add_development_dependency "insist", "1.0.0"
+end
diff --git a/logstash.gemspec b/logstash.gemspec
index 32010323d1f..50ae8ce7806 100644
--- a/logstash.gemspec
+++ b/logstash.gemspec
@@ -1,67 +1,127 @@
-Gem::Specification.new do |spec|
-  files = []
-  paths = %w{lib examples etc patterns}
-  paths << "test/logstash/"
-  paths << "test/logstash_test_runner.rb"
-  paths << "test/standalone.sh"
-  paths << "test/setup/elasticsearch/Makefile"
-  paths.each do |path|
-    if File.file?(path)
-      files << path
-    else
-      files += Dir["#{path}/**/*"]
-    end
+# -*- encoding: utf-8 -*-
+require File.expand_path('../lib/logstash/version', __FILE__)
+
+Gem::Specification.new do |gem|
+  gem.authors       = ["Jordan Sissel", "Pete Fritchman"]
+  gem.email         = ["jls@semicomplete.com", "petef@databits.net"]
+  gem.description   = %q{scalable log and event management (search, archive, pipeline)}
+  gem.summary       = %q{logstash - log and event management}
+  gem.homepage      = "http://logstash.net/"
+  gem.license       = "Apache License (2.0)"
+
+  gem.files         = `git ls-files`.split($\)
+  gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
+  gem.name          = "logstash"
+  gem.require_paths = ["lib"]
+  gem.version       = LOGSTASH_VERSION
+
+  # Core dependencies
+  gem.add_runtime_dependency "cabin", [">=0.6.0"] #(Apache 2.0 license)
+  gem.add_runtime_dependency "minitest"           #(MIT license) for running the tests from the jar,
+  gem.add_runtime_dependency "pry"                #(Ruby license)
+  gem.add_runtime_dependency "stud"               #(Apache 2.0 license)
+  gem.add_runtime_dependency "clamp"              #(MIT license) for command line args/flags
+
+  # upgrade i18n only post 0.6.11, see https://github.com/svenfuchs/i18n/issues/270
+  gem.add_runtime_dependency "i18n", ["=0.6.9"]   #(MIT license)
+
+  # Web dependencies
+  gem.add_runtime_dependency "ftw", ["~> 0.0.39"] #(Apache 2.0 license)
+  gem.add_runtime_dependency "mime-types"         #(GPL 2.0)
+  gem.add_runtime_dependency "rack"               #(MIT-style license)
+  gem.add_runtime_dependency "sinatra"            #(MIT-style license)
+
+  # Input/Output/Filter dependencies
+  #TODO Can these be optional?
+  gem.add_runtime_dependency "awesome_print"                    #(MIT license)
+  gem.add_runtime_dependency "aws-sdk"                          #(Apache 2.0 license)
+  gem.add_runtime_dependency "addressable"                      #(Apache 2.0 license)
+  gem.add_runtime_dependency "extlib", ["0.9.16"]               #(MIT license)
+  gem.add_runtime_dependency "ffi", ["~> 1.9.5"]                #(LGPL-3 license)
+  gem.add_runtime_dependency "ffi-rzmq", ["1.0.0"]              #(MIT license)
+  gem.add_runtime_dependency "filewatch", ["0.5.1"]             #(BSD license)
+  gem.add_runtime_dependency "gelfd", ["0.2.0"]                 #(Apache 2.0 license)
+  gem.add_runtime_dependency "gelf", ["1.3.2"]                  #(MIT license)
+  gem.add_runtime_dependency "gmetric", ["0.1.3"]               #(MIT license)
+  gem.add_runtime_dependency "jls-grok", ["0.11.0"]             #(BSD license)
+  gem.add_runtime_dependency "mail"                             #(MIT license)
+  gem.add_runtime_dependency "metriks"                          #(MIT license)
+  gem.add_runtime_dependency "redis"                            #(MIT license)
+  gem.add_runtime_dependency "statsd-ruby", ["1.2.0"]           #(MIT license)
+  gem.add_runtime_dependency "xml-simple"                       #(Ruby license?)
+  gem.add_runtime_dependency "xmpp4r", ["0.5"]                  #(Ruby license)
+  gem.add_runtime_dependency "jls-lumberjack", [">=0.0.20"]     #(Apache 2.0 license)
+  gem.add_runtime_dependency "geoip", [">= 1.3.2"]              #(GPL license)
+  gem.add_runtime_dependency "beefcake", "0.3.7"                #(MIT license)
+  gem.add_runtime_dependency "murmurhash3"                      #(MIT license)
+  gem.add_runtime_dependency "rufus-scheduler", "~> 2.0.24"     #(MIT license)
+  gem.add_runtime_dependency "user_agent_parser", [">= 2.0.0"]  #(MIT license)
+  gem.add_runtime_dependency "snmp"                             #(Ruby license)
+  gem.add_runtime_dependency "rbnacl"                           #(MIT license)
+  gem.add_runtime_dependency "bindata", [">= 1.5.0"]            #(Ruby license)
+  gem.add_runtime_dependency "twitter", "5.0.0.rc.1"            #(MIT license)
+  gem.add_runtime_dependency "edn"                              #(MIT license)
+  gem.add_runtime_dependency "elasticsearch"                    #(Apache 2.0 license)
+
+  # Plugin manager dependencies
+
+  # jar-dependencies 0.1.2 is included in jruby 1.7.6 no need to include here and
+  # this avoids the gemspec jar path parsing issue of jar-dependencies 0.1.2
+  #
+  # gem.add_runtime_dependency "jar-dependencies", [">= 0.1.2"]   #(MIT license)
+
+  gem.add_runtime_dependency "ruby-maven"                       #(EPL license)
+
+  if RUBY_PLATFORM == 'java'
+    gem.platform = RUBY_PLATFORM
+
+    # bouncy-castle-java 1.5.0147 and jruby-openssl 0.9.5 are included in jruby 1.7.6 no need to include here
+    # and this avoids the gemspec jar path parsing issue of jar-dependencies 0.1.2
+    #
+    # gem.add_runtime_dependency "bouncy-castle-java", ["~> 1.5.0147"] #(MIT license)
+    # gem.add_runtime_dependency "jruby-openssl", ["~> 0.9.5"]         #(CPL/GPL/LGPL license)
+
+    gem.add_runtime_dependency "jruby-httpclient"                    #(Apache 2.0 license)
+    gem.add_runtime_dependency "msgpack-jruby"                       #(Apache 2.0 license)
+    gem.add_runtime_dependency "jrjackson"                           #(Apache 2.0 license)
+    gem.add_runtime_dependency "jruby-kafka", [">=0.1.0"]            #(Apache 2.0 license)
+  else
+    gem.add_runtime_dependency "excon"    #(MIT license)
+    gem.add_runtime_dependency "msgpack"  #(Apache 2.0 license)
+    gem.add_runtime_dependency "oj"       #(MIT-style license)
   end
 
-  #rev = %x{svn info}.split("\n").grep(/Revision:/).first.split(" ").last.to_i
-  rev = Time.now.strftime("%Y%m%d%H%M%S")
-  spec.name = "logstash"
-  spec.version = "1.0.2"
-  spec.summary = "logstash - log and event management"
-  spec.description = "scalable log and event management (search, archive, pipeline)"
-  spec.license = "Apache License (2.0)"
-
-  #spec.add_dependency("eventmachine-tail") # TODO(sissel): remove, not for jruby
-  spec.add_dependency("json")
-
-  # New for our JRuby stuff
-  spec.add_dependency("file-tail")
-  spec.add_dependency("jruby-elasticsearch", ">= 0.0.2")
-  spec.add_dependency "bunny" # for amqp support
-  spec.add_dependency "uuidtools" # for naming amqp queues
-  spec.add_dependency "filewatch", "~> 0.2.3"  # for file tailing
-  spec.add_dependency "jls-grok", "~> 0.4.7" # for grok filter
-  spec.add_dependency "jruby-elasticsearch", "~> 0.0.7"
-  spec.add_dependency "stomp" # for stomp protocol
-  spec.add_dependency "json"
-  spec.add_dependency "awesome_print"
-
-  spec.add_dependency "rack"
-  spec.add_dependency "mizuno"
-  spec.add_dependency "sinatra"
-  spec.add_dependency "haml"
-
-  spec.add_dependency "mongo" # outputs/mongodb
-  spec.add_dependency "gelf" # outputs/gelf
-
-  # For the 'grok' filter
-  spec.add_dependency("jls-grok", "~> 0.4.7")
-
-  spec.add_dependency("bunny")
-  spec.add_dependency("uuidtools")
-
-  # For beanstalk://
-  #spec.add_dependency("em-jack")
-
-  spec.files = files
-  spec.require_paths << "lib"
-  spec.bindir = "bin"
-  spec.executables << "logstash"
-  spec.executables << "logstash-web"
-  spec.executables << "logstash-test"
-
-  spec.authors = ["Jordan Sissel", "Pete Fritchman"]
-  spec.email = ["jls@semicomplete.com", "petef@databits.net"]
-  spec.homepage = "http://logstash.net/"
-end
+  if RUBY_PLATFORM != 'java'
+    gem.add_runtime_dependency "bunny",      ["~> 1.5.0"] #(MIT license)
+  else
+    gem.add_runtime_dependency "march_hare", ["~> 2.5.1"] #(MIT license)
+  end
+
+  if RUBY_VERSION >= '1.9.1'
+    gem.add_runtime_dependency "cinch" # cinch requires 1.9.1+ #(MIT license)
+  end
+
+  if RUBY_ENGINE == "rbx"
+    # rubinius puts the ruby stdlib into gems.
+    gem.add_runtime_dependency "rubysl"
 
+    # Include racc to make the xml tests pass.
+    # https://github.com/rubinius/rubinius/issues/2632#issuecomment-26954565
+    gem.add_runtime_dependency "racc"
+  end
+
+  # These are runtime-deps so you can do 'java -jar logstash.jar rspec <test>'
+  gem.add_runtime_dependency "spoon"              #(Apache 2.0 license)
+  gem.add_runtime_dependency "mocha"              #(MIT license)
+  gem.add_runtime_dependency "shoulda"            #(MIT license)
+  gem.add_runtime_dependency "rspec", "~> 2.14.0" #(MIT license)
+  gem.add_runtime_dependency "insist", "1.0.0"    #(Apache 2.0 license)
+  gem.add_runtime_dependency "rumbster"           #(Apache 2.0 license) For faking smtp in email tests
+
+  # Development Deps
+  gem.add_development_dependency "coveralls"
+  gem.add_development_dependency "kramdown"       #(MIT license) pure-ruby markdown parser
+
+  # Jenkins Deps
+  gem.add_runtime_dependency "ci_reporter", "1.9.3"
+end
diff --git a/misc/rate.sh b/misc/rate.sh
deleted file mode 100755
index 9ff14b95345..00000000000
--- a/misc/rate.sh
+++ /dev/null
@@ -1,25 +0,0 @@
-#!/bin/zsh
-
-if [ "$#" -ne 1 ] ; then
-  echo "Usage; $0 logfile"
-  exit 1
-fi
-logfile="$1"
-
-pid=$(ps -u $USER -f | awk '/bin.logstash -[f]/ {print $2}')
-fileno=$(lsof -nPp $pid | grep -F "$logfile" |  awk '{ print int($4) }')
-pos=$(awk '/pos:/ {print $2}' /proc/$pid/fdinfo/$fileno)
-size=$(ls -ld "$logfile" | awk '{print $5}')
-starttime=$(awk '{print $22}' /proc/$pid/stat)
-curtime=$(awk '{print $1}' /proc/uptime)
-lines=$(dd if="$logfile" bs=$pos count=1 2> /dev/null | wc -l)
-percent=$(printf "%.2f%%" $(( ($pos / ($size + 0.0)) * 100 )))
-
-duration=$(($curtime - ($starttime / 100.)))
-rate=$(( $lines / (0.0 + $duration) ))
-
-ps --no-header -o "pid user args" -p $pid
-echo "Duration: $duration"
-echo "Lines: $lines (position: $pos, $percent)"
-echo "Rate: $rate" 
-
diff --git a/patterns/firewalls b/patterns/firewalls
index 9616f67407c..ff7baeae38e 100644
--- a/patterns/firewalls
+++ b/patterns/firewalls
@@ -1,2 +1,60 @@
 # NetScreen firewall logs
-NETSCREENSESSIONLOG %{SYSLOGDATE:date} %{IPORHOST:device} %{IPORHOST}: NetScreen device_id=%{WORD:device_id}%{DATA}: start_time=%{QUOTEDSTRING:start_time} duration=%{INT:duration} policy_id=%{INT:policy_id} service=%{DATA:service} proto=%{INT:proto} src zone=%{WORD:src_zone} dst zone=%{WORD:dst_zone} action=%{WORD:action} sent=%{INT:sent} rcvd=%{INT:rcvd} src=%{IPORHOST:src_ip} dst=%{IPORHOST:dst_ip} src_port=%{INT:src_port} dst_port=%{INT:dst_port} src-xlated ip=%{IPORHOST:src_xlated_ip} port=%{INT:src_xlated_port} dst-xlated ip=%{IPORHOST:dst_xlated_ip} port=%{INT:dst_xlated_port} session_id=%{INT:session_id} reason=%{GREEDYDATA:reason}
+NETSCREENSESSIONLOG %{SYSLOGTIMESTAMP:date} %{IPORHOST:device} %{IPORHOST}: NetScreen device_id=%{WORD:device_id}%{DATA}: start_time=%{QUOTEDSTRING:start_time} duration=%{INT:duration} policy_id=%{INT:policy_id} service=%{DATA:service} proto=%{INT:proto} src zone=%{WORD:src_zone} dst zone=%{WORD:dst_zone} action=%{WORD:action} sent=%{INT:sent} rcvd=%{INT:rcvd} src=%{IPORHOST:src_ip} dst=%{IPORHOST:dst_ip} src_port=%{INT:src_port} dst_port=%{INT:dst_port} src-xlated ip=%{IPORHOST:src_xlated_ip} port=%{INT:src_xlated_port} dst-xlated ip=%{IPORHOST:dst_xlated_ip} port=%{INT:dst_xlated_port} session_id=%{INT:session_id} reason=%{GREEDYDATA:reason}
+
+#== Cisco ASA ==
+CISCO_TAGGED_SYSLOG ^<%{POSINT:syslog_pri}>%{CISCOTIMESTAMP:timestamp}( %{SYSLOGHOST:sysloghost})?: %%{CISCOTAG:ciscotag}:
+CISCOTIMESTAMP %{MONTH} +%{MONTHDAY}(?: %{YEAR})? %{TIME}
+CISCOTAG [A-Z0-9]+-%{INT}-(?:[A-Z0-9_]+)
+# Common Particles
+CISCO_ACTION Built|Teardown|Deny|Denied|denied|requested|permitted|denied by ACL|discarded|est-allowed|Dropping|created|deleted
+CISCO_REASON Duplicate TCP SYN|Failed to locate egress interface|Invalid transport field|No matching connection|DNS Response|DNS Query|(?:%{WORD}\s*)*
+CISCO_DIRECTION Inbound|inbound|Outbound|outbound
+CISCO_INTERVAL first hit|%{INT}-second interval
+CISCO_XLATE_TYPE static|dynamic
+# ASA-2-106001
+CISCOFW106001 %{CISCO_DIRECTION:direction} %{WORD:protocol} connection %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{GREEDYDATA:tcp_flags} on interface %{GREEDYDATA:interface}
+# ASA-2-106006, ASA-2-106007, ASA-2-106010
+CISCOFW106006_106007_106010 %{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} (?:from|src) %{IP:src_ip}/%{INT:src_port}(\(%{DATA:src_fwuser}\))? (?:to|dst) %{IP:dst_ip}/%{INT:dst_port}(\(%{DATA:dst_fwuser}\))? (?:on interface %{DATA:interface}|due to %{CISCO_REASON:reason})
+# ASA-3-106014
+CISCOFW106014 %{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} src %{DATA:src_interface}:%{IP:src_ip}(\(%{DATA:src_fwuser}\))? dst %{DATA:dst_interface}:%{IP:dst_ip}(\(%{DATA:dst_fwuser}\))? \(type %{INT:icmp_type}, code %{INT:icmp_code}\)
+# ASA-6-106015
+CISCOFW106015 %{CISCO_ACTION:action} %{WORD:protocol} \(%{DATA:policy_id}\) from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{DATA:tcp_flags}  on interface %{GREEDYDATA:interface}
+# ASA-1-106021
+CISCOFW106021 %{CISCO_ACTION:action} %{WORD:protocol} reverse path check from %{IP:src_ip} to %{IP:dst_ip} on interface %{GREEDYDATA:interface}
+# ASA-4-106023
+CISCOFW106023 %{CISCO_ACTION:action} %{WORD:protocol} src %{DATA:src_interface}:%{IP:src_ip}(/%{INT:src_port})?(\(%{DATA:src_fwuser}\))? dst %{DATA:dst_interface}:%{IP:dst_ip}(/%{INT:dst_port})?(\(%{DATA:dst_fwuser}\))?( \(type %{INT:icmp_type}, code %{INT:icmp_code}\))? by access-group %{DATA:policy_id} \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
+# ASA-5-106100
+CISCOFW106100 access-list %{WORD:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} %{DATA:src_interface}/%{IP:src_ip}\(%{INT:src_port}\)(\(%{DATA:src_fwuser}\))? -> %{DATA:dst_interface}/%{IP:dst_ip}\(%{INT:dst_port}\)(\(%{DATA:src_fwuser}\))? hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
+# ASA-6-110002
+CISCOFW110002 %{CISCO_REASON:reason} for %{WORD:protocol} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}
+# ASA-6-302010
+CISCOFW302010 %{INT:connection_count} in use, %{INT:connection_count_max} most used
+# ASA-6-302013, ASA-6-302014, ASA-6-302015, ASA-6-302016
+CISCOFW302013_302014_302015_302016 %{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection %{INT:connection_id} for %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port}( \(%{IP:src_mapped_ip}/%{INT:src_mapped_port}\))?(\(%{DATA:src_fwuser}\))? to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}( \(%{IP:dst_mapped_ip}/%{INT:dst_mapped_port}\))?(\(%{DATA:dst_fwuser}\))?( duration %{TIME:duration} bytes %{INT:bytes})?(?: %{CISCO_REASON:reason})?( \(%{DATA:user}\))?
+# ASA-6-302020, ASA-6-302021
+CISCOFW302020_302021 %{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection for faddr %{IP:dst_ip}/%{INT:icmp_seq_num}(?:\(%{DATA:fwuser}\))? gaddr %{IP:src_xlated_ip}/%{INT:icmp_code_xlated} laddr %{IP:src_ip}/%{INT:icmp_code}( \(%{DATA:user}\))?
+# ASA-6-305011
+CISCOFW305011 %{CISCO_ACTION:action} %{CISCO_XLATE_TYPE:xlate_type} %{WORD:protocol} translation from %{DATA:src_interface}:%{IP:src_ip}(/%{INT:src_port})?(\(%{DATA:src_fwuser}\))? to %{DATA:src_xlated_interface}:%{IP:src_xlated_ip}/%{DATA:src_xlated_port}
+# ASA-3-313001, ASA-3-313004, ASA-3-313008
+CISCOFW313001_313004_313008 %{CISCO_ACTION:action} %{WORD:protocol} type=%{INT:icmp_type}, code=%{INT:icmp_code} from %{IP:src_ip} on interface %{DATA:interface}( to %{IP:dst_ip})?
+# ASA-4-313005
+CISCOFW313005 %{CISCO_REASON:reason} for %{WORD:protocol} error message: %{WORD:err_protocol} src %{DATA:err_src_interface}:%{IP:err_src_ip}(\(%{DATA:err_src_fwuser}\))? dst %{DATA:err_dst_interface}:%{IP:err_dst_ip}(\(%{DATA:err_dst_fwuser}\))? \(type %{INT:err_icmp_type}, code %{INT:err_icmp_code}\) on %{DATA:interface} interface\.  Original IP payload: %{WORD:protocol} src %{IP:orig_src_ip}/%{INT:orig_src_port}(\(%{DATA:orig_src_fwuser}\))? dst %{IP:orig_dst_ip}/%{INT:orig_dst_port}(\(%{DATA:orig_dst_fwuser}\))?
+# ASA-4-402117
+CISCOFW402117 %{WORD:protocol}: Received a non-IPSec packet \(protocol= %{WORD:orig_protocol}\) from %{IP:src_ip} to %{IP:dst_ip}
+# ASA-4-402119
+CISCOFW402119 %{WORD:protocol}: Received an %{WORD:orig_protocol} packet \(SPI= %{DATA:spi}, sequence number= %{DATA:seq_num}\) from %{IP:src_ip} \(user= %{DATA:user}\) to %{IP:dst_ip} that failed anti-replay checking
+# ASA-4-419001
+CISCOFW419001 %{CISCO_ACTION:action} %{WORD:protocol} packet from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}, reason: %{GREEDYDATA:reason}
+# ASA-4-419002
+CISCOFW419002 %{CISCO_REASON:reason} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port} with different initial sequence number
+# ASA-4-500004
+CISCOFW500004 %{CISCO_REASON:reason} for protocol=%{WORD:protocol}, from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}
+# ASA-6-602303, ASA-6-602304
+CISCOFW602303_602304 %{WORD:protocol}: An %{CISCO_DIRECTION:direction} %{GREEDYDATA:tunnel_type} SA \(SPI= %{DATA:spi}\) between %{IP:src_ip} and %{IP:dst_ip} \(user= %{DATA:user}\) has been %{CISCO_ACTION:action}
+# ASA-7-710001, ASA-7-710002, ASA-7-710003, ASA-7-710005, ASA-7-710006
+CISCOFW710001_710002_710003_710005_710006 %{WORD:protocol} (?:request|access) %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}
+# ASA-6-713172
+CISCOFW713172 Group = %{GREEDYDATA:group}, IP = %{IP:src_ip}, Automatic NAT Detection Status:\s+Remote end\s*%{DATA:is_remote_natted}\s*behind a NAT device\s+This\s+end\s*%{DATA:is_local_natted}\s*behind a NAT device
+# ASA-4-733100
+CISCOFW733100 \[\s*%{DATA:drop_type}\s*\] drop %{DATA:drop_rate_id} exceeded. Current burst rate is %{INT:drop_rate_current_burst} per second, max configured rate is %{INT:drop_rate_max_burst}; Current average rate is %{INT:drop_rate_current_avg} per second, max configured rate is %{INT:drop_rate_max_avg}; Cumulative total count is %{INT:drop_total_count}
+#== End Cisco ASA ==
diff --git a/patterns/grok-patterns b/patterns/grok-patterns
old mode 100644
new mode 100755
index f106eb06459..c6137aae5db
--- a/patterns/grok-patterns
+++ b/patterns/grok-patterns
@@ -1,4 +1,4 @@
-USERNAME [a-zA-Z0-9_-]+
+USERNAME [a-zA-Z0-9._-]+
 USER %{USERNAME}
 INT (?:[+-]?(?:[0-9]+))
 BASE10NUM (?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\.[0-9]+)?)|(?:\.[0-9]+)))
@@ -6,93 +6,89 @@ NUMBER (?:%{BASE10NUM})
 BASE16NUM (?<![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+))
 BASE16FLOAT \b(?<![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\.[0-9A-Fa-f]*)?)|(?:\.[0-9A-Fa-f]+)))\b
 
-POSINT \b(?:[0-9]+)\b
-TWODIGITINT [0-9]{2}
+POSINT \b(?:[1-9][0-9]*)\b
+NONNEGINT \b(?:[0-9]+)\b
 WORD \b\w+\b
 NOTSPACE \S+
+SPACE \s*
 DATA .*?
 GREEDYDATA .*
-QUOTEDSTRING (?:(?<!\\)(?:"(?:\\.|[^\\"])*"|(?:'(?:\\.|[^\\'])*')|(?:`(?:\\.|[^\\`])*`)))
+QUOTEDSTRING (?>(?<!\\)(?>"(?>\\.|[^\\"]+)+"|""|(?>'(?>\\.|[^\\']+)+')|''|(?>`(?>\\.|[^\\`]+)+`)|``))
+UUID [A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12}
 
 # Networking
 MAC (?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC})
 CISCOMAC (?:(?:[A-Fa-f0-9]{4}\.){2}[A-Fa-f0-9]{4})
 WINDOWSMAC (?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2})
 COMMONMAC (?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2})
-IP (?<![0-9])(?:(?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2}))(?![0-9])
+IPV6 ((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:)))(%.+)?
+IPV4 (?<![0-9])(?:(?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2}))(?![0-9])
+IP (?:%{IPV6}|%{IPV4})
 HOSTNAME \b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\.?|\b)
 HOST %{HOSTNAME}
 IPORHOST (?:%{HOSTNAME}|%{IP})
-HOSTPORT (?:%{IPORHOST=~/\./}:%{POSINT})
+HOSTPORT %{IPORHOST}:%{POSINT}
 
 # paths
 PATH (?:%{UNIXPATH}|%{WINPATH})
-UNIXPATH (?<![\w\\/])(?:/(?:[\w_%!$@:.,-]+|\\.)*)+
-#UNIXPATH (?<![\w\/])(?:/[^\/\s?*]*)+
-LINUXTTY (?:/dev/pts/%{POSINT})
-BSDTTY (?:/dev/tty[pq][a-z0-9])
-TTY (?:%{BSDTTY}|%{LINUXTTY})
-WINPATH (?:[A-Za-z]+:|\\)(?:\\[^\\?*]*)+
+UNIXPATH (?>/(?>[\w_%!$@:.,~-]+|\\.)*)+
+TTY (?:/dev/(pts|tty([pq])?)(\w+)?/?(?:[0-9]+))
+WINPATH (?>[A-Za-z]+:|\\)(?:\\[^\\?*]*)+
 URIPROTO [A-Za-z]+(\+[A-Za-z+]+)?
-URIHOST %{IPORHOST}(?::%{POSINT})?
+URIHOST %{IPORHOST}(?::%{POSINT:port})?
 # uripath comes loosely from RFC1738, but mostly from what Firefox
 # doesn't turn into %XX
-URIPATH (?:/[A-Za-z0-9$.+!*'(),~:#%_-]*)+
+URIPATH (?:/[A-Za-z0-9$.+!*'(){},~:;=@#%_\-]*)+
 #URIPARAM \?(?:[A-Za-z0-9]+(?:=(?:[^&]*))?(?:&(?:[A-Za-z0-9]+(?:=(?:[^&]*))?)?)*)?
-URIPARAM \?[A-Za-z0-9$.+!*'(),~#%&/=:;_-]*
+URIPARAM \?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\-\[\]]*
 URIPATHPARAM %{URIPATH}(?:%{URIPARAM})?
-URI %{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:$|%{URIPATHPARAM})
+URI %{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})?
 
 # Months: January, Feb, 3, 03, 12, December
 MONTH \b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\b
 MONTHNUM (?:0?[1-9]|1[0-2])
-MONTHDAY (?:3[01]|[1-2]?[0-9]|0?[1-9])
+MONTHNUM2 (?:0[1-9]|1[0-2])
+MONTHDAY (?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])
 
 # Days: Monday, Tue, Thu, etc...
 DAY (?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)
 
 # Years?
-YEAR [0-9]+
-# Time: HH:MM:SS
-#TIME \d{2}:\d{2}(?::\d{2}(?:\.\d+)?)?
-# I'm still on the fence about using grok to perform the time match,
-# since it's probably slower.
-# TIME %{POSINT<24}:%{POSINT<60}(?::%{POSINT<60}(?:\.%{POSINT})?)?
-HOUR (?:2[0123]|[01][0-9])
+YEAR (?>\d\d){1,2}
+HOUR (?:2[0123]|[01]?[0-9])
 MINUTE (?:[0-5][0-9])
 # '60' is a leap second in most time standards and thus is valid.
-SECOND (?:(?:[0-5][0-9]|60)(?:[.,][0-9]+)?)
+SECOND (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)
 TIME (?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])
 # datestamp is YYYY/MM/DD-HH:MM:SS.UUUU (or something like it)
 DATE_US %{MONTHNUM}[/-]%{MONTHDAY}[/-]%{YEAR}
-DATE_EU %{YEAR}[/-]%{MONTHNUM}[/-]%{MONTHDAY}
+DATE_EU %{MONTHDAY}[./-]%{MONTHNUM}[./-]%{YEAR}
 ISO8601_TIMEZONE (?:Z|[+-]%{HOUR}(?::?%{MINUTE}))
 ISO8601_SECOND (?:%{SECOND}|60)
 TIMESTAMP_ISO8601 %{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?
 DATE %{DATE_US}|%{DATE_EU}
 DATESTAMP %{DATE}[- ]%{TIME}
-TZ (?:[PMCE][SD]T)
+TZ (?:[PMCE][SD]T|UTC)
 DATESTAMP_RFC822 %{DAY} %{MONTH} %{MONTHDAY} %{YEAR} %{TIME} %{TZ}
-DATESTAMP_OTHER %{DAY} %{MONTH} %{MONTHDAY} %{TIME} (?:%{TZ} )?%{YEAR}
+DATESTAMP_RFC2822 %{DAY}, %{MONTHDAY} %{MONTH} %{YEAR} %{TIME} %{ISO8601_TIMEZONE}
+DATESTAMP_OTHER %{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{TZ} %{YEAR}
+DATESTAMP_EVENTLOG %{YEAR}%{MONTHNUM2}%{MONTHDAY}%{HOUR}%{MINUTE}%{SECOND}
 
 # Syslog Dates: Month Day HH:MM:SS
 SYSLOGTIMESTAMP %{MONTH} +%{MONTHDAY} %{TIME}
-PROG (?:[\w._/-]+)
+PROG (?:[\w._/%-]+)
 SYSLOGPROG %{PROG:program}(?:\[%{POSINT:pid}\])?
 SYSLOGHOST %{IPORHOST}
-SYSLOGFACILITY <%{POSINT:facility}.%{POSINT:priority}>
-HTTPDATE %{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT:ZONE}
+SYSLOGFACILITY <%{NONNEGINT:facility}.%{NONNEGINT:priority}>
+HTTPDATE %{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}
 
 # Shortcuts
 QS %{QUOTEDSTRING}
 
 # Log formats
 SYSLOGBASE %{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:
-COMBINEDAPACHELOG %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "%{WORD:verb} %{URIPATHPARAM:request} HTTP/%{NUMBER:httpversion}" %{NUMBER:response} (?:%{NUMBER:bytes}|-) "(?:%{URI:referrer}|-)" %{QS:agent}
-
-#
-# Custom formats
-# Add additional custom patterns below
-DATESTAMP_RAILS %{DAY} %{MONTH} %{MONTHDAY} %{TIME} (?:%{INT:ZONE} )?%{YEAR}
-DATESTAMP_MYSQL %{TWODIGITINT:year}%{TWODIGITINT:month}%{TWODIGITINT:day}\s+%{TIME}
+COMMONAPACHELOG %{IPORHOST:clientip} %{USER:ident} %{NOTSPACE:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})" %{NUMBER:response} (?:%{NUMBER:bytes}|-)
+COMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent}
 
+# Log Levels
+LOGLEVEL ([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)
diff --git a/patterns/haproxy b/patterns/haproxy
index 6a670852323..e10fd9706f0 100644
--- a/patterns/haproxy
+++ b/patterns/haproxy
@@ -1,5 +1,37 @@
-HAPROXYDATE %{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME}.%{INT:milliseconds}
-HAPROXYTERMINATIONSTATE [CAPRIcs-][RQCHDLT-][NIDV-][NIPRD-]
+## These patterns were tested w/ haproxy-1.4.15
 
-# parse an haproxy 'httplog' line
-HAPROXYHTTP %{SYSLOGDATE:date} %{IPORHOST:server} %{SYSLOGPROG}: %{IP:clientip}:%{INT:clientport} \[%{HAPROXYDATE:haproxydate}\] %{NOTSPACE:proxyname} %{NOTSPACE}/%{IPORHOST:backend} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{INT:time_duration} %{INT:response} %{INT:bytes} - - %{HAPROXYTERMINATIONSTATE:terminationstate} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn} %{INT:srv_queue}/%{INT:backend_queue} "%{WORD:verb} %{URIPATHPARAM:request} HTTP/%{NUMBER:version}"
+## Documentation of the haproxy log formats can be found at the following links:
+## http://code.google.com/p/haproxy-docs/wiki/HTTPLogFormat
+## http://code.google.com/p/haproxy-docs/wiki/TCPLogFormat
+
+HAPROXYTIME (?!<[0-9])%{HOUR:haproxy_hour}:%{MINUTE:haproxy_minute}(?::%{SECOND:haproxy_second})(?![0-9])
+HAPROXYDATE %{MONTHDAY:haproxy_monthday}/%{MONTH:haproxy_month}/%{YEAR:haproxy_year}:%{HAPROXYTIME:haproxy_time}.%{INT:haproxy_milliseconds}
+
+# Override these default patterns to parse out what is captured in your haproxy.cfg
+HAPROXYCAPTUREDREQUESTHEADERS %{DATA:captured_request_headers}
+HAPROXYCAPTUREDRESPONSEHEADERS %{DATA:captured_response_headers}
+
+# Example:
+#  These haproxy config lines will add data to the logs that are captured
+#  by the patterns below. Place them in your custom patterns directory to 
+#  override the defaults.  
+#
+#  capture request header Host len 40
+#  capture request header X-Forwarded-For len 50
+#  capture request header Accept-Language len 50
+#  capture request header Referer len 200
+#  capture request header User-Agent len 200
+#
+#  capture response header Content-Type len 30
+#  capture response header Content-Encoding len 10
+#  capture response header Cache-Control len 200
+#  capture response header Last-Modified len 200
+# 
+# HAPROXYCAPTUREDREQUESTHEADERS %{DATA:request_header_host}\|%{DATA:request_header_x_forwarded_for}\|%{DATA:request_header_accept_language}\|%{DATA:request_header_referer}\|%{DATA:request_header_user_agent}
+# HAPROXYCAPTUREDRESPONSEHEADERS %{DATA:response_header_content_type}\|%{DATA:response_header_content_encoding}\|%{DATA:response_header_cache_control}\|%{DATA:response_header_last_modified}
+
+# parse a haproxy 'httplog' line 
+HAPROXYHTTP %{SYSLOGTIMESTAMP:syslog_timestamp} %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\{%{HAPROXYCAPTUREDREQUESTHEADERS}\})?( )?(\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\})?( )?"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?"
+
+# parse a haproxy 'tcplog' line
+HAPROXYTCP %{SYSLOGTIMESTAMP:syslog_timestamp} %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_queue}/%{INT:time_backend_connect}/%{NOTSPACE:time_duration} %{NOTSPACE:bytes_read} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue}
diff --git a/patterns/java b/patterns/java
index eb83010bc2a..1d5a0e20e54 100644
--- a/patterns/java
+++ b/patterns/java
@@ -1,3 +1,7 @@
-JAVACLASS (?:[a-zA-Z0-9-]+\.)+[A-Za-z0-9]+
-JAVAFILE (?:[A-Za-z0-9_.-]+)
-JAVASTACKTRACEPART at %{JAVACLASS:class}\.%{WORD:method}\(%{JAVAFILE:file}:%{NUMBER:line}\)
+JAVACLASS (?:[a-zA-Z$_][a-zA-Z$_0-9]*\.)*[a-zA-Z$_][a-zA-Z$_0-9]*
+#Space is an allowed character to match special cases like 'Native Method' or 'Unknown Source'
+JAVAFILE (?:[A-Za-z0-9_. -]+)
+#Allow special <init> method
+JAVAMETHOD (?:(<init>)|[a-zA-Z$_][a-zA-Z$_0-9]*)
+#Line number is optional in special cases 'Native method' or 'Unknown source'
+JAVASTACKTRACEPART %{SPACE}at %{JAVACLASS:class}\.%{JAVAMETHOD:method}\(%{JAVAFILE:file}(?::%{NUMBER:line})?\)
diff --git a/patterns/junos b/patterns/junos
new file mode 100644
index 00000000000..bd796961d18
--- /dev/null
+++ b/patterns/junos
@@ -0,0 +1,9 @@
+# JUNOS 11.4 RT_FLOW patterns
+RT_FLOW_EVENT (RT_FLOW_SESSION_CREATE|RT_FLOW_SESSION_CLOSE|RT_FLOW_SESSION_DENY)
+
+RT_FLOW1 %{RT_FLOW_EVENT:event}: %{GREEDYDATA:close-reason}: %{IP:src-ip}/%{DATA:src-port}->%{IP:dst-ip}/%{DATA:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{DATA:nat-src-port}->%{IP:nat-dst-ip}/%{DATA:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} \d+\(%{DATA:sent}\) \d+\(%{DATA:received}\) %{INT:elapsed-time} .*
+
+RT_FLOW2 %{RT_FLOW_EVENT:event}: session created %{IP:src-ip}/%{DATA:src-port}->%{IP:dst-ip}/%{DATA:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{DATA:nat-src-port}->%{IP:nat-dst-ip}/%{DATA:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} .*
+
+RT_FLOW3 %{RT_FLOW_EVENT:event}: session denied %{IP:src-ip}/%{DATA:src-port}->%{IP:dst-ip}/%{DATA:dst-port} %{DATA:service} %{INT:protocol-id}\(\d\) %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} .*
+
diff --git a/patterns/linux-syslog b/patterns/linux-syslog
index a4ae0bbaaa5..81c1f86e192 100644
--- a/patterns/linux-syslog
+++ b/patterns/linux-syslog
@@ -1,3 +1,5 @@
+SYSLOG5424PRINTASCII [!-~]+
+
 SYSLOGBASE2 (?:%{SYSLOGTIMESTAMP:timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:
 SYSLOGPAMSESSION %{SYSLOGBASE} (?=%{GREEDYDATA:message})%{WORD:pam_module}\(%{DATA:pam_caller}\): session %{WORD:pam_session_state} for user %{USERNAME:username}(?: by %{GREEDYDATA:pam_by})?
 
@@ -5,3 +7,10 @@ CRON_ACTION [A-Z ]+
 CRONLOG %{SYSLOGBASE} \(%{USER:user}\) %{CRON_ACTION:action} \(%{DATA:message}\)
 
 SYSLOGLINE %{SYSLOGBASE2} %{GREEDYDATA:message}
+
+# IETF 5424 syslog(8) format (see http://www.rfc-editor.org/info/rfc5424)
+SYSLOG5424PRI <%{NONNEGINT:syslog5424_pri}>
+SYSLOG5424SD \[%{DATA}\]+
+SYSLOG5424BASE %{SYSLOG5424PRI}%{NONNEGINT:syslog5424_ver} +(?:%{TIMESTAMP_ISO8601:syslog5424_ts}|-) +(?:%{HOSTNAME:syslog5424_host}|-) +(-|%{SYSLOG5424PRINTASCII:syslog5424_app}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_proc}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_msgid}) +(?:%{SYSLOG5424SD:syslog5424_sd}|-|)
+
+SYSLOG5424LINE %{SYSLOG5424BASE} +%{GREEDYDATA:syslog5424_msg}
diff --git a/patterns/mcollective b/patterns/mcollective
new file mode 100644
index 00000000000..648b172eeda
--- /dev/null
+++ b/patterns/mcollective
@@ -0,0 +1 @@
+MCOLLECTIVEAUDIT %{TIMESTAMP_ISO8601:timestamp}:
diff --git a/patterns/mcollective-patterns b/patterns/mcollective-patterns
new file mode 100644
index 00000000000..bb2f7f9bc82
--- /dev/null
+++ b/patterns/mcollective-patterns
@@ -0,0 +1,4 @@
+# Remember, these can be multi-line events.
+MCOLLECTIVE ., \[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\]%{SPACE}%{LOGLEVEL:event_level}
+
+MCOLLECTIVEAUDIT %{TIMESTAMP_ISO8601:timestamp}:
diff --git a/patterns/mongodb b/patterns/mongodb
new file mode 100644
index 00000000000..47a957355c2
--- /dev/null
+++ b/patterns/mongodb
@@ -0,0 +1,4 @@
+MONGO_LOG %{SYSLOGTIMESTAMP:timestamp} \[%{WORD:component}\] %{GREEDYDATA:message}
+MONGO_QUERY \{ (?<={ ).*(?= } ntoreturn:) \}
+MONGO_SLOWQUERY %{WORD} %{MONGO_WORDDASH:database}\.%{MONGO_WORDDASH:collection} %{WORD}: %{MONGO_QUERY:query} %{WORD}:%{NONNEGINT:ntoreturn} %{WORD}:%{NONNEGINT:ntoskip} %{WORD}:%{NONNEGINT:nscanned}.*nreturned:%{NONNEGINT:nreturned}..+ (?<duration>[0-9]+)ms
+MONGO_WORDDASH \b[\w-]+\b
diff --git a/patterns/nagios b/patterns/nagios
index 19fac2a6cc4..9d3fa7b5fa8 100644
--- a/patterns/nagios
+++ b/patterns/nagios
@@ -1,7 +1,108 @@
-NAGIOSTIME \[%{NUMBER:epochtime}\]
+##################################################################################
+##################################################################################
+# Chop Nagios log files to smithereens!
+#
+# A set of GROK filters to process logfiles generated by Nagios.
+# While it does not, this set intends to cover all possible Nagios logs.
+#
+# Some more work needs to be done to cover all External Commands:
+#	http://old.nagios.org/developerinfo/externalcommands/commandlist.php
+#
+# If you need some support on these rules please contact:
+#	Jelle Smet http://smetj.net
+#
+#################################################################################
+#################################################################################
 
-NAGIOS_SERVICE_ALERT SERVICE ALERT: %{IPORHOST:hostname};%{DATA:checkname};%{DATA:state};%{DATA:statelevel};%{NUMBER:attempt};%{GREEDYDATA:message}
-NAGIOS_SERVICE_FLAPPING_ALERT SERVICE FLAPPING ALERT: %{IPORHOST:hostname};%{DATA:checkname};%{DATA:state};%{GREEDYDATA:message}
-NAGIOS_SERVICE_NOTIFICATION SERVICE NOTIFICATION: %{DATA:notifyname};%{IPORHOST:hostname};%{DATA:checkname};%{DATA:state};%{DATA:contact};%{GREEDYDATA:message}
+NAGIOSTIME \[%{NUMBER:nagios_epoch}\]
 
-NAGIOSLOGLINE %{NAGIOSTIME} (?:%{NAGIOS_SERVICE_ALERT}|%{NAGIOS_SERVICE_FLAPPING_ALERT}|%{NAGIOS_SERVICE_NOTIFICATION})
+###############################################
+######## Begin nagios log types
+###############################################
+NAGIOS_TYPE_CURRENT_SERVICE_STATE CURRENT SERVICE STATE
+NAGIOS_TYPE_CURRENT_HOST_STATE CURRENT HOST STATE
+
+NAGIOS_TYPE_SERVICE_NOTIFICATION SERVICE NOTIFICATION
+NAGIOS_TYPE_HOST_NOTIFICATION HOST NOTIFICATION
+
+NAGIOS_TYPE_SERVICE_ALERT SERVICE ALERT
+NAGIOS_TYPE_HOST_ALERT HOST ALERT
+
+NAGIOS_TYPE_SERVICE_FLAPPING_ALERT SERVICE FLAPPING ALERT
+NAGIOS_TYPE_HOST_FLAPPING_ALERT HOST FLAPPING ALERT
+
+NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT SERVICE DOWNTIME ALERT
+NAGIOS_TYPE_HOST_DOWNTIME_ALERT HOST DOWNTIME ALERT
+
+NAGIOS_TYPE_PASSIVE_SERVICE_CHECK PASSIVE SERVICE CHECK
+NAGIOS_TYPE_PASSIVE_HOST_CHECK PASSIVE HOST CHECK
+
+NAGIOS_TYPE_SERVICE_EVENT_HANDLER SERVICE EVENT HANDLER
+NAGIOS_TYPE_HOST_EVENT_HANDLER HOST EVENT HANDLER
+
+NAGIOS_TYPE_EXTERNAL_COMMAND EXTERNAL COMMAND
+NAGIOS_TYPE_TIMEPERIOD_TRANSITION TIMEPERIOD TRANSITION
+###############################################
+######## End nagios log types
+###############################################
+
+###############################################
+######## Begin external check types
+###############################################
+NAGIOS_EC_DISABLE_SVC_CHECK DISABLE_SVC_CHECK
+NAGIOS_EC_ENABLE_SVC_CHECK ENABLE_SVC_CHECK
+NAGIOS_EC_DISABLE_HOST_CHECK DISABLE_HOST_CHECK
+NAGIOS_EC_ENABLE_HOST_CHECK ENABLE_HOST_CHECK
+NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT PROCESS_SERVICE_CHECK_RESULT
+NAGIOS_EC_PROCESS_HOST_CHECK_RESULT PROCESS_HOST_CHECK_RESULT
+NAGIOS_EC_SCHEDULE_SERVICE_DOWNTIME SCHEDULE_SERVICE_DOWNTIME
+NAGIOS_EC_SCHEDULE_HOST_DOWNTIME SCHEDULE_HOST_DOWNTIME
+###############################################
+######## End external check types
+###############################################
+NAGIOS_WARNING Warning:%{SPACE}%{GREEDYDATA:nagios_message}
+
+NAGIOS_CURRENT_SERVICE_STATE %{NAGIOS_TYPE_CURRENT_SERVICE_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}
+NAGIOS_CURRENT_HOST_STATE %{NAGIOS_TYPE_CURRENT_HOST_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}
+
+NAGIOS_SERVICE_NOTIFICATION %{NAGIOS_TYPE_SERVICE_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}
+NAGIOS_HOST_NOTIFICATION %{NAGIOS_TYPE_HOST_NOTIFICATION}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}
+
+NAGIOS_SERVICE_ALERT %{NAGIOS_TYPE_SERVICE_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}
+NAGIOS_HOST_ALERT %{NAGIOS_TYPE_HOST_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}
+
+NAGIOS_SERVICE_FLAPPING_ALERT %{NAGIOS_TYPE_SERVICE_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}
+NAGIOS_HOST_FLAPPING_ALERT %{NAGIOS_TYPE_HOST_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}
+
+NAGIOS_SERVICE_DOWNTIME_ALERT %{NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
+NAGIOS_HOST_DOWNTIME_ALERT %{NAGIOS_TYPE_HOST_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
+
+NAGIOS_PASSIVE_SERVICE_CHECK %{NAGIOS_TYPE_PASSIVE_SERVICE_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
+NAGIOS_PASSIVE_HOST_CHECK %{NAGIOS_TYPE_PASSIVE_HOST_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
+
+NAGIOS_SERVICE_EVENT_HANDLER %{NAGIOS_TYPE_SERVICE_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}
+NAGIOS_HOST_EVENT_HANDLER %{NAGIOS_TYPE_HOST_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}
+
+NAGIOS_TIMEPERIOD_TRANSITION %{NAGIOS_TYPE_TIMEPERIOD_TRANSITION:nagios_type}: %{DATA:nagios_service};%{DATA:nagios_unknown1};%{DATA:nagios_unknown2};
+
+####################
+#### External checks
+####################
+
+#Disable host & service check
+NAGIOS_EC_LINE_DISABLE_SVC_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}
+NAGIOS_EC_LINE_DISABLE_HOST_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}
+
+#Enable host & service check
+NAGIOS_EC_LINE_ENABLE_SVC_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}
+NAGIOS_EC_LINE_ENABLE_HOST_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}
+
+#Process host & service check
+NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}
+NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_HOST_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}
+
+#Schedule host & service downtime
+NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_SCHEDULE_HOST_DOWNTIME:nagios_command};%{DATA:nagios_hostname};%{NUMBER:nagios_start_time};%{NUMBER:nagios_end_time};%{NUMBER:nagios_fixed};%{NUMBER:nagios_trigger_id};%{NUMBER:nagios_duration};%{DATA:author};%{DATA:comment}
+
+#End matching line
+NAGIOSLOGLINE %{NAGIOSTIME} (?:%{NAGIOS_WARNING}|%{NAGIOS_CURRENT_SERVICE_STATE}|%{NAGIOS_CURRENT_HOST_STATE}|%{NAGIOS_SERVICE_NOTIFICATION}|%{NAGIOS_HOST_NOTIFICATION}|%{NAGIOS_SERVICE_ALERT}|%{NAGIOS_HOST_ALERT}|%{NAGIOS_SERVICE_FLAPPING_ALERT}|%{NAGIOS_HOST_FLAPPING_ALERT}|%{NAGIOS_SERVICE_DOWNTIME_ALERT}|%{NAGIOS_HOST_DOWNTIME_ALERT}|%{NAGIOS_PASSIVE_SERVICE_CHECK}|%{NAGIOS_PASSIVE_HOST_CHECK}|%{NAGIOS_SERVICE_EVENT_HANDLER}|%{NAGIOS_HOST_EVENT_HANDLER}|%{NAGIOS_TIMEPERIOD_TRANSITION}|%{NAGIOS_EC_LINE_DISABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_ENABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_DISABLE_HOST_CHECK|%{NAGIOS_EC_LINE_ENABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT}|%{NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT}|%{NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME})
diff --git a/patterns/postgresql b/patterns/postgresql
new file mode 100644
index 00000000000..c5b3e90b725
--- /dev/null
+++ b/patterns/postgresql
@@ -0,0 +1,3 @@
+# Default postgresql pg_log format pattern
+POSTGRESQL %{DATESTAMP:timestamp} %{TZ} %{DATA:user_id} %{GREEDYDATA:connection_id} %{POSINT:pid}
+
diff --git a/patterns/redis b/patterns/redis
new file mode 100644
index 00000000000..8655c4f043e
--- /dev/null
+++ b/patterns/redis
@@ -0,0 +1,3 @@
+REDISTIMESTAMP %{MONTHDAY} %{MONTH} %{TIME}
+REDISLOG \[%{POSINT:pid}\] %{REDISTIMESTAMP:timestamp} \* 
+
diff --git a/patterns/ruby b/patterns/ruby
index f8cbb990abb..b1729cddcb0 100644
--- a/patterns/ruby
+++ b/patterns/ruby
@@ -1,2 +1,2 @@
 RUBY_LOGLEVEL (?:DEBUG|FATAL|ERROR|WARN|INFO)
-RUBY_LOGGER [DFEWI], \[%{TIMESTAMP_ISO8601} #{POSINT:pid}\] *%{RUBY_LOGLEVEL} -- %{DATA:progname}: %{DATA:message}
+RUBY_LOGGER [DFEWI], \[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\] *%{RUBY_LOGLEVEL:loglevel} -- +%{DATA:progname}: %{GREEDYDATA:message}
diff --git a/pkg/build.sh b/pkg/build.sh
new file mode 100755
index 00000000000..ef1be575476
--- /dev/null
+++ b/pkg/build.sh
@@ -0,0 +1,127 @@
+#!/bin/bash
+# We only need to build two packages now, rpm and deb.  Leaving the os/version stuff in case things change.
+
+[ ! -f ../.VERSION.mk ] && make -C .. .VERSION.mk
+
+. ../.VERSION.mk
+
+DEB_REVISION="${REVISION}"
+RPM_REVISION="${REVISION}"
+
+URL="http://logstash.net"
+DESCRIPTION="An extensible logging pipeline"
+
+if [ "$#" -ne 2 ] ; then
+  echo "Usage: $0 <os> <release>"
+  echo 
+  echo "Example: $0 ubuntu 12.10"
+  exit 1
+fi
+
+os=$1
+release=$2
+
+echo "Building package for $os $release"
+
+destdir=build/$(echo "$os" | tr ' ' '_')
+prefix=/opt/logstash
+
+if [ "$destdir/$prefix" != "/" -a -d "$destdir/$prefix" ] ; then
+  rm -rf "$destdir/$prefix"
+fi
+
+mkdir -p $destdir/$prefix
+
+# Deploy the tarball to /opt/logstash
+tar="$(dirname $0)/../build/logstash-$VERSION.tar.gz"
+if [ ! -f "$tar" ] ; then
+echo "Unable to find $tar"
+exit 1
+fi
+
+tar -C $destdir/$prefix --strip-components 1 -zxpf $tar
+
+case $os@$release in
+ centos@*|fedora@*|el6@*|sl6@*)
+    mkdir -p $destdir/etc/logrotate.d
+    mkdir -p $destdir/etc/sysconfig
+    mkdir -p $destdir/etc/init.d
+    mkdir -p $destdir/etc/logstash/conf.d
+    mkdir -p $destdir/opt/logstash/tmp
+    mkdir -p $destdir/var/lib/logstash
+    mkdir -p $destdir/var/run/logstash
+    mkdir -p $destdir/var/log/logstash
+    chmod 0755 $destdir/opt/logstash/bin/logstash
+    install -m644 logrotate.conf $destdir/etc/logrotate.d/logstash
+    install -m644 logstash.default $destdir/etc/sysconfig/logstash
+    install -m755 logstash.sysv $destdir/etc/init.d/logstash
+    install -m644 logstash-web.default $destdir/etc/sysconfig/logstash
+    install -m755 logstash-web.sysv $destdir/etc/init.d/logstash-web
+    ;;
+  ubuntu@*|debian@*)
+    mkdir -p $destdir/etc/logstash/conf.d
+    mkdir -p $destdir/etc/logrotate.d
+    mkdir -p $destdir/etc/init
+    mkdir -p $destdir/etc/init.d
+    mkdir -p $destdir/var/lib/logstash
+    mkdir -p $destdir/var/log/logstash
+    mkdir -p $destdir/etc/default
+    touch $destdir/etc/default/logstash
+    install -m644 logrotate.conf $destdir/etc/logrotate.d/logstash
+    install -m644 logstash.default $destdir/etc/default/logstash
+    install -m755 logstash.upstart.ubuntu $destdir/etc/init/logstash.conf
+    install -m755 logstash.sysv $destdir/etc/init.d/logstash
+    install -m644 logstash-web.default $destdir/etc/default/logstash-web
+    install -m755 logstash-web.upstart.ubuntu $destdir/etc/init/logstash-web.conf
+    install -m755 logstash-web.sysv $destdir/etc/init.d/logstash-web
+    ;;
+  *) 
+    echo "Unknown OS: $os $release"
+    exit 1
+    ;;
+esac
+
+description="logstash is a system for managing and processing events and logs"
+case $os in
+  centos|fedora|redhat|sl) 
+    fpm -s dir -t rpm -n logstash -v "$RELEASE" \
+      -a noarch --iteration "1_${RPM_REVISION}" \
+      --url "$URL" \
+      --description "$DESCRIPTION" \
+      -d "jre >= 1.6.0" \
+      --vendor "Elasticsearch" \
+      --license "ASL 2.0" \
+      --rpm-use-file-permissions \
+      --rpm-user root --rpm-group root \
+      --before-install centos/before-install.sh \
+      --before-remove centos/before-remove.sh \
+      --after-install centos/after-install.sh \
+      --config-files etc/sysconfig/logstash \
+      --config-files etc/logrotate.d/logstash \
+      -f -C $destdir .
+    ;;
+  ubuntu|debian)
+    if ! echo $RELEASE | grep -q '\.(dev\|rc.*)'; then
+      # This is a dev or RC version... So change the upstream version
+      # example: 1.2.2.dev => 1.2.2~dev
+      # This ensures a clean upgrade path.
+      RELEASE="$(echo $RELEASE | sed 's/\.\(dev\|rc.*\)/~\1/')"
+    fi
+
+    fpm -s dir -t deb -n logstash -v "$RELEASE" \
+      -a all --iteration "1-${DEB_REVISION}" \
+      --url "$URL" \
+      --description "$DESCRIPTION" \
+      --vendor "Elasticsearch" \
+      --license "Apache 2.0" \
+      -d "java7-runtime-headless | java6-runtime-headless | j2re1.7" \
+      --deb-user root --deb-group root \
+      --before-install $os/before-install.sh \
+      --before-remove $os/before-remove.sh \
+      --after-install $os/after-install.sh \
+      --config-files /etc/default/logstash \
+      --config-files /etc/default/logstash-web \
+      --config-files /etc/logrotate.d/logstash \
+      -f -C $destdir .
+    ;;
+esac
diff --git a/pkg/centos/after-install.sh b/pkg/centos/after-install.sh
new file mode 100644
index 00000000000..0e0be57a6d3
--- /dev/null
+++ b/pkg/centos/after-install.sh
@@ -0,0 +1,5 @@
+/sbin/chkconfig --add logstash
+
+chown -R logstash:logstash /opt/logstash
+chown logstash /var/log/logstash
+chown logstash:logstash /var/lib/logstash
diff --git a/pkg/centos/before-install.sh b/pkg/centos/before-install.sh
new file mode 100644
index 00000000000..5a852488ff3
--- /dev/null
+++ b/pkg/centos/before-install.sh
@@ -0,0 +1,10 @@
+# create logstash group
+if ! getent group logstash >/dev/null; then
+  groupadd -r logstash
+fi
+
+# create logstash user
+if ! getent passwd logstash >/dev/null; then
+  useradd -r -g logstash -d /opt/logstash \
+    -s /sbin/nologin -c "logstash" logstash
+fi
diff --git a/pkg/centos/before-remove.sh b/pkg/centos/before-remove.sh
new file mode 100644
index 00000000000..5109888475f
--- /dev/null
+++ b/pkg/centos/before-remove.sh
@@ -0,0 +1,11 @@
+if [ $1 -eq 0 ]; then
+  /sbin/service logstash stop >/dev/null 2>&1 || true
+  /sbin/chkconfig --del logstash
+  if getent passwd logstash >/dev/null ; then
+    userdel logstash
+  fi
+
+  if getent group logstash > /dev/null ; then
+    groupdel logstash
+  fi
+fi
diff --git a/pkg/debian/after-install.sh b/pkg/debian/after-install.sh
new file mode 100644
index 00000000000..5e0fc08f830
--- /dev/null
+++ b/pkg/debian/after-install.sh
@@ -0,0 +1,5 @@
+#!/bin/sh
+
+chown -R logstash:logstash /opt/logstash
+chown logstash /var/log/logstash
+chown logstash:logstash /var/lib/logstash
diff --git a/pkg/debian/before-install.sh b/pkg/debian/before-install.sh
new file mode 100644
index 00000000000..dfbde2020a9
--- /dev/null
+++ b/pkg/debian/before-install.sh
@@ -0,0 +1,12 @@
+#!/bin/sh
+
+# create logstash group
+if ! getent group logstash >/dev/null; then
+  groupadd -r logstash
+fi
+
+# create logstash user
+if ! getent passwd logstash >/dev/null; then
+  useradd -M -r -g logstash -d /var/lib/logstash \
+    -s /sbin/nologin -c "LogStash Service User" logstash
+fi
diff --git a/pkg/debian/before-remove.sh b/pkg/debian/before-remove.sh
new file mode 100644
index 00000000000..a3f911e60ea
--- /dev/null
+++ b/pkg/debian/before-remove.sh
@@ -0,0 +1,13 @@
+#!/bin/sh
+
+if [ $1 = "remove" ]; then
+  service logstash stop >/dev/null 2>&1 || true
+
+  if getent passwd logstash >/dev/null ; then
+    userdel logstash
+  fi
+
+  if getent group logstash >/dev/null ; then
+    groupdel logstash
+  fi
+fi
diff --git a/pkg/logrotate.conf b/pkg/logrotate.conf
new file mode 100644
index 00000000000..69977aeecc8
--- /dev/null
+++ b/pkg/logrotate.conf
@@ -0,0 +1,9 @@
+/var/log/logstash/*.log {
+        daily
+        rotate 7
+        copytruncate
+        compress
+        delaycompress
+        missingok
+        notifempty
+}
diff --git a/pkg/logstash-web.default b/pkg/logstash-web.default
new file mode 100644
index 00000000000..c9ed22be739
--- /dev/null
+++ b/pkg/logstash-web.default
@@ -0,0 +1,35 @@
+###############################
+# Default settings for logstash
+###############################
+
+# Override Java location
+#JAVACMD=/usr/bin/java
+
+# Set a home directory
+#LS_HOME=/var/lib/logstash
+
+# Arguments to pass to java
+#LS_HEAP_SIZE="256m"
+#LS_JAVA_OPTS="-Djava.io.tmpdir=$HOME"
+
+# Logstash filter worker threads
+#LS_WORKER_THREADS=1
+
+# pidfiles aren't used for upstart; this is for sysv users.
+#LS_PIDFILE=/var/run/logstash.pid
+
+# user id to be invoked as; for upstart: edit /etc/init/logstash.conf
+#LS_USER=logstash
+
+# logstash logging
+#LS_LOG_FILE=/var/log/logstash/logstash-web.log
+#LS_USE_GC_LOGGING="true"
+
+# logstash configuration directory
+#LS_CONF_DIR=/etc/logstash/conf.d
+
+# Open file limit; cannot be overridden in upstart
+#LS_OPEN_FILES=2048
+
+# Nice level
+#LS_NICE=0
diff --git a/pkg/logstash-web.sysv b/pkg/logstash-web.sysv
new file mode 100755
index 00000000000..4e3c7a0d82e
--- /dev/null
+++ b/pkg/logstash-web.sysv
@@ -0,0 +1,149 @@
+#!/bin/sh
+# Init script for logstash Webserver
+# Maintained by Elasticsearch
+# Generated by pleaserun.
+# Implemented based on LSB Core 3.1:
+#   * Sections: 20.2, 20.3
+#
+### BEGIN INIT INFO
+# Provides:          logstash-web
+# Required-Start:    $remote_fs $syslog
+# Required-Stop:     $remote_fs $syslog
+# Default-Start:     2 3 4 5
+# Default-Stop:      0 1 6
+# Short-Description: 
+# Description:        Starts Logstash webserveras a daemon.
+### END INIT INFO
+
+PATH=/sbin:/usr/sbin:/bin:/usr/bin
+export PATH
+
+if [ `id -u` -ne 0 ]; then
+   echo "You need root privileges to run this script"
+   exit 1
+fi
+
+name=logstash-web
+pidfile="/var/run/$name.pid"
+
+LS_USER=logstash
+LS_GROUP=logstash
+LS_HOME=/var/lib/logstash
+LS_HEAP_SIZE="500m"
+LS_JAVA_OPTS="-Djava.io.tmpdir=${LS_HOME}"
+LS_LOG_FILE=/var/log/logstash/$name.log
+LS_CONF_DIR=/etc/logstash/conf.d
+LS_OPEN_FILES=16384
+LS_NICE=19
+LS_OPTS=""
+LS_WEB_ADDRESS="0.0.0.0"
+LS_WEB_PORT=9292
+
+[ -r /etc/default/$name ] && . /etc/default/$name
+[ -r /etc/sysconfig/$name ] && . /etc/sysconfig/$name
+
+program=/opt/logstash/bin/logstash
+args="web -a ${LS_WEB_ADDRESS} -p ${LS_WEB_PORT}"
+
+start() {
+
+
+  JAVA_OPTS=${LS_JAVA_OPTS}
+  export PATH HOME JAVA_OPTS LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
+  # Run the program!
+  chroot --userspec $LS_USER:$LS_GROUP / sh -c "
+    cd $LS_HOME
+    nice ${LS_NICE}
+    ulimit -n ${LS_OPEN_FILES}
+    exec \"$program\" $args
+  " > /var/log/logstash/$name.stdout 2> /var/log/logstash/$name.err &
+
+  # Generate the pidfile from here. If we instead made the forked process
+  # generate it there will be a race condition between the pidfile writing
+  # and a process possibly asking for status.
+  echo $! > $pidfile
+
+  echo "$name started."
+  return 0
+}
+
+stop() {
+  # Try a few times to kill TERM the program
+  if status ; then
+    pid=`cat "$pidfile"`
+    echo "Killing $name (pid $pid) with SIGTERM"
+    kill -TERM $pid
+    # Wait for it to exit.
+    for i in 1 2 3 4 5 ; do
+      echo "Waiting $name (pid $pid) to die..."
+      status || break
+      sleep 1
+    done
+    if status ; then
+      echo "$name stop failed; still running."
+    else
+      echo "$name stopped."
+    fi
+  fi
+}
+
+status() {
+  if [ -f "$pidfile" ] ; then
+    pid=`cat "$pidfile"`
+    if kill -0 $pid > /dev/null 2> /dev/null ; then
+      # process by this pid is running.
+      # It may not be our pid, but that's what you get with just pidfiles.
+      # TODO(sissel): Check if this process seems to be the same as the one we
+      # expect. It'd be nice to use flock here, but flock uses fork, not exec,
+      # so it makes it quite awkward to use in this case.
+      return 0
+    else
+      return 2 # program is dead but pid file exists
+    fi
+  else
+    return 3 # program is not running
+  fi
+}
+
+force_stop() {
+  if status ; then
+    stop
+    status && kill -KILL `cat "$pidfile"`
+  fi
+}
+
+
+case "$1" in
+  start)
+    status
+    code=$?
+    if [ $code -eq 0 ]; then
+      echo "$name is already running"
+    else
+      start
+    fi
+    exit $code
+    ;;
+  stop) stop ;;
+  force-stop) force_stop ;;
+  status) 
+    status
+    code=$?
+    if [ $code -eq 0 ] ; then
+      echo "$name is running"
+    else
+      echo "$name is not running"
+    fi
+    exit $code
+    ;;
+  restart) 
+    
+    stop && start 
+    ;;
+  *)
+    echo "Usage: $SCRIPTNAME {start|stop|force-stop|status|restart}" >&2
+    exit 3
+  ;;
+esac
+
+exit $?
diff --git a/pkg/logstash-web.sysv.debian b/pkg/logstash-web.sysv.debian
new file mode 100644
index 00000000000..9d438d66777
--- /dev/null
+++ b/pkg/logstash-web.sysv.debian
@@ -0,0 +1,183 @@
+#!/bin/bash
+#
+# /etc/init.d/logstash-web -- startup script for LogStash web server.
+#
+### BEGIN INIT INFO
+# Provides:          logstash-web
+# Required-Start:    $all
+# Required-Stop:     $all
+# Default-Start:     2 3 4 5
+# Default-Stop:      0 1 6
+# Short-Description: Starts logstash-web
+# Description:       Starts logstash web server using start-stop-daemon
+### END INIT INFO
+
+set -e
+
+NAME=logstash-web
+DESC="Logstash Web Server"
+DEFAULT=/etc/default/$NAME
+
+if [ `id -u` -ne 0 ]; then
+   echo "You need root privileges to run this script"
+   exit 1
+fi
+
+. /lib/lsb/init-functions
+
+if [ -r /etc/default/rcS ]; then
+   . /etc/default/rcS
+fi
+
+# The following variables can be overwritten in $DEFAULT
+PATH=/bin:/usr/bin:/sbin:/usr/sbin
+
+# See contents of file named in $DEFAULT for comments
+LS_USER=logstash
+LS_GROUP=logstash
+LS_HOME=/var/lib/logstash
+LS_HEAP_SIZE="500m"
+LS_JAVA_OPTS="-Djava.io.tmpdir=${LS_HOME}"
+LS_LOG_FILE=""
+LS_CONF_DIR=/etc/logstash/conf.d
+LS_OPEN_FILES=2048
+LS_NICE=19
+LS_OPTS=""
+LS_PIDFILE=/var/run/$NAME.pid
+
+# End of variables that can be overwritten in $DEFAULT
+
+# overwrite settings from default file
+if [ -f "$DEFAULT" ]; then
+   . "$DEFAULT"
+fi
+
+# Define other required variables
+[ -n "${LS_LOG_FILE}" ] && LS_OPTS="${LSOPTS} -l ${LS_LOG_FILE}"
+PID_FILE=${LS_PIDFILE}
+DAEMON=/opt/logstash/bin/logstash
+DAEMON_OPTS="web ${LS_OPTS}"
+
+# Check DAEMON exists
+if ! test -e $DAEMON; then
+   log_failure_msg "Script $DAEMON doesn't exist"
+   exit 1
+fi
+
+case "$1" in
+   start)
+      if [ -z "$DAEMON" ]; then
+         log_failure_msg "no logstash script found - $DAEMON"
+         exit 1
+      fi
+
+      # Check if a config file exists
+      if [ ! "$(ls -A $LS_CONF_DIR/*.conf 2> /dev/null)" ]; then
+         log_failure_msg "There aren't any configuration files in $LS_CONF_DIR"
+         exit 1
+      fi
+
+      log_daemon_msg "Starting $DESC"
+
+      # Parse the actual JAVACMD from the process' environment, we don't care about errors.
+      JAVA=$(cat /proc/$(cat ${PID_FILE} 2>/dev/null)/environ 2>/dev/null | awk -F= 'BEGIN {RS="\0"} /JAVACMD/ {print $2}')
+      if start-stop-daemon --test --start --pidfile "$PID_FILE" \
+         --user "$LS_USER" --exec "$JAVA" \
+      >/dev/null; then
+         # Prepare environment
+         HOME="${HOME:-$LS_HOME}"
+         JAVA_OPTS="${LS_JAVA_OPTS}"
+         ulimit -n ${LS_OPEN_FILES}
+	 cd "${LS_HOME}"
+         export PATH HOME JAVACMD JAVA_OPTS LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
+
+         # Start Daemon
+         start-stop-daemon --start -b --user "$LS_USER" -c "$LS_USER":"$LS_GROUP" \
+           -d "$LS_HOME" --nicelevel "$LS_NICE" --pidfile "$PID_FILE" --make-pidfile \
+           --exec $DAEMON -- $DAEMON_OPTS
+
+         sleep 1
+
+         # Parse the actual JAVACMD from the process' environment, we don't care about errors.
+         JAVA=$(cat /proc/$(cat ${PID_FILE} 2>/dev/null)/environ 2>/dev/null | awk -F= 'BEGIN {RS="\0"} /JAVACMD/ {print $2}')
+         if start-stop-daemon --test --start --pidfile "$PID_FILE" \
+             --user "$LS_USER" --exec "$JAVA" \
+         >/dev/null; then
+
+            if [ -f "$PID_FILE" ]; then
+               rm -f "$PID_FILE"
+            fi
+
+            log_end_msg 1
+         else
+            log_end_msg 0
+         fi
+      else
+         log_progress_msg "(already running)"
+         log_end_msg 0
+      fi
+   ;;
+   stop)
+      log_daemon_msg "Stopping $DESC"
+
+      set +e
+
+      if [ -f "$PID_FILE" ]; then
+         start-stop-daemon --stop --pidfile "$PID_FILE" \
+            --user "$LS_USER" \
+            --retry=TERM/20/KILL/5 >/dev/null
+
+         if [ $? -eq 1 ]; then
+            log_progress_msg "$DESC is not running but pid file exists, cleaning up"
+         elif [ $? -eq 3 ]; then
+            PID="`cat $PID_FILE`"
+            log_failure_msg "Failed to stop $DESC (pid $PID)"
+            exit 1
+         fi
+
+         rm -f "$PID_FILE"
+      else
+         log_progress_msg "(not running)"
+      fi
+
+      log_end_msg 0
+      set -e
+   ;;
+   status)
+      set +e
+
+      # Parse the actual JAVACMD from the process' environment, we don't care about errors.
+      JAVA=$(cat /proc/$(cat ${PID_FILE} 2>/dev/null)/environ 2>/dev/null | awk -F= 'BEGIN {RS="\0"} /JAVACMD/ {print $2}')
+      start-stop-daemon --test --start --pidfile "$PID_FILE" \
+         --user "$LS_USER" --exec "$JAVA" \
+      >/dev/null 2>&1
+
+      if [ "$?" = "0" ]; then
+         if [ -f "$PID_FILE" ]; then
+            log_success_msg "$DESC is not running, but pid file exists."
+            exit 1
+         else
+            log_success_msg "$DESC is not running."
+            exit 3
+         fi
+      else
+         log_success_msg "$DESC is running with pid `cat $PID_FILE`"
+      fi
+
+      set -e
+   ;;
+   restart|force-reload)
+      if [ -f "$PID_FILE" ]; then
+         $0 stop
+         sleep 1
+      fi
+
+      $0 start
+   ;;
+   *)
+      log_success_msg "Usage: $0 {start|stop|restart|force-reload|status}"
+      exit 1
+   ;;
+esac
+
+exit 0
diff --git a/pkg/logstash-web.sysv.redhat b/pkg/logstash-web.sysv.redhat
new file mode 100755
index 00000000000..b78374ce13f
--- /dev/null
+++ b/pkg/logstash-web.sysv.redhat
@@ -0,0 +1,134 @@
+#! /bin/sh
+#
+#       /etc/rc.d/init.d/logstash-web
+#
+#       Starts Logstash Web Server as a daemon
+#
+# chkconfig: 2345 90 10
+# description: Starts Logstash Web Server as a daemon.
+
+### BEGIN INIT INFO
+# Provides: logstash-web
+# Required-Start: $local_fs $remote_fs
+# Required-Stop: $local_fs $remote_fs
+# Default-Start: 2 3 4 5
+# Default-Stop: S 0 1 6
+# Short-Description: Logstash-Web
+# Description: Starts Logstash Web Server as a daemon.
+### END INIT INFO
+
+. /etc/rc.d/init.d/functions
+
+NAME=logstash-web
+DESC="Logstash Web Daemon"
+DEFAULT=/etc/sysconfig/$NAME
+
+if [ `id -u` -ne 0 ]; then
+   echo "You need root privileges to run this script"
+   exit 1
+fi
+
+# The following variables can be overwritten in $DEFAULT
+PATH=/bin:/usr/bin:/sbin:/usr/sbin
+
+# See contents of file named in $DEFAULT for comments
+LS_USER=logstash
+LS_GROUP=logstash
+LS_HOME=/var/lib/logstash
+LS_HEAP_SIZE="500m"
+LS_JAVA_OPTS="-Djava.io.tmpdir=${LS_HOME}"
+LS_LOG_FILE=
+LS_CONF_DIR=/etc/logstash/conf.d
+LS_OPEN_FILES=2048
+LS_NICE=19
+LS_OPTS=""
+LS_PIDFILE=/var/run/$NAME/$NAME.pid
+
+# End of variables that can be overwritten in $DEFAULT
+
+if [ -f "$DEFAULT" ]; then
+  . "$DEFAULT"
+fi
+
+# Define other required variables
+PID_FILE=${LS_PIDFILE}
+test -n "${LS_LOG_FILE}" && LS_OPTS="${LS_OPTS} -l ${LS_LOG_FILE}"
+
+DAEMON="/opt/logstash/bin/logstash"
+DAEMON_OPTS="web ${LS_OPTS}"
+
+#
+# Function that starts the daemon/service
+#
+do_start()
+{
+
+  if [ -z "$DAEMON" ]; then
+    echo "not found - $DAEMON"
+    exit 1
+  fi
+
+  if pidofproc -p "$PID_FILE" >/dev/null; then
+    exit 0
+  fi
+
+  # Prepare environment
+  HOME="${HOME:-$LS_HOME}"
+  JAVA_OPTS="${LS_JAVA_OPTS}"
+  ulimit -n ${LS_OPEN_FILES}
+  cd "${LS_HOME}"
+  export PATH HOME JAVA_OPTS LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
+  test -n "${JAVACMD}" && export JAVACMD
+
+  nice -n ${LS_NICE} runuser -s /bin/sh -c "exec $DAEMON $DAEMON_OPTS" ${LS_USER} > /dev/null 2>&1 < /dev/null &
+
+  RETVAL=$?
+  local PID=$!
+  # runuser forks rather than execing our process.
+  usleep 500000
+  JAVA_PID=$(ps axo ppid,pid | awk -v "ppid=$PID" '$1==ppid {print $2}')
+  PID=${JAVA_PID:-$PID}
+  echo $PID > $PID_FILE
+  [ "$PID" = "$JAVA_PID" ] && success
+}
+
+#
+# Function that stops the daemon/service
+#
+do_stop()
+{
+    killproc -p $PID_FILE $DAEMON
+    RETVAL=$?
+    echo
+    [ $RETVAL = 0 ] && rm -f ${PID_FILE}
+}
+
+case "$1" in
+  start)
+    echo -n "Starting $DESC: "
+    do_start
+    touch /var/run/logstash/$NAME
+    ;;
+  stop)
+    echo -n "Stopping $DESC: "
+    do_stop
+    rm /var/run/logstash/$NAME
+    ;;
+  restart|reload)
+    echo -n "Restarting $DESC: "
+    do_stop
+    do_start
+    ;;
+  status)
+    echo -n "$DESC"
+    status -p $PID_FILE
+    exit $?
+    ;;
+  *)
+    echo "Usage: $SCRIPTNAME {start|stop|status|restart}" >&2
+    exit 3
+    ;;
+esac
+
+echo
+exit 0
diff --git a/pkg/logstash-web.upstart.ubuntu b/pkg/logstash-web.upstart.ubuntu
new file mode 100644
index 00000000000..465369fcae4
--- /dev/null
+++ b/pkg/logstash-web.upstart.ubuntu
@@ -0,0 +1,48 @@
+# logstash-web - web instance
+#
+
+description     "logstash-web agent"
+
+start on virtual-filesystems
+stop on runlevel [06]
+
+# Respawn it if the process exits
+respawn
+
+# We're setting high here, we'll re-limit below.
+limit nofile 65550 65550
+
+setuid logstash
+setgid logstash
+
+# You need to chdir somewhere writable because logstash needs to unpack a few
+# temporary files on startup.
+console log
+script
+  # Defaults
+  PATH=/bin:/usr/bin
+  LS_HOME=/var/lib/logstash
+  LS_HEAP_SIZE="500m"
+  LS_JAVA_OPTS="-Djava.io.tmpdir=${LS_HOME}"
+  LS_LOG_FILE=/var/log/logstash/logstash.log
+  LS_USE_GC_LOGGING=""
+  LS_CONF_DIR=/etc/logstash/conf.d
+  LS_OPEN_FILES=2048
+  LS_NICE=19
+  LS_OPTS=""
+
+  # Override our defaults with user defaults:
+  [ -f /etc/default/logstash-web ] && . /etc/default/logstash-web
+
+  HOME="${HOME:-$LS_HOME}"
+  JAVA_OPTS="${LS_JAVA_OPTS}"
+  # Reset filehandle limit
+  ulimit -n ${LS_OPEN_FILES}
+  cd "${LS_HOME}"
+
+  # Export variables
+  export PATH HOME JAVA_OPTS LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
+  test -n "${JAVACMD}" && export JAVACMD
+
+  exec nice -n ${LS_NICE} /opt/logstash/bin/logstash web ${LS_OPTS}
+end script
diff --git a/pkg/logstash.default b/pkg/logstash.default
new file mode 100644
index 00000000000..dcfda0dea58
--- /dev/null
+++ b/pkg/logstash.default
@@ -0,0 +1,35 @@
+###############################
+# Default settings for logstash
+###############################
+
+# Override Java location
+#JAVACMD=/usr/bin/java
+
+# Set a home directory
+#LS_HOME=/var/lib/logstash
+
+# Arguments to pass to logstash agent
+#LS_OPTS=""
+
+# Arguments to pass to java
+#LS_HEAP_SIZE="500m"
+#LS_JAVA_OPTS="-Djava.io.tmpdir=$HOME"
+
+# pidfiles aren't used for upstart; this is for sysv users.
+#LS_PIDFILE=/var/run/logstash.pid
+
+# user id to be invoked as; for upstart: edit /etc/init/logstash.conf
+#LS_USER=logstash
+
+# logstash logging
+#LS_LOG_FILE=/var/log/logstash/logstash.log
+#LS_USE_GC_LOGGING="true"
+
+# logstash configuration directory
+#LS_CONF_DIR=/etc/logstash/conf.d
+
+# Open file limit; cannot be overridden in upstart
+#LS_OPEN_FILES=16384
+
+# Nice level
+#LS_NICE=19
diff --git a/pkg/logstash.sysv b/pkg/logstash.sysv
new file mode 100755
index 00000000000..fddc14d5ed9
--- /dev/null
+++ b/pkg/logstash.sysv
@@ -0,0 +1,153 @@
+#!/bin/sh
+# Init script for logstash
+# Maintained by Elasticsearch
+# Generated by pleaserun.
+# Implemented based on LSB Core 3.1:
+#   * Sections: 20.2, 20.3
+#
+### BEGIN INIT INFO
+# Provides:          logstash
+# Required-Start:    $remote_fs $syslog
+# Required-Stop:     $remote_fs $syslog
+# Default-Start:     2 3 4 5
+# Default-Stop:      0 1 6
+# Short-Description: 
+# Description:        Starts Logstash as a daemon.
+### END INIT INFO
+
+PATH=/sbin:/usr/sbin:/bin:/usr/bin
+export PATH
+
+if [ `id -u` -ne 0 ]; then
+   echo "You need root privileges to run this script"
+   exit 1
+fi
+
+name=logstash
+pidfile="/var/run/$name.pid"
+
+LS_USER=logstash
+LS_GROUP=logstash
+LS_HOME=/var/lib/logstash
+LS_HEAP_SIZE="500m"
+LS_JAVA_OPTS="-Djava.io.tmpdir=${LS_HOME}"
+LS_LOG_DIR=/var/log/logstash
+LS_LOG_FILE="${LS_LOG_DIR}/$name.log"
+LS_CONF_DIR=/etc/logstash/conf.d
+LS_OPEN_FILES=16384
+LS_NICE=19
+LS_OPTS=""
+
+[ -r /etc/default/$name ] && . /etc/default/$name
+[ -r /etc/sysconfig/$name ] && . /etc/sysconfig/$name
+
+program=/opt/logstash/bin/logstash
+args="agent -f ${LS_CONF_DIR} -l ${LS_LOG_FILE} ${LS_OPTS}"
+
+start() {
+
+
+  JAVA_OPTS=${LS_JAVA_OPTS}
+  HOME=${LS_HOME}
+  export PATH HOME JAVA_OPTS LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
+
+  # set ulimit as (root, presumably) first, before we drop privileges
+  ulimit -n ${LS_OPEN_FILES}
+
+  # Run the program!
+  nice -n ${LS_NICE} chroot --userspec $LS_USER:$LS_GROUP / sh -c "
+    cd $LS_HOME
+    ulimit -n ${LS_OPEN_FILES}
+    exec \"$program\" $args
+  " > "${LS_LOG_DIR}/$name.stdout" 2> "${LS_LOG_DIR}/$name.err" &
+
+  # Generate the pidfile from here. If we instead made the forked process
+  # generate it there will be a race condition between the pidfile writing
+  # and a process possibly asking for status.
+  echo $! > $pidfile
+
+  echo "$name started."
+  return 0
+}
+
+stop() {
+  # Try a few times to kill TERM the program
+  if status ; then
+    pid=`cat "$pidfile"`
+    echo "Killing $name (pid $pid) with SIGTERM"
+    kill -TERM $pid
+    # Wait for it to exit.
+    for i in 1 2 3 4 5 ; do
+      echo "Waiting $name (pid $pid) to die..."
+      status || break
+      sleep 1
+    done
+    if status ; then
+      echo "$name stop failed; still running."
+    else
+      echo "$name stopped."
+    fi
+  fi
+}
+
+status() {
+  if [ -f "$pidfile" ] ; then
+    pid=`cat "$pidfile"`
+    if kill -0 $pid > /dev/null 2> /dev/null ; then
+      # process by this pid is running.
+      # It may not be our pid, but that's what you get with just pidfiles.
+      # TODO(sissel): Check if this process seems to be the same as the one we
+      # expect. It'd be nice to use flock here, but flock uses fork, not exec,
+      # so it makes it quite awkward to use in this case.
+      return 0
+    else
+      return 2 # program is dead but pid file exists
+    fi
+  else
+    return 3 # program is not running
+  fi
+}
+
+force_stop() {
+  if status ; then
+    stop
+    status && kill -KILL `cat "$pidfile"`
+  fi
+}
+
+
+case "$1" in
+  start)
+    status
+    code=$?
+    if [ $code -eq 0 ]; then
+      echo "$name is already running"
+    else
+      start
+      code=$?
+    fi
+    exit $code
+    ;;
+  stop) stop ;;
+  force-stop) force_stop ;;
+  status) 
+    status
+    code=$?
+    if [ $code -eq 0 ] ; then
+      echo "$name is running"
+    else
+      echo "$name is not running"
+    fi
+    exit $code
+    ;;
+  restart) 
+    
+    stop && start 
+    ;;
+  *)
+    echo "Usage: $SCRIPTNAME {start|stop|force-stop|status|restart}" >&2
+    exit 3
+  ;;
+esac
+
+exit $?
diff --git a/pkg/logstash.sysv.debian b/pkg/logstash.sysv.debian
new file mode 100644
index 00000000000..61e8c809933
--- /dev/null
+++ b/pkg/logstash.sysv.debian
@@ -0,0 +1,182 @@
+#!/bin/bash
+#
+# /etc/init.d/logstash -- startup script for LogStash.
+#
+### BEGIN INIT INFO
+# Provides:          logstash
+# Required-Start:    $all
+# Required-Stop:     $all
+# Default-Start:     2 3 4 5
+# Default-Stop:      0 1 6
+# Short-Description: Starts logstash
+# Description:       Starts logstash using start-stop-daemon
+### END INIT INFO
+
+set -e
+
+NAME=logstash
+DESC="Logstash Daemon"
+DEFAULT=/etc/default/$NAME
+
+if [ `id -u` -ne 0 ]; then
+   echo "You need root privileges to run this script"
+   exit 1
+fi
+
+. /lib/lsb/init-functions
+
+if [ -r /etc/default/rcS ]; then
+   . /etc/default/rcS
+fi
+
+# The following variables can be overwritten in $DEFAULT
+PATH=/bin:/usr/bin:/sbin:/usr/sbin
+
+# See contents of file named in $DEFAULT for comments
+LS_USER=logstash
+LS_GROUP=logstash
+LS_HOME=/var/lib/logstash
+LS_HEAP_SIZE="500m"
+LS_JAVA_OPTS="-Djava.io.tmpdir=${LS_HOME}"
+LS_LOG_FILE=/var/log/logstash/$NAME.log
+LS_CONF_DIR=/etc/logstash/conf.d
+LS_OPEN_FILES=16384
+LS_NICE=19
+LS_OPTS=""
+LS_PIDFILE=/var/run/$NAME.pid
+
+# End of variables that can be overwritten in $DEFAULT
+
+# overwrite settings from default file
+if [ -f "$DEFAULT" ]; then
+   . "$DEFAULT"
+fi
+
+# Define other required variables
+PID_FILE=${LS_PIDFILE}
+DAEMON=/opt/logstash/bin/logstash
+DAEMON_OPTS="agent -f ${LS_CONF_DIR} -l ${LS_LOG_FILE} ${LS_OPTS}"
+
+# Check DAEMON exists
+if ! test -e $DAEMON; then
+   log_failure_msg "Script $DAEMON doesn't exist"
+   exit 1
+fi
+
+case "$1" in
+   start)
+      if [ -z "$DAEMON" ]; then
+         log_failure_msg "no logstash script found - $DAEMON"
+         exit 1
+      fi
+
+      # Check if a config file exists
+      if [ ! "$(ls -A $LS_CONF_DIR/*.conf 2> /dev/null)" ]; then
+         log_failure_msg "There aren't any configuration files in $LS_CONF_DIR"
+         exit 1
+      fi
+
+      log_daemon_msg "Starting $DESC"
+
+      # Parse the actual JAVACMD from the process' environment, we don't care about errors.
+      JAVA=$(cat /proc/$(cat "${PID_FILE}" 2>/dev/null)/environ 2>/dev/null | grep -z ^JAVACMD= | cut -d= -f2)
+      if start-stop-daemon --test --start --pidfile "$PID_FILE" \
+         --user "$LS_USER" --exec "$JAVA" \
+      >/dev/null; then
+         # Prepare environment
+         HOME="${HOME:-$LS_HOME}"
+         JAVA_OPTS="${LS_JAVA_OPTS}"
+         ulimit -n ${LS_OPEN_FILES}
+	 cd "${LS_HOME}"
+         export PATH HOME JAVACMD JAVA_OPTS LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
+
+         # Start Daemon
+         start-stop-daemon --start -b --user "$LS_USER" -c "$LS_USER":"$LS_GROUP" \
+           -d "$LS_HOME" --nicelevel "$LS_NICE" --pidfile "$PID_FILE" --make-pidfile \
+           --exec $DAEMON -- $DAEMON_OPTS
+
+         sleep 1
+
+         # Parse the actual JAVACMD from the process' environment, we don't care about errors.
+         JAVA=$(cat /proc/$(cat "${PID_FILE}" 2>/dev/null)/environ 2>/dev/null | grep -z ^JAVACMD= | cut -d= -f2)
+         if start-stop-daemon --test --start --pidfile "$PID_FILE" \
+             --user "$LS_USER" --exec "$JAVA" \
+         >/dev/null; then
+
+            if [ -f "$PID_FILE" ]; then
+               rm -f "$PID_FILE"
+            fi
+
+            log_end_msg 1
+         else
+            log_end_msg 0
+         fi
+      else
+         log_progress_msg "(already running)"
+         log_end_msg 0
+      fi
+   ;;
+   stop)
+      log_daemon_msg "Stopping $DESC"
+
+      set +e
+
+      if [ -f "$PID_FILE" ]; then
+         start-stop-daemon --stop --pidfile "$PID_FILE" \
+            --user "$LS_USER" \
+            --retry=TERM/20/KILL/5 >/dev/null
+
+         if [ $? -eq 1 ]; then
+            log_progress_msg "$DESC is not running but pid file exists, cleaning up"
+         elif [ $? -eq 3 ]; then
+            PID="`cat $PID_FILE`"
+            log_failure_msg "Failed to stop $DESC (pid $PID)"
+            exit 1
+         fi
+
+         rm -f "$PID_FILE"
+      else
+         log_progress_msg "(not running)"
+      fi
+
+      log_end_msg 0
+      set -e
+   ;;
+   status)
+      set +e
+
+      # Parse the actual JAVACMD from the process' environment, we don't care about errors.
+      JAVA=$(cat /proc/$(cat "${PID_FILE}" 2>/dev/null)/environ 2>/dev/null | grep -z ^JAVACMD= | cut -d= -f2)
+      start-stop-daemon --test --start --pidfile "$PID_FILE" \
+         --user "$LS_USER" --exec "$JAVA" \
+      >/dev/null 2>&1
+
+      if [ "$?" = "0" ]; then
+         if [ -f "$PID_FILE" ]; then
+            log_success_msg "$DESC is not running, but pid file exists."
+            exit 1
+         else
+            log_success_msg "$DESC is not running."
+            exit 3
+         fi
+      else
+         log_success_msg "$DESC is running with pid `cat $PID_FILE`"
+      fi
+
+      set -e
+   ;;
+   restart|force-reload)
+      if [ -f "$PID_FILE" ]; then
+         $0 stop
+         sleep 1
+      fi
+
+      $0 start
+   ;;
+   *)
+      log_success_msg "Usage: $0 {start|stop|restart|force-reload|status}"
+      exit 1
+   ;;
+esac
+
+exit 0
diff --git a/pkg/logstash.sysv.redhat b/pkg/logstash.sysv.redhat
new file mode 100755
index 00000000000..f95f3c1db34
--- /dev/null
+++ b/pkg/logstash.sysv.redhat
@@ -0,0 +1,133 @@
+#! /bin/sh
+#
+#       /etc/rc.d/init.d/logstash
+#
+#       Starts Logstash as a daemon
+#
+# chkconfig: 2345 90 10
+# description: Starts Logstash as a daemon.
+
+### BEGIN INIT INFO
+# Provides: logstash
+# Required-Start: $local_fs $remote_fs
+# Required-Stop: $local_fs $remote_fs
+# Default-Start: 2 3 4 5
+# Default-Stop: S 0 1 6
+# Short-Description: Logstash
+# Description: Starts Logstash as a daemon.
+### END INIT INFO
+
+. /etc/rc.d/init.d/functions
+
+NAME=logstash
+DESC="Logstash Daemon"
+DEFAULT=/etc/sysconfig/$NAME
+
+if [ `id -u` -ne 0 ]; then
+   echo "You need root privileges to run this script"
+   exit 1
+fi
+
+# The following variables can be overwritten in $DEFAULT
+PATH=/bin:/usr/bin:/sbin:/usr/sbin
+
+# See contents of file named in $DEFAULT for comments
+LS_USER=logstash
+LS_GROUP=logstash
+LS_HOME=/var/lib/logstash
+LS_HEAP_SIZE="500m"
+LS_JAVA_OPTS="-Djava.io.tmpdir=${LS_HOME}"
+LS_LOG_FILE=/var/log/logstash/$NAME.log
+LS_CONF_DIR=/etc/logstash/conf.d
+LS_OPEN_FILES=16384
+LS_NICE=19
+LS_OPTS=""
+LS_PIDFILE=/var/run/$NAME/$NAME.pid
+
+# End of variables that can be overwritten in $DEFAULT
+
+if [ -f "$DEFAULT" ]; then
+  . "$DEFAULT"
+fi
+
+# Define other required variables
+PID_FILE=${LS_PIDFILE}
+
+DAEMON="/opt/logstash/bin/logstash"
+DAEMON_OPTS="agent -f ${LS_CONF_DIR} -l ${LS_LOG_FILE} ${LS_OPTS}"
+
+#
+# Function that starts the daemon/service
+#
+do_start()
+{
+
+  if [ -z "$DAEMON" ]; then
+    echo "not found - $DAEMON"
+    exit 1
+  fi
+
+  if pidofproc -p "$PID_FILE" >/dev/null; then
+    exit 0
+  fi
+
+  # Prepare environment
+  HOME="${HOME:-$LS_HOME}"
+  JAVA_OPTS="${LS_JAVA_OPTS}"
+  ulimit -n ${LS_OPEN_FILES}
+  cd "${LS_HOME}"
+  export PATH HOME JAVA_OPTS LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
+  test -n "${JAVACMD}" && export JAVACMD
+
+  nice -n ${LS_NICE} runuser -s /bin/sh -c "exec $DAEMON $DAEMON_OPTS" ${LS_USER} >> $LS_LOG_FILE 2>&1 < /dev/null &
+
+  RETVAL=$?
+  local PID=$!
+  # runuser forks rather than execing our process.
+  usleep 500000
+  JAVA_PID=$(ps axo ppid,pid | awk -v "ppid=$PID" '$1==ppid {print $2}')
+  PID=${JAVA_PID:-$PID}
+  echo $PID > $PID_FILE
+  [ "$PID" = "$JAVA_PID" ] && success
+}
+
+#
+# Function that stops the daemon/service
+#
+do_stop()
+{
+    killproc -p $PID_FILE $DAEMON
+    RETVAL=$?
+    echo
+    [ $RETVAL = 0 ] && rm -f ${PID_FILE}
+}
+
+case "$1" in
+  start)
+    echo -n "Starting $DESC: "
+    do_start
+    touch /var/run/logstash/$NAME
+    ;;
+  stop)
+    echo -n "Stopping $DESC: "
+    do_stop
+    rm /var/run/logstash/$NAME
+    ;;
+  restart|reload)
+    echo -n "Restarting $DESC: "
+    do_stop
+    do_start
+    ;;
+  status)
+    echo -n "$DESC"
+    status -p $PID_FILE
+    exit $?
+    ;;
+  *)
+    echo "Usage: $SCRIPTNAME {start|stop|status|restart}" >&2
+    exit 3
+    ;;
+esac
+
+echo
+exit 0
diff --git a/pkg/logstash.upstart.ubuntu b/pkg/logstash.upstart.ubuntu
new file mode 100644
index 00000000000..54994f70226
--- /dev/null
+++ b/pkg/logstash.upstart.ubuntu
@@ -0,0 +1,48 @@
+# logstash - agent instance
+#
+
+description     "logstash agent"
+
+start on virtual-filesystems
+stop on runlevel [06]
+
+# Respawn it if the process exits
+respawn
+
+# We're setting high here, we'll re-limit below.
+limit nofile 65550 65550
+
+setuid logstash
+setgid logstash
+
+# You need to chdir somewhere writable because logstash needs to unpack a few
+# temporary files on startup.
+console log
+script
+  # Defaults
+  PATH=/bin:/usr/bin
+  LS_HOME=/var/lib/logstash
+  LS_HEAP_SIZE="500m"
+  LS_JAVA_OPTS="-Djava.io.tmpdir=${LS_HOME}"
+  LS_LOG_FILE=/var/log/logstash/logstash.log
+  LS_USE_GC_LOGGING=""
+  LS_CONF_DIR=/etc/logstash/conf.d
+  LS_OPEN_FILES=16384
+  LS_NICE=19
+  LS_OPTS=""
+
+  # Override our defaults with user defaults:
+  [ -f /etc/default/logstash ] && . /etc/default/logstash
+
+  HOME="${HOME:-$LS_HOME}"
+  JAVA_OPTS="${LS_JAVA_OPTS}"
+  # Reset filehandle limit
+  ulimit -n ${LS_OPEN_FILES}
+  cd "${LS_HOME}"
+
+  # Export variables
+  export PATH HOME JAVA_OPTS LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
+  test -n "${JAVACMD}" && export JAVACMD
+
+  exec nice -n ${LS_NICE} /opt/logstash/bin/logstash agent -f "${LS_CONF_DIR}" -l "${LS_LOG_FILE}" ${LS_OPTS}
+end script
diff --git a/pkg/ubuntu/after-install.sh b/pkg/ubuntu/after-install.sh
new file mode 100644
index 00000000000..5e0fc08f830
--- /dev/null
+++ b/pkg/ubuntu/after-install.sh
@@ -0,0 +1,5 @@
+#!/bin/sh
+
+chown -R logstash:logstash /opt/logstash
+chown logstash /var/log/logstash
+chown logstash:logstash /var/lib/logstash
diff --git a/pkg/ubuntu/before-install.sh b/pkg/ubuntu/before-install.sh
new file mode 100644
index 00000000000..dfbde2020a9
--- /dev/null
+++ b/pkg/ubuntu/before-install.sh
@@ -0,0 +1,12 @@
+#!/bin/sh
+
+# create logstash group
+if ! getent group logstash >/dev/null; then
+  groupadd -r logstash
+fi
+
+# create logstash user
+if ! getent passwd logstash >/dev/null; then
+  useradd -M -r -g logstash -d /var/lib/logstash \
+    -s /sbin/nologin -c "LogStash Service User" logstash
+fi
diff --git a/pkg/ubuntu/before-remove.sh b/pkg/ubuntu/before-remove.sh
new file mode 100644
index 00000000000..a3f911e60ea
--- /dev/null
+++ b/pkg/ubuntu/before-remove.sh
@@ -0,0 +1,13 @@
+#!/bin/sh
+
+if [ $1 = "remove" ]; then
+  service logstash stop >/dev/null 2>&1 || true
+
+  if getent passwd logstash >/dev/null ; then
+    userdel logstash
+  fi
+
+  if getent group logstash >/dev/null ; then
+    groupdel logstash
+  fi
+fi
diff --git a/rakelib/artifacts.rake b/rakelib/artifacts.rake
new file mode 100644
index 00000000000..ef30e0f3890
--- /dev/null
+++ b/rakelib/artifacts.rake
@@ -0,0 +1,176 @@
+def staging
+  "build/staging"
+end
+
+namespace "artifact" do
+  require "logstash/environment"
+  def package_files
+    [
+      "LICENSE",
+      "CHANGELOG",
+      "CONTRIBUTORS",
+      "{bin,lib,spec,locales}/{,**/*}",
+      "patterns/**/*",
+      "vendor/??*/**/*",
+      File.join(LogStash::Environment.gem_home.gsub(Dir.pwd + "/", ""), "{gems,specifications}/**/*"),
+      "Rakefile",
+      "rakelib/*",
+    ]
+  end
+
+  def exclude_globs
+    return @exclude_globs if @exclude_globs
+    @exclude_globs = []
+    #gitignore = File.join(File.dirname(__FILE__), "..", ".gitignore")
+    #if File.exists?(gitignore)
+      #@exclude_globs += File.read(gitignore).split("\n")
+    #end
+    @exclude_globs << "spec/reports/**/*"
+    @exclude_globs << "**/*.gem"
+    @exclude_globs << "**/test/files/slow-xpath.xml"
+    return @exclude_globs
+  end
+
+  def excludes
+    return @excludes if @excludes
+    @excludes = exclude_globs.collect { |g| Rake::FileList[g] }.flatten
+  end
+
+  def exclude?(path)
+    excludes.any? { |ex| path == ex || (File.directory?(ex) && path =~ /^#{ex}\//) }
+  end
+
+  def files
+    return @files if @files
+    @files = package_files.collect do |glob|
+      Rake::FileList[glob].reject { |path| exclude?(path) }
+    end.flatten.uniq
+  end
+  
+  desc "Build a tar.gz of logstash with all dependencies"
+  task "tar" => ["bootstrap"] do
+    require "zlib"
+    require "archive/tar/minitar"
+    require "logstash/version"
+    tarpath = "build/logstash-#{LOGSTASH_VERSION}.tar.gz"
+    tarfile = File.new(tarpath, "wb")
+    gz = Zlib::GzipWriter.new(tarfile, Zlib::BEST_COMPRESSION)
+    tar = Archive::Tar::Minitar::Output.new(gz)
+    files.each do |path|
+      Archive::Tar::Minitar.pack_file(path, tar)
+    end
+    tar.close
+    gz.close
+    puts "Complete: #{tarpath}"
+  end
+
+  def package(platform, version)
+    Rake::Task["dependency:fpm"].invoke
+    require "fpm/errors" # TODO(sissel): fix this in fpm
+    require "fpm/package/dir"
+    require "fpm/package/gem" # TODO(sissel): fix this in fpm; rpm needs it.
+
+    dir = FPM::Package::Dir.new
+
+    files.each do |path|
+      next if File.directory?(path)
+      dir.input("#{path}=/opt/logstash/#{path}")
+    end
+
+    basedir = File.join(File.dirname(__FILE__), "..")
+
+    File.join(basedir, "pkg", "logrotate.conf").tap do |path|
+      dir.input("#{path}=/etc/logrotate.d/logstash")
+    end
+
+    case platform
+      when "redhat", "centos"
+        File.join(basedir, "pkg", "logrotate.conf").tap do |path|
+          dir.input("#{path}=/etc/logrotate.d/logstash")
+        end
+        File.join(basedir, "pkg", "logstash.default").tap do |path|
+          dir.input("#{path}=/etc/sysconfig/logstash")
+        end
+        require "fpm/package/rpm"
+        out = dir.convert(FPM::Package::RPM)
+        out.license = "ASL 2.0" # Red Hat calls 'Apache Software License' == ASL
+        out.attributes[:rpm_use_file_permissions] = true
+        out.attributes[:rpm_user] = "root"
+        out.attributes[:rpm_group] = "root"
+        out.config_files << "etc/sysconfig/logstash"
+        out.config_files << "etc/logrotate.d/logstash"
+      when "debian", "ubuntu"
+        File.join(basedir, "pkg", "logstash.default").tap do |path|
+          dir.input("#{path}=/etc/default/logstash")
+        end
+        require "fpm/package/deb"
+        out = dir.convert(FPM::Package::Deb)
+        out.license = "Apache 2.0"
+        out.attributes[:deb_user] = "root"
+        out.attributes[:deb_group] = "root"
+        out.attributes[:deb_suggests] = "java7-runtime-headless"
+        # TODO(sissel): this file should go away once pleaserun is implemented.
+        out.config_files << "/etc/default/logstash"
+
+        out.config_files << "/etc/logrotate.d/logstash"
+    end
+
+    # Packaging install/removal scripts
+    ["before", "after"].each do |stage|
+      ["install", "remove"].each do |action|
+        script = "#{stage}-#{action}" # like, "before-install"
+        script_sym = script.gsub("-", "_").to_sym
+        script_path = File.join(File.dirname(__FILE__), "..", "pkg", platform, "#{script}.sh")
+        next unless File.exists?(script_path)
+
+        out.scripts[script_sym] = File.read(script_path)
+      end
+    end
+
+    # TODO(sissel): Invoke Pleaserun to generate the init scripts/whatever
+
+    out.name = "logstash"
+    out.version = LOGSTASH_VERSION
+    out.architecture = "all"
+    # TODO(sissel): Include the git commit hash?
+    out.iteration = "1" # what revision?
+    out.url = "http://www.elasticsearch.org/overview/logstash/"
+    out.description = "An extensible logging pipeline"
+    out.vendor = "Elasticsearch"
+    out.dependencies << "logrotate"
+
+    # We don't specify a dependency on Java because:
+    # - On Red Hat, Oracle and Red Hat both label their java packages in
+    #   incompatible ways. Further, there is no way to guarantee a qualified
+    #   version is available to install.
+    # - On Debian and Ubuntu, there is no Oracle package and specifying a
+    #   correct version of OpenJDK is impossible because there is no guarantee that
+    #   is impossible for the same reasons as the Red Hat section above.
+    # References:
+    # - http://www.elasticsearch.org/blog/java-1-7u55-safe-use-elasticsearch-lucene/
+    # - deb: https://github.com/elasticsearch/logstash/pull/1008
+    # - rpm: https://github.com/elasticsearch/logstash/pull/1290
+    # - rpm: https://github.com/elasticsearch/logstash/issues/1673
+    # - rpm: https://logstash.jira.com/browse/LOGSTASH-1020
+    
+    out.attributes[:force?] = true # overwrite the rpm/deb/etc being created
+    begin
+      path = File.join(basedir, "build", out.to_s)
+      x = out.output(path)
+      puts "Completed: #{path}"
+    ensure
+      out.cleanup
+    end
+  end # def package
+
+  desc "Build an RPM of logstash with all dependencies"
+  task "rpm" => ["bootstrap"] do
+    package("centos", "5")
+  end
+
+  desc "Build an RPM of logstash with all dependencies"
+  task "deb" => ["bootstrap"] do
+    package("ubuntu", "12.04")
+  end
+end
+
diff --git a/rakelib/bootstrap.rake b/rakelib/bootstrap.rake
new file mode 100644
index 00000000000..477adb4f1e7
--- /dev/null
+++ b/rakelib/bootstrap.rake
@@ -0,0 +1,3 @@
+
+
+task "bootstrap" => [ "vendor:all", "compile:all" ]
diff --git a/rakelib/build.rake b/rakelib/build.rake
new file mode 100644
index 00000000000..2b443add8b7
--- /dev/null
+++ b/rakelib/build.rake
@@ -0,0 +1,6 @@
+directory "build" do |task, args|
+  mkdir_p task.name unless File.directory?(task.name)
+end
+directory "build/bootstrap" => "build" do |task, args|
+  mkdir_p task.name unless File.directory?(task.name)
+end
diff --git a/rakelib/bundler_patch.rb b/rakelib/bundler_patch.rb
new file mode 100644
index 00000000000..a68cb60736d
--- /dev/null
+++ b/rakelib/bundler_patch.rb
@@ -0,0 +1,11 @@
+# Patch bundler to write a .lock file specific to the version of ruby.
+# This keeps MRI/JRuby/RBX from conflicting over the Gemfile.lock updates
+module Bundler
+  module SharedHelpers
+    def default_lockfile
+      ruby = "#{LogStash::Environment.ruby_engine}-#{LogStash::Environment.ruby_abi_version}"
+      return Pathname.new("#{default_gemfile}.#{ruby}.lock")
+    end
+  end
+end
+
diff --git a/rakelib/compile.rake b/rakelib/compile.rake
new file mode 100644
index 00000000000..97744da8396
--- /dev/null
+++ b/rakelib/compile.rake
@@ -0,0 +1,18 @@
+
+rule ".rb" => ".treetop" do |task, args|
+  # TODO(sissel): Treetop 1.5.x doesn't seem to work well, but I haven't
+  # investigated what the cause might be. -Jordan
+  Rake::Task["gem:require"].invoke("treetop", "~> 1.4.0", ENV["GEM_HOME"])
+  require "treetop"
+  compiler = Treetop::Compiler::GrammarCompiler.new
+  compiler.compile(task.source, task.name)
+  puts "Compiling #{task.source}"
+end
+
+namespace "compile" do
+  desc "Compile the config grammar"
+  task "grammar" => "lib/logstash/config/grammar.rb"
+
+  desc "Build everything"
+  task "all" => "grammar"
+end
diff --git a/rakelib/copy.rake b/rakelib/copy.rake
new file mode 100644
index 00000000000..f40747cc049
--- /dev/null
+++ b/rakelib/copy.rake
@@ -0,0 +1,4 @@
+
+def staging
+  "build/staging"
+end
diff --git a/rakelib/dependency.rake b/rakelib/dependency.rake
new file mode 100644
index 00000000000..601f9a10aeb
--- /dev/null
+++ b/rakelib/dependency.rake
@@ -0,0 +1,23 @@
+
+namespace "dependency" do
+  task "bundler" do
+    Rake::Task["gem:require"].invoke("bundler", ">= 1.3.5", ENV["GEM_HOME"])
+    #require_relative "bundler_patch"
+  end
+
+  task "rbx-stdlib" do
+    Rake::Task["gem:require"].invoke("rubysl", ">= 0", ENV["GEM_HOME"])
+  end # task rbx-stdlib
+
+  task "archive-tar-minitar" do
+    Rake::Task["gem:require"].invoke("minitar", ">= 0", ENV["GEM_HOME"])
+  end # task archive-minitar
+
+  task "stud" do
+    Rake::Task["gem:require"].invoke("stud", ">= 0", ENV["GEM_HOME"])
+  end # task stud
+
+  task "fpm" do
+    Rake::Task["gem:require"].invoke("fpm", ">= 0", ENV["GEM_HOME"])
+  end # task stud
+end # namespace dependency
diff --git a/rakelib/fetch.rake b/rakelib/fetch.rake
new file mode 100644
index 00000000000..0eded6c0158
--- /dev/null
+++ b/rakelib/fetch.rake
@@ -0,0 +1,82 @@
+require "net/http"
+require "uri"
+require "digest/sha1"
+
+directory "vendor/_" => ["vendor"] do |task, args|
+  mkdir task.name
+end
+
+def fetch(url, sha1, output)
+  puts "Downloading #{url}"
+  actual_sha1 = download(url, output)
+
+  if actual_sha1 != sha1
+    fail "SHA1 does not match (expected '#{sha1}' but got '#{actual_sha1}')"
+  end
+end # def fetch
+
+def file_fetch(url, sha1)
+  filename = File.basename(URI(url).path)
+  output = "vendor/_/#{filename}"
+  task output => [ "vendor/_" ] do
+    begin
+      actual_sha1 = file_sha1(output)
+      if actual_sha1 != sha1
+        fetch(url, sha1, output)
+      end
+    rescue Errno::ENOENT
+      fetch(url, sha1, output)
+    end
+  end.invoke
+
+  return output
+end
+
+def file_sha1(path)
+  digest = Digest::SHA1.new
+  fd = File.new(path, "r")
+  while true
+    begin
+      digest << fd.sysread(16384)
+    rescue EOFError
+      break
+    end
+  end
+  return digest.hexdigest
+ensure
+  fd.close if fd
+end
+
+def download(url, output)
+  uri = URI(url)
+  digest = Digest::SHA1.new
+  tmp = "#{output}.tmp"
+  Net::HTTP.start(uri.host, uri.port, :use_ssl => (uri.scheme == "https")) do |http|
+    request = Net::HTTP::Get.new(uri.path)
+    http.request(request) do |response|
+      fail "HTTP fetch failed for #{url}. #{response}" if response.code != "200"
+      size = (response["content-length"].to_i || -1).to_f
+      count = 0
+      File.open(tmp, "w") do |fd|
+        response.read_body do |chunk|
+          fd.write(chunk)
+          digest << chunk
+          if size > 0 && $stdout.tty?
+            count += chunk.bytesize
+            $stdout.write(sprintf("\r%0.2f%%", count/size * 100))
+          end
+        end
+      end
+      $stdout.write("\r      \r") if $stdout.tty?
+    end
+  end
+
+  File.rename(tmp, output)
+
+  return digest.hexdigest
+rescue SocketError => e
+  puts "Failure while downloading #{url}: #{e}"
+  raise
+ensure
+  File.unlink(tmp) if File.exist?(tmp)
+end # def download
diff --git a/rakelib/gems.rake b/rakelib/gems.rake
new file mode 100644
index 00000000000..ebf80e8b6e7
--- /dev/null
+++ b/rakelib/gems.rake
@@ -0,0 +1,47 @@
+require "rubygems/specification"
+require "rubygems/commands/install_command"
+require "logstash/JRUBY-PR1448" if RUBY_PLATFORM == "java" && Gem.win_platform?
+
+ENV["GEM_HOME"] = ENV["GEM_PATH"] = "build/bootstrap/"
+Gem.use_paths(ENV["GEM_HOME"])
+
+namespace "gem" do
+  task "require",  :name, :requirement, :target do |task, args|
+    name, requirement, target = args[:name], args[:requirement], args[:target]
+    begin
+      gem name, requirement
+    rescue Gem::LoadError => e
+      puts "Installing #{name} #{requirement} because the build process needs it."
+      Rake::Task["gem:install"].invoke(name, requirement, target)
+    end
+    task.reenable # Allow this task to be run again
+  end
+
+  task "install", [:name, :requirement, :target] =>  ["build/bootstrap"] do |task, args|
+    name, requirement, target = args[:name], args[:requirement], args[:target]
+    puts "[bootstrap] Fetching and installing gem: #{name} (#{requirement})"
+
+    installer = Gem::Commands::InstallCommand.new
+    installer.options[:generate_rdoc] = false
+    installer.options[:generate_ri] = false
+    installer.options[:version] = requirement
+    installer.options[:args] = [name]
+    installer.options[:install_dir] = target
+
+    # ruby 2.0.0 / rubygems 2.x; disable documentation generation
+    installer.options[:document] = []
+    begin
+      installer.execute
+    rescue Gem::LoadError => e
+    # For some weird reason the rescue from the 'require' task is being brought down here
+    # We don't know why placing this solves it, but it does.
+    rescue Gem::SystemExitException => e
+      if e.exit_code != 0
+        puts "Installation of #{name} failed"
+        raise
+      end
+    end
+
+    task.reenable # Allow this task to be run again
+  end # task "install"
+end # namespace "gem"
diff --git a/rakelib/test.rake b/rakelib/test.rake
new file mode 100644
index 00000000000..b8edfa026f2
--- /dev/null
+++ b/rakelib/test.rake
@@ -0,0 +1,18 @@
+
+namespace "test" do
+  task "default" => [ "bootstrap" ] do
+    require "logstash/environment"
+    LogStash::Environment.set_gem_paths!
+    require 'rspec/core'
+    RSpec::Core::Runner.run(Rake::FileList["spec/**/*.rb"])
+  end
+
+  task "fail-fast" => [ "bootstrap" ] do
+    require "logstash/environment"
+    LogStash::Environment.set_gem_paths!
+    require 'rspec/core'
+    RSpec::Core::Runner.run(["--fail-fast", *Rake::FileList["spec/**/*.rb"]])
+  end
+end
+
+task "test" => [ "test:default" ] 
diff --git a/rakelib/vendor.rake b/rakelib/vendor.rake
new file mode 100644
index 00000000000..ec9eeece067
--- /dev/null
+++ b/rakelib/vendor.rake
@@ -0,0 +1,245 @@
+
+DOWNLOADS = {
+  "elasticsearch" => { "version" => "1.3.0", "sha1" => "f9e02e2cdcb55e7e8c5c60e955f793f68b7dec75" },
+  "collectd" => { "version" => "5.4.0", "sha1" => "a90fe6cc53b76b7bdd56dc57950d90787cb9c96e" },
+  #"jruby" => { "version" => "1.7.13", "sha1" => "0dfca68810a5eed7f12ae2007dc2cc47554b4cc6" }, # jruby-complete
+  "jruby" => { "version" => "1.7.16", "sha1" => "4c912b648f6687622ba590ca2a28746d1cd5d550" },
+  "kibana" => { "version" => "3.1.0", "sha1" => "effc20c83c0cb8d5e844d2634bd1854a1858bc43" },
+  "geoip" => {
+    "GeoLiteCity" => { "version" => "2013-01-18", "sha1" => "15aab9a90ff90c4784b2c48331014d242b86bf82", },
+    "GeoIPASNum" => { "version" => "2014-02-12", "sha1" => "6f33ca0b31e5f233e36d1f66fbeae36909b58f91", }
+  },
+  "kafka" => { "version" => "0.8.1.1", "sha1" => "d73cc87fcb01c62fdad8171b7bb9468ac1156e75", "scala_version" => "2.9.2" },
+}
+
+def vendor(*args)
+  return File.join("vendor", *args)
+end
+
+# Untar any files from the given tarball file name.
+#
+# A tar entry is passed to the block. The block should should return 
+# * nil to skip this file
+# * or, the desired string filename to write the file to.
+def untar(tarball, &block)
+  Rake::Task["dependency:archive-tar-minitar"].invoke
+  require "archive/tar/minitar"
+  tgz = Zlib::GzipReader.new(File.open(tarball))
+  # Pull out typesdb
+  tar = Archive::Tar::Minitar::Input.open(tgz)
+  tar.each do |entry|
+    path = block.call(entry)
+    next if path.nil?
+    parent = File.dirname(path)
+    
+    mkdir_p parent unless File.directory?(parent)
+
+    # Skip this file if the output file is the same size
+    if entry.directory?
+      mkdir path unless File.directory?(path)
+    else
+      entry_mode = entry.instance_eval { @mode } & 0777
+      if File.exists?(path)
+        stat = File.stat(path)
+        # TODO(sissel): Submit a patch to archive-tar-minitar upstream to
+        # expose headers in the entry.
+        entry_size = entry.instance_eval { @size }
+        # If file sizes are same, skip writing.
+        next if stat.size == entry_size && (stat.mode & 0777) == entry_mode
+      end
+      puts "Extracting #{entry.full_name} from #{tarball} #{entry_mode.to_s(8)}"
+      File.open(path, "w") do |fd|
+        # eof? check lets us skip empty files. Necessary because the API provided by
+        # Archive::Tar::Minitar::Reader::EntryStream only mostly acts like an
+        # IO object. Something about empty files in this EntryStream causes
+        # IO.copy_stream to throw "can't convert nil into String" on JRuby
+        # TODO(sissel): File a bug about this.
+        while !entry.eof?
+          chunk = entry.read(16384)
+          fd.write(chunk)
+        end
+          #IO.copy_stream(entry, fd)
+      end
+      File.chmod(entry_mode, path)
+    end
+  end
+  tar.close
+end # def untar
+
+namespace "vendor" do
+  task "jruby" do |task, args|
+    name = task.name.split(":")[1]
+    info = DOWNLOADS[name]
+    version = info["version"]
+    #url = "http://jruby.org.s3.amazonaws.com/downloads/#{version}/jruby-complete-#{version}.jar"
+    url = "http://jruby.org.s3.amazonaws.com/downloads/#{version}/jruby-bin-#{version}.tar.gz"
+
+    download = file_fetch(url, info["sha1"])
+    parent = vendor(name).gsub(/\/$/, "")
+    directory parent => "vendor" do
+      mkdir parent
+    end.invoke unless Rake::Task.task_defined?(parent)
+    
+    prefix_re = /^#{Regexp.quote("jruby-#{version}/")}/
+    untar(download) do |entry|
+      out = entry.full_name.gsub(prefix_re, "")
+      next if out =~ /^samples/
+      next if out =~ /@LongLink/
+      vendor(name, out)
+    end # untar
+  end # jruby
+  task "all" => "jruby"
+
+  task "geoip" do |task, args|
+    vendor_name = "geoip"
+    parent = vendor(vendor_name).gsub(/\/$/, "")
+    directory parent => "vendor" do
+      mkdir parent
+    end.invoke unless Rake::Task.task_defined?(parent)
+
+    vendor(vendor_name).tap { |v| mkdir_p v unless File.directory?(v) }
+    files = DOWNLOADS[vendor_name]
+    files.each do |name, info|
+      version = info["version"]
+      url = "http://logstash.objects.dreamhost.com/maxmind/#{name}-#{version}.dat.gz"
+      download = file_fetch(url, info["sha1"])
+      outpath = vendor(vendor_name, "#{name}.dat")
+      tgz = Zlib::GzipReader.new(File.open(download))
+      begin
+        File.open(outpath, "w") do |out|
+          IO::copy_stream(tgz, out)
+        end
+      rescue
+        File.unlink(outpath) if File.file?(outpath)
+        raise
+      end
+      tgz.close
+    end
+  end
+  task "all" => "geoip"
+
+  task "kibana" do |task, args|
+    name = task.name.split(":")[1]
+    info = DOWNLOADS[name]
+    version = info["version"]
+    url = "https://download.elasticsearch.org/kibana/kibana/kibana-#{version}.tar.gz"
+    download = file_fetch(url, info["sha1"])
+
+    parent = vendor(name).gsub(/\/$/, "")
+    directory parent => "vendor" do
+      mkdir parent
+    end.invoke unless Rake::Task.task_defined?(parent)
+
+    prefix_re = /^#{Regexp.quote("kibana-#{version}/")}/
+    untar(download) do |entry|
+      vendor(name, entry.full_name.gsub(prefix_re, ""))
+    end # untar
+  end # task kibana
+  task "all" => "kibana"
+
+  task "kafka" do |task, args|
+    name = task.name.split(":")[1]
+    info = DOWNLOADS[name]
+    version = info["version"]
+    scala_version = info["scala_version"]
+    url = "https://archive.apache.org/dist/kafka/#{version}/kafka_#{scala_version}-#{version}.tgz"
+    download = file_fetch(url, info["sha1"])
+
+    parent = vendor(name).gsub(/\/$/, "")
+    directory parent => "vendor" do
+      mkdir parent
+    end.invoke unless Rake::Task.task_defined?(parent)
+
+    untar(download) do |entry|
+      next unless entry.full_name =~ /\.jar$/
+      vendor(name, File.basename(entry.full_name))
+    end
+  end # task kafka
+  task "all" => "kafka"
+
+  task "elasticsearch" do |task, args|
+    name = task.name.split(":")[1]
+    info = DOWNLOADS[name]
+    version = info["version"]
+    url = "https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-#{version}.tar.gz"
+    download = file_fetch(url, info["sha1"])
+
+    parent = vendor(name).gsub(/\/$/, "")
+    directory parent => "vendor" do
+      mkdir parent
+    end.invoke unless Rake::Task.task_defined?(parent)
+
+    untar(download) do |entry|
+      next unless entry.full_name =~ /\.jar$/
+      vendor(name, File.basename(entry.full_name))
+    end # untar
+  end # task elasticsearch
+  task "all" => "elasticsearch"
+
+  task "collectd" do |task, args|
+    name = task.name.split(":")[1]
+    info = DOWNLOADS[name]
+    version = info["version"]
+    sha1 = info["sha1"]
+    url = "https://collectd.org/files/collectd-#{version}.tar.gz"
+
+    download = file_fetch(url, sha1)
+
+    parent = vendor(name).gsub(/\/$/, "")
+    directory parent => "vendor" do
+      mkdir parent
+    end unless Rake::Task.task_defined?(parent)
+
+    file vendor(name, "types.db") => [download, parent] do |task, args|
+      next if File.exists?(task.name)
+      untar(download) do |entry|
+        next unless entry.full_name == "collectd-#{version}/src/types.db"
+        vendor(name, File.basename(entry.full_name))
+      end # untar
+    end.invoke
+  end
+  task "all" => "collectd"
+
+  task "gems" => [ "dependency:bundler" ] do
+    require "logstash/environment"
+    Rake::Task["dependency:rbx-stdlib"] if LogStash::Environment.ruby_engine == "rbx"
+    Rake::Task["dependency:stud"].invoke
+
+    # Skip bundler if we've already done this recently.
+    donefile = File.join(LogStash::Environment.gem_home, ".done")
+    if File.file?(donefile) 
+      age = (Time.now - File.lstat(donefile).mtime)
+      # Skip if the donefile was last modified recently
+      next if age < 300
+    end
+
+    # Try installing a few times in case we hit the "bad_record_mac" ssl error during installation.
+    10.times do
+      begin
+        #Bundler::CLI.start(["install", "--gemfile=tools/Gemfile", "--path", LogStash::Environment.gem_home, "--clean", "--standalone", "--without", "development", "--jobs", 4])
+        # There doesn't seem to be a way to invoke Bundler::CLI *and* have a
+        # different GEM_HOME set that doesn't impact Bundler's view of what
+        # gems are available. I asked about this in #bundler on freenode, and I
+        # was told to stop using the bundler ruby api. Oh well :(
+        bundler = File.join(Gem.bindir, "bundle")
+        if ENV['USE_RUBY'] == '1'
+          # Use the local jruby binary
+          jruby = 'ruby'
+        else
+          # Use the vendored jruby binary
+          jruby = File.join("vendor", "jruby", "bin", "jruby")
+        end
+        cmd = [jruby,  bundler, "install", "--gemfile=tools/Gemfile", "--path", LogStash::Environment::BUNDLE_DIR, "--standalone", "--clean", "--without", "development", "--jobs", "4"]
+        system(*cmd)
+        raise RuntimeError, $!.to_s unless $?.success?
+        break
+      rescue Gem::RemoteFetcher::FetchError => e
+        puts e.message
+        puts e.backtrace.inspect
+        sleep 5 #slow down a bit before retry
+      end
+    end
+    File.write(donefile, Time.now.to_s)
+  end # task gems
+  task "all" => "gems"
+end
diff --git a/rakelib/z_rubycheck.rake b/rakelib/z_rubycheck.rake
new file mode 100644
index 00000000000..0de9f9ded2e
--- /dev/null
+++ b/rakelib/z_rubycheck.rake
@@ -0,0 +1,12 @@
+if ENV['USE_RUBY'] != '1'
+  if RUBY_ENGINE != "jruby" or Gem.ruby !~ /vendor\/jruby\/bin\/jruby/
+    puts "Restarting myself under Vendored JRuby (currently #{RUBY_ENGINE} #{RUBY_VERSION})" 
+
+    # Make sure we have JRuby, then rerun ourselves under jruby.
+    Rake::Task["vendor:jruby"].invoke
+
+    jruby = File.join("vendor", "jruby", "bin", "jruby")
+    rake = File.join("vendor", "jruby", "bin", "rake")
+    exec(jruby, "-S", rake, *ARGV)
+  end
+end
diff --git a/require-analyze.rb b/require-analyze.rb
new file mode 100644
index 00000000000..f69d858aa45
--- /dev/null
+++ b/require-analyze.rb
@@ -0,0 +1,22 @@
+require "csv"
+
+#0.003,psych/nodes/mapping,/Users/jls/.rvm/rubies/jruby-1.7.8/lib/ruby/shared/psych/nodes.rb:6:in `(root)'
+
+durations = {}
+durations.default = 0
+
+CSV.foreach(ARGV[0]) do |duration, path, source|
+  source, line, where = source.split(":")
+  #{"0.002"=>"/Users/jls/projects/logstash/vendor/bundle/jruby/1.9/gems/clamp-0.6.3/lib/clamp.rb"}
+  if source.include?("jruby/1.9/gems")
+    # Get the gem name
+    source = source.gsub(/.*\/jruby\/1.9\/gems/, "")[/[^\/]+/]
+  elsif source.include?("/lib/logstash/")
+    source = source.gsub(/^.*(\/lib\/logstash\/)/, "/lib/logstash/")
+  end
+  durations[source] += duration.to_f
+end
+
+durations.sort_by { |k,v| v }.each do |k,v| 
+  puts "#{v} #{k}"
+end
diff --git a/screencast/000.intro b/screencast/000.intro
deleted file mode 100644
index f8c5d9fc2a6..00000000000
--- a/screencast/000.intro
+++ /dev/null
@@ -1,21 +0,0 @@
-%K Escape
-1GdGi
-tail -f is nice, but it doesn't scale.
-
-Plus, the output is just a stream of text. Aren't logs really messages?
-
-Enter logstash. 
-
-logstash gives you a pipe metaphor similar to the unix model.  Stuff goes in; stuff gets modified; stuff goes out. Think: sed.
-
-Powershell built on the unix pipe model by allowing you to pipe objects instead of just text. (If you haven't seen powershell yet, go check it out, it is awesome)
-
-Let's take that piped object model and apply it to logs, events, and the network.
-
-* Input from files, processes, etc. 
-* Parse it and package it into an object.
-* Ship it to anything willing to listen.
-
-If we provide a framework for doing this, you can easily ship logs to message queues, databases, archive servers, web browsers, etc. 
-
-Let's show a bit of logstash.
diff --git a/screencast/001.config b/screencast/001.config
deleted file mode 100644
index b2e25c9151d..00000000000
--- a/screencast/001.config
+++ /dev/null
@@ -1,37 +0,0 @@
-cd ~/projects/logstash
-%E rm ~/projects/logstash/etc/logstash-demo.yaml
-
-vi etc/logstash-demo.yaml
-:set paste
-
-%K control+l
-i
-# Remember that logstash provides a way to specify inputs, filters, and
-# outputs. For this demo, I'll just show inputs + outputs
----
-inputs:
-  # You can also tag inputs for easier handling later in your pipeline.
-  linux-syslog: # this is the 'linux-syslog' tag
-  - /var/log/messages # watch /var/log/messages (uses eventmachine-tail)
-  - /var/log/kern.log
-  - /var/log/auth.log
-  - /var/log/user.log
-  apache-access: # similar, different tag.
-  - /var/log/apache2/access.log
-  apache-error:
-  - /var/log/apache2/access.log
-  #other:
-  #- amqp://myamqpserver/fanout/rawlogs # an amqp fanout as input
-  #- amqp://myamqpserver/topic/rawlogs # an amqp topic as input
-  #- syslog:///  # take input via syslog protocol over the network
-outputs:
-  #- amqp://myamqpserver/topic/logs  # broadcast logs to an AMQP topic
-  #- mongodb://mongoserver/logs      # store events in mongodb
-  #- stdout:///                      # send to stdout (like tail -f, but better)
-  #- syslog://syslogserver/          # send to another syslog server
-  - websocket:///                    # send to websockets
-%E sleep 3
-
-%K Escape
-
-ZZ
diff --git a/screencast/002.webdemo b/screencast/002.webdemo
deleted file mode 100644
index ff712f66e39..00000000000
--- a/screencast/002.webdemo
+++ /dev/null
@@ -1,41 +0,0 @@
-%K control+a c
-%E sleep 2
-cd ~/projects/logstash
-export RUBYLIB=lib
-ruby bin/logstash -f etc/logstash-demo.yaml
-
-%E sleep 2
-# Now let's pop open google chrome (supports WebSockets) and watch
-# some logs...
-
-%E xdotool search --title " - Google Chrome" windowactivate --sync %@
-%K control+l BackSpace
-http://snack.home/~jls/ws
-
-%E logger -p 1 -t demo "This log is coming to you live."; sleep 2
-%E logger -p 1 -t demo "Any log being received on a logstash input can be viewed here, or stored in a database, or shipped elsewhere for processing."; sleep 2;
-%E logger -p 1 -t demo "Everything is piped input -> filter -> output."; sleep 2;
-%E logger -p 1 -t demo "The output of one can be the input of another. Chain by chain. "; sleep 2;
-%E logger -p 1 -t demo "The way you deal with logs is about to change."; sleep 2;
-
-%E xdotool search --onlyvisible gnome-terminal windowsize --usehints 70 7 windowactivate --sync windowmove 3000 0 
-%K control+minus
-%K control+a c
-%E sleep 2
-# Now we can watch logs in the browser...
-curl -o /dev/null http://snack.home/~jls/something
-!!
-!!
-!!
-
-logger -p 1 -t logging-example 'Hello world!'
-logger -p 1 -t logging-example "Welcome to logstash. $RANDOM"
-!!
-!!
-!!
-%E sleep 2
-
-
-# It's fast, too.
-seq 15 | xargs -n1 logger -p 1 -t fastlogs "real time feeds == :)"
-
diff --git a/screencast/README b/screencast/README
deleted file mode 100644
index e07052716e2..00000000000
--- a/screencast/README
+++ /dev/null
@@ -1,3 +0,0 @@
-The code here was used to automatically direct a screencast demonstrating logstash.
-
-The resulting video is here: http://www.youtube.com/watch?v=Fi7OaiNqPCc
diff --git a/screencast/run.rb b/screencast/run.rb
deleted file mode 100644
index 748c78cec03..00000000000
--- a/screencast/run.rb
+++ /dev/null
@@ -1,52 +0,0 @@
-#!/usr/bin/env ruby
-#
-
-require "rubygems"
-
-#if ENV["DISPLAY"] != ":1" 
-  #puts "$DISPLAY is wrong."
-  #exit 1
-#end
-
-def type(string)
-  system("xdotool", "type", "--clearmodifiers", "--delay", "100", string)
-  puts "Typing: #{string}"
-  #puts string.inspect
-  #$stdout.flush
-end
-
-def run(string)
-  command = string[3..-1].chomp
-  system(command)
-end
-
-def key(string)
-  keyseq = string[3..-1].chomp.split(/ +/)
-  system("xdotool", "key", "--clearmodifiers",  *keyseq)
-  puts keyseq.inspect
-  #puts string.inspect
-  #$stdout.flush
-end
-
-handlers = [
-  [/^[,]/m, proc { |s| type(s); sleep(0.4) } ], # comma
-  [/^[.;:?!]+/m, proc { |s| type(s); sleep(1) } ], # punctuation
-  [/^[\n]{2}/m, proc { |s| type(s); sleep(1) } ], # new paragraph
-  #[/^[\n](?! *[*-])/m, proc { |s| type(" ") } ], # continuation of a paragraph
-  #[/^[\n](?= *[*-])/m, proc { |s| type("\n") } ], # lists or other itemized things
-  [/^[\n]/m, proc { |s| type(s) } ], # lists or other itemized things
-  [/^%E[^\n]*\n/m, proc { |s| run(s) } ], # execute a command
-  [/^%K[^\n]*\n/m, proc { |s| key(s) } ], # type a specific keystroke
-  [/^[^,.;:?!\n]+/m, proc { |s| type(s) } ], # otherwise just type it
-] 
-
-data = $stdin.read
-while data.length > 0
-  match, func = handlers.collect { |re, f| [re.match(data), f] }\
-                        .select { |m,f| m.begin(0) == 0 rescue false }.first
-  str = match.to_s
-  func.call(str)
-  $stdout.flush
-  #sleep 3
-  data = data[match.end(0)..-1]
-end
diff --git a/spec/README.md b/spec/README.md
new file mode 100644
index 00000000000..2be4a4b1c2f
--- /dev/null
+++ b/spec/README.md
@@ -0,0 +1,14 @@
+# How to run these tests
+
+Run one:
+
+  `rspec spec/the/test.rb`
+
+Run them all:
+
+  `rspec spec/**/*.rb`
+
+Debug one test:
+
+  `LOGSTASH_DEBUG=y rspec spec/the/test.rb`
+
diff --git a/spec/codecs/collectd_spec.rb b/spec/codecs/collectd_spec.rb
new file mode 100644
index 00000000000..fb638a95090
--- /dev/null
+++ b/spec/codecs/collectd_spec.rb
@@ -0,0 +1,210 @@
+require "logstash/codecs/collectd"
+require "logstash/event"
+require "insist"
+require "tempfile"
+
+describe LogStash::Codecs::Collectd do
+  context "None" do
+    subject do
+      next LogStash::Codecs::Collectd.new({})
+    end
+
+    it "should parse a normal packet" do
+      payload = ["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"].pack('H*')
+
+      counter = 0
+      subject.decode(payload) do |event|
+        case counter
+        when 0
+          insist { event['host'] } == "lieters-klaptop.prot.plexis.eu"
+          insist { event['plugin'] } == "interface"
+          insist { event['plugin_instance'] } == "wlan0"
+          insist { event['collectd_type'] } == "if_errors"
+          insist { event['rx'] } == 0
+          insist { event['tx'] } == 0
+        when 2
+          insist { event['host'] } == "lieters-klaptop.prot.plexis.eu"
+          insist { event['plugin'] } == "entropy"
+          insist { event['collectd_type'] } == "entropy"
+          insist { event['value'] } == 157.0
+        end
+        counter += 1
+      end
+      insist { counter } == 28
+    end # it "should parse a normal packet"
+
+    it "should drop a part with an header length" do
+      payload = ["000000236c6965746572732d6b6c6170746f702e70726f742e706c657869732e6575000008000c14b0a645f3eb73c30009000c00000002800000000002000e696e74657266616365000003000a776c616e30000004000e69665f6572726f7273000006001800020202000000000000000000000000000000000008000c14b0a645f3eb525e000300076c6f000004000f69665f7061636b6574730000060018000202020000000000001cd80000000000001cd80008000c14b0a645f3ebf8c10002000c656e74726f70790000030005000004000c656e74726f7079000006000f0001010000000000a063400008000c14b0a645f3eb6c700002000e696e74657266616365000003000a776c616e30000004000f69665f7061636b657473000006001800020202000000000002d233000000000001c3b10008000c14b0a645f3eb59b1000300076c6f000004000e69665f6572726f7273000006001800020202000000000000000000000000000000000008000c14b0a645f425380b00020009737761700000030005000004000973776170000005000975736564000006000f00010100000000000000000008000c14b0a645f4254c8d0005000966726565000006000f00010100000000fcffdf410008000c14b0a645f4255ae70005000b636163686564000006000f00010100000000000000000008000c14b0a645f426f09f0004000c737761705f696f0000050007696e000006000f00010200000000000000000008000c14b0a645f42701e7000500086f7574000006000f00010200000000000000000008000c14b0a645f42a0edf0002000a7573657273000004000a75736572730000050005000006000f00010100000000000022400008000c14b0a645f5967c8b0002000e70726f636573736573000004000d70735f7374617465000005000c72756e6e696e67000006000f00010100000000000000000008000c14b0a645f624706c0005000d736c656570696e67000006000f0001010000000000c067400008000c14b0a645f624861a0005000c7a6f6d62696573000006000f00010100000000000000000008000c14b0a645f62494740005000c73746f70706564000006000f00010100000000000010400008000c14b0a645f6254aa90005000b706167696e67000006000f00010100000000000000000008000c14b0a645f6255b110005000c626c6f636b6564000006000f00010100000000000000000008000c14b0a645f62763060004000e666f726b5f726174650000050005000006000f00010200000000000025390008000c14b0a64873bf8f47000200086370750000030006300000040008637075000005000975736572000006000f0001020000000000023caa0008000c14b0a64873bfc9dd000500096e696365000006000f00010200000000000000030008000c14b0a64873bfe9350005000b73797374656d000006000f00010200000000000078bc0008000c14b0a64873c004290005000969646c65000006000f00010200000000000941fe0008000c14b0a64873c020920005000977616974000006000f00010200000000000002050008000c14b0a64873c03e280005000e696e74657272757074000006000f00010200000000000000140008000c14b0a64873c04ba20005000c736f6674697271000006000f00010200000000000001890008000c14b0a64873c058860005000a737465616c000006000f00010200000000000000000008000c14b0a64873c071b80003000631000005000975736572000006000f000102000000000002440e0008000c14b0a64873c07f31000500316e696365000006000f0001020000000000000007"].pack('H*')
+      counter = 0
+      subject.decode(payload) do |event|
+        case counter
+        when 0
+          insist { event['host'] } == "lieters-klaptop.prot.plexis.eu"
+          insist { event['plugin'] } == "interface"
+          insist { event['plugin_instance'] } == "wlan0"
+          insist { event['collectd_type'] } == "if_errors"
+          insist { event['rx'] } == 0
+          insist { event['tx'] } == 0
+        when 2
+          insist { event['host'] } == "lieters-klaptop.prot.plexis.eu"
+          insist { event['plugin'] } == "entropy"
+          insist { event['collectd_type'] } == "entropy"
+          insist { event['value'] } == 157.0
+        end
+        counter += 1
+      end
+      # One of these will fail because I altered the payload from the normal packet
+      insist { counter } == 27
+    end # it "should drop a part with an header length"
+
+    # This payload contains a NaN value
+    it "should replace a NaN with a zero and add tag '_collectdNaN' by default" do
+      payload = ["00000015746573742e6578616d706c652e636f6d000008000c14dc4c81831ef78b0009000c00000000400000000002000970696e67000004000970696e67000005001c70696e672d7461726765742e6578616d706c652e636f6d000006000f000101000000000000f87f"].pack('H*')
+      counter = 0
+      subject.decode(payload) do |event|
+        case counter
+        when 0
+          insist { event['host'] } == "test.example.com"
+          insist { event['plugin'] } == "ping"
+          insist { event['type_instance'] } == "ping-target.example.com"
+          insist { event['collectd_type'] } == "ping"
+          insist { event['value'] } == 0   # Not a NaN
+          insist { event['tags'] } == ["_collectdNaN"]
+        end
+        counter += 1
+      end
+      insist { counter } == 1
+    end # it "should replace a NaN with a zero and add tag '_collectdNaN' by default"
+  end # context "None"
+
+  context "Replace nan_value and nan_tag with non-default values" do
+    subject do
+      next LogStash::Codecs::Collectd.new({"nan_value" => 1,
+                                           "nan_tag" => "NaN_encountered"})
+    end
+    # This payload contains a NaN value
+    it "should replace a NaN with the specified value and tag 'NaN_encountered'" do
+      payload = ["00000015746573742e6578616d706c652e636f6d000008000c14dc4c81831ef78b0009000c00000000400000000002000970696e67000004000970696e67000005001c70696e672d7461726765742e6578616d706c652e636f6d000006000f000101000000000000f87f"].pack('H*')
+      counter = 0
+      subject.decode(payload) do |event|
+        case counter
+        when 0
+          insist { event['host'] } == "test.example.com"
+          insist { event['plugin'] } == "ping"
+          insist { event['type_instance'] } == "ping-target.example.com"
+          insist { event['collectd_type'] } == "ping"
+          insist { event['value'] } == 1   # Not a NaN
+          insist { event['tags'] } == ["NaN_encountered"]
+        end
+        counter += 1
+      end
+      insist { counter } == 1
+    end # it "should replace a NaN with the specified value and tag 'NaN_encountered'"
+  end # context "Replace nan_value and nan_tag with non-default values"
+
+  context "Warn on NaN event" do
+    subject do
+      next LogStash::Codecs::Collectd.new({"nan_handling" => "warn"})
+    end
+    # This payload contains a NaN value
+    it "should replace a NaN with a zero and receive a warning when 'nan_handling' set to warn" do
+      payload = ["00000015746573742e6578616d706c652e636f6d000008000c14dc4c81831ef78b0009000c00000000400000000002000970696e67000004000970696e67000005001c70696e672d7461726765742e6578616d706c652e636f6d000006000f000101000000000000f87f"].pack('H*')
+      counter = 0
+      subject.logger.should_receive(:warn).with("NaN replaced by 0")
+      subject.decode(payload) do |event|
+        case counter
+        when 0
+          insist { event['host'] } == "test.example.com"
+          insist { event['plugin'] } == "ping"
+          insist { event['type_instance'] } == "ping-target.example.com"
+          insist { event['collectd_type'] } == "ping"
+          insist { event['value'] } == 0   # Not a NaN
+        end
+        counter += 1
+      end
+      insist { counter } == 1
+    end # it "should replace a NaN with a zero and receive a warning when 'nan_handling' set to warn"
+  end # context "Warn on NaN event"
+
+  context "Drop NaN event" do
+    subject do
+      next LogStash::Codecs::Collectd.new({"nan_handling" => "drop"})
+    end
+    # This payload contains a NaN value
+    it "should drop an event with a NaN value when 'nan_handling' set to drop" do
+      payload = ["00000015746573742e6578616d706c652e636f6d000008000c14dc4c81831ef78b0009000c00000000400000000002000970696e67000004000970696e67000005001c70696e672d7461726765742e6578616d706c652e636f6d000006000f000101000000000000f87f"].pack('H*')
+      counter = 0
+      subject.decode(payload) do |event|
+        case counter
+        when 0
+          insist { event['host'] } == "test.example.com"
+          insist { event['plugin'] } == "ping"
+          insist { event['type_instance'] } == "ping-target.example.com"
+          insist { event['collectd_type'] } == "ping"
+          insist { event['value'] } == NaN   # NaN
+        end
+        counter += 1 # Because we're dropping this, it should not increment
+      end
+      insist { counter } == 0 # We expect no increment
+    end # it "should drop an event with a NaN value when 'nan_handling' set to drop"
+  end # context "Drop NaN event"
+
+  # Create an authfile for the next tests
+  authfile = Tempfile.new('logstash-collectd-authfile')
+  File.open(authfile.path, "a") do |fd|
+    fd.puts("pieter: aapje1234")
+  end
+  context "Sign" do
+    subject do
+      next LogStash::Codecs::Collectd.new({"authfile" => authfile.path,
+                                           "security_level" => "Sign"})
+    end
+
+    it "should parse a correctly signed packet" do
+      payload = ["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"].pack('H*')
+      counter = 0
+      subject.decode(payload) do |event|
+        counter += 1
+      end
+
+      insist { counter } == 24
+    end # it "should parse a correctly signed packet"
+
+    it "should not parse an incorrectly signed packet" do
+      payload = ["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"].pack('H*')
+      counter = 0
+      subject.decode(payload) do |event|
+        counter += 1
+      end
+
+      insist { counter } == 0
+    end # it "should not parse and incorrectly signed packet"
+  end # context "Sign"
+
+  context "Encrypt" do
+    subject do
+      next LogStash::Codecs::Collectd.new({"authfile" => authfile.path,
+                                           "security_level" => "Encrypt"})
+    end
+
+    it "should parse an encrypted packet", :export_cypher => true do
+      payload = ["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"].pack('H*')
+      counter = 0
+      subject.decode(payload) do |event|
+        counter += 1
+      end
+
+      insist { counter } == 24
+    end # it "should parse an encrypted packet"
+
+    it "should not parse unencrypted packets when encrypt is configured" do
+      payload = ["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"].pack('H*')
+      counter = 0
+      subject.decode(payload) do |event|
+        counter += 1
+      end
+
+      insist { counter } == 0
+    end # it "should not parse unencrypted packets when encrypt is configured"
+  end # context "Encrypt"
+end # describe LogStash::Codecs::Collectd
diff --git a/spec/codecs/edn_lines_spec.rb b/spec/codecs/edn_lines_spec.rb
new file mode 100644
index 00000000000..79a25ba84a2
--- /dev/null
+++ b/spec/codecs/edn_lines_spec.rb
@@ -0,0 +1,73 @@
+require "logstash/codecs/edn_lines"
+require "logstash/event"
+require "logstash/json"
+require "insist"
+require "edn"
+
+describe LogStash::Codecs::EDNLines do
+  subject do
+    next LogStash::Codecs::EDNLines.new
+  end
+
+  context "#decode" do
+    it "should return an event from edn data" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a", "b", "c"]}, "@timestamp" => "2014-05-30T02:52:17.929Z"}
+      subject.decode(data.to_edn + "\n") do |event|
+        insist { event }.is_a?(LogStash::Event)
+        insist { event["foo"] } == data["foo"]
+        insist { event["baz"] } == data["baz"]
+        insist { event["bah"] } == data["bah"]
+        insist { event["@timestamp"].to_iso8601 } == data["@timestamp"]
+      end
+    end
+
+    it "should return an event from edn data when a newline is recieved" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}, "@timestamp" => "2014-05-30T02:52:17.929Z"}
+      subject.decode(data.to_edn) do |event|
+        insist {false}
+      end
+      subject.decode("\n") do |event|
+        insist { event.is_a? LogStash::Event }
+        insist { event["foo"] } == data["foo"]
+        insist { event["baz"] } == data["baz"]
+        insist { event["bah"] } == data["bah"]
+        insist { event["@timestamp"].to_iso8601 } == data["@timestamp"]
+      end
+    end
+  end
+
+  context "#encode" do
+    it "should return edn data from pure ruby hash" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}, "@timestamp" => "2014-05-30T02:52:17.929Z"}
+      event = LogStash::Event.new(data)
+      got_event = false
+      subject.on_event do |d|
+        insist { EDN.read(d)["foo"] } == data["foo"]
+        insist { EDN.read(d)["baz"] } == data["baz"]
+        insist { EDN.read(d)["bah"] } == data["bah"]
+        insist { EDN.read(d)["@timestamp"] } == "2014-05-30T02:52:17.929Z"
+        insist { EDN.read(d)["@timestamp"] } == event["@timestamp"].to_iso8601
+       got_event = true
+      end
+      subject.encode(event)
+      insist { got_event }
+    end
+
+    it "should return edn data rom deserialized json with normalization" do
+      data = LogStash::Json.load('{"foo": "bar", "baz": {"bah": ["a","b","c"]}, "@timestamp": "2014-05-30T02:52:17.929Z"}')
+      event = LogStash::Event.new(data)
+      got_event = false
+      subject.on_event do |d|
+        insist { EDN.read(d)["foo"] } == data["foo"]
+        insist { EDN.read(d)["baz"] } == data["baz"]
+        insist { EDN.read(d)["bah"] } == data["bah"]
+        insist { EDN.read(d)["@timestamp"] } == "2014-05-30T02:52:17.929Z"
+        insist { EDN.read(d)["@timestamp"] } == event["@timestamp"].to_iso8601
+       got_event = true
+      end
+      subject.encode(event)
+      insist { got_event }
+    end
+  end
+
+end
diff --git a/spec/codecs/edn_spec.rb b/spec/codecs/edn_spec.rb
new file mode 100644
index 00000000000..5fa49e58151
--- /dev/null
+++ b/spec/codecs/edn_spec.rb
@@ -0,0 +1,61 @@
+require "logstash/codecs/edn"
+require "logstash/event"
+require "logstash/json"
+require "insist"
+require "edn"
+
+describe LogStash::Codecs::EDN do
+  subject do
+    next LogStash::Codecs::EDN.new
+  end
+
+  context "#decode" do
+    it "should return an event from edn data" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a", "b", "c"]}, "@timestamp" => "2014-05-30T02:52:17.929Z"}
+      subject.decode(data.to_edn) do |event|
+        insist { event }.is_a?(LogStash::Event)
+        insist { event["foo"] } == data["foo"]
+        insist { event["baz"] } == data["baz"]
+        insist { event["bah"] } == data["bah"]
+        insist { event["@timestamp"].to_iso8601 } == data["@timestamp"]
+      end
+    end
+  end
+
+  context "#encode" do
+    it "should return edn data from pure ruby hash" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}, "@timestamp" => "2014-05-30T02:52:17.929Z"}
+      event = LogStash::Event.new(data)
+      got_event = false
+      subject.on_event do |d|
+        insist { EDN.read(d)["foo"] } == data["foo"]
+        insist { EDN.read(d)["baz"] } == data["baz"]
+        insist { EDN.read(d)["bah"] } == data["bah"]
+        insist { EDN.read(d)["@timestamp"] } == "2014-05-30T02:52:17.929Z"
+        got_event = true
+      end
+      subject.encode(event)
+      insist { got_event }
+    end
+
+    # this is to test the case where the event data has been produced by json
+    # deserialization using JrJackson in :raw mode which creates Java LinkedHashMap
+    # and not Ruby Hash which will not be monkey patched with the #to_edn method
+    it "should return edn data from deserialized json with normalization" do
+      data = LogStash::Json.load('{"foo": "bar", "baz": {"bah": ["a","b","c"]}, "@timestamp": "2014-05-30T02:52:17.929Z"}')
+      event = LogStash::Event.new(data)
+      got_event = false
+      subject.on_event do |d|
+        insist { EDN.read(d)["foo"] } == data["foo"]
+        insist { EDN.read(d)["baz"] } == data["baz"]
+        insist { EDN.read(d)["bah"] } == data["bah"]
+        insist { EDN.read(d)["@timestamp"] } == "2014-05-30T02:52:17.929Z"
+        insist { EDN.read(d)["@timestamp"] } == event["@timestamp"].to_iso8601
+        got_event = true
+      end
+      subject.encode(event)
+      insist { got_event }
+    end
+  end
+
+end
diff --git a/spec/codecs/graphite_spec.rb b/spec/codecs/graphite_spec.rb
new file mode 100644
index 00000000000..8be9ef0af00
--- /dev/null
+++ b/spec/codecs/graphite_spec.rb
@@ -0,0 +1,96 @@
+require "logstash/codecs/graphite"
+require "logstash/event"
+require "insist"
+
+describe LogStash::Codecs::Graphite do
+  subject do
+    next LogStash::Codecs::Graphite.new
+  end
+
+  context "#decode" do
+    it "should return an event from single full graphite line" do
+      name = Random.srand.to_s(36)
+      value = Random.rand*1000
+      timestamp = Time.now.gmtime.to_i
+      subject.decode("#{name} #{value} #{timestamp}\n") do |event|
+        insist { event.is_a? LogStash::Event }
+        insist { event[name] } == value
+      end
+    end
+
+    it "should return multiple events given multiple graphite formated lines" do
+      total_count = Random.rand(20)
+      names = Array.new(total_count) { Random.srand.to_s(36) }
+      values = Array.new(total_count) { Random.rand*1000 }
+      timestamps = Array.new(total_count) { Time.now.gmtime.to_i }
+      data = Array.new(total_count) {|i| "#{names[i]} #{values[i]} #{timestamps[i]}\n"}
+      counter = 0
+      subject.decode(data.join('')) do |event|
+        insist { event.is_a? LogStash::Event }
+        insist { event[names[counter]] } == values[counter]
+        counter = counter+1
+      end
+      insist { counter } == total_count
+    end
+
+    it "should not return an event until newline is hit" do
+      name = Random.srand.to_s(36)
+      value = Random.rand*1000
+      timestamp = Time.now.gmtime.to_i
+      event_returned = false
+      subject.decode("#{name} #{value} #{timestamp}") do |event|
+        event_returned = true
+      end
+      insist { !event_returned }
+      subject.decode("\n") do |event|
+        insist { event.is_a? LogStash::Event }
+        insist { event[name] } == value
+        event_returned = true
+      end
+      insist { event_returned }
+    end
+  end
+
+  context "#encode" do
+    it "should emit an graphite formatted line" do
+      name = Random.srand.to_s(36)
+      value = Random.rand*1000
+      timestamp = Time.now.gmtime
+      subject.metrics = {name => value}
+      subject.on_event do |event|
+        insist { event.is_a? String }
+        insist { event } == "#{name} #{value} #{timestamp.to_i}\n"
+      end
+      subject.encode(LogStash::Event.new("@timestamp" => timestamp))
+    end
+
+    it "should treat fields as metrics if fields as metrics flag is set" do
+      name = Random.srand.to_s(36)
+      value = Random.rand*1000
+      timestamp = Time.now.gmtime
+      subject.fields_are_metrics = true
+      subject.on_event do |event|
+        insist { event.is_a? String }
+        insist { event } == "#{name} #{value} #{timestamp.to_i}\n"
+      end
+      subject.encode(LogStash::Event.new({name => value, "@timestamp" => timestamp}))
+
+      #even if metrics param is set
+      subject.metrics = {"foo" => 4}
+      subject.encode(LogStash::Event.new({name => value, "@timestamp" => timestamp}))
+    end
+
+    it "should change the metric name format when metrics_format is set" do
+      name = Random.srand.to_s(36)
+      value = Random.rand*1000
+      timestamp = Time.now.gmtime
+      subject.metrics = {name => value}
+      subject.metrics_format = "foo.bar.*.baz"
+      subject.on_event do |event|
+        insist { event.is_a? String }
+        insist { event } == "foo.bar.#{name}.baz #{value} #{timestamp.to_i}\n"
+      end
+      subject.encode(LogStash::Event.new("@timestamp" => timestamp))
+    end
+  end
+end
diff --git a/spec/codecs/json_lines_spec.rb b/spec/codecs/json_lines_spec.rb
new file mode 100644
index 00000000000..630e3fa7b6b
--- /dev/null
+++ b/spec/codecs/json_lines_spec.rb
@@ -0,0 +1,78 @@
+require "logstash/codecs/json_lines"
+require "logstash/event"
+require "logstash/json"
+require "insist"
+
+describe LogStash::Codecs::JSONLines do
+  subject do
+    next LogStash::Codecs::JSONLines.new
+  end
+
+  context "#decode" do
+    it "should return an event from json data" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
+      subject.decode(LogStash::Json.dump(data) + "\n") do |event|
+        insist { event.is_a? LogStash::Event }
+        insist { event["foo"] } == data["foo"]
+        insist { event["baz"] } == data["baz"]
+        insist { event["bah"] } == data["bah"]
+      end
+    end
+
+    it "should return an event from json data when a newline is recieved" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
+      subject.decode(LogStash::Json.dump(data)) do |event|
+        insist {false}
+      end
+      subject.decode("\n") do |event|
+        insist { event.is_a? LogStash::Event }
+        insist { event["foo"] } == data["foo"]
+        insist { event["baz"] } == data["baz"]
+        insist { event["bah"] } == data["bah"]
+      end
+    end
+
+    context "processing plain text" do
+      it "falls back to plain text" do
+        decoded = false
+        subject.decode("something that isn't json\n") do |event|
+          decoded = true
+          insist { event.is_a?(LogStash::Event) }
+          insist { event["message"] } == "something that isn't json"
+        end
+        insist { decoded } == true
+      end
+    end
+
+    context "processing weird binary blobs" do
+      it "falls back to plain text and doesn't crash (LOGSTASH-1595)" do
+        decoded = false
+        blob = (128..255).to_a.pack("C*").force_encoding("ASCII-8BIT")
+        subject.decode(blob)
+        subject.decode("\n") do |event|
+          decoded = true
+          insist { event.is_a?(LogStash::Event) }
+          insist { event["message"].encoding.to_s } == "UTF-8"
+        end
+        insist { decoded } == true
+      end
+    end
+  end
+
+  context "#encode" do
+    it "should return json data" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
+      event = LogStash::Event.new(data)
+      got_event = false
+      subject.on_event do |d|
+        insist { d } == "#{LogStash::Event.new(data).to_json}\n"
+        insist { LogStash::Json.load(d)["foo"] } == data["foo"]
+        insist { LogStash::Json.load(d)["baz"] } == data["baz"]
+        insist { LogStash::Json.load(d)["bah"] } == data["bah"]
+        got_event = true
+      end
+      subject.encode(event)
+      insist { got_event }
+    end
+  end
+end
diff --git a/spec/codecs/json_spec.rb b/spec/codecs/json_spec.rb
new file mode 100644
index 00000000000..4cb128534c8
--- /dev/null
+++ b/spec/codecs/json_spec.rb
@@ -0,0 +1,83 @@
+require "logstash/codecs/json"
+require "logstash/event"
+require "logstash/json"
+require "insist"
+
+describe LogStash::Codecs::JSON do
+  subject do
+    next LogStash::Codecs::JSON.new
+  end
+
+  context "#decode" do
+    it "should return an event from json data" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
+      subject.decode(LogStash::Json.dump(data)) do |event|
+        insist { event.is_a? LogStash::Event }
+        insist { event["foo"] } == data["foo"]
+        insist { event["baz"] } == data["baz"]
+        insist { event["bah"] } == data["bah"]
+      end
+    end
+
+    it "should be fast", :performance => true do
+      json = '{"message":"Hello world!","@timestamp":"2013-12-21T07:01:25.616Z","@version":"1","host":"Macintosh.local","sequence":1572456}'
+      iterations = 500000
+      count = 0
+
+      # Warmup
+      10000.times { subject.decode(json) { } }
+
+      start = Time.now
+      iterations.times do
+        subject.decode(json) do |event|
+          count += 1
+        end
+      end
+      duration = Time.now - start
+      insist { count } == iterations
+      puts "codecs/json rate: #{"%02.0f/sec" % (iterations / duration)}, elapsed: #{duration}s"
+    end
+
+    context "processing plain text" do
+      it "falls back to plain text" do
+        decoded = false
+        subject.decode("something that isn't json") do |event|
+          decoded = true
+          insist { event.is_a?(LogStash::Event) }
+          insist { event["message"] } == "something that isn't json"
+        end
+        insist { decoded } == true
+      end
+    end
+
+    context "processing weird binary blobs" do
+      it "falls back to plain text and doesn't crash (LOGSTASH-1595)" do
+        decoded = false
+        blob = (128..255).to_a.pack("C*").force_encoding("ASCII-8BIT")
+        subject.decode(blob) do |event|
+          decoded = true
+          insist { event.is_a?(LogStash::Event) }
+          insist { event["message"].encoding.to_s } == "UTF-8"
+        end
+        insist { decoded } == true
+      end
+    end
+  end
+
+  context "#encode" do
+    it "should return json data" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
+      event = LogStash::Event.new(data)
+      got_event = false
+      subject.on_event do |d|
+        insist { d.chomp } == LogStash::Event.new(data).to_json
+        insist { LogStash::Json.load(d)["foo"] } == data["foo"]
+        insist { LogStash::Json.load(d)["baz"] } == data["baz"]
+        insist { LogStash::Json.load(d)["bah"] } == data["bah"]
+        got_event = true
+      end
+      subject.encode(event)
+      insist { got_event }
+    end
+  end
+end
diff --git a/spec/codecs/json_spooler_spec.rb b/spec/codecs/json_spooler_spec.rb
new file mode 100644
index 00000000000..20aef79b596
--- /dev/null
+++ b/spec/codecs/json_spooler_spec.rb
@@ -0,0 +1,47 @@
+require "logstash/codecs/json_spooler"
+require "logstash/event"
+require "logstash/json"
+require "insist"
+
+describe LogStash::Codecs::JsonSpooler do
+  subject do
+    # mute deprecation message
+    expect_any_instance_of(LogStash::Codecs::JsonSpooler).to receive(:register).and_return(nil)
+
+    LogStash::Codecs::JsonSpooler.new
+  end
+
+  context "#decode" do
+    it "should return an event from spooled json data" do
+      data = {"a" => 1}
+      events = [LogStash::Event.new(data), LogStash::Event.new(data),
+        LogStash::Event.new(data)]
+      subject.decode(LogStash::Json.dump(events)) do |event|
+        insist { event.is_a? LogStash::Event }
+        insist { event["a"] } == data["a"]
+      end
+    end
+  end
+
+  context "#encode" do
+    it "should return spooled json data" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
+      subject.spool_size = 3
+      got_event = false
+      subject.on_event do |d|
+        events = LogStash::Json.load(d)
+        insist { events.is_a? Array }
+        insist { events[0].is_a? LogStash::Event }
+        insist { events[0]["foo"] } == data["foo"]
+        insist { events[0]["baz"] } == data["baz"]
+        insist { events[0]["bah"] } == data["bah"]
+        insist { events.length } == 3
+        got_event = true
+      end
+      3.times do
+        subject.encode(LogStash::Event.new(data))
+      end
+      insist { got_event }
+    end
+  end
+end
diff --git a/spec/codecs/line_spec.rb b/spec/codecs/line_spec.rb
new file mode 100644
index 00000000000..9f3a18532af
--- /dev/null
+++ b/spec/codecs/line_spec.rb
@@ -0,0 +1,66 @@
+# encoding: utf-8
+
+require "logstash/codecs/line"
+require "logstash/event"
+
+describe LogStash::Codecs::Line do
+  subject do
+    next LogStash::Codecs::Line.new
+  end
+
+  context "#encode" do
+    let (:event) {LogStash::Event.new({"message" => "hello world", "host" => "test"})}
+
+    it "should return a default date formatted line" do
+      expect(subject).to receive(:on_event).once.and_call_original
+      subject.on_event do |d|
+        insist {d} == event.to_s + "\n"
+      end
+      subject.encode(event)
+    end
+
+    it "should respect the supplied format" do
+      format = "%{host}"
+      subject.format = format
+      expect(subject).to receive(:on_event).once.and_call_original
+      subject.on_event do |d|
+        insist {d} == event.sprintf(format) + "\n"
+      end
+      subject.encode(event)
+    end
+  end
+
+  context "#decode" do
+    it "should return an event from an ascii string" do
+      decoded = false
+      subject.decode("hello world\n") do |e|
+        decoded = true
+        insist { e.is_a?(LogStash::Event) }
+        insist { e["message"] } == "hello world"
+      end
+      insist { decoded } == true
+    end
+
+    it "should return an event from a valid utf-8 string" do
+      subject.decode("M√ºnchen\n") do |e|
+        insist { e.is_a?(LogStash::Event) }
+        insist { e["message"] } == "M√ºnchen"
+      end
+    end
+  end
+
+  context "#flush" do
+    it "should convert charsets" do
+      garbage = [0xD0].pack("C")
+      subject.decode(garbage) do |e|
+        fail "Should not get here."
+      end
+      count = 0
+      subject.flush do |event|
+        count += 1
+        insist { event["message"].encoding } == Encoding::UTF_8
+      end
+      insist { count } == 1
+    end
+  end
+end
diff --git a/spec/codecs/msgpack_spec.rb b/spec/codecs/msgpack_spec.rb
new file mode 100644
index 00000000000..ba0c451bd14
--- /dev/null
+++ b/spec/codecs/msgpack_spec.rb
@@ -0,0 +1,57 @@
+require "logstash/codecs/msgpack"
+require "logstash/event"
+require "insist"
+
+describe LogStash::Codecs::Msgpack do
+  subject do
+    next LogStash::Codecs::Msgpack.new
+  end
+
+  context "#decode" do
+    it "should return an event from msgpack data" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}, "@timestamp" => "2014-05-30T02:52:17.929Z"}
+      subject.decode(MessagePack.pack(data)) do |event|
+        insist { event.is_a? LogStash::Event }
+        insist { event["foo"] } == data["foo"]
+        insist { event["baz"] } == data["baz"]
+        insist { event["bah"] } == data["bah"]
+        insist { event["@timestamp"].to_iso8601 } == data["@timestamp"]
+      end
+    end
+  end
+
+  context "#encode" do
+    it "should return msgpack data from pure ruby hash" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}, "@timestamp" => "2014-05-30T02:52:17.929Z"}
+      event = LogStash::Event.new(data)
+      got_event = false
+      subject.on_event do |d|
+        insist { MessagePack.unpack(d)["foo"] } == data["foo"]
+        insist { MessagePack.unpack(d)["baz"] } == data["baz"]
+        insist { MessagePack.unpack(d)["bah"] } == data["bah"]
+        insist { MessagePack.unpack(d)["@timestamp"] } == "2014-05-30T02:52:17.929Z"
+        insist { MessagePack.unpack(d)["@timestamp"] } == event["@timestamp"].to_iso8601
+        got_event = true
+      end
+      subject.encode(event)
+      insist { got_event }
+    end
+
+    it "should return msgpack data from deserialized json with normalization" do
+      data = LogStash::Json.load('{"foo": "bar", "baz": {"bah": ["a","b","c"]}, "@timestamp": "2014-05-30T02:52:17.929Z"}')
+      event = LogStash::Event.new(data)
+      got_event = false
+      subject.on_event do |d|
+        insist { MessagePack.unpack(d)["foo"] } == data["foo"]
+        insist { MessagePack.unpack(d)["baz"] } == data["baz"]
+        insist { MessagePack.unpack(d)["bah"] } == data["bah"]
+        insist { MessagePack.unpack(d)["@timestamp"] } == "2014-05-30T02:52:17.929Z"
+        insist { MessagePack.unpack(d)["@timestamp"] } == event["@timestamp"].to_iso8601
+        got_event = true
+      end
+      subject.encode(event)
+      insist { got_event }
+    end
+  end
+
+end
diff --git a/spec/codecs/multiline_spec.rb b/spec/codecs/multiline_spec.rb
new file mode 100644
index 00000000000..2c75317f21c
--- /dev/null
+++ b/spec/codecs/multiline_spec.rb
@@ -0,0 +1,160 @@
+# encoding: utf-8
+
+require "logstash/codecs/multiline"
+require "logstash/event"
+require "insist"
+
+describe LogStash::Codecs::Multiline do
+  context "#decode" do
+    it "should be able to handle multiline events with additional lines space-indented" do
+      codec = LogStash::Codecs::Multiline.new("pattern" => "^\\s", "what" => "previous")
+      lines = [ "hello world", "   second line", "another first line" ]
+      events = []
+      lines.each do |line|
+        codec.decode(line) do |event|
+          events << event
+        end
+      end
+      codec.flush { |e| events << e }
+      insist { events.size } == 2
+      insist { events[0]["message"] } == "hello world\n   second line"
+      insist { events[0]["tags"] }.include?("multiline")
+      insist { events[1]["message"] } == "another first line"
+      insist { events[1]["tags"] }.nil?
+    end
+
+    it "should allow custom tag added to multiline events" do
+      codec = LogStash::Codecs::Multiline.new("pattern" => "^\\s", "what" => "previous", "multiline_tag" => "hurray" )
+      lines = [ "hello world", "   second line", "another first line" ]
+      events = []
+      lines.each do |line|
+        codec.decode(line) do |event|
+          events << event
+        end
+      end
+      codec.flush { |e| events << e }
+      insist { events.size } == 2
+      insist { events[0]["tags"] }.include?("hurray")
+      insist { events[1]["tags"] }.nil?
+    end
+
+    it "should allow grok patterns to be used" do
+      codec = LogStash::Codecs::Multiline.new(
+        "pattern" => "^%{NUMBER} %{TIME}",
+        "negate" => true,
+        "what" => "previous"
+      )
+
+      lines = [ "120913 12:04:33 first line", "second line", "third line" ]
+
+      events = []
+      lines.each do |line|
+        codec.decode(line) do |event|
+          events << event
+        end
+      end
+      codec.flush { |e| events << e }
+
+      insist { events.size } == 1
+      insist { events.first["message"] } == lines.join("\n")
+    end
+
+
+    context "using default UTF-8 charset" do
+
+      it "should decode valid UTF-8 input" do
+        codec = LogStash::Codecs::Multiline.new("pattern" => "^\\s", "what" => "previous")
+        lines = [ "foobar", "Œ∫·ΩπœÉŒºŒµ" ]
+        events = []
+        lines.each do |line|
+          insist { line.encoding.name } == "UTF-8"
+          insist { line.valid_encoding? } == true
+
+          codec.decode(line) { |event| events << event }
+        end
+        codec.flush { |e| events << e }
+        insist { events.size } == 2
+
+        events.zip(lines).each do |tuple|
+          insist { tuple[0]["message"] } == tuple[1]
+          insist { tuple[0]["message"].encoding.name } == "UTF-8"
+        end
+      end
+
+      it "should escape invalid sequences" do
+        codec = LogStash::Codecs::Multiline.new("pattern" => "^\\s", "what" => "previous")
+        lines = [ "foo \xED\xB9\x81\xC3", "bar \xAD" ]
+        events = []
+        lines.each do |line|
+          insist { line.encoding.name } == "UTF-8"
+          insist { line.valid_encoding? } == false
+
+          codec.decode(line) { |event| events << event }
+        end
+        codec.flush { |e| events << e }
+        insist { events.size } == 2
+
+        events.zip(lines).each do |tuple|
+          insist { tuple[0]["message"] } == tuple[1].inspect[1..-2]
+          insist { tuple[0]["message"].encoding.name } == "UTF-8"
+        end
+      end
+    end
+
+
+    context "with valid non UTF-8 source encoding" do
+
+      it "should encode to UTF-8" do
+        codec = LogStash::Codecs::Multiline.new("charset" => "ISO-8859-1", "pattern" => "^\\s", "what" => "previous")
+        samples = [
+          ["foobar", "foobar"],
+          ["\xE0 Montr\xE9al", "√† Montr√©al"],
+        ]
+
+        # lines = [ "foo \xED\xB9\x81\xC3", "bar \xAD" ]
+        events = []
+        samples.map{|(a, b)| a.force_encoding("ISO-8859-1")}.each do |line|
+          insist { line.encoding.name } == "ISO-8859-1"
+          insist { line.valid_encoding? } == true
+
+          codec.decode(line) { |event| events << event }
+        end
+        codec.flush { |e| events << e }
+        insist { events.size } == 2
+
+        events.zip(samples.map{|(a, b)| b}).each do |tuple|
+          insist { tuple[1].encoding.name } == "UTF-8"
+          insist { tuple[0]["message"] } == tuple[1]
+          insist { tuple[0]["message"].encoding.name } == "UTF-8"
+        end
+      end
+    end
+
+    context "with invalid non UTF-8 source encoding" do
+
+     it "should encode to UTF-8" do
+        codec = LogStash::Codecs::Multiline.new("charset" => "ASCII-8BIT", "pattern" => "^\\s", "what" => "previous")
+        samples = [
+          ["\xE0 Montr\xE9al", "ÔøΩ MontrÔøΩal"],
+          ["\xCE\xBA\xCF\x8C\xCF\x83\xCE\xBC\xCE\xB5", "ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ"],
+        ]
+        events = []
+        samples.map{|(a, b)| a.force_encoding("ASCII-8BIT")}.each do |line|
+          insist { line.encoding.name } == "ASCII-8BIT"
+          insist { line.valid_encoding? } == true
+
+          codec.decode(line) { |event| events << event }
+        end
+        codec.flush { |e| events << e }
+        insist { events.size } == 2
+
+        events.zip(samples.map{|(a, b)| b}).each do |tuple|
+          insist { tuple[1].encoding.name } == "UTF-8"
+          insist { tuple[0]["message"] } == tuple[1]
+          insist { tuple[0]["message"].encoding.name } == "UTF-8"
+        end
+      end
+
+    end
+  end
+end
diff --git a/spec/codecs/oldlogstashjson_spec.rb b/spec/codecs/oldlogstashjson_spec.rb
new file mode 100644
index 00000000000..3bb037b1a3b
--- /dev/null
+++ b/spec/codecs/oldlogstashjson_spec.rb
@@ -0,0 +1,56 @@
+require "logstash/codecs/oldlogstashjson"
+require "logstash/event"
+require "logstash/json"
+require "insist"
+
+describe LogStash::Codecs::OldLogStashJSON do
+  subject do
+    next LogStash::Codecs::OldLogStashJSON.new
+  end
+
+  context "#decode" do
+    it "should return a new (v1) event from old (v0) json data" do
+      data = {"@message" => "bar", "@source_host" => "localhost",
+              "@tags" => ["a","b","c"]}
+      subject.decode(LogStash::Json.dump(data)) do |event|
+        insist { event.is_a? LogStash::Event }
+        insist { event["@timestamp"] } != nil
+        insist { event["type"] } == data["@type"]
+        insist { event["message"] } == data["@message"]
+        insist { event["host"] } == data["@source_host"]
+        insist { event["tags"] } == data["@tags"]
+        insist { event["path"] } == nil # @source_path not in v0 test data
+      end
+    end
+
+    it "should accept invalid json" do
+      subject.decode("some plain text") do |event|
+        insist { event["message"] } == "some plain text"
+      end
+    end
+  end
+
+  context "#encode" do
+    it "should return old (v0) json data" do
+      data = {"type" => "t", "message" => "wat!?",
+              "host" => "localhost", "path" => "/foo",
+              "tags" => ["a","b","c"],
+              "bah" => "baz"}
+      event = LogStash::Event.new(data)
+      got_event = false
+      subject.on_event do |d|
+        insist { LogStash::Json.load(d)["@timestamp"] } != nil
+        insist { LogStash::Json.load(d)["@type"] } == data["type"]
+        insist { LogStash::Json.load(d)["@message"] } == data["message"]
+        insist { LogStash::Json.load(d)["@source_host"] } == data["host"]
+        insist { LogStash::Json.load(d)["@source_path"] } == data["path"]
+        insist { LogStash::Json.load(d)["@tags"] } == data["tags"]
+        insist { LogStash::Json.load(d)["@fields"]["bah"] } == "baz"
+        insist { LogStash::Json.load(d)["@fields"]["@version"] } == nil
+        got_event = true
+      end
+      subject.encode(event)
+      insist { got_event }
+    end
+  end
+end
diff --git a/spec/codecs/plain_spec.rb b/spec/codecs/plain_spec.rb
new file mode 100644
index 00000000000..c7b555585a1
--- /dev/null
+++ b/spec/codecs/plain_spec.rb
@@ -0,0 +1,106 @@
+# encoding: utf-8
+
+require "logstash/codecs/plain"
+require "logstash/event"
+require "insist"
+
+describe LogStash::Codecs::Plain do
+  context "#decode" do
+    it "should return a valid event" do
+      subject.decode("Testing decoding.") do |event|
+        insist { event.is_a? LogStash::Event }
+      end
+    end
+
+    context "using default UTF-8 charset" do
+
+      it "should decode valid UTF-8 input" do
+        ["foobar", "Œ∫·ΩπœÉŒºŒµ"].each do |data|
+          insist { data.encoding.name } == "UTF-8"
+          insist { data.valid_encoding? } == true
+          subject.decode(data) do |event|
+            insist { event["message"] } == data
+            insist { event["message"].encoding.name } == "UTF-8"
+          end
+        end
+      end
+
+      it "should escape invalid sequences" do
+        ["foo \xED\xB9\x81\xC3", "bar \xAD"].each do |data|
+          insist { data.encoding.name } == "UTF-8"
+          insist { data.valid_encoding? } == false
+          subject.decode(data) do |event|
+            insist { event["message"] } == data.inspect[1..-2]
+            insist { event["message"].encoding.name } == "UTF-8"
+          end
+        end
+      end
+    end
+
+
+    context "with valid non UTF-8 source encoding" do
+
+      subject{LogStash::Codecs::Plain.new("charset" => "ISO-8859-1")}
+
+      it "should encode to UTF-8" do
+        samples = [
+          ["foobar", "foobar"],
+          ["\xE0 Montr\xE9al", "√† Montr√©al"],
+        ]
+        samples.map{|(a, b)| [a.force_encoding("ISO-8859-1"), b]}.each do |(a, b)|
+          insist { a.encoding.name } == "ISO-8859-1"
+          insist { b.encoding.name } == "UTF-8"
+          insist { a.valid_encoding? } == true
+
+          subject.decode(a) do |event|
+            insist { event["message"] } == b
+            insist { event["message"].encoding.name } == "UTF-8"
+          end
+        end
+      end
+    end
+
+    context "with invalid non UTF-8 source encoding" do
+
+      subject{LogStash::Codecs::Plain.new("charset" => "ASCII-8BIT")}
+
+      it "should encode to UTF-8" do
+        samples = [
+          ["\xE0 Montr\xE9al", "ÔøΩ MontrÔøΩal"],
+          ["\xCE\xBA\xCF\x8C\xCF\x83\xCE\xBC\xCE\xB5", "ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ"],
+        ]
+        samples.map{|(a, b)| [a.force_encoding("ASCII-8BIT"), b]}.each do |(a, b)|
+          insist { a.encoding.name } == "ASCII-8BIT"
+          insist { b.encoding.name } == "UTF-8"
+          insist { a.valid_encoding? } == true
+
+          subject.decode(a) do |event|
+            insist { event["message"] } == b
+            insist { event["message"].encoding.name } == "UTF-8"
+          end
+        end
+      end
+    end
+  end
+
+  context "#encode" do
+    it "should return a plain text encoding" do
+      event = LogStash::Event.new
+      event["message"] = "Hello World."
+      subject.on_event do |data|
+        insist { data } == event.to_s
+      end
+      subject.encode(event)
+    end
+
+    it "should respect the format setting" do
+      format = "%{[hello]} %{[something][fancy]}"
+      codec = LogStash::Codecs::Plain.new("format" => format)
+      event = LogStash::Event.new("hello" => "world", "something" => { "fancy" => 123 })
+      codec.on_event do |data|
+        insist { data } == event.sprintf(format)
+      end
+      codec.encode(event)
+    end
+  end
+end
diff --git a/spec/codecs/spool_spec.rb b/spec/codecs/spool_spec.rb
new file mode 100644
index 00000000000..5bdd6ee856b
--- /dev/null
+++ b/spec/codecs/spool_spec.rb
@@ -0,0 +1,35 @@
+require "logstash/codecs/spool"
+require "logstash/event"
+require "insist"
+
+describe LogStash::Codecs::Spool do
+  subject do
+    next LogStash::Codecs::Spool.new
+  end
+
+  context "#decode" do
+    it "should return multiple spooled events" do
+      e1 = LogStash::Event.new
+      e2 = LogStash::Event.new
+      e3 = LogStash::Event.new
+      subject.decode([e1,e2,e3]) do |event|
+        insist { event.is_a? LogStash::Event }
+      end
+    end
+  end
+
+  context "#encode" do
+    it "should return a spooled event" do
+      spool_size = Random.rand(10)
+      subject.spool_size = spool_size
+      got_event = false
+      subject.on_event do |data|
+        got_event = true
+      end
+      spool_size.times do
+        subject.encode(LogStash::Event.new)
+      end
+      insist { got_event }
+    end
+  end
+end
diff --git a/spec/core/conditionals_spec.rb b/spec/core/conditionals_spec.rb
new file mode 100644
index 00000000000..2bb46b2825f
--- /dev/null
+++ b/spec/core/conditionals_spec.rb
@@ -0,0 +1,396 @@
+require "spec_helper"
+
+module ConditionalFanciness
+  def description
+    return example.metadata[:example_group][:description_args][0]
+  end
+
+  def conditional(expression, &block)
+    describe(expression) do
+      config <<-CONFIG
+        filter {
+          if #{expression} {
+            mutate { add_tag => "success" }
+          } else {
+            mutate { add_tag => "failure" }
+          }
+        }
+      CONFIG
+      instance_eval(&block)
+    end
+  end
+end
+
+describe "conditionals in output" do
+  extend ConditionalFanciness
+
+  describe "simple" do
+    config <<-CONFIG
+      input {
+        generator {
+          message => '{"foo":{"bar"},"baz": "quux"}'
+          count => 1
+        }
+      }
+      output {
+        if [foo] == "bar" {
+          stdout { }
+        }
+      }
+    CONFIG
+
+    agent do
+      #LOGSTASH-2288, should not fail raising an exception
+    end
+  end
+end
+
+describe "conditionals in filter" do
+  extend ConditionalFanciness
+
+  describe "simple" do
+    config <<-CONFIG
+      filter {
+        mutate { add_field => { "always" => "awesome" } }
+        if [foo] == "bar" {
+          mutate { add_field => { "hello" => "world" } }
+        } else if [bar] == "baz" {
+          mutate { add_field => { "fancy" => "pants" } }
+        } else {
+          mutate { add_field => { "free" => "hugs" } }
+        }
+      }
+    CONFIG
+
+    sample({"foo" => "bar"}) do
+      insist { subject["always"] } == "awesome"
+      insist { subject["hello"] } == "world"
+      insist { subject["fancy"] }.nil?
+      insist { subject["free"] }.nil?
+    end
+
+    sample({"notfoo" => "bar"}) do
+      insist { subject["always"] } == "awesome"
+      insist { subject["hello"] }.nil?
+      insist { subject["fancy"] }.nil?
+      insist { subject["free"] } == "hugs"
+    end
+
+    sample({"bar" => "baz"}) do
+      insist { subject["always"] } == "awesome"
+      insist { subject["hello"] }.nil?
+      insist { subject["fancy"] } == "pants"
+      insist { subject["free"] }.nil?
+    end
+  end
+
+  describe "nested" do
+    config <<-CONFIG
+      filter {
+        if [nest] == 123 {
+          mutate { add_field => { "always" => "awesome" } }
+          if [foo] == "bar" {
+            mutate { add_field => { "hello" => "world" } }
+          } else if [bar] == "baz" {
+            mutate { add_field => { "fancy" => "pants" } }
+          } else {
+            mutate { add_field => { "free" => "hugs" } }
+          }
+        }
+      }
+    CONFIG
+
+    sample("foo" => "bar", "nest" => 124) do
+      insist { subject["always"] }.nil?
+      insist { subject["hello"] }.nil?
+      insist { subject["fancy"] }.nil?
+      insist { subject["free"] }.nil?
+    end
+
+    sample("foo" => "bar", "nest" => 123) do
+      insist { subject["always"] } == "awesome"
+      insist { subject["hello"] } == "world"
+      insist { subject["fancy"] }.nil?
+      insist { subject["free"] }.nil?
+    end
+
+    sample("notfoo" => "bar", "nest" => 123) do
+      insist { subject["always"] } == "awesome"
+      insist { subject["hello"] }.nil?
+      insist { subject["fancy"] }.nil?
+      insist { subject["free"] } == "hugs"
+    end
+
+    sample("bar" => "baz", "nest" => 123) do
+      insist { subject["always"] } == "awesome"
+      insist { subject["hello"] }.nil?
+      insist { subject["fancy"] } == "pants"
+      insist { subject["free"] }.nil?
+    end
+  end
+
+  describe "comparing two fields" do
+    config <<-CONFIG
+      filter {
+        if [foo] == [bar] {
+          mutate { add_tag => woot }
+        }
+      }
+    CONFIG
+
+    sample("foo" => 123, "bar" => 123) do
+      insist { subject["tags"] }.include?("woot")
+    end
+  end
+
+  describe "the 'in' operator" do
+    config <<-CONFIG
+      filter {
+        if [foo] in [foobar] {
+          mutate { add_tag => "field in field" }
+        }
+        if [foo] in "foo" {
+          mutate { add_tag => "field in string" }
+        }
+        if "hello" in [greeting] {
+          mutate { add_tag => "string in field" }
+        }
+        if [foo] in ["hello", "world", "foo"] {
+          mutate { add_tag => "field in list" }
+        }
+        if [missing] in [alsomissing] {
+          mutate { add_tag => "shouldnotexist" }
+        }
+        if !("foo" in ["hello", "world"]) {
+          mutate { add_tag => "shouldexist" }
+        }
+      }
+    CONFIG
+
+    sample("foo" => "foo", "foobar" => "foobar", "greeting" => "hello world") do
+      insist { subject["tags"] }.include?("field in field")
+      insist { subject["tags"] }.include?("field in string")
+      insist { subject["tags"] }.include?("string in field")
+      insist { subject["tags"] }.include?("field in list")
+      reject { subject["tags"] }.include?("shouldnotexist")
+      insist { subject["tags"] }.include?("shouldexist")
+    end
+  end
+
+  describe "the 'not in' operator" do
+    config <<-CONFIG
+      filter {
+        if "foo" not in "baz" { mutate { add_tag => "baz" } }
+        if "foo" not in "foo" { mutate { add_tag => "foo" } }
+        if !("foo" not in "foo") { mutate { add_tag => "notfoo" } }
+        if "foo" not in [somelist] { mutate { add_tag => "notsomelist" } }
+        if "one" not in [somelist] { mutate { add_tag => "somelist" } }
+        if "foo" not in [alsomissing] { mutate { add_tag => "no string in missing field" } }
+      }
+    CONFIG
+
+    sample("foo" => "foo", "somelist" => [ "one", "two" ], "foobar" => "foobar", "greeting" => "hello world", "tags" => [ "fancypantsy" ]) do
+      # verify the original exists
+      insist { subject["tags"] }.include?("fancypantsy")
+
+      insist { subject["tags"] }.include?("baz")
+      reject { subject["tags"] }.include?("foo")
+      insist { subject["tags"] }.include?("notfoo")
+      insist { subject["tags"] }.include?("notsomelist")
+      reject { subject["tags"] }.include?("somelist")
+      insist { subject["tags"] }.include?("no string in missing field")
+    end
+  end
+
+  describe "operators" do
+    conditional "[message] == 'sample'" do
+      sample("sample") { insist { subject["tags"] }.include?("success") }
+      sample("different") { insist { subject["tags"] }.include?("failure") }
+    end
+
+    conditional "[message] != 'sample'" do
+      sample("sample") { insist { subject["tags"] }.include?("failure") }
+      sample("different") { insist { subject["tags"] }.include?("success") }
+    end
+
+    conditional "[message] < 'sample'" do
+      sample("apple") { insist { subject["tags"] }.include?("success") }
+      sample("zebra") { insist { subject["tags"] }.include?("failure") }
+    end
+
+    conditional "[message] > 'sample'" do
+      sample("zebra") { insist { subject["tags"] }.include?("success") }
+      sample("apple") { insist { subject["tags"] }.include?("failure") }
+    end
+
+    conditional "[message] <= 'sample'" do
+      sample("apple") { insist { subject["tags"] }.include?("success") }
+      sample("zebra") { insist { subject["tags"] }.include?("failure") }
+      sample("sample") { insist { subject["tags"] }.include?("success") }
+    end
+
+    conditional "[message] >= 'sample'" do
+      sample("zebra") { insist { subject["tags"] }.include?("success") }
+      sample("sample") { insist { subject["tags"] }.include?("success") }
+      sample("apple") { insist { subject["tags"] }.include?("failure") }
+    end
+
+    conditional "[message] =~ /sample/" do
+      sample("apple") { insist { subject["tags"] }.include?("failure") }
+      sample("sample") { insist { subject["tags"] }.include?("success") }
+      sample("some sample") { insist { subject["tags"] }.include?("success") }
+    end
+
+    conditional "[message] !~ /sample/" do
+      sample("apple") { insist { subject["tags"] }.include?("success") }
+      sample("sample") { insist { subject["tags"] }.include?("failure") }
+      sample("some sample") { insist { subject["tags"] }.include?("failure") }
+    end
+
+  end
+
+  describe "negated expressions" do
+    conditional "!([message] == 'sample')" do
+      sample("sample") { reject { subject["tags"] }.include?("success") }
+      sample("different") { reject { subject["tags"] }.include?("failure") }
+    end
+
+    conditional "!([message] != 'sample')" do
+      sample("sample") { reject { subject["tags"] }.include?("failure") }
+      sample("different") { reject { subject["tags"] }.include?("success") }
+    end
+
+    conditional "!([message] < 'sample')" do
+      sample("apple") { reject { subject["tags"] }.include?("success") }
+      sample("zebra") { reject { subject["tags"] }.include?("failure") }
+    end
+
+    conditional "!([message] > 'sample')" do
+      sample("zebra") { reject { subject["tags"] }.include?("success") }
+      sample("apple") { reject { subject["tags"] }.include?("failure") }
+    end
+
+    conditional "!([message] <= 'sample')" do
+      sample("apple") { reject { subject["tags"] }.include?("success") }
+      sample("zebra") { reject { subject["tags"] }.include?("failure") }
+      sample("sample") { reject { subject["tags"] }.include?("success") }
+    end
+
+    conditional "!([message] >= 'sample')" do
+      sample("zebra") { reject { subject["tags"] }.include?("success") }
+      sample("sample") { reject { subject["tags"] }.include?("success") }
+      sample("apple") { reject { subject["tags"] }.include?("failure") }
+    end
+
+    conditional "!([message] =~ /sample/)" do
+      sample("apple") { reject { subject["tags"] }.include?("failure") }
+      sample("sample") { reject { subject["tags"] }.include?("success") }
+      sample("some sample") { reject { subject["tags"] }.include?("success") }
+    end
+
+    conditional "!([message] !~ /sample/)" do
+      sample("apple") { reject { subject["tags"] }.include?("success") }
+      sample("sample") { reject { subject["tags"] }.include?("failure") }
+      sample("some sample") { reject { subject["tags"] }.include?("failure") }
+    end
+
+  end
+
+  describe "value as an expression" do
+    # testing that a field has a value should be true.
+    conditional "[message]" do
+      sample("apple") { insist { subject["tags"] }.include?("success") }
+      sample("sample") { insist { subject["tags"] }.include?("success") }
+      sample("some sample") { insist { subject["tags"] }.include?("success") }
+    end
+
+    # testing that a missing field has a value should be false.
+    conditional "[missing]" do
+      sample("apple") { insist { subject["tags"] }.include?("failure") }
+      sample("sample") { insist { subject["tags"] }.include?("failure") }
+      sample("some sample") { insist { subject["tags"] }.include?("failure") }
+    end
+  end
+
+  describe "logic operators" do
+    describe "and" do
+      conditional "[message] and [message]" do
+        sample("whatever") { insist { subject["tags"] }.include?("success") }
+      end
+      conditional "[message] and ![message]" do
+        sample("whatever") { insist { subject["tags"] }.include?("failure") }
+      end
+      conditional "![message] and [message]" do
+        sample("whatever") { insist { subject["tags"] }.include?("failure") }
+      end
+      conditional "![message] and ![message]" do
+        sample("whatever") { insist { subject["tags"] }.include?("failure") }
+      end
+    end
+
+    describe "or" do
+      conditional "[message] or [message]" do
+        sample("whatever") { insist { subject["tags"] }.include?("success") }
+      end
+      conditional "[message] or ![message]" do
+        sample("whatever") { insist { subject["tags"] }.include?("success") }
+      end
+      conditional "![message] or [message]" do
+        sample("whatever") { insist { subject["tags"] }.include?("success") }
+      end
+      conditional "![message] or ![message]" do
+        sample("whatever") { insist { subject["tags"] }.include?("failure") }
+      end
+    end
+  end
+
+  describe "field references" do
+    conditional "[field with space]" do
+      sample("field with space" => "hurray") do
+        insist { subject["tags"].include?("success") }
+      end
+    end
+
+    conditional "[field with space] == 'hurray'" do
+      sample("field with space" => "hurray") do
+        insist { subject["tags"].include?("success") }
+      end
+    end
+
+    conditional "[nested field][reference with][some spaces] == 'hurray'" do
+      sample({"nested field" => { "reference with" => { "some spaces" => "hurray" } } }) do
+        insist { subject["tags"].include?("success") }
+      end
+    end
+  end
+
+  describe "new events from root" do
+    config <<-CONFIG
+      filter {
+        if [type] == "original" {
+          clone {
+            clones => ["clone"]
+          }
+        }
+        if [type] == "original" {
+          mutate { add_field => { "cond1" => "true" } }
+        } else {
+          mutate { add_field => { "cond2" => "true" } }
+        }
+      }
+    CONFIG
+
+    sample({"type" => "original"}) do
+      insist { subject }.is_a?(Array)
+      insist { subject.length } == 2
+
+      insist { subject[0]["type"] } == "original"
+      insist { subject[0]["cond1"] } == "true"
+      insist { subject[0]["cond2"] } == nil
+
+      insist { subject[1]["type"] } == "clone"
+      # insist { subject[1]["cond1"] } == nil
+      # insist { subject[1]["cond2"] } == "true"
+    end
+  end
+end
diff --git a/spec/core/config_spec.rb b/spec/core/config_spec.rb
new file mode 100644
index 00000000000..2221540536b
--- /dev/null
+++ b/spec/core/config_spec.rb
@@ -0,0 +1,31 @@
+# config syntax tests
+#
+
+require "logstash/config/grammar"
+require "logstash/config/config_ast"
+
+describe LogStashConfigParser do
+  it "should permit single-quoted attribute names" do
+    parser = LogStashConfigParser.new
+    config = parser.parse(%q(
+      input {
+        example {
+          'foo' => 'bar'
+          test => { 'bar' => 'baz' }
+        }
+      }
+    ))
+
+    reject { config }.nil?
+  end
+
+  it "should permit empty plugin sections" do
+    parser = LogStashConfigParser.new
+    config = parser.parse(%q(
+      filter {
+      }
+    ))
+
+    reject { config }.nil?
+  end
+end
diff --git a/spec/core/event_spec.rb b/spec/core/event_spec.rb
new file mode 100644
index 00000000000..eaf19c08201
--- /dev/null
+++ b/spec/core/event_spec.rb
@@ -0,0 +1,384 @@
+# encoding: utf-8
+
+require "logstash/event"
+require "insist"
+
+describe LogStash::Event do
+  subject do
+    LogStash::Event.new(
+      "@timestamp" => Time.iso8601("2013-01-01T00:00:00.000Z"),
+      "type" => "sprintf",
+      "message" => "hello world",
+      "tags" => [ "tag1" ],
+      "source" => "/home/foo",
+      "a" => "b",
+      "c" => {
+        "d" => "f",
+        "e" => {"f" => "g"}
+      },
+      "f" => { "g" => { "h" => "i" } },
+      "j" => {
+          "k1" => "v",
+          "k2" => [ "w", "x" ],
+          "k3" => {"4" => "m"},
+          5 => 6,
+          "5" => 7
+      }
+    )
+  end
+
+  context "[]=" do
+    it "should raise an exception if you attempt to set @timestamp to a value type other than a Time object" do
+      insist { subject["@timestamp"] = "crash!" }.raises(TypeError)
+    end
+
+    it "should assign simple fields" do
+      insist { subject["foo"] }.nil?
+      insist { subject["foo"] = "bar" } == "bar"
+      insist { subject["foo"] } == "bar"
+    end
+
+    it "should overwrite simple fields" do
+      insist { subject["foo"] }.nil?
+      insist { subject["foo"] = "bar"} == "bar"
+      insist { subject["foo"] } == "bar"
+
+      insist { subject["foo"] = "baz"} == "baz"
+      insist { subject["foo"] } == "baz"
+    end
+
+    it "should assign deep fields" do
+      insist { subject["[foo][bar]"] }.nil?
+      insist { subject["[foo][bar]"] = "baz"} == "baz"
+      insist { subject["[foo][bar]"] } == "baz"
+    end
+
+    it "should overwrite deep fields" do
+      insist { subject["[foo][bar]"] }.nil?
+      insist { subject["[foo][bar]"] = "baz"} == "baz"
+      insist { subject["[foo][bar]"] } == "baz"
+
+      insist { subject["[foo][bar]"] = "zab"} == "zab"
+      insist { subject["[foo][bar]"] } == "zab"
+    end
+  end
+
+  context "#sprintf" do
+    it "should report a unix timestamp for %{+%s}" do
+      insist { subject.sprintf("%{+%s}") } == "1356998400"
+    end
+
+    it "should report a time with %{+format} syntax", :if => RUBY_ENGINE == "jruby" do
+      insist { subject.sprintf("%{+YYYY}") } == "2013"
+      insist { subject.sprintf("%{+MM}") } == "01"
+      insist { subject.sprintf("%{+HH}") } == "00"
+    end
+
+    it "should report fields with %{field} syntax" do
+      insist { subject.sprintf("%{type}") } == "sprintf"
+      insist { subject.sprintf("%{message}") } == subject["message"]
+    end
+
+    it "should print deep fields" do
+      insist { subject.sprintf("%{[j][k1]}") } == "v"
+      insist { subject.sprintf("%{[j][k2][0]}") } == "w"
+    end
+
+    it "should be able to take a non-string for the format" do
+      insist { subject.sprintf(2) } == "2"
+    end
+  end
+
+  context "#[]" do
+    it "should fetch data" do
+      insist { subject["type"] } == "sprintf"
+    end
+    it "should fetch fields" do
+      insist { subject["a"] } == "b"
+      insist { subject['c']['d'] } == "f"
+    end
+    it "should fetch deep fields" do
+      insist { subject["[j][k1]"] } == "v"
+      insist { subject["[c][d]"] } == "f"
+      insist { subject['[f][g][h]'] } == "i"
+      insist { subject['[j][k3][4]'] } == "m"
+      insist { subject['[j][5]'] } == 7
+
+    end
+
+    it "should be fast?", :performance => true do
+      count = 1000000
+      2.times do
+        start = Time.now
+        count.times { subject["[j][k1]"] }
+        duration = Time.now - start
+        puts "event #[] rate: #{"%02.0f/sec" % (count / duration)}, elapsed: #{duration}s"
+      end
+    end
+  end
+
+  context "#overwrite" do
+    it "should swap data with new content" do
+      new_event = LogStash::Event.new(
+        "type" => "new",
+        "message" => "foo bar",
+      )
+      subject.overwrite(new_event)
+
+      insist { subject["message"] } == "foo bar"
+      insist { subject["type"] } == "new"
+
+      ["tags", "source", "a", "c", "f", "j"].each do |field|
+        insist { subject[field] } == nil
+      end
+    end
+  end
+
+  context "#append" do
+    it "should append strings to an array" do
+      subject.append(LogStash::Event.new("message" => "another thing"))
+      insist { subject["message"] } == [ "hello world", "another thing" ]
+    end
+
+    it "should concatenate tags" do
+      subject.append(LogStash::Event.new("tags" => [ "tag2" ]))
+      insist { subject["tags"] } == [ "tag1", "tag2" ]
+    end
+
+    context "when event field is nil" do
+      it "should add single value as string" do
+        subject.append(LogStash::Event.new({"field1" => "append1"}))
+        insist { subject[ "field1" ] } == "append1"
+      end
+      it "should add multi values as array" do
+        subject.append(LogStash::Event.new({"field1" => [ "append1","append2" ]}))
+        insist { subject[ "field1" ] } == [ "append1","append2" ]
+      end
+    end
+
+    context "when event field is a string" do
+      before { subject[ "field1" ] = "original1" }
+
+      it "should append string to values, if different from current" do
+        subject.append(LogStash::Event.new({"field1" => "append1"}))
+        insist { subject[ "field1" ] } == [ "original1", "append1" ]
+      end
+      it "should not change value, if appended value is equal current" do
+        subject.append(LogStash::Event.new({"field1" => "original1"}))
+        insist { subject[ "field1" ] } == "original1"
+      end
+      it "should concatenate values in an array" do
+        subject.append(LogStash::Event.new({"field1" => [ "append1" ]}))
+        insist { subject[ "field1" ] } == [ "original1", "append1" ]
+      end
+      it "should join array, removing duplicates" do
+        subject.append(LogStash::Event.new({"field1" => [ "append1","original1" ]}))
+        insist { subject[ "field1" ] } == [ "original1", "append1" ]
+      end
+    end
+    context "when event field is an array" do
+      before { subject[ "field1" ] = [ "original1", "original2" ] }
+
+      it "should append string values to array, if not present in array" do
+        subject.append(LogStash::Event.new({"field1" => "append1"}))
+        insist { subject[ "field1" ] } == [ "original1", "original2", "append1" ]
+      end
+      it "should not append string values, if the array already contains it" do
+        subject.append(LogStash::Event.new({"field1" => "original1"}))
+        insist { subject[ "field1" ] } == [ "original1", "original2" ]
+      end
+      it "should join array, removing duplicates" do
+        subject.append(LogStash::Event.new({"field1" => [ "append1","original1" ]}))
+        insist { subject[ "field1" ] } == [ "original1", "original2", "append1" ]
+      end
+    end
+  end
+
+  it "timestamp parsing speed", :performance => true do
+    warmup = 10000
+    count = 1000000
+
+    data = { "@timestamp" => "2013-12-21T07:25:06.605Z" }
+    event = LogStash::Event.new(data)
+    insist { event["@timestamp"] }.is_a?(LogStash::Timestamp)
+
+    duration = 0
+    [warmup, count].each do |i|
+      start = Time.now
+      i.times do
+        data = { "@timestamp" => "2013-12-21T07:25:06.605Z" }
+        LogStash::Event.new(data.clone)
+      end
+      duration = Time.now - start
+    end
+    puts "event @timestamp parse rate: #{"%02.0f/sec" % (count / duration)}, elapsed: #{duration}s"
+  end
+
+  context "acceptable @timestamp formats" do
+    subject { LogStash::Event.new }
+
+    formats = [
+      "YYYY-MM-dd'T'HH:mm:ss.SSSZ",
+      "YYYY-MM-dd'T'HH:mm:ss.SSSSSSZ",
+      "YYYY-MM-dd'T'HH:mm:ss.SSS",
+      "YYYY-MM-dd'T'HH:mm:ss",
+      "YYYY-MM-dd'T'HH:mm:ssZ",
+    ]
+    formats.each do |format|
+      it "includes #{format}" do
+        time = subject.sprintf("%{+#{format}}")
+        begin
+          LogStash::Event.new("@timestamp" => time)
+        rescue => e
+          raise StandardError, "Time '#{time}' was rejected. #{e.class}: #{e.to_s}"
+        end
+      end
+    end
+
+    context "from LOGSTASH-1738" do
+      it "does not error" do
+        LogStash::Event.new("@timestamp" => "2013-12-29T23:12:52.371240+02:00")
+      end
+    end
+
+    context "from LOGSTASH-1732" do
+      it "does not error" do
+        LogStash::Event.new("@timestamp" => "2013-12-27T11:07:25+00:00")
+      end
+    end
+  end
+
+  context "timestamp initialization" do
+    let(:logger) { double("logger") }
+
+    it "should coerce timestamp" do
+      t = Time.iso8601("2014-06-12T00:12:17.114Z")
+      expect(LogStash::Timestamp).to receive(:coerce).exactly(3).times.and_call_original
+      insist{LogStash::Event.new("@timestamp" => t).timestamp.to_i} == t.to_i
+      insist{LogStash::Event.new("@timestamp" => LogStash::Timestamp.new(t)).timestamp.to_i} == t.to_i
+      insist{LogStash::Event.new("@timestamp" => "2014-06-12T00:12:17.114Z").timestamp.to_i} == t.to_i
+    end
+
+    it "should assign current time when no timestamp" do
+      ts = LogStash::Timestamp.now
+      expect(LogStash::Timestamp).to receive(:now).and_return(ts)
+      insist{LogStash::Event.new({}).timestamp.to_i} == ts.to_i
+    end
+
+    it "should tag and warn for invalid value" do
+      ts = LogStash::Timestamp.now
+      expect(LogStash::Timestamp).to receive(:now).twice.and_return(ts)
+      expect(Cabin::Channel).to receive(:get).twice.and_return(logger)
+      expect(logger).to receive(:warn).twice
+
+      event = LogStash::Event.new("@timestamp" => :foo)
+      insist{event.timestamp.to_i} == ts.to_i
+      insist{event["tags"]} == [LogStash::Event::TIMESTAMP_FAILURE_TAG]
+      insist{event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]} == :foo
+
+      event = LogStash::Event.new("@timestamp" => 666)
+      insist{event.timestamp.to_i} == ts.to_i
+      insist{event["tags"]} == [LogStash::Event::TIMESTAMP_FAILURE_TAG]
+      insist{event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]} == 666
+    end
+
+    it "should tag and warn for invalid string format" do
+      ts = LogStash::Timestamp.now
+      expect(LogStash::Timestamp).to receive(:now).and_return(ts)
+      expect(Cabin::Channel).to receive(:get).and_return(logger)
+      expect(logger).to receive(:warn)
+
+      event = LogStash::Event.new("@timestamp" => "foo")
+      insist{event.timestamp.to_i} == ts.to_i
+      insist{event["tags"]} == [LogStash::Event::TIMESTAMP_FAILURE_TAG]
+      insist{event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]} == "foo"
+    end
+  end
+
+  context "to_json" do
+    it "should support to_json" do
+      new_event = LogStash::Event.new(
+        "@timestamp" => Time.iso8601("2014-09-23T19:26:15.832Z"),
+        "message" => "foo bar",
+      )
+      json = new_event.to_json
+
+      insist { json } ==  "{\"@timestamp\":\"2014-09-23T19:26:15.832Z\",\"message\":\"foo bar\",\"@version\":\"1\"}"
+    end
+
+    it "should support to_json and ignore arguments" do
+      new_event = LogStash::Event.new(
+        "@timestamp" => Time.iso8601("2014-09-23T19:26:15.832Z"),
+        "message" => "foo bar",
+      )
+      json = new_event.to_json(:foo => 1, :bar => "baz")
+
+      insist { json } ==  "{\"@timestamp\":\"2014-09-23T19:26:15.832Z\",\"message\":\"foo bar\",\"@version\":\"1\"}"
+    end
+  end
+
+  context "metadata" do
+    context "with existing metadata" do
+      subject { LogStash::Event.new("hello" => "world", "@metadata" => { "fancy" => "pants" }) }
+      it "should not include metadata in to_hash" do
+        reject { subject.to_hash.keys }.include?("@metadata")
+
+        # 'hello', '@timestamp', and '@version'
+        insist { subject.to_hash.keys.count } == 3
+      end
+
+      it "should still allow normal field access" do
+        insist { subject["hello"] } == "world"
+      end
+    end
+
+    context "with set metadata" do
+      let(:fieldref) { "[@metadata][foo][bar]" }
+      let(:value) { "bar" }
+      subject { LogStash::Event.new("normal" => "normal") }
+      before do
+        # Verify the test is configured correctly.
+        insist { fieldref }.start_with?("[@metadata]")
+
+        # Set it.
+        subject[fieldref] = value
+      end
+
+      it "should still allow normal field access" do
+        insist { subject["normal"] } == "normal"
+      end
+
+      it "should allow getting" do
+        insist { subject[fieldref] } == value
+      end
+
+      it "should be hidden from .to_json" do
+        require "json"
+        obj = JSON.parse(subject.to_json)
+        reject { obj }.include?("@metadata")
+      end
+
+      it "should be hidden from .to_hash" do
+        reject { subject.to_hash }.include?("@metadata")
+      end
+
+      it "should be accessible through #to_hash_with_metadata" do
+        obj = subject.to_hash_with_metadata
+        insist { obj }.include?("@metadata")
+        insist { obj["@metadata"]["foo"]["bar"] } == value
+      end
+    end
+    
+    context "with no metadata" do
+      subject { LogStash::Event.new("foo" => "bar") }
+      it "should have no metadata" do
+        insist { subject["@metadata"] }.empty?
+      end
+      it "should still allow normal field access" do
+        insist { subject["foo"] } == "bar"
+      end
+    end
+
+  end
+
+end
diff --git a/spec/core/runner_spec.rb b/spec/core/runner_spec.rb
new file mode 100644
index 00000000000..01c7587f63e
--- /dev/null
+++ b/spec/core/runner_spec.rb
@@ -0,0 +1,54 @@
+require "logstash/runner"
+require "logstash/agent"
+require "logstash/kibana"
+require "stud/task"
+
+class NullRunner
+  def run(args); end
+end
+
+describe LogStash::Runner do
+
+  context "argument parsing" do
+    it "should run agent" do
+      expect(Stud::Task).to receive(:new).once.and_return(nil)
+      args = ["agent", "-e", ""]
+      expect(subject.run(args)).to eq(nil)
+    end
+
+    it "should run agent help" do
+      expect(subject).to receive(:show_help).once.and_return(nil)
+      args = ["agent", "-h"]
+      expect(subject.run(args).wait).to eq(0)
+    end
+
+    it "should show help with no arguments" do
+      expect($stderr).to receive(:puts).once.and_return("No command given")
+      expect($stderr).to receive(:puts).once
+      args = []
+      expect(subject.run(args).wait).to eq(1)
+    end
+
+    it "should show help for unknown commands" do
+      expect($stderr).to receive(:puts).once.and_return("No such command welp")
+      expect($stderr).to receive(:puts).once
+      args = ["welp"]
+      expect(subject.run(args).wait).to eq(1)
+    end
+
+    it "should run agent help and not run following commands" do
+      expect(subject).to receive(:show_help).once.and_return(nil)
+      args = ["agent", "-h", "web"]
+      expect(subject.run(args).wait).to eq(0)
+    end
+
+    it "should not run agent and web" do
+      expect(Stud::Task).to receive(:new).once
+      args = ["agent", "-e", "", "web"]
+      args = subject.run(args)
+      expect(args).to eq(nil)
+
+      expect(LogStash::Kibana::Runner).to_not receive(:new)
+    end
+  end
+end
diff --git a/spec/core/timestamp_spec.rb b/spec/core/timestamp_spec.rb
new file mode 100644
index 00000000000..1ebbab634b3
--- /dev/null
+++ b/spec/core/timestamp_spec.rb
@@ -0,0 +1,42 @@
+require "logstash/timestamp"
+
+describe LogStash::Timestamp do
+
+  it "should parse its own iso8601 output" do
+    t = Time.now
+    ts = LogStash::Timestamp.new(t)
+    expect(LogStash::Timestamp.parse_iso8601(ts.to_iso8601).to_i).to eq(t.to_i)
+  end
+
+  it "should coerce iso8601 string" do
+    t = Time.now
+    ts = LogStash::Timestamp.new(t)
+    expect(LogStash::Timestamp.coerce(ts.to_iso8601).to_i).to eq(t.to_i)
+  end
+
+  it "should coerce Time" do
+    t = Time.now
+    expect(LogStash::Timestamp.coerce(t).to_i).to eq(t.to_i)
+  end
+
+  it "should coerce Timestamp" do
+    t = LogStash::Timestamp.now
+    expect(LogStash::Timestamp.coerce(t).to_i).to eq(t.to_i)
+  end
+
+  it "should raise on invalid string coerce" do
+    expect{LogStash::Timestamp.coerce("foobar")}.to raise_error LogStash::TimestampParserError
+  end
+
+  it "should return nil on invalid object coerce" do
+    expect(LogStash::Timestamp.coerce(:foobar)).to be_nil
+  end
+
+  it "should support to_json" do
+    expect(LogStash::Timestamp.parse_iso8601("2014-09-23T00:00:00-0800").to_json).to eq("\"2014-09-23T08:00:00.000Z\"")
+  end
+
+  it "should support to_json and ignore arguments" do
+    expect(LogStash::Timestamp.parse_iso8601("2014-09-23T00:00:00-0800").to_json(:some => 1, :argumnents => "test")).to eq("\"2014-09-23T08:00:00.000Z\"")
+  end
+end
diff --git a/spec/core/web_spec.rb b/spec/core/web_spec.rb
new file mode 100644
index 00000000000..2a5947546a2
--- /dev/null
+++ b/spec/core/web_spec.rb
@@ -0,0 +1,9 @@
+require "insist"
+
+describe "web tests" do
+  context "rack rubygem" do
+    it "must be available" do
+      require "rack"
+    end
+  end
+end
diff --git a/spec/examples/fail2ban_spec.rb b/spec/examples/fail2ban_spec.rb
new file mode 100644
index 00000000000..ee352d2004d
--- /dev/null
+++ b/spec/examples/fail2ban_spec.rb
@@ -0,0 +1,30 @@
+# encoding: utf-8
+
+require "spec_helper"
+
+describe "fail2ban logs", :if => RUBY_ENGINE == "jruby"  do
+  
+
+  # The logstash config goes here.
+  # At this time, only filters are supported.
+  config <<-CONFIG
+    filter {
+      grok {
+        pattern => "^%{TIMESTAMP_ISO8601:timestamp} fail2ban\.actions: %{WORD:level} \\[%{WORD:program}\\] %{WORD:action} %{IP:ip}"
+        singles => true
+      }
+      date {
+        match => [ "timestamp", "yyyy-MM-dd HH:mm:ss,SSS" ]
+      }
+      mutate {
+        remove => timestamp
+      }
+    }
+  CONFIG
+
+  sample "2013-06-28 15:10:59,891 fail2ban.actions: WARNING [ssh] Ban 95.78.163.5" do
+    insist { subject["program"] } == "ssh"
+    insist { subject["action"] } == "Ban"
+    insist { subject["ip"] } == "95.78.163.5"
+  end
+end
diff --git a/spec/examples/graphite-input_spec.rb b/spec/examples/graphite-input_spec.rb
new file mode 100644
index 00000000000..579b214d88d
--- /dev/null
+++ b/spec/examples/graphite-input_spec.rb
@@ -0,0 +1,43 @@
+# encoding: utf-8
+
+require "spec_helper"
+
+describe "receive graphite input", :if => RUBY_ENGINE == "jruby" do
+  
+
+  # The logstash config goes here.
+  # At this time, only filters are supported.
+  config <<-CONFIG
+    # input {
+    #   tcp {
+    #     port => 1234
+    #     mode => server
+    #     type => graphite
+    #   }
+    # }
+    filter {
+      grok {
+        pattern => "%{DATA:name} %{NUMBER:value:float} %{POSINT:ts}"
+        singles => true
+      }
+      date {
+        match => ["ts", UNIX]
+      }
+      mutate {
+        remove => ts
+      }
+    }
+  CONFIG
+
+  type "graphite"
+
+  sample "foo.bar.baz 4025.34 1364606522" do
+    insist { subject }.include?("name")
+    insist { subject }.include?("value")
+
+    insist { subject["name"] } == "foo.bar.baz"
+    insist { subject["value"] } == 4025.34
+    insist { subject["@timestamp"].time } == Time.iso8601("2013-03-30T01:22:02.000Z")
+
+  end
+end
diff --git a/spec/examples/mysql-slow-query_spec.rb b/spec/examples/mysql-slow-query_spec.rb
new file mode 100644
index 00000000000..bce57b96458
--- /dev/null
+++ b/spec/examples/mysql-slow-query_spec.rb
@@ -0,0 +1,70 @@
+require "spec_helper"
+
+# Skip until we convert this to use multiline codec
+describe "parse mysql slow query log", :if => false do
+  
+
+  config <<-'CONFIG'
+    filter {
+      grep {
+        # Drop the '# Time:' lines since they only appear when the 'time'
+        # changes and are omitted otherwise. Further, there's always (from what
+        # I have seen) a 'SET timestamp=123456789' line in each query event, so
+        # I use that as the timestamp instead.
+        #
+        # mysql logs are messed up, so this is pretty much best effort.
+        match => [ "@message", "^# Time: " ]
+        negate => true
+      }
+
+      grok {
+        singles => true
+        pattern => [
+          "^# User@Host: %{USER:user}\[[^\]]+\] @ %{HOST:host} \[%{IP:ip}?]",
+          "^# Query_time: %{NUMBER:duration:float} \s*Lock_time: %{NUMBER:lock_wait:float} \s*Rows_sent: %{NUMBER:results:int} \s*Rows_examined: %{NUMBER:scanned:int}",
+          "^SET timestamp=%{NUMBER:timestamp};"
+        ]
+      }
+
+      multiline {
+        pattern => "^# User@Host: "
+        negate => true
+        what => previous
+      }
+
+      date {
+        match => ["timestamp", UNIX]
+      }
+
+      mutate {
+        remove => "timestamp"
+      }
+    }
+  CONFIG
+
+  lines = <<-'MYSQL_SLOW_LOGS'
+# Time: 121004  6:00:27
+# User@Host: someuser[someuser] @ db.example.com [1.2.3.4]
+# Query_time: 0.018143  Lock_time: 0.000042 Rows_sent: 237  Rows_examined: 286
+use somedb;
+SET timestamp=1349355627;
+SELECT option_name, option_value FROM wp_options WHERE autoload = 'yes';
+MYSQL_SLOW_LOGS
+
+  sample lines.split("\n") do
+    reject { subject }.is_a? Array # 1 event expected
+    insist { subject.message.split("\n").size } == 5 # 5 lines
+
+    lines.split("\n")[1..5].each_with_index do |line, i|
+      insist { subject.message.split("\n")[i] } == line
+    end
+
+    insist { subject["user"] } == "someuser"
+    insist { subject["host"] } == "db.example.com"
+    insist { subject["ip"] } == "1.2.3.4"
+    insist { subject["duration"] } == 0.018143
+    insist { subject["lock_wait"] } == 0.000042
+    insist { subject["results"] } == 237
+    insist { subject["scanned"] } == 286
+  end
+end
diff --git a/spec/examples/parse-apache-logs_spec.rb b/spec/examples/parse-apache-logs_spec.rb
new file mode 100644
index 00000000000..7a403f66857
--- /dev/null
+++ b/spec/examples/parse-apache-logs_spec.rb
@@ -0,0 +1,69 @@
+# encoding: utf-8
+
+require "spec_helper"
+
+describe "apache common log format", :if => RUBY_ENGINE == "jruby" do
+  
+
+  # The logstash config goes here.
+  # At this time, only filters are supported.
+  config <<-CONFIG
+    filter {
+      grok {
+        pattern => "%{COMBINEDAPACHELOG}"
+        singles => true
+      }
+      date {
+        match => ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]
+        locale => "en"
+      }
+    }
+  CONFIG
+
+  # Here we provide a sample log event for the testing suite.
+  #
+  # Any filters you define above will be applied the same way the logstash
+  # agent performs. Inside the 'sample ... ' block the 'subject' will be
+  # a LogStash::Event object for you to inspect and verify for correctness.
+  sample '198.151.8.4 - - [29/Aug/2012:20:17:38 -0400] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:14.0) Gecko/20100101 Firefox/14.0.1"' do
+
+    # These 'insist' and 'reject' calls use my 'insist' rubygem.
+    # See http://rubydoc.info/gems/insist for more info.
+
+    # Require that grok does not fail to parse this event.
+    insist { subject["tags"] }.nil?
+
+    # Ensure that grok captures certain expected fields.
+    insist { subject }.include?("agent")
+    insist { subject }.include?("bytes")
+    insist { subject }.include?("clientip")
+    insist { subject }.include?("httpversion")
+    insist { subject }.include?("timestamp")
+    insist { subject }.include?("verb")
+    insist { subject }.include?("response")
+    insist { subject }.include?("request")
+
+    # Ensure that those fields match expected values from the event.
+    insist { subject["clientip"] } == "198.151.8.4"
+    insist { subject["timestamp"] } == "29/Aug/2012:20:17:38 -0400"
+    insist { subject["verb"] } == "GET"
+    insist { subject["request"] } == "/favicon.ico"
+    insist { subject["httpversion"] } == "1.1"
+    insist { subject["response"] } == "200"
+    insist { subject["bytes"] } == "3638"
+    insist { subject["referrer"] } == '"-"'
+    insist { subject["agent"] } == "\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:14.0) Gecko/20100101 Firefox/14.0.1\""
+
+    # Verify date parsing
+    insist { subject.timestamp.time } == Time.iso8601("2012-08-30T00:17:38.000Z")
+  end
+
+  sample '61.135.248.195 - - [26/Sep/2012:11:49:20 -0400] "GET /projects/keynav/ HTTP/1.1" 200 18985 "" "Mozilla/5.0 (compatible; YodaoBot/1.0; http://www.yodao.com/help/webmaster/spider/; )"' do
+    insist { subject["tags"] }.nil?
+    insist { subject["clientip"] } == "61.135.248.195"
+  end
+
+  sample '72.14.164.185 - - [25/Sep/2012:12:05:02 -0400] "GET /robots.txt HTTP/1.1" 200 - "www.brandimensions.com" "BDFetch"' do
+    insist { subject["tags"] }.nil?
+  end
+end
diff --git a/spec/examples/parse-haproxy-logs_spec.rb b/spec/examples/parse-haproxy-logs_spec.rb
new file mode 100644
index 00000000000..8c8abcfc044
--- /dev/null
+++ b/spec/examples/parse-haproxy-logs_spec.rb
@@ -0,0 +1,117 @@
+# encoding: utf-8
+
+require "spec_helper"
+
+describe "haproxy httplog format" do
+  
+
+  # The logstash config goes here.
+  # At this time, only filters are supported.
+  config <<-CONFIG
+  filter {
+    grok {
+      pattern => "%{HAPROXYHTTP}"
+    }
+  }
+  CONFIG
+  # Here we provide a sample log event for the testing suite.
+  #
+  # Any filters you define above will be applied the same way the logstash
+  # agent performs. Inside the 'sample ... ' block the 'subject' will be
+  # a LogStash::Event object for you to inspect and verify for correctness.
+  # HAPROXYHTTP %{SYSLOGTIMESTAMP:syslog_timestamp} %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\{%{HAPROXYCAPTUREDREQUESTHEADERS}\})?( )?(\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\})?( )?"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?"
+
+  sample 'Feb  6 12:14:14 localhost haproxy[14389]: 10.0.1.2:33317 [06/Feb/2009:12:14:14.655] http-in static/srv1 10/0/30/69/109 200 2750 - - ---- 1/1/1/1/0 0/0 {1wt.eu} {} "GET /index.html HTTP/1.1"' do
+
+    # These 'insist' and 'reject' calls use my 'insist' rubygem.
+    # See http://rubydoc.info/gems/insist for more info.
+
+    # Require that grok does not fail to parse this event.
+    insist { subject["tags"] }.nil?
+
+
+    # Ensure that grok captures certain expected fields.
+    insist { subject }.include?("syslog_timestamp")
+    insist { subject }.include?("syslog_server")
+    insist { subject }.include?("program")
+    insist { subject }.include?("pid")
+    insist { subject }.include?("client_ip")
+    insist { subject }.include?("client_port")
+    insist { subject }.include?("accept_date")
+    insist { subject }.include?("haproxy_monthday")
+    insist { subject }.include?("haproxy_month")
+    insist { subject }.include?("haproxy_year")
+    insist { subject }.include?("haproxy_time")
+    insist { subject }.include?("haproxy_hour")
+    insist { subject }.include?("haproxy_minute")
+    insist { subject }.include?("haproxy_second")
+    insist { subject }.include?("haproxy_milliseconds")
+    insist { subject }.include?("frontend_name")
+    insist { subject }.include?("backend_name")
+    insist { subject }.include?("server_name")
+    insist { subject }.include?("time_request")
+    insist { subject }.include?("time_queue")
+    insist { subject }.include?("time_backend_connect")
+    insist { subject }.include?("time_backend_response")
+    insist { subject }.include?("time_duration")
+    insist { subject }.include?("http_status_code")
+    insist { subject }.include?("bytes_read")
+    insist { subject }.include?("captured_request_cookie")
+    insist { subject }.include?("captured_response_cookie")
+    insist { subject }.include?("termination_state")
+    insist { subject }.include?("actconn")
+    insist { subject }.include?("feconn")
+    insist { subject }.include?("beconn")
+    insist { subject }.include?("srvconn")
+    insist { subject }.include?("retries")
+    insist { subject }.include?("srv_queue")
+    insist { subject }.include?("backend_queue")
+    insist { subject }.include?("captured_request_headers")
+    insist { subject }.include?("http_verb")
+    insist { subject }.include?("http_request")
+    insist { subject }.include?("http_version")
+
+#    # Ensure that those fields match expected values from the event.
+
+    insist{ subject["syslog_timestamp"] } == "Feb  6 12:14:14"
+    insist{ subject["syslog_server"] } == "localhost"
+    insist{ subject["program"] } == "haproxy"
+    insist{ subject["pid"] } == "14389"
+    insist{ subject["client_ip"] } == "10.0.1.2"
+    insist{ subject["client_port"] } == "33317"
+    insist{ subject["accept_date"] } == "06/Feb/2009:12:14:14.655"
+    insist{ subject["haproxy_monthday"] } == "06"
+    insist{ subject["haproxy_month"] } == "Feb"
+    insist{ subject["haproxy_year"] } == "2009"
+    insist{ subject["haproxy_time"] } == "12:14:14"
+    insist{ subject["haproxy_hour"] } == "12"
+    insist{ subject["haproxy_minute"] } == "14"
+    insist{ subject["haproxy_second"] } == "14"
+    insist{ subject["haproxy_milliseconds"] } == "655"
+    insist{ subject["frontend_name"] } == "http-in"
+    insist{ subject["backend_name"] } == "static"
+    insist{ subject["server_name"] } == "srv1"
+    insist{ subject["time_request"] } == "10"
+    insist{ subject["time_queue"] } == "0"
+    insist{ subject["time_backend_connect"] } == "30"
+    insist{ subject["time_backend_response"] } == "69"
+    insist{ subject["time_duration"] } == "109"
+    insist{ subject["http_status_code"] } == "200"
+    insist{ subject["bytes_read"] } == "2750"
+    insist{ subject["captured_request_cookie"] } == "-"
+    insist{ subject["captured_response_cookie"] } == "-"
+    insist{ subject["termination_state"] } == "----"
+    insist{ subject["actconn"] } == "1"
+    insist{ subject["feconn"] } == "1"
+    insist{ subject["beconn"] } == "1"
+    insist{ subject["srvconn"] } == "1"
+    insist{ subject["retries"] } == "0"
+    insist{ subject["srv_queue"] } == "0"
+    insist{ subject["backend_queue"] } == "0"
+    insist{ subject["captured_request_headers"] } == "1wt.eu"
+    insist{ subject["http_verb"] } == "GET"
+    insist{ subject["http_request"] } == "/index.html"
+    insist{ subject["http_version"] } == "1.1"
+  end
+
+end
diff --git a/spec/examples/syslog_spec.rb b/spec/examples/syslog_spec.rb
new file mode 100644
index 00000000000..1559a125cb2
--- /dev/null
+++ b/spec/examples/syslog_spec.rb
@@ -0,0 +1,50 @@
+# encoding: utf-8
+
+require "spec_helper"
+
+describe "parse syslog", :if => RUBY_ENGINE == "jruby" do
+  
+
+  config <<-'CONFIG'
+    filter {
+      grok {
+          type => "syslog"
+          singles => true
+          pattern => [ "<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{PROG:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" ]
+          add_field => [ "received_at", "%{@timestamp}" ]
+          add_field => [ "received_from", "%{source_host}" ]
+      }
+      syslog_pri {
+          type => "syslog"
+      }
+      date {
+          type => "syslog"
+          match => ["syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
+      }
+      mutate {
+          type => "syslog"
+          exclude_tags => "_grokparsefailure"
+          replace => [ "source_host", "%{syslog_hostname}" ]
+          replace => [ "message", "%{syslog_message}" ]
+      }
+      mutate {
+          type => "syslog"
+          remove => [ "syslog_hostname", "syslog_message", "syslog_timestamp" ]
+      }
+    }
+  CONFIG
+
+  sample("message" => "<164>Oct 26 15:19:25 1.2.3.4 %ASA-4-106023: Deny udp src DRAC:10.1.2.3/43434 dst outside:192.168.0.1/53 by access-group \"acl_drac\" [0x0, 0x0]", "type" => "syslog") do
+    insist { subject["type"] } == "syslog"
+    insist { subject["tags"] }.nil?
+    insist { subject["syslog_pri"] } == "164"
+  end
+
+  # Single digit day
+  sample("message" => "<164>Oct  6 15:19:25 1.2.3.4 %ASA-4-106023: Deny udp src DRAC:10.1.2.3/43434 dst outside:192.168.0.1/53 by access-group \"acl_drac\" [0x0, 0x0]", "type" => "syslog") do
+    insist { subject["type"] } == "syslog"
+    insist { subject["tags"] }.nil?
+    insist { subject["syslog_pri"] } == "164"
+    #insist { subject.timestamp } == "2012-10-26T15:19:25.000Z"
+  end
+end
diff --git a/spec/filters/anonymize_spec.rb b/spec/filters/anonymize_spec.rb
new file mode 100644
index 00000000000..dae90e620bc
--- /dev/null
+++ b/spec/filters/anonymize_spec.rb
@@ -0,0 +1,190 @@
+# encoding: utf-8
+
+require "spec_helper"
+require "logstash/filters/anonymize"
+
+describe LogStash::Filters::Anonymize do
+
+  describe "anonymize ipaddress with IPV4_NETWORK algorithm" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        anonymize {
+          fields => ["clientip"]
+          algorithm => "IPV4_NETWORK"
+          key => 24
+        }
+      }
+    CONFIG
+
+    sample("clientip" => "233.255.13.44") do
+      insist { subject["clientip"] } == "233.255.13.0"
+    end
+  end
+
+  describe "anonymize string with MURMUR3 algorithm" do
+    config <<-CONFIG
+      filter {
+        anonymize {
+          fields => ["clientip"]
+          algorithm => "MURMUR3"
+          key => ""
+        }
+      }
+    CONFIG
+
+    sample("clientip" => "123.52.122.33") do
+      insist { subject["clientip"] } == 1541804874
+    end
+  end
+
+   describe "anonymize string with SHA1 alogrithm" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        anonymize {
+          fields => ["clientip"]
+          key => "longencryptionkey"
+          algorithm => 'SHA1'
+        }
+      }
+    CONFIG
+
+    sample("clientip" => "123.123.123.123") do
+      insist { subject["clientip"] } == "fdc60acc4773dc5ac569ffb78fcb93c9630797f4"
+    end
+  end
+
+  # HMAC-SHA224 isn't implemented in JRuby OpenSSL
+  #describe "anonymize string with SHA224 alogrithm" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    #config <<-CONFIG
+      #filter {
+        #anonymize {
+          #fields => ["clientip"]
+          #key => "longencryptionkey"
+          #algorithm => 'SHA224'
+        #}
+      #}
+    #CONFIG
+
+    #sample("clientip" => "123.123.123.123") do
+      #insist { subject["clientip"] } == "5744bbcc4f64acb6a805b7fee3013a8958cc8782d3fb0fb318cec915"
+    #end
+  #end
+
+  describe "anonymize string with SHA256 alogrithm" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        anonymize {
+          fields => ["clientip"]
+          key => "longencryptionkey"
+          algorithm => 'SHA256'
+        }
+      }
+    CONFIG
+
+    sample("clientip" => "123.123.123.123") do
+      insist { subject["clientip"] } == "345bec3eff242d53b568916c2610b3e393d885d6b96d643f38494fd74bf4a9ca"
+    end
+  end
+
+  describe "anonymize string with SHA384 alogrithm" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        anonymize {
+          fields => ["clientip"]
+          key => "longencryptionkey"
+          algorithm => 'SHA384'
+        }
+      }
+    CONFIG
+
+    sample("clientip" => "123.123.123.123") do
+      insist { subject["clientip"] } == "22d4c0e8c4fbcdc4887d2038fca7650f0e2e0e2457ff41c06eb2a980dded6749561c814fe182aff93e2538d18593947a"
+    end
+  end
+
+  describe "anonymize string with SHA512 alogrithm" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        anonymize {
+          fields => ["clientip"]
+          key => "longencryptionkey"
+          algorithm => 'SHA512'
+        }
+      }
+    CONFIG
+
+    sample("clientip" => "123.123.123.123") do
+      insist { subject["clientip"] } == "11c19b326936c08d6c50a3c847d883e5a1362e6a64dd55201a25f2c1ac1b673f7d8bf15b8f112a4978276d573275e3b14166e17246f670c2a539401c5bfdace8"
+    end
+  end
+
+  # HMAC-MD4 isn't implemented in JRuby OpenSSL
+  #describe "anonymize string with MD4 alogrithm" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    #config <<-CONFIG
+      #filter {
+        #anonymize {
+          #fields => ["clientip"]
+          #key => "longencryptionkey"
+          #algorithm => 'MD4'
+        #}
+      #}
+    #CONFIG
+#
+    #sample("clientip" => "123.123.123.123") do
+      #insist { subject["clientip"] } == "0845cb571ab3646e51a07bcabf05e33d"
+    #end
+  #end
+
+  describe "anonymize string with MD5 alogrithm" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        anonymize {
+          fields => ["clientip"]
+          key => "longencryptionkey"
+          algorithm => 'MD5'
+        }
+      }
+    CONFIG
+
+    sample("clientip" => "123.123.123.123") do
+      insist { subject["clientip"] } == "9336c879e305c9604a3843fc3e75948f"
+    end
+  end
+
+  describe "Test field with multiple values" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        anonymize {
+          fields => ["clientip"]
+          key => "longencryptionkey"
+          algorithm => 'MD5'
+        }
+      }
+    CONFIG
+
+    sample("clientip" => [ "123.123.123.123", "223.223.223.223" ]) do
+      insist { subject["clientip"]} == [ "9336c879e305c9604a3843fc3e75948f", "7a6c66b8d3f42a7d650e3354af508df3" ]
+    end
+  end
+
+
+
+end
diff --git a/spec/filters/checksum_spec.rb b/spec/filters/checksum_spec.rb
new file mode 100644
index 00000000000..a79c70e8b97
--- /dev/null
+++ b/spec/filters/checksum_spec.rb
@@ -0,0 +1,42 @@
+# encoding: utf-8
+
+require "spec_helper"
+require "logstash/filters/checksum"
+require 'openssl'
+
+describe LogStash::Filters::Checksum do
+
+  LogStash::Filters::Checksum::ALGORITHMS.each do |alg|
+    describe "#{alg} checksum with single field" do
+      config <<-CONFIG
+        filter {
+          checksum {
+            algorithm => "#{alg}"
+            keys => ["test"]
+          }
+        }
+        CONFIG
+
+      sample "test" => "foo bar" do
+        insist { !subject["logstash_checksum"].nil? }
+        insist { subject["logstash_checksum"] } == OpenSSL::Digest.hexdigest(alg, "|test|foo bar|")
+      end
+    end
+
+    describe "#{alg} checksum with multiple keys" do
+      config <<-CONFIG
+        filter {
+          checksum {
+            algorithm => "#{alg}"
+            keys => ["test1", "test2"]
+          }
+        }
+        CONFIG
+
+      sample "test1" => "foo", "test2" => "bar" do
+        insist { !subject["logstash_checksum"].nil? }
+        insist { subject["logstash_checksum"] } == OpenSSL::Digest.hexdigest(alg, "|test1|foo|test2|bar|")
+      end
+    end
+  end
+end
diff --git a/spec/filters/clone_spec.rb b/spec/filters/clone_spec.rb
new file mode 100644
index 00000000000..df4edeff24e
--- /dev/null
+++ b/spec/filters/clone_spec.rb
@@ -0,0 +1,84 @@
+require "spec_helper"
+require "logstash/filters/clone"
+
+describe LogStash::Filters::Clone do
+
+  describe "all defaults" do
+    type "original"
+    config <<-CONFIG
+      filter {
+        clone {
+          type => "original"
+          clones => ["clone", "clone", "clone"]
+        }
+      }
+    CONFIG
+
+    sample("message" => "hello world", "type" => "original") do
+      insist { subject }.is_a? Array
+      insist { subject.length } == 4
+      subject.each_with_index do |s,i|
+        if i == 0 # last one should be 'original'
+          insist { s["type"] } == "original"
+        else
+          insist { s["type"]} == "clone"
+        end
+        insist { s["message"] } == "hello world"
+      end
+    end
+  end
+
+  describe "Complex use" do
+    config <<-CONFIG
+      filter {
+        clone {
+          type => "nginx-access"
+          tags => ['TESTLOG']
+          clones => ["nginx-access-clone1", "nginx-access-clone2"]
+          add_tag => ['RABBIT','NO_ES']
+          remove_tag => ["TESTLOG"]
+        }
+      }
+    CONFIG
+
+    sample("type" => "nginx-access", "tags" => ["TESTLOG"], "message" => "hello world") do
+      insist { subject }.is_a? Array
+      insist { subject.length } == 3
+
+      insist { subject[0]["type"] } == "nginx-access"
+      #Initial event remains unchanged
+      insist { subject[0]["tags"] }.include? "TESTLOG"
+      reject { subject[0]["tags"] }.include? "RABBIT"
+      reject { subject[0]["tags"] }.include? "NO_ES"
+      #All clones go through filter_matched
+      insist { subject[1]["type"] } == "nginx-access-clone1"
+      reject { subject[1]["tags"] }.include? "TESTLOG"
+      insist { subject[1]["tags"] }.include? "RABBIT"
+      insist { subject[1]["tags"] }.include? "NO_ES"
+
+      insist { subject[2]["type"] } == "nginx-access-clone2"
+      reject { subject[2]["tags"] }.include? "TESTLOG"
+      insist { subject[2]["tags"] }.include? "RABBIT"
+      insist { subject[2]["tags"] }.include? "NO_ES"
+
+    end
+  end
+
+  describe "Bug LOGSTASH-1225" do
+    ### LOGSTASH-1225: Cannot clone events containing numbers.
+    config <<-CONFIG
+      filter {
+        clone {
+          clones => [ 'clone1' ]
+        }
+      }
+    CONFIG
+
+    sample("type" => "bug-1225", "message" => "unused", "number" => 5) do
+      insist { subject[0]["number"] } == 5
+      insist { subject[1]["number"] } == 5
+    end
+  end
+
+
+end
diff --git a/spec/filters/csv_spec.rb b/spec/filters/csv_spec.rb
new file mode 100644
index 00000000000..f9d7d3a00c2
--- /dev/null
+++ b/spec/filters/csv_spec.rb
@@ -0,0 +1,175 @@
+# encoding: utf-8
+
+require "spec_helper"
+require "logstash/filters/csv"
+
+describe LogStash::Filters::CSV do
+
+  describe "all defaults" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        csv { }
+      }
+    CONFIG
+
+    sample "big,bird,sesame street" do
+      insist { subject["column1"] } == "big"
+      insist { subject["column2"] } == "bird"
+      insist { subject["column3"] } == "sesame street"
+    end
+  end
+
+  describe "custom separator" do
+    config <<-CONFIG
+      filter {
+        csv {
+          separator => ";"
+        }
+      }
+    CONFIG
+
+    sample "big,bird;sesame street" do
+      insist { subject["column1"] } == "big,bird"
+      insist { subject["column2"] } == "sesame street"
+    end
+  end
+
+  describe "custom quote char" do
+    config <<-CONFIG
+      filter {
+        csv {
+          quote_char => "'"
+        }
+      }
+    CONFIG
+
+    sample "big,bird,'sesame street'" do
+      insist { subject["column1"] } == "big"
+      insist { subject["column2"] } == "bird"
+      insist { subject["column3"] } == "sesame street"
+    end
+  end
+
+  describe "default quote char" do
+    config <<-CONFIG
+      filter {
+        csv {
+        }
+      }
+    CONFIG
+
+    sample 'big,bird,"sesame, street"' do
+      insist { subject["column1"] } == "big"
+      insist { subject["column2"] } == "bird"
+      insist { subject["column3"] } == "sesame, street"
+    end
+  end
+  describe "null quote char" do
+    config <<-CONFIG
+      filter {
+        csv {
+          quote_char => "\x00"
+        }
+      }
+    CONFIG
+
+    sample 'big,bird,"sesame" street' do
+      insist { subject["column1"] } == 'big'
+      insist { subject["column2"] } == 'bird'
+      insist { subject["column3"] } == '"sesame" street'
+    end
+  end
+
+  describe "given columns" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        csv {
+          columns => ["first", "last", "address" ]
+        }
+      }
+    CONFIG
+
+    sample "big,bird,sesame street" do
+      insist { subject["first"] } == "big"
+      insist { subject["last"] } == "bird"
+      insist { subject["address"] } == "sesame street"
+    end
+  end
+
+  describe "parse csv with more data than defined column names" do
+    config <<-CONFIG
+      filter {
+        csv {
+          columns => ["custom1", "custom2"]
+        }
+      }
+    CONFIG
+
+    sample "val1,val2,val3" do
+      insist { subject["custom1"] } == "val1"
+      insist { subject["custom2"] } == "val2"
+      insist { subject["column3"] } == "val3"
+    end
+  end
+
+
+  describe "parse csv from a given source with column names" do
+    config <<-CONFIG
+      filter {
+        csv {
+          source => "datafield"
+          columns => ["custom1", "custom2", "custom3"]
+        }
+      }
+    CONFIG
+
+    sample("datafield" => "val1,val2,val3") do
+      insist { subject["custom1"] } == "val1"
+      insist { subject["custom2"] } == "val2"
+      insist { subject["custom3"] } == "val3"
+    end
+  end
+
+  describe "given target" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        csv {
+          target => "data"
+        }
+      }
+    CONFIG
+
+    sample "big,bird,sesame street" do
+      insist { subject["data"]["column1"] } == "big"
+      insist { subject["data"]["column2"] } == "bird"
+      insist { subject["data"]["column3"] } == "sesame street"
+    end
+  end
+
+  describe "given target and source" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        csv {
+          source => "datain"
+          target => "data"
+        }
+      }
+    CONFIG
+
+    sample("datain" => "big,bird,sesame street") do
+      insist { subject["data"]["column1"] } == "big"
+      insist { subject["data"]["column2"] } == "bird"
+      insist { subject["data"]["column3"] } == "sesame street"
+    end
+  end
+
+
+end
diff --git a/spec/filters/date_spec.rb b/spec/filters/date_spec.rb
new file mode 100644
index 00000000000..39936fe581f
--- /dev/null
+++ b/spec/filters/date_spec.rb
@@ -0,0 +1,406 @@
+require "spec_helper"
+require "logstash/filters/date"
+
+puts "Skipping date performance tests because this ruby is not jruby" if RUBY_ENGINE != "jruby"
+RUBY_ENGINE == "jruby" and describe LogStash::Filters::Date do
+
+  describe "giving an invalid match config, raise a configuration error" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "mydate"]
+          locale => "en"
+        }
+      }
+    CONFIG
+
+    sample "not_really_important" do
+      insist {subject}.raises LogStash::ConfigurationError
+    end
+
+  end
+
+  describe "parsing with ISO8601" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "mydate", "ISO8601" ]
+          locale => "en"
+          timezone => "UTC"
+        }
+      }
+    CONFIG
+
+    times = {
+      "2001-01-01T00:00:00-0800"         => "2001-01-01T08:00:00.000Z",
+      "1974-03-02T04:09:09-0800"         => "1974-03-02T12:09:09.000Z",
+      "2010-05-03T08:18:18+00:00"        => "2010-05-03T08:18:18.000Z",
+      "2004-07-04T12:27:27-00:00"        => "2004-07-04T12:27:27.000Z",
+      "2001-09-05T16:36:36+0000"         => "2001-09-05T16:36:36.000Z",
+      "2001-11-06T20:45:45-0000"         => "2001-11-06T20:45:45.000Z",
+      "2001-12-07T23:54:54Z"             => "2001-12-07T23:54:54.000Z",
+
+      # TODO: This test assumes PDT
+      #"2001-01-01T00:00:00.123"          => "2001-01-01T08:00:00.123Z",
+
+      "2010-05-03T08:18:18.123+00:00"    => "2010-05-03T08:18:18.123Z",
+      "2004-07-04T12:27:27.123-04:00"    => "2004-07-04T16:27:27.123Z",
+      "2001-09-05T16:36:36.123+0700"     => "2001-09-05T09:36:36.123Z",
+      "2001-11-06T20:45:45.123-0000"     => "2001-11-06T20:45:45.123Z",
+      "2001-12-07T23:54:54.123Z"         => "2001-12-07T23:54:54.123Z",
+
+      #Almost ISO8601 support, with timezone
+
+      "2001-11-06 20:45:45.123-0000"     => "2001-11-06T20:45:45.123Z",
+      "2001-12-07 23:54:54.123Z"         => "2001-12-07T23:54:54.123Z",
+
+      #Almost ISO8601 support, without timezone
+
+      "2001-11-06 20:45:45.123"     => "2001-11-06T20:45:45.123Z",
+
+    }
+
+    times.each do |input, output|
+      sample("mydate" => input) do
+        begin
+          insist { subject["mydate"] } == input
+          insist { subject["@timestamp"].time } == Time.iso8601(output).utc
+        rescue
+          #require "pry"; binding.pry
+          raise
+        end
+      end
+    end # times.each
+  end
+
+  describe "parsing with java SimpleDateFormat syntax" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "mydate", "MMM dd HH:mm:ss Z" ]
+          locale => "en"
+        }
+      }
+    CONFIG
+
+    now = Time.now
+    year = now.year
+    require 'java'
+
+    times = {
+      "Nov 24 01:29:01 -0800" => "#{year}-11-24T09:29:01.000Z",
+    }
+    times.each do |input, output|
+      sample("mydate" => input) do
+        insist { subject["mydate"] } == input
+        insist { subject["@timestamp"].time } == Time.iso8601(output).utc
+      end
+    end # times.each
+  end
+
+  describe "parsing with UNIX" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "mydate", "UNIX" ]
+          locale => "en"
+        }
+      }
+    CONFIG
+
+    times = {
+      "0"          => "1970-01-01T00:00:00.000Z",
+      "1000000000" => "2001-09-09T01:46:40.000Z",
+
+      # LOGSTASH-279 - sometimes the field is a number.
+      0          => "1970-01-01T00:00:00.000Z",
+      1000000000 => "2001-09-09T01:46:40.000Z"
+    }
+    times.each do |input, output|
+      sample("mydate" => input) do
+        insist { subject["mydate"] } == input
+        insist { subject["@timestamp"].time } == Time.iso8601(output).utc
+      end
+    end # times.each
+
+    #Invalid value should not be evaluated to zero (String#to_i madness)
+    sample("mydate" => "%{bad_value}") do
+      insist { subject["mydate"] } == "%{bad_value}"
+      insist { subject["@timestamp"] } != Time.iso8601("1970-01-01T00:00:00.000Z").utc
+    end
+  end
+
+  describe "parsing microsecond-precise times with UNIX (#213)" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "mydate", "UNIX" ]
+          locale => "en"
+        }
+      }
+    CONFIG
+
+    sample("mydate" => "1350414944.123456") do
+      # Joda time only supports milliseconds :\
+      insist { subject.timestamp.time } == Time.iso8601("2012-10-16T12:15:44.123-07:00").utc
+    end
+
+    #Support float values
+    sample("mydate" => 1350414944.123456) do
+      insist { subject["mydate"] } == 1350414944.123456
+      insist { subject["@timestamp"].time } == Time.iso8601("2012-10-16T12:15:44.123-07:00").utc
+    end
+
+    #Invalid value should not be evaluated to zero (String#to_i madness)
+    sample("mydate" => "%{bad_value}") do
+      insist { subject["mydate"] } == "%{bad_value}"
+      insist { subject["@timestamp"] } != Time.iso8601("1970-01-01T00:00:00.000Z").utc
+    end
+  end
+
+  describe "parsing with UNIX_MS" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "mydate", "UNIX_MS" ]
+          locale => "en"
+        }
+      }
+    CONFIG
+
+    times = {
+      "0"          => "1970-01-01T00:00:00.000Z",
+      "456"          => "1970-01-01T00:00:00.456Z",
+      "1000000000123" => "2001-09-09T01:46:40.123Z",
+
+      # LOGSTASH-279 - sometimes the field is a number.
+      0          => "1970-01-01T00:00:00.000Z",
+      456          => "1970-01-01T00:00:00.456Z",
+      1000000000123 => "2001-09-09T01:46:40.123Z"
+    }
+    times.each do |input, output|
+      sample("mydate" => input) do
+        insist { subject["mydate"] } == input
+        insist { subject["@timestamp"].time } == Time.iso8601(output)
+      end
+    end # times.each
+  end
+
+  describe "failed parses should not cause a failure (LOGSTASH-641)" do
+    config <<-'CONFIG'
+      input {
+        generator {
+          lines => [
+            '{ "mydate": "this will not parse" }',
+            '{ }'
+          ]
+          codec => json
+          type => foo
+          count => 1
+        }
+      }
+      filter {
+        date {
+          match => [ "mydate", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
+          locale => "en"
+        }
+      }
+      output {
+        null { }
+      }
+    CONFIG
+
+    agent do
+      # nothing to do, if this crashes it's an error..
+    end
+  end
+
+  describe "TAI64N support" do
+    config <<-'CONFIG'
+      filter {
+        date {
+          match => [ "t",  TAI64N ]
+          locale => "en"
+        }
+      }
+    CONFIG
+
+    # Try without leading "@"
+    sample("t" => "4000000050d506482dbdf024") do
+      insist { subject.timestamp.time } == Time.iso8601("2012-12-22T01:00:46.767Z").utc
+    end
+
+    # Should still parse successfully if it's a full tai64n time (with leading
+    # '@')
+    sample("t" => "@4000000050d506482dbdf024") do
+      insist { subject.timestamp.time } == Time.iso8601("2012-12-22T01:00:46.767Z").utc
+    end
+  end
+
+  describe "accept match config option with hash value (LOGSTASH-735)" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "mydate", "ISO8601" ]
+          locale => "en"
+        }
+      }
+    CONFIG
+
+    time = "2001-09-09T01:46:40.000Z"
+
+    sample("mydate" => time) do
+      insist { subject["mydate"] } == time
+      insist { subject["@timestamp"].time } == Time.iso8601(time).utc
+    end
+  end
+
+  describe "support deep nested field access" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "[data][deep]", "ISO8601" ]
+          locale => "en"
+        }
+      }
+    CONFIG
+
+    sample("data" => { "deep" => "2013-01-01T00:00:00.000Z" }) do
+      insist { subject["@timestamp"].time } == Time.iso8601("2013-01-01T00:00:00.000Z").utc
+    end
+  end
+
+  describe "failing to parse should not throw an exception" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "thedate", "yyyy/MM/dd" ]
+          locale => "en"
+        }
+      }
+    CONFIG
+
+    sample("thedate" => "2013/Apr/21") do
+      insist { subject["@timestamp"] } != "2013-04-21T00:00:00.000Z"
+    end
+  end
+
+   describe "success to parse should apply on_success config(add_tag,add_field...)" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "thedate", "yyyy/MM/dd" ]
+          add_tag => "tagged"
+        }
+      }
+    CONFIG
+
+    sample("thedate" => "2013/04/21") do
+      insist { subject["@timestamp"] } != "2013-04-21T00:00:00.000Z"
+      insist { subject["tags"] } == ["tagged"]
+    end
+  end
+
+   describe "failing to parse should not apply on_success config(add_tag,add_field...)" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "thedate", "yyyy/MM/dd" ]
+          add_tag => "tagged"
+        }
+      }
+    CONFIG
+
+    sample("thedate" => "2013/Apr/21") do
+      insist { subject["@timestamp"] } != "2013-04-21T00:00:00.000Z"
+      insist { subject["tags"] } == nil
+    end
+  end
+
+  describe "parsing with timezone parameter" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => ["mydate", "yyyy MMM dd HH:mm:ss"]
+          locale => "en"
+          timezone => "America/Los_Angeles"
+        }
+      }
+    CONFIG
+
+    require 'java'
+    times = {
+      "2013 Nov 24 01:29:01" => "2013-11-24T09:29:01.000Z",
+      "2013 Jun 24 01:29:01" => "2013-06-24T08:29:01.000Z",
+    }
+    times.each do |input, output|
+      sample("mydate" => input) do
+        insist { subject["mydate"] } == input
+        insist { subject["@timestamp"].time } == Time.iso8601(output).utc
+      end
+    end # times.each
+  end
+
+  describe "LOGSTASH-34 - Default year should be this year" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "message", "EEE MMM dd HH:mm:ss" ]
+          locale => "en"
+        }
+      }
+    CONFIG
+
+    sample "Sun Jun 02 20:38:03" do
+      insist { subject["@timestamp"].year } == Time.now.year
+    end
+  end
+
+  describe "Supporting locale only" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "message", "dd MMMM yyyy" ]
+          locale => "fr"
+          timezone => "UTC"
+        }
+      }
+    CONFIG
+
+    sample "14 juillet 1789" do
+      insist { subject["@timestamp"].time } == Time.iso8601("1789-07-14T00:00:00.000Z").utc
+    end
+  end
+
+  describe "Supporting locale+country in BCP47" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "message", "dd MMMM yyyy" ]
+          locale => "fr-FR"
+          timezone => "UTC"
+        }
+      }
+    CONFIG
+
+    sample "14 juillet 1789" do
+      insist { subject["@timestamp"].time } == Time.iso8601("1789-07-14T00:00:00.000Z").utc
+    end
+  end
+
+  describe "Supporting locale+country in POSIX (internally replace '_' by '-')" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "message", "dd MMMM yyyy" ]
+          locale => "fr_FR"
+          timezone => "UTC"
+        }
+      }
+    CONFIG
+
+    sample "14 juillet 1789" do
+      insist { subject["@timestamp"].time } == Time.iso8601("1789-07-14T00:00:00.000Z").utc
+    end
+  end
+end
diff --git a/spec/filters/dns_spec.rb b/spec/filters/dns_spec.rb
new file mode 100644
index 00000000000..0c430909bce
--- /dev/null
+++ b/spec/filters/dns_spec.rb
@@ -0,0 +1,209 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/filters/dns"
+require "resolv"
+
+describe LogStash::Filters::DNS do
+  before(:each) do
+    allow_any_instance_of(Resolv).to receive(:getaddress).with("carrera.databits.net").and_return("199.192.228.250")
+    allow_any_instance_of(Resolv).to receive(:getaddress).with("does.not.exist").and_raise(Resolv::ResolvError)
+    allow_any_instance_of(Resolv).to receive(:getaddress).with("nonexistanthostname###.net").and_raise(Resolv::ResolvError)
+    allow_any_instance_of(Resolv).to receive(:getname).with("199.192.228.250").and_return("carrera.databits.net")
+    allow_any_instance_of(Resolv).to receive(:getname).with("127.0.0.1").and_return("localhost")
+    allow_any_instance_of(Resolv).to receive(:getname).with("128.0.0.1").and_raise(Resolv::ResolvError)
+    allow_any_instance_of(Resolv).to receive(:getname).with("199.192.228.250").and_return("carrera.databits.net")
+  end
+
+  describe "dns reverse lookup, replace (on a field)" do
+    config <<-CONFIG
+      filter {
+        dns {
+          reverse => "foo"
+          action => "replace"
+        }
+      }
+    CONFIG
+
+    sample("foo" => "199.192.228.250") do
+      insist { subject["foo"] } == "carrera.databits.net"
+    end
+  end
+
+  describe "dns reverse lookup, append" do
+    config <<-CONFIG
+      filter {
+        dns {
+          reverse => "foo"
+          action => "append"
+        }
+      }
+    CONFIG
+
+    sample("foo" => "199.192.228.250") do
+      insist { subject["foo"][0] } == "199.192.228.250"
+      insist { subject["foo"][1] } == "carrera.databits.net"
+    end
+  end
+
+  describe "dns reverse lookup, not an IP" do
+    config <<-CONFIG
+      filter {
+        dns {
+          reverse => "foo"
+        }
+      }
+    CONFIG
+
+    sample("foo" => "not.an.ip") do
+      insist { subject["foo"] } == "not.an.ip"
+    end
+  end
+
+  describe "dns resolve lookup, replace" do
+    config <<-CONFIG
+      filter {
+        dns {
+          resolve => ["host"]
+          action => "replace"
+          add_tag => ["success"]
+        }
+      }
+    CONFIG
+
+    sample("host" => "carrera.databits.net") do
+      insist { subject["host"] } == "199.192.228.250"
+      insist { subject["tags"] } == ["success"]
+    end
+  end
+
+  describe "dns fail resolve lookup, don't add tag" do
+    config <<-CONFIG
+      filter {
+        dns {
+          resolve => ["host1", "host2"]
+          action => "replace"
+          add_tag => ["success"]
+        }
+      }
+    CONFIG
+
+    sample("host1" => "carrera.databits.net", "host2" => "nonexistanthostname###.net") do
+      insist { subject["tags"] }.nil?
+      insist { subject["host1"] } == "carrera.databits.net"
+      insist { subject["host2"] } == "nonexistanthostname###.net"
+    end
+  end
+
+  describe "dns resolves lookups, adds tag" do
+    config <<-CONFIG
+      filter {
+        dns {
+          resolve => ["host1", "host2"]
+          action => "replace"
+          add_tag => ["success"]
+        }
+      }
+    CONFIG
+
+    sample("host1" => "carrera.databits.net", "host2" => "carrera.databits.net") do
+      insist { subject["tags"] } == ["success"]
+    end
+  end
+
+  describe "dns resolves and reverses, fails last, no tag" do
+    config <<-CONFIG
+      filter {
+        dns {
+          resolve => ["host1"]
+          reverse => ["ip1", "ip2"]
+          action => "replace"
+          add_tag => ["success"]
+        }
+      }
+    CONFIG
+
+    sample("host1" => "carrera.databits.net",
+           "ip1" => "127.0.0.1",
+           "ip2" => "128.0.0.1") do
+      insist { subject["tags"] }.nil?
+      insist { subject["host1"] } == "carrera.databits.net"
+      insist { subject["ip1"] } == "127.0.0.1"
+      insist { subject["ip2"] } == "128.0.0.1"
+    end
+  end
+
+  describe "dns resolve lookup, replace (on a field)" do
+    config <<-CONFIG
+      filter {
+        dns {
+          resolve => "foo"
+          action => "replace"
+        }
+      }
+    CONFIG
+
+    sample("foo" => "carrera.databits.net") do
+      insist { subject["foo"] } == "199.192.228.250"
+    end
+  end
+
+  describe "dns resolve lookup, skip multi-value" do
+    config <<-CONFIG
+      filter {
+        dns {
+          resolve => "foo"
+          action => "replace"
+        }
+      }
+    CONFIG
+
+    sample("foo" => ["carrera.databits.net", "foo.databits.net"]) do
+      insist { subject["foo"] } == ["carrera.databits.net", "foo.databits.net"]
+    end
+  end
+
+  describe "dns resolve lookup, append" do
+    config <<-CONFIG
+      filter {
+        dns {
+          resolve => "foo"
+          action => "append"
+        }
+      }
+    CONFIG
+
+    sample("foo" => "carrera.databits.net") do
+      insist { subject["foo"][0] } == "carrera.databits.net"
+      insist { subject["foo"][1] } == "199.192.228.250"
+    end
+  end
+
+  describe "dns resolve lookup, append with multi-value does nothing" do
+    config <<-CONFIG
+      filter {
+        dns {
+          resolve => "foo"
+          action => "append"
+        }
+      }
+    CONFIG
+
+    sample("foo" => ["carrera.databits.net", "foo.databits.net"]) do
+      insist { subject["foo"] } == ["carrera.databits.net", "foo.databits.net"]
+    end
+  end
+
+  describe "dns resolve lookup, not a valid hostname" do
+    config <<-CONFIG
+      filter {
+        dns {
+          resolve=> "foo"
+        }
+      }
+    CONFIG
+
+    sample("foo" => "does.not.exist") do
+      insist { subject["foo"] } == "does.not.exist"
+    end
+  end
+end
diff --git a/spec/filters/drop_spec.rb b/spec/filters/drop_spec.rb
new file mode 100644
index 00000000000..8d8fcb65628
--- /dev/null
+++ b/spec/filters/drop_spec.rb
@@ -0,0 +1,18 @@
+require "spec_helper"
+require "logstash/filters/drop"
+
+describe LogStash::Filters::Drop do
+
+  describe "drop the event" do
+    config <<-CONFIG
+      filter {
+        drop { }
+      }
+    CONFIG
+
+    sample "hello" do
+      insist { subject }.nil?
+    end
+  end
+
+end
diff --git a/spec/filters/filter_chains_spec.rb b/spec/filters/filter_chains_spec.rb
new file mode 100644
index 00000000000..7c956de50ce
--- /dev/null
+++ b/spec/filters/filter_chains_spec.rb
@@ -0,0 +1,122 @@
+# encoding: utf-8
+
+require "spec_helper"
+require "logstash/filters/split"
+require "logstash/filters/clone"
+
+describe LogStash::Filters do
+  
+
+  describe "chain split with mutate filter" do
+    config <<-CONFIG
+      filter {
+        split { }
+        mutate { replace => [ "message", "test" ] }
+      }
+    CONFIG
+
+    sample "hello\nbird" do
+      insist { subject.length } == 2
+      insist { subject[0]["message"] } == "test"
+      insist { subject[1]["message"] } == "test"
+    end
+  end
+
+
+  describe "new events bug #793" do
+    config <<-CONFIG
+      filter {
+        split { terminator => "," }
+        mutate { rename => { "message" => "fancypants" } }
+      }
+    CONFIG
+
+    sample "hello,world" do
+      insist { subject.length } == 2
+      insist { subject[0]["fancypants"] } == "hello"
+      insist { subject[1]["fancypants"] } == "world"
+    end
+  end
+
+  describe "split then multiple mutate" do
+    config <<-CONFIG
+      filter {
+        split { }
+        mutate { replace => [ "message", "test" ] }
+        mutate { replace => [ "message", "test2" ] }
+        mutate { replace => [ "message", "test3" ] }
+        mutate { replace => [ "message", "test4" ] }
+      }
+    CONFIG
+
+    sample "big\nbird" do
+      insist { subject.length } == 2
+      insist { subject[0]["message"] } == "test4"
+      insist { subject[1]["message"] } == "test4"
+    end
+  end
+
+  describe "split then clone" do
+    config <<-CONFIG
+      filter {
+        split { }
+        clone { clones => ['clone1', 'clone2'] }
+      }
+    CONFIG
+
+    sample "big\nbird" do
+      insist { subject.length } == 6
+
+      insist { subject[0]["message"] } == "big"
+      insist { subject[0]["type"] } == nil
+
+      insist { subject[1]["message"] } == "big"
+      insist { subject[1]["type"] } == "clone1"
+
+      insist { subject[2]["message"] } == "big"
+      insist { subject[2]["type"] } == "clone2"
+
+      insist { subject[3]["message"] } == "bird"
+      insist { subject[3]["type"] } == nil
+
+      insist { subject[4]["message"] } == "bird"
+      insist { subject[4]["type"] } == "clone1"
+
+      insist { subject[5]["message"] } == "bird"
+      insist { subject[5]["type"] } == "clone2"
+    end
+  end
+
+  describe "clone with conditionals, see bug #1548" do
+    type "original"
+    config <<-CONFIG
+      filter {
+        clone {
+          clones => ["clone"]
+        }
+        if [type] == "clone" {
+          mutate { add_field => { "clone" => "true" } }
+        } else {
+          mutate { add_field => { "original" => "true" } }
+        }
+      }
+    CONFIG
+
+    sample("message" => "hello world") do
+      insist { subject }.is_a? Array
+      # subject.each{|event| puts(event.inspect + "\n")}
+      insist { subject.length } == 2
+
+      insist { subject.first["type"] } == nil
+      insist { subject.first["original"] } == "true"
+      insist { subject.first["clone"]} == nil
+      insist { subject.first["message"] } == "hello world"
+
+      insist { subject.last["type"]} == "clone"
+      insist { subject.last["original"] } == nil
+      insist { subject.last["clone"]} == "true"
+      insist { subject.last["message"] } == "hello world"
+    end
+  end
+
+end
diff --git a/spec/filters/fingerprint_spec.rb b/spec/filters/fingerprint_spec.rb
new file mode 100644
index 00000000000..346ed84f621
--- /dev/null
+++ b/spec/filters/fingerprint_spec.rb
@@ -0,0 +1,200 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/filters/fingerprint"
+
+describe LogStash::Filters::Fingerprint do
+
+  describe "fingerprint ipaddress with IPV4_NETWORK method" do
+    config <<-CONFIG
+      filter {
+        fingerprint {
+          source => ["clientip"]
+          method => "IPV4_NETWORK"
+          key => 24
+        }
+      }
+    CONFIG
+
+    sample("clientip" => "233.255.13.44") do
+      insist { subject["fingerprint"] } == "233.255.13.0"
+    end
+  end
+
+  describe "fingerprint string with MURMUR3 method" do
+    config <<-CONFIG
+      filter {
+        fingerprint {
+          source => ["clientip"]
+          method => "MURMUR3"
+        }
+      }
+    CONFIG
+
+    sample("clientip" => "123.52.122.33") do
+      insist { subject["fingerprint"] } == 1541804874
+    end
+  end
+
+   describe "fingerprint string with SHA1 alogrithm" do
+    config <<-CONFIG
+      filter {
+        fingerprint {
+          source => ["clientip"]
+          key => "longencryptionkey"
+          method => 'SHA1'
+        }
+      }
+    CONFIG
+
+    sample("clientip" => "123.123.123.123") do
+      insist { subject["fingerprint"] } == "fdc60acc4773dc5ac569ffb78fcb93c9630797f4"
+    end
+  end
+
+  describe "fingerprint string with SHA256 alogrithm" do
+    config <<-CONFIG
+      filter {
+        fingerprint {
+          source => ["clientip"]
+          key => "longencryptionkey"
+          method => 'SHA256'
+        }
+      }
+    CONFIG
+
+    sample("clientip" => "123.123.123.123") do
+      insist { subject["fingerprint"] } == "345bec3eff242d53b568916c2610b3e393d885d6b96d643f38494fd74bf4a9ca"
+    end
+  end
+
+  describe "fingerprint string with SHA384 alogrithm" do
+    config <<-CONFIG
+      filter {
+        fingerprint {
+          source => ["clientip"]
+          key => "longencryptionkey"
+          method => 'SHA384'
+        }
+      }
+    CONFIG
+
+    sample("clientip" => "123.123.123.123") do
+      insist { subject["fingerprint"] } == "22d4c0e8c4fbcdc4887d2038fca7650f0e2e0e2457ff41c06eb2a980dded6749561c814fe182aff93e2538d18593947a"
+    end
+  end
+
+  describe "fingerprint string with SHA512 alogrithm" do
+    config <<-CONFIG
+      filter {
+        fingerprint {
+          source => ["clientip"]
+          key => "longencryptionkey"
+          method => 'SHA512'
+        }
+      }
+    CONFIG
+
+    sample("clientip" => "123.123.123.123") do
+      insist { subject["fingerprint"] } == "11c19b326936c08d6c50a3c847d883e5a1362e6a64dd55201a25f2c1ac1b673f7d8bf15b8f112a4978276d573275e3b14166e17246f670c2a539401c5bfdace8"
+    end
+  end
+
+  describe "fingerprint string with MD5 alogrithm" do
+    config <<-CONFIG
+      filter {
+        fingerprint {
+          source => ["clientip"]
+          key => "longencryptionkey"
+          method => 'MD5'
+        }
+      }
+    CONFIG
+
+    sample("clientip" => "123.123.123.123") do
+      insist { subject["fingerprint"] } == "9336c879e305c9604a3843fc3e75948f"
+    end
+  end
+
+  describe "Test field with multiple values" do
+    config <<-CONFIG
+      filter {
+        fingerprint {
+          source => ["clientip"]
+          key => "longencryptionkey"
+          method => 'MD5'
+        }
+      }
+    CONFIG
+
+    sample("clientip" => [ "123.123.123.123", "223.223.223.223" ]) do
+      insist { subject["fingerprint"]} == [ "9336c879e305c9604a3843fc3e75948f", "7a6c66b8d3f42a7d650e3354af508df3" ]
+    end
+  end
+
+  describe "Concatenate multiple values into 1" do
+    config <<-CONFIG
+      filter {
+        fingerprint {
+          source => ['field1', 'field2']
+          key => "longencryptionkey"
+          method => 'MD5'
+        }
+      }
+    CONFIG
+
+    sample("field1" => "test1", "field2" => "test2") do
+      insist { subject["fingerprint"]} == "872da745e45192c2a1d4bf7c1ff8a370"
+    end
+  end
+
+  describe "PUNCTUATION method" do
+    config <<-CONFIG
+      filter {
+        fingerprint {
+          source => 'field1'
+          method => 'PUNCTUATION'
+        }
+      }
+    CONFIG
+
+    sample("field1" =>  "PHP Warning:  json_encode() [<a href='function.json-encode'>function.json-encode</a>]: Invalid UTF-8 sequence in argument in /var/www/htdocs/test.php on line 233") do
+      insist { subject["fingerprint"] } == ":_()[<='.-'>.-</>]:-////."
+    end
+  end
+
+  context 'Timestamps' do
+    epoch_time = Time.at(0).gmtime
+
+    describe 'OpenSSL Fingerprinting' do
+      config <<-CONFIG
+        filter {
+          fingerprint {
+            source => ['@timestamp']
+            key    => '0123'
+            method => 'SHA1'
+          }
+        }
+      CONFIG
+
+      sample("@timestamp" => epoch_time) do
+        insist { subject["fingerprint"] } == '1d5379ec92d86a67cfc642d55aa050ca312d3b9a'
+      end
+    end
+
+    describe 'MURMUR3 Fingerprinting' do
+      config <<-CONFIG
+        filter {
+          fingerprint {
+            source => ['@timestamp']
+            method => 'MURMUR3'
+          }
+        }
+      CONFIG
+
+      sample("@timestamp" => epoch_time) do
+        insist { subject["fingerprint"] } == 743372282
+      end
+    end
+  end
+
+end
diff --git a/spec/filters/geoip_spec.rb b/spec/filters/geoip_spec.rb
new file mode 100644
index 00000000000..1ecb56b6470
--- /dev/null
+++ b/spec/filters/geoip_spec.rb
@@ -0,0 +1,120 @@
+require "spec_helper"
+require "logstash/filters/geoip"
+
+describe LogStash::Filters::GeoIP do
+
+  describe "defaults" do
+    config <<-CONFIG
+      filter {
+        geoip { 
+          source => "ip"
+          #database => "vendor/geoip/GeoLiteCity.dat"
+        }
+      }
+    CONFIG
+
+    sample("ip" => "8.8.8.8") do
+      insist { subject }.include?("geoip")
+
+      expected_fields = %w(ip country_code2 country_code3 country_name
+                           continent_code region_name city_name postal_code
+                           latitude longitude dma_code area_code timezone
+                           location )
+      expected_fields.each do |f|
+        insist { subject["geoip"] }.include?(f)
+      end
+    end
+
+    sample("ip" => "127.0.0.1") do
+      # assume geoip fails on localhost lookups
+      reject { subject }.include?("geoip")
+    end
+  end
+
+  describe "Specify the target" do
+    config <<-CONFIG
+      filter {
+        geoip { 
+          source => "ip"
+          #database => "vendor/geoip/GeoLiteCity.dat"
+          target => src_ip
+        }
+      }
+    CONFIG
+
+    sample("ip" => "8.8.8.8") do
+      insist { subject }.include?("src_ip")
+
+      expected_fields = %w(ip country_code2 country_code3 country_name
+                           continent_code region_name city_name postal_code
+                           latitude longitude dma_code area_code timezone
+                           location )
+      expected_fields.each do |f|
+        insist { subject["src_ip"] }.include?(f)
+      end
+    end
+
+    sample("ip" => "127.0.0.1") do
+      # assume geoip fails on localhost lookups
+      reject { subject }.include?("src_ip")
+    end
+  end
+
+  describe "correct encodings with default db" do
+    config <<-CONFIG
+      filter {
+        geoip {
+          source => "ip"
+        }
+      }
+    CONFIG
+    expected_fields = %w(ip country_code2 country_code3 country_name
+                           continent_code region_name city_name postal_code
+                           dma_code area_code timezone)
+
+    sample("ip" => "1.1.1.1") do
+      checked = 0
+      expected_fields.each do |f|
+        next unless subject["geoip"][f]
+        checked += 1
+        insist { subject["geoip"][f].encoding } == Encoding::UTF_8
+      end
+      insist { checked } > 0
+    end
+    sample("ip" => "189.2.0.0") do
+      checked = 0
+      expected_fields.each do |f|
+        next unless subject["geoip"][f]
+        checked += 1
+        insist { subject["geoip"][f].encoding } == Encoding::UTF_8
+      end
+      insist { checked } > 0
+    end
+
+  end
+
+  describe "correct encodings with ASN db" do
+    config <<-CONFIG
+      filter {
+        geoip {
+          source => "ip"
+          database => "vendor/geoip/GeoIPASNum.dat"
+        }
+      }
+    CONFIG
+
+
+    sample("ip" => "1.1.1.1") do
+      insist { subject["geoip"]["asn"].encoding } == Encoding::UTF_8
+    end
+    sample("ip" => "187.2.0.0") do
+      insist { subject["geoip"]["asn"].encoding } == Encoding::UTF_8
+    end
+    sample("ip" => "189.2.0.0") do
+      insist { subject["geoip"]["asn"].encoding } == Encoding::UTF_8
+    end
+    sample("ip" => "161.24.0.0") do
+      insist { subject["geoip"]["asn"].encoding } == Encoding::UTF_8
+    end
+  end
+end
diff --git a/spec/filters/grok-patterns/java_spec.rb b/spec/filters/grok-patterns/java_spec.rb
new file mode 100644
index 00000000000..7423cec2415
--- /dev/null
+++ b/spec/filters/grok-patterns/java_spec.rb
@@ -0,0 +1,157 @@
+# encoding: utf-8
+require "spec_helper"
+
+# Test suite for the grok patterns defined in patterns/java
+# For each pattern:
+#  - a sample is considered valid i.e. "should match"  where message == result
+#  - a sample is considered invalid i.e. "should NOT match"  where message != result
+#
+describe "java grok pattern" do
+
+  describe "JAVACLASS" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{JAVACLASS:result}" }
+        }
+      }
+    CONFIG
+
+    context "should match" do
+      [
+        "package.Class",
+        "package.class", #camel case is not mandatory
+        "package.subpackage.Class",
+        "package._subpackage.Class",
+        "package._.Class",
+        "package.Class$InnerClass", #java inner class
+        "classWithNoPackage",
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} == message
+        end
+      end
+    end
+
+    context "should NOT match" do
+      [
+        "package.Illegal!Class",
+        "illegal.!package.Class",
+        "package-with-hyphen.Class",
+        "123package.Class",
+        "package.123Class",
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} != message
+        end
+      end
+    end
+  end
+
+  describe "JAVAMETHOD" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{JAVAMETHOD:result}" }
+        }
+      }
+    CONFIG
+
+    context "should match" do
+      [
+        "methodName",
+        "method_name",
+        "methodWithNumber0",
+        "_method",
+        "_",
+        "<init>", # Special constructor method
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} == message
+        end
+      end
+    end
+
+    context "should NOT match" do
+      [
+        "method-name",
+        "method!name",
+        "method.name",
+        "method>name",
+        "<notinit>",
+        "-",
+        "123method",
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} != message
+        end
+      end
+    end
+  end
+
+  describe "JAVAFILE" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{JAVAFILE:result}" }
+        }
+      }
+    CONFIG
+
+    context "should match" do
+      [
+        "Wombat.java",
+        "CacheAwareContextLoaderDelegate.java",
+        "Native Method",
+        "Unknown Source",
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} == message
+        end
+      end
+    end
+
+    context "should NOT match" do
+      [
+         #Sorry no idea
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} != message
+        end
+      end
+    end
+  end
+
+  describe "JAVASTACKTRACEPART" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{JAVASTACKTRACEPART:result}" }
+        }
+      }
+    CONFIG
+
+    context "should match" do
+      [
+        "at com.xyz.Wombat(Wombat.java:57)",
+        "at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)",
+        "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
+        "at org.jnp.server.NamingServer_Stub.lookup(Unknown Source)",
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} == message
+        end
+      end
+    end
+
+    context "should NOT match" do
+      [
+        #Sorry no idea
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} != message
+        end
+      end
+    end
+  end
+end
diff --git a/spec/filters/grok/timeout1_spec.rb b/spec/filters/grok/timeout1_spec.rb
new file mode 100644
index 00000000000..3d165856184
--- /dev/null
+++ b/spec/filters/grok/timeout1_spec.rb
@@ -0,0 +1,40 @@
+require "spec_helper"
+require "grok-pure"
+require "timeout"
+
+describe "grok known timeout failures" do
+  describe "more apache log timeouts" do
+    subject { Grok.new }
+    before :each do
+      patterns = Dir.glob(File.join(File.dirname(__FILE__), "../../../patterns/*"))
+      patterns.each { |path| subject.add_patterns_from_file(path) }
+      subject.add_pattern("RESPONSE_BYTES", '[0-9_-]+')
+      subject.add_pattern("POST_CONN_STATUS", '[+-X]')
+      subject.add_pattern("CUSTOMAPACHELOG", '%{HOSTNAME:servername} %{IP:remote_host} %{NONNEGINT:servetime_secs} \[%{HTTPDATE:apache_timestamp}\] \"%{WORD:method} %{URIPATH:url}\" %{QS:query} %{POSINT:status} %{RESPONSE_BYTES:bytes} %{POST_CONN_STATUS:post_conn_status} \"(?:%{URI:referrer}|-)\" %{QS:agent} %{QS:cookie}')
+      subject.compile("%{CUSTOMAPACHELOG}")
+    end
+
+    it "should not timeout" do
+      data = File.open(__FILE__); data.each { |line| break if line == "__END__\n" }
+      # puts subject.expanded_pattern
+      data.each do |line|
+        # This timeout will toss an exception if it takes too long.
+        Timeout.timeout(1) do
+          subject.match(line.chomp)
+          # puts :matched => subject.match(line.chomp)
+        end
+      end
+    end
+  end
+end
+
+__END__
+example.com 11.22.33.44 1 [24/Oct/2012:08:47:14 +0200] "GET /need-russia-foo.php" "?login=100536&gclid=CJHRrKeHmbMCFWbKtAodn24ArA" 200 145492 + "http://www.google.de/#hl=de&sclient=psy-ab&q=foo+russland+kosten&oq=foo+rus&gs_l=hp.1.1.0l4.0.0.1.297.0.0.0.0.0.0.0.0..0.0...0.0...1c.uULgSTd5tzc&pbx=1&bav=on.2,or.r_gc.r_pw.r_qf.&fp=c4affe4f526437d4&bpcl=35466521&biw=1600&bih=799" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: -" "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
+example.com 11.22.33.44 0 [24/Oct/2012:08:47:18 +0200] "GET /ajax/ajax.fillPurposes2.php" "?destination=RUS&nationality=DEU&state=&localCode=DE" 200 271 + "http://example.com/need-russia-foo.php?login=100536&gclid=CJHRrKeHmbMCFWbKtAodn24ArA" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: PHPSESSID=b9bctuvsf2bdjcorbpogdkbll6p4p8uj; code=100536; __utma=256812172.1575809976.1351061239.1351061239.1351061239.1; __utmb=256812172.2.10.1351061239; __utmc=256812172; __utmz=256812172.1351061239.1.1.utmgclid=CJHRrKeHmbMCFWbKtAodn24ArA|utmccn=(not%20set)|utmcmd=(not%20set)|utmctr=foo%20russland%20kosten; __utmv=256812172.100536" "Accept: application/json, text/javascript, */*; q=0.01" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
+example.com 11.22.33.44 0 [24/Oct/2012:08:49:30 +0200] "GET /ajax/ajax.fillPurposes2.php" "?destination=RUS&nationality=DEU&state=Baden-W%C3%BCrttemberg&localCode=DE" 200 271 + "http://example.com/need-russia-foo.php?login=100536&gclid=CJHRrKeHmbMCFWbKtAodn24ArA" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: PHPSESSID=b9bctuvsf2bdjcorbpogdkbll6p4p8uj; code=100536; __utma=256812172.1575809976.1351061239.1351061239.1351061239.1; __utmb=256812172.2.10.1351061239; __utmc=256812172; __utmz=256812172.1351061239.1.1.utmgclid=CJHRrKeHmbMCFWbKtAodn24ArA|utmccn=(not%20set)|utmcmd=(not%20set)|utmctr=foo%20russland%20kosten; __utmv=256812172.100536" "Accept: application/json, text/javascript, */*; q=0.01" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
+example.com 11.22.33.44 0 [24/Oct/2012:08:49:41 +0200] "GET /ajax/ajax.fooPopup.php" "?passport_from=DEU&state_of_residence=Baden-W%C3%BCrttemberg&traveling_to[0]=RUS&traveling_for%5B0%5D=P&traveling_to%5B1%5D=&traveling_for%5B1%5D=&traveling_to%5B2%5D=&traveling_for%5B2%5D=&traveling_to%5B3%5D=&traveling_for%5B3%5D=&account_number=100536&account_exists=N" 200 2459 + "http://example.com/need-russia-foo.php?login=100536&gclid=CJHRrKeHmbMCFWbKtAodn24ArA" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: PHPSESSID=b9bctuvsf2bdjcorbpogdkbll6p4p8uj; code=100536; __utma=256812172.1575809976.1351061239.1351061239.1351061239.1; __utmb=256812172.2.10.1351061239; __utmc=256812172; __utmz=256812172.1351061239.1.1.utmgclid=CJHRrKeHmbMCFWbKtAodn24ArA|utmccn=(not%20set)|utmcmd=(not%20set)|utmctr=foo%20russland%20kosten; __utmv=256812172.100536" "Accept: text/html, */*; q=0.01" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
+example.com 11.22.33.44 0 [24/Oct/2012:08:49:45 +0200] "GET /ajax/ajax.fooPopupValid.php" "?codeId=1588744&countryCode=RUS&travelingFor=P&entry=S&passportFrom=DEU&_=1351061386611" 200 490 + "http://example.com/need-russia-foo.php?login=100536&gclid=CJHRrKeHmbMCFWbKtAodn24ArA" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: PHPSESSID=b9bctuvsf2bdjcorbpogdkbll6p4p8uj; code=100536; __utma=256812172.1575809976.1351061239.1351061239.1351061239.1; __utmb=256812172.2.10.1351061239; __utmc=256812172; __utmz=256812172.1351061239.1.1.utmgclid=CJHRrKeHmbMCFWbKtAodn24ArA|utmccn=(not%20set)|utmcmd=(not%20set)|utmctr=foo%20russland%20kosten; __utmv=256812172.100536" "Accept: text/javascript, application/javascript, application/ecmascript, application/x-ecmascript, */*; q=0.01" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
+example.com 11.22.33.44 0 [24/Oct/2012:08:49:50 +0200] "POST /ajax/ajax.fooPopup.php" "" 302 - + "http://example.com/need-russia-foo.php?login=100536&gclid=CJHRrKeHmbMCFWbKtAodn24ArA" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: PHPSESSID=b9bctuvsf2bdjcorbpogdkbll6p4p8uj; code=100536; __utma=256812172.1575809976.1351061239.1351061239.1351061239.1; __utmb=256812172.2.10.1351061239; __utmc=256812172; __utmz=256812172.1351061239.1.1.utmgclid=CJHRrKeHmbMCFWbKtAodn24ArA|utmccn=(not%20set)|utmcmd=(not%20set)|utmctr=foo%20russland%20kosten; __utmv=256812172.100536" "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
+example.com 11.22.33.44 1 [24/Oct/2012:08:49:50 +0200] "GET /requirements.php" "" 200 23780 + "http://example.com/need-russia-foo.php?login=100536&gclid=CJHRrKeHmbMCFWbKtAodn24ArA" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: PHPSESSID=b9bctuvsf2bdjcorbpogdkbll6p4p8uj; code=100536; __utma=256812172.1575809976.1351061239.1351061239.1351061239.1; __utmb=256812172.2.10.1351061239; __utmc=256812172; __utmz=256812172.1351061239.1.1.utmgclid=CJHRrKeHmbMCFWbKtAodn24ArA|utmccn=(not%20set)|utmcmd=(not%20set)|utmctr=foo%20russland%20kosten; __utmv=256812172.100536" "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
+example.com 11.22.33.44 1 [24/Oct/2012:08:49:52 +0200] "GET /ajax/ajax.requirementsFeesTable.php" "?text=RUS_0" 200 1406 + "http://example.com/requirements.php" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: PHPSESSID=b9bctuvsf2bdjcorbpogdkbll6p4p8uj; code=100536; __utma=256812172.1575809976.1351061239.1351061239.1351061239.1; __utmb=256812172.2.10.1351061239; __utmc=256812172; __utmz=256812172.1351061239.1.1.utmgclid=CJHRrKeHmbMCFWbKtAodn24ArA|utmccn=(not%20set)|utmcmd=(not%20set)|utmctr=foo%20russland%20kosten; __utmv=256812172.100536; 100536-AB-/eta-requirements=%2Feta-requirements; 100536-AB-/esta-requirements=%2Festa-requirements" "Accept: */*" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
+example.com 194.127.209.156 0 [24/Oct/2012:10:54:06 +0200] "GET /ajax/ajax.fillPurposes2.php" "?destination=SAU&nationality=AUT&state=&localCode=DE" 200 236 + "http://example.com/foo.php?traveling_to=SAU&traveling_for=B&nationality=AUT&login=524412&state_of_residence=Baden-W\xc3\xbcrttemberg&use_lang=de&utm_source=trans&utm_medium=email&utm_campaign=cir-foo" "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)" "Cookie: PHPSESSID=h9gbvo04nomip8ca5ljko3cprrkttq9 ; code=524412" "Accept: application/json, text/javascript, */*; q=0.01" "Lang: de" "Encoding: gzip, deflate" "Charset: -"
diff --git a/spec/filters/grok/timeout2_spec.rb b/spec/filters/grok/timeout2_spec.rb
new file mode 100644
index 00000000000..11332c9c269
--- /dev/null
+++ b/spec/filters/grok/timeout2_spec.rb
@@ -0,0 +1,59 @@
+require "spec_helper"
+require "grok-pure"
+require "timeout"
+
+describe "grok known timeout failures" do
+  
+
+  describe "user reported timeout" do
+    config <<-'CONFIG'
+      filter {
+        grok {
+         match  => { "message" => "%{SYSLOGBASE:ts1} \[\#\|%{TIMESTAMP_ISO8601:ts2}\|%{DATA} for %{PATH:url} = %{POSINT:delay} ms.%{GREEDYDATA}" }
+        }
+      }
+    CONFIG
+
+    start = Time.now
+    line = 'Nov 13 19:23:34 qa-api1 glassfish: [#|2012-11-13T19:23:25.604+0000|INFO|glassfish3.1.2|com.locusenergy.platform.messages.LocusMessage|_ThreadID=59;_ThreadName=Thread-2;|API TIMER - Cache HIT user: null for /kiosks/194/energyreadings/data?tz=America/New_York&fields=kwh&type=gen&end=2012-11-13T23:59:59&start=2010-12-16T00:00:00-05:00&gran=yearly = 5 ms.|#]'
+
+    sample line do
+      duration = Time.now - start
+      # insist { duration } < 0.03  #TODO refactor performance tests
+    end
+  end
+
+  describe "user reported timeout" do
+    config <<-'CONFIG'
+      filter {
+        grok {
+          pattern => [
+            "%{DATA:http_host} %{IPORHOST:clientip} %{USER:ident} %{USER:http_auth} \[%{HTTPDATE:http_timestamp}\] \"%{WORD:http_method} %{DATA:http_request} HTTP/%{NUMBER:http_version}\" %{NUMBER:http_response_code} (?:%{NUMBER:bytes}|-) \"(?:%{URI:http_referrer}|-)\" %{QS:http_user_agent} %{QS:http_x_forwarded_for} %{USER:ssl_chiper} %{NUMBER:request_time} (?:%{DATA:gzip_ratio}|-) (?:%{DATA:upstream}|-) (?:%{NUMBER:upstream_time}|-) (?:%{WORD:geoip_country}|-)",
+            "%{DATA:http_host} %{IPORHOST:clientip} %{USER:ident} %{USER:http_auth} \[%{HTTPDATE:http_timestamp}\] \"%{WORD:http_method} %{DATA:http_request} HTTP/%{NUMBER:http_version}\" %{NUMBER:http_response_code} (?:%{NUMBER:bytes}|-) \"(?:%{URI:http_referrer}|-)\" %{QS:http_user_agent} %{QS:http_x_forwarded_for} %{USER:ssl_chiper} %{NUMBER:request_time} (?:%{DATA:gzip_ratio}|-) (?:%{DATA:upstream}|-) (?:%{NUMBER:upstream_time}|-)"
+          ]
+        }
+      }
+    CONFIG
+
+    #TODO fixme
+
+    # start = Time.now
+    # sample 'www.example.com 10.6.10.13 - - [09/Aug/2012:16:19:39 +0200] "GET /index.php HTTP/1.1" 403 211 "-" "Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.8.1.12) Gecko/20080201 Firefox/2.0.0.12" "-" - 0.019 - 10.6.10.12:81 0.002 US' do
+    #   duration = Time.now - start
+    #   # insist { duration } < 1  #TODO refactor performance tests
+    #   puts( subject["tags"])
+    #   reject { subject["tags"] }.include?("_grokparsefailure")
+    #   insist { subject["geoip_country"] } == ["US"]
+    # end
+
+
+    # sample 'www.example.com 10.6.10.13 - - [09/Aug/2012:16:19:39 +0200] "GET /index.php HTTP/1.1" 403 211 "-" "Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.8.1.12) Gecko/20080201 Firefox/2.0.0.12" "-" - 0.019 - 10.6.10.12:81 0.002 -' do
+    #   duration = Time.now - start
+    #   # insist { duration } < 1 #TODO refactor performance tests
+    #   reject { subject["tags"] }.include?("_grokparsefailure")
+    #   insist { subject["geoip_country"].nil? } == true
+    # end
+  end
+end
+
+__END__
diff --git a/spec/filters/grok_spec.rb b/spec/filters/grok_spec.rb
new file mode 100644
index 00000000000..ac192a66c20
--- /dev/null
+++ b/spec/filters/grok_spec.rb
@@ -0,0 +1,648 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/filters/grok"
+
+describe LogStash::Filters::Grok do
+
+  describe "simple syslog line" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{SYSLOGLINE}" }
+          singles => true
+          overwrite => [ "message" ]
+        }
+      }
+    CONFIG
+
+    sample "Mar 16 00:01:25 evita postfix/smtpd[1713]: connect from camomile.cloud9.net[168.100.1.3]" do
+      insist { subject["tags"] }.nil?
+      insist { subject["logsource"] } == "evita"
+      insist { subject["timestamp"] } == "Mar 16 00:01:25"
+      insist { subject["message"] } == "connect from camomile.cloud9.net[168.100.1.3]"
+      insist { subject["program"] } == "postfix/smtpd"
+      insist { subject["pid"] } == "1713"
+    end
+  end
+
+  describe "ietf 5424 syslog line" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{SYSLOG5424LINE}" }
+          singles => true
+        }
+      }
+    CONFIG
+
+    sample "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug 4123 - [id1 foo=\"bar\"][id2 baz=\"something\"] Hello, syslog." do
+      insist { subject["tags"] }.nil?
+      insist { subject["syslog5424_pri"] } == "191"
+      insist { subject["syslog5424_ver"] } == "1"
+      insist { subject["syslog5424_ts"] } == "2009-06-30T18:30:00+02:00"
+      insist { subject["syslog5424_host"] } == "paxton.local"
+      insist { subject["syslog5424_app"] } == "grokdebug"
+      insist { subject["syslog5424_proc"] } == "4123"
+      insist { subject["syslog5424_msgid"] } == nil
+      insist { subject["syslog5424_sd"] } == "[id1 foo=\"bar\"][id2 baz=\"something\"]"
+      insist { subject["syslog5424_msg"] } == "Hello, syslog."
+    end
+
+    sample "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug - - [id1 foo=\"bar\"] No process ID." do
+      insist { subject["tags"] }.nil?
+      insist { subject["syslog5424_pri"] } == "191"
+      insist { subject["syslog5424_ver"] } == "1"
+      insist { subject["syslog5424_ts"] } == "2009-06-30T18:30:00+02:00"
+      insist { subject["syslog5424_host"] } == "paxton.local"
+      insist { subject["syslog5424_app"] } == "grokdebug"
+      insist { subject["syslog5424_proc"] } == nil
+      insist { subject["syslog5424_msgid"] } == nil
+      insist { subject["syslog5424_sd"] } == "[id1 foo=\"bar\"]"
+      insist { subject["syslog5424_msg"] } == "No process ID."
+    end
+
+    sample "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug 4123 - - No structured data." do
+      insist { subject["tags"] }.nil?
+      insist { subject["syslog5424_pri"] } == "191"
+      insist { subject["syslog5424_ver"] } == "1"
+      insist { subject["syslog5424_ts"] } == "2009-06-30T18:30:00+02:00"
+      insist { subject["syslog5424_host"] } == "paxton.local"
+      insist { subject["syslog5424_app"] } == "grokdebug"
+      insist { subject["syslog5424_proc"] } == "4123"
+      insist { subject["syslog5424_msgid"] } == nil
+      insist { subject["syslog5424_sd"] } == nil
+      insist { subject["syslog5424_msg"] } == "No structured data."
+    end
+
+    sample "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug - - - No PID or SD." do
+      insist { subject["tags"] }.nil?
+      insist { subject["syslog5424_pri"] } == "191"
+      insist { subject["syslog5424_ver"] } == "1"
+      insist { subject["syslog5424_ts"] } == "2009-06-30T18:30:00+02:00"
+      insist { subject["syslog5424_host"] } == "paxton.local"
+      insist { subject["syslog5424_app"] } == "grokdebug"
+      insist { subject["syslog5424_proc"] } == nil
+      insist { subject["syslog5424_msgid"] } == nil
+      insist { subject["syslog5424_sd"] } == nil
+      insist { subject["syslog5424_msg"] } == "No PID or SD."
+    end
+
+    sample "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug 4123 -  Missing structured data." do
+      insist { subject["tags"] }.nil?
+      insist { subject["syslog5424_pri"] } == "191"
+      insist { subject["syslog5424_ver"] } == "1"
+      insist { subject["syslog5424_ts"] } == "2009-06-30T18:30:00+02:00"
+      insist { subject["syslog5424_host"] } == "paxton.local"
+      insist { subject["syslog5424_app"] } == "grokdebug"
+      insist { subject["syslog5424_proc"] } == "4123"
+      insist { subject["syslog5424_msgid"] } == nil
+      insist { subject["syslog5424_sd"] } == nil
+      insist { subject["syslog5424_msg"] } == "Missing structured data."
+    end
+
+    sample "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug  4123 - - Additional spaces." do
+      insist { subject["tags"] }.nil?
+      insist { subject["syslog5424_pri"] } == "191"
+      insist { subject["syslog5424_ver"] } == "1"
+      insist { subject["syslog5424_ts"] } == "2009-06-30T18:30:00+02:00"
+      insist { subject["syslog5424_host"] } == "paxton.local"
+      insist { subject["syslog5424_app"] } == "grokdebug"
+      insist { subject["syslog5424_proc"] } == "4123"
+      insist { subject["syslog5424_msgid"] } == nil
+      insist { subject["syslog5424_sd"] } == nil
+      insist { subject["syslog5424_msg"] } == "Additional spaces."
+    end
+
+    sample "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug  4123 -  Additional spaces and missing SD." do
+      insist { subject["tags"] }.nil?
+      insist { subject["syslog5424_pri"] } == "191"
+      insist { subject["syslog5424_ver"] } == "1"
+      insist { subject["syslog5424_ts"] } == "2009-06-30T18:30:00+02:00"
+      insist { subject["syslog5424_host"] } == "paxton.local"
+      insist { subject["syslog5424_app"] } == "grokdebug"
+      insist { subject["syslog5424_proc"] } == "4123"
+      insist { subject["syslog5424_msgid"] } == nil
+      insist { subject["syslog5424_sd"] } == nil
+      insist { subject["syslog5424_msg"] } == "Additional spaces and missing SD."
+    end
+
+    sample "<30>1 2014-04-04T16:44:07+02:00 osctrl01 dnsmasq-dhcp 8048 - -  Appname contains a dash" do
+      insist { subject["tags"] }.nil?
+      insist { subject["syslog5424_pri"] } == "30"
+      insist { subject["syslog5424_ver"] } == "1"
+      insist { subject["syslog5424_ts"] } == "2014-04-04T16:44:07+02:00"
+      insist { subject["syslog5424_host"] } == "osctrl01"
+      insist { subject["syslog5424_app"] } == "dnsmasq-dhcp"
+      insist { subject["syslog5424_proc"] } == "8048"
+      insist { subject["syslog5424_msgid"] } == nil
+      insist { subject["syslog5424_sd"] } == nil
+      insist { subject["syslog5424_msg"] } == "Appname contains a dash"
+    end
+
+    sample "<30>1 2014-04-04T16:44:07+02:00 osctrl01 - 8048 - -  Appname is nil" do
+      insist { subject["tags"] }.nil?
+      insist { subject["syslog5424_pri"] } == "30"
+      insist { subject["syslog5424_ver"] } == "1"
+      insist { subject["syslog5424_ts"] } == "2014-04-04T16:44:07+02:00"
+      insist { subject["syslog5424_host"] } == "osctrl01"
+      insist { subject["syslog5424_app"] } == nil
+      insist { subject["syslog5424_proc"] } == "8048"
+      insist { subject["syslog5424_msgid"] } == nil
+      insist { subject["syslog5424_sd"] } == nil
+      insist { subject["syslog5424_msg"] } == "Appname is nil"
+    end
+  end
+
+  describe "parsing an event with multiple messages (array of strings)", :if => false do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "(?:hello|world) %{NUMBER}" }
+          named_captures_only => false
+        }
+      }
+    CONFIG
+
+    sample("message" => [ "hello 12345", "world 23456" ]) do
+      insist { subject["NUMBER"] } == [ "12345", "23456" ]
+    end
+  end
+
+  describe "coercing matched values" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{NUMBER:foo:int} %{NUMBER:bar:float}" }
+          singles => true
+        }
+      }
+    CONFIG
+
+    sample "400 454.33" do
+      insist { subject["foo"] } == 400
+      insist { subject["foo"] }.is_a?(Fixnum)
+      insist { subject["bar"] } == 454.33
+      insist { subject["bar"] }.is_a?(Float)
+    end
+  end
+
+  describe "in-line pattern definitions" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{FIZZLE=\\d+}" }
+          named_captures_only => false
+          singles => true
+        }
+      }
+    CONFIG
+
+    sample "hello 1234" do
+      insist { subject["FIZZLE"] } == "1234"
+    end
+  end
+
+  describe "processing selected fields" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{WORD:word}" }
+          match => { "examplefield" => "%{NUMBER:num}" }
+          break_on_match => false
+          singles => true
+        }
+      }
+    CONFIG
+
+    sample("message" => "hello world", "examplefield" => "12345") do
+      insist { subject["examplefield"] } == "12345"
+      insist { subject["word"] } == "hello"
+    end
+  end
+
+  describe "adding fields on match" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "matchme %{NUMBER:fancy}" }
+          singles => true
+          add_field => [ "new_field", "%{fancy}" ]
+        }
+      }
+    CONFIG
+
+    sample "matchme 1234" do
+      insist { subject["tags"] }.nil?
+      insist { subject["new_field"] } == "1234"
+    end
+
+    sample "this will not be matched" do
+      insist { subject["tags"] }.include?("_grokparsefailure")
+      reject { subject }.include?("new_field")
+    end
+  end
+
+  context "empty fields" do
+    describe "drop by default" do
+      config <<-CONFIG
+        filter {
+          grok {
+            match => { "message" => "1=%{WORD:foo1} *(2=%{WORD:foo2})?" }
+          }
+        }
+      CONFIG
+
+      sample "1=test" do
+        insist { subject["tags"] }.nil?
+        insist { subject }.include?("foo1")
+
+        # Since 'foo2' was not captured, it must not be present in the event.
+        reject { subject }.include?("foo2")
+      end
+    end
+
+    describe "keep if keep_empty_captures is true" do
+      config <<-CONFIG
+        filter {
+          grok {
+            match => { "message" => "1=%{WORD:foo1} *(2=%{WORD:foo2})?" }
+            keep_empty_captures => true
+          }
+        }
+      CONFIG
+
+      sample "1=test" do
+        insist { subject["tags"] }.nil?
+        # use .to_hash for this test, for now, because right now
+        # the Event.include? returns false for missing fields as well
+        # as for fields with nil values.
+        insist { subject.to_hash }.include?("foo2")
+        insist { subject.to_hash }.include?("foo2")
+      end
+    end
+  end
+
+  describe "when named_captures_only == false" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "Hello %{WORD}. %{WORD:foo}" }
+          named_captures_only => false
+          singles => true
+        }
+      }
+    CONFIG
+
+    sample "Hello World, yo!" do
+      insist { subject }.include?("WORD")
+      insist { subject["WORD"] } == "World"
+      insist { subject }.include?("foo")
+      insist { subject["foo"] } == "yo"
+    end
+  end
+
+  describe "using oniguruma named captures (?<name>regex)" do
+    context "plain regexp" do
+      config <<-'CONFIG'
+        filter {
+          grok {
+            singles => true
+            match => { "message" => "(?<foo>\w+)" }
+          }
+        }
+      CONFIG
+      sample "hello world" do
+        insist { subject["tags"] }.nil?
+        insist { subject["foo"] } == "hello"
+      end
+    end
+
+    context "grok patterns" do
+      config <<-'CONFIG'
+        filter {
+          grok {
+            singles => true
+            match => { "message" => "(?<timestamp>%{DATE_EU} %{TIME})" }
+          }
+        }
+      CONFIG
+
+      sample "fancy 12-12-12 12:12:12" do
+        insist { subject["tags"] }.nil?
+        insist { subject["timestamp"] } == "12-12-12 12:12:12"
+      end
+    end
+  end
+
+  describe "grok on integer types" do
+    config <<-'CONFIG'
+      filter {
+        grok {
+          match => { "status" => "^403$" }
+          add_tag => "four_oh_three"
+        }
+      }
+    CONFIG
+
+    sample("status" => 403) do
+      reject { subject["tags"] }.include?("_grokparsefailure")
+      insist { subject["tags"] }.include?("four_oh_three")
+    end
+  end
+
+  describe "grok on float types" do
+    config <<-'CONFIG'
+      filter {
+        grok {
+          match => { "version" => "^1.0$" }
+          add_tag => "one_point_oh"
+        }
+      }
+    CONFIG
+
+    sample("version" => 1.0) do
+      insist { subject["tags"] }.include?("one_point_oh")
+      insist { subject["tags"] }.include?("one_point_oh")
+    end
+  end
+
+  describe "grok on %{LOGLEVEL}" do
+    config <<-'CONFIG'
+      filter {
+        grok {
+          pattern => "%{LOGLEVEL:level}: error!"
+        }
+      }
+    CONFIG
+
+    log_level_names = %w(
+      trace Trace TRACE
+      debug Debug DEBUG
+      notice Notice Notice
+      info Info INFO
+      warn warning Warn Warning WARN WARNING
+      err error Err Error ERR ERROR
+      crit critical Crit Critical CRIT CRITICAL
+      fatal Fatal FATAL
+      severe Severe SEVERE
+      emerg emergency Emerg Emergency EMERG EMERGENCY
+    )
+    log_level_names.each do |level_name|
+      sample "#{level_name}: error!" do
+        insist { subject['level'] } == level_name
+      end
+    end
+  end
+
+  describe "tagging on failure" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "matchme %{NUMBER:fancy}" }
+          tag_on_failure => false
+        }
+      }
+    CONFIG
+
+    sample "matchme 1234" do
+      insist { subject["tags"] }.nil?
+    end
+
+    sample "this will not be matched" do
+      insist { subject["tags"] }.include?("false")
+    end
+  end
+
+  describe "captures named fields even if the whole text matches" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{DATE_EU:stimestamp}" }
+          singles => true
+        }
+      }
+    CONFIG
+
+    sample "11/01/01" do
+      insist { subject["stimestamp"] } == "11/01/01"
+    end
+  end
+
+  describe "allow dashes in capture names" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{WORD:foo-bar}" }
+          singles => true
+        }
+      }
+    CONFIG
+
+    sample "hello world" do
+      insist { subject["foo-bar"] } == "hello"
+    end
+  end
+
+  describe "performance test", :performance => true do
+    event_count = 100000
+    min_rate = 2000
+
+    max_duration = event_count / min_rate
+    input = "Nov 24 01:29:01 -0800"
+    config <<-CONFIG
+      input {
+        generator {
+          count => #{event_count}
+          message => "Mar 16 00:01:25 evita postfix/smtpd[1713]: connect from camomile.cloud9.net[168.100.1.3]"
+        }
+      }
+      filter {
+        grok {
+          match => { "message" => "%{SYSLOGLINE}" }
+          singles => true
+          overwrite => [ "message" ]
+        }
+      }
+      output { null { } }
+    CONFIG
+
+    2.times do
+      start = Time.now
+      agent do
+        duration = (Time.now - start)
+        puts "filters/grok parse rate: #{"%02.0f/sec" % (event_count / duration)}, elapsed: #{duration}s"
+        insist { duration } < max_duration
+      end
+    end
+  end
+
+  describe "singles with duplicate-named fields" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{INT:foo}|%{WORD:foo}" }
+          singles => true
+        }
+      }
+    CONFIG
+
+    sample "hello world" do
+      insist { subject["foo"] }.is_a?(String)
+    end
+
+    sample "123 world" do
+      insist { subject["foo"] }.is_a?(String)
+    end
+  end
+
+  describe "break_on_match default should be true and first match should exit filter" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{INT:foo}"
+                     "somefield" => "%{INT:bar}"}
+        }
+      }
+    CONFIG
+
+    sample("message" => "hello world 123", "somefield" => "testme abc 999") do
+      insist { subject["foo"] } == "123"
+      insist { subject["bar"] }.nil?
+    end
+  end
+
+  describe "break_on_match when set to false should try all patterns" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{INT:foo}"
+                     "somefield" => "%{INT:bar}"}
+          break_on_match => false
+        }
+      }
+    CONFIG
+
+    sample("message" => "hello world 123", "somefield" => "testme abc 999") do
+      insist { subject["foo"] } == "123"
+      insist { subject["bar"] } == "999"
+    end
+  end
+
+  describe "LOGSTASH-1547 - break_on_match should work on fields with multiple patterns" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => ["%{GREEDYDATA:name1}beard", "tree%{GREEDYDATA:name2}"] }
+          break_on_match => false
+        }
+      }
+    CONFIG
+
+    sample "treebranch" do
+      insist { subject["name2"] } == "branch"
+    end
+
+    sample "bushbeard" do
+      insist { subject["name1"] } == "bush"
+    end
+
+    sample "treebeard" do
+      insist { subject["name1"] } == "tree"
+      insist { subject["name2"] } == "beard"
+    end
+  end
+
+  describe "break_on_match default for array input with single grok pattern" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{INT:foo}"}
+        }
+      }
+    CONFIG
+
+    # array input --
+    sample("message" => ["hello world 123", "line 23"]) do
+      insist { subject["foo"] } == ["123", "23"]
+      insist { subject["tags"] }.nil?
+    end
+
+    # array input, one of them matches
+    sample("message" => ["hello world 123", "abc"]) do
+      insist { subject["foo"] } == "123"
+      insist { subject["tags"] }.nil?
+    end
+  end
+
+  describe "break_on_match = true (default) for array input with multiple grok pattern" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => ["%{INT:foo}", "%{WORD:bar}"] }
+        }
+      }
+    CONFIG
+
+    # array input --
+    sample("message" => ["hello world 123", "line 23"]) do
+      insist { subject["foo"] } == ["123", "23"]
+      insist { subject["bar"] }.nil?
+      insist { subject["tags"] }.nil?
+    end
+
+    # array input, one of them matches
+    sample("message" => ["hello world", "line 23"]) do
+      insist { subject["bar"] } == "hello"
+      insist { subject["foo"] } == "23"
+      insist { subject["tags"] }.nil?
+    end
+  end
+
+  describe "break_on_match = false for array input with multiple grok pattern" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => ["%{INT:foo}", "%{WORD:bar}"] }
+          break_on_match => false
+        }
+      }
+    CONFIG
+
+    # array input --
+    sample("message" => ["hello world 123", "line 23"]) do
+      insist { subject["foo"] } == ["123", "23"]
+      insist { subject["bar"] } == ["hello", "line"]
+      insist { subject["tags"] }.nil?
+    end
+
+    # array input, one of them matches
+    sample("message" => ["hello world", "line 23"]) do
+      insist { subject["bar"] } == ["hello", "line"]
+      insist { subject["foo"] } == "23"
+      insist { subject["tags"] }.nil?
+    end
+  end
+
+  describe  "grok with unicode" do
+    config <<-CONFIG
+      filter {
+        grok {
+          #pattern => "<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{PROG:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}"
+          pattern => "<%{POSINT:syslog_pri}>%{SPACE}%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{PROG:syslog_program}(:?)(?:\\[%{GREEDYDATA:syslog_pid}\\])?(:?) %{GREEDYDATA:syslog_message}"
+        }
+      }
+    CONFIG
+
+    sample "<22>Jan  4 07:50:46 mailmaster postfix/policy-spf[9454]: : SPF permerror (Junk encountered in record 'v=spf1 mx a:mail.domain.no ip4:192.168.0.4 ÔøΩall'): Envelope-from: email@domain.no" do
+      insist { subject["tags"] }.nil?
+      insist { subject["syslog_pri"] } == "22"
+      insist { subject["syslog_program"] } == "postfix/policy-spf"
+    end
+  end
+
+
+end
diff --git a/spec/filters/json_spec.rb b/spec/filters/json_spec.rb
new file mode 100644
index 00000000000..1a8536aeb20
--- /dev/null
+++ b/spec/filters/json_spec.rb
@@ -0,0 +1,89 @@
+require "spec_helper"
+require "logstash/filters/json"
+require "logstash/timestamp"
+
+describe LogStash::Filters::Json do
+
+  describe "parse message into the event" do
+    config <<-CONFIG
+      filter {
+        json {
+          # Parse message as JSON
+          source => "message"
+        }
+      }
+    CONFIG
+
+    sample '{ "hello": "world", "list": [ 1, 2, 3 ], "hash": { "k": "v" } }' do
+      insist { subject["hello"] } == "world"
+      insist { subject["list" ].to_a } == [1,2,3] # to_a for JRuby + JrJacksom which creates Java ArrayList
+      insist { subject["hash"] } == { "k" => "v" }
+    end
+  end
+
+  describe "parse message into a target field" do
+    config <<-CONFIG
+      filter {
+        json {
+          # Parse message as JSON, store the results in the 'data' field'
+          source => "message"
+          target => "data"
+        }
+      }
+    CONFIG
+
+    sample '{ "hello": "world", "list": [ 1, 2, 3 ], "hash": { "k": "v" } }' do
+      insist { subject["data"]["hello"] } == "world"
+      insist { subject["data"]["list" ].to_a } == [1,2,3] # to_a for JRuby + JrJacksom which creates Java ArrayList
+      insist { subject["data"]["hash"] } == { "k" => "v" }
+    end
+  end
+
+  describe "tag invalid json" do
+    config <<-CONFIG
+      filter {
+        json {
+          # Parse message as JSON, store the results in the 'data' field'
+          source => "message"
+          target => "data"
+        }
+      }
+    CONFIG
+
+    sample "invalid json" do
+      insist { subject["tags"] }.include?("_jsonparsefailure")
+    end
+  end
+
+  describe "fixing @timestamp (#pull 733)" do
+    config <<-CONFIG
+      filter {
+        json {
+          source => "message"
+        }
+      }
+    CONFIG
+
+    sample "{ \"@timestamp\": \"2013-10-19T00:14:32.996Z\" }" do
+      insist { subject["@timestamp"] }.is_a?(LogStash::Timestamp)
+      insist { LogStash::Json.dump(subject["@timestamp"]) } == "\"2013-10-19T00:14:32.996Z\""
+    end
+  end
+
+  describe "source == target" do
+    config <<-CONFIG
+      filter {
+        json {
+          source => "example"
+          target => "example"
+        }
+      }
+    CONFIG
+
+    sample({ "example" => "{ \"hello\": \"world\" }" }) do
+      insist { subject["example"] }.is_a?(Hash)
+      insist { subject["example"]["hello"] } == "world"
+    end
+  end
+
+end
diff --git a/spec/filters/kv_spec.rb b/spec/filters/kv_spec.rb
new file mode 100644
index 00000000000..d907a9e80d4
--- /dev/null
+++ b/spec/filters/kv_spec.rb
@@ -0,0 +1,436 @@
+require "spec_helper"
+require "logstash/filters/kv"
+
+describe LogStash::Filters::KV do
+
+  describe "defaults" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        kv { }
+      }
+    CONFIG
+
+    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
+      insist { subject["hello"] } == "world"
+      insist { subject["foo"] } == "bar"
+      insist { subject["baz"] } == "fizz"
+      insist { subject["doublequoted"] } == "hello world"
+      insist { subject["singlequoted"] } == "hello world"
+    end
+
+  end
+
+   describe "LOGSTASH-624: allow escaped space in key or value " do
+    config <<-CONFIG
+      filter {
+        kv { value_split => ':' }
+      }
+    CONFIG
+
+    sample 'IKE:=Quick\ Mode\ completion IKE\ IDs:=subnet:\ x.x.x.x\ (mask=\ 255.255.255.254)\ and\ host:\ y.y.y.y' do
+      insist { subject["IKE"] } == '=Quick\ Mode\ completion'
+      insist { subject['IKE\ IDs'] } == '=subnet:\ x.x.x.x\ (mask=\ 255.255.255.254)\ and\ host:\ y.y.y.y'
+    end
+  end
+
+  describe "test value_split" do
+    config <<-CONFIG
+      filter {
+        kv { value_split => ':' }
+      }
+    CONFIG
+
+    sample "hello:=world foo:bar baz=:fizz doublequoted:\"hello world\" singlequoted:'hello world'" do
+      insist { subject["hello"] } == "=world"
+      insist { subject["foo"] } == "bar"
+      insist { subject["baz="] } == "fizz"
+      insist { subject["doublequoted"] } == "hello world"
+      insist { subject["singlequoted"] } == "hello world"
+    end
+
+  end
+
+  describe "test field_split" do
+    config <<-CONFIG
+      filter {
+        kv { field_split => '?&' }
+      }
+    CONFIG
+
+    sample "?hello=world&foo=bar&baz=fizz&doublequoted=\"hello world\"&singlequoted='hello world'&ignoreme&foo12=bar12" do
+      insist { subject["hello"] } == "world"
+      insist { subject["foo"] } == "bar"
+      insist { subject["baz"] } == "fizz"
+      insist { subject["doublequoted"] } == "hello world"
+      insist { subject["singlequoted"] } == "hello world"
+      insist { subject["foo12"] } == "bar12"
+    end
+
+  end
+
+  describe  "delimited fields should override space default (reported by LOGSTASH-733)" do
+    config <<-CONFIG
+      filter {
+        kv { field_split => "|" }
+      }
+    CONFIG
+
+    sample "field1=test|field2=another test|field3=test3" do
+      insist { subject["field1"] } == "test"
+      insist { subject["field2"] } == "another test"
+      insist { subject["field3"] } == "test3"
+    end
+  end
+
+  describe "test prefix" do
+    config <<-CONFIG
+      filter {
+        kv { prefix => '__' }
+      }
+    CONFIG
+
+    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
+      insist { subject["__hello"] } == "world"
+      insist { subject["__foo"] } == "bar"
+      insist { subject["__baz"] } == "fizz"
+      insist { subject["__doublequoted"] } == "hello world"
+      insist { subject["__singlequoted"] } == "hello world"
+    end
+
+  end
+
+  describe "speed test", :performance => true do
+    count = 10000 + rand(3000)
+    config <<-CONFIG
+      input {
+        generator {
+          count => #{count}
+          type => foo
+          message => "hello=world bar='baz fizzle'"
+        }
+      }
+
+      filter {
+        kv { }
+      }
+
+      output  {
+        null { }
+      }
+    CONFIG
+
+    start = Time.now
+    agent do
+      duration = (Time.now - start)
+      puts "filters/kv rate: #{"%02.0f/sec" % (count / duration)}, elapsed: #{duration}s"
+    end
+  end
+
+  describe "add_tag" do
+    context "should activate when successful" do
+      config <<-CONFIG
+        filter {
+          kv { add_tag => "hello" }
+        }
+      CONFIG
+
+      sample "hello=world" do
+        insist { subject["hello"] } == "world"
+        insist { subject["tags"] }.include?("hello")
+      end
+    end
+    context "should not activate when failing" do
+      config <<-CONFIG
+        filter {
+          kv { add_tag => "hello" }
+        }
+      CONFIG
+
+      sample "this is not key value" do
+        insist { subject["tags"] }.nil?
+      end
+    end
+  end
+
+  describe "add_field" do
+    context "should activate when successful" do
+      config <<-CONFIG
+        filter {
+          kv { add_field => [ "whoa", "fancypants" ] }
+        }
+      CONFIG
+
+      sample "hello=world" do
+        insist { subject["hello"] } == "world"
+        insist { subject["whoa"] } == "fancypants"
+      end
+    end
+
+    context "should not activate when failing" do
+      config <<-CONFIG
+        filter {
+          kv { add_tag => "hello" }
+        }
+      CONFIG
+
+      sample "this is not key value" do
+        reject { subject["whoa"] } == "fancypants"
+      end
+    end
+  end
+
+  #New tests
+  describe "test target" do
+    config <<-CONFIG
+      filter {
+        kv { target => 'kv' }
+      }
+    CONFIG
+
+    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
+      insist { subject["kv"]["hello"] } == "world"
+      insist { subject["kv"]["foo"] } == "bar"
+      insist { subject["kv"]["baz"] } == "fizz"
+      insist { subject["kv"]["doublequoted"] } == "hello world"
+      insist { subject["kv"]["singlequoted"] } == "hello world"
+      insist {subject["kv"].count } == 5
+    end
+
+  end
+
+  describe "test empty target" do
+    config <<-CONFIG
+      filter {
+        kv { target => 'kv' }
+      }
+    CONFIG
+
+    sample "hello:world:foo:bar:baz:fizz" do
+      insist { subject["kv"] } == nil
+    end
+  end
+
+
+  describe "test data from specific sub source" do
+    config <<-CONFIG
+      filter {
+        kv {
+          source => "data"
+        }
+      }
+    CONFIG
+    sample("data" => "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'") do
+      insist { subject["hello"] } == "world"
+      insist { subject["foo"] } == "bar"
+      insist { subject["baz"] } == "fizz"
+      insist { subject["doublequoted"] } == "hello world"
+      insist { subject["singlequoted"] } == "hello world"
+    end
+  end
+
+  describe "test data from specific top source" do
+    config <<-CONFIG
+      filter {
+        kv {
+          source => "@data"
+        }
+      }
+    CONFIG
+    sample({"@data" => "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'"}) do
+      insist { subject["hello"] } == "world"
+      insist { subject["foo"] } == "bar"
+      insist { subject["baz"] } == "fizz"
+      insist { subject["doublequoted"] } == "hello world"
+      insist { subject["singlequoted"] } == "hello world"
+    end
+  end
+
+
+  describe "test data from specific sub source and target" do
+    config <<-CONFIG
+      filter {
+        kv {
+          source => "data"
+          target => "kv"
+        }
+      }
+    CONFIG
+    sample("data" => "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'") do
+      insist { subject["kv"]["hello"] } == "world"
+      insist { subject["kv"]["foo"] } == "bar"
+      insist { subject["kv"]["baz"] } == "fizz"
+      insist { subject["kv"]["doublequoted"] } == "hello world"
+      insist { subject["kv"]["singlequoted"] } == "hello world"
+      insist { subject["kv"].count } == 5
+    end
+  end
+
+  describe "test data from nil sub source, should not issue a warning" do
+    config <<-CONFIG
+      filter {
+        kv {
+          source => "non-exisiting-field"
+          target => "kv"
+        }
+      }
+    CONFIG
+    sample "" do
+      insist { subject["non-exisiting-field"] } == nil
+      insist { subject["kv"] } == nil
+    end
+  end
+
+  describe "test include_keys" do
+    config <<-CONFIG
+      filter {
+        kv {
+          include_keys => [ "foo", "singlequoted" ]
+        }
+      }
+    CONFIG
+
+    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
+      insist { subject["foo"] } == "bar"
+      insist { subject["singlequoted"] } == "hello world"
+    end
+  end
+
+  describe "test exclude_keys" do
+    config <<-CONFIG
+      filter {
+        kv {
+          exclude_keys => [ "foo", "singlequoted" ]
+        }
+      }
+    CONFIG
+
+    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
+      insist { subject["hello"] } == "world"
+      insist { subject["baz"] } == "fizz"
+      insist { subject["doublequoted"] } == "hello world"
+    end
+  end
+
+  describe "test include_keys with prefix" do
+    config <<-CONFIG
+      filter {
+        kv {
+          include_keys => [ "foo", "singlequoted" ]
+          prefix       => "__"
+        }
+      }
+    CONFIG
+
+    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
+      insist { subject["__foo"] } == "bar"
+      insist { subject["__singlequoted"] } == "hello world"
+    end
+  end
+
+  describe "test exclude_keys with prefix" do
+    config <<-CONFIG
+      filter {
+        kv {
+          exclude_keys => [ "foo", "singlequoted" ]
+          prefix       => "__"
+        }
+      }
+    CONFIG
+
+    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
+      insist { subject["__hello"] } == "world"
+      insist { subject["__baz"] } == "fizz"
+      insist { subject["__doublequoted"] } == "hello world"
+    end
+  end
+  
+  describe "test include_keys with dynamic key" do
+    config <<-CONFIG
+      filter {
+        kv {
+          source => "data"
+          include_keys => [ "%{key}"]
+        }
+      }
+    CONFIG
+    
+    sample({"data" => "foo=bar baz=fizz", "key" => "foo"}) do
+      insist { subject["foo"] } == "bar"
+      insist { subject["baz"] } == nil
+    end
+  end
+  
+  describe "test exclude_keys with dynamic key" do
+    config <<-CONFIG
+      filter {
+        kv {
+          source => "data"
+          exclude_keys => [ "%{key}"]
+        }
+      }
+    CONFIG
+    
+    sample({"data" => "foo=bar baz=fizz", "key" => "foo"}) do
+      insist { subject["foo"] } == nil
+      insist { subject["baz"] } == "fizz"
+    end
+  end
+
+  describe "test include_keys and exclude_keys" do
+    config <<-CONFIG
+      filter {
+        kv {
+          # This should exclude everything as a result of both settings.
+          include_keys => [ "foo", "singlequoted" ]
+          exclude_keys => [ "foo", "singlequoted" ]
+        }
+      }
+    CONFIG
+
+    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
+      %w(hello foo baz doublequoted singlequoted).each do |field|
+        reject { subject }.include?(field)
+      end
+    end
+  end
+
+  describe "test default_keys" do
+    config <<-CONFIG
+      filter {
+        kv {
+          default_keys => [ "foo", "xxx",
+                            "goo", "yyy" ]
+        }
+      }
+    CONFIG
+
+    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
+      insist { subject["hello"] } == "world"
+      insist { subject["foo"] } == "bar"
+      insist { subject["goo"] } == "yyy"
+      insist { subject["baz"] } == "fizz"
+      insist { subject["doublequoted"] } == "hello world"
+      insist { subject["singlequoted"] } == "hello world"
+    end
+  end
+
+  describe "overwriting a string field (often the source)" do
+    config <<-CONFIG
+      filter {
+        kv {
+          source => "happy"
+          target => "happy"
+        }
+      }
+    CONFIG
+
+    sample("happy" => "foo=bar baz=fizz") do
+      insist { subject["[happy][foo]"] } == "bar"
+      insist { subject["[happy][baz]"] } == "fizz"
+    end
+
+  end
+
+end
diff --git a/spec/filters/metrics_spec.rb b/spec/filters/metrics_spec.rb
new file mode 100644
index 00000000000..921c7fe9968
--- /dev/null
+++ b/spec/filters/metrics_spec.rb
@@ -0,0 +1,233 @@
+require "spec_helper"
+require "logstash/filters/metrics"
+
+describe LogStash::Filters::Metrics do
+
+  context "with basic meter config" do
+    context "when no events were received" do
+      it "should not flush" do
+        config = {"meter" => ["http.%{response}"]}
+        filter = LogStash::Filters::Metrics.new config
+        filter.register
+
+        events = filter.flush
+        insist { events }.nil?
+      end
+    end
+
+    context "when events are received" do
+      context "on the first flush" do
+        subject {
+          config = {"meter" => ["http.%{response}"]}
+          filter = LogStash::Filters::Metrics.new config
+          filter.register
+          filter.filter LogStash::Event.new({"response" => 200})
+          filter.filter LogStash::Event.new({"response" => 200})
+          filter.filter LogStash::Event.new({"response" => 404})
+          filter.flush
+        }
+
+        it "should flush counts" do
+          insist { subject.length } == 1
+          insist { subject.first["http.200.count"] } == 2
+          insist { subject.first["http.404.count"] } == 1
+        end
+
+        it "should include rates and percentiles" do
+          metrics = ["http.200.rate_1m", "http.200.rate_5m", "http.200.rate_15m",
+                     "http.404.rate_1m", "http.404.rate_5m", "http.404.rate_15m"]
+          metrics.each do |metric|
+            insist { subject.first }.include? metric
+          end
+        end
+      end
+
+      context "on the second flush" do
+        it "should not reset counts" do
+          config = {"meter" => ["http.%{response}"]}
+          filter = LogStash::Filters::Metrics.new config
+          filter.register
+          filter.filter LogStash::Event.new({"response" => 200})
+          filter.filter LogStash::Event.new({"response" => 200})
+          filter.filter LogStash::Event.new({"response" => 404})
+
+          events = filter.flush
+          events = filter.flush
+          insist { events.length } == 1
+          insist { events.first["http.200.count"] } == 2
+          insist { events.first["http.404.count"] } == 1
+        end
+      end
+    end
+
+    context "when custom rates and percentiles are selected" do
+      context "on the first flush" do
+        subject {
+          config = {
+            "meter" => ["http.%{response}"],
+            "rates" => [1]
+          }
+          filter = LogStash::Filters::Metrics.new config
+          filter.register
+          filter.filter LogStash::Event.new({"response" => 200})
+          filter.filter LogStash::Event.new({"response" => 200})
+          filter.filter LogStash::Event.new({"response" => 404})
+          filter.flush
+        }
+
+        it "should include only the requested rates" do
+          rate_fields = subject.first.to_hash.keys.select {|field| field.start_with?("http.200.rate") }
+          insist { rate_fields.length } == 1
+          insist { rate_fields }.include? "http.200.rate_1m"
+        end
+      end
+    end
+  end
+
+  context "with multiple instances" do
+    it "counts should be independent" do
+      config_tag1 = {"meter" => ["http.%{response}"], "tags" => ["tag1"]}
+      config_tag2 = {"meter" => ["http.%{response}"], "tags" => ["tag2"]}
+      filter_tag1 = LogStash::Filters::Metrics.new config_tag1
+      filter_tag2 = LogStash::Filters::Metrics.new config_tag2
+      event_tag1 = LogStash::Event.new({"response" => 200, "tags" => [ "tag1" ]})
+      event_tag2 = LogStash::Event.new({"response" => 200, "tags" => [ "tag2" ]})
+      event2_tag2 = LogStash::Event.new({"response" => 200, "tags" => [ "tag2" ]})
+      filter_tag1.register
+      filter_tag2.register
+
+      [event_tag1, event_tag2, event2_tag2].each do |event|
+        filter_tag1.filter event
+        filter_tag2.filter event
+      end
+
+      events_tag1 = filter_tag1.flush
+      events_tag2 = filter_tag2.flush
+
+      insist { events_tag1.first["http.200.count"] } == 1
+      insist { events_tag2.first["http.200.count"] } == 2
+    end
+  end
+
+  context "with timer config" do
+    context "on the first flush" do
+      subject {
+        config = {"timer" => ["http.request_time", "%{request_time}"]}
+        filter = LogStash::Filters::Metrics.new config
+        filter.register
+        filter.filter LogStash::Event.new({"request_time" => 10})
+        filter.filter LogStash::Event.new({"request_time" => 20})
+        filter.filter LogStash::Event.new({"request_time" => 30})
+        filter.flush
+      }
+
+      it "should flush counts" do
+        insist { subject.length } == 1
+        insist { subject.first["http.request_time.count"] } == 3
+      end
+
+      it "should include rates and percentiles keys" do
+        metrics = ["rate_1m", "rate_5m", "rate_15m", "p1", "p5", "p10", "p90", "p95", "p99"]
+        metrics.each do |metric|
+          insist { subject.first }.include? "http.request_time.#{metric}"
+        end
+      end
+
+      it "should include min value" do
+        insist { subject.first['http.request_time.min'] } == 10.0
+      end
+
+      it "should include mean value" do
+        insist { subject.first['http.request_time.mean'] } == 20.0
+      end
+
+      it "should include stddev value" do
+        insist { subject.first['http.request_time.stddev'] } == Math.sqrt(10.0)
+      end
+
+      it "should include max value" do
+        insist { subject.first['http.request_time.max'] } == 30.0
+      end
+
+      it "should include percentile value" do
+        insist { subject.first['http.request_time.p99'] } == 30.0
+      end
+    end
+  end
+
+  context "when custom rates and percentiles are selected" do
+    context "on the first flush" do
+      subject {
+        config = {
+          "timer" => ["http.request_time", "request_time"],
+          "rates" => [1],
+          "percentiles" => [1, 2]
+        }
+        filter = LogStash::Filters::Metrics.new config
+        filter.register
+        filter.filter LogStash::Event.new({"request_time" => 1})
+        filter.flush
+      }
+
+      it "should flush counts" do
+        insist { subject.length } == 1
+        insist { subject.first["http.request_time.count"] } == 1
+      end
+
+      it "should include only the requested rates" do
+        rate_fields = subject.first.to_hash.keys.select {|field| field.start_with?("http.request_time.rate") }
+        insist { rate_fields.length } == 1
+        insist { rate_fields }.include? "http.request_time.rate_1m"
+      end
+
+      it "should include only the requested percentiles" do
+        percentile_fields = subject.first.to_hash.keys.select {|field| field.start_with?("http.request_time.p") }
+        insist { percentile_fields.length } == 2
+        insist { percentile_fields }.include? "http.request_time.p1"
+        insist { percentile_fields }.include? "http.request_time.p2"
+      end
+    end
+  end
+
+
+  context "when a custom flush_interval is set" do
+    it "should flush only when required" do
+      config = {"meter" => ["http.%{response}"], "flush_interval" => 15}
+      filter = LogStash::Filters::Metrics.new config
+      filter.register
+      filter.filter LogStash::Event.new({"response" => 200})
+
+      insist { filter.flush }.nil?        # 5s
+      insist { filter.flush }.nil?        # 10s
+      insist { filter.flush.length } == 1 # 15s
+      insist { filter.flush }.nil?        # 20s
+      insist { filter.flush }.nil?        # 25s
+      insist { filter.flush.length } == 1 # 30s
+    end
+  end
+
+  context "when a custom clear_interval is set" do
+    it "should clear the metrics after interval has passed" do
+      config = {"meter" => ["http.%{response}"], "clear_interval" => 15}
+      filter = LogStash::Filters::Metrics.new config
+      filter.register
+      filter.filter LogStash::Event.new({"response" => 200})
+
+      insist { filter.flush.first["http.200.count"] } == 1 # 5s
+      insist { filter.flush.first["http.200.count"] } == 1 # 10s
+      insist { filter.flush.first["http.200.count"] } == 1 # 15s
+      insist { filter.flush }.nil?                         # 20s
+    end
+  end
+
+  context "when invalid rates are set" do
+    subject {
+      config = {"meter" => ["http.%{response}"], "rates" => [90]}
+      filter = LogStash::Filters::Metrics.new config
+    }
+
+    it "should raise an error" do
+      insist {subject.register }.raises(LogStash::ConfigurationError)
+    end
+  end
+end
diff --git a/spec/filters/multiline_spec.rb b/spec/filters/multiline_spec.rb
new file mode 100644
index 00000000000..a9e1b18307d
--- /dev/null
+++ b/spec/filters/multiline_spec.rb
@@ -0,0 +1,153 @@
+# encoding: utf-8
+
+require "spec_helper"
+require "logstash/filters/multiline"
+
+describe LogStash::Filters::Multiline do
+
+  describe "simple multiline" do
+    config <<-CONFIG
+    filter {
+      multiline {
+        periodic_flush => false
+        pattern => "^\\s"
+        what => previous
+      }
+    }
+    CONFIG
+
+    sample [ "hello world", "   second line", "another first line" ] do
+      expect(subject).to be_a(Array)
+      insist { subject.size } == 2
+      insist { subject[0]["message"] } == "hello world\n   second line"
+      insist { subject[1]["message"] } == "another first line"
+    end
+  end
+
+  describe "multiline using grok patterns" do
+    config <<-CONFIG
+    filter {
+      multiline {
+        pattern => "^%{NUMBER} %{TIME}"
+        negate => true
+        what => previous
+      }
+    }
+    CONFIG
+
+    sample [ "120913 12:04:33 first line", "second line", "third line" ] do
+      insist { subject["message"] } ==  "120913 12:04:33 first line\nsecond line\nthird line"
+    end
+  end
+
+  describe "multiline safety among multiple concurrent streams" do
+    config <<-CONFIG
+      filter {
+        multiline {
+          pattern => "^\\s"
+          what => previous
+        }
+      }
+    CONFIG
+
+    count = 50
+    stream_count = 3
+
+    # first make sure to have starting lines for all streams
+    eventstream = stream_count.times.map do |i|
+      stream = "stream#{i}"
+      lines = [LogStash::Event.new("message" => "hello world #{stream}", "host" => stream, "type" => stream)]
+      lines += rand(5).times.map do |n|
+        LogStash::Event.new("message" => "   extra line in #{stream}", "host" => stream, "type" => stream)
+      end
+    end
+
+    # them add starting lines for random stream with sublines also for random stream
+    eventstream += (count - stream_count).times.map do |i|
+      stream = "stream#{rand(stream_count)}"
+      lines = [LogStash::Event.new("message" => "hello world #{stream}", "host" => stream, "type" => stream)]
+      lines += rand(5).times.map do |n|
+        stream = "stream#{rand(stream_count)}"
+        LogStash::Event.new("message" => "   extra line in #{stream}", "host" => stream, "type" => stream)
+      end
+    end
+
+    events = eventstream.flatten.map{|event| event.to_hash}
+
+    sample events do
+      expect(subject).to be_a(Array)
+      insist { subject.size } == count
+
+      subject.each_with_index do |event, i|
+        insist { event["type"] == event["host"] } == true
+        stream = event["type"]
+        insist { event["message"].split("\n").first } =~ /hello world /
+        insist { event["message"].scan(/stream\d/).all?{|word| word == stream} } == true
+      end
+    end
+  end
+
+  describe "multiline add/remove tags and fields only when matched" do
+    config <<-CONFIG
+      filter {
+        mutate {
+          add_tag => "dummy"
+        }
+        multiline {
+          add_tag => [ "nope" ]
+          remove_tag => "dummy"
+          add_field => [ "dummy2", "value" ]
+          pattern => "an unlikely match"
+          what => previous
+        }
+      }
+    CONFIG
+
+    sample [ "120913 12:04:33 first line", "120913 12:04:33 second line" ] do
+      expect(subject).to be_a(Array)
+      insist { subject.size } == 2
+
+      subject.each do |s|
+        insist { s["tags"].include?("nope")  } == false
+        insist { s["tags"].include?("dummy") } == true
+        insist { s.include?("dummy2") } == false
+      end
+    end
+  end
+
+  describe "regression test for GH issue #1258" do
+    config <<-CONFIG
+      filter {
+        multiline {
+          pattern => "^\s"
+          what => "next"
+          add_tag => ["multi"]
+        }
+      }
+    CONFIG
+
+    sample [ "  match", "nomatch" ] do
+      expect(subject).to be_a(LogStash::Event)
+      insist { subject["message"] } == "  match\nnomatch"
+    end
+  end
+
+  describe "multiple match/nomatch" do
+    config <<-CONFIG
+      filter {
+        multiline {
+          pattern => "^\s"
+          what => "next"
+          add_tag => ["multi"]
+        }
+      }
+    CONFIG
+
+    sample ["  match1", "nomatch1", "  match2", "nomatch2"] do
+      expect(subject).to be_a(Array)
+      insist { subject.size } == 2
+      insist { subject[0]["message"] } == "  match1\nnomatch1"
+      insist { subject[1]["message"] } == "  match2\nnomatch2"
+    end
+  end
+end
diff --git a/spec/filters/mutate_spec.rb b/spec/filters/mutate_spec.rb
new file mode 100644
index 00000000000..38afe392a77
--- /dev/null
+++ b/spec/filters/mutate_spec.rb
@@ -0,0 +1,258 @@
+# encoding: utf-8
+
+require "spec_helper"
+require "logstash/filters/mutate"
+
+describe LogStash::Filters::Mutate do
+
+  context "config validation" do
+   describe "invalid convert type should raise a configuration error" do
+      config <<-CONFIG
+        filter {
+          mutate {
+            convert => [ "message", "int"] //should be integer
+          }
+        }
+      CONFIG
+
+      sample "not_really_important" do
+        insist {subject}.raises LogStash::ConfigurationError
+      end
+    end
+    describe "invalid gsub triad should raise a configuration error" do
+      config <<-CONFIG
+        filter {
+          mutate {
+            gsub => [ "message", "toreplace"]
+          }
+        }
+      CONFIG
+
+      sample "not_really_important" do
+        insist {subject}.raises LogStash::ConfigurationError
+      end
+    end
+  end
+
+  describe "basics" do
+    config <<-CONFIG
+      filter {
+        mutate {
+          lowercase => "lowerme"
+          uppercase => "upperme"
+          convert => [ "intme", "integer", "floatme", "float" ]
+          rename => [ "rename1", "rename2" ]
+          replace => [ "replaceme", "hello world" ]
+          replace => [ "newfield", "newnew" ]
+          update => [ "nosuchfield", "weee" ]
+          update => [ "updateme", "updated" ]
+          remove => [ "removeme" ]
+        }
+      }
+    CONFIG
+
+    event = {
+      "lowerme" => [ "ExAmPlE" ],
+      "upperme" => [ "ExAmPlE" ],
+      "intme" => [ "1234", "7890.4", "7.9" ],
+      "floatme" => [ "1234.455" ],
+      "rename1" => [ "hello world" ],
+      "updateme" => [ "who cares" ],
+      "replaceme" => [ "who cares" ],
+      "removeme" => [ "something" ]
+    }
+
+    sample event do
+      insist { subject["lowerme"] } == ['example']
+      insist { subject["upperme"] } == ['EXAMPLE']
+      insist { subject["intme"] }   == [1234, 7890, 7]
+      insist { subject["floatme"] } == [1234.455]
+      reject { subject }.include?("rename1")
+      insist { subject["rename2"] } == [ "hello world" ]
+      reject { subject }.include?("removeme")
+
+      insist { subject }.include?("newfield")
+      insist { subject["newfield"] } == "newnew"
+      reject { subject }.include?("nosuchfield")
+      insist { subject["updateme"] } == "updated"
+    end
+  end
+
+  describe "remove multiple fields" do
+    config '
+      filter {
+        mutate {
+          remove => [ "remove-me", "remove-me2", "diedie", "[one][two]" ]
+        }
+      }'
+
+    sample(
+      "remove-me"  => "Goodbye!",
+      "remove-me2" => 1234,
+      "diedie"     => [1, 2, 3, 4],
+      "survivor"   => "Hello.",
+      "one" => { "two" => "wee" }
+    ) do
+      insist { subject["survivor"] } == "Hello."
+      reject { subject }.include?("remove-me")
+      reject { subject }.include?("remove-me2")
+      reject { subject }.include?("diedie")
+      reject { subject["one"] }.include?("two")
+    end
+  end
+
+  describe "convert one field to string" do
+    config '
+      filter {
+        mutate {
+          convert => [ "unicorns", "string" ]
+        }
+      }'
+
+    sample("unicorns" => 1234) do
+      insist { subject["unicorns"] } == "1234"
+    end
+  end
+
+  describe "gsub on a String" do
+    config '
+      filter {
+        mutate {
+          gsub => [ "unicorns", "but extinct", "and common" ]
+        }
+      }'
+
+    sample("unicorns" => "Magnificient, but extinct, animals") do
+      insist { subject["unicorns"] } == "Magnificient, and common, animals"
+    end
+  end
+
+  describe "gsub on an Array of Strings" do
+    config '
+      filter {
+        mutate {
+          gsub => [ "unicorns", "extinct", "common" ]
+        }
+      }'
+
+    sample("unicorns" => [
+      "Magnificient extinct animals", "Other extinct ideas" ]
+    ) do
+      insist { subject["unicorns"] } == [
+        "Magnificient common animals",
+        "Other common ideas"
+      ]
+    end
+  end
+
+  describe "gsub on multiple fields" do
+    config '
+      filter {
+        mutate {
+          gsub => [ "colors", "red", "blue",
+                    "shapes", "square", "circle" ]
+        }
+      }'
+
+    sample("colors" => "One red car", "shapes" => "Four red squares") do
+      insist { subject["colors"] } == "One blue car"
+      insist { subject["shapes"] } == "Four red circles"
+    end
+  end
+
+  describe "regression - mutate should lowercase a field created by grok" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{WORD:foo}" }
+        }
+        mutate {
+          lowercase => "foo"
+        }
+      }
+    CONFIG
+
+    sample "HELLO WORLD" do
+      insist { subject["foo"] } == "hello"
+    end
+  end
+
+  describe "LOGSTASH-757: rename should do nothing with a missing field" do
+    config <<-CONFIG
+      filter {
+        mutate {
+          rename => [ "nosuchfield", "hello" ]
+        }
+      }
+    CONFIG
+
+    sample "whatever" do
+      reject { subject }.include?("nosuchfield")
+      reject { subject }.include?("hello")
+    end
+  end
+
+  describe "convert should work on nested fields" do
+    config <<-CONFIG
+      filter {
+        mutate {
+          convert => [ "[foo][bar]", "integer" ]
+        }
+      }
+    CONFIG
+
+    sample({ "foo" => { "bar" => "1000" } }) do
+      insist { subject["[foo][bar]"] } == 1000
+      insist { subject["[foo][bar]"] }.is_a?(Fixnum)
+    end
+  end
+
+  #LOGSTASH-1529
+  describe "gsub on a String with dynamic fields (%{}) in pattern" do
+    config '
+      filter {
+        mutate {
+          gsub => [ "unicorns", "of type %{unicorn_type}", "green" ]
+        }
+      }'
+
+    sample("unicorns" => "Unicorns of type blue are common", "unicorn_type" => "blue") do
+      insist { subject["unicorns"] } == "Unicorns green are common"
+    end
+  end
+
+  #LOGSTASH-1529
+  describe "gsub on a String with dynamic fields (%{}) in pattern and replace" do
+    config '
+      filter {
+        mutate {
+          gsub => [ "unicorns2", "of type %{unicorn_color}", "%{unicorn_color} and green" ]
+        }
+      }'
+
+    sample("unicorns2" => "Unicorns of type blue are common", "unicorn_color" => "blue") do
+      insist { subject["unicorns2"] } == "Unicorns blue and green are common"
+    end
+  end
+
+  #LOGSTASH-1529
+  describe "gsub on a String array with dynamic fields in pattern" do
+    config '
+      filter {
+        mutate {
+          gsub => [ "unicorns_array", "of type %{color}", "blue and green" ]
+        }
+      }'
+
+    sample("unicorns_array" => [
+        "Unicorns of type blue are found in Alaska", "Unicorns of type blue are extinct" ],
+           "color" => "blue"
+    ) do
+      insist { subject["unicorns_array"] } == [
+          "Unicorns blue and green are found in Alaska",
+          "Unicorns blue and green are extinct"
+      ]
+    end
+  end
+end
+
diff --git a/spec/filters/noop_spec.rb b/spec/filters/noop_spec.rb
new file mode 100644
index 00000000000..e2cb6ecb428
--- /dev/null
+++ b/spec/filters/noop_spec.rb
@@ -0,0 +1,220 @@
+require "spec_helper"
+require "logstash/filters/noop"
+
+#NOOP filter is perfect for testing Filters::Base features with minimal overhead
+describe LogStash::Filters::NOOP do
+
+  describe "adding multiple value to one field" do
+    config <<-CONFIG
+    filter {
+      noop {
+        add_field => ["new_field", "new_value"]
+        add_field => ["new_field", "new_value_2"]
+      }
+    }
+    CONFIG
+
+    sample "example" do
+      insist { subject["new_field"] } == ["new_value", "new_value_2"]
+    end
+  end
+
+  describe "type parsing" do
+    config <<-CONFIG
+    filter {
+      noop {
+        type => "noop"
+        add_tag => ["test"]
+      }
+    }
+    CONFIG
+
+    sample("type" => "noop") do
+      insist { subject["tags"] } == ["test"]
+    end
+
+    sample("type" => "not_noop") do
+      insist { subject["tags"] }.nil?
+    end
+  end
+
+  describe "tags parsing with one tag" do
+    config <<-CONFIG
+    filter {
+      noop {
+        type => "noop"
+        tags => ["t1"]
+        add_tag => ["test"]
+      }
+    }
+    CONFIG
+
+    sample("type" => "noop") do
+      insist { subject["tags"] }.nil?
+    end
+
+    sample("type" => "noop", "tags" => ["t1", "t2"]) do
+      insist { subject["tags"] } == ["t1", "t2", "test"]
+    end
+  end
+
+  describe "tags parsing with multiple tags" do
+    config <<-CONFIG
+    filter {
+      noop {
+        type => "noop"
+        tags => ["t1", "t2"]
+        add_tag => ["test"]
+      }
+    }
+    CONFIG
+
+    sample("type" => "noop") do
+      insist { subject["tags"] }.nil?
+    end
+
+    sample("type" => "noop", "tags" => ["t1"]) do
+      insist { subject["tags"] } == ["t1"]
+    end
+
+    sample("type" => "noop", "tags" => ["t1", "t2"]) do
+      insist { subject["tags"] } == ["t1", "t2", "test"]
+    end
+
+    sample("type" => "noop", "tags" => ["t1", "t2", "t3"]) do
+      insist { subject["tags"] } == ["t1", "t2", "t3", "test"]
+    end
+  end
+
+  describe "exclude_tags with 1 tag" do
+    config <<-CONFIG
+    filter {
+      noop {
+        type => "noop"
+        tags => ["t1"]
+        add_tag => ["test"]
+        exclude_tags => ["t2"]
+      }
+    }
+    CONFIG
+
+    sample("type" => "noop") do
+      insist { subject["tags"] }.nil?
+    end
+
+    sample("type" => "noop", "tags" => ["t1"]) do
+      insist { subject["tags"] } == ["t1", "test"]
+    end
+
+    sample("type" => "noop", "tags" => ["t1", "t2"]) do
+      insist { subject["tags"] } == ["t1", "t2"]
+    end
+  end
+
+  describe "exclude_tags with >1 tags" do
+    config <<-CONFIG
+    filter {
+      noop {
+        type => "noop"
+        tags => ["t1"]
+        add_tag => ["test"]
+        exclude_tags => ["t2", "t3"]
+      }
+    }
+    CONFIG
+
+    sample("type" => "noop", "tags" => ["t1", "t2", "t4"]) do
+      insist { subject["tags"] } == ["t1", "t2", "t4"]
+    end
+
+    sample("type" => "noop", "tags" => ["t1", "t3", "t4"]) do
+      insist { subject["tags"] } == ["t1", "t3", "t4"]
+    end
+
+    sample("type" => "noop", "tags" => ["t1", "t4", "t5"]) do
+      insist { subject["tags"] } == ["t1", "t4", "t5", "test"]
+    end
+  end
+
+  describe "remove_tag" do
+    config <<-CONFIG
+    filter {
+      noop {
+        type => "noop"
+        tags => ["t1"]
+        remove_tag => ["t2", "t3"]
+      }
+    }
+    CONFIG
+
+    sample("type" => "noop", "tags" => ["t4"]) do
+      insist { subject["tags"] } == ["t4"]
+    end
+
+    sample("type" => "noop", "tags" => ["t1", "t2", "t3"]) do
+      insist { subject["tags"] } == ["t1"]
+    end
+
+    sample("type" => "noop", "tags" => ["t1", "t2"]) do
+      insist { subject["tags"] } == ["t1"]
+    end
+  end
+
+  describe "remove_tag with dynamic value" do
+    config <<-CONFIG
+    filter {
+      noop {
+        type => "noop"
+        tags => ["t1"]
+        remove_tag => ["%{blackhole}"]
+      }
+    }
+    CONFIG
+
+    sample("type" => "noop", "tags" => ["t1", "goaway", "t3"], "blackhole" => "goaway") do
+      insist { subject["tags"] } == ["t1", "t3"]
+    end
+  end
+
+  describe "remove_field" do
+    config <<-CONFIG
+    filter {
+      noop {
+        type => "noop"
+        remove_field => ["t2", "t3"]
+      }
+    }
+    CONFIG
+
+    sample("type" => "noop", "t4" => "four") do
+      insist { subject }.include?("t4")
+    end
+
+    sample("type" => "noop", "t1" => "one", "t2" => "two", "t3" => "three") do
+      insist { subject }.include?("t1")
+      reject { subject }.include?("t2")
+      reject { subject }.include?("t3")
+    end
+
+    sample("type" => "noop", "t1" => "one", "t2" => "two") do
+      insist { subject }.include?("t1")
+      reject { subject }.include?("t2")
+    end
+  end
+
+  describe "remove_field with dynamic value in field name" do
+    config <<-CONFIG
+    filter {
+      noop {
+        type => "noop"
+        remove_field => ["%{blackhole}"]
+      }
+    }
+    CONFIG
+
+    sample("type" => "noop", "blackhole" => "go", "go" => "away") do
+      insist { subject }.include?("blackhole")
+      reject { subject }.include?("go")
+    end
+  end
+end
diff --git a/spec/filters/ruby_spec.rb b/spec/filters/ruby_spec.rb
new file mode 100644
index 00000000000..b1c83f19e0a
--- /dev/null
+++ b/spec/filters/ruby_spec.rb
@@ -0,0 +1,55 @@
+require "spec_helper"
+require "logstash/filters/ruby"
+require "logstash/filters/date"
+
+describe LogStash::Filters::Ruby do
+
+  describe "generate pretty json on event.to_hash" do
+    # this obviously tests the Ruby filter but also makes sure
+    # the fix for issue #1771 is correct and that to_json is
+    # compatible with the json gem convention.
+
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "mydate", "ISO8601" ]
+          locale => "en"
+          timezone => "UTC"
+        }
+        ruby {
+          init => "require 'json'"
+          code => "event['pretty'] = JSON.pretty_generate(event.to_hash)"
+        }
+      }
+    CONFIG
+
+    sample("message" => "hello world", "mydate" => "2014-09-23T00:00:00-0800") do
+      # json is rendered in pretty json since the JSON.pretty_generate created json from the event hash
+      insist { subject["pretty"] } == "{\n  \"message\": \"hello world\",\n  \"mydate\": \"2014-09-23T00:00:00-0800\",\n  \"@version\": \"1\",\n  \"@timestamp\": \"2014-09-23T08:00:00.000Z\"\n}"
+    end
+  end
+
+  describe "generate pretty json on event.to_hash" do
+    # this obviously tests the Ruby filter but asses that using the json gem directly
+    # on even will correctly call the to_json method but will use the logstash json
+    # generation and thus will not work with pretty_generate.
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "mydate", "ISO8601" ]
+          locale => "en"
+          timezone => "UTC"
+        }
+        ruby {
+          init => "require 'json'"
+          code => "event['pretty'] = JSON.pretty_generate(event)"
+        }
+      }
+    CONFIG
+
+    sample("message" => "hello world", "mydate" => "2014-09-23T00:00:00-0800") do
+      # if this eventually breaks because we removed the custom to_json and/or added pretty support to JrJackson then all is good :)
+      insist { subject["pretty"] } == "{\"message\":\"hello world\",\"mydate\":\"2014-09-23T00:00:00-0800\",\"@version\":\"1\",\"@timestamp\":\"2014-09-23T08:00:00.000Z\"}"
+    end
+  end
+end
diff --git a/spec/filters/split_spec.rb b/spec/filters/split_spec.rb
new file mode 100644
index 00000000000..05fbbd7f076
--- /dev/null
+++ b/spec/filters/split_spec.rb
@@ -0,0 +1,59 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/filters/split"
+
+describe LogStash::Filters::Split do
+
+  describe "all defaults" do
+    config <<-CONFIG
+      filter {
+        split { }
+      }
+    CONFIG
+
+    sample "big\nbird\nsesame street" do
+      insist { subject.length } == 3
+      insist { subject[0]["message"] } == "big"
+      insist { subject[1]["message"] } == "bird"
+      insist { subject[2]["message"] } == "sesame street"
+    end
+  end
+
+  describe "custome terminator" do
+    config <<-CONFIG
+      filter {
+        split {
+          terminator => "\t"
+        }
+      }
+    CONFIG
+
+    sample "big\tbird\tsesame street" do
+      insist { subject.length } == 3
+      insist { subject[0]["message"] } == "big"
+      insist { subject[1]["message"] } == "bird"
+      insist { subject[2]["message"] } == "sesame street"
+    end
+  end
+
+  describe "custom field" do
+    config <<-CONFIG
+      filter {
+        split {
+          field => "custom"
+        }
+      }
+    CONFIG
+
+    sample("custom" => "big\nbird\nsesame street", "do_not_touch" => "1\n2\n3") do
+      insist { subject.length } == 3
+      subject.each do |s|
+         insist { s["do_not_touch"] } == "1\n2\n3"
+      end
+      insist { subject[0]["custom"] } == "big"
+      insist { subject[1]["custom"] } == "bird"
+      insist { subject[2]["custom"] } == "sesame street"
+    end
+  end
+
+end
diff --git a/spec/filters/spool_spec.rb b/spec/filters/spool_spec.rb
new file mode 100644
index 00000000000..b68c3483cd8
--- /dev/null
+++ b/spec/filters/spool_spec.rb
@@ -0,0 +1,88 @@
+require "spec_helper"
+require "logstash/filters/spool"
+
+#NOOP filter is perfect for testing Filters::Base features with minimal overhead
+describe LogStash::Filters::Spool do
+
+  # spool test are really flush tests. spool does nothing more than waiting for flush to be called.
+
+  describe "flush one event" do
+    config <<-CONFIG
+    filter {
+      spool { }
+    }
+    CONFIG
+
+    sample "foo" do
+      insist { subject["message"] } == "foo"
+    end
+  end
+
+  describe "spooling multiple events" do
+    config <<-CONFIG
+    filter {
+      spool { }
+    }
+    CONFIG
+
+    sample ["foo", "bar"] do
+      insist { subject[0]["message"] } == "foo"
+      insist { subject[1]["message"] } == "bar"
+    end
+  end
+
+  describe "spooling events through conditionals" do
+    config <<-CONFIG
+    filter {
+      spool { }
+      if [message] == "foo" {
+        mutate { add_field => { "cond1" => "true" } }
+      } else {
+        mutate { add_field => { "cond2" => "true" } }
+      }
+      mutate { add_field => { "last" => "true" } }
+    }
+    CONFIG
+
+    sample ["foo", "bar"] do
+      insist { subject[0]["message"] } == "foo"
+      insist { subject[0]["cond1"] } == "true"
+      insist { subject[0]["cond2"] } == nil
+      insist { subject[0]["last"] } == "true"
+
+      insist { subject[1]["message"] } == "bar"
+      insist { subject[1]["cond1"] } == nil
+      insist { subject[1]["cond2"] } == "true"
+      insist { subject[1]["last"] } == "true"
+    end
+  end
+
+ describe "spooling eventS with conditionals" do
+    config <<-CONFIG
+    filter {
+      mutate { add_field => { "first" => "true" } }
+      if [message] == "foo" {
+        spool { }
+      } else {
+        mutate { add_field => { "cond2" => "true" } }
+      }
+      mutate { add_field => { "last" => "true" } }
+    }
+    CONFIG
+
+    sample ["foo", "bar"] do
+      # here received events will be reversed since the spooled one will be flushed last, at shutdown
+
+      insist { subject[0]["message"] } == "bar"
+      insist { subject[0]["first"] } == "true"
+      insist { subject[0]["cond2"] } == "true"
+      insist { subject[0]["last"] } == "true"
+
+      insist { subject[1]["message"] } == "foo"
+      insist { subject[1]["first"] } == "true"
+      insist { subject[1]["cond2"] } == nil
+      insist { subject[1]["last"] } == "true"
+    end
+  end
+
+end
diff --git a/spec/filters/throttle_spec.rb b/spec/filters/throttle_spec.rb
new file mode 100644
index 00000000000..746fe0663cc
--- /dev/null
+++ b/spec/filters/throttle_spec.rb
@@ -0,0 +1,196 @@
+require "spec_helper"
+require "logstash/filters/throttle"
+
+describe LogStash::Filters::Throttle do
+
+  describe "no before_count" do
+    config <<-CONFIG
+      filter {
+        throttle {
+          period => 60
+          after_count => 2
+          key => "%{host}"
+          add_tag => [ "throttled" ]
+        }
+      }
+    CONFIG
+
+    event = {
+      "host" => "server1"
+    }
+
+    sample event do
+      insist { subject["tags"] } == nil
+    end
+  end
+  
+  describe "before_count throttled" do
+    config <<-CONFIG
+      filter {
+        throttle {
+          period => 60
+          before_count => 2
+          after_count => 3
+          key => "%{host}"
+          add_tag => [ "throttled" ]
+        }
+      }
+    CONFIG
+
+    event = {
+      "host" => "server1"
+    }
+
+    sample event do
+      insist { subject["tags"] } == [ "throttled" ]
+    end
+  end
+  
+  describe "before_count exceeded" do
+    config <<-CONFIG
+      filter {
+        throttle {
+          period => 60
+          before_count => 2
+          after_count => 3
+          key => "%{host}"
+          add_tag => [ "throttled" ]
+        }
+      }
+    CONFIG
+
+    events = [{
+      "host" => "server1"
+    }, {
+      "host" => "server1"
+    }]
+
+    sample events do
+      insist { subject[0]["tags"] } == [ "throttled" ]
+      insist { subject[1]["tags"] } == nil
+    end
+  end
+  
+  describe "after_count exceeded" do
+    config <<-CONFIG
+      filter {
+        throttle {
+          period => 60
+          before_count => 2
+          after_count => 3
+          key => "%{host}"
+          add_tag => [ "throttled" ]
+        }
+      }
+    CONFIG
+
+    events = [{
+      "host" => "server1"
+    }, {
+      "host" => "server1"
+    }, {
+      "host" => "server1"
+    }, {
+      "host" => "server1"
+    }]
+
+    sample events do
+      insist { subject[0]["tags"] } == [ "throttled" ]
+      insist { subject[1]["tags"] } == nil
+      insist { subject[2]["tags"] } == nil
+      insist { subject[3]["tags"] } == [ "throttled" ]
+    end
+  end
+  
+  describe "different keys" do
+    config <<-CONFIG
+      filter {
+        throttle {
+          period => 60
+          after_count => 2
+          key => "%{host}"
+          add_tag => [ "throttled" ]
+        }
+      }
+    CONFIG
+
+    events = [{
+      "host" => "server1"
+    }, {
+      "host" => "server2"
+    }, {
+      "host" => "server3"
+    }, {
+      "host" => "server4"
+    }]
+
+    sample events do
+      subject.each { | s |
+        insist { s["tags"] } == nil
+      }
+    end
+  end
+  
+  describe "composite key" do
+    config <<-CONFIG
+      filter {
+        throttle {
+          period => 60
+          after_count => 1
+          key => "%{host}%{message}"
+          add_tag => [ "throttled" ]
+        }
+      }
+    CONFIG
+
+    events = [{
+      "host" => "server1",
+      "message" => "foo"
+    }, {
+      "host" => "server1",
+      "message" => "bar"
+    }, {
+      "host" => "server2",
+      "message" => "foo"
+    }, {
+      "host" => "server2",
+      "message" => "bar"
+    }]
+
+    sample events do
+      subject.each { | s |
+        insist { s["tags"] } == nil
+      }
+    end
+  end
+  
+  describe "max_counter exceeded" do
+    config <<-CONFIG
+      filter {
+        throttle {
+          period => 60
+          after_count => 1
+          max_counters => 2
+          key => "%{message}"
+          add_tag => [ "throttled" ]
+        }
+      }
+    CONFIG
+
+    events = [{
+      "message" => "foo"
+    }, {
+      "message" => "bar"
+    }, {
+      "message" => "poo"
+    }, {
+      "message" => "foo"
+    }]
+
+    sample events do
+      insist { subject[3]["tags"] } == nil
+    end
+  end
+
+end # LogStash::Filters::Throttle
+
diff --git a/spec/filters/urldecode_spec.rb b/spec/filters/urldecode_spec.rb
new file mode 100644
index 00000000000..4b8225d7a0c
--- /dev/null
+++ b/spec/filters/urldecode_spec.rb
@@ -0,0 +1,54 @@
+# encoding: utf-8
+
+require "spec_helper"
+require "logstash/filters/urldecode"
+
+describe LogStash::Filters::Urldecode do
+
+  describe "urldecode of correct urlencoded data" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        urldecode {
+        }
+      }
+    CONFIG
+
+    sample("message" => "http%3A%2F%2Flogstash.net%2Fdocs%2F1.3.2%2Ffilters%2Furldecode") do
+      insist { subject["message"] } == "http://logstash.net/docs/1.3.2/filters/urldecode"
+    end
+  end
+
+  describe "urldecode of incorrect urlencoded data" do
+    config <<-CONFIG
+      filter {
+        urldecode {
+        }
+      }
+    CONFIG
+
+    sample("message" => "http://logstash.net/docs/1.3.2/filters/urldecode") do
+      insist { subject["message"] } == "http://logstash.net/docs/1.3.2/filters/urldecode"
+    end
+  end
+
+   describe "urldecode with all_fields set to true" do
+    # The logstash config goes here.
+    # At this time, only filters are supported.
+    config <<-CONFIG
+      filter {
+        urldecode {
+          all_fields => true
+        }
+      }
+    CONFIG
+
+    sample("message" => "http%3A%2F%2Flogstash.net%2Fdocs%2F1.3.2%2Ffilters%2Furldecode", "nonencoded" => "http://logstash.net/docs/1.3.2/filters/urldecode") do
+      insist { subject["message"] } == "http://logstash.net/docs/1.3.2/filters/urldecode"
+      insist { subject["nonencoded"] } == "http://logstash.net/docs/1.3.2/filters/urldecode"
+    end
+
+  end
+
+end
diff --git a/spec/filters/useragent_spec.rb b/spec/filters/useragent_spec.rb
new file mode 100644
index 00000000000..cd1c6c1f4e6
--- /dev/null
+++ b/spec/filters/useragent_spec.rb
@@ -0,0 +1,43 @@
+# encoding: utf-8
+
+require "spec_helper"
+require "logstash/filters/useragent"
+
+describe LogStash::Filters::UserAgent do
+
+  describe "defaults" do
+    config <<-CONFIG
+      filter {
+        useragent {
+          source => "message"
+          target => "ua"
+        }
+      }
+    CONFIG
+
+    sample "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.63 Safari/537.31" do
+      insist { subject }.include?("ua")
+      insist { subject["ua"]["name"] } == "Chrome"
+      insist { subject["ua"]["os"] } == "Linux"
+      insist { subject["ua"]["major"] } == "26"
+      insist { subject["ua"]["minor"] } == "0"
+    end
+  end
+
+  describe "Without target field" do
+    config <<-CONFIG
+      filter {
+        useragent {
+          source => "message"
+        }
+      }
+    CONFIG
+
+    sample "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.63 Safari/537.31" do
+      insist { subject["name"] } == "Chrome"
+      insist { subject["os"] } == "Linux"
+      insist { subject["major"] } == "26"
+      insist { subject["minor"] } == "0"
+    end
+  end
+end
diff --git a/spec/filters/xml_spec.rb b/spec/filters/xml_spec.rb
new file mode 100644
index 00000000000..591f2a16759
--- /dev/null
+++ b/spec/filters/xml_spec.rb
@@ -0,0 +1,175 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/filters/xml"
+
+describe LogStash::Filters::Xml do
+
+  describe "parse standard xml (Deprecated checks)" do
+    config <<-CONFIG
+    filter {
+      xml {
+        source => "raw"
+        target => "data"
+      }
+    }
+    CONFIG
+
+    sample("raw" => '<foo key="value"/>') do
+      insist { subject["tags"] }.nil?
+      insist { subject["data"]} == {"key" => "value"}
+    end
+
+    #From parse xml with array as a value
+    sample("raw" => '<foo><key>value1</key><key>value2</key></foo>') do
+      insist { subject["tags"] }.nil?
+      insist { subject["data"]} == {"key" => ["value1", "value2"]}
+    end
+
+    #From parse xml with hash as a value
+    sample("raw" => '<foo><key1><key2>value</key2></key1></foo>') do
+      insist { subject["tags"] }.nil?
+      insist { subject["data"]} == {"key1" => [{"key2" => ["value"]}]}
+    end
+
+    #From bad xml
+    sample("raw" => '<foo /') do
+      insist { subject["tags"] }.include?("_xmlparsefailure")
+    end
+  end
+
+  describe "parse standard xml but do not store (Deprecated checks)" do
+    config <<-CONFIG
+    filter {
+      xml {
+        source => "raw"
+        target => "data"
+        store_xml => false
+      }
+    }
+    CONFIG
+
+    sample("raw" => '<foo key="value"/>') do
+      insist { subject["tags"] }.nil?
+      insist { subject["data"]} == nil
+    end
+  end
+
+  describe "parse xml and store values with xpath (Deprecated checks)" do
+    config <<-CONFIG
+    filter {
+      xml {
+        source => "raw"
+        target => "data"
+        xpath => [ "/foo/key/text()", "xpath_field" ]
+      }
+    }
+    CONFIG
+
+    # Single value
+    sample("raw" => '<foo><key>value</key></foo>') do
+      insist { subject["tags"] }.nil?
+      insist { subject["xpath_field"]} == ["value"]
+    end
+
+    #Multiple values
+    sample("raw" => '<foo><key>value1</key><key>value2</key></foo>') do
+      insist { subject["tags"] }.nil?
+      insist { subject["xpath_field"]} == ["value1","value2"]
+    end
+  end
+
+  ## New tests
+
+  describe "parse standard xml" do
+    config <<-CONFIG
+    filter {
+      xml {
+        source => "xmldata"
+        target => "data"
+      }
+    }
+    CONFIG
+
+    sample("xmldata" => '<foo key="value"/>') do
+      insist { subject["tags"] }.nil?
+      insist { subject["data"]} == {"key" => "value"}
+    end
+
+    #From parse xml with array as a value
+    sample("xmldata" => '<foo><key>value1</key><key>value2</key></foo>') do
+      insist { subject["tags"] }.nil?
+      insist { subject["data"]} == {"key" => ["value1", "value2"]}
+    end
+
+    #From parse xml with hash as a value
+    sample("xmldata" => '<foo><key1><key2>value</key2></key1></foo>') do
+      insist { subject["tags"] }.nil?
+      insist { subject["data"]} == {"key1" => [{"key2" => ["value"]}]}
+    end
+
+    #From bad xml
+    sample("xmldata" => '<foo /') do
+      insist { subject["tags"] }.include?("_xmlparsefailure")
+    end
+  end
+
+  describe "parse standard xml but do not store" do
+    config <<-CONFIG
+    filter {
+      xml {
+        source => "xmldata"
+        target => "data"
+        store_xml => false
+      }
+    }
+    CONFIG
+
+    sample("xmldata" => '<foo key="value"/>') do
+      insist { subject["tags"] }.nil?
+      insist { subject["data"]} == nil
+    end
+  end
+
+  describe "parse xml and store values with xpath" do
+    config <<-CONFIG
+    filter {
+      xml {
+        source => "xmldata"
+        target => "data"
+        xpath => [ "/foo/key/text()", "xpath_field" ]
+      }
+    }
+    CONFIG
+
+    # Single value
+    sample("xmldata" => '<foo><key>value</key></foo>') do
+      insist { subject["tags"] }.nil?
+      insist { subject["xpath_field"]} == ["value"]
+    end
+
+    #Multiple values
+    sample("xmldata" => '<foo><key>value1</key><key>value2</key></foo>') do
+      insist { subject["tags"] }.nil?
+      insist { subject["xpath_field"]} == ["value1","value2"]
+    end
+  end
+
+  describe "parse correctly non ascii content with xpath" do
+    config <<-CONFIG
+    filter {
+      xml {
+        source => "xmldata"
+        target => "data"
+        xpath => [ "/foo/key/text()", "xpath_field" ]
+      }
+    }
+    CONFIG
+
+    # Single value
+    sample("xmldata" => '<foo><key>Fran√ßais</key></foo>') do
+      insist { subject["tags"] }.nil?
+      insist { subject["xpath_field"]} == ["Fran√ßais"]
+    end
+  end
+
+end
diff --git a/spec/inputs/base_spec.rb b/spec/inputs/base_spec.rb
new file mode 100644
index 00000000000..4aec45401bb
--- /dev/null
+++ b/spec/inputs/base_spec.rb
@@ -0,0 +1,13 @@
+# encoding: utf-8
+require "spec_helper"
+
+describe "LogStash::Inputs::Base#fix_streaming_codecs" do
+  it "should carry the charset setting along when switching" do
+    require "logstash/inputs/tcp"
+    require "logstash/codecs/plain"
+    plain = LogStash::Codecs::Plain.new("charset" => "CP1252")
+    tcp = LogStash::Inputs::Tcp.new("codec" => plain, "port" => 3333)
+    tcp.instance_eval { fix_streaming_codecs }
+    insist { tcp.codec.charset } == "CP1252"
+  end
+end
diff --git a/spec/inputs/collectd_spec.rb b/spec/inputs/collectd_spec.rb
new file mode 100644
index 00000000000..7f3093a8d44
--- /dev/null
+++ b/spec/inputs/collectd_spec.rb
@@ -0,0 +1,329 @@
+require "spec_helper"
+require "socket"
+require "tempfile"
+
+describe "inputs/collectd", :socket => true do
+  
+  udp_sock = UDPSocket.new(Socket::AF_INET)
+
+  describe "parses a normal packet" do
+    config <<-CONFIG
+      input {
+        collectd {
+          type => "collectd"
+          host => "127.0.0.1"
+          # normal collectd port + 1
+          port => 25827
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      # Sleep so collectd can init itself
+      sleep 3
+
+      # Actual data :D
+      msg = ["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"].pack('H*')
+      udp_sock.send(msg, 0, "127.0.0.1", 25827)
+
+      sleep 1
+      insist { queue.size } == 28
+
+      events = 3.times.collect { queue.pop }
+      # Checking the timestamp fails with:
+      # Expected "2013-12-31T10:14:47.811Z", but got "2013-12-31T10:14:47.811Z"
+      # So... yeah.....
+
+      #timestamp = Time.iso8601("2013-12-31T10:14:47.811Z")
+
+      #insist { events[0]['@timestamp'] } == timestamp.utc
+      insist { events[0]['host'] } == "lieters-klaptop.prot.plexis.eu"
+      insist { events[0]['plugin'] } == "interface"
+      insist { events[0]['plugin_instance'] } == "wlan0"
+      insist { events[0]['collectd_type'] } == "if_errors"
+      insist { events[0]['rx'] } == 0
+      insist { events[0]['tx'] } == 0
+
+      #insist { events[2]['@timestamp'] } == timestamp
+      insist { events[2]['host'] } == "lieters-klaptop.prot.plexis.eu"
+      insist { events[2]['plugin'] } == "entropy"
+      insist { events[2]['collectd_type'] } == "entropy"
+      insist { events[2]['value'] } == 157.0
+    end
+  end
+
+  # Create an authfile
+  authfile = Tempfile.new('logstash-collectd-authfile')
+  File.open(authfile.path, "a") do |fd|
+    fd.puts("pieter: aapje1234")
+  end
+
+  describe "Parses correctly signed packet" do
+    config <<-CONFIG
+      input {
+        collectd {
+          type           => "collectd"
+          host           => "127.0.0.1"
+          # normal collectd port + 1
+          port           => 25827
+          authfile       => "#{authfile.path}"
+          security_level => "Sign"
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      # Sleep so collectd can init itself
+      sleep 3
+      msg = ["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"].pack('H*')
+      udp_sock.send(msg, 0, "127.0.0.1", 25827)
+
+      # give it time to process
+      sleep 3
+
+      insist { queue.size } == 24
+    end
+  end
+
+  describe "Does not parse incorrectly signed packet" do
+    config <<-CONFIG
+      input {
+        collectd {
+          type           => "collectd"
+          host           => "127.0.0.1"
+          # normal collectd port + 1
+          port           => 25827
+          authfile       => "#{authfile.path}"
+          security_level => "Sign"
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      # Sleep so collectd can init itself
+      sleep 3
+
+      # Wrong hash in packet
+      msg = ["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"].pack('H*')
+      udp_sock.send(msg, 0, "127.0.0.1", 25827)
+
+      # give it time to process
+      sleep 1
+
+      insist { queue.size } == 0
+    end # input
+  end # describe "Does not parse incorrectly signed packet"
+
+  describe "parses encrypted packet" do
+    config <<-CONFIG
+      input {
+        collectd {
+          type           => "collectd"
+          host           => "127.0.0.1"
+          # normal collectd port + 1
+          port           => 25827
+          authfile       => "#{authfile.path}"
+          security_level => "Encrypt"
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      # Sleep so collectd can init itself
+      sleep 3
+
+      msg = ["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"].pack('H*')
+      udp_sock.send(msg, 0, "127.0.0.1", 25827)
+
+      # give it time to process
+      sleep 2
+
+      insist { queue.size } == 24
+    end # input
+  end # describe "parses encrypted packet"
+
+  describe "does not parse unencrypted when configured to do so" do
+    config <<-CONFIG
+      input {
+        collectd {
+          type           => "collectd"
+          host           => "127.0.0.1"
+          # normal collectd port + 1
+          port           => 25827
+          authfile       => "#{authfile.path}"
+          security_level => "Encrypt"
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      # Sleep so collectd can init itself
+      sleep 3
+
+      msg = ["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"].pack('H*')
+      udp_sock.send(msg, 0, "127.0.0.1", 25827)
+
+      # give it time to process
+      sleep 2
+
+      insist { queue.size } == 0
+
+    end # input
+  end # describe
+
+  describe "changes NaN to 0 in the default config" do
+    config <<-CONFIG
+      input {
+        collectd {
+          type           => "collectd"
+          host           => "127.0.0.1"
+          # normal collectd port + 1
+          port           => 25827
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      # Sleep so collectd can init itself
+      sleep 3
+
+      msg = ['000000356b756d696e613a70726f64756374696f6e3a6c6965746572732d6b6c6170746f702e70726f742e706c657869732e6575000008000c14baed07bfc492e90009000c00000002800000000002000c63707566726571000004000c63707566726571000005000631000006000f0001010000000084d7c7410008000c14baed07bfc39a790005000630000006000f0001010000000084d7c7410008000c14baed07bfca78480002000764660000030009726f6f74000004000f64665f636f6d706c6578000005000966726565000006000f000101000000002e82ef410008000c14baed07bfcaa14c0005000d7265736572766564000006000f00010100000000a09ec5410008000c14baed07bfcaad4f0005000975736564000006000f00010100000080080d04420008000c14baed07bfcb0f2900030009626f6f74000005000966726565000006000f0001010000000048fcca410008000c14baed07bfcb1bc20005000d7265736572766564000006000f00010100000000c0cc90410008000c14baed07bfcb285b0005000975736564000006000f00010100000000009586410008000c14baed07bfcb489500030009686f6d65000005000966726565000006000f000101000000c0557a12420008000c14baed07bfcb54980005000d7265736572766564000006000f000101000000000020e4410008000c14baed07bfcb5f6f0005000975736564000006000f00010100000000d2181c420008000c14baed07bfc2f24f000200086370750000030006310000040008637075000005000e696e74657272757074000006000f00010200000000000000020008000c14baed07bfc2d4b80005000969646c65000006000f0001020000000000022ada0008000c14baed07bfc2bc68000500096e696365000006000f00010200000000000000080008000c14baed07bfeb5d1e0002000e6d656d6361636865640000030005000004001a6d656d6361636865645f636f6e6e656374696f6e73000005000c63757272656e74000006000f00010100000000000014400008000c14baed07bfeb947c000400166d656d6361636865645f636f6d6d616e640000050008676574000006000f00010200000000000000000008000c14baed07bfebb42100050008736574000006000f00010200000000000000000008000c14baed07bfebcd9e0005000a666c757368000006000f00010200000000000000000008000c14baed07bfebe5ee0005000a746f756368000006000f00010200000000000000000008000c14baed07bfebfdf5000400126d656d6361636865645f6f7073000005000968697473000006000f00010200000000000000000008000c14baed07bfec0ea80005000b6d6973736573000006000f00010200000000000000000008000c14baed07bfec278f00050010696e63725f6d6973736573000006000f00010200000000000000000008000c14baed07bfec36350005000e696e63725f68697473000006000f00010200000000000000000008000c14baed07bfec45bc00050010646563725f6d6973736573000006000f00010200000000000000000008000c14baed07bfec54620005000e646563725f68697473000006000f00010200000000000000000008000c14baed07bfeca08e0004000d70735f636f756e740000050005000006001800020101000000000000f87f00000000000010400008000c14baed07bfecce42000400146d656d6361636865645f6974656d73000005000c63757272656e74000006000f0001010000000000000000'].pack('H*')
+
+      udp_sock.send(msg, 0, "127.0.0.1", 25827)
+
+      # give it time to process
+      sleep 2
+
+      insist { queue.size } == 27
+
+      events = 26.times.collect { queue.pop }
+
+      insist { events[25]['tags'] } == ['_collectdNaN']
+      insist { events[25]['threads'] } == 4
+      insist { events[25]['processes'] } == 0
+
+    end # input do
+  end # describe
+
+  describe "changes NaN to -1 when configged to do so" do
+    config <<-CONFIG
+      input {
+        collectd {
+          type           => "collectd"
+          host           => "127.0.0.1"
+          # normal collectd port + 1
+          port           => 25827
+          nan_value      => -1
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      # Sleep so collectd can init itself
+      sleep 3
+
+      msg = ['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'].pack('H*')
+
+      udp_sock.send(msg, 0, "127.0.0.1", 25827)
+
+      # give it time to process
+      sleep 2
+
+      insist { queue.size } == 27
+
+      events = 26.times.collect { queue.pop }
+
+      insist { events[25]['tags'] } == ['_collectdNaN']
+      insist { events[25]['threads'] } == 4
+      insist { events[25]['processes'] } == -1
+
+    end # input do
+  end # describe
+
+  describe "Drops the event when NaN is found" do
+    config <<-CONFIG
+      input {
+        collectd {
+          type           => "collectd"
+          host           => "127.0.0.1"
+          # normal collectd port + 1
+          port           => 25827
+          nan_handeling  => 'drop'
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      # Sleep so collectd can init itself
+      sleep 3
+
+      msg = ['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'].pack('H*')
+
+      udp_sock.send(msg, 0, "127.0.0.1", 25827)
+
+      # give it time to process
+      sleep 2
+
+      insist { queue.size } == 26
+    end # input do
+  end # describe
+
+  describe "Empty nan_tag doesnt add a tag" do
+    config <<-CONFIG
+      input {
+        collectd {
+          type           => "collectd"
+          host           => "127.0.0.1"
+          # normal collectd port + 1
+          port           => 25827
+          nan_tag        => ''
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      # Sleep so collectd can init itself
+      sleep 3
+
+      msg = ['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'].pack('H*')
+
+      udp_sock.send(msg, 0, "127.0.0.1", 25827)
+
+      # give it time to process
+      sleep 2
+
+      events = 26.times.collect { queue.pop }
+
+      insist { events[25]['tags'] }.nil?
+    end # input do
+  end # describe
+
+end # describe "inputs/collectd"
+
diff --git a/spec/inputs/elasticsearch_spec.rb b/spec/inputs/elasticsearch_spec.rb
new file mode 100644
index 00000000000..4eb0f456346
--- /dev/null
+++ b/spec/inputs/elasticsearch_spec.rb
@@ -0,0 +1,80 @@
+require "spec_helper"
+require "logstash/inputs/elasticsearch"
+
+describe "inputs/elasticsearch" do
+  
+
+  search_response = <<-RESPONSE
+    {
+      "_scroll_id":"xxx",
+      "took":5,
+      "timed_out":false,
+      "_shards":{"total":15,"successful":15,"failed":0},
+      "hits":{
+        "total":1000050,
+        "max_score":1.0,
+        "hits":[
+          {
+            "_index":"logstash2",
+            "_type":"logs",
+            "_id":"AmaqL7VuSWKF-F6N_Gz72g",
+            "_score":1.0,
+            "_source" : {
+              "message":"foobar",
+              "@version":"1",
+              "@timestamp":"2014-05-19T21:08:39.000Z",
+              "host":"colin-mbp13r"
+            }
+          }
+        ]
+      }
+    }
+  RESPONSE
+
+  scroll_response = <<-RESPONSE
+    {
+      "hits":{
+        "hits":[]
+      }
+    }
+  RESPONSE
+
+  config <<-CONFIG
+    input {
+      elasticsearch {
+        host => "localhost"
+        scan => false
+      }
+    }
+  CONFIG
+
+  it "should retrieve json event from elasticseach" do
+    # I somewhat duplicated our "input" rspec extension because I needed to add mocks for the the actual ES calls
+    # and rspec expectations need to be in "it" statement but the "input" extension defines the "it"
+    # TODO(colin) see how we can improve our rspec extension to better integrate in these scenarios
+
+    expect_any_instance_of(LogStash::Inputs::Elasticsearch).to receive(:execute_search_request).and_return(search_response)
+    expect_any_instance_of(LogStash::Inputs::Elasticsearch).to receive(:execute_scroll_request).with(any_args).and_return(scroll_response)
+
+    pipeline = LogStash::Pipeline.new(config)
+    queue = Queue.new
+    pipeline.instance_eval do
+      @output_func = lambda { |event| queue << event }
+    end
+    pipeline_thread = Thread.new { pipeline.run }
+    event = queue.pop
+
+    insist { event["message"] } == "foobar"
+
+    # do not call pipeline.shutdown here, as it will stop the plugin execution randomly
+    # and maybe kill input before calling execute_scroll_request.
+    # TODO(colin) we should rework the pipeliene shutdown to allow a soft/clean shutdown mecanism,
+    # using a shutdown event which can be fed into each plugin queue and when the plugin sees it
+    # exits after completing its processing.
+    #
+    # pipeline.shutdown
+    #
+    # instead, since our scroll_response will terminate the plugin, we can just join the pipeline thread
+    pipeline_thread.join
+  end
+end
diff --git a/spec/inputs/file_spec.rb b/spec/inputs/file_spec.rb
new file mode 100644
index 00000000000..462ade1c454
--- /dev/null
+++ b/spec/inputs/file_spec.rb
@@ -0,0 +1,132 @@
+# encoding: utf-8
+
+require "spec_helper"
+require "tempfile"
+
+describe "inputs/file" do
+  
+
+  describe "starts at the end of an existing file" do
+    tmp_file = Tempfile.new('logstash-spec-input-file')
+
+    config <<-CONFIG
+      input {
+        file {
+          type => "blah"
+          path => "#{tmp_file.path}"
+          sincedb_path => "/dev/null"
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      File.open(tmp_file, "w") do |fd|
+        fd.puts("ignore me 1")
+        fd.puts("ignore me 2")
+      end
+
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      # at this point even if pipeline.ready? == true the plugins
+      # threads might still be initializing so we cannot know when the
+      # file plugin will have seen the original file, it could see it
+      # after the first(s) hello world appends below, hence the
+      # retry logic.
+
+      retries = 0
+      loop do
+        insist { retries } < 20 # 2 secs should be plenty?
+
+        File.open(tmp_file, "a") do |fd|
+          fd.puts("hello")
+          fd.puts("world")
+        end
+
+        if queue.size >= 2
+          events = 2.times.collect { queue.pop }
+          insist { events[0]["message"] } == "hello"
+          insist { events[1]["message"] } == "world"
+          break
+        end
+
+        sleep(0.1)
+        retries += 1
+      end
+    end
+  end
+
+  describe "can start at the beginning of an existing file" do
+    tmp_file = Tempfile.new('logstash-spec-input-file')
+
+    config <<-CONFIG
+      input {
+        file {
+          type => "blah"
+          path => "#{tmp_file.path}"
+          start_position => "beginning"
+          sincedb_path => "/dev/null"
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      File.open(tmp_file, "a") do |fd|
+        fd.puts("hello")
+        fd.puts("world")
+      end
+
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      events = 2.times.collect { queue.pop }
+      insist { events[0]["message"] } == "hello"
+      insist { events[1]["message"] } == "world"
+    end
+  end
+
+  describe "restarts at the sincedb value" do
+    tmp_file = Tempfile.new('logstash-spec-input-file')
+    tmp_sincedb = Tempfile.new('logstash-spec-input-file-sincedb')
+
+    config <<-CONFIG
+      input {
+        file {
+          type => "blah"
+          path => "#{tmp_file.path}"
+          start_position => "beginning"
+          sincedb_path => "#{tmp_sincedb.path}"
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      File.open(tmp_file, "w") do |fd|
+        fd.puts("hello")
+        fd.puts("world")
+      end
+
+      t = Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      events = 2.times.collect { queue.pop }
+      pipeline.shutdown
+      t.join
+
+      File.open(tmp_file, "a") do |fd|
+        fd.puts("foo")
+        fd.puts("bar")
+        fd.puts("baz")
+      end
+
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      events = 3.times.collect { queue.pop }
+
+      insist { events[0]["message"] } == "foo"
+      insist { events[1]["message"] } == "bar"
+      insist { events[2]["message"] } == "baz"
+    end
+  end
+end
diff --git a/spec/inputs/gelf_spec.rb b/spec/inputs/gelf_spec.rb
new file mode 100644
index 00000000000..32f3aaca99c
--- /dev/null
+++ b/spec/inputs/gelf_spec.rb
@@ -0,0 +1,52 @@
+
+require "spec_helper"
+require "gelf"
+describe "inputs/gelf" do
+  
+
+  describe "reads chunked gelf messages " do
+    port = 12209
+    host = "127.0.0.1"
+    chunksize = 1420
+    gelfclient = GELF::Notifier.new(host,port,chunksize)
+
+    config <<-CONFIG
+      input {
+        gelf {
+          port => "#{port}"
+          host => "#{host}"
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      # generate random characters (message is zipped!) from printable ascii ( SPACE till ~ )
+      # to trigger gelf chunking
+      s = StringIO.new
+      for i in 1..2000
+        s << 32 + rand(126-32)
+      end
+      large_random = s.string
+
+      [ "hello",
+        "world",
+        large_random,
+        "we survived gelf!"
+      ].each do |m|
+  	    gelfclient.notify!( "short_message" => m )
+        # poll at most 10 times
+        waits = 0
+        while waits < 10 and queue.size == 0
+          sleep 0.1
+          waits += 1
+        end
+        insist { queue.size } > 0
+        insist { queue.pop["message"] } == m
+      end
+
+    end
+  end
+end
diff --git a/spec/inputs/generator_spec.rb b/spec/inputs/generator_spec.rb
new file mode 100644
index 00000000000..0df2c0dae26
--- /dev/null
+++ b/spec/inputs/generator_spec.rb
@@ -0,0 +1,86 @@
+require "spec_helper"
+
+describe "inputs/generator" do
+  
+
+  context "performance", :performance => true do
+    event_count = 100000 + rand(50000)
+
+    config <<-CONFIG
+      input {
+        generator {
+          type => "blah"
+          count => #{event_count}
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      start = Time.now
+      Thread.new { pipeline.run }
+      event_count.times do |i|
+        event = queue.pop
+        insist { event["sequence"] } == i
+      end
+      duration = Time.now - start
+      puts "inputs/generator rate: #{"%02.0f/sec" % (event_count / duration)}, elapsed: #{duration}s"
+      pipeline.shutdown
+    end # input
+  end
+
+  context "generate configured message" do
+    config <<-CONFIG
+      input {
+        generator {
+          count => 2
+          message => "foo"
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      event = queue.pop
+      insist { event["sequence"] } == 0
+      insist { event["message"] } == "foo"
+
+      event = queue.pop
+      insist { event["sequence"] } == 1
+      insist { event["message"] } == "foo"
+
+      insist { queue.size } == 0
+      pipeline.shutdown
+    end # input
+
+    context "generate message from stdin" do
+      config <<-CONFIG
+        input {
+          generator {
+            count => 2
+            message => "stdin"
+          }
+        }
+      CONFIG
+
+      input do |pipeline, queue|
+        saved_stdin = $stdin
+        stdin_mock = StringIO.new
+        $stdin = stdin_mock
+        stdin_mock.should_receive(:readline).once.and_return("bar")
+
+        Thread.new { pipeline.run }
+        event = queue.pop
+        insist { event["sequence"] } == 0
+        insist { event["message"] } == "bar"
+
+        event = queue.pop
+        insist { event["sequence"] } == 1
+        insist { event["message"] } == "bar"
+
+        insist { queue.size } == 0
+        pipeline.shutdown
+        $stdin = saved_stdin
+      end # input
+    end
+  end
+end
diff --git a/spec/inputs/imap_spec.rb b/spec/inputs/imap_spec.rb
new file mode 100644
index 00000000000..51da9a61fd3
--- /dev/null
+++ b/spec/inputs/imap_spec.rb
@@ -0,0 +1,92 @@
+# encoding: utf-8
+
+require "logstash/inputs/imap"
+require "mail"
+
+describe LogStash::Inputs::IMAP do
+  user = "logstash"
+  password = "secret"
+  msg_time = Time.new
+  msg_text = "foo\nbar\nbaz"
+  msg_html = "<p>a paragraph</p>\n\n"
+
+  subject do
+    Mail.new do
+      from     "me@example.com"
+      to       "you@example.com"
+      subject  "logstash imap input test"
+      date     msg_time
+      body     msg_text
+      add_file :filename => "some.html", :content => msg_html
+    end
+  end
+
+  context "with both text and html parts" do
+    context "when no content-type selected" do
+      it "should select text/plain part" do
+        config = {"type" => "imap", "host" => "localhost",
+                  "user" => "#{user}", "password" => "#{password}"}
+
+        input = LogStash::Inputs::IMAP.new config
+        input.register
+        event = input.parse_mail(subject)
+        insist { event["message"] } == msg_text
+      end
+    end
+
+    context "when text/html content-type selected" do
+      it "should select text/html part" do
+        config = {"type" => "imap", "host" => "localhost",
+                  "user" => "#{user}", "password" => "#{password}",
+                  "content_type" => "text/html"}
+
+        input = LogStash::Inputs::IMAP.new config
+        input.register
+        event = input.parse_mail(subject)
+        insist { event["message"] } == msg_html
+      end
+    end
+  end
+
+  context "when subject is in RFC 2047 encoded-word format" do
+    it "should be decoded" do
+      subject.subject = "=?iso-8859-1?Q?foo_:_bar?="
+      config = {"type" => "imap", "host" => "localhost",
+                "user" => "#{user}", "password" => "#{password}"}
+
+      input = LogStash::Inputs::IMAP.new config
+      input.register
+      event = input.parse_mail(subject)
+      insist { event["subject"] } == "foo : bar"
+    end
+  end
+
+  context "with multiple values for same header" do
+    it "should add 2 values as array in event" do
+      subject.received = "test1"
+      subject.received = "test2"
+
+      config = {"type" => "imap", "host" => "localhost",
+                "user" => "#{user}", "password" => "#{password}"}
+
+      input = LogStash::Inputs::IMAP.new config
+      input.register
+      event = input.parse_mail(subject)
+      insist { event["received"] } == ["test1", "test2"]
+    end
+
+    it "should add more than 2 values as array in event" do
+      subject.received = "test1"
+      subject.received = "test2"
+      subject.received = "test3"
+
+      config = {"type" => "imap", "host" => "localhost",
+                "user" => "#{user}", "password" => "#{password}"}
+
+      input = LogStash::Inputs::IMAP.new config
+      input.register
+      event = input.parse_mail(subject)
+      insist { event["received"] } == ["test1", "test2", "test3"]
+    end
+  end
+end
diff --git a/spec/inputs/log4j_spec.rb b/spec/inputs/log4j_spec.rb
new file mode 100644
index 00000000000..8e38bb4db0d
--- /dev/null
+++ b/spec/inputs/log4j_spec.rb
@@ -0,0 +1,13 @@
+# encoding: utf-8
+
+require "logstash/plugin"
+
+describe "inputs/log4j" do
+
+  it "should register" do
+    input = LogStash::Plugin.lookup("input", "log4j").new("mode" => "client")
+
+    # register will try to load jars and raise if it cannot find jars or if org.apache.log4j.spi.LoggingEvent class is not present
+    expect {input.register}.to_not raise_error
+  end
+end
diff --git a/spec/inputs/pipe_spec.rb b/spec/inputs/pipe_spec.rb
new file mode 100644
index 00000000000..e78a3dcceb5
--- /dev/null
+++ b/spec/inputs/pipe_spec.rb
@@ -0,0 +1,60 @@
+# encoding: utf-8
+require "spec_helper"
+require "tempfile"
+
+describe "inputs/pipe" do
+  
+
+  describe "echo" do
+    event_count = 1
+    tmp_file = Tempfile.new('logstash-spec-input-pipe')
+
+    config <<-CONFIG
+    input {
+      pipe {
+        command => "echo ‚òπ"
+      }
+    }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      events = event_count.times.collect { queue.pop }
+      event_count.times do |i|
+        insist { events[i]["message"] } == "‚òπ"
+      end
+    end # input
+  end
+
+  describe "tail -f" do
+    event_count = 10
+    tmp_file = Tempfile.new('logstash-spec-input-pipe')
+
+    config <<-CONFIG
+    input {
+      pipe {
+        command => "tail -f #{tmp_file.path}"
+      }
+    }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      File.open(tmp_file, "a") do |fd|
+        event_count.times do |i|
+          # unicode smiley for testing unicode support!
+          fd.puts("#{i} ‚òπ")
+        end
+      end
+      events = event_count.times.collect { queue.pop }
+      event_count.times do |i|
+        insist { events[i]["message"] } == "#{i} ‚òπ"
+      end
+    end # input
+  end
+
+end
diff --git a/spec/inputs/redis_spec.rb b/spec/inputs/redis_spec.rb
new file mode 100644
index 00000000000..04eceaf1348
--- /dev/null
+++ b/spec/inputs/redis_spec.rb
@@ -0,0 +1,63 @@
+require "spec_helper"
+require "redis"
+
+def populate(key, event_count)
+  require "logstash/event"
+  redis = Redis.new(:host => "localhost")
+  event_count.times do |value|
+    event = LogStash::Event.new("sequence" => value)
+    Stud::try(10.times) do
+      redis.rpush(key, event.to_json)
+    end
+  end
+end
+
+def process(pipeline, queue, event_count)
+  sequence = 0
+  Thread.new { pipeline.run }
+  event_count.times do |i|
+    event = queue.pop
+    insist { event["sequence"] } == i
+  end
+  pipeline.shutdown
+end # process
+
+describe "inputs/redis", :redis => true do
+  
+
+  describe "read events from a list" do
+    key = 10.times.collect { rand(10).to_s }.join("")
+    event_count = 1000 + rand(50)
+    config <<-CONFIG
+      input {
+        redis {
+          type => "blah"
+          key => "#{key}"
+          data_type => "list"
+        }
+      }
+    CONFIG
+
+    before(:each) { populate(key, event_count) }
+
+    input { |pipeline, queue| process(pipeline, queue, event_count) }
+  end
+
+  describe "read events from a list with batch_count=5" do
+    key = 10.times.collect { rand(10).to_s }.join("")
+    event_count = 1000 + rand(50)
+    config <<-CONFIG
+      input {
+        redis {
+          type => "blah"
+          key => "#{key}"
+          data_type => "list"
+          batch_count => #{rand(20)+1}
+        }
+      }
+    CONFIG
+
+    before(:each) { populate(key, event_count) }
+    input { |pipeline, queue| process(pipeline, queue, event_count) }
+  end
+end
diff --git a/spec/inputs/stdin_spec.rb b/spec/inputs/stdin_spec.rb
new file mode 100644
index 00000000000..8da76aad9d6
--- /dev/null
+++ b/spec/inputs/stdin_spec.rb
@@ -0,0 +1,23 @@
+# encoding: utf-8
+require "spec_helper"
+require "socket"
+require "logstash/inputs/stdin"
+
+describe LogStash::Inputs::Stdin do
+  context "codec (PR #1372)" do
+    it "switches from plain to line" do
+      require "logstash/codecs/plain"
+      require "logstash/codecs/line"
+      plugin = LogStash::Inputs::Stdin.new("codec" => LogStash::Codecs::Plain.new)
+      plugin.register
+      insist { plugin.codec }.is_a?(LogStash::Codecs::Line)
+    end
+    it "switches from json to json_lines" do
+      require "logstash/codecs/json"
+      require "logstash/codecs/json_lines"
+      plugin = LogStash::Inputs::Stdin.new("codec" => LogStash::Codecs::JSON.new)
+      plugin.register
+      insist { plugin.codec }.is_a?(LogStash::Codecs::JSONLines)
+    end
+  end
+end
diff --git a/spec/inputs/syslog_spec.rb b/spec/inputs/syslog_spec.rb
new file mode 100644
index 00000000000..48bb7552c37
--- /dev/null
+++ b/spec/inputs/syslog_spec.rb
@@ -0,0 +1,92 @@
+# coding: utf-8
+require "spec_helper"
+require "socket"
+require "logstash/inputs/syslog"
+require "logstash/event"
+
+describe "inputs/syslog" do
+  
+
+  it "should properly handle priority, severity and facilities", :socket => true do
+    port = 5511
+    event_count = 10
+
+    config <<-CONFIG
+      input {
+        syslog {
+          type => "blah"
+          port => #{port}
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      socket = Stud.try(5.times) { TCPSocket.new("127.0.0.1", port) }
+      event_count.times do |i|
+        socket.puts("<164>Oct 26 15:19:25 1.2.3.4 %ASA-4-106023: Deny udp src DRAC:10.1.2.3/43434 dst outside:192.168.0.1/53 by access-group \"acl_drac\" [0x0, 0x0]")
+      end
+      socket.close
+
+      events = event_count.times.collect { queue.pop }
+
+      insist { events.length } == event_count
+      event_count.times do |i|
+        insist { events[i]["priority"] } == 164
+        insist { events[i]["severity"] } == 4
+        insist { events[i]["facility"] } == 20
+      end
+    end
+  end
+
+  it "should add unique tag when grok parsing fails with live syslog input", :socket => true do
+    port = 5511
+    event_count = 10
+
+    config <<-CONFIG
+      input {
+        syslog {
+          type => "blah"
+          port => #{port}
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      socket = Stud.try(5.times) { TCPSocket.new("127.0.0.1", port) }
+      event_count.times do |i|
+        socket.puts("message which causes the a grok parse failure")
+      end
+      socket.close
+
+      events = event_count.times.collect { queue.pop }
+
+      insist { events.length } == event_count
+      event_count.times do |i|
+        insist { events[i]["tags"] } == ["_grokparsefailure_sysloginputplugin"]
+      end
+    end
+  end
+
+  it "should add unique tag when grok parsing fails" do
+    input = LogStash::Inputs::Syslog.new({})
+    input.register
+
+    # event which is not syslog should have a new tag
+    event = LogStash::Event.new({ "message" => "hello world, this is not syslog RFC3164" })
+    input.syslog_relay(event)
+    insist { event["tags"] } ==  ["_grokparsefailure_sysloginput"]
+
+    syslog_event = LogStash::Event.new({ "message" => "<164>Oct 26 15:19:25 1.2.3.4 %ASA-4-106023: Deny udp src DRAC:10.1.2.3/43434" })
+    input.syslog_relay(syslog_event)
+    insist { syslog_event["priority"] } ==  164
+    insist { syslog_event["severity"] } ==  4
+    insist { syslog_event["tags"] } ==  nil
+  end
+
+end
diff --git a/spec/inputs/tcp_spec.rb b/spec/inputs/tcp_spec.rb
new file mode 100644
index 00000000000..bbec11f010d
--- /dev/null
+++ b/spec/inputs/tcp_spec.rb
@@ -0,0 +1,280 @@
+# encoding: utf-8
+require "spec_helper"
+require "socket"
+require "timeout"
+require "logstash/json"
+require "logstash/inputs/tcp"
+require 'stud/try'
+
+describe LogStash::Inputs::Tcp do
+  
+
+  context "codec (PR #1372)" do
+    it "switches from plain to line" do
+      require "logstash/codecs/plain"
+      require "logstash/codecs/line"
+      plugin = LogStash::Inputs::Tcp.new("codec" => LogStash::Codecs::Plain.new, "port" => 0)
+      plugin.register
+      insist { plugin.codec }.is_a?(LogStash::Codecs::Line)
+    end
+    it "switches from json to json_lines" do
+      require "logstash/codecs/json"
+      require "logstash/codecs/json_lines"
+      plugin = LogStash::Inputs::Tcp.new("codec" => LogStash::Codecs::JSON.new, "port" => 0)
+      plugin.register
+      insist { plugin.codec }.is_a?(LogStash::Codecs::JSONLines)
+    end
+  end
+
+  describe "read plain with unicode", :socket => true do
+    event_count = 10
+    port = 5511
+    config <<-CONFIG
+      input {
+        tcp {
+          port => #{port}
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      socket = Stud::try(5.times) { TCPSocket.new("127.0.0.1", port) }
+      event_count.times do |i|
+        # unicode smiley for testing unicode support!
+        socket.puts("#{i} ‚òπ")
+      end
+      socket.close
+
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < event_count}
+
+      events = event_count.times.collect { queue.pop }
+      event_count.times do |i|
+        insist { events[i]["message"] } == "#{i} ‚òπ"
+      end
+    end # input
+  end
+
+  describe "read events with plain codec and ISO-8859-1 charset" do
+    port = 5513
+    charset = "ISO-8859-1"
+    config <<-CONFIG
+      input {
+        tcp {
+          port => #{port}
+          codec => plain { charset => "#{charset}" }
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      socket = Stud::try(5.times) { TCPSocket.new("127.0.0.1", port) }
+      text = "\xA3" # the ¬£ symbol in ISO-8859-1 aka Latin-1
+      text.force_encoding("ISO-8859-1")
+      socket.puts(text)
+      socket.close
+
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < 1}
+
+      event = queue.pop
+      # Make sure the 0xA3 latin-1 code converts correctly to UTF-8.
+      pending("charset conv broken") do
+        insist { event["message"].size } == 1
+        insist { event["message"].bytesize } == 2
+        insist { event["message"] } == "¬£"
+      end
+    end # input
+  end
+
+  describe "read events with json codec" do
+    port = 5514
+    config <<-CONFIG
+      input {
+        tcp {
+          port => #{port}
+          codec => json
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      data = {
+        "hello" => "world",
+        "foo" => [1,2,3],
+        "baz" => { "1" => "2" },
+        "host" => "example host"
+      }
+
+      socket = Stud::try(5.times) { TCPSocket.new("127.0.0.1", port) }
+      socket.puts(LogStash::Json.dump(data))
+      socket.close
+
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < 1}
+
+      event = queue.pop
+      insist { event["hello"] } == data["hello"]
+      insist { event["foo"].to_a } == data["foo"] # to_a to cast Java ArrayList produced by JrJackson
+      insist { event["baz"] } == data["baz"]
+
+      # Make sure the tcp input, w/ json codec, uses the event's 'host' value,
+      # if present, instead of providing its own
+      insist { event["host"] } == data["host"]
+    end # input
+  end
+
+  describe "read events with json codec (testing 'host' handling)" do
+    port = 5514
+    config <<-CONFIG
+      input {
+        tcp {
+          port => #{port}
+          codec => json
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      data = {
+        "hello" => "world"
+      }
+
+      socket = Stud::try(5.times) { TCPSocket.new("127.0.0.1", port) }
+      socket.puts(LogStash::Json.dump(data))
+      socket.close
+
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < 1}
+
+      event = queue.pop
+      insist { event["hello"] } == data["hello"]
+      insist { event }.include?("host")
+    end # input
+  end
+
+  describe "read events with json_lines codec" do
+    port = 5515
+    config <<-CONFIG
+      input {
+        tcp {
+          port => #{port}
+          codec => json_lines
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      data = {
+        "hello" => "world",
+        "foo" => [1,2,3],
+        "baz" => { "1" => "2" },
+        "idx" => 0
+      }
+
+      socket = Stud::try(5.times) { TCPSocket.new("127.0.0.1", port) }
+      (1..5).each do |idx|
+        data["idx"] = idx
+        socket.puts(LogStash::Json.dump(data) + "\n")
+      end # do
+      socket.close
+
+      (1..5).each do |idx|
+        event = queue.pop
+        insist { event["hello"] } == data["hello"]
+        insist { event["foo"].to_a } == data["foo"] # to_a to cast Java ArrayList produced by JrJackson
+        insist { event["baz"] } == data["baz"]
+        insist { event["idx"] } == idx
+      end # do
+    end # input
+  end # describe
+
+  describe "one message per connection" do
+    event_count = 10
+    port = 5516
+    config <<-CONFIG
+      input {
+        tcp {
+          port => #{port}
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      event_count.times do |i|
+        socket = Stud::try(5.times) { TCPSocket.new("127.0.0.1", port) }
+        socket.puts("#{i}")
+        socket.flush
+        socket.close
+      end
+      
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < event_count}
+
+      # since each message is sent on its own tcp connection & thread, exact receiving order cannot be garanteed
+      events = event_count.times.collect{queue.pop}.sort_by{|event| event["message"]}
+
+      event_count.times do |i|
+        insist { events[i]["message"] } == "#{i}"
+      end
+    end # input
+  end
+
+  describe "connection threads are cleaned up when connection is closed" do
+    event_count = 10
+    port = 5517
+    config <<-CONFIG
+      input {
+        tcp {
+          port => #{port}
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      inputs = pipeline.instance_variable_get("@inputs")
+      insist { inputs.size } == 1
+
+      sockets = event_count.times.map do |i|
+        socket = Stud::try(5.times) { TCPSocket.new("127.0.0.1", port) }
+        socket.puts("#{i}")
+        socket.flush
+        socket
+      end
+
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < event_count}
+
+      # we should have "event_count" pending threads since sockets were not closed yet
+      client_threads = inputs[0].instance_variable_get("@client_threads")
+      insist { client_threads.size } == event_count
+
+      # close all sockets and make sure there is not more pending threads
+      sockets.each{|socket| socket.close}
+      Timeout.timeout(1) {sleep 0.1 while client_threads.size > 0}
+      insist { client_threads.size } == 0 # this check is actually useless per previous line
+
+    end # input
+  end
+end
diff --git a/spec/logstash_helpers.rb b/spec/logstash_helpers.rb
new file mode 100644
index 00000000000..0439661e059
--- /dev/null
+++ b/spec/logstash_helpers.rb
@@ -0,0 +1,77 @@
+require "logstash/agent"
+require "logstash/pipeline"
+require "logstash/event"
+
+module LogStashHelper
+
+  def config(configstr)
+    let(:config) { configstr }
+  end # def config
+
+  def type(default_type)
+    let(:default_type) { default_type }
+  end
+
+  def tags(*tags)
+    let(:default_tags) { tags }
+    puts "Setting default tags: #{tags}"
+  end
+
+  def sample(sample_event, &block)
+    name = sample_event.is_a?(String) ? sample_event : LogStash::Json.dump(sample_event)
+    name = name[0..50] + "..." if name.length > 50
+
+    describe "\"#{name}\"" do
+      let(:pipeline) { LogStash::Pipeline.new(config) }
+      let(:event) do
+        sample_event = [sample_event] unless sample_event.is_a?(Array)
+        next sample_event.collect do |e|
+          e = { "message" => e } if e.is_a?(String)
+          next LogStash::Event.new(e)
+        end
+      end
+
+      let(:results) do
+        results = []
+        pipeline.instance_eval { @filters.each(&:register) }
+
+        event.each do |e|
+          pipeline.filter(e) {|new_event| results << new_event }
+        end
+
+        pipeline.flush_filters(:final => true) do |e|
+          results << e unless e.cancelled?
+        end
+
+        results
+      end
+
+      subject { results.length > 1 ? results: results.first }
+
+      it("when processed", &block)
+    end
+  end # def sample
+
+  def input(&block)
+    it "inputs" do
+      pipeline = LogStash::Pipeline.new(config)
+      queue = Queue.new
+      pipeline.instance_eval do
+        @output_func = lambda { |event| queue << event }
+      end
+      block.call(pipeline, queue)
+      pipeline.shutdown
+    end
+  end # def input
+
+  def agent(&block)
+
+    it("agent(#{caller[0].gsub(/ .*/, "")}) runs") do
+      pipeline = LogStash::Pipeline.new(config)
+      pipeline.run
+      block.call
+    end
+  end # def agent
+
+end # module LogStash
+
diff --git a/spec/outputs/cloudwatch_spec.rb b/spec/outputs/cloudwatch_spec.rb
new file mode 100644
index 00000000000..047d9968c5e
--- /dev/null
+++ b/spec/outputs/cloudwatch_spec.rb
@@ -0,0 +1,18 @@
+require "spec_helper"
+require "logstash/plugin"
+require "logstash/json"
+
+describe "outputs/cloudwatch" do
+  
+
+  output = LogStash::Plugin.lookup("output", "cloudwatch").new
+
+  it "should register" do
+    expect {output.register}.to_not raise_error
+  end
+
+  it "should respond correctly to a receive call" do
+    event = LogStash::Event.new
+    expect { output.receive(event) }.to_not raise_error
+  end
+end
diff --git a/spec/outputs/csv_spec.rb b/spec/outputs/csv_spec.rb
new file mode 100644
index 00000000000..5430cb3235b
--- /dev/null
+++ b/spec/outputs/csv_spec.rb
@@ -0,0 +1,266 @@
+require "csv"
+require "tempfile"
+require "spec_helper"
+require "logstash/outputs/csv"
+
+describe LogStash::Outputs::CSV do
+  
+
+  describe "Write a single field to a csv file" do
+    tmpfile = Tempfile.new('logstash-spec-output-csv')
+    config <<-CONFIG
+      input {
+        generator {
+          add_field => ["foo","bar"]
+          count => 1
+        }
+      }
+      output {
+        csv {
+          path => "#{tmpfile.path}"
+          fields => "foo"
+        }
+      }
+    CONFIG
+
+    agent do
+      lines = File.readlines(tmpfile.path)
+      insist {lines.count} == 1
+      insist {lines[0]} == "bar\n"
+    end
+  end
+
+  describe "write multiple fields and lines to a csv file" do
+    tmpfile = Tempfile.new('logstash-spec-output-csv')
+    config <<-CONFIG
+      input {
+        generator {
+          add_field => ["foo", "bar", "baz", "quux"]
+          count => 2
+        }
+      }
+      output {
+        csv {
+          path => "#{tmpfile.path}"
+          fields => ["foo", "baz"]
+        }
+      }
+    CONFIG
+
+    agent do
+      lines = File.readlines(tmpfile.path)
+      insist {lines.count} == 2
+      insist {lines[0]} == "bar,quux\n"
+      insist {lines[1]} == "bar,quux\n"
+    end
+  end
+
+  describe "missing event fields are empty in csv" do
+    tmpfile = Tempfile.new('logstash-spec-output-csv')
+    config <<-CONFIG
+      input {
+        generator {
+          add_field => ["foo","bar", "baz", "quux"]
+          count => 1
+        }
+      }
+      output {
+        csv {
+          path => "#{tmpfile.path}"
+          fields => ["foo", "not_there", "baz"]
+        }
+      }
+    CONFIG
+
+    agent do
+      lines = File.readlines(tmpfile.path)
+      insist {lines.count} == 1
+      insist {lines[0]} == "bar,,quux\n"
+    end
+  end
+
+  describe "commas are quoted properly" do
+    tmpfile = Tempfile.new('logstash-spec-output-csv')
+    config <<-CONFIG
+      input {
+        generator {
+          add_field => ["foo","one,two", "baz", "quux"]
+          count => 1
+        }
+      }
+      output {
+        csv {
+          path => "#{tmpfile.path}"
+          fields => ["foo", "baz"]
+        }
+      }
+    CONFIG
+
+    agent do
+      lines = File.readlines(tmpfile.path)
+      insist {lines.count} == 1
+      insist {lines[0]} == "\"one,two\",quux\n"
+    end
+  end
+
+  describe "new lines are quoted properly" do
+    tmpfile = Tempfile.new('logstash-spec-output-csv')
+    config <<-CONFIG
+      input {
+        generator {
+          add_field => ["foo","one\ntwo", "baz", "quux"]
+          count => 1
+        }
+      }
+      output {
+        csv {
+          path => "#{tmpfile.path}"
+          fields => ["foo", "baz"]
+        }
+      }
+    CONFIG
+
+    agent do
+      lines = CSV.read(tmpfile.path)
+      insist {lines.count} == 1
+      insist {lines[0][0]} == "one\ntwo"
+    end
+  end
+
+  describe "fields that are are objects are written as JSON" do
+    tmpfile = Tempfile.new('logstash-spec-output-csv')
+    config <<-CONFIG
+      input {
+        generator {
+          message => '{"foo":{"one":"two"},"baz": "quux"}'
+          count => 1
+        }
+      }
+      filter {
+        json { source => "message"}
+      }
+      output {
+        csv {
+          path => "#{tmpfile.path}"
+          fields => ["foo", "baz"]
+        }
+      }
+    CONFIG
+
+    agent do
+      lines = CSV.read(tmpfile.path)
+      insist {lines.count} == 1
+      insist {lines[0][0]} == '{"one":"two"}'
+    end
+  end
+
+  describe "can address nested field using field reference syntax" do
+    tmpfile = Tempfile.new('logstash-spec-output-csv')
+    config <<-CONFIG
+      input {
+        generator {
+          message => '{"foo":{"one":"two"},"baz": "quux"}'
+          count => 1
+        }
+      }
+      filter {
+        json { source => "message"}
+      }
+      output {
+        csv {
+          path => "#{tmpfile.path}"
+          fields => ["[foo][one]", "baz"]
+        }
+      }
+    CONFIG
+
+    agent do
+      lines = CSV.read(tmpfile.path)
+      insist {lines.count} == 1
+      insist {lines[0][0]} == "two"
+      insist {lines[0][1]} == "quux"
+    end
+  end
+
+  describe "missing nested field is blank" do
+    tmpfile = Tempfile.new('logstash-spec-output-csv')
+    config <<-CONFIG
+      input {
+        generator {
+          message => '{"foo":{"one":"two"},"baz": "quux"}'
+          count => 1
+        }
+      }
+      filter {
+        json { source => "message"}
+      }
+      output {
+        csv {
+          path => "#{tmpfile.path}"
+          fields => ["[foo][missing]", "baz"]
+        }
+      }
+    CONFIG
+
+    agent do
+      lines = File.readlines(tmpfile.path)
+      insist {lines.count} == 1
+      insist {lines[0]} == ",quux\n"
+    end
+  end
+
+  describe "can choose field seperator" do
+    tmpfile = Tempfile.new('logstash-spec-output-csv')
+    config <<-CONFIG
+      input {
+        generator {
+          message => '{"foo":"one","bar": "two"}'
+          count => 1
+        }
+      }
+      filter {
+        json { source => "message"}
+      }
+      output {
+        csv {
+          path => "#{tmpfile.path}"
+          fields => ["foo", "bar"]
+          csv_options => {"col_sep" => "|"}
+        }
+      }
+    CONFIG
+
+    agent do
+      lines = File.readlines(tmpfile.path)
+      insist {lines.count} == 1
+      insist {lines[0]} == "one|two\n"
+    end
+  end
+  describe "can choose line seperator" do
+    tmpfile = Tempfile.new('logstash-spec-output-csv')
+    config <<-CONFIG
+      input {
+        generator {
+          message => '{"foo":"one","bar": "two"}'
+          count => 2
+        }
+      }
+      filter {
+        json { source => "message"}
+      }
+      output {
+        csv {
+          path => "#{tmpfile.path}"
+          fields => ["foo", "bar"]
+          csv_options => {"col_sep" => "|" "row_sep" => "\t"}
+        }
+      }
+    CONFIG
+
+    agent do
+      lines = File.readlines(tmpfile.path)
+      insist {lines.count} == 1
+      insist {lines[0]} == "one|two\tone|two\t"
+    end
+  end
+end
diff --git a/spec/outputs/elasticsearch_http_spec.rb b/spec/outputs/elasticsearch_http_spec.rb
new file mode 100644
index 00000000000..a89526216a7
--- /dev/null
+++ b/spec/outputs/elasticsearch_http_spec.rb
@@ -0,0 +1,241 @@
+require "spec_helper"
+require "logstash/json"
+
+describe "outputs/elasticsearch_http", :elasticsearch => true do
+  
+
+  describe "ship lots of events w/ default index_type" do
+    # Generate a random index name
+    index = 10.times.collect { rand(10).to_s }.join("")
+    type = 10.times.collect { rand(10).to_s }.join("")
+
+    # Write about 10000 events. Add jitter to increase likeliness of finding
+    # boundary-related bugs.
+    event_count = 10000 + rand(500)
+    flush_size = rand(200) + 1
+
+    config <<-CONFIG
+      input {
+        generator {
+          message => "hello world"
+          count => #{event_count}
+          type => "#{type}"
+        }
+      }
+      output {
+        elasticsearch_http {
+          host => "127.0.0.1"
+          port => 9200
+          index => "#{index}"
+          flush_size => #{flush_size}
+        }
+      }
+    CONFIG
+
+    agent do
+      # Try a few times to check if we have the correct number of events stored
+      # in ES.
+      #
+      # We try multiple times to allow final agent flushes as well as allowing
+      # elasticsearch to finish processing everything.
+      ftw = FTW::Agent.new
+      ftw.post!("http://localhost:9200/#{index}/_refresh")
+
+      # Wait until all events are available.
+      Stud::try(10.times) do
+        data = ""
+        response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
+        response.read_body { |chunk| data << chunk }
+        result = LogStash::Json.load(data)
+        count = result["count"]
+        insist { count } == event_count
+      end
+
+      response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
+      data = ""
+      response.read_body { |chunk| data << chunk }
+      result = LogStash::Json.load(data)
+      result["hits"]["hits"].each do |doc|
+        # With no 'index_type' set, the document type should be the type
+        # set on the input
+        insist { doc["_type"] } == type
+        insist { doc["_index"] } == index
+        insist { doc["_source"]["message"] } == "hello world"
+      end
+    end
+  end
+
+  describe "testing index_type" do
+    describe "no type value" do
+      # Generate a random index name
+      index = 10.times.collect { rand(10).to_s }.join("")
+      event_count = 100 + rand(100)
+      flush_size = rand(200) + 1
+
+      config <<-CONFIG
+        input {
+          generator {
+            message => "hello world"
+            count => #{event_count}
+          }
+        }
+        output {
+          elasticsearch_http {
+            host => "127.0.0.1"
+            index => "#{index}"
+            flush_size => #{flush_size}
+          }
+        }
+      CONFIG
+
+      agent do
+        ftw = FTW::Agent.new
+        ftw.post!("http://localhost:9200/#{index}/_refresh")
+
+        # Wait until all events are available.
+        Stud::try(10.times) do
+          data = ""
+          response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
+          response.read_body { |chunk| data << chunk }
+          result = LogStash::Json.load(data)
+          count = result["count"]
+          insist { count } == event_count
+        end
+
+        response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
+        data = ""
+        response.read_body { |chunk| data << chunk }
+        result = LogStash::Json.load(data)
+        result["hits"]["hits"].each do |doc|
+          insist { doc["_type"] } == "logs"
+        end
+      end
+    end
+
+    describe "default event type value" do
+      # Generate a random index name
+      index = 10.times.collect { rand(10).to_s }.join("")
+      event_count = 100 + rand(100)
+      flush_size = rand(200) + 1
+
+      config <<-CONFIG
+        input {
+          generator {
+            message => "hello world"
+            count => #{event_count}
+            type => "generated"
+          }
+        }
+        output {
+          elasticsearch_http {
+            host => "127.0.0.1"
+            index => "#{index}"
+            flush_size => #{flush_size}
+          }
+        }
+      CONFIG
+
+      agent do
+        ftw = FTW::Agent.new
+        ftw.post!("http://localhost:9200/#{index}/_refresh")
+
+        # Wait until all events are available.
+        Stud::try(10.times) do
+          data = ""
+          response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
+          response.read_body { |chunk| data << chunk }
+          result = LogStash::Json.load(data)
+          count = result["count"]
+          insist { count } == event_count
+        end
+
+        response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
+        data = ""
+        response.read_body { |chunk| data << chunk }
+        result = LogStash::Json.load(data)
+        result["hits"]["hits"].each do |doc|
+          insist { doc["_type"] } == "generated"
+        end
+      end
+    end
+  end
+
+  describe "index template expected behavior" do
+    subject do
+      Elasticsearch::Client.new.indices.delete_template(:name => "*")
+      require "logstash/outputs/elasticsearch_http"
+      settings = {
+        "manage_template" => true,
+        "template_overwrite" => true,
+        "host" => "localhost"
+      }
+      output = LogStash::Outputs::ElasticSearchHTTP.new(settings)
+      output.register
+      next output
+    end
+
+    before :each do
+      require "elasticsearch"
+      @es = Elasticsearch::Client.new
+      @es.indices.delete(:index => "*")
+
+      subject.receive(LogStash::Event.new("message" => "sample message here"))
+      subject.receive(LogStash::Event.new("somevalue" => 100))
+      subject.receive(LogStash::Event.new("somevalue" => 10))
+      subject.receive(LogStash::Event.new("somevalue" => 1))
+      subject.receive(LogStash::Event.new("country" => "us"))
+      subject.receive(LogStash::Event.new("country" => "at"))
+      subject.receive(LogStash::Event.new("geoip" => { "location" => [ 0.0, 0.0 ] }))
+      subject.buffer_flush(:final => true)
+      @es.indices.refresh
+
+      # Wait or fail until everything's indexed.
+      Stud::try(20.times) do
+        r = @es.search
+        insist { r["hits"]["total"] } == 7
+      end
+    end
+
+    it "permits phrase searching on string fields" do
+      results = @es.search(:q => "message:\"sample message\"")
+      insist { results["hits"]["total"] } == 1
+      insist { results["hits"]["hits"][0]["_source"]["message"] } == "sample message here"
+    end
+
+    it "numbers dynamically map to a numeric type and permit range queries" do
+      results = @es.search(:q => "somevalue:[5 TO 105]")
+      insist { results["hits"]["total"] } == 2
+
+      values = results["hits"]["hits"].collect { |r| r["_source"]["somevalue"] }
+      insist { values }.include?(10)
+      insist { values }.include?(100)
+      reject { values }.include?(1)
+    end
+
+    it "creates .raw field fro any string field which is not_analyzed" do
+      results = @es.search(:q => "message.raw:\"sample message here\"")
+      insist { results["hits"]["total"] } == 1
+      insist { results["hits"]["hits"][0]["_source"]["message"] } == "sample message here"
+
+      # partial or terms should not work.
+      results = @es.search(:q => "message.raw:\"sample\"")
+      insist { results["hits"]["total"] } == 0
+    end
+
+    it "make [geoip][location] a geo_point" do
+      results = @es.search(:body => { "filter" => { "geo_distance" => { "distance" => "1000km", "geoip.location" => { "lat" => 0.5, "lon" => 0.5 } } } })
+      insist { results["hits"]["total"] } == 1
+      insist { results["hits"]["hits"][0]["_source"]["geoip"]["location"] } == [ 0.0, 0.0 ]
+    end
+
+    it "should index stopwords like 'at' " do
+      results = @es.search(:body => { "facets" => { "t" => { "terms" => { "field" => "country" } } } })["facets"]["t"]
+      terms = results["terms"].collect { |t| t["term"] }
+
+      insist { terms }.include?("us")
+
+      # 'at' is a stopword, make sure stopwords are not ignored.
+      insist { terms }.include?("at")
+    end
+  end
+end
diff --git a/spec/outputs/elasticsearch_river_spec.rb b/spec/outputs/elasticsearch_river_spec.rb
new file mode 100644
index 00000000000..0afd94d918f
--- /dev/null
+++ b/spec/outputs/elasticsearch_river_spec.rb
@@ -0,0 +1,14 @@
+# encoding: utf-8
+
+require "logstash/plugin"
+
+describe "outputs/elasticsearch_river" do
+
+  it "should register" do
+    output = LogStash::Plugin.lookup("output", "elasticsearch_river").new("es_host" => "localhost", "rabbitmq_host" => "localhost")
+    output.stub(:prepare_river)
+
+    # register will try to load jars and raise if it cannot find jars
+    expect {output.register}.to_not raise_error
+  end
+end
diff --git a/spec/outputs/elasticsearch_spec.rb b/spec/outputs/elasticsearch_spec.rb
new file mode 100644
index 00000000000..450e7a262fb
--- /dev/null
+++ b/spec/outputs/elasticsearch_spec.rb
@@ -0,0 +1,389 @@
+require "spec_helper"
+require "ftw"
+require "logstash/plugin"
+require "logstash/json"
+require "stud/try"
+
+describe "outputs/elasticsearch" do
+
+
+  it "should register" do
+    output = LogStash::Plugin.lookup("output", "elasticsearch").new("embedded" => "false", "protocol" => "transport", "manage_template" => "false")
+
+    # register will try to load jars and raise if it cannot find jars
+    expect {output.register}.to_not raise_error
+  end
+
+  describe "ship lots of events w/ default index_type", :elasticsearch => true do
+    # Generate a random index name
+    index = 10.times.collect { rand(10).to_s }.join("")
+    type = 10.times.collect { rand(10).to_s }.join("")
+
+    # Write about 10000 events. Add jitter to increase likeliness of finding
+    # boundary-related bugs.
+    event_count = 10000 + rand(500)
+    flush_size = rand(200) + 1
+
+    config <<-CONFIG
+      input {
+        generator {
+          message => "hello world"
+          count => #{event_count}
+          type => "#{type}"
+        }
+      }
+      output {
+        elasticsearch {
+          host => "127.0.0.1"
+          index => "#{index}"
+          flush_size => #{flush_size}
+        }
+      }
+    CONFIG
+
+    agent do
+      # Try a few times to check if we have the correct number of events stored
+      # in ES.
+      #
+      # We try multiple times to allow final agent flushes as well as allowing
+      # elasticsearch to finish processing everything.
+      ftw = FTW::Agent.new
+      ftw.post!("http://localhost:9200/#{index}/_refresh")
+
+      # Wait until all events are available.
+      Stud::try(10.times) do
+        data = ""
+        response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
+        response.read_body { |chunk| data << chunk }
+        result = LogStash::Json.load(data)
+        count = result["count"]
+        insist { count } == event_count
+      end
+
+      response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
+      data = ""
+      response.read_body { |chunk| data << chunk }
+      result = LogStash::Json.load(data)
+      result["hits"]["hits"].each do |doc|
+        # With no 'index_type' set, the document type should be the type
+        # set on the input
+        insist { doc["_type"] } == type
+        insist { doc["_index"] } == index
+        insist { doc["_source"]["message"] } == "hello world"
+      end
+    end
+  end
+
+  describe "testing index_type", :elasticsearch => true do
+    describe "no type value" do
+      # Generate a random index name
+      index = 10.times.collect { rand(10).to_s }.join("")
+      event_count = 100 + rand(100)
+      flush_size = rand(200) + 1
+
+      config <<-CONFIG
+        input {
+          generator {
+            message => "hello world"
+            count => #{event_count}
+          }
+        }
+        output {
+          elasticsearch {
+            host => "127.0.0.1"
+            index => "#{index}"
+            flush_size => #{flush_size}
+          }
+        }
+      CONFIG
+
+      agent do
+        ftw = FTW::Agent.new
+        ftw.post!("http://localhost:9200/#{index}/_refresh")
+
+        # Wait until all events are available.
+        Stud::try(10.times) do
+          data = ""
+          response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
+          response.read_body { |chunk| data << chunk }
+          result = LogStash::Json.load(data)
+          count = result["count"]
+          insist { count } == event_count
+        end
+
+        response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
+        data = ""
+        response.read_body { |chunk| data << chunk }
+        result = LogStash::Json.load(data)
+        result["hits"]["hits"].each do |doc|
+          insist { doc["_type"] } == "logs"
+        end
+      end
+    end
+
+    describe "default event type value" do
+      # Generate a random index name
+      index = 10.times.collect { rand(10).to_s }.join("")
+      event_count = 100 + rand(100)
+      flush_size = rand(200) + 1
+
+      config <<-CONFIG
+        input {
+          generator {
+            message => "hello world"
+            count => #{event_count}
+            type => "generated"
+          }
+        }
+        output {
+          elasticsearch {
+            host => "127.0.0.1"
+            index => "#{index}"
+            flush_size => #{flush_size}
+          }
+        }
+      CONFIG
+
+      agent do
+        ftw = FTW::Agent.new
+        ftw.post!("http://localhost:9200/#{index}/_refresh")
+
+        # Wait until all events are available.
+        Stud::try(10.times) do
+          data = ""
+          response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
+          response.read_body { |chunk| data << chunk }
+          result = LogStash::Json.load(data)
+          count = result["count"]
+          insist { count } == event_count
+        end
+
+        response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
+        data = ""
+        response.read_body { |chunk| data << chunk }
+        result = LogStash::Json.load(data)
+        result["hits"]["hits"].each do |doc|
+          insist { doc["_type"] } == "generated"
+        end
+      end
+    end
+  end
+
+  describe "action => ...", :elasticsearch => true do
+    index_name = 10.times.collect { rand(10).to_s }.join("")
+
+    config <<-CONFIG
+      input {
+        generator {
+          message => "hello world"
+          count => 100
+        }
+      }
+      output {
+        elasticsearch {
+          host => "127.0.0.1"
+          index => "#{index_name}"
+        }
+      }
+    CONFIG
+
+
+    agent do
+      ftw = FTW::Agent.new
+      ftw.post!("http://localhost:9200/#{index_name}/_refresh")
+
+      # Wait until all events are available.
+      Stud::try(10.times) do
+        data = ""
+        response = ftw.get!("http://127.0.0.1:9200/#{index_name}/_count?q=*")
+        response.read_body { |chunk| data << chunk }
+        result = LogStash::Json.load(data)
+        count = result["count"]
+        insist { count } == 100
+      end
+
+      response = ftw.get!("http://127.0.0.1:9200/#{index_name}/_search?q=*&size=1000")
+      data = ""
+      response.read_body { |chunk| data << chunk }
+      result = LogStash::Json.load(data)
+      result["hits"]["hits"].each do |doc|
+        insist { doc["_type"] } == "logs"
+      end
+    end
+
+    describe "default event type value", :elasticsearch => true do
+      # Generate a random index name
+      index = 10.times.collect { rand(10).to_s }.join("")
+      event_count = 100 + rand(100)
+      flush_size = rand(200) + 1
+
+      config <<-CONFIG
+        input {
+          generator {
+            message => "hello world"
+            count => #{event_count}
+            type => "generated"
+          }
+        }
+        output {
+          elasticsearch {
+            host => "127.0.0.1"
+            index => "#{index}"
+            flush_size => #{flush_size}
+          }
+        }
+      CONFIG
+
+      agent do
+        ftw = FTW::Agent.new
+        ftw.post!("http://localhost:9200/#{index}/_refresh")
+
+        # Wait until all events are available.
+        Stud::try(10.times) do
+          data = ""
+          response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
+          response.read_body { |chunk| data << chunk }
+          result = LogStash::Json.load(data)
+          count = result["count"]
+          insist { count } == event_count
+        end
+
+        response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
+        data = ""
+        response.read_body { |chunk| data << chunk }
+        result = LogStash::Json.load(data)
+        result["hits"]["hits"].each do |doc|
+          insist { doc["_type"] } == "generated"
+        end
+      end
+    end
+  end
+
+  describe "index template expected behavior", :elasticsearch => true do
+    ["node", "transport", "http"].each do |protocol|
+      context "with protocol => #{protocol}" do
+        subject do
+          require "logstash/outputs/elasticsearch"
+          settings = {
+            "manage_template" => true,
+            "template_overwrite" => true,
+            "protocol" => protocol,
+            "host" => "localhost"
+          }
+          next LogStash::Outputs::ElasticSearch.new(settings)
+        end
+
+        before :each do
+          # Delete all templates first.
+          require "elasticsearch"
+
+          # Clean ES of data before we start.
+          @es = Elasticsearch::Client.new
+          @es.indices.delete_template(:name => "*")
+
+          @es.indices.delete(:index => "*")
+
+          subject.register
+
+          subject.receive(LogStash::Event.new("message" => "sample message here"))
+          subject.receive(LogStash::Event.new("somevalue" => 100))
+          subject.receive(LogStash::Event.new("somevalue" => 10))
+          subject.receive(LogStash::Event.new("somevalue" => 1))
+          subject.receive(LogStash::Event.new("country" => "us"))
+          subject.receive(LogStash::Event.new("country" => "at"))
+          subject.receive(LogStash::Event.new("geoip" => { "location" => [ 0.0, 0.0 ] }))
+          subject.buffer_flush(:final => true)
+          @es.indices.refresh
+
+          # Wait or fail until everything's indexed.
+          Stud::try(20.times) do
+            r = @es.search
+            insist { r["hits"]["total"] } == 7
+          end
+        end
+
+        it "permits phrase searching on string fields" do
+          results = @es.search(:q => "message:\"sample message\"")
+          insist { results["hits"]["total"] } == 1
+          insist { results["hits"]["hits"][0]["_source"]["message"] } == "sample message here"
+        end
+
+        it "numbers dynamically map to a numeric type and permit range queries" do
+          results = @es.search(:q => "somevalue:[5 TO 105]")
+          insist { results["hits"]["total"] } == 2
+
+          values = results["hits"]["hits"].collect { |r| r["_source"]["somevalue"] }
+          insist { values }.include?(10)
+          insist { values }.include?(100)
+          reject { values }.include?(1)
+        end
+
+        it "creates .raw field fro any string field which is not_analyzed" do
+          results = @es.search(:q => "message.raw:\"sample message here\"")
+          insist { results["hits"]["total"] } == 1
+          insist { results["hits"]["hits"][0]["_source"]["message"] } == "sample message here"
+
+          # partial or terms should not work.
+          results = @es.search(:q => "message.raw:\"sample\"")
+          insist { results["hits"]["total"] } == 0
+        end
+
+        it "make [geoip][location] a geo_point" do
+          results = @es.search(:body => { "filter" => { "geo_distance" => { "distance" => "1000km", "geoip.location" => { "lat" => 0.5, "lon" => 0.5 } } } })
+          insist { results["hits"]["total"] } == 1
+          insist { results["hits"]["hits"][0]["_source"]["geoip"]["location"] } == [ 0.0, 0.0 ]
+        end
+
+        it "should index stopwords like 'at' " do
+          results = @es.search(:body => { "facets" => { "t" => { "terms" => { "field" => "country" } } } })["facets"]["t"]
+          terms = results["terms"].collect { |t| t["term"] }
+
+          insist { terms }.include?("us")
+
+          # 'at' is a stopword, make sure stopwords are not ignored.
+          insist { terms }.include?("at")
+        end
+      end
+    end
+  end
+
+  describe "elasticsearch protocol", :elasticsearch => true do
+    # ElasticSearch related jars
+    LogStash::Environment.load_elasticsearch_jars!
+    # Load elasticsearch protocol
+    require "logstash/outputs/elasticsearch/protocol"
+
+    describe "elasticsearch node client" do
+      # Test ElasticSearch Node Client
+      # Reference: http://www.elasticsearch.org/guide/reference/modules/discovery/zen/
+
+      it "should support hosts in both string and array" do
+        # Because we defined *hosts* method in NodeClient as private,
+        # we use *obj.send :method,[args...]* to call method *hosts*
+        client = LogStash::Outputs::Elasticsearch::Protocols::NodeClient.new
+
+        # Node client should support host in string
+        # Case 1: default :host in string
+        insist { client.send :hosts, :host => "host",:port => 9300 } == "host:9300"
+        # Case 2: :port =~ /^\d+_\d+$/
+        insist { client.send :hosts, :host => "host",:port => "9300-9302"} == "host:9300,host:9301,host:9302"
+        # Case 3: :host =~ /^.+:.+$/
+        insist { client.send :hosts, :host => "host:9303",:port => 9300 } == "host:9303"
+        # Case 4:  :host =~ /^.+:.+$/ and :port =~ /^\d+_\d+$/
+        insist { client.send :hosts, :host => "host:9303",:port => "9300-9302"} == "host:9303"
+
+        # Node client should support host in array
+        # Case 5: :host in array with single item
+        insist { client.send :hosts, :host => ["host"],:port => 9300 } == ("host:9300")
+        # Case 6: :host in array with more than one items
+        insist { client.send :hosts, :host => ["host1","host2"],:port => 9300 } == "host1:9300,host2:9300"
+        # Case 7: :host in array with more than one items and :port =~ /^\d+_\d+$/
+        insist { client.send :hosts, :host => ["host1","host2"],:port => "9300-9302" } == "host1:9300,host1:9301,host1:9302,host2:9300,host2:9301,host2:9302"
+        # Case 8: :host in array with more than one items and some :host =~ /^.+:.+$/
+        insist { client.send :hosts, :host => ["host1","host2:9303"],:port => 9300 } == "host1:9300,host2:9303"
+        # Case 9: :host in array with more than one items, :port =~ /^\d+_\d+$/ and some :host =~ /^.+:.+$/
+        insist { client.send :hosts, :host => ["host1","host2:9303"],:port => "9300-9302" } == "host1:9300,host1:9301,host1:9302,host2:9303"
+      end
+    end
+  end
+end
diff --git a/spec/outputs/email_spec.rb b/spec/outputs/email_spec.rb
new file mode 100644
index 00000000000..f2a2639c870
--- /dev/null
+++ b/spec/outputs/email_spec.rb
@@ -0,0 +1,173 @@
+require "spec_helper"
+require "rumbster"
+require "message_observers"
+
+describe "outputs/email", :broken => true do
+    
+
+    port = 2525
+    let (:rumbster) { Rumbster.new(port) }
+    let (:message_observer) { MailMessageObserver.new }
+
+    before :each do
+        rumbster.add_observer message_observer
+        rumbster.start
+    end
+
+    after :each do
+        rumbster.stop
+    end
+
+    describe  "use a list of email as mail.to (LOGSTASH-827)" do
+        config <<-CONFIG
+        input {
+            generator {
+                message => "hello world"
+                count => 1
+                type => "generator"
+            }
+        }
+        filter {
+            noop {
+                add_field => ["dummy_match", "ok"]
+            }
+        }
+        output{
+            email {
+                to => "email1@host, email2@host"
+                match => ["mymatch", "dummy_match,ok"]
+                options => ["port", #{port}]
+            }
+        }
+        CONFIG
+
+        agent do
+            insist {message_observer.messages.size} == 1
+            insist {message_observer.messages[0].to} == ["email1@host", "email2@host"]
+        end
+    end
+
+    describe  "use an array of email as mail.to (LOGSTASH-827)" do
+        config <<-CONFIG
+        input {
+            generator {
+                message => "hello world"
+                count => 1
+                type => "generator"
+            }
+        }
+        filter {
+            noop {
+                add_field => ["dummy_match", "ok"]
+                add_field => ["to_addr", "email1@host"]
+                add_field => ["to_addr", "email2@host"]
+            }
+        }
+        output{
+            email {
+                to => "%{to_addr}"
+                match => ["mymatch", "dummy_match,ok"]
+                options => ["port", #{port}]
+            }
+        }
+        CONFIG
+
+        agent do
+            insist {message_observer.messages.size} == 1
+            insist {message_observer.messages[0].to} == ["email1@host", "email2@host"]
+        end
+    end
+
+    describe  "multi-lined text body (LOGSTASH-841)" do
+        config <<-CONFIG
+        input {
+            generator {
+                message => "hello world"
+                count => 1
+                type => "generator"
+            }
+        }
+        filter {
+            noop {
+                add_field => ["dummy_match", "ok"]
+            }
+        }
+        output{
+            email {
+                to => "me@host"
+                subject => "Hello World"
+                body => "Line1\\nLine2\\nLine3"
+                match => ["mymatch", "dummy_match,*"]
+                options => ["port", #{port}]
+            }
+        }
+        CONFIG
+
+        agent do
+            insist {message_observer.messages.size} == 1
+            insist {message_observer.messages[0].subject} == "Hello World"
+            insist {message_observer.messages[0].body.raw_source} == "Line1\r\nLine2\r\nLine3"
+        end
+    end
+
+    describe  "use nil authenticationType (LOGSTASH-559)" do
+        config <<-CONFIG
+        input {
+            generator {
+                message => "hello world"
+                count => 1
+                type => "generator"
+            }
+        }
+        filter {
+            noop {
+                add_field => ["dummy_match", "ok"]
+            }
+        }
+        output{
+            email {
+                to => "me@host"
+                subject => "Hello World"
+                body => "Line1\\nLine2\\nLine3"
+                match => ["mymatch", "dummy_match,*"]
+                options => ["port", #{port}, "authenticationType", "nil"]
+            }
+        }
+        CONFIG
+
+        agent do
+            insist {message_observer.messages.size} == 1
+            insist {message_observer.messages[0].subject} == "Hello World"
+            insist {message_observer.messages[0].body.raw_source} == "Line1\r\nLine2\r\nLine3"
+        end
+    end
+
+    describe  "match on source and message (LOGSTASH-826)" do
+        config <<-CONFIG
+        input {
+            generator {
+                message => "hello world"
+                count => 1
+                type => "generator"
+            }
+        }
+        output{
+            email {
+                to => "me@host"
+                subject => "Hello World"
+                body => "Mail body"
+                match => ["messageAndSourceMatch", "message,*hello,,and,source,*generator"]
+                options => ["port", #{port}, "authenticationType", "nil"]
+            }
+        }
+        CONFIG
+
+        agent do
+            insist {message_observer.messages.size} == 1
+            insist {message_observer.messages[0].subject} == "Hello World"
+            insist {message_observer.messages[0].body.raw_source} == "Mail body"
+        end
+    end
+end
+
+
diff --git a/spec/outputs/file_spec.rb b/spec/outputs/file_spec.rb
new file mode 100644
index 00000000000..aef261acc2e
--- /dev/null
+++ b/spec/outputs/file_spec.rb
@@ -0,0 +1,73 @@
+require "spec_helper"
+require "logstash/outputs/file"
+require "logstash/json"
+require "tempfile"
+
+describe LogStash::Outputs::File do
+  
+
+  describe "ship lots of events to a file" do
+    event_count = 10000 + rand(500)
+    tmp_file = Tempfile.new('logstash-spec-output-file')
+
+    config <<-CONFIG
+      input {
+        generator {
+          message => "hello world"
+          count => #{event_count}
+          type => "generator"
+        }
+      }
+      output {
+        file {
+          path => "#{tmp_file.path}"
+        }
+      }
+    CONFIG
+
+    agent do
+      line_num = 0
+      # Now check all events for order and correctness.
+      File.foreach(tmp_file) do |line|
+        event = LogStash::Event.new(LogStash::Json.load(line))
+        insist {event["message"]} == "hello world"
+        insist {event["sequence"]} == line_num
+        line_num += 1
+      end
+      insist {line_num} == event_count
+    end # agent
+  end
+
+  describe "ship lots of events to a file gzipped" do
+    event_count = 10000 + rand(500)
+    tmp_file = Tempfile.new('logstash-spec-output-file')
+
+    config <<-CONFIG
+      input {
+        generator {
+          message => "hello world"
+          count => #{event_count}
+          type => "generator"
+        }
+      }
+      output {
+        file {
+          path => "#{tmp_file.path}"
+          gzip => true
+        }
+      }
+    CONFIG
+
+    agent do
+      line_num = 0
+      # Now check all events for order and correctness.
+      Zlib::GzipReader.open(tmp_file.path).each_line do |line|
+        event = LogStash::Event.new(LogStash::Json.load(line))
+        insist {event["message"]} == "hello world"
+        insist {event["sequence"]} == line_num
+        line_num += 1
+      end
+      insist {line_num} == event_count
+    end # agent
+  end
+end
diff --git a/spec/outputs/graphite_spec.rb b/spec/outputs/graphite_spec.rb
new file mode 100644
index 00000000000..d8bfe240e66
--- /dev/null
+++ b/spec/outputs/graphite_spec.rb
@@ -0,0 +1,236 @@
+require "spec_helper"
+require "logstash/outputs/graphite"
+require "mocha/api"
+
+describe LogStash::Outputs::Graphite, :socket => true do
+  
+
+  describe "defaults should include all metrics" do
+    port = 4939
+    config <<-CONFIG
+      input {
+        generator {
+          message => "foo=fancy bar=42"
+          count => 1
+          type => "generator"
+        }
+      }
+
+      filter {
+        kv { }
+      }
+
+      output {
+        graphite {
+          host => "localhost"
+          port => #{port}
+          metrics => [ "hurray.%{foo}", "%{bar}" ]
+        }
+      }
+    CONFIG
+
+    let(:queue) { Queue.new }
+    before :each do
+      server = TCPServer.new("127.0.0.1", port)
+      Thread.new do
+        client = server.accept
+        p client
+        while true
+          p :read
+          line = client.readline
+          p :done
+          queue << line
+          p line
+        end
+      end
+    end
+
+    agent do
+      lines = queue.pop
+
+      insist { lines.size } == 1
+      insist { lines }.any? { |l| l =~ /^hurray.fancy 42.0 \d{10,}\n$/ }
+    end
+  end
+
+  describe "fields_are_metrics => true" do
+    describe "metrics_format => ..." do
+      describe "match one key" do
+        config <<-CONFIG
+          input {
+            generator {
+              message => "foo=123"
+              count => 1
+              type => "generator"
+            }
+          }
+
+          filter {
+            kv { }
+          }
+
+          output {
+            graphite {
+                host => "localhost"
+                port => 2003
+                fields_are_metrics => true
+                include_metrics => ["foo"]
+                metrics_format => "foo.bar.sys.data.*"
+                debug => true
+            }
+          }
+        CONFIG
+
+        agent do
+          @mock.rewind
+          lines = @mock.readlines
+          insist { lines.size } == 1
+          insist { lines[0] } =~ /^foo.bar.sys.data.foo 123.0 \d{10,}\n$/
+        end
+      end
+
+      describe "match all keys" do
+        config <<-CONFIG
+          input {
+            generator {
+              message => "foo=123 bar=42"
+              count => 1
+              type => "generator"
+            }
+          }
+
+          filter {
+            kv { }
+          }
+
+          output {
+            graphite {
+                host => "localhost"
+                port => 2003
+                fields_are_metrics => true
+                include_metrics => [".*"]
+                metrics_format => "foo.bar.sys.data.*"
+                debug => true
+            }
+          }
+        CONFIG
+
+        agent do
+          @mock.rewind
+          lines = @mock.readlines.delete_if { |l| l =~ /\.sequence \d+/ }
+
+          insist { lines.size } == 2
+          insist { lines }.any? { |l| l =~ /^foo.bar.sys.data.foo 123.0 \d{10,}\n$/ }
+          insist { lines }.any? { |l| l =~ /^foo.bar.sys.data.bar 42.0 \d{10,}\n$/ }
+        end
+      end
+
+      describe "no match" do
+        config <<-CONFIG
+          input {
+            generator {
+              message => "foo=123 bar=42"
+              count => 1
+              type => "generator"
+            }
+          }
+
+          filter {
+            kv { }
+          }
+
+          output {
+            graphite {
+              host => "localhost"
+              port => 2003
+              fields_are_metrics => true
+              include_metrics => ["notmatchinganything"]
+              metrics_format => "foo.bar.sys.data.*"
+              debug => true
+            }
+          }
+        CONFIG
+
+        agent do
+          @mock.rewind
+          lines = @mock.readlines
+          insist { lines.size } == 0
+        end
+      end
+
+      describe "match one key with invalid metric_format" do
+        config <<-CONFIG
+          input {
+            generator {
+              message => "foo=123"
+              count => 1
+              type => "generator"
+            }
+          }
+
+          filter {
+            kv { }
+          }
+
+          output {
+            graphite {
+                host => "localhost"
+                port => 2003
+                fields_are_metrics => true
+                include_metrics => ["foo"]
+                metrics_format => "invalidformat"
+                debug => true
+            }
+          }
+        CONFIG
+
+        agent do
+          @mock.rewind
+          lines = @mock.readlines
+          insist { lines.size } == 1
+          insist { lines[0] } =~ /^foo 123.0 \d{10,}\n$/
+        end
+      end
+    end
+  end
+
+  describe "fields are metrics = false" do
+    describe "metrics_format not set" do
+      describe "match one key with metrics list" do
+        config <<-CONFIG
+          input {
+            generator {
+              message => "foo=123"
+              count => 1
+              type => "generator"
+            }
+          }
+
+          filter {
+            kv { }
+          }
+
+          output {
+            graphite {
+                host => "localhost"
+                port => 2003
+                fields_are_metrics => false
+                include_metrics => ["foo"]
+                metrics => [ "custom.foo", "%{foo}" ]
+                debug => true
+            }
+          }
+        CONFIG
+
+        agent do
+          @mock.rewind
+          lines = @mock.readlines
+
+          insist { lines.size } == 1
+          insist { lines[0] } =~ /^custom.foo 123.0 \d{10,}\n$/
+        end
+      end
+
+    end
+  end
+end
diff --git a/spec/outputs/redis_spec.rb b/spec/outputs/redis_spec.rb
new file mode 100644
index 00000000000..70cc362b1bc
--- /dev/null
+++ b/spec/outputs/redis_spec.rb
@@ -0,0 +1,128 @@
+require "spec_helper"
+require "logstash/outputs/redis"
+require "logstash/json"
+require "redis"
+
+describe LogStash::Outputs::Redis, :redis => true do
+  
+
+  describe "ship lots of events to a list" do
+    key = 10.times.collect { rand(10).to_s }.join("")
+    event_count = 10000 + rand(500)
+
+    config <<-CONFIG
+      input {
+        generator {
+          message => "hello world"
+          count => #{event_count}
+          type => "generator"
+        }
+      }
+      output {
+        redis {
+          host => "127.0.0.1"
+          key => "#{key}"
+          data_type => list
+        }
+      }
+    CONFIG
+
+    agent do
+      # Query redis directly and inspect the goodness.
+      redis = Redis.new(:host => "127.0.0.1")
+
+      # The list should contain the number of elements our agent pushed up.
+      insist { redis.llen(key) } == event_count
+
+      # Now check all events for order and correctness.
+      event_count.times do |value|
+        id, element = redis.blpop(key, 0)
+        event = LogStash::Event.new(LogStash::Json.load(element))
+        insist { event["sequence"] } == value
+        insist { event["message"] } == "hello world"
+      end
+
+      # The list should now be empty
+      insist { redis.llen(key) } == 0
+    end # agent
+  end
+
+  describe "batch mode" do
+    key = 10.times.collect { rand(10).to_s }.join("")
+    event_count = 200000
+
+    config <<-CONFIG
+      input {
+        generator {
+          message => "hello world"
+          count => #{event_count}
+          type => "generator"
+        }
+      }
+      output {
+        redis {
+          host => "127.0.0.1"
+          key => "#{key}"
+          data_type => list
+          batch => true
+          batch_timeout => 5
+          timeout => 5
+        }
+      }
+    CONFIG
+
+    agent do
+      # we have to wait for teardown to execute & flush the last batch.
+      # otherwise we might start doing assertions before everything has been
+      # sent out to redis.
+      sleep 2
+
+      redis = Redis.new(:host => "127.0.0.1")
+
+      # The list should contain the number of elements our agent pushed up.
+      insist { redis.llen(key) } == event_count
+
+      # Now check all events for order and correctness.
+      event_count.times do |value|
+        id, element = redis.blpop(key, 0)
+        event = LogStash::Event.new(LogStash::Json.load(element))
+        insist { event["sequence"] } == value
+        insist { event["message"] } == "hello world"
+      end
+
+      # The list should now be empty
+      insist { redis.llen(key) } == 0
+    end # agent
+  end
+
+  describe "converts US-ASCII to utf-8 without failures" do
+    key = 10.times.collect { rand(10).to_s }.join("")
+
+    config <<-CONFIG
+      input {
+        generator {
+          charset => "US-ASCII"
+          message => "\xAD\u0000"
+          count => 1
+          type => "generator"
+        }
+      }
+      output {
+        redis {
+          host => "127.0.0.1"
+          key => "#{key}"
+          data_type => list
+        }
+      }
+    CONFIG
+
+    agent do
+      # Query redis directly and inspect the goodness.
+      redis = Redis.new(:host => "127.0.0.1")
+
+      # The list should contain no elements.
+      insist { redis.llen(key) } == 1
+    end # agent
+  end
+end
+
diff --git a/spec/outputs/statsd_spec.rb b/spec/outputs/statsd_spec.rb
new file mode 100644
index 00000000000..3aa95e0d8a8
--- /dev/null
+++ b/spec/outputs/statsd_spec.rb
@@ -0,0 +1,86 @@
+require "spec_helper"
+require "logstash/outputs/statsd"
+require "mocha/api"
+require "socket"
+
+describe LogStash::Outputs::Statsd do
+  
+  port = 4399
+  udp_server = UDPSocket.new
+  udp_server.bind("127.0.0.1", port)
+
+  describe "send metric to statsd" do
+    config <<-CONFIG
+      input {
+        generator {
+          message => "valid"
+          count => 1
+        }
+      }
+
+      output {
+        statsd {
+          host => "localhost"
+          sender => "spec"
+          port => #{port}
+          count => [ "test.valid", "0.1" ]
+        }
+      }
+    CONFIG
+
+    agent do
+      metric, *data = udp_server.recvfrom(100)
+      insist { metric } == "logstash.spec.test.valid:0.1|c"
+    end
+  end
+
+  describe "output a very small float" do
+    config <<-CONFIG
+      input {
+        generator {
+          message => "valid"
+          count => 1
+        }
+      }
+
+      output {
+        statsd {
+          host => "localhost"
+          sender => "spec"
+          port => #{port}
+          count => [ "test.valid", 0.000001 ]
+        }
+      }
+    CONFIG
+
+    agent do
+      metric, *data = udp_server.recvfrom(100)
+      insist { metric } == "logstash.spec.test.valid:0.000001|c"
+    end
+  end
+
+  describe "output a very big float" do
+    config <<-CONFIG
+      input {
+        generator {
+          message => "valid"
+          count => 1
+        }
+      }
+
+      output {
+        statsd {
+          host => "localhost"
+          sender => "spec"
+          port => #{port}
+          count => [ "test.valid", 9999999999999.01 ]
+        }
+      }
+    CONFIG
+
+    agent do
+      metric, *data = udp_server.recvfrom(100)
+      insist { metric } == "logstash.spec.test.valid:9999999999999.01|c"
+    end
+  end
+end
diff --git a/spec/performance/date.rb b/spec/performance/date.rb
new file mode 100644
index 00000000000..47d3b0402c5
--- /dev/null
+++ b/spec/performance/date.rb
@@ -0,0 +1,31 @@
+require "spec_helper"
+require "logstash/filters/date"
+
+puts "Skipping date tests because this ruby is not jruby" if RUBY_ENGINE != "jruby"
+describe LogStash::Filters::Date, :if => RUBY_ENGINE == "jruby", :performance => true do
+  
+
+  describe "speed test of date parsing" do
+    it "should be fast" do
+      event_count = 100000
+      min_rate = 4000
+      max_duration = event_count / min_rate
+      input = "Nov 24 01:29:01 -0800"
+
+      filter = LogStash::Filters::Date.new("match" => [ "mydate", "MMM dd HH:mm:ss Z" ])
+      filter.register
+      duration = 0
+      # 10000 for warmup
+      [10000, event_count].each do |iterations|
+        start = Time.now
+        iterations.times do
+          event = LogStash::Event.new("mydate" => input)
+          filter.execute(event)
+        end
+        duration = Time.now - start
+      end
+      puts "filters/date parse rate: #{"%02.0f/sec" % (event_count / duration)}, elapsed: #{duration}s"
+      insist { duration } < max_duration
+    end
+  end
+end
diff --git a/spec/performance/speed.rb b/spec/performance/speed.rb
new file mode 100644
index 00000000000..d2f66dd8da8
--- /dev/null
+++ b/spec/performance/speed.rb
@@ -0,0 +1,22 @@
+require "spec_helper"
+
+describe "speed tests", :performance => true do
+  
+  count = 1000000
+
+  config <<-CONFIG
+    input {
+      generator {
+        type => foo
+        count => #{count}
+      }
+    }
+    output { null { } }
+  CONFIG
+
+  start = Time.now
+  agent do
+    duration = (Time.now - start)
+    puts "speed rate: #{"%02.0f/sec" % (count / duration)}, elapsed: #{duration}s"
+  end
+end
diff --git a/spec/spec_helper.rb b/spec/spec_helper.rb
new file mode 100644
index 00000000000..49da3cdc7f7
--- /dev/null
+++ b/spec/spec_helper.rb
@@ -0,0 +1,50 @@
+require "logstash/logging"
+require 'logstash_helpers'
+require "insist"
+
+if ENV['COVERAGE']
+  require 'simplecov'
+  require 'coveralls'
+
+  SimpleCov.formatter = SimpleCov::Formatter::MultiFormatter[
+    SimpleCov::Formatter::HTMLFormatter,
+    Coveralls::SimpleCov::Formatter
+  ]
+  SimpleCov.start do
+    add_filter 'spec/'
+    add_filter 'vendor/'
+  end
+end
+
+$TESTING = true
+if RUBY_VERSION < "1.9.2"
+  $stderr.puts "Ruby 1.9.2 or later is required. (You are running: " + RUBY_VERSION + ")"
+  raise LoadError
+end
+
+$logger = LogStash::Logger.new(STDOUT)
+if ENV["TEST_DEBUG"]
+  $logger.level = :debug
+else
+  $logger.level = :error
+end
+
+puts("Using Accessor#strict_set for specs")
+# mokey path LogStash::Event to use strict_set in tests
+# ugly, I know, but this avoids adding conditionals in performance critical section
+class LogStash::Event
+  alias_method :setval, :[]=
+  def []=(str, value)
+    if str == TIMESTAMP && !value.is_a?(LogStash::Timestamp)
+      raise TypeError, "The field '@timestamp' must be a LogStash::Timestamp, not a #{value.class} (#{value})"
+    end
+    LogStash::Event.validate_value(value)
+    setval(str, value)
+  end # def []=
+end
+
+RSpec.configure do |config|
+  config.extend LogStashHelper
+  config.filter_run_excluding :redis => true, :socket => true, :performance => true, :elasticsearch => true, :broken => true, :export_cypher => true
+end
+
diff --git a/spec/support/akamai-grok_spec.rb b/spec/support/akamai-grok_spec.rb
new file mode 100644
index 00000000000..cb678a2dee6
--- /dev/null
+++ b/spec/support/akamai-grok_spec.rb
@@ -0,0 +1,25 @@
+require "spec_helper"
+
+describe "Akamai Grok pattern" do
+
+  config <<-'CONFIG'
+    filter {
+      grok {
+        pattern => "%{COMBINEDAPACHELOG}"
+      }
+     
+     
+      date {
+        # Try to pull the timestamp from the 'timestamp' field
+        match => [ "timestamp", "dd'/'MMM'/'yyyy:HH:mm:ss Z" ]
+      }
+    }
+  CONFIG
+
+  line = '192.168.1.1 - - [25/Mar/2013:20:33:56 +0000] "GET /www.somewebsite.co.uk/dwr/interface/AjaxNewsletter.js HTTP/1.1" 200 794 "http://www.somewebsite.co.uk/money/index.html" "Mozilla/5.0 (Linux; U; Android 2.3.6; en-gb; GT-I8160 Build/GINGERBREAD) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1" "NREUM=s=1364243891214&r=589267&p=101913; __utma=259942479.284548354.1358973919.1363109625.1364243485.15; __utmb=259942479.4.10.1364243485; __utmc=259942479; __utmz=259942479.1359409342.3.3.utmcsr=investing.somewebsite.co.uk|utmccn=(referral)|utmcmd=referral|utmcct=/performance/overview/; asi_segs=D05509_10903|D05509_11337|D05509_11335|D05509_11341|D05509_11125|D05509_11301|D05509_11355|D05509_11508|D05509_11624|D05509_10784|D05509_11003|D05509_10699|D05509_11024|D05509_11096|D05509_11466|D05509_11514|D05509_11598|D05509_11599|D05509_11628|D05509_11681; rsi_segs=D05509_10903|D05509_11337|D05509_11335|D05509_11341|D05509_11125|D05509_11301|D05509_11355|D05509_11508|D05509_11624|D05509_10784|D05509_11003|D05509_10699|D05509_11024|D05509_11096|D05509_11466|D05509_11514|D05509_11598|D05509_11599|D05509_11628|D05509_11681|D05509_11701|D05509_11818|D05509_11850|D05509_11892|D05509_11893|D05509_12074|D05509_12091|D05509_12093|D05509_12095|D05509_12136|D05509_12137|D05509_12156|D05509_0; s_pers=%20s_nr%3D1361998955946%7C1364590955946%3B%20s_pn2%3D/money/home%7C1364245284228%3B%20s_c39%3D/money/home%7C1364245522830%3B%20s_visit%3D1%7C1364245693534%3B; s_sess=%20s_pn%3D/money/home%3B%20s_cc%3Dtrue%3B%20s_sq%3D%3B"'
+
+  sample line do
+    #puts subject["@timestamp"]
+    #puts subject["timestamp"]
+  end
+end
diff --git a/spec/support/date-http_spec.rb b/spec/support/date-http_spec.rb
new file mode 100644
index 00000000000..fb25f48b45b
--- /dev/null
+++ b/spec/support/date-http_spec.rb
@@ -0,0 +1,17 @@
+require "spec_helper"
+
+describe "http dates", :if => RUBY_ENGINE == "jruby" do
+
+  config <<-'CONFIG'
+    filter {
+      date {
+        match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
+        locale => "en"
+      }
+    }
+  CONFIG
+
+  sample("timestamp" => "25/Mar/2013:20:33:56 +0000") do
+    insist { subject["@timestamp"].time } == Time.iso8601("2013-03-25T20:33:56.000Z")
+  end
+end
diff --git a/spec/support/pull375_spec.rb b/spec/support/pull375_spec.rb
new file mode 100644
index 00000000000..b0851ff8eb5
--- /dev/null
+++ b/spec/support/pull375_spec.rb
@@ -0,0 +1,23 @@
+# encoding: utf-8
+
+# This spec covers the question here:
+# https://github.com/logstash/logstash/pull/375
+
+require "spec_helper"
+
+describe "pull #375" do
+  
+  describe  "kv after grok" do
+    config <<-CONFIG
+      filter {
+        grok { pattern => "%{URIPATH:mypath}%{URIPARAM:myparams}" }
+        kv { source => "myparams" field_split => "&?" }
+      }
+    CONFIG
+
+    sample "/some/path?foo=bar&baz=fizz" do
+      insist { subject["foo"] } == "bar"
+      insist { subject["baz"] } == "fizz"
+    end
+  end
+end
diff --git a/spec/util/accessors_spec.rb b/spec/util/accessors_spec.rb
new file mode 100644
index 00000000000..ca6ea831c14
--- /dev/null
+++ b/spec/util/accessors_spec.rb
@@ -0,0 +1,187 @@
+# encoding: utf-8
+
+require "spec_helper"
+require "logstash/util/accessors"
+
+describe LogStash::Util::Accessors, :if => true do
+
+  context "using simple field" do
+
+    it "should get value of word key" do
+      str = "hello"
+      data = { "hello" => "world" }
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.get(str) } == data[str]
+    end
+
+    it "should get value of key with spaces" do
+      str = "hel lo"
+      data = { "hel lo" => "world" }
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.get(str) } == data[str]
+    end
+
+    it "should get value of numeric key string" do
+      str = "1"
+      data = { "1" => "world" }
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.get(str) } == data[str]
+    end
+
+    it "should handle delete" do
+      str = "simple"
+      data = { "simple" => "things" }
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.del(str) } == "things"
+      insist { data }.empty?
+    end
+
+    it "should set string value" do
+      str = "simple"
+      data = {}
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.set(str, "things") } == "things"
+      insist { data } == { "simple" => "things" }
+    end
+
+    it "should set array value" do
+      str = "simple"
+      data = {}
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.set(str, ["foo", "bar"]) } == ["foo", "bar"]
+      insist { data } == { "simple" => ["foo", "bar"]}
+    end
+  end
+
+  context "using field path" do
+
+    it "should get shallow string value of word key" do
+      str = "[hello]"
+      data = { "hello" =>  "world" }
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.get(str) } == "world"
+    end
+
+    it "should get shallow string value of key with spaces" do
+      str = "[hel lo]"
+      data = { "hel lo" =>  "world" }
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.get(str) } == "world"
+    end
+
+    it "should get shallow string value of numeric key string" do
+      str = "[1]"
+      data = { "1" =>  "world" }
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.get(str) } == "world"
+    end
+
+    it "should get deep string value" do
+      str = "[hello][world]"
+      data = { "hello" => { "world" => "foo", "bar" => "baz" } }
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.get(str) } == data["hello"]["world"]
+    end
+
+    it "should get deep string value" do
+      str = "[hello][world]"
+      data = { "hello" => { "world" => "foo", "bar" => "baz" } }
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.get(str) } == data["hello"]["world"]
+    end
+
+    it "should handle delete" do
+      str = "[hello][world]"
+      data = { "hello" => { "world" => "foo", "bar" => "baz" } }
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.del(str) } ==  "foo"
+
+      # Make sure the "world" key is removed.
+      insist { data["hello"] } == { "bar" => "baz" }
+    end
+
+    it "should set shallow string value" do
+      str = "[hello]"
+      data = {}
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.set(str, "foo") } == "foo"
+      insist { data } == { "hello" => "foo" }
+    end
+
+    it "should strict_set shallow string value" do
+      str = "[hello]"
+      data = {}
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.strict_set(str, "foo") } == "foo"
+      insist { data } == { "hello" => "foo" }
+    end
+
+    it "should set deep string value" do
+      str = "[hello][world]"
+      data = {}
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.set(str, "foo") } == "foo"
+      insist { data } == { "hello" => { "world" => "foo" } }
+    end
+
+    it "should set deep array value" do
+      str = "[hello][world]"
+      data = {}
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.set(str, ["foo", "bar"]) } == ["foo", "bar"]
+      insist { data } == { "hello" => { "world" => ["foo", "bar"] } }
+    end
+
+    it "should strict_set deep array value" do
+      str = "[hello][world]"
+      data = {}
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.strict_set(str, ["foo", "bar"]) } == ["foo", "bar"]
+      insist { data } == { "hello" => { "world" => ["foo", "bar"] } }
+    end
+
+    it "should retrieve array item" do
+      data = { "hello" => { "world" => ["a", "b"], "bar" => "baz" } }
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.get("[hello][world][0]") } == data["hello"]["world"][0]
+      insist { accessors.get("[hello][world][1]") } == data["hello"]["world"][1]
+    end
+
+    it "should retrieve array item containing hash" do
+      data = { "hello" => { "world" => [ { "a" => 123 }, { "b" => 345 } ], "bar" => "baz" } }
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.get("[hello][world][0][a]") } == data["hello"]["world"][0]["a"]
+      insist { accessors.get("[hello][world][1][b]") } == data["hello"]["world"][1]["b"]
+    end
+  end
+
+  context "using invalid encoding" do
+    it "strinct_set should raise on non UTF-8 string encoding" do
+      str = "[hello]"
+      data = {}
+      accessors = LogStash::Util::Accessors.new(data)
+      expect { accessors.strict_set(str, "foo".encode("US-ASCII")) }.to raise_error
+    end
+
+    it "strinct_set should raise on non UTF-8 string encoding in array" do
+      str = "[hello]"
+      data = {}
+      accessors = LogStash::Util::Accessors.new(data)
+      expect { accessors.strict_set(str, ["foo", "bar".encode("US-ASCII")]) }.to raise_error
+    end
+
+    it "strinct_set should raise on invalid UTF-8 string encoding" do
+      str = "[hello]"
+      data = {}
+      accessors = LogStash::Util::Accessors.new(data)
+      expect { accessors.strict_set(str, "foo \xED\xB9\x81\xC3") }.to raise_error
+    end
+
+    it "strinct_set should raise on invalid UTF-8 string encoding in array" do
+      str = "[hello]"
+      data = {}
+      accessors = LogStash::Util::Accessors.new(data)
+      expect { accessors.strict_set(str, ["foo", "bar \xED\xB9\x81\xC3"]) }.to raise_error
+    end
+  end
+end
diff --git a/spec/util/charset_spec.rb b/spec/util/charset_spec.rb
new file mode 100644
index 00000000000..8514e1a5755
--- /dev/null
+++ b/spec/util/charset_spec.rb
@@ -0,0 +1,75 @@
+# encoding: utf-8
+
+require "spec_helper"
+require "logstash/util/charset"
+
+describe LogStash::Util::Charset do
+  let(:logger) { double("logger") }
+
+  context "with valid UTF-8 source encoding" do
+    subject {LogStash::Util::Charset.new("UTF-8")}
+
+    it "should return untouched data" do
+      ["foobar", "Œ∫·ΩπœÉŒºŒµ"].each do |data|
+        insist { data.encoding.name } == "UTF-8"
+        insist { subject.convert(data) } == data
+        insist { subject.convert(data).encoding.name } == "UTF-8"
+      end
+    end
+  end
+
+  context "with invalid UTF-8 source encoding" do
+    subject do
+      LogStash::Util::Charset.new("UTF-8").tap do |charset|
+        charset.logger = logger
+      end
+    end
+
+    it "should escape invalid sequences" do
+      ["foo \xED\xB9\x81\xC3", "bar \xAD"].each do |data|
+        insist { data.encoding.name } == "UTF-8"
+        insist { data.valid_encoding? } == false
+        expect(logger).to receive(:warn).exactly(2).times
+#logger.should_receive(:warn).twice
+        insist { subject.convert(data) } == data.inspect[1..-2]
+        insist { subject.convert(data).encoding.name } == "UTF-8"
+      end
+    end
+
+  end
+
+  context "with valid non UTF-8 source encoding" do
+    subject {LogStash::Util::Charset.new("ISO-8859-1")}
+
+    it "should encode to UTF-8" do
+      samples = [
+        ["foobar", "foobar"],
+        ["\xE0 Montr\xE9al", "√† Montr√©al"],
+      ]
+      samples.map{|(a, b)| [a.force_encoding("ISO-8859-1"), b]}.each do |(a, b)|
+        insist { a.encoding.name } == "ISO-8859-1"
+        insist { b.encoding.name } == "UTF-8"
+        insist { a.valid_encoding? } == true
+        insist { subject.convert(a).encoding.name } == "UTF-8"
+        insist { subject.convert(a) } == b
+      end
+    end
+  end
+
+  context "with invalid non UTF-8 source encoding" do
+    subject {LogStash::Util::Charset.new("ASCII-8BIT")}
+
+    it "should encode to UTF-8 and replace invalid chars" do
+      samples = [
+        ["\xE0 Montr\xE9al", "ÔøΩ MontrÔøΩal"],
+        ["\xCE\xBA\xCF\x8C\xCF\x83\xCE\xBC\xCE\xB5", "ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ"],
+      ]
+      samples.map{|(a, b)| [a.force_encoding("ASCII-8BIT"), b]}.each do |(a, b)|
+        insist { a.encoding.name } == "ASCII-8BIT"
+        insist { b.encoding.name } == "UTF-8"
+        insist { subject.convert(a).encoding.name } == "UTF-8"
+        insist { subject.convert(a) } == b
+      end
+    end
+  end
+end
diff --git a/spec/util/environment_spec.rb b/spec/util/environment_spec.rb
new file mode 100644
index 00000000000..4c43b9c5e27
--- /dev/null
+++ b/spec/util/environment_spec.rb
@@ -0,0 +1,16 @@
+require "logstash/environment"
+
+describe LogStash::Environment do
+
+  describe "load_elasticsearch_jars!" do
+
+    it "should load elasticsarch jars" do
+      expect{LogStash::Environment.load_elasticsearch_jars!}.to_not raise_error
+    end
+
+    it "should raise when cannot find elasticsarch jars" do
+      stub_const("LogStash::Environment::ELASTICSEARCH_DIR", "/some/invalid/path")
+      expect{LogStash::Environment.load_elasticsearch_jars!}.to raise_error(LogStash::EnvironmentError)
+    end
+  end
+end
diff --git a/spec/util/fieldeval_spec.rb b/spec/util/fieldeval_spec.rb
new file mode 100644
index 00000000000..963ad3925f5
--- /dev/null
+++ b/spec/util/fieldeval_spec.rb
@@ -0,0 +1,96 @@
+require "spec_helper"
+require "logstash/util/fieldreference"
+
+describe LogStash::Util::FieldReference, :if => true do
+
+  context "using simple accessor" do
+
+    it "should retrieve value" do
+      str = "hello"
+      m = eval(subject.compile(str))
+      data = { "hello" => "world" }
+      insist { m.call(data) } == data[str]
+    end
+
+    it "should handle delete in block" do
+      str = "simple"
+      m = eval(subject.compile(str))
+      data = { "simple" => "things" }
+      m.call(data) { |obj, key| obj.delete(key) }
+      insist { data }.empty?
+    end
+
+    it "should handle assignment in block" do
+      str = "simple"
+      m = eval(subject.compile(str))
+      data = {}
+      insist { m.call(data) { |obj, key| obj[key] = "things" }} == "things"
+      insist { data } == { "simple" => "things" }
+    end
+
+    it "should handle assignment using set" do
+      str = "simple"
+      data = {}
+      insist { subject.set(str, "things", data) } == "things"
+      insist { data } == { "simple" => "things" }
+    end
+  end
+
+  context "using accessor path" do
+
+    it "should retrieve shallow value" do
+      str = "[hello]"
+      m = eval(subject.compile(str))
+      data = { "hello" =>  "world" }
+      insist { m.call(data) } == "world"
+    end
+
+    it "should retrieve deep value" do
+      str = "[hello][world]"
+      m = eval(subject.compile(str))
+      data = { "hello" => { "world" => "foo", "bar" => "baz" } }
+      insist { m.call(data) } == data["hello"]["world"]
+    end
+
+    it "should handle delete in block" do
+      str = "[hello][world]"
+      m = eval(subject.compile(str))
+      data = { "hello" => { "world" => "foo", "bar" => "baz" } }
+      m.call(data) { |obj, key| obj.delete(key) }
+
+      # Make sure the "world" key is removed.
+      insist { data["hello"] } == { "bar" => "baz" }
+    end
+
+    it "should not handle assignment in block" do
+      str = "[hello][world]"
+      m = eval(subject.compile(str))
+      data = {}
+      insist { m.call(data) { |obj, key| obj[key] = "things" }}.nil?
+      insist { data } == { }
+    end
+
+    it "should set shallow value" do
+      str = "[hello]"
+      data = {}
+      insist { subject.set(str, "foo", data) } == "foo"
+      insist { data } == { "hello" => "foo" }
+    end
+
+    it "should set deep value" do
+      str = "[hello][world]"
+      data = {}
+      insist { subject.set(str, "foo", data) } == "foo"
+      insist { data } == { "hello" => { "world" => "foo" } }
+    end
+
+    it "should retrieve array item" do
+      data = { "hello" => { "world" => ["a", "b"], "bar" => "baz" } }
+      m = eval(subject.compile("[hello][world][0]"))
+      insist { m.call(data) } == data["hello"]["world"][0]
+
+      m = eval(subject.compile("[hello][world][1]"))
+      insist { m.call(data) } == data["hello"]["world"][1]
+    end
+  end
+end
diff --git a/spec/util/jar_spec.rb b/spec/util/jar_spec.rb
new file mode 100644
index 00000000000..b644580bda5
--- /dev/null
+++ b/spec/util/jar_spec.rb
@@ -0,0 +1,27 @@
+require "insist"
+
+describe "logstash jar features", :if => (__FILE__ =~ /file:.*!/) do
+  let(:jar_root) { __FILE__.split("!").first + "!" }
+
+  it "must be only run from a jar" do
+    insist { __FILE__ } =~ /file:.*!/
+  end
+
+  it "must contain GeoLiteCity.dat" do
+    path = File.join(jar_root, "GeoLiteCity.dat")
+    insist { File }.exists?(path)
+  end
+
+  it "must contain vendor/ua-parser/regexes.yaml" do
+    path = File.join(jar_root, "vendor/ua-parser/regexes.yaml")
+    insist { File }.exists?(path)
+  end
+
+  it "must successfully load aws-sdk (LOGSTASH-1718)" do
+    require "aws-sdk"
+    # trigger autoload
+    AWS::Errors
+    AWS::Record
+    AWS::Core::AsyncHandle
+  end
+end
diff --git a/spec/util/json_spec.rb b/spec/util/json_spec.rb
new file mode 100644
index 00000000000..a745f91a1e8
--- /dev/null
+++ b/spec/util/json_spec.rb
@@ -0,0 +1,96 @@
+# encoding: utf-8
+require "logstash/json"
+require "logstash/environment"
+require "logstash/util"
+
+describe LogStash::Json do
+
+  let(:hash)   {{"a" => 1}}
+  let(:json_hash)   {"{\"a\":1}"}
+
+  let(:string) {"foobar"}
+  let(:json_string) {"\"foobar\""}
+
+  let(:array)  {["foo", "bar"]}
+  let(:json_array)  {"[\"foo\",\"bar\"]"}
+
+  let(:multi) {
+    [
+      {:ruby => "foo bar baz", :json => "\"foo bar baz\""},
+      {:ruby => "1", :json => "\"1\""},
+      {:ruby => {"a" => true}, :json => "{\"a\":true}"},
+      {:ruby => {"a" => nil}, :json => "{\"a\":null}"},
+      {:ruby => ["a", "b"], :json => "[\"a\",\"b\"]"},
+      {:ruby => [1, 2], :json => "[1,2]"},
+      {:ruby => [1, nil], :json => "[1,null]"},
+      {:ruby => {"a" => [1, 2]}, :json => "{\"a\":[1,2]}"},
+      {:ruby => {"a" => {"b" => 2}}, :json => "{\"a\":{\"b\":2}}"},
+      # {:ruby => , :json => },
+    ]
+  }
+
+  if LogStash::Environment.jruby?
+
+    ### JRuby specific
+
+    context "jruby deserialize" do
+      it "should respond to load and deserialize object" do
+        expect(JrJackson::Raw).to receive(:parse_raw).with(json_hash).and_call_original
+        expect(LogStash::Json.load(json_hash)).to eql(hash)
+      end
+    end
+
+    context "jruby serialize" do
+      it "should respond to dump and serialize object" do
+        expect(JrJackson::Json).to receive(:dump).with(string).and_call_original
+        expect(LogStash::Json.dump(string)).to eql(json_string)
+      end
+
+      it "should call JrJackson::Raw.generate for Hash" do
+        expect(JrJackson::Raw).to receive(:generate).with(hash).and_call_original
+        expect(LogStash::Json.dump(hash)).to eql(json_hash)
+      end
+
+      it "should call JrJackson::Raw.generate for Array" do
+        expect(JrJackson::Raw).to receive(:generate).with(array).and_call_original
+        expect(LogStash::Json.dump(array)).to eql(json_array)
+      end
+
+    end
+
+  else
+
+    ### MRI specific
+
+    it "should respond to load and deserialize object on mri" do
+      expect(Oj).to receive(:load).with(json).and_call_original
+      expect(LogStash::Json.load(json)).to eql(hash)
+    end
+
+    it "should respond to dump and serialize object on mri" do
+      expect(Oj).to receive(:dump).with(hash, anything).and_call_original
+      expect(LogStash::Json.dump(hash)).to eql(json)
+    end
+  end
+
+  ### non specific
+
+  it "should correctly deserialize" do
+    multi.each do |test|
+      # because JrJackson in :raw mode uses Java::JavaUtil::LinkedHashMap and
+      # Java::JavaUtil::ArrayList, we must cast to compare.
+      # other than that, they quack like their Ruby equivalent
+      expect(LogStash::Util.normalize(LogStash::Json.load(test[:json]))).to eql(test[:ruby])
+    end
+  end
+
+  it "should correctly serialize" do
+    multi.each do |test|
+      expect(LogStash::Json.dump(test[:ruby])).to eql(test[:json])
+    end
+  end
+
+  it "should raise Json::ParserError on invalid json" do
+    expect{LogStash::Json.load("abc")}.to raise_error LogStash::Json::ParserError
+  end
+end
diff --git a/spec/util_spec.rb b/spec/util_spec.rb
new file mode 100644
index 00000000000..aeff9bdb469
--- /dev/null
+++ b/spec/util_spec.rb
@@ -0,0 +1,33 @@
+require "logstash/util"
+
+
+describe LogStash::Util do
+
+  context "stringify_keys" do
+    it "should convert hash symbol keys to strings" do
+      expect(LogStash::Util.stringify_symbols({:a => 1, "b" => 2})).to eq({"a" => 1, "b" => 2})
+    end
+
+    it "should keep non symbolic hash keys as is" do
+      expect(LogStash::Util.stringify_symbols({1 => 1, 2.0 => 2})).to eq({1 => 1, 2.0 => 2})
+    end
+
+    it "should convert inner hash keys to strings" do
+      expect(LogStash::Util.stringify_symbols({:a => 1, "b" => {:c => 3}})).to eq({"a" => 1, "b" => {"c" => 3}})
+      expect(LogStash::Util.stringify_symbols([:a, 1, "b", {:c => 3}])).to eq(["a", 1, "b", {"c" => 3}])
+    end
+
+    it "should convert hash symbol values to strings" do
+      expect(LogStash::Util.stringify_symbols({:a => :a, "b" => :b})).to eq({"a" => "a", "b" => "b"})
+    end
+
+    it "should convert array symbol values to strings" do
+      expect(LogStash::Util.stringify_symbols([1, :a])).to eq([1, "a"])
+    end
+
+    it "should convert innner array symbol values to strings" do
+      expect(LogStash::Util.stringify_symbols({:a => [1, :b]})).to eq({"a" => [1, "b"]})
+      expect(LogStash::Util.stringify_symbols([:a, [1, :b]])).to eq(["a", [1, "b"]])
+    end
+  end
+end
diff --git a/test/conf/amqp-to-elasticsearch.conf b/test/conf/amqp-to-elasticsearch.conf
deleted file mode 100644
index 4ec0c5a60e8..00000000000
--- a/test/conf/amqp-to-elasticsearch.conf
+++ /dev/null
@@ -1,15 +0,0 @@
-input {
-  amqp {
-    type => "foo"
-    host => "localhost"
-    exchange_type => "direct"
-    name => "logstash"
-    #user => "guest"
-    #password => "guest"
-  }
-}
-
-output {
-  stdout { }
-  elasticsearch { }
-}
diff --git a/test/conf/stdin-to-amqp.conf b/test/conf/stdin-to-amqp.conf
deleted file mode 100644
index 7b0480e948a..00000000000
--- a/test/conf/stdin-to-amqp.conf
+++ /dev/null
@@ -1,21 +0,0 @@
-input {
-  stdin {
-    type => "stuff"
-  }
-}
-
-filter {
-  grok {
-    type => "stuff"
-    pattern => ["%{SYSLOG_SUDO}", "%{SYSLOG_KERNEL}", "%{SYSLOGLINE}"]
-  }
-}
-
-output {
-  stdout { }
-  amqp {
-    host => "localhost"
-    exchange_type => "direct"
-    name => "logstash"
-  }
-}
diff --git a/test/conf/stdin-to-elasticsearch.conf b/test/conf/stdin-to-elasticsearch.conf
deleted file mode 100644
index 6642f3bfba3..00000000000
--- a/test/conf/stdin-to-elasticsearch.conf
+++ /dev/null
@@ -1,10 +0,0 @@
-input {
-  stdin {
-    type => "stuff"
-  }
-}
-
-output {
-  stdout { }
-  elasticsearch { }
-}
diff --git a/test/integration/README.md b/test/integration/README.md
new file mode 100644
index 00000000000..acc19206d7f
--- /dev/null
+++ b/test/integration/README.md
@@ -0,0 +1,59 @@
+# integration tests
+
+## performance tests
+
+### run.rb
+
+executes a single test.
+
+a test can be execute for a specific number of events of for a specific duration.
+
+- logstash config are in `test/integration/config`
+- sample input files are in `test/integration/input`
+
+#### by number of events
+
+```
+ruby test/integration/run.rb --events [number of events] --config [logstash config file] --input [sample input events file]
+```
+
+the sample input events file will be sent to logstash stdin repetedly until the required number of events is reached
+
+#### by target duration
+
+```
+ruby test/integration/run.rb --time [number of seconds] --config [logstash config file] --input [sample input events file]
+```
+
+the sample input events file will be sent to logstash stdin repetedly until the test elaspsed time reached the target time
+
+
+### suite.rb
+
+- suites are in `test/integration/suite`
+
+```
+ruby test/integration/suite.rb [suite file]
+```
+
+a suite file defines a series of tests to run.
+
+#### suite file format
+
+```ruby
+# each test can be executed by either target duration using :time => N secs
+# or by number of events with :events => N
+#
+#[
+#  {:name => "simple json out", :config => "config/simple_json_out.conf", :input => "input/simple_10.txt", :time => 30},
+#  {:name => "simple json out", :config => "config/simple_json_out.conf", :input => "input/simple_10.txt", :events => 50000},
+#]
+#
+[
+  {:name => "simple json out", :config => "config/simple_json_out.conf", :input => "input/simple_10.txt", :time => 60},
+  {:name => "simple line out", :config => "config/simple.conf", :input => "input/simple_10.txt", :time => 60},
+  {:name => "json codec", :config => "config/json_inout_codec.conf", :input => "input/json_medium.txt", :time => 60},
+  {:name => "json filter", :config => "config/json_inout_filter.conf", :input => "input/json_medium.txt", :time => 60},
+  {:name => "complex syslog", :config => "config/complex_syslog.conf", :input => "input/syslog_acl_10.txt", :time => 60},
+]
+```
\ No newline at end of file
diff --git a/test/integration/config/complex_syslog.conf b/test/integration/config/complex_syslog.conf
new file mode 100644
index 00000000000..c7db7bf51a8
--- /dev/null
+++ b/test/integration/config/complex_syslog.conf
@@ -0,0 +1,46 @@
+input {
+  stdin {
+    type => syslog
+  }
+}
+
+filter {
+  if [type] == "syslog" {
+    grok {
+      match => { "message" => "<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{PROG:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
+      add_field => [ "received_at", "%{@timestamp}" ]
+      add_field => [ "received_from", "%{syslog_hostname}" ]
+    }
+    syslog_pri { }
+    date {
+      match => ["syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
+    }
+
+    if [syslog_timestamp] {
+      mutate {
+        add_field => [ "[times][created_at]",  "%{syslog_timestamp}"]
+        add_field => [ "[times][received_at]",  "%{@timestamp}"]
+      }
+    }
+
+    mutate {
+      add_field => [ "[hosts][source]",  "%{received_from}"]
+      add_field => [ "[level][facility]",  "%{syslog_facility}"]
+      add_field => [ "[level][severity]",  "%{syslog_severity}"]
+    }
+
+    if !("_grokparsefailure" in [tags]) {
+      mutate {
+        replace => [ "@source_host", "%{syslog_hostname}" ]
+        replace => [ "@message", "%{syslog_message}" ]
+      }
+    }
+    mutate {
+      remove_field => [ "syslog_hostname", "syslog_message", "syslog_timestamp" ]
+    }
+  }
+}
+
+output {
+  stdout { codec => json_lines }
+}
diff --git a/test/integration/config/json_inout_codec.conf b/test/integration/config/json_inout_codec.conf
new file mode 100644
index 00000000000..d8b79e2190d
--- /dev/null
+++ b/test/integration/config/json_inout_codec.conf
@@ -0,0 +1,11 @@
+input {
+  stdin { codec => "json_lines" }
+}
+
+filter {
+  noop {}
+}
+
+output {
+  stdout { codec => json_lines }
+}
diff --git a/test/integration/config/json_inout_filter.conf b/test/integration/config/json_inout_filter.conf
new file mode 100644
index 00000000000..afad781c5c4
--- /dev/null
+++ b/test/integration/config/json_inout_filter.conf
@@ -0,0 +1,11 @@
+input {
+  stdin {}
+}
+
+filter {
+  json { source => "message" }
+}
+
+output {
+  stdout { codec => json_lines }
+}
diff --git a/test/integration/config/simple.conf b/test/integration/config/simple.conf
new file mode 100644
index 00000000000..a3967bd1231
--- /dev/null
+++ b/test/integration/config/simple.conf
@@ -0,0 +1,11 @@
+input {
+  stdin {}
+}
+
+filter {
+  noop {}
+}
+
+output {
+  stdout { codec => line }
+}
diff --git a/test/integration/config/simple_json_out.conf b/test/integration/config/simple_json_out.conf
new file mode 100644
index 00000000000..2c98527c328
--- /dev/null
+++ b/test/integration/config/simple_json_out.conf
@@ -0,0 +1,11 @@
+input {
+  stdin {}
+}
+
+filter {
+  noop {}
+}
+
+output {
+  stdout { codec => json_lines }
+}
diff --git a/test/integration/input/json_medium.txt b/test/integration/input/json_medium.txt
new file mode 100644
index 00000000000..96b808f22b1
--- /dev/null
+++ b/test/integration/input/json_medium.txt
@@ -0,0 +1,10 @@
+{"_scroll_id":"xxx", "took":5, "timed_out":false, "_shards":{"total":15,"successful":15,"failed":0}, "hits":{"total":1000050, "max_score":1.0, "hits":[{"_index":"logstash2", "_type":"logs", "_id":"AmaqL7VuSWKF-F6N_Gz72g", "_score":1.0, "_source" : {"message":"foobar", "@version":"1", "@timestamp":"2014-05-19T21:08:39.000Z", "host":"colin-mbp13r"} } ] } }
+{"_scroll_id":"xxx", "took":5, "timed_out":false, "_shards":{"total":15,"successful":15,"failed":0}, "hits":{"total":1000050, "max_score":1.0, "hits":[{"_index":"logstash2", "_type":"logs", "_id":"AmaqL7VuSWKF-F6N_Gz72g", "_score":1.0, "_source" : {"message":"foobar", "@version":"1", "@timestamp":"2014-05-19T21:08:39.000Z", "host":"colin-mbp13r"} } ] } }
+{"_scroll_id":"xxx", "took":5, "timed_out":false, "_shards":{"total":15,"successful":15,"failed":0}, "hits":{"total":1000050, "max_score":1.0, "hits":[{"_index":"logstash2", "_type":"logs", "_id":"AmaqL7VuSWKF-F6N_Gz72g", "_score":1.0, "_source" : {"message":"foobar", "@version":"1", "@timestamp":"2014-05-19T21:08:39.000Z", "host":"colin-mbp13r"} } ] } }
+{"_scroll_id":"xxx", "took":5, "timed_out":false, "_shards":{"total":15,"successful":15,"failed":0}, "hits":{"total":1000050, "max_score":1.0, "hits":[{"_index":"logstash2", "_type":"logs", "_id":"AmaqL7VuSWKF-F6N_Gz72g", "_score":1.0, "_source" : {"message":"foobar", "@version":"1", "@timestamp":"2014-05-19T21:08:39.000Z", "host":"colin-mbp13r"} } ] } }
+{"_scroll_id":"xxx", "took":5, "timed_out":false, "_shards":{"total":15,"successful":15,"failed":0}, "hits":{"total":1000050, "max_score":1.0, "hits":[{"_index":"logstash2", "_type":"logs", "_id":"AmaqL7VuSWKF-F6N_Gz72g", "_score":1.0, "_source" : {"message":"foobar", "@version":"1", "@timestamp":"2014-05-19T21:08:39.000Z", "host":"colin-mbp13r"} } ] } }
+{"_scroll_id":"xxx", "took":5, "timed_out":false, "_shards":{"total":15,"successful":15,"failed":0}, "hits":{"total":1000050, "max_score":1.0, "hits":[{"_index":"logstash2", "_type":"logs", "_id":"AmaqL7VuSWKF-F6N_Gz72g", "_score":1.0, "_source" : {"message":"foobar", "@version":"1", "@timestamp":"2014-05-19T21:08:39.000Z", "host":"colin-mbp13r"} } ] } }
+{"_scroll_id":"xxx", "took":5, "timed_out":false, "_shards":{"total":15,"successful":15,"failed":0}, "hits":{"total":1000050, "max_score":1.0, "hits":[{"_index":"logstash2", "_type":"logs", "_id":"AmaqL7VuSWKF-F6N_Gz72g", "_score":1.0, "_source" : {"message":"foobar", "@version":"1", "@timestamp":"2014-05-19T21:08:39.000Z", "host":"colin-mbp13r"} } ] } }
+{"_scroll_id":"xxx", "took":5, "timed_out":false, "_shards":{"total":15,"successful":15,"failed":0}, "hits":{"total":1000050, "max_score":1.0, "hits":[{"_index":"logstash2", "_type":"logs", "_id":"AmaqL7VuSWKF-F6N_Gz72g", "_score":1.0, "_source" : {"message":"foobar", "@version":"1", "@timestamp":"2014-05-19T21:08:39.000Z", "host":"colin-mbp13r"} } ] } }
+{"_scroll_id":"xxx", "took":5, "timed_out":false, "_shards":{"total":15,"successful":15,"failed":0}, "hits":{"total":1000050, "max_score":1.0, "hits":[{"_index":"logstash2", "_type":"logs", "_id":"AmaqL7VuSWKF-F6N_Gz72g", "_score":1.0, "_source" : {"message":"foobar", "@version":"1", "@timestamp":"2014-05-19T21:08:39.000Z", "host":"colin-mbp13r"} } ] } }
+{"_scroll_id":"xxx", "took":5, "timed_out":false, "_shards":{"total":15,"successful":15,"failed":0}, "hits":{"total":1000050, "max_score":1.0, "hits":[{"_index":"logstash2", "_type":"logs", "_id":"AmaqL7VuSWKF-F6N_Gz72g", "_score":1.0, "_source" : {"message":"foobar", "@version":"1", "@timestamp":"2014-05-19T21:08:39.000Z", "host":"colin-mbp13r"} } ] } }
diff --git a/test/integration/input/simple_10.txt b/test/integration/input/simple_10.txt
new file mode 100644
index 00000000000..8a9b58e04d6
--- /dev/null
+++ b/test/integration/input/simple_10.txt
@@ -0,0 +1,10 @@
+test 01
+test 02
+test 03
+test 04
+test 05
+test 06
+test 07
+test 08
+test 09
+test 10
\ No newline at end of file
diff --git a/test/integration/input/syslog_acl_10.txt b/test/integration/input/syslog_acl_10.txt
new file mode 100644
index 00000000000..d277d856660
--- /dev/null
+++ b/test/integration/input/syslog_acl_10.txt
@@ -0,0 +1,10 @@
+<164>Oct 26 15:19:25 1.2.3.4 %ASA-4-106023: Deny udp src DRAC:10.1.2.3/43434 dst outside:192.168.0.1/53 by access-group "acl_drac" [0x0, 0x0]
+<164>Oct  6 15:20:25 2.2.3.4 %ASA-4-106023: Deny udp src DRAC:10.1.2.4/43434 dst outside:192.168.0.1/53 by access-group "acl_drac" [0x0, 0x0]
+<164>Oct  1 15:21:25 3.2.3.4 %ASA-4-106023: Allow tcp src DRAC:10.1.2.5/43434 dst outside:192.168.0.1/53 by access-group "acl_drac" [0x0, 0x0]
+<164>Oct 30 15:22:25 4.2.3.4 %ASA-4-106023: Allow tcp src DRAC:10.1.2.6/43434 dst outside:192.168.0.1/53 by access-group "acl_drac" [0x0, 0x0]
+<164>Oct 26 15:19:25 1.2.3.4 %ASA-4-106023: Deny udp src DRAC:10.1.2.3/43434 dst outside:192.168.0.1/53 by access-group "acl_drac" [0x0, 0x0]
+<164>Oct  6 15:20:25 2.2.3.4 %ASA-4-106023: Deny udp src DRAC:10.1.2.4/43434 dst outside:192.168.0.1/53 by access-group "acl_drac" [0x0, 0x0]
+<164>Oct  1 15:21:25 3.2.3.4 %ASA-4-106023: Allow tcp src DRAC:10.1.2.5/43434 dst outside:192.168.0.1/53 by access-group "acl_drac" [0x0, 0x0]
+<164>Oct 30 15:22:25 4.2.3.4 %ASA-4-106023: Allow tcp src DRAC:10.1.2.6/43434 dst outside:192.168.0.1/53 by access-group "acl_drac" [0x0, 0x0]
+<164>Oct 26 15:19:25 1.2.3.4 %ASA-4-106023: Deny udp src DRAC:10.1.2.3/43434 dst outside:192.168.0.1/53 by access-group "acl_drac" [0x0, 0x0]
+<164>Oct  6 15:20:25 2.2.3.4 %ASA-4-106023: Deny udp src DRAC:10.1.2.4/43434 dst outside:192.168.0.1/53 by access-group "acl_drac" [0x0, 0x0]
diff --git a/test/integration/run.rb b/test/integration/run.rb
new file mode 100644
index 00000000000..9d8a4d976a9
--- /dev/null
+++ b/test/integration/run.rb
@@ -0,0 +1,166 @@
+# encoding: utf-8
+
+require "benchmark"
+require "thread"
+require "open3"
+
+INITIAL_MESSAGE = ">>> lorem ipsum start".freeze
+LAST_MESSAGE = ">>> lorem ipsum stop".freeze
+LOGSTASH_BIN = File.join(File.expand_path("../../../bin/", __FILE__), "logstash")
+REFRESH_COUNT = 100
+
+Thread.abort_on_exception = true
+
+def feed_input_events(io, events_count, lines, last_message)
+  loop_count = (events_count / lines.size).ceil # how many time we send the input file over
+
+  (1..loop_count).each{lines.each {|line| io.puts(line)}}
+
+  io.puts(last_message)
+  io.flush
+
+  loop_count * lines.size
+end
+
+def feed_input_interval(io, seconds, lines, last_message)
+  loop_count = (2000 / lines.size).ceil # check time every ~2000(ceil) input lines
+  lines_per_iteration = loop_count * lines.size
+  start_time = Time.now
+  count = 0
+
+  while true
+    (1..loop_count).each{lines.each {|line| io.puts(line)}}
+    count += lines_per_iteration
+    break if (Time.now - start_time) >= seconds
+  end
+
+  io.puts(last_message)
+  io.flush
+
+  count
+end
+
+# below stats counter and output reader threads are sharing state using
+# the @stats_lock mutex, @stats_count and @stats. this is a bit messy and should be
+# refactored into a proper class eventually
+
+def detach_stats_counter
+  Thread.new do
+    loop do
+      start = @stats_lock.synchronize{@stats_count}
+      sleep(1)
+      @stats_lock.synchronize{@stats << (@stats_count - start)}
+    end
+  end
+end
+
+# detach_output_reader spawns a thread that will fill in the @stats instance var with tps samples for every seconds
+# @stats access is synchronized using the @stats_lock mutex but can be safely used
+# once the output reader thread is completed.
+def detach_output_reader(io, regex)
+  Thread.new(io, regex) do |io, regex|
+    i = 0
+    @stats = []
+    @stats_count = 0
+    @stats_lock = Mutex.new
+    t = detach_stats_counter
+
+    expect_output(io, regex) do
+      i += 1
+      # avoid mutex synchronize on every loop cycle, using REFRESH_COUNT = 100 results in
+      # much lower mutex overhead and still provides a good resolution since we are typically
+      # have 2000..100000 tps
+      @stats_lock.synchronize{@stats_count = i} if (i % REFRESH_COUNT) == 0
+    end
+
+    @stats_lock.synchronize{t.kill}
+  end
+end
+
+def read_input_file(file_path)
+  IO.readlines(file_path).map(&:chomp)
+end
+
+def expect_output(io, regex)
+  io.each_line do |line|
+    puts("received: #{line}") if @debug
+    yield if block_given?
+    break if line =~ regex
+  end
+end
+
+def percentile(array, percentile)
+  count = (array.length * (1.0 - percentile)).floor
+  array.sort[-count..-1]
+end
+
+#
+## script main
+
+# standalone quick & dirty options parsing
+args = ARGV.dup
+if args.size != 6
+  $stderr.puts("usage: ruby run.rb --events [events count] --config [config file] --input [input file]")
+  $stderr.puts("       ruby run.rb --time [seconds] --config [config file] --input [input file]")
+  exit(1)
+end
+
+options = {}
+while !args.empty?
+  config = args.shift.to_s.strip
+  option = args.shift.to_s.strip
+  raise(IllegalArgumentException, "invalid option for #{config}") if option.empty?
+  case config
+  when "--events"
+    options[:events] = option
+  when "--time"
+    options[:time] = option
+  when "--config"
+    options[:config] = option
+  when "--input"
+    options[:input] = option
+  else
+    raise(IllegalArgumentException, "invalid config #{config}")
+  end
+end
+
+@debug = !!ENV["DEBUG"]
+
+required_events_count = options[:events].to_i # total number of events to feed, independant of input file size
+required_run_time = options[:time].to_i
+input_lines = read_input_file(options[:input])
+
+puts("using config file=#{options[:config]}, input file=#{options[:input]}") if @debug
+
+command = [LOGSTASH_BIN, "-f", options[:config], "2>&1"]
+puts("launching #{command.join(" ")}") if @debug
+
+real_events_count = 0
+
+Open3.popen3(*command) do |i, o, e|
+  puts("sending initial event") if @debug
+  i.puts(INITIAL_MESSAGE)
+  i.flush
+
+  puts("waiting for initial event") if @debug
+  expect_output(o, /#{INITIAL_MESSAGE}/)
+
+  puts("starting output reader thread") if @debug
+  reader = detach_output_reader(o, /#{LAST_MESSAGE}/)
+  puts("starting feeding input") if @debug
+
+  elaspsed = Benchmark.realtime do
+    real_events_count = if required_events_count > 0
+      feed_input_events(i, [required_events_count, input_lines.size].max, input_lines, LAST_MESSAGE)
+    else
+      feed_input_interval(i, required_run_time, input_lines, LAST_MESSAGE)
+    end
+
+    puts("waiting for output reader to complete") if @debug
+    reader.join
+  end
+
+  # the reader thread updates the @stats tps array
+  p = percentile(@stats, 0.80)
+  puts("elaspsed=#{"%.2f" % elaspsed}s, events=#{real_events_count}, avg tps=#{"%.0f" % (real_events_count / elaspsed)}, best tps=#{p.last}, avg top 20% tps=#{"%.0f" % (p.reduce(:+) / p.size)}")
+end
diff --git a/test/integration/suite.rb b/test/integration/suite.rb
new file mode 100644
index 00000000000..b0bcf7787da
--- /dev/null
+++ b/test/integration/suite.rb
@@ -0,0 +1,25 @@
+# encoding: utf-8
+
+RUNNER = File.join(File.expand_path(File.dirname(__FILE__)), "run.rb")
+BASE_DIR = File.expand_path(File.dirname(__FILE__))
+
+#
+## script main
+
+if ARGV.size != 1
+  $stderr.puts("usage: ruby suite.rb [suite file]")
+  exit(1)
+end
+
+@debug = !!ENV["DEBUG"]
+
+tests = eval(IO.read(ARGV[0]))
+
+tests.each do |test|
+  duration = test[:events] ? ["--events", test[:events]] : ["--time", test[:time]]
+  command = ["ruby", RUNNER, *duration, "--config", File.join(BASE_DIR, test[:config]), "--input", File.join(BASE_DIR, test[:input])]
+  IO.popen(command.join(" "), "r") do |io|
+    print("name=#{test[:name]}, ")
+    io.each_line{|line| puts(line)}
+  end
+end
diff --git a/test/integration/suite/basic_performance_long.rb b/test/integration/suite/basic_performance_long.rb
new file mode 100644
index 00000000000..ce7474b4d21
--- /dev/null
+++ b/test/integration/suite/basic_performance_long.rb
@@ -0,0 +1,17 @@
+# format description:
+# each test can be executed by either target duration using :time => N secs
+# or by number of events with :events => N
+#
+#[
+#  {:name => "simple json out", :config => "config/simple_json_out.conf", :input => "input/simple_10.txt", :time => 30},
+#  {:name => "simple json out", :config => "config/simple_json_out.conf", :input => "input/simple_10.txt", :events => 50000},
+#]
+#
+[
+  {:name => "simple line in/out", :config => "config/simple.conf", :input => "input/simple_10.txt", :time => 120},
+  {:name => "simple line in/json out", :config => "config/simple_json_out.conf", :input => "input/simple_10.txt", :time => 120},
+  {:name => "json codec in/out", :config => "config/json_inout_codec.conf", :input => "input/json_medium.txt", :time => 120},
+  {:name => "line in/json filter/json out", :config => "config/json_inout_filter.conf", :input => "input/json_medium.txt", :time => 120},
+  {:name => "apache in/json out", :config => "config/standard_apache.conf", :input => "input/apache_log.txt", :time => 120},
+  {:name => "syslog in/json out", :config => "config/complex_syslog.conf", :input => "input/syslog_acl_10.txt", :time => 120},
+]
\ No newline at end of file
diff --git a/test/integration/suite/basic_performance_quick.rb b/test/integration/suite/basic_performance_quick.rb
new file mode 100644
index 00000000000..14b2a1215eb
--- /dev/null
+++ b/test/integration/suite/basic_performance_quick.rb
@@ -0,0 +1,17 @@
+# format description:
+# each test can be executed by either target duration using :time => N secs
+# or by number of events with :events => N
+#
+#[
+#  {:name => "simple json out", :config => "config/simple_json_out.conf", :input => "input/simple_10.txt", :time => 30},
+#  {:name => "simple json out", :config => "config/simple_json_out.conf", :input => "input/simple_10.txt", :events => 50000},
+#]
+#
+[
+  {:name => "simple line in/out", :config => "config/simple.conf", :input => "input/simple_10.txt", :time => 30},
+  {:name => "simple line in/json out", :config => "config/simple_json_out.conf", :input => "input/simple_10.txt", :time => 30},
+  {:name => "json codec in/out", :config => "config/json_inout_codec.conf", :input => "input/json_medium.txt", :time => 30},
+  {:name => "line in/json filter/json out", :config => "config/json_inout_filter.conf", :input => "input/json_medium.txt", :time => 30},
+  {:name => "apache in/json out", :config => "config/standard_apache.conf", :input => "input/apache_log.txt", :time => 30},
+  {:name => "syslog in/json out", :config => "config/complex_syslog.conf", :input => "input/syslog_acl_10.txt", :time => 30},
+]
\ No newline at end of file
diff --git a/test/jenkins/config.xml.erb b/test/jenkins/config.xml.erb
new file mode 100644
index 00000000000..f6a4401a19b
--- /dev/null
+++ b/test/jenkins/config.xml.erb
@@ -0,0 +1,74 @@
+<?xml version='1.0' encoding='UTF-8'?>
+<project>
+  <actions/>
+  <description></description>
+  <keepDependencies>false</keepDependencies>
+  <properties/>
+  <scm class="hudson.plugins.git.GitSCM" plugin="git@1.1.25">
+    <configVersion>2</configVersion>
+    <userRemoteConfigs>
+      <hudson.plugins.git.UserRemoteConfig>
+        <name></name>
+        <refspec></refspec>
+        <url>https://github.com/logstash/logstash.git</url>
+      </hudson.plugins.git.UserRemoteConfig>
+    </userRemoteConfigs>
+    <branches>
+      <hudson.plugins.git.BranchSpec>
+        <name>master</name>
+      </hudson.plugins.git.BranchSpec>
+    </branches>
+    <disableSubmodules>false</disableSubmodules>
+    <recursiveSubmodules>false</recursiveSubmodules>
+    <doGenerateSubmoduleConfigurations>false</doGenerateSubmoduleConfigurations>
+    <authorOrCommitter>false</authorOrCommitter>
+    <clean>false</clean>
+    <wipeOutWorkspace>false</wipeOutWorkspace>
+    <pruneBranches>false</pruneBranches>
+    <remotePoll>false</remotePoll>
+    <ignoreNotifyCommit>false</ignoreNotifyCommit>
+    <useShallowClone>false</useShallowClone>
+    <buildChooser class="hudson.plugins.git.util.DefaultBuildChooser"/>
+    <gitTool>Default</gitTool>
+    <submoduleCfg class="list"/>
+    <relativeTargetDir></relativeTargetDir>
+    <reference></reference>
+    <excludedRegions></excludedRegions>
+    <excludedUsers></excludedUsers>
+    <gitConfigName></gitConfigName>
+    <gitConfigEmail></gitConfigEmail>
+    <skipTag>false</skipTag>
+    <includedRegions></includedRegions>
+    <scmName></scmName>
+  </scm>
+  <canRoam>true</canRoam>
+  <disabled>false</disabled>
+  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
+  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
+  <triggers class="vector">
+    <com.cloudbees.jenkins.GitHubPushTrigger plugin="github@1.4">
+      <spec></spec>
+    </com.cloudbees.jenkins.GitHubPushTrigger>
+  </triggers>
+  <concurrentBuild>false</concurrentBuild>
+  <builders>
+    <hudson.tasks.Shell>
+      <command>export JRUBY_OPTS=--1.9
+
+bundle install
+bundle exec rspec spec/<%= plugin_path %>
+</command>
+    </hudson.tasks.Shell>
+  </builders>
+  <publishers/>
+  <buildWrappers>
+    <ruby-proxy-object>
+      <ruby-object ruby-class="Jenkins::Plugin::Proxies::BuildWrapper" pluginid="rvm">
+        <pluginid pluginid="rvm" ruby-class="String">rvm</pluginid>
+        <object ruby-class="RvmWrapper" pluginid="rvm">
+          <impl pluginid="rvm" ruby-class="String">1.6.8@logstash</impl>
+        </object>
+      </ruby-object>
+    </ruby-proxy-object>
+  </buildWrappers>
+</project>
diff --git a/test/jenkins/create-jobs.rb b/test/jenkins/create-jobs.rb
new file mode 100644
index 00000000000..c0e0899629a
--- /dev/null
+++ b/test/jenkins/create-jobs.rb
@@ -0,0 +1,23 @@
+#!/usr/bin/env ruby
+
+require "erb"
+
+if ENV["JENKINS_HOME"].nil?
+  puts "No JENKINS_HOME set."
+  exit 1
+end
+
+plugindir = File.join(File.dirname(__FILE__), "..", "..", "lib", "logstash")
+
+plugins = %w(inputs filters outputs).collect { |t| Dir.glob(File.join(plugindir, t, "*.rb")) }.flatten
+
+template = ERB.new(File.read(File.join(File.dirname(__FILE__), "config.xml.erb")))
+plugins.each do |path|
+  job = path.gsub(/.*\/([^\/]+)\/([^\/]+)\.rb$/, 'plugin.\1.\2')
+  plugin_path = path.gsub(/.*\/([^\/]+)\/([^\/]+)$/, '\1/\2')
+
+  jobdir = File.join(ENV["JENKINS_HOME"], "jobs", job)
+  puts "Writing #{jobdir}/config.xml"
+  Dir.mkdir(jobdir) if !Dir.exists?(jobdir)
+  File.write(File.join(jobdir, "config.xml"), template.result(binding))
+end
diff --git a/test/jenkins/generatorjob.config.xml b/test/jenkins/generatorjob.config.xml
new file mode 100644
index 00000000000..20f372f7e86
--- /dev/null
+++ b/test/jenkins/generatorjob.config.xml
@@ -0,0 +1,66 @@
+<?xml version='1.0' encoding='UTF-8'?>
+<project>
+  <actions/>
+  <description></description>
+  <keepDependencies>false</keepDependencies>
+  <properties/>
+  <scm class="hudson.plugins.git.GitSCM" plugin="git@1.1.25">
+    <configVersion>2</configVersion>
+    <userRemoteConfigs>
+      <hudson.plugins.git.UserRemoteConfig>
+        <name></name>
+        <refspec></refspec>
+        <url>https://github.com/logstash/logstash.git</url>
+      </hudson.plugins.git.UserRemoteConfig>
+    </userRemoteConfigs>
+    <branches>
+      <hudson.plugins.git.BranchSpec>
+        <name>master</name>
+      </hudson.plugins.git.BranchSpec>
+    </branches>
+    <disableSubmodules>false</disableSubmodules>
+    <recursiveSubmodules>false</recursiveSubmodules>
+    <doGenerateSubmoduleConfigurations>false</doGenerateSubmoduleConfigurations>
+    <authorOrCommitter>false</authorOrCommitter>
+    <clean>false</clean>
+    <wipeOutWorkspace>false</wipeOutWorkspace>
+    <pruneBranches>false</pruneBranches>
+    <remotePoll>false</remotePoll>
+    <ignoreNotifyCommit>false</ignoreNotifyCommit>
+    <useShallowClone>false</useShallowClone>
+    <buildChooser class="hudson.plugins.git.util.DefaultBuildChooser"/>
+    <gitTool>Default</gitTool>
+    <submoduleCfg class="list"/>
+    <relativeTargetDir></relativeTargetDir>
+    <reference></reference>
+    <excludedRegions></excludedRegions>
+    <excludedUsers></excludedUsers>
+    <gitConfigName></gitConfigName>
+    <gitConfigEmail></gitConfigEmail>
+    <skipTag>false</skipTag>
+    <includedRegions></includedRegions>
+    <scmName></scmName>
+  </scm>
+  <canRoam>true</canRoam>
+  <disabled>false</disabled>
+  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
+  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
+  <triggers class="vector"/>
+  <concurrentBuild>false</concurrentBuild>
+  <builders>
+    <hudson.tasks.Shell>
+      <command>ruby test/jenkins/create-jobs.rb</command>
+    </hudson.tasks.Shell>
+  </builders>
+  <publishers/>
+  <buildWrappers>
+    <ruby-proxy-object>
+      <ruby-object ruby-class="Jenkins::Plugin::Proxies::BuildWrapper" pluginid="rvm">
+        <pluginid pluginid="rvm" ruby-class="String">rvm</pluginid>
+        <object ruby-class="RvmWrapper" pluginid="rvm">
+          <impl pluginid="rvm" ruby-class="String">1.9.3</impl>
+        </object>
+      </ruby-object>
+    </ruby-proxy-object>
+  </buildWrappers>
+</project>
\ No newline at end of file
diff --git a/test/jruby/blah.sh b/test/jruby/blah.sh
deleted file mode 100644
index 4b926293f84..00000000000
--- a/test/jruby/blah.sh
+++ /dev/null
@@ -1,4 +0,0 @@
-[ -z "$1" ] && set -- 3000
-ulimit -c unlimited
-(sleep 5; seq -f "Feb 21 19:38:11 snack foo: hello world %g" $1 ; sleep 10) \
-  | exec -a "logstash" jruby -J-server -J-Xmx20m -J-verbose:gc ../../bin/logstash -f simple.conf -v
diff --git a/test/jruby/simple.conf b/test/jruby/simple.conf
deleted file mode 100644
index 92f668f7ff2..00000000000
--- a/test/jruby/simple.conf
+++ /dev/null
@@ -1,16 +0,0 @@
-input {
-  stdin { 
-    type => "foo"
-  }
-}
-
-#filter {
-  #grok {
-    #type => "foo"
-    ##pattern => "%{SYSLOGLINE}"
-  #}
-#}
-
-output {
-  stdout { debug => true }
-}
diff --git a/test/logstash/filters/test_date.rb b/test/logstash/filters/test_date.rb
deleted file mode 100644
index f6b6dd38022..00000000000
--- a/test/logstash/filters/test_date.rb
+++ /dev/null
@@ -1,119 +0,0 @@
-require "rubygems"
-$:.unshift File.dirname(__FILE__) + "/../../../lib"
-$:.unshift File.dirname(__FILE__) + "/../../"
-
-require "test/unit"
-require "logstash"
-require "logstash/loadlibs"
-require "logstash/filters"
-require "logstash/filters/date"
-require "logstash/event"
-require "timeout"
-
-class TestFilterDate < Test::Unit::TestCase
-
-  # These tests assume a given timezone.
-  def setup
-    ENV["TZ"] = "PST8PDT"
-  end
-
-  def test_name(name)
-    @typename = name.gsub(/[ ]/, "_")
-  end
-
-  def config(cfg)
-    cfg["type"] = @typename
-    cfg.each_key do |key|
-      if cfg[key].is_a?(String)
-        cfg[key] = [cfg[key]]
-      end
-    end
-
-    @filter = LogStash::Filters::Date.new(cfg)
-    @filter.register
-  end
-
-  def test_iso8601
-    test_name "iso8601"
-    config "field1" => "ISO8601"
-
-    times = {
-      "2001-01-01T00:00:00-0800"         => "2001-01-01T08:00:00.000Z",
-      "1974-03-02T04:09:09-0800"         => "1974-03-02T12:09:09.000Z",
-      "2010-05-03T08:18:18+00:00"        => "2010-05-03T08:18:18.000Z",
-      "2004-07-04T12:27:27-00:00"        => "2004-07-04T12:27:27.000Z",
-      "2001-09-05T16:36:36+0000"         => "2001-09-05T16:36:36.000Z",
-      "2001-11-06T20:45:45-0000"         => "2001-11-06T20:45:45.000Z",
-      "2001-12-07T23:54:54Z"             => "2001-12-07T23:54:54.000Z",
-      "2001-01-01T00:00:00.123"          => "2001-01-01T08:00:00.123Z",
-
-      # older daylights savings?
-      "1974-03-02T04:09:09.123"          => "1974-03-02T11:09:09.123Z",
-      "2010-05-03T08:18:18.123+00:00"    => "2010-05-03T08:18:18.123Z",
-      "2004-07-04T12:27:27.123-04:00"    => "2004-07-04T16:27:27.123Z",
-      "2001-09-05T16:36:36.123+0700"     => "2001-09-05T09:36:36.123Z",
-      "2001-11-06T20:45:45.123-0000"     => "2001-11-06T20:45:45.123Z",
-      "2001-12-07T23:54:54.123Z"         => "2001-12-07T23:54:54.123Z",
-    }
-    
-    event = LogStash::Event.new
-    event.type = @typename
-    times.each do |input, output|
-      event.fields["field1"] = input
-      @filter.filter(event)
-      assert_equal(output, event.timestamp,
-                   "Time '#{input}' should parse to '#{output}' but got '#{event.timestamp}'")
-    end
-  end # def test_iso8601
-
-  def test_formats
-    test_name "format test"
-    #config "field1" => "%b %e %H:%M:%S"
-    config "field1" => "MMM dd HH:mm:ss"
-
-    now = Time.now
-    now += now.gmt_offset
-    year = now.year
-    times = {
-      "Nov 24 01:29:01" => "#{year}-11-24T09:29:01.000Z",
-    }
-
-    event = LogStash::Event.new
-    event.type = @typename
-    times.each do |input, output|
-      event.fields["field1"] = input
-      @filter.filter(event)
-      assert_equal(output, event.timestamp)
-    end
-  end # test_formats
-
-  def test_speed
-    test_name "speed test"
-    config "field1" => "MMM dd HH:mm:ss"
-    iterations = 50000
-
-    start = Time.now
-    gmt_now = start + start.gmt_offset
-    year = gmt_now.year
-    input = "Nov 24 01:29:01" 
-    output = "#{year}-11-24T09:29:01.000Z"
-
-    event = LogStash::Event.new
-    event.type = @typename
-    event.fields["field1"] = input
-    check_interval = 1500
-    max_duration = 10
-    Timeout.timeout(max_duration * 2) do 
-      1.upto(50000).each do |i|
-        @filter.filter(event)
-        if i % check_interval == 0
-          assert_equal(event.timestamp, output)
-        end
-      end
-    end # Timeout.timeout
-
-    duration = Time.now - start
-    puts "filters/date speed test; #{iterations} iterations: #{duration} seconds (#{iterations / duration} per sec)"
-    assert(duration < 10, "Should be able to do #{iterations} date parses in less than #{max_duration} seconds, got #{duration} seconds")
-  end # test_formats
-end
diff --git a/test/logstash/filters/test_grep.rb b/test/logstash/filters/test_grep.rb
deleted file mode 100644
index 661ecaa2e88..00000000000
--- a/test/logstash/filters/test_grep.rb
+++ /dev/null
@@ -1,165 +0,0 @@
-require "rubygems"
-$:.unshift File.dirname(__FILE__) + "/../../../lib"
-$:.unshift File.dirname(__FILE__) + "/../../"
-
-require "test/unit"
-require "logstash/loadlibs"
-require "logstash"
-require "logstash/filters"
-require "logstash/filters/grep"
-require "logstash/event"
-
-class TestFilterGrep < Test::Unit::TestCase
-  def setup
-    @filter = LogStash::Filters.from_name("grep", {})
-  end # def setup
-
-  def test_name(name)
-    @typename = name
-  end # def test_name
-
-  def config(cfg)
-    cfg["type"] = @typename
-    cfg.each_key do |key|
-      if cfg[key].is_a?(String)
-        cfg[key] = cfg[key].to_a
-      end
-    end
-
-    @filter = LogStash::Filters::Grep.new(cfg)
-    @filter.register
-  end # def config
-
-  def test_single_match
-    test_name "single_match"
-    config "str" => "test"
-
-    event = LogStash::Event.new
-    event.type = @typename
-    event["str"] = "test: this should not be dropped"
-    @filter.filter(event)
-    assert_equal(false, event.cancelled?)
-  end # def test_single_match
-
-  def test_single_match_drop
-    test_name "single_match_dropp"
-    config "str" => "test"
-
-    event = LogStash::Event.new
-    event.type = @typename
-    event["str"] = "foo: this should be dropped"
-    @filter.filter(event)
-    assert_equal(true, event.cancelled?)
-  end # def test_single_match_drop
-
-  def test_multiple_match
-    test_name "multiple_match"
-    config "str" => "test", "bar" => "baz"
-
-    event = LogStash::Event.new
-    event.type = @typename
-    event["str"] = "test: this should not be dropped"
-    event["bar"] = "foo baz foo"
-    @filter.filter(event)
-    assert_equal(false, event.cancelled?)
-  end # test_multiple_match
-
-  def test_multiple_match_drop
-    test_name "multiple_match_drop"
-    config "str" => "test", "bar" => "baz"
-
-    event = LogStash::Event.new
-    event.type = @typename
-    event["str"] = "test: this should be dropped"
-    event["bar"] = "foo bAz foo"
-    @filter.filter(event)
-    assert_equal(true, event.cancelled?)
-  end # test_multiple_match_drop
-
-  def test_single_match_regexp
-    test_name "single_match_regexp"
-    config "str" => "(?i)test.*foo"
-
-    event = LogStash::Event.new
-    event.type = @typename
-    event["str"] = "TeST regexp match FoO"
-    @filter.filter(event)
-    assert_equal(false, event.cancelled?)
-  end # def test_single_match_regexp
-
-  def test_single_match_regexp_drop
-    test_name "single_match_regexp_drop"
-    config "str" => "test.*foo"
-
-    event = LogStash::Event.new
-    event.type = @typename
-    event["str"] = "TeST regexp match FoO"
-    @filter.filter(event)
-    assert_equal(true, event.cancelled?)
-  end # def test_single_match_regexp_drop
-
-  def test_add_fields
-    test_name "add_field"
-    config "str" => "test",
-           "add_field" => ["new_field", "new_value"]
-
-    event = LogStash::Event.new
-    event.type = @typename
-    event["str"] = "test"
-    @filter.filter(event)
-    assert_equal(["new_value"], event["new_field"])
-  end # def test_add_fields
-
-  def test_add_fields_with_format
-    test_name "add_field_with_format"
-    config "str" => "test",
-           "add_field" => ["new_field", "%{@type}"]
-
-    event = LogStash::Event.new
-    event.type = @typename
-    event["str"] = "test"
-    @filter.filter(event)
-    assert_equal([event.type], event["new_field"])
-  end # def test_add_fields_with_format
-
-  def __DISABLED_FOR_NOW_test_add_fields_multiple_match
-    test_name "add_fields_multiple_match"
-    #config "match" => {"str" => "test"},
-           #"add_fields" => {"new_field" => "new_value"}},
-           #"match" => {"str" => ".*"},
-             #"add_fields" => {"new_field" => "new_value_2"}},
-           #]
-
-    event = LogStash::Event.new
-    event.type = @typename
-    event["str"] = "test"
-    @filter.filter(event)
-    assert_equal(["new_value", "new_value_2"], event["new_field"])
-  end # def test_add_fields_multiple_match
-
-  def test_add_tags
-    test_name "add_tags"
-    config "str" => "test",
-           "add_tag" => ["new_tag"]
-
-    event = LogStash::Event.new
-    event.tags << "tag"
-    event.type = @typename
-    event["str"] = "test"
-    @filter.filter(event)
-    assert_equal(["tag", "new_tag"], event.tags)
-  end # def test_add_tags
-
-  def test_add_tags_with_format
-    test_name "add_tags_with_format"
-    config "str" => "test",
-           "add_tag" => ["%{str}"]
-
-    event = LogStash::Event.new
-    event.tags << "tag"
-    event.type = @typename
-    event["str"] = "test"
-    @filter.filter(event)
-    assert_equal(["tag", event["str"]], event.tags)
-  end # def test_add_tags
-end # TestFilterGrep
diff --git a/test/logstash/filters/test_grok.rb b/test/logstash/filters/test_grok.rb
deleted file mode 100644
index eeab09b93b2..00000000000
--- a/test/logstash/filters/test_grok.rb
+++ /dev/null
@@ -1,131 +0,0 @@
-require "rubygems"
-$:.unshift File.dirname(__FILE__) + "/../../../lib"
-$:.unshift File.dirname(__FILE__) + "/../../"
-
-require "test/unit"
-require "logstash"
-require "logstash/loadlibs"
-require "logstash/filters"
-require "logstash/filters/grok"
-require "logstash/event"
-
-class TestFilterGrok < Test::Unit::TestCase
-
-  def test_name(name)
-    @typename = name.gsub(/[ ]/, "_")
-  end
-
-  def config(cfg)
-    cfg["type"] = @typename
-    cfg.each_key do |key|
-      if cfg[key].is_a?(String)
-        cfg[key] = cfg[key].to_a
-      end
-    end
-
-    @filter = LogStash::Filters::Grok.new(cfg)
-    @filter.register
-  end
-
-  def test_grok_normal
-    test_name "groknormal"
-    config "pattern" => [ "%{SYSLOGLINE}" ]
-    
-    event = LogStash::Event.new
-    event.type = @typename
-
-    logsource = "evita"
-    timestamp = "Mar 16 00:01:25"
-    message = "connect from camomile.cloud9.net[168.100.1.3]"
-    program = "postfix/smtpd"
-    pid = "1713"
-
-    #event.message = "Mar 16 00:01:25 evita postfix/smtpd[1713]: connect from camomile.cloud9.net[168.100.1.3]"
-    event.message = "#{timestamp} #{logsource} #{program}[#{pid}]: #{message}"
-
-    @filter.filter(event)
-    assert_equal(event.fields["logsource"], [logsource], 
-                 "Expected field 'logsource' to be [#{logsource.inspect}], is #{event.fields["logsource"].inspect}")
-
-    assert_equal(event.fields["timestamp"], [timestamp], "Expected field 'timestamp' to be [#{timestamp.inspect}], is #{event.fields["timestamp"].inspect}")
-
-    assert_equal(event.fields["message"], [message], "Expected field 'message' to be ['#{message.inspect}'], is #{event.fields["message"].inspect}")
-
-    assert_equal(event.fields["program"], [program], "Expected field 'program' to be ['#{program.inspect}'], is #{event.fields["program"].inspect}")
-
-    assert_equal(event.fields["pid"], [pid], "Expected field 'pid' to be ['#{pid.inspect}'], is #{event.fields["pid"].inspect}")
-  end # def test_grok_normal
-
-  def test_grok_multiple_message
-    test_name "groknormal"
-    config "pattern" => [ "(?:hello|world) %{NUMBER}" ]
-    
-    event = LogStash::Event.new
-    event.type = @typename
-    event.message = [ "hello 12345", "world 23456" ]
-
-    @filter.filter(event)
-    assert_equal(event.fields["NUMBER"].sort, ["12345", "23456"])
-  end # def test_grok_multiple_message
-
-  def test_speed
-    test_name "grokspeed"
-    config "pattern" => [ "%{SYSLOGLINE}" ]
-
-    iterations = 5000
-
-    start = Time.now
-
-    event = LogStash::Event.new
-    event.type = @typename
-
-    logsource = "evita"
-    timestamp = "Mar 16 00:01:25"
-    message = "connect from camomile.cloud9.net[168.100.1.3]"
-    program = "postfix/smtpd"
-    pid = "1713"
-
-    event.message = "#{timestamp} #{logsource} #{program}[#{pid}]: #{message}"
-
-    check_interval = 1500
-    1.upto(iterations).each do |i|
-      event.fields.clear
-      @filter.filter(event)
-    end
-
-    duration = Time.now - start
-    max_duration = 10
-    puts "filters/grok speed test; #{iterations} iterations: #{duration} seconds (#{"%.3f" % (iterations / duration)} per sec)"
-    assert(duration < max_duration, "Should be able to do #{iterations} grok parses in less than #{max_duration} seconds, got #{duration} seconds")
-  end # test_formats
-
-  def test_grok_type_hinting_int
-    test_name "groktypehinting_int"
-    config "pattern" => [ "%{NUMBER:foo:int}" ]
-    
-    event = LogStash::Event.new
-    event.type = @typename
-
-    expect = 12345
-    event.message = "#{expect}"
-
-    @filter.filter(event)
-    assert_equal(expect.class, event.fields["foo"].first.class, "Expected field 'foo' to be of type #{expect.class.name} but got #{event.fields["foo"].first.class.name}")
-    assert_equal([expect], event.fields["foo"], "Expected field 'foo' to be [#{expect.inspect}], is #{event.fields["expect"].inspect}")
-  end # def test_grok_type_hinting_int
-
-  def test_grok_type_hinting_float
-    test_name "groktypehinting_float"
-    config "pattern" => [ "%{NUMBER:foo:float}" ]
-    
-    event = LogStash::Event.new
-    event.type = @typename
-
-    expect = 3.1415
-    event.message = "#{expect}"
-
-    @filter.filter(event)
-    assert_equal(expect.class, event.fields["foo"].first.class, "Expected field 'foo' to be of type #{expect.class.name} but got #{event.fields["foo"].first.class.name}")
-    assert_equal([expect], event.fields["foo"], "Expected field 'foo' to be [#{expect.inspect}], is #{event.fields["expect"].inspect}")
-  end # def test_grok_type_hinting_float
-end
diff --git a/test/logstash/filters/test_multiline.rb b/test/logstash/filters/test_multiline.rb
deleted file mode 100644
index aca53280b8c..00000000000
--- a/test/logstash/filters/test_multiline.rb
+++ /dev/null
@@ -1,202 +0,0 @@
-require "rubygems"
-$:.unshift File.dirname(__FILE__) + "/../../../lib"
-$:.unshift File.dirname(__FILE__) + "/../../"
-require "test/unit"
-require "logstash"
-require "logstash/filters"
-require "logstash/filters/multiline"
-require "logstash/event"
-
-class TestFilterMultiline < Test::Unit::TestCase
-  def setup
-    @filter = LogStash::Filters.from_name("multiline", {})
-  end
-
-  def test_name(name)
-    @typename = name
-  end
-
-  def config(cfg)
-    cfg["type"] = @typename
-    cfg.each_key do |key|
-      if !cfg[key].is_a?(Array)
-        cfg[key] = [cfg[key]]
-      end
-    end
-
-    @filter = LogStash::Filters::Multiline.new(cfg)
-    @filter.register
-  end
-
-  def test_with_next
-    test_name "with next"
-    config "pattern" => "\\.\\.\\.$", "what" => "next"
-
-    inputs = [
-      "hello world ...",
-      "and more!",
-      "one",
-      "two...",
-      "two again",
-    ]
-
-    expected_outputs = [
-      "hello world ...\nand more!",
-      "one",
-      "two...\ntwo again",
-    ]
-         
-    outputs = []
-
-    inputs.each do |input|
-      event = LogStash::Event.new
-      event.type = @typename
-      event.message = input
-      @filter.filter(event)
-      if !event.cancelled?
-        outputs << event.message
-      end
-    end
-
-    assert_equal(expected_outputs.length, outputs.length,
-                 "Incorrect number of output events")
-    expected_outputs.zip(outputs).each do |expected, actual|
-      assert_equal(expected, actual)
-    end
-  end # def test_with_next
-  
-  def test_with_previous
-    test_name "with previous"
-    config "pattern" => "^\\s", "what" => "previous"
-
-    inputs = [
-      "hello world ...",
-      "   and more!",
-      "one",
-      "two",
-      "   two 1",
-      "   two 2",
-      "   two 3",
-      "three",
-    ]
-
-    expected_outputs = [
-      "hello world ...\n   and more!",
-      "one",
-      "two\n   two 1\n   two 2\n   two 3",
-      "three"
-    ]
-         
-    outputs = []
-
-    inputs.each do |input|
-      event = LogStash::Event.new
-      event.type = @typename
-      event.message = input
-      @filter.filter(event)
-      if !event.cancelled?
-        outputs << event.message
-      end
-    end
-    last = @filter.flush("unknown", @typename)
-    if last
-      outputs << last.message
-    end
-
-    assert_equal(expected_outputs.length, outputs.length,
-                 "Incorrect number of output events")
-    expected_outputs.zip(outputs).each do |expected, actual|
-      assert_equal(expected, actual)
-    end
-  end
-
-  def test_with_negate_true
-    @logger = LogStash::Logger.new(STDERR)
-    test_name "with negate true"
-    config "pattern" => "^\\S", "what" => "previous", "negate" => "true"
-
-    inputs = [
-      "hello world ...",
-      "   and more!",
-      "one",
-      "two",
-      "   two 1",
-      "   two 2",
-      "   two 3",
-      "three",
-    ]
-
-    expected_outputs = [
-      "hello world ...\n   and more!",
-      "one",
-      "two\n   two 1\n   two 2\n   two 3",
-      "three"
-    ]
-
-    outputs = []
-
-    inputs.each do |input|
-      event = LogStash::Event.new
-      event.type = @typename
-      event.message = input
-      @filter.filter(event)
-      if !event.cancelled?
-        outputs << event.message
-      end
-    end
-    last = @filter.flush("unknown", @typename)
-    if last
-      outputs << last.message
-    end
-    assert_equal(expected_outputs.length, outputs.length,
-                 "Incorrect number of output events")
-    expected_outputs.zip(outputs).each do |expected, actual|
-      assert_equal(expected, actual)
-    end
-  end
-
-  def test_with_negate_false
-    @logger = LogStash::Logger.new(STDERR)
-    test_name "with negate true"
-    config "pattern" => "^\\s", "what" => "previous", "negate" => "false"
-
-    inputs = [
-      "hello world ...",
-      "   and more!",
-      "one",
-      "two",
-      "   two 1",
-      "   two 2",
-      "   two 3",
-      "three",
-    ]
-
-    expected_outputs = [
-      "hello world ...\n   and more!",
-      "one",
-      "two\n   two 1\n   two 2\n   two 3",
-      "three"
-    ]
-
-    outputs = []
-
-    inputs.each do |input|
-      event = LogStash::Event.new
-      event.type = @typename
-      event.message = input
-      @filter.filter(event)
-      if !event.cancelled?
-        outputs << event.message
-      end
-    end
-    last = @filter.flush("unknown", @typename)
-    if last
-      outputs << last.message
-    end
-    assert_equal(expected_outputs.length, outputs.length,
-                 "Incorrect number of output events")
-    expected_outputs.zip(outputs).each do |expected, actual|
-      assert_equal(expected, actual)
-    end
-  end
-end
diff --git a/test/logstash/inputs/test_file.rb b/test/logstash/inputs/test_file.rb
deleted file mode 100644
index a466372e82f..00000000000
--- a/test/logstash/inputs/test_file.rb
+++ /dev/null
@@ -1,85 +0,0 @@
-#!/usr/bin/env ruby
-require 'rubygems'
-$:.unshift File.dirname(__FILE__) + "/../../../lib"
-$:.unshift File.dirname(__FILE__) + "/../../"
-
-require "test/unit"
-require "tempfile"
-require "thread"
-require "logstash/loadlibs"
-require "logstash/agent"
-require "logstash/logging"
-require "logstash/util"
-require "socket"
-
-class TestInputFile < Test::Unit::TestCase
-  def setup
-    @tmpfile = Tempfile.new(self.class.name)
-    @hostname = Socket.gethostname
-    @type = "logstash-test"
-
-    @agent = LogStash::Agent.new
-    config = LogStash::Config::File.new(path=nil, string=<<-CONFIG)
-      input {
-        file {
-          path => "#{@tmpfile.path}"
-          type => "#{@type}"
-        }
-      }
-
-      output {
-        internal { }
-      }
-    CONFIG
-    
-    waitqueue = Queue.new
-
-    Thread.new do
-      @agent.run_with_config(config) do
-        waitqueue << :ready
-      end
-    end
-
-    # Wait for the agent to be ready.
-    waitqueue.pop
-    @output = @agent.outputs.first
-  end # def setup
-
-  def test_simple
-    data = [ "hello", "world", "hello world 1 2 3 4", "1", "2", "3", "4", "5" ]
-    remaining = data.size
-    expect_data = data.clone
-
-    queue = Queue.new
-    @output.subscribe do |event|
-      queue << event
-    end
-
-    # Write to the file periodically
-    Thread.new do
-      LogStash::Util.set_thread_name("#{__FILE__} - periodic writer")
-      loop do
-        out = data.shift((rand * 3).to_i + 1).join("\n")
-        @tmpfile.puts out
-        @tmpfile.flush
-        break if data.length == 0
-      end # loop
-    end # timer thread
-
-    loop do
-      event = queue.pop
-      expect_message = expect_data.shift
-      assert_equal(expect_message, event.message)
-      assert_equal("file://#{@hostname}#{@tmpfile.path}", event.source)
-      assert_equal(@type, event.type, "type")
-      assert_equal([], event.tags, "tags should be empty")
-
-      # Done testing if we run out of data.
-      if expect_data.size == 0
-        @agent.stop 
-        break
-      end
-    end
-  end # def test_simple
-end # class TestInputFile
-
diff --git a/test/logstash/inputs/test_stomp.rb b/test/logstash/inputs/test_stomp.rb
deleted file mode 100644
index 0b0b2343967..00000000000
--- a/test/logstash/inputs/test_stomp.rb
+++ /dev/null
@@ -1,117 +0,0 @@
-require "rubygems"
-$:.unshift File.dirname(__FILE__) + "/../../../lib"
-$:.unshift File.dirname(__FILE__) + "/../../"
-
-require "logstash/testcase"
-require "logstash/agent"
-require "logstash/stomp/handler"
-require "logstash/logging"
-
-# TODO(sissel): Add tests coverage for authenticated stomp sessions
-# TODO(sissel): What about queue vs fanout vs topic?
-
-class TestInputStomp < LogStash::TestCase
-  def em_setup
-    @flags ||= []
-
-    # Run stompserver in debug mode if desired.
-    @flags << "-d" if $DEBUG
-
-    # Launch stomp server on a random port
-    stomp_done = false
-    @stomp_pid = nil
-    1.upto(30) do
-      @port = (rand * 30000 + 20000).to_i
-      @stomp_pid = Process.fork do
-        args = ["-p", @port.to_s, *@flags]
-
-        # Ask rubygems where stmpserver is
-        stompserver = Gem.bin_path('stompserver', 'stompserver')
-
-        # Run this ruby. This makes things work better if we're run under a strange
-        # environment like rvm or gems installed to ~/.gems.
-        ruby = File.readlink("/proc/self/exe")
-        cmd = [ruby, "-rubygems", stompserver, *args]
-        exec(*cmd)
-        $stderr.puts($!)
-        exit 1
-      end
-      
-      # Let stompserver start up and try to start listening.
-      # Hard to otherwise test this. Maybe a tcp connection with timeouts?
-      sleep(2)
-      Process.waitpid(@stomp_pid, Process::WNOHANG)
-      if $? != nil and $?.exited?
-        # Try again
-      else
-        stomp_done = true
-        break
-      end
-    end
-
-    if !stomp_done
-      raise "Stompserver failed to start (failure to find ephemeral port? stompserver not installed?)"
-    end
-
-    @queue = "/queue/testing"
-    config = {
-      "inputs" => {
-        @type => [
-          "stomp://localhost:#{@port}#{@queue}"
-        ]
-      },
-      "outputs" => [
-        "internal:///"
-      ]
-    }
-
-    super(config)
-
-    @stomp = EventMachine::connect("127.0.0.1", @port, LogStash::Stomp::Handler,
-                                   nil, LogStash::Logger.new(STDERR),
-                                   URI.parse(config["inputs"][@type][0]))
-    @stomp.should_subscribe = false
-  end # def em_setup
-
-  def test_stomp_basic
-    inputs = [
-      LogStash::Event.new("@message" => "hello world", "@type" => @type),
-      LogStash::Event.new("@message" => "one two three", "@type" => @type),
-      LogStash::Event.new("@message" => "one two three", "@type" => @type,
-                          "@fields" => { "field1" => "value1"})
-    ]
-    EventMachine::run do
-      em_setup
-      expected_events = inputs.clone
-      @output.subscribe do |event|
-        expect = expected_events.shift
-        #ap :event => event.to_hash
-
-        assert_equal(expect.message, event.message, "@message")
-        assert_equal(expect.type, event.type, "@type")
-        assert_equal(expect.tags, event.tags, "@tags")
-        assert_equal(expect.timestamp, event.timestamp, "@tags")
-        assert_equal(expect.fields, event.fields, "@tags")
-        @agent.stop if expected_events.size == 0
-      end
-
-      timer = EM::PeriodicTimer.new(0.2) do
-        next if !@stomp.ready
-
-        if inputs.size == 0
-          timer.cancel
-          next
-        end
-
-        event = inputs.shift
-        @stomp.send @queue, event.to_json
-      end
-    end
-  end # def test_stomp_basic
-
-  def teardown
-    if @stomp_pid
-      Process.kill("KILL", @stomp_pid)
-    end
-  end # def teardown
-end # class TestInputStomp
diff --git a/test/logstash/inputs/test_syslog.rb b/test/logstash/inputs/test_syslog.rb
deleted file mode 100644
index 7c9b5259afa..00000000000
--- a/test/logstash/inputs/test_syslog.rb
+++ /dev/null
@@ -1,76 +0,0 @@
-require "rubygems"
-$:.unshift File.dirname(__FILE__) + "/../../../lib"
-$:.unshift File.dirname(__FILE__) + "/../../"
-
-require "logstash/testcase"
-require "logstash/agent"
-
-class TestInputSyslog < LogStash::TestCase
-  def em_setup
-    config = {
-      "inputs" => {
-        @type => [
-        ]
-      },
-      "outputs" => [
-        "internal:///"
-      ]
-    }
-
-    done = false
-    # TODO(sissel): refactor this into something reusable?
-    1.upto(30) do
-      begin
-        # Grab a a random port to listen on.
-        @port = (rand * 30000 + 20000).to_i
-        config["inputs"][@type] = ["syslog://127.0.0.1:#{@port}"]
-        super(config)
-        done = true
-        break
-      rescue => e
-        # Verified working with EventMachine 0.12.10
-        if e.is_a?(RuntimeError) && e.message == "no acceptor"
-          # ignore, it's likely we tried to listen on a port already in use.
-        else
-          raise e
-        end
-      end # rescue
-    end # loop for an ephemeral port
-
-    if !done
-      raise "Couldn't find a port to bind on."
-    end
-
-    # Override input.
-    @connection = EventMachine::connect("127.0.0.1", @port)
-    @input = EventMachine::Channel.new
-    @input.subscribe do |message|
-      @connection.send_data(message)
-    end
-  end # def em_setup
-
-  def test_syslog_normal
-    inputs = [
-      "<1>Dec 19 12:30:48 snack nagios3: Auto-save of retention data completed successfully.",
-      "<2>Dec 19 11:35:32 carrera sshd[28882]: Failed password for invalid user PlcmSpIp from 121.9.210.245 port 48846 ssh2"
-    ]
-
-    EventMachine.run do
-      em_setup
-
-      expected_messages = inputs.clone
-      @output.subscribe do |event|
-        expect = expected_messages.shift
-        #assert_equal(expect, event.message)
-        assert_equal(expect.split(" ", 5)[-1], event.message)
-        if expected_messages.size == 0
-          @agent.stop
-        end
-      end
-
-      EM::PeriodicTimer.new(0.3) do 
-        @input.push inputs.shift + "\n" if inputs.size > 0
-      end
-    end
-  end
-end # class TestInputSyslog
diff --git a/test/logstash/outputs/test_elasticsearch.rb b/test/logstash/outputs/test_elasticsearch.rb
deleted file mode 100644
index 1f898272a89..00000000000
--- a/test/logstash/outputs/test_elasticsearch.rb
+++ /dev/null
@@ -1,124 +0,0 @@
-require "rubygems"
-$:.unshift File.dirname(__FILE__) + "/../../../lib"
-$:.unshift File.dirname(__FILE__) + "/../../"
-
-require "logstash/loadlibs"
-require "logstash/testcase"
-require "logstash/agent"
-require "logstash/logging"
-require "logstash/outputs/elasticsearch"
-require "logstash/search/elasticsearch"
-require "logstash/search/query"
-
-require "spoon" # rubygem 'spoon' - implements posix_spawn via FFI
-
-class TestOutputElasticSearch < Test::Unit::TestCase
-  ELASTICSEARCH_VERSION = "0.16.0"
-
-  def setup
-    start_elasticsearch
-    #@cluster = "logstash-test-1234"
-
-    @output = LogStash::Outputs::Elasticsearch.new({
-      "host" => ["localhost"],
-      "index" => ["test"],
-      "type" => ["foo"],
-      "cluster" => [@cluster],
-    })
-    @output.register
-  end # def setup
-
-  def start_elasticsearch
-    # install
-    version = self.class::ELASTICSEARCH_VERSION
-    system("make -C #{File.dirname(__FILE__)}/../../setup/elasticsearch/ init-elasticsearch-#{version} wipe-elasticsearch-#{version} #{$DEBUG ? "" : "> /dev/null 2>&1"}")
-
-    #1.upto(30) do
-      # Pick a random port
-      #@port_http = (rand * 30000 + 20000).to_i
-      #@port_tcp = (rand * 30000 + 20000).to_i
-    #end # try a few times to launch ES on a random port.
-    
-    # Listen on random ports, I don't need them anyway.
-    @port_http = 0
-    @port_tcp = 0
-
-    teardown if @es_pid
-    @cluster = "logstash-test-#{$$}"
-
-    puts "Starting ElasticSearch #{version}"
-    @clusterflags = "-Des.cluster.name=#{@cluster}"
-
-    ENV["ESFLAGS"] = "-Des.http.port=#{@port_http} -Des.transport.tcp.port=#{@port_tcp} "
-    ENV["ESFLAGS"] += @clusterflags
-    ENV["ESFLAGS"] += " > /dev/null 2>&1" if !$DEBUG
-    cmd = ["make", "-C", "#{File.dirname(__FILE__)}/../../setup/elasticsearch/",]
-    cmd << "-s" if !$DEBUG
-    cmd << "run-elasticsearch-#{version}"
-    @es_pid = Spoon.spawnp(*cmd)
-
-    # Assume it's up and happy, or will be.
-    #raise "ElasticSearch failed to start or was otherwise not running properly?"
-  end # def start_elasticsearch
-
-  def teardown
-    # Kill the whole process group for elasticsearch
-    Process.kill("KILL", -1 * @es_pid) rescue nil
-    Process.kill("KILL", @es_pid) rescue nil
-
-    # TODO(sissel): Until I fix the way elasticsearch server is run,
-    # we'll use pkill...
-    system("pkill -9 -f 'java.*#{@clusterflags}.*Bootstrap'")
-  end # def teardown
-
-  def test_elasticsearch_basic
-    events = []
-    myfile = File.basename(__FILE__)
-    1.upto(5).each do |i|
-      events << LogStash::Event.new("@message" => "just another log rollin' #{i}",
-                                    "@source" => "logstash tests in #{myfile}")
-    end
-
-    # TODO(sissel): Need a way to hook when the agent is ready?
-    events.each do |e|
-      puts "Pushing event: #{e}" if $DEBUG
-      @output.receive(e)
-    end
-
-    tries = 30 
-    es = LogStash::Search::ElasticSearch.new(:cluster => @cluster)
-    loop do
-      puts "Tries left: #{tries}" if $DEBUG
-      query = LogStash::Search::Query.new(:query_string => "*", :count => 5)
-      es.search(query, async=false) do |result|
-        if events.size == result.events.size
-          puts "Found #{result.events.size} events, ready to verify!"
-          expected = events.clone
-          assert_equal(events.size, result.events.size)
-          #events.each { |e| p :expect => e }
-          result.events.each do |event|
-            assert(expected.include?(event), "Found event in results that was not expected: #{event.inspect}\n\nExpected: #{events.map{ |a| a.inspect }.join("\n")}")
-          end
-
-          return
-        else
-          tries -= 1
-          if tries <= 0
-            assert(false, "Gave up trying to query elasticsearch. Maybe we aren't indexing properly?")
-            return
-          end
-        end # if events.size == hits.size
-      end # es.search
-
-      sleep 0.2
-    end # loop
-  end # def test_elasticsearch_basic
-end # class TestOutputElasticSearch
-
-#class TestOutputElasticSearch0_15_1 < TestOutputElasticSearch
-  #ELASTICSEARCH_VERSION = self.name[/[0-9_]+/].gsub("_", ".")
-#end # class TestOutputElasticSearch0_15_1
-
-#class TestOutputElasticSearch0_13_1 < TestOutputElasticSearch
-  #ELASTICSEARCH_VERSION = self.name[/[0-9_]+/].gsub("_", ".")
-#end # class TestOutputElasticSearch0_13_1
diff --git a/test/logstash/test_event.rb b/test/logstash/test_event.rb
deleted file mode 100644
index 83565d0739d..00000000000
--- a/test/logstash/test_event.rb
+++ /dev/null
@@ -1,30 +0,0 @@
-require "rubygems"
-$:.unshift File.dirname(__FILE__) + "/../../lib"
-$:.unshift File.dirname(__FILE__) + "/../"
-
-require "test/unit"
-require "logstash"
-require "logstash/event"
-
-class TestEvent < Test::Unit::TestCase
-  def test_name(name)
-    @typename = name
-  end
-
-  def test_sprintf
-    test_name "sprintf"
-    event = LogStash::Event.new
-    event.type = @typename
-    event.message = "hello world"
-    event.source = "/home/foo"
-    event["test"] = "test"
-
-    ["@type", "@message", "@source", "test"].each do |name|
-      assert_equal(event[name], event.sprintf("%{#{name}}"))
-    end
-
-    event.fields["foo"] = ["one", "two", "three"]
-
-    assert_equal(event.fields["foo"].join(","), event.sprintf("%{foo}"))
-  end # def test_sprintf
-end # TestEvent
diff --git a/test/logstash/test_syntax.rb b/test/logstash/test_syntax.rb
deleted file mode 100644
index f1cbef7cd66..00000000000
--- a/test/logstash/test_syntax.rb
+++ /dev/null
@@ -1,15 +0,0 @@
-require "rubygems"
-require "test/unit"
-
-class SyntaxCheckTest < Test::Unit::TestCase
-  def setup
-    @dir = File.dirname(__FILE__)
-  end
-
-  def test_ruby_syntax
-    Dir["#{@dir}/../**/*.rb"].each do |path|
-      output = %x{ruby -c #{path} 2>&1}
-      assert_equal(0, $?.exitstatus, "Syntax error for #{path}: #{output}")
-    end
-  end
-end
diff --git a/test/logstash/testcase.rb b/test/logstash/testcase.rb
deleted file mode 100644
index 8f2ddb79f34..00000000000
--- a/test/logstash/testcase.rb
+++ /dev/null
@@ -1,20 +0,0 @@
-require "rubygems"
-$:.unshift File.dirname(__FILE__) + "/../../../lib"
-$:.unshift File.dirname(__FILE__) + "/../../"
-
-require "test/unit"
-require "socket"
-require "logstash/namespace"
-
-class LogStash::TestCase < Test::Unit::TestCase
-  def setup
-    super
-    @type = "default"
-    @hostname = Socket.gethostname
-  end
-
-  # We have to include at least one test here, otherwise Test::Unit barfs about
-  # not tests for this class, even though it's just a superclass for real test
-  # cases.
-  def test_ok; end
-end # class LogStash::TestCase
diff --git a/test/logstash/web/api_test.txt b/test/logstash/web/api_test.txt
deleted file mode 100644
index e8cdbd24f94..00000000000
--- a/test/logstash/web/api_test.txt
+++ /dev/null
@@ -1,11 +0,0 @@
-
-# Should emit pretty
-  http://localhost:9292/api/search?q=*&pretty=true
-  http://localhost:9292/api/histogram?q=*&pretty=true
-
-# Should be normal json
-  http://localhost:9292/api/search?q=*
-  http://localhost:9292/api/histogram?q=*
-
-# Should be plain text
-  http://localhost:9292/api/search.txt?q=*
diff --git a/test/logstash_test_runner.rb b/test/logstash_test_runner.rb
deleted file mode 100644
index 2cd95cc48dd..00000000000
--- a/test/logstash_test_runner.rb
+++ /dev/null
@@ -1,35 +0,0 @@
-require "rubygems"
-$:.unshift "#{File.dirname(__FILE__)}/../lib/"
-
-# Fix for Ruby 1.9.x
-$:.unshift "#{File.dirname(__FILE__)}"
-
-def skip(path)
-  puts "Skipping tests: #{path}"
-end
-
-def use(path)
-  puts "Loading tests from #{path}"
-  require path
-end
-
-use "logstash/test_syntax"
-use "logstash/test_event"
-use "logstash/filters/test_date"
-use "logstash/filters/test_grep"
-use "logstash/filters/test_multiline"
-use "logstash/filters/test_grok"
-
-if __FILE__ =~ /^file:.*\.jar/
-  puts "Skipping elasticsearch tests since we're running from a jar."
-  skip "logstash/outputs/test_elasticsearch"
-else
-  use "logstash/outputs/test_elasticsearch"
-end
-
-skip "logstash/inputs/test_file"
-skip "logstash/inputs/test_syslog"
-skip "logstash/inputs/test_stomp"
-
-Test::Unit::AutoRunner.run
-
diff --git a/test/setup/elasticsearch/Makefile b/test/setup/elasticsearch/Makefile
deleted file mode 100644
index fe695a9f80b..00000000000
--- a/test/setup/elasticsearch/Makefile
+++ /dev/null
@@ -1,23 +0,0 @@
-
-elasticsearch-%.zip:
-	wget --no-check-certificate http://github.com/downloads/elasticsearch/elasticsearch/$@
-
-elasticsearch-%:
-	$(MAKE) $@.zip
-	unzip $@.zip
-
-init-elasticsearch-%:
-	$(MAKE) elasticsearch-$*
-	cd elasticsearch-$*; bin/plugin install river-rabbitmq
-
-run-elasticsearch-%:
-	$(MAKE) elasticsearch-$*
-	cd elasticsearch-$*; \
-	bin/elasticsearch -f $(ESFLAGS)
-
-wipe-elasticsearch-%:
-	$(MAKE) elasticsearch-$*
-	rm -rf elasticsearch-$*/work elasticsearch-$*/data
-
-remove-elasticsearch-%:
-	rm -rf elasticsearch-$* elasticsearch-$*.zip
diff --git a/test/standalone.sh b/test/standalone.sh
deleted file mode 100644
index 96c522b2a4f..00000000000
--- a/test/standalone.sh
+++ /dev/null
@@ -1,49 +0,0 @@
-#!/usr/bin/env bash
-rvm="$HOME/.rvm/scripts/rvm"
-
-if [ ! -f "$rvm" ] ; then
-  echo "rvm not found? You should install it."
-  exit 1
-fi
-
-#cat << RVMRC > $HOME/.rvmrc
-#rvm_install_on_use_flag=1
-#rvm_project_rvmrc=0
-#rvm_gemset_create_on_use_flag=1
-#RVMRC
-
-. "$rvm"
-rvm rvmrc trust logstash
-
-if [ "$1" = "" ] ; then
-  set -- "jruby-1.6.0"
-fi
-
-ruby="$1"
-gemset="logstash-testing"
-
-run() {
-  echo "$@"
-  "$@"
-}
-
-if ! run rvm list | grep "$ruby" ; then
-  run rvm install "$ruby"
-fi
-
-
-rm -f *.gem
-rvm --with-rubies "$ruby" gemset create $gemset
-
-# stompserver says it wants 'hoe >= 1.1.1' and the latest 'hoe' requires
-# rubygems >1.4, so, upgrade I guess... I hate ruby sometimes.
-run rvm "$ruby@$gemset" gem update --system
-run rvm "$ruby@$gemset" gem install --no-ri --no-rdoc stompserver
-
-run rvm "$ruby@$gemset" gem uninstall -ax logstash || true
-rm *.gem
-run rvm "$ruby@$gemset" gem build logstash.gemspec
-run rvm "$ruby@$gemset" gem install --no-ri --no-rdoc logstash-*.gem
-
-echo "Running tests now..."
-run rvm "$ruby@$gemset" exec logstash-test
diff --git a/tools/Gemfile b/tools/Gemfile
new file mode 100644
index 00000000000..0483cae4262
--- /dev/null
+++ b/tools/Gemfile
@@ -0,0 +1,5 @@
+source "https://rubygems.org"
+gemspec :path => File.expand_path(File.join(File.dirname(__FILE__), "..")), :name => "logstash", :development_group => :development
+
+# in development if a local, unpublished gems is required, you must add it first in the gemspec without the :path option
+# and also add it here with the :path option.
\ No newline at end of file
diff --git a/tools/Gemfile.beaker b/tools/Gemfile.beaker
new file mode 100644
index 00000000000..bcc56637eeb
--- /dev/null
+++ b/tools/Gemfile.beaker
@@ -0,0 +1,16 @@
+source 'https://rubygems.org'
+
+gem 'beaker'
+gem 'beaker-rspec'
+gem 'pry'
+gem 'docker-api', '~> 1.13.0'
+gem 'rubysl-securerandom'
+gem 'rspec_junit_formatter'
+gem 'rspec', '~> 2.14.0'
+
+case RUBY_VERSION
+when '1.8.7'
+  gem 'rake', '~> 10.1.0'
+else
+  gem 'rake'
+end
diff --git a/tools/Gemfile.jruby-1.9.lock b/tools/Gemfile.jruby-1.9.lock
new file mode 100644
index 00000000000..172b1472108
--- /dev/null
+++ b/tools/Gemfile.jruby-1.9.lock
@@ -0,0 +1,352 @@
+PATH
+  remote: /Users/colin/dev/src/elasticsearch/logstash
+  specs:
+    logstash (1.5.0.dev-java)
+      addressable
+      awesome_print
+      aws-sdk
+      beefcake (= 0.3.7)
+      bindata (>= 1.5.0)
+      cabin (>= 0.6.0)
+      ci_reporter
+      cinch
+      clamp
+      edn
+      elasticsearch
+      extlib (= 0.9.16)
+      ffi (~> 1.9.5)
+      ffi-rzmq (= 1.0.0)
+      filewatch (= 0.5.1)
+      ftw (~> 0.0.39)
+      gelf (= 1.3.2)
+      gelfd (= 0.2.0)
+      geoip (>= 1.3.2)
+      gmetric (= 0.1.3)
+      i18n (= 0.6.9)
+      insist (= 1.0.0)
+      jls-grok (= 0.11.0)
+      jls-lumberjack (>= 0.0.20)
+      jrjackson
+      jruby-httpclient
+      jruby-kafka (>= 0.1.0)
+      mail
+      march_hare (~> 2.5.1)
+      metriks
+      mime-types
+      minitest
+      mocha
+      msgpack-jruby
+      murmurhash3
+      pry
+      rack
+      rbnacl
+      redis
+      rspec (~> 2.14.0)
+      ruby-maven
+      rufus-scheduler (~> 2.0.24)
+      rumbster
+      shoulda
+      sinatra
+      snmp
+      spoon
+      statsd-ruby (= 1.2.0)
+      stud
+      twitter (= 5.0.0.rc.1)
+      user_agent_parser (>= 2.0.0)
+      xml-simple
+      xmpp4r (= 0.5)
+
+GEM
+  remote: https://rubygems.org/
+  specs:
+    activesupport (4.1.6)
+      i18n (~> 0.6, >= 0.6.9)
+      json (~> 1.7, >= 1.7.7)
+      minitest (~> 5.1)
+      thread_safe (~> 0.1)
+      tzinfo (~> 1.1)
+    addressable (2.3.6)
+    atomic (1.1.16-java)
+    avl_tree (1.1.3)
+    awesome_print (1.2.0)
+    aws-sdk (1.54.0)
+      aws-sdk-v1 (= 1.54.0)
+    aws-sdk-v1 (1.54.0)
+      json (~> 1.4)
+      nokogiri (>= 1.4.4)
+    axiom-types (0.1.1)
+      descendants_tracker (~> 0.0.4)
+      ice_nine (~> 0.11.0)
+      thread_safe (~> 0.3, >= 0.3.1)
+    backports (3.6.1)
+    beefcake (0.3.7)
+    bindata (2.1.0)
+    buftok (0.1)
+    builder (3.2.2)
+    cabin (0.6.1)
+    ci_reporter (1.9.3)
+      builder (>= 2.1.2)
+    cinch (2.1.0)
+    clamp (0.6.3)
+    coderay (1.1.0)
+    coercible (1.0.0)
+      descendants_tracker (~> 0.0.1)
+    coveralls (0.7.1)
+      multi_json (~> 1.3)
+      rest-client
+      simplecov (>= 0.7)
+      term-ansicolor
+      thor
+    descendants_tracker (0.0.4)
+      thread_safe (~> 0.3, >= 0.3.1)
+    diff-lcs (1.2.5)
+    docile (1.1.5)
+    edn (1.0.6)
+    elasticsearch (1.0.5)
+      elasticsearch-api (= 1.0.5)
+      elasticsearch-transport (= 1.0.5)
+    elasticsearch-api (1.0.5)
+      multi_json
+    elasticsearch-transport (1.0.5)
+      faraday
+      multi_json
+    equalizer (0.0.9)
+    extlib (0.9.16)
+    faraday (0.9.0)
+      multipart-post (>= 1.2, < 3)
+    ffi (1.9.5-java)
+    ffi-rzmq (1.0.0)
+      ffi
+    filewatch (0.5.1)
+    ftw (0.0.40)
+      addressable
+      backports (>= 2.6.2)
+      cabin (> 0)
+      http_parser.rb (= 0.5.3)
+    gelf (1.3.2)
+      json
+    gelfd (0.2.0)
+    geoip (1.4.0)
+    gmetric (0.1.3)
+    hitimes (1.2.2-java)
+    http (0.5.1)
+      http_parser.rb
+    http_parser.rb (0.5.3-java)
+    i18n (0.6.9)
+    ice_nine (0.11.0)
+    insist (1.0.0)
+    jbundler (0.5.5)
+      bundler (~> 1.5)
+      ruby-maven (>= 3.1.1.0.1, < 3.1.2)
+    jls-grok (0.11.0)
+      cabin (>= 0.6.0)
+    jls-lumberjack (0.0.20)
+    jrjackson (0.2.7)
+    jruby-httpclient (1.1.1-java)
+    jruby-kafka (0.2.1-java)
+      jbundler (= 0.5.5)
+    json (1.8.1-java)
+    kramdown (1.4.2)
+    mail (2.5.3)
+      i18n (>= 0.4.0)
+      mime-types (~> 1.16)
+      treetop (~> 1.4.8)
+    march_hare (2.5.1-java)
+    maven-tools (1.0.5)
+      virtus (~> 1.0)
+    metaclass (0.0.4)
+    method_source (0.8.2)
+    metriks (0.9.9.6)
+      atomic (~> 1.0)
+      avl_tree (~> 1.1.2)
+      hitimes (~> 1.1)
+    mime-types (1.25.1)
+    minitest (5.4.2)
+    mocha (1.1.0)
+      metaclass (~> 0.0.1)
+    msgpack-jruby (1.4.0-java)
+    multi_json (1.10.1)
+    multipart-post (2.0.0)
+    murmurhash3 (0.1.4)
+    netrc (0.7.7)
+    nokogiri (1.6.3.1-java)
+    polyglot (0.3.5)
+    pry (0.10.1-java)
+      coderay (~> 1.1.0)
+      method_source (~> 0.8.1)
+      slop (~> 3.4)
+      spoon (~> 0.0)
+    rack (1.5.2)
+    rack-protection (1.5.3)
+      rack
+    rbnacl (3.1.2)
+      ffi
+    redis (3.1.0)
+    rest-client (1.7.2)
+      mime-types (>= 1.16, < 3.0)
+      netrc (~> 0.7)
+    rspec (2.14.1)
+      rspec-core (~> 2.14.0)
+      rspec-expectations (~> 2.14.0)
+      rspec-mocks (~> 2.14.0)
+    rspec-core (2.14.8)
+    rspec-expectations (2.14.5)
+      diff-lcs (>= 1.1.3, < 2.0)
+    rspec-mocks (2.14.6)
+    ruby-maven (3.1.1.0.8)
+      maven-tools (~> 1.0.1)
+      ruby-maven-libs (= 3.1.1)
+    ruby-maven-libs (3.1.1)
+    rufus-scheduler (2.0.24)
+      tzinfo (>= 0.3.22)
+    rumbster (1.1.1)
+      mail (= 2.5.3)
+    shoulda (3.5.0)
+      shoulda-context (~> 1.0, >= 1.0.1)
+      shoulda-matchers (>= 1.4.1, < 3.0)
+    shoulda-context (1.2.1)
+    shoulda-matchers (2.7.0)
+      activesupport (>= 3.0.0)
+    simple_oauth (0.2.0)
+    simplecov (0.9.1)
+      docile (~> 1.1.0)
+      multi_json (~> 1.0)
+      simplecov-html (~> 0.8.0)
+    simplecov-html (0.8.0)
+    sinatra (1.4.5)
+      rack (~> 1.4)
+      rack-protection (~> 1.4)
+      tilt (~> 1.3, >= 1.3.4)
+    slop (3.6.0)
+    snmp (1.2.0)
+    spoon (0.0.4)
+      ffi
+    statsd-ruby (1.2.0)
+    stud (0.0.17)
+      ffi
+      metriks
+    term-ansicolor (1.3.0)
+      tins (~> 1.0)
+    thor (0.19.1)
+    thread_safe (0.3.4-java)
+    tilt (1.4.1)
+    tins (1.3.3)
+    treetop (1.4.15)
+      polyglot
+      polyglot
+      polyglot
+      polyglot
+      polyglot
+      polyglot
+      polyglot
+      polyglot
+      polyglot
+      polyglot
+      polyglot
+      polyglot
+      polyglot
+      polyglot
+      polyglot
+      polyglot
+      polyglot
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+      polyglot (>= 0.3.1)
+    twitter (5.0.0.rc.1)
+      buftok (~> 0.1.0)
+      faraday (>= 0.8, < 0.10)
+      http (>= 0.5.0.pre2, < 0.6)
+      http_parser.rb (~> 0.5.0)
+      json (~> 1.8)
+      simple_oauth (~> 0.2.0)
+    tzinfo (1.2.2)
+      thread_safe (~> 0.1)
+    user_agent_parser (2.1.5)
+    virtus (1.0.3)
+      axiom-types (~> 0.1)
+      coercible (~> 1.0)
+      descendants_tracker (~> 0.0, >= 0.0.3)
+      equalizer (~> 0.0, >= 0.0.9)
+    xml-simple (1.1.4)
+    xmpp4r (0.5)
+
+PLATFORMS
+  java
+
+DEPENDENCIES
+  coveralls
+  kramdown
+  logstash!
diff --git a/tools/Gemfile.rbx-2.1.lock b/tools/Gemfile.rbx-2.1.lock
new file mode 100644
index 00000000000..a2543530874
--- /dev/null
+++ b/tools/Gemfile.rbx-2.1.lock
@@ -0,0 +1,420 @@
+GEM
+  remote: https://rubygems.org/
+  specs:
+    activesupport (3.2.17)
+      i18n (~> 0.6, >= 0.6.4)
+      multi_json (~> 1.0)
+    addressable (2.3.5)
+    amq-protocol (1.9.2)
+    atomic (1.1.15)
+    avl_tree (1.1.3)
+    awesome_print (1.2.0)
+    aws-sdk (1.35.0)
+      json (~> 1.4)
+      nokogiri (>= 1.4.4)
+      uuidtools (~> 2.1)
+    backports (3.6.0)
+    beefcake (0.3.7)
+    bindata (2.0.0)
+    blankslate (2.1.2.4)
+    buftok (0.1)
+    builder (3.2.2)
+    bunny (1.1.3)
+      amq-protocol (>= 1.9.2)
+    cabin (0.6.1)
+    ci_reporter (1.9.1)
+      builder (>= 2.1.2)
+    cinch (2.1.0)
+    clamp (0.6.3)
+    coderay (1.1.0)
+    coveralls (0.7.0)
+      multi_json (~> 1.3)
+      rest-client
+      simplecov (>= 0.7)
+      term-ansicolor
+      thor
+    diff-lcs (1.2.5)
+    docile (1.1.3)
+    edn (1.0.2)
+      parslet (~> 1.4.0)
+    excon (0.32.0)
+    extlib (0.9.16)
+    faraday (0.9.0)
+      multipart-post (>= 1.2, < 3)
+    ffi (1.9.3)
+    ffi-rzmq (1.0.0)
+      ffi
+    ffi2-generators (0.1.1)
+    filewatch (0.5.1)
+    ftw (0.0.39)
+      addressable
+      backports (>= 2.6.2)
+      cabin (> 0)
+      http_parser.rb (= 0.5.3)
+    gelf (1.3.2)
+      json
+    gelfd (0.2.0)
+    geoip (1.3.5)
+    gmetric (0.1.3)
+    haml (4.0.5)
+      tilt
+    hitimes (1.2.1)
+    http (0.5.0)
+      http_parser.rb
+    http_parser.rb (0.5.3)
+    i18n (0.6.9)
+    insist (1.0.0)
+    jls-grok (0.10.12)
+      cabin (>= 0.6.0)
+    jls-lumberjack (0.0.19)
+    json (1.8.1)
+    mail (2.5.3)
+      i18n (>= 0.4.0)
+      mime-types (~> 1.16)
+      treetop (~> 1.4.8)
+    metaclass (0.0.4)
+    method_source (0.8.2)
+    metriks (0.9.9.6)
+      atomic (~> 1.0)
+      avl_tree (~> 1.1.2)
+      hitimes (~> 1.1)
+    mime-types (1.25.1)
+    mini_portile (0.5.2)
+    minitest (5.3.0)
+    mocha (1.0.0)
+      metaclass (~> 0.0.1)
+    msgpack (0.5.8)
+    multi_json (1.8.4)
+    multipart-post (2.0.0)
+    murmurhash3 (0.1.4)
+    nokogiri (1.6.1)
+      mini_portile (~> 0.5.0)
+    parslet (1.4.0)
+      blankslate (~> 2.0)
+    polyglot (0.3.4)
+    pry (0.9.12.6)
+      coderay (~> 1.0)
+      method_source (~> 0.8)
+      slop (~> 3.4)
+    racc (1.4.11)
+    rbnacl (2.0.0)
+      ffi
+    redis (3.0.7)
+    rest-client (1.6.7)
+      mime-types (>= 1.16)
+    rspec (2.14.1)
+      rspec-core (~> 2.14.0)
+      rspec-expectations (~> 2.14.0)
+      rspec-mocks (~> 2.14.0)
+    rspec-core (2.14.7)
+    rspec-expectations (2.14.5)
+      diff-lcs (>= 1.1.3, < 2.0)
+    rspec-mocks (2.14.6)
+    rubysl (2.0.15)
+      rubysl-abbrev (~> 2.0)
+      rubysl-base64 (~> 2.0)
+      rubysl-benchmark (~> 2.0)
+      rubysl-bigdecimal (~> 2.0)
+      rubysl-cgi (~> 2.0)
+      rubysl-cgi-session (~> 2.0)
+      rubysl-cmath (~> 2.0)
+      rubysl-complex (~> 2.0)
+      rubysl-continuation (~> 2.0)
+      rubysl-coverage (~> 2.0)
+      rubysl-csv (~> 2.0)
+      rubysl-curses (~> 2.0)
+      rubysl-date (~> 2.0)
+      rubysl-delegate (~> 2.0)
+      rubysl-digest (~> 2.0)
+      rubysl-drb (~> 2.0)
+      rubysl-e2mmap (~> 2.0)
+      rubysl-english (~> 2.0)
+      rubysl-enumerator (~> 2.0)
+      rubysl-erb (~> 2.0)
+      rubysl-etc (~> 2.0)
+      rubysl-expect (~> 2.0)
+      rubysl-fcntl (~> 2.0)
+      rubysl-fiber (~> 2.0)
+      rubysl-fileutils (~> 2.0)
+      rubysl-find (~> 2.0)
+      rubysl-forwardable (~> 2.0)
+      rubysl-getoptlong (~> 2.0)
+      rubysl-gserver (~> 2.0)
+      rubysl-io-console (~> 2.0)
+      rubysl-io-nonblock (~> 2.0)
+      rubysl-io-wait (~> 2.0)
+      rubysl-ipaddr (~> 2.0)
+      rubysl-irb (~> 2.0)
+      rubysl-logger (~> 2.0)
+      rubysl-mathn (~> 2.0)
+      rubysl-matrix (~> 2.0)
+      rubysl-mkmf (~> 2.0)
+      rubysl-monitor (~> 2.0)
+      rubysl-mutex_m (~> 2.0)
+      rubysl-net-ftp (~> 2.0)
+      rubysl-net-http (~> 2.0)
+      rubysl-net-imap (~> 2.0)
+      rubysl-net-pop (~> 2.0)
+      rubysl-net-protocol (~> 2.0)
+      rubysl-net-smtp (~> 2.0)
+      rubysl-net-telnet (~> 2.0)
+      rubysl-nkf (~> 2.0)
+      rubysl-observer (~> 2.0)
+      rubysl-open-uri (~> 2.0)
+      rubysl-open3 (~> 2.0)
+      rubysl-openssl (~> 2.0)
+      rubysl-optparse (~> 2.0)
+      rubysl-ostruct (~> 2.0)
+      rubysl-pathname (~> 2.0)
+      rubysl-prettyprint (~> 2.0)
+      rubysl-prime (~> 2.0)
+      rubysl-profile (~> 2.0)
+      rubysl-profiler (~> 2.0)
+      rubysl-pstore (~> 2.0)
+      rubysl-pty (~> 2.0)
+      rubysl-rational (~> 2.0)
+      rubysl-readline (~> 2.0)
+      rubysl-resolv (~> 2.0)
+      rubysl-rexml (~> 2.0)
+      rubysl-rinda (~> 2.0)
+      rubysl-rss (~> 2.0)
+      rubysl-scanf (~> 2.0)
+      rubysl-securerandom (~> 2.0)
+      rubysl-set (~> 2.0)
+      rubysl-shellwords (~> 2.0)
+      rubysl-singleton (~> 2.0)
+      rubysl-socket (~> 2.0)
+      rubysl-stringio (~> 2.0)
+      rubysl-strscan (~> 2.0)
+      rubysl-sync (~> 2.0)
+      rubysl-syslog (~> 2.0)
+      rubysl-tempfile (~> 2.0)
+      rubysl-thread (~> 2.0)
+      rubysl-thwait (~> 2.0)
+      rubysl-time (~> 2.0)
+      rubysl-timeout (~> 2.0)
+      rubysl-tmpdir (~> 2.0)
+      rubysl-tsort (~> 2.0)
+      rubysl-un (~> 2.0)
+      rubysl-uri (~> 2.0)
+      rubysl-weakref (~> 2.0)
+      rubysl-webrick (~> 2.0)
+      rubysl-xmlrpc (~> 2.0)
+      rubysl-yaml (~> 2.0)
+      rubysl-zlib (~> 2.0)
+    rubysl-abbrev (2.0.4)
+    rubysl-base64 (2.0.0)
+    rubysl-benchmark (2.0.1)
+    rubysl-bigdecimal (2.0.2)
+    rubysl-cgi (2.0.1)
+    rubysl-cgi-session (2.0.1)
+    rubysl-cmath (2.0.0)
+    rubysl-complex (2.0.0)
+    rubysl-continuation (2.0.0)
+    rubysl-coverage (2.0.3)
+    rubysl-csv (2.0.2)
+      rubysl-english (~> 2.0)
+    rubysl-curses (2.0.1)
+    rubysl-date (2.0.6)
+    rubysl-delegate (2.0.1)
+    rubysl-digest (2.0.3)
+    rubysl-drb (2.0.1)
+    rubysl-e2mmap (2.0.0)
+    rubysl-english (2.0.0)
+    rubysl-enumerator (2.0.0)
+    rubysl-erb (2.0.1)
+    rubysl-etc (2.0.3)
+      ffi2-generators (~> 0.1)
+    rubysl-expect (2.0.0)
+    rubysl-fcntl (2.0.4)
+      ffi2-generators (~> 0.1)
+    rubysl-fiber (2.0.0)
+    rubysl-fileutils (2.0.3)
+    rubysl-find (2.0.1)
+    rubysl-forwardable (2.0.1)
+    rubysl-getoptlong (2.0.0)
+    rubysl-gserver (2.0.0)
+      rubysl-socket (~> 2.0)
+      rubysl-thread (~> 2.0)
+    rubysl-io-console (2.0.0)
+    rubysl-io-nonblock (2.0.0)
+    rubysl-io-wait (2.0.0)
+    rubysl-ipaddr (2.0.0)
+    rubysl-irb (2.0.4)
+      rubysl-e2mmap (~> 2.0)
+      rubysl-mathn (~> 2.0)
+      rubysl-readline (~> 2.0)
+      rubysl-thread (~> 2.0)
+    rubysl-logger (2.0.0)
+    rubysl-mathn (2.0.0)
+    rubysl-matrix (2.1.0)
+      rubysl-e2mmap (~> 2.0)
+    rubysl-mkmf (2.0.1)
+      rubysl-fileutils (~> 2.0)
+      rubysl-shellwords (~> 2.0)
+    rubysl-monitor (2.0.0)
+    rubysl-mutex_m (2.0.0)
+    rubysl-net-ftp (2.0.1)
+    rubysl-net-http (2.0.4)
+      rubysl-cgi (~> 2.0)
+      rubysl-erb (~> 2.0)
+      rubysl-singleton (~> 2.0)
+    rubysl-net-imap (2.0.1)
+    rubysl-net-pop (2.0.1)
+    rubysl-net-protocol (2.0.1)
+    rubysl-net-smtp (2.0.1)
+    rubysl-net-telnet (2.0.0)
+    rubysl-nkf (2.0.1)
+    rubysl-observer (2.0.0)
+    rubysl-open-uri (2.0.0)
+    rubysl-open3 (2.0.0)
+    rubysl-openssl (2.1.0)
+    rubysl-optparse (2.0.1)
+      rubysl-shellwords (~> 2.0)
+    rubysl-ostruct (2.0.4)
+    rubysl-pathname (2.0.0)
+    rubysl-prettyprint (2.0.2)
+    rubysl-prime (2.0.1)
+    rubysl-profile (2.0.0)
+    rubysl-profiler (2.0.1)
+    rubysl-pstore (2.0.0)
+    rubysl-pty (2.0.2)
+    rubysl-rational (2.0.1)
+    rubysl-readline (2.0.2)
+    rubysl-resolv (2.1.0)
+    rubysl-rexml (2.0.2)
+    rubysl-rinda (2.0.1)
+    rubysl-rss (2.0.0)
+    rubysl-scanf (2.0.0)
+    rubysl-securerandom (2.0.0)
+    rubysl-set (2.0.1)
+    rubysl-shellwords (2.0.0)
+    rubysl-singleton (2.0.0)
+    rubysl-socket (2.0.1)
+    rubysl-stringio (2.0.0)
+    rubysl-strscan (2.0.0)
+    rubysl-sync (2.0.0)
+    rubysl-syslog (2.0.1)
+      ffi2-generators (~> 0.1)
+    rubysl-tempfile (2.0.1)
+    rubysl-thread (2.0.2)
+    rubysl-thwait (2.0.0)
+    rubysl-time (2.0.3)
+    rubysl-timeout (2.0.0)
+    rubysl-tmpdir (2.0.0)
+    rubysl-tsort (2.0.1)
+    rubysl-un (2.0.0)
+      rubysl-fileutils (~> 2.0)
+      rubysl-optparse (~> 2.0)
+    rubysl-uri (2.0.0)
+    rubysl-weakref (2.0.0)
+    rubysl-webrick (2.0.0)
+    rubysl-xmlrpc (2.0.0)
+    rubysl-yaml (2.0.4)
+    rubysl-zlib (2.0.1)
+    rufus-scheduler (2.0.24)
+      tzinfo (>= 0.3.22)
+    rumbster (1.1.1)
+      mail (= 2.5.3)
+    sass (3.2.14)
+    shoulda (3.5.0)
+      shoulda-context (~> 1.0, >= 1.0.1)
+      shoulda-matchers (>= 1.4.1, < 3.0)
+    shoulda-context (1.1.6)
+    shoulda-matchers (2.5.0)
+      activesupport (>= 3.0.0)
+    simple_oauth (0.2.0)
+    simplecov (0.8.2)
+      docile (~> 1.1.0)
+      multi_json
+      simplecov-html (~> 0.8.0)
+    simplecov-html (0.8.0)
+    slop (3.4.7)
+    snmp (1.1.1)
+    spoon (0.0.4)
+      ffi
+    statsd-ruby (1.2.0)
+    stud (0.0.17)
+      ffi
+      metriks
+    term-ansicolor (1.3.0)
+      tins (~> 1.0)
+    thor (0.18.1)
+    thread_safe (0.2.0)
+      atomic (>= 1.1.7, < 2)
+    tilt (2.0.0)
+    tins (1.0.0)
+    treetop (1.4.15)
+      polyglot
+      polyglot (>= 0.3.1)
+    twitter (5.0.0.rc.1)
+      buftok (~> 0.1.0)
+      faraday (>= 0.8, < 0.10)
+      http (>= 0.5.0.pre2, < 0.6)
+      http_parser.rb (~> 0.5.0)
+      json (~> 1.8)
+      simple_oauth (~> 0.2.0)
+    tzinfo (1.1.0)
+      thread_safe (~> 0.1)
+    user_agent_parser (2.1.2)
+    uuidtools (2.1.4)
+    xml-simple (1.1.3)
+    xmpp4r (0.5)
+
+PLATFORMS
+  ruby
+
+DEPENDENCIES
+  addressable
+  awesome_print
+  aws-sdk
+  beefcake (= 0.3.7)
+  bindata (>= 1.5.0)
+  bunny (~> 1.1.0)
+  cabin (>= 0.6.0)
+  ci_reporter
+  cinch
+  clamp
+  coveralls
+  edn
+  excon
+  extlib (= 0.9.16)
+  ffi
+  ffi-rzmq (= 1.0.0)
+  filewatch (= 0.5.1)
+  ftw (~> 0.0.39)
+  gelf (= 1.3.2)
+  gelfd (= 0.2.0)
+  geoip (>= 1.3.2)
+  gmetric (= 0.1.3)
+  haml
+  i18n (>= 0.6.6)
+  insist (= 1.0.0)
+  jls-grok (= 0.10.12)
+  jls-lumberjack (>= 0.0.19)
+  json
+  mail
+  metriks
+  mime-types
+  minitest
+  mocha
+  msgpack
+  murmurhash3
+  pry
+  racc
+  rbnacl
+  redis
+  rspec
+  rubysl
+  rufus-scheduler (~> 2.0.24)
+  rumbster
+  sass
+  shoulda
+  snmp
+  spoon
+  statsd-ruby (= 1.2.0)
+  stud
+  twitter (= 5.0.0.rc.1)
+  user_agent_parser (>= 2.0.0)
+  xml-simple
+  xmpp4r (= 0.5)
diff --git a/tools/Gemfile.ruby-1.9.1.lock b/tools/Gemfile.ruby-1.9.1.lock
new file mode 100644
index 00000000000..56999a0b9b4
--- /dev/null
+++ b/tools/Gemfile.ruby-1.9.1.lock
@@ -0,0 +1,227 @@
+GEM
+  remote: https://rubygems.org/
+  specs:
+    activesupport (3.2.17)
+      i18n (~> 0.6, >= 0.6.4)
+      multi_json (~> 1.0)
+    addressable (2.3.5)
+    amq-protocol (1.9.2)
+    atomic (1.1.15)
+    avl_tree (1.1.3)
+    awesome_print (1.2.0)
+    aws-sdk (1.35.0)
+      json (~> 1.4)
+      nokogiri (>= 1.4.4)
+      uuidtools (~> 2.1)
+    backports (3.6.0)
+    beefcake (0.3.7)
+    bindata (2.0.0)
+    blankslate (2.1.2.4)
+    buftok (0.1)
+    builder (3.2.2)
+    bunny (1.1.3)
+      amq-protocol (>= 1.9.2)
+    cabin (0.6.1)
+    ci_reporter (1.9.1)
+      builder (>= 2.1.2)
+    cinch (2.1.0)
+    clamp (0.6.3)
+    coderay (1.1.0)
+    coveralls (0.7.0)
+      multi_json (~> 1.3)
+      rest-client
+      simplecov (>= 0.7)
+      term-ansicolor
+      thor
+    diff-lcs (1.2.5)
+    docile (1.1.3)
+    edn (1.0.2)
+      parslet (~> 1.4.0)
+    elasticsearch (1.0.1)
+      elasticsearch-api (= 1.0.1)
+      elasticsearch-transport (= 1.0.1)
+    elasticsearch-api (1.0.1)
+      multi_json
+    elasticsearch-transport (1.0.1)
+      faraday
+      multi_json
+    excon (0.32.0)
+    extlib (0.9.16)
+    faraday (0.9.0)
+      multipart-post (>= 1.2, < 3)
+    ffi (1.9.3)
+    ffi-rzmq (1.0.0)
+      ffi
+    filewatch (0.5.1)
+    ftw (0.0.39)
+      addressable
+      backports (>= 2.6.2)
+      cabin (> 0)
+      http_parser.rb (= 0.5.3)
+    gelf (1.3.2)
+      json
+    gelfd (0.2.0)
+    geoip (1.3.5)
+    gmetric (0.1.3)
+    hitimes (1.2.1)
+    http (0.5.0)
+      http_parser.rb
+    http_parser.rb (0.5.3)
+    i18n (0.6.9)
+    insist (1.0.0)
+    jls-grok (0.10.12)
+      cabin (>= 0.6.0)
+    jls-lumberjack (0.0.20)
+    json (1.8.1)
+    mail (2.5.3)
+      i18n (>= 0.4.0)
+      mime-types (~> 1.16)
+      treetop (~> 1.4.8)
+    metaclass (0.0.4)
+    method_source (0.8.2)
+    metriks (0.9.9.6)
+      atomic (~> 1.0)
+      avl_tree (~> 1.1.2)
+      hitimes (~> 1.1)
+    mime-types (1.25.1)
+    mini_portile (0.5.2)
+    minitest (5.3.0)
+    mocha (1.0.0)
+      metaclass (~> 0.0.1)
+    msgpack (0.5.8)
+    multi_json (1.8.4)
+    multipart-post (2.0.0)
+    murmurhash3 (0.1.4)
+    nokogiri (1.6.1)
+      mini_portile (~> 0.5.0)
+    parslet (1.4.0)
+      blankslate (~> 2.0)
+    polyglot (0.3.4)
+    pry (0.9.12.6)
+      coderay (~> 1.0)
+      method_source (~> 0.8)
+      slop (~> 3.4)
+    rack (1.5.2)
+    rack-protection (1.5.2)
+      rack
+    rbnacl (2.0.0)
+      ffi
+    redis (3.0.7)
+    rest-client (1.6.7)
+      mime-types (>= 1.16)
+    rspec (2.14.1)
+      rspec-core (~> 2.14.0)
+      rspec-expectations (~> 2.14.0)
+      rspec-mocks (~> 2.14.0)
+    rspec-core (2.14.7)
+    rspec-expectations (2.14.5)
+      diff-lcs (>= 1.1.3, < 2.0)
+    rspec-mocks (2.14.6)
+    rufus-scheduler (2.0.24)
+      tzinfo (>= 0.3.22)
+    rumbster (1.1.1)
+      mail (= 2.5.3)
+    shoulda (3.5.0)
+      shoulda-context (~> 1.0, >= 1.0.1)
+      shoulda-matchers (>= 1.4.1, < 3.0)
+    shoulda-context (1.1.6)
+    shoulda-matchers (2.5.0)
+      activesupport (>= 3.0.0)
+    simple_oauth (0.2.0)
+    simplecov (0.8.2)
+      docile (~> 1.1.0)
+      multi_json
+      simplecov-html (~> 0.8.0)
+    simplecov-html (0.8.0)
+    sinatra (1.4.4)
+      rack (~> 1.4)
+      rack-protection (~> 1.4)
+      tilt (~> 1.3, >= 1.3.4)
+    slop (3.4.7)
+    snmp (1.1.1)
+    spoon (0.0.4)
+      ffi
+    statsd-ruby (1.2.0)
+    stud (0.0.17)
+      ffi
+      metriks
+    term-ansicolor (1.3.0)
+      tins (~> 1.0)
+    thor (0.18.1)
+    thread_safe (0.2.0)
+      atomic (>= 1.1.7, < 2)
+    tilt (1.4.1)
+    tins (1.0.0)
+    treetop (1.4.15)
+      polyglot
+      polyglot (>= 0.3.1)
+    twitter (5.0.0.rc.1)
+      buftok (~> 0.1.0)
+      faraday (>= 0.8, < 0.10)
+      http (>= 0.5.0.pre2, < 0.6)
+      http_parser.rb (~> 0.5.0)
+      json (~> 1.8)
+      simple_oauth (~> 0.2.0)
+    tzinfo (1.1.0)
+      thread_safe (~> 0.1)
+    user_agent_parser (2.1.2)
+    uuidtools (2.1.4)
+    xml-simple (1.1.3)
+    xmpp4r (0.5)
+
+PLATFORMS
+  ruby
+
+DEPENDENCIES
+  addressable
+  awesome_print
+  aws-sdk
+  beefcake (= 0.3.7)
+  bindata (>= 1.5.0)
+  bunny (~> 1.1.0)
+  cabin (>= 0.6.0)
+  ci_reporter
+  cinch
+  clamp
+  coveralls
+  edn
+  elasticsearch
+  excon
+  extlib (= 0.9.16)
+  ffi
+  ffi-rzmq (= 1.0.0)
+  filewatch (= 0.5.1)
+  ftw (~> 0.0.39)
+  gelf (= 1.3.2)
+  gelfd (= 0.2.0)
+  geoip (>= 1.3.2)
+  gmetric (= 0.1.3)
+  i18n (>= 0.6.6)
+  insist (= 1.0.0)
+  jls-grok (= 0.10.12)
+  jls-lumberjack (>= 0.0.20)
+  json
+  mail
+  metriks
+  mime-types
+  minitest
+  mocha
+  msgpack
+  murmurhash3
+  pry
+  rack
+  rbnacl
+  redis
+  rspec
+  rufus-scheduler (~> 2.0.24)
+  rumbster
+  shoulda
+  sinatra
+  snmp
+  spoon
+  statsd-ruby (= 1.2.0)
+  stud
+  twitter (= 5.0.0.rc.1)
+  user_agent_parser (>= 2.0.0)
+  xml-simple
+  xmpp4r (= 0.5)
diff --git a/tools/Gemfile.ruby-2.1.0.lock b/tools/Gemfile.ruby-2.1.0.lock
new file mode 100644
index 00000000000..e5162a84d39
--- /dev/null
+++ b/tools/Gemfile.ruby-2.1.0.lock
@@ -0,0 +1,214 @@
+GEM
+  remote: https://rubygems.org/
+  specs:
+    activesupport (3.2.17)
+      i18n (~> 0.6, >= 0.6.4)
+      multi_json (~> 1.0)
+    addressable (2.3.5)
+    amq-protocol (1.9.2)
+    atomic (1.1.15)
+    avl_tree (1.1.3)
+    awesome_print (1.2.0)
+    aws-sdk (1.35.0)
+      json (~> 1.4)
+      nokogiri (>= 1.4.4)
+      uuidtools (~> 2.1)
+    backports (3.6.0)
+    beefcake (0.3.7)
+    bindata (2.0.0)
+    blankslate (2.1.2.4)
+    buftok (0.1)
+    builder (3.2.2)
+    bunny (1.1.3)
+      amq-protocol (>= 1.9.2)
+    cabin (0.6.1)
+    ci_reporter (1.9.1)
+      builder (>= 2.1.2)
+    cinch (2.1.0)
+    clamp (0.6.3)
+    coderay (1.1.0)
+    coveralls (0.7.0)
+      multi_json (~> 1.3)
+      rest-client
+      simplecov (>= 0.7)
+      term-ansicolor
+      thor
+    diff-lcs (1.2.5)
+    docile (1.1.3)
+    edn (1.0.2)
+      parslet (~> 1.4.0)
+    excon (0.32.0)
+    extlib (0.9.16)
+    faraday (0.9.0)
+      multipart-post (>= 1.2, < 3)
+    ffi (1.9.3)
+    ffi-rzmq (1.0.0)
+      ffi
+    filewatch (0.5.1)
+    ftw (0.0.39)
+      addressable
+      backports (>= 2.6.2)
+      cabin (> 0)
+      http_parser.rb (= 0.5.3)
+    gelf (1.3.2)
+      json
+    gelfd (0.2.0)
+    geoip (1.3.5)
+    gmetric (0.1.3)
+    haml (4.0.5)
+      tilt
+    hitimes (1.2.1)
+    http (0.5.0)
+      http_parser.rb
+    http_parser.rb (0.5.3)
+    i18n (0.6.9)
+    insist (1.0.0)
+    jls-grok (0.10.12)
+      cabin (>= 0.6.0)
+    jls-lumberjack (0.0.19)
+    json (1.8.1)
+    mail (2.5.3)
+      i18n (>= 0.4.0)
+      mime-types (~> 1.16)
+      treetop (~> 1.4.8)
+    metaclass (0.0.4)
+    method_source (0.8.2)
+    metriks (0.9.9.6)
+      atomic (~> 1.0)
+      avl_tree (~> 1.1.2)
+      hitimes (~> 1.1)
+    mime-types (1.25.1)
+    mini_portile (0.5.2)
+    minitest (5.3.0)
+    mocha (1.0.0)
+      metaclass (~> 0.0.1)
+    msgpack (0.5.8)
+    multi_json (1.8.4)
+    multipart-post (2.0.0)
+    murmurhash3 (0.1.4)
+    nokogiri (1.6.1)
+      mini_portile (~> 0.5.0)
+    parslet (1.4.0)
+      blankslate (~> 2.0)
+    polyglot (0.3.4)
+    pry (0.9.12.6)
+      coderay (~> 1.0)
+      method_source (~> 0.8)
+      slop (~> 3.4)
+    rbnacl (2.0.0)
+      ffi
+    redis (3.0.7)
+    rest-client (1.6.7)
+      mime-types (>= 1.16)
+    rspec (2.14.1)
+      rspec-core (~> 2.14.0)
+      rspec-expectations (~> 2.14.0)
+      rspec-mocks (~> 2.14.0)
+    rspec-core (2.14.7)
+    rspec-expectations (2.14.5)
+      diff-lcs (>= 1.1.3, < 2.0)
+    rspec-mocks (2.14.6)
+    rufus-scheduler (2.0.24)
+      tzinfo (>= 0.3.22)
+    rumbster (1.1.1)
+      mail (= 2.5.3)
+    sass (3.2.14)
+    shoulda (3.5.0)
+      shoulda-context (~> 1.0, >= 1.0.1)
+      shoulda-matchers (>= 1.4.1, < 3.0)
+    shoulda-context (1.1.6)
+    shoulda-matchers (2.5.0)
+      activesupport (>= 3.0.0)
+    simple_oauth (0.2.0)
+    simplecov (0.8.2)
+      docile (~> 1.1.0)
+      multi_json
+      simplecov-html (~> 0.8.0)
+    simplecov-html (0.8.0)
+    slop (3.4.7)
+    snmp (1.1.1)
+    spoon (0.0.4)
+      ffi
+    statsd-ruby (1.2.0)
+    stud (0.0.17)
+      ffi
+      metriks
+    term-ansicolor (1.3.0)
+      tins (~> 1.0)
+    thor (0.18.1)
+    thread_safe (0.2.0)
+      atomic (>= 1.1.7, < 2)
+    tilt (2.0.0)
+    tins (1.0.0)
+    treetop (1.4.15)
+      polyglot
+      polyglot (>= 0.3.1)
+    twitter (5.0.0.rc.1)
+      buftok (~> 0.1.0)
+      faraday (>= 0.8, < 0.10)
+      http (>= 0.5.0.pre2, < 0.6)
+      http_parser.rb (~> 0.5.0)
+      json (~> 1.8)
+      simple_oauth (~> 0.2.0)
+    tzinfo (1.1.0)
+      thread_safe (~> 0.1)
+    user_agent_parser (2.1.2)
+    uuidtools (2.1.4)
+    xml-simple (1.1.3)
+    xmpp4r (0.5)
+
+PLATFORMS
+  ruby
+
+DEPENDENCIES
+  addressable
+  awesome_print
+  aws-sdk
+  beefcake (= 0.3.7)
+  bindata (>= 1.5.0)
+  bunny (~> 1.1.0)
+  cabin (>= 0.6.0)
+  ci_reporter
+  cinch
+  clamp
+  coveralls
+  edn
+  excon
+  extlib (= 0.9.16)
+  ffi
+  ffi-rzmq (= 1.0.0)
+  filewatch (= 0.5.1)
+  ftw (~> 0.0.39)
+  gelf (= 1.3.2)
+  gelfd (= 0.2.0)
+  geoip (>= 1.3.2)
+  gmetric (= 0.1.3)
+  haml
+  i18n (>= 0.6.6)
+  insist (= 1.0.0)
+  jls-grok (= 0.10.12)
+  jls-lumberjack (>= 0.0.19)
+  json
+  mail
+  metriks
+  mime-types
+  minitest
+  mocha
+  msgpack
+  murmurhash3
+  pry
+  rbnacl
+  redis
+  rspec
+  rufus-scheduler (~> 2.0.24)
+  rumbster
+  sass
+  shoulda
+  snmp
+  spoon
+  statsd-ruby (= 1.2.0)
+  stud
+  twitter (= 5.0.0.rc.1)
+  user_agent_parser (>= 2.0.0)
+  xml-simple
+  xmpp4r (= 0.5)
diff --git a/tools/release.sh b/tools/release.sh
new file mode 100644
index 00000000000..435196a95cb
--- /dev/null
+++ b/tools/release.sh
@@ -0,0 +1,67 @@
+#!/bin/bash
+
+logstash=$PWD
+contrib=$PWD/../logstash-contrib/
+
+workdir="$PWD/build/release/"
+mkdir -p $workdir
+
+# circuit breaker to fail if there's something silly wrong.
+if [ -z "$workdir" ] ; then
+  echo "workdir is empty?!"
+  exit 1
+fi
+
+if [ ! -d "$contrib" ] ; then
+  echo "Missing: $contrib"
+  echo "Maybe git clone it?"
+  exit 1
+fi
+
+set -e
+
+prepare() {
+  rsync -a --delete $logstash/{bin,docs,lib,spec,Makefile,gembag.rb,logstash.gemspec,tools,locales,patterns,LICENSE,README.md} $contrib/{lib,spec} $workdir
+  rm -f $logstash/.VERSION.mk
+  make -C $logstash .VERSION.mk
+  make -C $logstash tarball package
+  make -C $contrib tarball package
+  cp $logstash/.VERSION.mk $workdir
+  rm -f $workdir/build/pkg
+  rm -f $workdir/build/*.{zip,rpm,gz,deb} || true
+}
+
+docs() {
+  make -C $workdir build
+  (cd $contrib; find lib/logstash -type f -name '*.rb') > $workdir/build/contrib_plugins
+  make -C $workdir -j 4 docs
+}
+
+tests() {
+  make -C $logstash test QUIET=
+  make -C $logstash tarball test QUIET=
+}
+
+packages() {
+  for path in $logstash $contrib ; do
+    rm -f $path/build/*.tar.gz
+    rm -f $path/build/*.zip
+    echo "Building packages: $path"
+    make -C $path tarball
+    for dir in build pkg . ; do
+      [ ! -d "$path/$dir" ] && continue
+      (cd $path/$dir;
+        for i in *.gz *.rpm *.deb *.zip *.jar ; do
+          [ ! -f "$i" ] && continue
+          echo "Copying $path/$dir/$i"
+          cp $i $workdir/build
+        done
+      )
+    done
+  done
+}
+
+prepare
+tests
+docs
+packages
diff --git a/tools/upload.sh b/tools/upload.sh
new file mode 100644
index 00000000000..72684486c8c
--- /dev/null
+++ b/tools/upload.sh
@@ -0,0 +1,8 @@
+
+basedir=$(dirname $0)/../
+bucket=download.elasticsearch.org
+
+s3cmd put -P $basedir/build/release/build/*.gz s3://${bucket}/logstash/logstash/
+s3cmd put -P $basedir/build/release/build/*.rpm s3://${bucket}/logstash/logstash/packages/centos/
+s3cmd put -P $basedir/build/release/build/*.deb s3://${bucket}/logstash/logstash/packages/debian
+s3cmd put -P $basedir/build/release/build/*.deb s3://${bucket}/logstash/logstash/packages/ubuntu
