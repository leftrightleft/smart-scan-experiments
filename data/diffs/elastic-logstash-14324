diff --git a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java
index 0e27ec93f99..86574a3ae3d 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java
@@ -40,11 +40,14 @@
 
 import java.io.Closeable;
 import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.channels.FileChannel;
 import java.nio.channels.FileLock;
 import java.nio.file.Files;
 import java.nio.file.NoSuchFileException;
 import java.nio.file.Path;
 import java.nio.file.StandardCopyOption;
+import java.nio.file.StandardOpenOption;
 import java.time.Clock;
 import java.time.Duration;
 import java.time.Instant;
@@ -101,6 +104,7 @@ private enum FinalizeWhen {ALWAYS, ONLY_IF_STALE};
     private final AtomicBoolean open = new AtomicBoolean(true);
     private ScheduledExecutorService flushScheduler;
     private final LongAdder droppedEvents = new LongAdder();
+    private final LongAdder expiredEvents = new LongAdder();
     private String lastError = "no errors";
     private final Clock clock;
     private Optional<Timestamp> oldestSegmentTimestamp;
@@ -195,6 +199,10 @@ public long getDroppedEvents() {
         return droppedEvents.longValue();
     }
 
+    public long getExpiredEvents() {
+        return expiredEvents.longValue();
+    }
+
     public String getLastError() {
         return lastError;
     }
@@ -250,32 +258,63 @@ private void innerWriteEntry(DLQEntry entry) throws IOException {
         byte[] record = entry.serialize();
         int eventPayloadSize = RECORD_HEADER_SIZE + record.length;
         executeAgeRetentionPolicy();
-
-        if (currentQueueSize.longValue() + eventPayloadSize > maxQueueSize) {
-            if (storageType == QueueStorageType.DROP_NEWER) {
-                lastError = String.format("Cannot write event to DLQ(path: %s): reached maxQueueSize of %d", queuePath, maxQueueSize);
-                logger.error(lastError);
-                droppedEvents.add(1L);
-                return;
-            } else {
-                do {
-                    dropTailSegment();
-                } while (currentQueueSize.longValue() + eventPayloadSize > maxQueueSize);
-            }
+        boolean skipWrite = executeStoragePolicy(eventPayloadSize);
+        if (skipWrite) {
+            return;
         }
-        if (currentWriter.getPosition() + eventPayloadSize > maxSegmentSize) {
+
+        if (exceedSegmentSize(eventPayloadSize)) {
             finalizeSegment(FinalizeWhen.ALWAYS);
         }
-        currentQueueSize.getAndAdd(currentWriter.writeEvent(record));
+        long writtenBytes = currentWriter.writeEvent(record);
+        currentQueueSize.getAndAdd(writtenBytes);
         lastWrite = Instant.now();
     }
 
-    private void executeAgeRetentionPolicy() throws IOException {
+    private boolean exceedSegmentSize(int eventPayloadSize) throws IOException {
+        return currentWriter.getPosition() + eventPayloadSize > maxSegmentSize;
+    }
+
+    private void executeAgeRetentionPolicy() {
         if (isOldestSegmentExpired()) {
-            deleteExpiredSegments();
+            try {
+                deleteExpiredSegments();
+            } catch (IOException ex) {
+                logger.error("Can't remove some DLQ files while cleaning expired segments", ex);
+            }
+        }
+    }
+
+    /**
+     * @param eventPayloadSize payload size in bytes.
+     * @return boolean true if event write has to be skipped.
+     * */
+    private boolean executeStoragePolicy(int eventPayloadSize) {
+        if (!exceedMaxQueueSize(eventPayloadSize)) {
+            return false;
+        }
+        
+        if (storageType == QueueStorageType.DROP_NEWER) {
+            lastError = String.format("Cannot write event to DLQ(path: %s): reached maxQueueSize of %d", queuePath, maxQueueSize);
+            logger.error(lastError);
+            droppedEvents.add(1L);
+            return true;
+        } else {
+            try {
+                do {
+                    dropTailSegment();
+                } while (exceedMaxQueueSize(eventPayloadSize));
+            } catch (IOException ex) {
+                logger.error("Can't remove some DLQ files while removing older segments", ex);
+            }
+            return false;
         }
     }
 
+    private boolean exceedMaxQueueSize(int eventPayloadSize) {
+        return currentQueueSize.longValue() + eventPayloadSize > maxQueueSize;
+    }
+
     private boolean isOldestSegmentExpired() {
         if (retentionTime == null) {
             return false;
@@ -292,7 +331,7 @@ private void deleteExpiredSegments() throws IOException {
         do {
             if (oldestSegmentPath.isPresent()) {
                 Path beheadedSegment = oldestSegmentPath.get();
-                deleteTailSegment(beheadedSegment, "age retention policy");
+                expiredEvents.add(deleteTailSegment(beheadedSegment, "age retention policy"));
             }
             updateOldestSegmentReference();
             cleanNextSegment = isOldestSegmentExpired();
@@ -301,13 +340,77 @@ private void deleteExpiredSegments() throws IOException {
         this.currentQueueSize.set(computeQueueSize());
     }
 
-    private void deleteTailSegment(Path segment, String motivation) throws IOException {
+    /**
+     * Deletes the segment path, if present. Also return the number of events it contains.
+     *
+     * @param segment
+     *      The segment file to delete.
+     * @param motivation
+     *      Description of delete motivation.
+     * @return the number of events contained in the segment or 0 if the segment was already removed.
+     * @throws IOException if any other IO related error happens during deletion of the segment.
+     * */
+    private long deleteTailSegment(Path segment, String motivation) throws IOException {
         try {
+            long eventsInSegment = countEventsInSegment(segment);
             Files.delete(segment);
             logger.debug("Removed segment file {} due to {}", motivation, segment);
+            return eventsInSegment;
         } catch (NoSuchFileException nsfex) {
             // the last segment was deleted by another process, maybe the reader that's cleaning consumed segments
             logger.debug("File not found {}, maybe removed by the reader pipeline", segment);
+            return 0;
+        }
+    }
+
+    /**
+     * Count the number of 'c' and 's' records in segment.
+     * An event can't be bigger than the segments so in case of records split across multiple event blocks,
+     * the segment has to contain both the start 's' record, all the middle 'm' up to the end 'e' records.
+     * */
+    @SuppressWarnings("fallthrough")
+    private long countEventsInSegment(Path segment) throws IOException {
+        try (FileChannel channel = FileChannel.open(segment, StandardOpenOption.READ)) {
+            // verify minimal segment size
+            if (channel.size() < VERSION_SIZE + RECORD_HEADER_SIZE) {
+                return 0L;
+            }
+
+            // skip the DLQ version byte
+            channel.position(1);
+            int posInBlock = 0;
+            int currentBlockIdx = 0;
+            long countedEvents = 0;
+            do {
+                ByteBuffer headerBuffer = ByteBuffer.allocate(RECORD_HEADER_SIZE);
+                long startPosition = channel.position();
+                // if record header can't be fully contained in the block, align to the next
+                if (posInBlock + RECORD_HEADER_SIZE + 1 > BLOCK_SIZE) {
+                    channel.position((++currentBlockIdx) * BLOCK_SIZE + VERSION_SIZE);
+                    posInBlock = 0;
+                }
+
+                channel.read(headerBuffer);
+                headerBuffer.flip();
+                RecordHeader recordHeader = RecordHeader.get(headerBuffer);
+                if (recordHeader == null) {
+                    // continue with next record, skipping this
+                    logger.error("Can't decode record header, position {} current post {} current events count {}", startPosition, channel.position(), countedEvents);
+                } else {
+                    switch (recordHeader.getType()) {
+                        case START:
+                        case COMPLETE:
+                            countedEvents++;
+                        case MIDDLE:
+                        case END: {
+                            channel.position(channel.position() + recordHeader.getSize());
+                            posInBlock += RECORD_HEADER_SIZE + recordHeader.getSize();
+                        }
+                    }
+                }
+            } while (channel.position() < channel.size());
+
+            return countedEvents;
         }
     }
 
diff --git a/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java b/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
index 14fb03aa588..ece0df7b7c2 100644
--- a/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
+++ b/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
@@ -122,6 +122,9 @@ public class AbstractPipelineExt extends RubyBasicObject {
     private static final RubySymbol DROPPED_EVENTS =
             RubyUtil.RUBY.newSymbol("dropped_events");
 
+    private static final RubySymbol EXPIRED_EVENTS =
+            RubyUtil.RUBY.newSymbol("expired_events");
+
     private static final RubySymbol LAST_ERROR =
             RubyUtil.RUBY.newSymbol("last_error");
 
@@ -385,6 +388,10 @@ public final IRubyObject collectDlqStats(final ThreadContext context) {
                     context, LAST_ERROR,
                     dlqWriter(context).callMethod(context, "get_last_error")
             );
+            getDlqMetric(context).gauge(
+                    context, EXPIRED_EVENTS,
+                    dlqWriter(context).callMethod(context, "get_expired_events")
+            );
         }
         return context.nil;
     }
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterAgeRetentionTest.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterAgeRetentionTest.java
index c03ceba5719..d5306d71b3c 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterAgeRetentionTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterAgeRetentionTest.java
@@ -18,16 +18,18 @@
 
 import static org.hamcrest.MatcherAssert.assertThat;
 import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.lessThan;
 import static org.junit.Assert.assertEquals;
 import static org.logstash.common.io.DeadLetterQueueTestUtils.FULL_SEGMENT_FILE_SIZE;
 import static org.logstash.common.io.DeadLetterQueueTestUtils.GB;
 import static org.logstash.common.io.DeadLetterQueueTestUtils.MB;
-import static org.logstash.common.io.RecordIOWriter.BLOCK_SIZE;
-import static org.logstash.common.io.RecordIOWriter.VERSION_SIZE;
+import static org.logstash.common.io.RecordIOWriter.*;
 
 public class DeadLetterQueueWriterAgeRetentionTest {
 
+    // 319 events of 32Kb generates 10 Mb of data
+    private static final int EVENTS_TO_FILL_A_SEGMENT = 319;
+    private ForwardableClock fakeClock;
+
     static class ForwardableClock extends Clock {
 
         private Clock currentClock;
@@ -64,6 +66,8 @@ public Instant instant() {
     @Before
     public void setUp() throws Exception {
         dir = temporaryFolder.newFolder().toPath();
+        final Clock pointInTimeFixedClock = Clock.fixed(Instant.parse("2022-02-22T10:20:30.00Z"), ZoneId.of("Europe/Rome"));
+        fakeClock = new ForwardableClock(pointInTimeFixedClock);
     }
 
     @Test
@@ -79,7 +83,6 @@ public void testRemovesOlderSegmentsWhenWriteOnReopenedDLQContainingExpiredSegme
         // Exercise
         final long prevQueueSize;
         final long beheadedQueueSize;
-
         try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
                 .newBuilder(dir, 10 * MB, 1 * GB, Duration.ofSeconds(1))
                 .retentionTime(Duration.ofDays(2))
@@ -93,6 +96,7 @@ public void testRemovesOlderSegmentsWhenWriteOnReopenedDLQContainingExpiredSegme
             // when a new write happens in a reopened queue
             writeManager.writeEntry(entry);
             beheadedQueueSize = writeManager.getCurrentQueueSize();
+            assertEquals("No event is expired after reopen of DLQ", 0, writeManager.getExpiredEvents());
         }
 
         // then the age policy must remove the expired segments
@@ -110,10 +114,10 @@ private void prepareDLQWithFirstSegmentOlderThanRetainPeriod(Event event, Forwar
                 .build()) {
 
             // 320 generates 10 Mb of data
-            for (int i = 0; i < 320 - 1; i++) {
+            for (int i = 0; i < EVENTS_TO_FILL_A_SEGMENT; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", i), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(startTime++));
                 final int serializationLength = entry.serialize().length;
-                assertThat("setup: serialized entry size...", serializationLength, Matchers.is(lessThan(BLOCK_SIZE)));
+                assertEquals("setup: serialized entry size...", serializationLength + RECORD_HEADER_SIZE, BLOCK_SIZE);
                 messageSize += serializationLength;
                 writeManager.writeEntry(entry);
             }
@@ -125,8 +129,6 @@ private void prepareDLQWithFirstSegmentOlderThanRetainPeriod(Event event, Forwar
     public void testRemovesOlderSegmentsWhenWritesIntoDLQContainingExpiredSegments() throws IOException {
         final Event event = DeadLetterQueueReaderTest.createEventWithConstantSerializationOverhead(Collections.emptyMap());
         event.setField("message", DeadLetterQueueReaderTest.generateMessageContent(32479));
-        final Clock pointInTimeFixedClock = Clock.fixed(Instant.parse("2022-02-22T10:20:30.00Z"), ZoneId.of("Europe/Rome"));
-        final ForwardableClock fakeClock = new ForwardableClock(pointInTimeFixedClock);
 
         long startTime = fakeClock.instant().toEpochMilli();
         int messageSize = 0;
@@ -138,11 +140,11 @@ public void testRemovesOlderSegmentsWhenWritesIntoDLQContainingExpiredSegments()
                 .clock(fakeClock)
                 .build()) {
 
-            // 320 generates 10 Mb of data
-            for (int i = 0; i < 320 - 1; i++) {
+            // 319 generates 10 Mb of data
+            for (int i = 0; i < EVENTS_TO_FILL_A_SEGMENT; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", i), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(startTime++));
                 final int serializationLength = entry.serialize().length;
-                assertThat("setup: serialized entry size...", serializationLength, Matchers.is(lessThan(BLOCK_SIZE)));
+                assertEquals("setup: serialized entry size...", serializationLength + RECORD_HEADER_SIZE, BLOCK_SIZE);
                 messageSize += serializationLength;
                 writeManager.writeEntry(entry);
             }
@@ -162,6 +164,7 @@ public void testRemovesOlderSegmentsWhenWritesIntoDLQContainingExpiredSegments()
 
             // then the age policy must remove the expired segments
             assertEquals("Write should push off the age expired segments",VERSION_SIZE + BLOCK_SIZE, beheadedQueueSize);
+            assertEquals("The number of events removed should count as expired", EVENTS_TO_FILL_A_SEGMENT, writeManager.getExpiredEvents());
         }
     }
 
@@ -170,9 +173,6 @@ public void testRemoveMultipleOldestSegmentsWhenRetainedAgeIsExceeded() throws I
         final Event event = DeadLetterQueueReaderTest.createEventWithConstantSerializationOverhead(Collections.emptyMap());
         event.setField("message", DeadLetterQueueReaderTest.generateMessageContent(32479));
 
-        final Clock pointInTimeFixedClock = Clock.fixed(Instant.parse("2022-02-22T10:20:30.00Z"), ZoneId.of("Europe/Rome"));
-        final ForwardableClock fakeClock = new ForwardableClock(pointInTimeFixedClock);
-
         long startTime = fakeClock.instant().toEpochMilli();
         int messageSize = 0;
 
@@ -184,11 +184,11 @@ public void testRemoveMultipleOldestSegmentsWhenRetainedAgeIsExceeded() throws I
                 .build()) {
 
             // given DLQ with a couple of segments filled of expired events
-            // 320 generates 10 Mb of data
-            for (int i = 0; i < 319 * 2; i++) {
+            // 319 generates 10 Mb of data
+            for (int i = 0; i < EVENTS_TO_FILL_A_SEGMENT * 2; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", i), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(startTime++));
                 final int serializationLength = entry.serialize().length;
-                assertThat("setup: serialized entry size...", serializationLength, Matchers.is(lessThan(BLOCK_SIZE)));
+                assertEquals("setup: serialized entry size...", serializationLength + RECORD_HEADER_SIZE, BLOCK_SIZE);
                 messageSize += serializationLength;
                 writeManager.writeEntry(entry);
             }
@@ -210,6 +210,7 @@ public void testRemoveMultipleOldestSegmentsWhenRetainedAgeIsExceeded() throws I
 
             // then the age policy must remove the expired segments
             assertEquals("Write should push off the age expired segments",VERSION_SIZE + BLOCK_SIZE, beheadedQueueSize);
+            assertEquals("The number of events removed should count as expired", EVENTS_TO_FILL_A_SEGMENT * 2, writeManager.getExpiredEvents());
         }
     }
 }
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java
index b92600d9484..e55cf219efa 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java
@@ -235,7 +235,7 @@ private void sleep(long millis) throws InterruptedException {
     }
 
     private long dlqLength() throws IOException {
-        try(final Stream<Path> files = Files.list(dir)) {
+        try (final Stream<Path> files = Files.list(dir)) {
             return files.filter(p -> p.toString().endsWith(".log"))
                 .mapToLong(p -> p.toFile().length()).sum();
         }
