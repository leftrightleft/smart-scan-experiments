diff --git a/.gitignore b/.gitignore
index fcc2c1133b7..72e92c59021 100644
--- a/.gitignore
+++ b/.gitignore
@@ -22,3 +22,4 @@ data
 etc/jira-output.conf
 coverage/*
 .VERSION.mk
+.idea/*
diff --git a/CHANGELOG b/CHANGELOG
index f050f172abf..8c55f513fcf 100644
--- a/CHANGELOG
+++ b/CHANGELOG
@@ -1,3 +1,18 @@
+1.4.2 (June 24, 2014)
+  # general
+  - fixed path issues when invoking bin/logstash outside its home directory
+
+  # input
+  - bugfix: generator: fixed stdin option support
+  - bugfix: file: fixed debian 7 path issue
+
+  # codecs
+  - improvement: stdin/tcp: automatically select json_line and line codecs with the tcp and stdin streaming imputs
+  - improvement: collectd: add support for NaN values
+
+  # outputs
+  - improvement: nagios_nsca: fix external command invocation to avoid shell escaping
+
 1.4.1 (May 6, 2014)
   # General
   - bumped Elasticsearch to 1.1.1 and Kibana to 3.0.1
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index 99e6f382f82..0249977e1e6 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -26,7 +26,7 @@ irc.freenode.org and ask for help there!
 
 ## Have an Idea or Feature Request?
 
-* File a ticket on [jira](https://logstash.jira.com/secure/Dashboard.jspa), or email the
+* File a ticket on [github](https://github.com/elasticsearch/logstash/issues), or email the
   [mailing list](http://groups.google.com/group/logstash-users), or email
   me personally (jls@semicomplete.com) if that is more comfortable.
 
@@ -34,14 +34,14 @@ irc.freenode.org and ask for help there!
 
 If you think you found a bug, it probably is a bug.
 
-* File it on [jira](https://logstash.jira.com/secure/Dashboard.jspa)
+* File it on [github](https://github.com/elasticsearch/logstash/issues)
 * or the [mailing list](http://groups.google.com/group/logstash-users).
 
 # Contributing Documentation and Code Changes
 
 If you have a bugfix or new feature that you would like to contribute to
 logstash, and you think it will take more than a few minutes to produce the fix
-(ie; write code), it is worth discussing the change with the logstash users and developers first! You can reach us via [jira](https://logstash.jira.com/secure/Dashboard.jspa), the [mailing list](http://groups.google.com/group/logstash-users), or via IRC (#logstash on freenode irc)
+(ie; write code), it is worth discussing the change with the logstash users and developers first! You can reach us via [github](https://github.com/elasticsearch/logstash/issues), the [mailing list](http://groups.google.com/group/logstash-users), or via IRC (#logstash on freenode irc)
 
 ## Contribution Steps
 
@@ -55,7 +55,7 @@ logstash, and you think it will take more than a few minutes to produce the fix
 3. Send a pull request! Push your changes to your fork of the repository and
    [submit a pull
    request](https://help.github.com/articles/using-pull-requests). In the pull
-   request, describe what your changes do and mention any jira issues related
+   request, describe what your changes do and mention any bugs/issues related
    to the pull request.
 
 
diff --git a/Makefile b/Makefile
index f8b63897d66..6bb7d1dc7e4 100644
--- a/Makefile
+++ b/Makefile
@@ -194,7 +194,7 @@ vendor-gems: | vendor/bundle
 .PHONY: vendor/bundle
 vendor/bundle: | vendor $(JRUBY)
 	@echo "=> Ensuring ruby gems dependencies are in $@..."
-	$(QUIET)USE_JRUBY=1 bin/logstash deps $(QUIET_OUTPUT)
+	$(QUIET)bin/logstash deps $(QUIET_OUTPUT)
 	@# Purge any junk that fattens our jar without need!
 	@# The riak gem includes previous gems in the 'pkg' dir. :(
 	-$(QUIET)rm -rf $@/jruby/1.9/gems/riak-client-1.0.3/pkg
@@ -220,10 +220,10 @@ vendor/ua-parser/regexes.yaml: | vendor/ua-parser/
 .PHONY: test
 test: QUIET_OUTPUT=
 test: | $(JRUBY) vendor-elasticsearch vendor-geoip vendor-collectd vendor-gems
-	$(SPEC_ENV) USE_JRUBY=1 bin/logstash rspec $(SPEC_OPTS) --order rand --fail-fast $(TESTS)
+	$(SPEC_ENV) bin/logstash rspec $(SPEC_OPTS) --order rand --fail-fast $(TESTS)
 
 .PHONY: reporting-test
-reporting-test: SPEC_ENV=JRUBY_OPTS=--debug COVERAGE=TRUE
+reporting-test: SPEC_ENV=JRUBY_OPTS=--debug
 reporting-test: SPEC_OPTS=--format CI::Reporter::RSpec
 reporting-test: | test
 
@@ -355,7 +355,7 @@ show:
 
 .PHONY: prepare-tarball
 prepare-tarball tarball zip: WORKDIR=build/tarball/logstash-$(VERSION)
-prepare-tarball: vendor/kibana $(ELASTICSEARCH) $(JRUBY) $(GEOIP) $(TYPESDB) vendor-gems
+prepare-tarball: vendor/kibana $(ELASTICSEARCH) $(JRUBY) vendor-geoip $(TYPESDB) vendor-gems
 prepare-tarball: vendor/ua-parser/regexes.yaml
 prepare-tarball:
 	@echo "=> Preparing tarball"
@@ -382,4 +382,4 @@ tarball-test: #build/logstash-$(VERSION).tar.gz
 	$(QUIET)-rm -rf build/test-tarball/
 	$(QUIET)mkdir -p build/test-tarball/
 	tar -C build/test-tarball --strip-components 1 -xf build/logstash-$(VERSION).tar.gz
-	(cd build/test-tarball; USE_JRUBY=1 bin/logstash rspec $(TESTS) --fail-fast)
+	(cd build/test-tarball; bin/logstash rspec $(TESTS) --fail-fast)
diff --git a/README.md b/README.md
index 347478df719..4ac1c9b8822 100644
--- a/README.md
+++ b/README.md
@@ -29,16 +29,11 @@ You can also find documentation on the <http://logstash.net> site.
 
 ## Developing
 
-If you don't have JRuby already (or don't use rvm, rbenv, etc), you can have `bin/logstash` fetch it for you by setting `USE_JRUBY`:
-
-    USE_JRUBY=1 bin/logstash ...
-
-Otherwise, here's how to get started with rvm:
-
-    # Install JRuby with rvm
-    rvm install jruby-1.7.11
-    rvm use jruby-1.7.11
+Here's how to get started:
 
+    # Install jruby
+    make vendor-jruby
+    
 Now install dependencies:
 
     # Install logstash ruby dependencies
@@ -57,6 +52,8 @@ Other commands:
     # This will download the elasticsearch jars so Logstash can use them.
     make vendor-elasticsearch
 
+Notes about using other rubies. If you don't use rvm, you can probably skip this paragraph. Logstash works with other rubies, and if you wish to use your own ruby instead of the JRuby the Makefile gives you, you must set `USE_RUBY=1` in your environment.
+
 ## Testing
 
 There are a few ways to run the tests. For development, using `bin/logstash
diff --git a/bin/logstash b/bin/logstash
index b4b28015a41..4dd48808b3d 100755
--- a/bin/logstash
+++ b/bin/logstash
@@ -10,10 +10,6 @@
 #
 # NOTE: One extra command is available 'deps'
 # The 'deps' command will install dependencies for logstash.
-#
-# If you do not have ruby installed, you can set "USE_JRUBY=1"
-# in your environment and this script will download and use
-# a release of JRuby for you.
 
 # Defaults you can override with environment variables
 LS_HEAP_SIZE="${LS_HEAP_SIZE:=500m}"
@@ -32,18 +28,16 @@ case $1 in
   env) env "$@" ;;
   -*)
     if [ -z "$VENDORED_JRUBY" ] ; then
-      exec "${RUBYCMD}" "-I${RUBYLIB}" "${basedir}/lib/logstash/runner.rb" "agent" "$@"
+      exec "${RUBYCMD}" "${basedir}/lib/logstash/runner.rb" "agent" "$@"
     else
-      exec "${JAVACMD}" $JAVA_OPTS "-jar" "$JRUBY_JAR" "-I${RUBYLIB}" "${basedir}/lib/logstash/runner.rb" "agent" "$@"
+      exec "${JAVACMD}" $JAVA_OPTS "-jar" "$JRUBY_JAR" "${basedir}/lib/logstash/runner.rb" "agent" "$@"
     fi
     ;;
   *)
     if [ -z "$VENDORED_JRUBY" ] ; then
-      exec "${RUBYCMD}" "-I${RUBYLIB}" "${basedir}/lib/logstash/runner.rb" "$@"
+      exec "${RUBYCMD}" "${basedir}/lib/logstash/runner.rb" "$@"
     else
-      exec "${JAVACMD}" $JAVA_OPTS "-jar" "$JRUBY_JAR" "-I${RUBYLIB}" "${basedir}/lib/logstash/runner.rb" "$@"
+      exec "${JAVACMD}" $JAVA_OPTS "-jar" "$JRUBY_JAR" "${basedir}/lib/logstash/runner.rb" "$@"
     fi
     ;;
-esac
-
-
+esac
\ No newline at end of file
diff --git a/bin/logstash.lib.sh b/bin/logstash.lib.sh
index fe49c8f6009..bd4b56824e7 100755
--- a/bin/logstash.lib.sh
+++ b/bin/logstash.lib.sh
@@ -1,24 +1,5 @@
 basedir=$(cd `dirname $0`/..; pwd)
 
-setup_ruby() {
-  export RUBYLIB="${basedir}/lib"
-
-  # Verify ruby works
-  if ! ruby -e 'puts "HURRAY"' 2> /dev/null | grep -q "HURRAY" ; then
-    echo "No ruby program found. Cannot start."
-    exit 1
-  fi
-
-  # set $RUBY and $RUBYVER
-  eval $(ruby -rrbconfig -e 'puts "RUBYVER=#{RbConfig::CONFIG["ruby_version"]}"; puts "RUBY=#{RUBY_ENGINE}"')
-
-  RUBYCMD="ruby"
-  VENDORED_JRUBY=
-
-  export GEM_HOME="${basedir}/vendor/bundle/${RUBY}/${RUBYVER}"
-  export GEM_PATH=
-}
-
 setup_java() {
   if [ -z "$JAVACMD" ] ; then
     if [ -n "$JAVA_HOME" ] ; then
@@ -38,11 +19,6 @@ setup_java() {
     exit 1
   fi
 
-  if [ "$(basename $JAVACMD)" = "drip" ] ; then
-    export DRIP_INIT_CLASS="org.jruby.main.DripMain"
-    export DRIP_INIT=
-  fi
-
   JAVA_OPTS="$JAVA_OPTS -Xmx${LS_HEAP_SIZE}"
   JAVA_OPTS="$JAVA_OPTS -XX:+UseParNewGC"
   JAVA_OPTS="$JAVA_OPTS -XX:+UseConcMarkSweepGC"
@@ -65,25 +41,64 @@ setup_java() {
   export JAVA_OPTS
 }
 
-setup_vendored_jruby() {
-  RUBYVER=1.9
-  RUBY=jruby
+setup_drip() {
+  if [ -z $DRIP_JAVACMD ] ; then
+    JAVACMD="drip"
+  fi
 
+  # resolve full path to the drip command.
+  if [ ! -f "$JAVACMD" ] ; then
+    JAVACMD=$(which $JAVACMD 2>/dev/null)
+  fi
+
+  if [ ! -x "$JAVACMD" ] ; then
+    echo "Could not find executable drip binary. Please install drip in your PATH"
+    exit 1
+  fi
+
+  # faster JRuby startup options https://github.com/jruby/jruby/wiki/Improving-startup-time
+  # since we are using drip to speed up, we may as well throw these in also
+  if [ "$USE_RUBY" = "1" ] ; then
+    export JRUBY_OPTS="-J-XX:+TieredCompilation -J-XX:TieredStopAtLevel=1 -J-noverify"
+  else
+    JAVA_OPTS="$JAVA_OPTS -XX:+TieredCompilation -XX:TieredStopAtLevel=1 -noverify"
+  fi
+  export JAVACMD
+  export DRIP_INIT_CLASS="org.jruby.main.DripMain"
+  export DRIP_INIT=""
+}
+
+setup_vendored_jruby() {
   JRUBY_JAR=$(ls "${basedir}"/vendor/jar/jruby-complete-*.jar)
   VENDORED_JRUBY=1
+}
 
-  export RUBYLIB="${basedir}/lib"
-  export GEM_HOME="${basedir}/vendor/bundle/${RUBY}/${RUBYVER}"
-  export GEM_PATH=
+setup_ruby() {
+  RUBYCMD="ruby"
+  VENDORED_JRUBY=
 }
 
 setup() {
-  setup_java
-  if [ -z "$USE_JRUBY" -a \( -d "$basedir/.git" -o ! -z "$USE_RUBY" \) ] ; then
+  # first check if we want to use drip, which can be used in vendored jruby mode
+  # and also when setting USE_RUBY=1 if the ruby interpretor is in fact jruby
+  if [ ! -z "$JAVACMD" ] ; then
+    if [ "$(basename $JAVACMD)" = "drip" ] ; then
+      DRIP_JAVACMD=1
+      USE_DRIP=1
+    fi
+  fi
+  if [ "$USE_DRIP" = "1" ] ; then
+    setup_drip
+  fi
+
+  if [ "$USE_RUBY" = "1" ] ; then
     setup_ruby
   else
+    setup_java
     setup_vendored_jruby
   fi
+
+  export RUBYLIB="${basedir}/lib"
 }
 
 install_deps() {
diff --git a/docs/learn.md b/docs/learn.md
index a2fa5412df7..e26bf320fa8 100644
--- a/docs/learn.md
+++ b/docs/learn.md
@@ -33,7 +33,7 @@ email the mailing list (logstash-users@googlegroups.com). Further, there is also
 an IRC channel - #logstash on irc.freenode.org.
 
 If you find a bug or have a feature request, file them
-on <http://logstash.jira.com/>. (Honestly though, if you prefer email or irc
+on [github](https://github.com/elasticsearch/logstas/issues). (Honestly though, if you prefer email or irc
 for such things, that works for me, too.)
 
 ## Download It
diff --git a/dripmain.rb b/dripmain.rb
new file mode 100644
index 00000000000..23426a5b063
--- /dev/null
+++ b/dripmain.rb
@@ -0,0 +1,29 @@
+# dripmain.rb is called by org.jruby.main.DripMain to further warm the JVM with any preloading
+# that we can do to speedup future startup using drip.
+
+# we are out of the application context here so setup the load path and gem paths
+lib_path = File.expand_path(File.join(File.dirname(__FILE__), "./lib"))
+$:.unshift(lib_path)
+
+require "logstash/environment"
+LogStash::Environment.set_gem_paths!
+
+# typical required gems and libs
+require "i18n"
+I18n.enforce_available_locales = true
+I18n.load_path << LogStash::Environment.locales_path("en.yml")
+require "cabin"
+require "stud/trap"
+require "stud/task"
+require "clamp"
+require "rspec"
+require "rspec/core/runner"
+
+require "logstash/namespace"
+require "logstash/program"
+require "logstash/agent"
+require "logstash/kibana"
+require "logstash/util"
+require "logstash/errors"
+require "logstash/pipeline"
+require "logstash/plugin"
diff --git a/gembag.rb b/gembag.rb
index 9f03c8c9bfd..8fad3b43753 100644
--- a/gembag.rb
+++ b/gembag.rb
@@ -1,12 +1,7 @@
 #!/usr/bin/env ruby
 
-require "rbconfig"
-
-rubyabi = RbConfig::CONFIG["ruby_version"]
-target = "#{Dir.pwd}/vendor/bundle"
-gemdir = "#{target}/#{RUBY_ENGINE}/#{rubyabi}/"
-ENV["GEM_HOME"] = gemdir
-ENV["GEM_PATH"] = ""
+require "logstash/environment"
+LogStash::Environment.set_gem_paths!
 
 require "rubygems/specification"
 require "rubygems/commands/install_command"
@@ -48,13 +43,13 @@ def install_gem(name, requirement, target)
 module Bundler
   module SharedHelpers
     def default_lockfile
-      ruby = "#{RUBY_ENGINE}-#{RbConfig::CONFIG["ruby_version"]}"
+      ruby = "#{LogStash::Environment.ruby_engine}-#{LogStash::Environment.gem_ruby_version}"
       return Pathname.new("#{default_gemfile}.#{ruby}.lock")
     end
   end
 end
 
-if RUBY_ENGINE == "rbx"
+if LogStash::Environment.ruby_engine == "rbx"
   begin
     gem("rubysl")
   rescue Gem::LoadError => e
@@ -65,7 +60,7 @@ def default_lockfile
 # Try installing a few times in case we hit the "bad_record_mac" ssl error during installation.
 10.times do
   begin
-    Bundler::CLI.start(["install", "--gemfile=tools/Gemfile", "--path", target, "--clean"])
+    Bundler::CLI.start(["install", "--gemfile=tools/Gemfile", "--path", LogStash::Environment.gem_target, "--clean", "--without", "development"])
     break
   rescue Gem::RemoteFetcher::FetchError => e
     puts e.message
diff --git a/lib/logstash/codecs/collectd.rb b/lib/logstash/codecs/collectd.rb
index 2e7bf9afdf3..770dc58a295 100644
--- a/lib/logstash/codecs/collectd.rb
+++ b/lib/logstash/codecs/collectd.rb
@@ -2,6 +2,7 @@
 require "date"
 require "logstash/codecs/base"
 require "logstash/namespace"
+require "logstash/errors"
 require "tempfile"
 require "time"
 
@@ -39,6 +40,7 @@
 class ProtocolError < LogStash::Error; end
 class HeaderError < LogStash::Error; end
 class EncryptionError < LogStash::Error; end
+class NaNError < LogStash::Error; end
 
 class LogStash::Codecs::Collectd < LogStash::Codecs::Base
   config_name "collectd"
@@ -75,20 +77,21 @@ class LogStash::Codecs::Collectd < LogStash::Codecs::Base
 
   COLLECTD_TYPE_FIELDS = {
     'host' => true,
-    '@timestamp' => true, 
-    'plugin' => true, 
+    '@timestamp' => true,
+    'plugin' => true,
     'plugin_instance' => true,
+    'type_instance' => true,
   }
 
   INTERVAL_VALUES_FIELDS = {
-    "interval" => true, 
+    "interval" => true,
     "values" => true,
   }
 
   INTERVAL_BASE_FIELDS = {
     'host' => true,
     'collectd_type' => true,
-    'plugin' => true, 
+    'plugin' => true,
     'plugin_instance' => true,
     '@timestamp' => true,
     'type_instance' => true,
@@ -115,6 +118,20 @@ class LogStash::Codecs::Collectd < LogStash::Codecs::Base
   # collectd [Network plugin](https://collectd.org/wiki/index.php/Plugin:Network)
   config :security_level, :validate => [SECURITY_NONE, SECURITY_SIGN, SECURITY_ENCR],
     :default => "None"
+  
+  # What to do when a value in the event is NaN (Not a Number)
+  # - change_value (default): Change the NaN to the value of the nan_value option and add nan_tag as a tag
+  # - warn: Change the NaN to the value of the nan_value option, print a warning to the log and add nan_tag as a tag
+  # - drop: Drop the event containing the NaN (this only drops the single event, not the whole packet)
+  config :nan_handling, :validate => ['change_value','warn','drop'], :default => 'change_value'
+  
+  # Only relevant when nan_handeling is set to 'change_value'
+  # Change NaN to this configured value
+  config :nan_value, :validate => :number, :default => 0
+  
+  # The tag to add to the event if a NaN value was found
+  # Set this to an empty string ('') if you don't want to tag
+  config :nan_tag, :validate => :string, :default => '_collectdNaN'
 
   # Path to the authentication file. This file should have the same format as
   # the [AuthFile](http://collectd.org/documentation/manpages/collectd.conf.5.shtml#authfile_filename)
@@ -125,6 +142,7 @@ class LogStash::Codecs::Collectd < LogStash::Codecs::Base
   public
   def register
     @logger.info("Starting Collectd codec...")
+    init_lambdas!
     if @typesdb.nil?
       @typesdb = LogStash::Environment.vendor_path("collectd/types.db")
       if !File.exists?(@typesdb)
@@ -167,103 +185,121 @@ def get_types(paths)
     return types
   end # def get_types
 
-  # Lambdas for hash + closure methodology
-  # This replaces when statements for fixed values and is much faster
-  string_decoder = lambda { |body| body.pack("C*")[0..-2] }
-  numeric_decoder = lambda { |body| body.slice!(0..7).pack("C*").unpack("E")[0] }
-  counter_decoder = lambda { |body| body.slice!(0..7).pack("C*").unpack("Q>")[0] }
-  gauge_decoder   = lambda { |body| body.slice!(0..7).pack("C*").unpack("E")[0] }
-  derive_decoder  = lambda { |body| body.slice!(0..7).pack("C*").unpack("q>")[0] }
-  # For Low-Resolution time
-  time_decoder = lambda do |body|
-    byte1, byte2 = body.pack("C*").unpack("NN")
-    Time.at(( ((byte1 << 32) + byte2))).utc
-  end
-  # Hi-Resolution time
-  hirestime_decoder = lambda do |body|
-    byte1, byte2 = body.pack("C*").unpack("NN")
-    Time.at(( ((byte1 << 32) + byte2) * (2**-30) )).utc
-  end
-  # Hi resolution intervals
-  hiresinterval_decoder = lambda do |body|
-    byte1, byte2 = body.pack("C*").unpack("NN")
-    Time.at(( ((byte1 << 32) + byte2) * (2**-30) )).to_i
-  end
-  # Values decoder
-  values_decoder = lambda do |body|
-    body.slice!(0..1)       # Prune the header
-    if body.length % 9 == 0 # Should be 9 fields
-      count = 0
-      retval = []
-      # Iterate through and take a slice each time
-      types = body.slice!(0..((body.length/9)-1))
-      while body.length > 0
-        # Use another hash + closure here...
-        retval << VALUES_DECODER[types[count]].call(body)
-        count += 1
+  def init_lambdas!
+    # Lambdas for hash + closure methodology
+    # This replaces when statements for fixed values and is much faster
+    string_decoder  = lambda { |body| body.pack("C*")[0..-2] }
+    numeric_decoder = lambda { |body| body.slice!(0..7).pack("C*").unpack("E")[0] }
+    counter_decoder = lambda { |body| body.slice!(0..7).pack("C*").unpack("Q>")[0] }
+    gauge_decoder   = lambda { |body| body.slice!(0..7).pack("C*").unpack("E")[0] }
+    derive_decoder  = lambda { |body| body.slice!(0..7).pack("C*").unpack("q>")[0] }
+    # For Low-Resolution time
+    time_decoder = lambda do |body|
+      byte1, byte2 = body.pack("C*").unpack("NN")
+      Time.at(( ((byte1 << 32) + byte2))).utc
+    end
+    # Hi-Resolution time
+    hirestime_decoder = lambda do |body|
+      byte1, byte2 = body.pack("C*").unpack("NN")
+      Time.at(( ((byte1 << 32) + byte2) * (2**-30) )).utc
+    end
+    # Hi resolution intervals
+    hiresinterval_decoder = lambda do |body|
+      byte1, byte2 = body.pack("C*").unpack("NN")
+      Time.at(( ((byte1 << 32) + byte2) * (2**-30) )).to_i
+    end
+    # Value type decoder
+    value_type_decoder = lambda do |body|
+      body.slice!(0..1)       # Prune the header
+      if body.length % 9 == 0 # Should be 9 fields
+        count = 0
+        retval = []
+        # Iterate through and take a slice each time
+        types = body.slice!(0..((body.length/9)-1))
+        while body.length > 0
+          # Use another hash + closure here...
+          v = @values_decoder[types[count]].call(body)
+          if types[count] == 1 && v.nan?
+            case @nan_handling
+            when 'drop'; drop = true
+            else
+              v = @nan_value
+              add_nan_tag = true
+              @nan_handling == 'warn' && @logger.warn("NaN replaced by #{@nan_value}")
+            end
+          end
+          retval << v
+          count += 1
+        end
+      else
+        @logger.error("Incorrect number of data fields for collectd record", :body => body.to_s)
       end
-    else
-      @logger.error("Incorrect number of data fields for collectd record", :body => body.to_s)
+      return retval, drop, add_nan_tag
     end
-    return retval
-  end
-  # Signature
-  signature_decoder = lambda do |body|
-    if body.length < 32
-      @logger.warning("SHA256 signature too small (got #{body.length} bytes instead of 32)")
-    elsif body.length < 33
-      @logger.warning("Received signature without username")
-    else
+    # Signature
+    signature_decoder = lambda do |body|
+      if body.length < 32
+        @logger.warning("SHA256 signature too small (got #{body.length} bytes instead of 32)")
+      elsif body.length < 33
+        @logger.warning("Received signature without username")
+      else
+        retval = []
+        # Byte 32 till the end contains the username as chars (=unsigned ints)
+        retval << body[32..-1].pack('C*')
+        # Byte 0 till 31 contain the signature
+        retval << body[0..31].pack('C*')
+      end
+      return retval
+    end
+    # Encryption
+    encryption_decoder = lambda do |body|
       retval = []
-      # Byte 32 till the end contains the username as chars (=unsigned ints)
-      retval << body[32..-1].pack('C*')
-      # Byte 0 till 31 contain the signature
-      retval << body[0..31].pack('C*')
+      user_length = (body.slice!(0) << 8) + body.slice!(0)
+      retval << body.slice!(0..user_length-1).pack('C*') # Username
+      retval << body.slice!(0..15).pack('C*')            # IV
+      retval << body.pack('C*')
+      return retval
     end
-    return retval
-  end
-  # Encryption
-  encryption_decoder = lambda do |body|
-    retval = []
-    user_length = (body.slice!(0) << 8) + body.slice!(0)
-    retval << body.slice!(0..user_length-1).pack('C*') # Username
-    retval << body.slice!(0..15).pack('C*')            # IV
-    retval << body.pack('C*')
-    return retval
-  end
-  # Lambda Hashes
-  ID_DECODER = {
-    0 => string_decoder,
-    1 => time_decoder,
-    2 => string_decoder,
-    3 => string_decoder,
-    4 => string_decoder,
-    5 => string_decoder,
-    6 => values_decoder,
-    7 => numeric_decoder,
-    8 => hirestime_decoder,
-    9 => hiresinterval_decoder,
-    256 => string_decoder,
-    257 => numeric_decoder,
-    512 => signature_decoder,
-    528 => encryption_decoder
-  }
-  # TYPE VALUES:
-  # 0: COUNTER
-  # 1: GAUGE
-  # 2: DERIVE
-  # 3: ABSOLUTE
-  VALUES_DECODER = {
-    0 => counter_decoder,
-    1 => gauge_decoder,
-    2 => derive_decoder,
-    3 => counter_decoder
-  }
+    @id_decoder = {
+      0 => string_decoder,
+      1 => time_decoder,
+      2 => string_decoder,
+      3 => string_decoder,
+      4 => string_decoder,
+      5 => string_decoder,
+      6 => value_type_decoder,
+      7 => numeric_decoder,
+      8 => hirestime_decoder,
+      9 => hiresinterval_decoder,
+      256 => string_decoder,
+      257 => numeric_decoder,
+      512 => signature_decoder,
+      528 => encryption_decoder
+    }
+    # TYPE VALUES:
+    # 0: COUNTER
+    # 1: GAUGE
+    # 2: DERIVE
+    # 3: ABSOLUTE
+    @values_decoder = {
+      0 => counter_decoder,
+      1 => gauge_decoder,
+      2 => derive_decoder,
+      3 => counter_decoder
+    }
+  end # def init_lambdas!
 
   public
   def get_values(id, body)
+    drop = false
+    add_tag = false
+    if id == 6
+      retval, drop, add_nan_tag = @id_decoder[id].call(body)
     # Use hash + closure/lambda to speed operations
-    ID_DECODER[id].call(body)
+    else
+      retval = @id_decoder[id].call(body)
+    end
+    return retval, drop, add_nan_tag
   end
 
   private
@@ -369,7 +405,7 @@ def decode(payload)
         next
       end
 
-      values = get_values(typenum, body)
+      values, drop, add_nan_tag = get_values(typenum, body)
 
       case typenum
       when SIGNATURE_TYPE
@@ -424,9 +460,17 @@ def decode(payload)
           # This is better than looping over all keys every time.
           collectd.delete('type_instance') if collectd['type_instance'] == ""
           collectd.delete('plugin_instance') if collectd['plugin_instance'] == ""
+          if add_nan_tag
+            collectd['tags'] ||= []
+            collectd['tags'] << @nan_tag
+          end
           # This ugly little shallow-copy hack keeps the new event from getting munged by the cleanup
           # With pass-by-reference we get hosed (if we pass collectd, then clean it up rapidly, values can disappear)
-          yield LogStash::Event.new(collectd.dup)
+          if !drop # Drop the event if it's flagged true
+            yield LogStash::Event.new(collectd.dup)
+          else
+            raise(NaNError)
+          end
         end
         # Clean up the event
         collectd.each_key do |k|
@@ -434,8 +478,8 @@ def decode(payload)
         end
       end
     end # while payload.length > 0 do
-  rescue EncryptionError, ProtocolError, HeaderError
+  rescue EncryptionError, ProtocolError, HeaderError, NaNError
     # basically do nothing, we just want out
   end # def decode
 
-end # class LogStash::Codecs::Collectd
+end # class LogStash::Codecs::Collectd
\ No newline at end of file
diff --git a/lib/logstash/codecs/edn.rb b/lib/logstash/codecs/edn.rb
index f5686db0bf1..449b7cec40c 100644
--- a/lib/logstash/codecs/edn.rb
+++ b/lib/logstash/codecs/edn.rb
@@ -1,5 +1,6 @@
 require "logstash/codecs/base"
 require "logstash/codecs/line"
+require "logstash/util"
 
 class LogStash::Codecs::EDN < LogStash::Codecs::Base
   config_name "edn"
@@ -14,15 +15,20 @@ def register
   def decode(data)
     begin
       yield LogStash::Event.new(EDN.read(data))
-    rescue
-      @logger.info("EDN parse failure. Falling back to plain-text", :error => e, :data => data)
+    rescue => e
+      @logger.warn("EDN parse failure. Falling back to plain-text", :error => e, :data => data)
       yield LogStash::Event.new("message" => data)
     end
   end
 
   public
-  def encode(data)
-    @on_event.call(data.to_hash.to_edn)
+  def encode(event)
+    # use normalize to make sure returned Hash is pure Ruby
+    # #to_edn which relies on pure Ruby object recognition
+    data = LogStash::Util.normalize(event.to_hash)
+    # timestamp is serialized as a iso8601 string
+    # merge to avoid modifying data which could have side effects if multiple outputs
+    @on_event.call(data.merge(LogStash::Event::TIMESTAMP => event.timestamp.to_iso8601).to_edn)
   end
 
 end
diff --git a/lib/logstash/codecs/edn_lines.rb b/lib/logstash/codecs/edn_lines.rb
index 8b6b490c239..3c4a0a38b84 100644
--- a/lib/logstash/codecs/edn_lines.rb
+++ b/lib/logstash/codecs/edn_lines.rb
@@ -1,5 +1,6 @@
 require "logstash/codecs/base"
 require "logstash/codecs/line"
+require "logstash/util"
 
 class LogStash::Codecs::EDNLines < LogStash::Codecs::Base
   config_name "edn_lines"
@@ -22,15 +23,20 @@ def decode(data)
       begin
         yield LogStash::Event.new(EDN.read(event["message"]))
       rescue => e
-        @logger.info("EDN parse failure. Falling back to plain-text", :error => e, :data => data)
+        @logger.warn("EDN parse failure. Falling back to plain-text", :error => e, :data => data)
         yield LogStash::Event.new("message" => data)
       end
     end
   end
 
   public
-  def encode(data)
-    @on_event.call(data.to_hash.to_edn + "\n")
+  def encode(event)
+    # use normalize to make sure returned Hash is pure Ruby for
+    # #to_edn which relies on pure Ruby object recognition
+    data = LogStash::Util.normalize(event.to_hash)
+    # timestamp is serialized as a iso8601 string
+    # merge to avoid modifying data which could have side effects if multiple outputs
+    @on_event.call(data.merge(LogStash::Event::TIMESTAMP => event.timestamp.to_iso8601).to_edn + NL)
   end
 
 end
diff --git a/lib/logstash/codecs/fluent.rb b/lib/logstash/codecs/fluent.rb
index d1e6acd336e..cbcaf3ebe46 100644
--- a/lib/logstash/codecs/fluent.rb
+++ b/lib/logstash/codecs/fluent.rb
@@ -1,6 +1,8 @@
 # encoding: utf-8
 require "logstash/codecs/base"
 require "logstash/util/charset"
+require "logstash/timestamp"
+require "logstash/util"
 
 # This codec handles fluentd's msgpack schema.
 #
@@ -38,7 +40,7 @@ def decode(data)
     @decoder.feed(data)
     @decoder.each do |tag, epochtime, map|
       event = LogStash::Event.new(map.merge(
-        "@timestamp" => Time.at(epochtime),
+        LogStash::Event::TIMESTAMP => LogStash::Timestamp.at(epochtime),
         "tags" => tag
       ))
       yield event
@@ -48,8 +50,14 @@ def decode(data)
   public
   def encode(event)
     tag = event["tags"] || "log"
-    epochtime = event["@timestamp"].to_i
-    @on_event.call(MessagePack.pack([ tag, epochtime, event.to_hash ]))
+    epochtime = event.timestamp.to_i
+
+    # use normalize to make sure returned Hash is pure Ruby for
+    # MessagePack#pack which relies on pure Ruby object recognition
+    data = LogStash::Util.normalize(event.to_hash)
+    # timestamp is serialized as a iso8601 string
+    # merge to avoid modifying data which could have side effects if multiple outputs
+    @on_event.call(MessagePack.pack([tag, epochtime, data.merge(LogStash::Event::TIMESTAMP => event.timestamp.to_iso8601)]))
   end # def encode
 
 end # class LogStash::Codecs::Fluent
diff --git a/lib/logstash/codecs/graphite.rb b/lib/logstash/codecs/graphite.rb
index e3510f3b65e..4471df9aa59 100644
--- a/lib/logstash/codecs/graphite.rb
+++ b/lib/logstash/codecs/graphite.rb
@@ -1,7 +1,7 @@
 # encoding: utf-8
 require "logstash/codecs/base"
 require "logstash/codecs/line"
-require "json"
+require "logstash/timestamp"
 
 # This codec will encode and decode Graphite formated lines.
 class LogStash::Codecs::Graphite < LogStash::Codecs::Base
@@ -52,7 +52,7 @@ def initialize(params={})
   def decode(data)
     @lines.decode(data) do |event|
       name, value, time = event["message"].split(" ")
-      yield LogStash::Event.new(name => value.to_f, "@timestamp" => Time.at(time.to_i).gmtime)
+      yield LogStash::Event.new(name => value.to_f, LogStash::Event::TIMESTAMP => LogStash::Timestamp.at(time.to_i))
     end # @lines.decode
   end # def decode
 
@@ -93,7 +93,7 @@ def encode(event)
     if messages.empty?
       @logger.debug("Message is empty, not emiting anything.", :messages => messages)
     else
-      message = messages.join("\n") + "\n"
+      message = messages.join(NL) + NL
       @logger.debug("Emiting carbon messages", :messages => messages)
 
       @on_event.call(message)
diff --git a/lib/logstash/codecs/json.rb b/lib/logstash/codecs/json.rb
index 718498cad0b..1ba0163f8d8 100644
--- a/lib/logstash/codecs/json.rb
+++ b/lib/logstash/codecs/json.rb
@@ -1,9 +1,9 @@
 # encoding: utf-8
 require "logstash/codecs/base"
-require "logstash/codecs/line"
-require "json"
+require "logstash/util/charset"
+require "logstash/json"
 
-# This codec may be used to decode (via inputs) and encode (via outputs) 
+# This codec may be used to decode (via inputs) and encode (via outputs)
 # full JSON messages.  If you are streaming JSON messages delimited
 # by '\n' then see the `json_lines` codec.
 # Encoding will result in a single JSON string.
@@ -28,21 +28,21 @@ def register
     @converter = LogStash::Util::Charset.new(@charset)
     @converter.logger = @logger
   end
-  
+
   public
   def decode(data)
     data = @converter.convert(data)
     begin
-      yield LogStash::Event.new(JSON.parse(data))
-    rescue JSON::ParserError => e
+      yield LogStash::Event.new(LogStash::Json.load(data))
+    rescue LogStash::Json::ParserError => e
       @logger.info("JSON parse failure. Falling back to plain-text", :error => e, :data => data)
       yield LogStash::Event.new("message" => data)
     end
   end # def decode
 
   public
-  def encode(data)
-    @on_event.call(data.to_json)
+  def encode(event)
+    @on_event.call(event.to_json)
   end # def encode
 
 end # class LogStash::Codecs::JSON
diff --git a/lib/logstash/codecs/json_lines.rb b/lib/logstash/codecs/json_lines.rb
index e3dac771d17..f319340f7f7 100644
--- a/lib/logstash/codecs/json_lines.rb
+++ b/lib/logstash/codecs/json_lines.rb
@@ -1,7 +1,7 @@
 # encoding: utf-8
 require "logstash/codecs/base"
 require "logstash/codecs/line"
-require "json"
+require "logstash/json"
 
 # This codec will decode streamed JSON that is newline delimited.
 # For decoding line-oriented JSON payload in the redis or file inputs,
@@ -29,14 +29,14 @@ def initialize(params={})
     @lines = LogStash::Codecs::Line.new
     @lines.charset = @charset
   end
-  
+
   public
   def decode(data)
 
     @lines.decode(data) do |event|
       begin
-        yield LogStash::Event.new(JSON.parse(event["message"]))
-      rescue JSON::ParserError => e
+        yield LogStash::Event.new(LogStash::Json.load(event["message"]))
+      rescue LogStash::Json::ParserError => e
         @logger.info("JSON parse failure. Falling back to plain-text", :error => e, :data => data)
         yield LogStash::Event.new("message" => event["message"])
       end
@@ -44,10 +44,10 @@ def decode(data)
   end # def decode
 
   public
-  def encode(data)
+  def encode(event)
     # Tack on a \n for now because previously most of logstash's JSON
     # outputs emitted one per line, and whitespace is OK in json.
-    @on_event.call(data.to_json + "\n")
+    @on_event.call(event.to_json + NL)
   end # def encode
 
 end # class LogStash::Codecs::JSON
diff --git a/lib/logstash/codecs/json_spooler.rb b/lib/logstash/codecs/json_spooler.rb
index a143971eeac..2dc512b2fa5 100644
--- a/lib/logstash/codecs/json_spooler.rb
+++ b/lib/logstash/codecs/json_spooler.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/codecs/base"
 require "logstash/codecs/spool"
+require "logstash/json"
 
 # This is the base class for logstash codecs.
 class LogStash::Codecs::JsonSpooler < LogStash::Codecs::Spool
@@ -14,14 +15,14 @@ def register
 
   public
   def decode(data)
-    super(JSON.parse(data.force_encoding(Encoding::UTF_8))) do |event|
+    super(LogStash::Json.load(data.force_encoding(Encoding::UTF_8))) do |event|
       yield event
     end
   end # def decode
 
   public
-  def encode(data)
-    super(data)
+  def encode(event)
+    super(event)
   end # def encode
 
 end # class LogStash::Codecs::Json
diff --git a/lib/logstash/codecs/line.rb b/lib/logstash/codecs/line.rb
index 21ae47a892b..12107e569eb 100644
--- a/lib/logstash/codecs/line.rb
+++ b/lib/logstash/codecs/line.rb
@@ -30,7 +30,7 @@ def register
     @converter = LogStash::Util::Charset.new(@charset)
     @converter.logger = @logger
   end
-  
+
   public
   def decode(data)
     @buffer.extract(data).each do |line|
@@ -47,11 +47,11 @@ def flush(&block)
   end
 
   public
-  def encode(data)
-    if data.is_a? LogStash::Event and @format
-      @on_event.call(data.sprintf(@format) + "\n")
+  def encode(event)
+    if event.is_a? LogStash::Event and @format
+      @on_event.call(event.sprintf(@format) + NL)
     else
-      @on_event.call(data.to_s + "\n")
+      @on_event.call(event.to_s + NL)
     end
   end # def encode
 
diff --git a/lib/logstash/codecs/msgpack.rb b/lib/logstash/codecs/msgpack.rb
index 05dedf449c5..b2f45e28a7f 100644
--- a/lib/logstash/codecs/msgpack.rb
+++ b/lib/logstash/codecs/msgpack.rb
@@ -1,5 +1,7 @@
 # encoding: utf-8
 require "logstash/codecs/base"
+require "logstash/timestamp"
+require "logstash/util"
 
 class LogStash::Codecs::Msgpack < LogStash::Codecs::Base
   config_name "msgpack"
@@ -18,7 +20,6 @@ def decode(data)
     begin
       # Msgpack does not care about UTF-8
       event = LogStash::Event.new(MessagePack.unpack(data))
-      event["@timestamp"] = Time.at(event["@timestamp"]).utc if event["@timestamp"].is_a? Float
       event["tags"] ||= []
       if @format
         event["message"] ||= event.sprintf(@format)
@@ -36,8 +37,12 @@ def decode(data)
 
   public
   def encode(event)
-    event["@timestamp"] = event["@timestamp"].to_f
-    @on_event.call event.to_hash.to_msgpack
+    # use normalize to make sure returned Hash is pure Ruby for
+    # MessagePack#pack which relies on pure Ruby object recognition
+    data = LogStash::Util.normalize(event.to_hash)
+    # timestamp is serialized as a iso8601 string
+    # merge to avoid modifying data which could have side effects if multiple outputs
+    @on_event.call(MessagePack.pack(data.merge(LogStash::Event::TIMESTAMP => event.timestamp.to_iso8601)))
   end # def encode
 
 end # class LogStash::Codecs::Msgpack
diff --git a/lib/logstash/codecs/multiline.rb b/lib/logstash/codecs/multiline.rb
index 1bbdecb0d43..4815ffa960f 100644
--- a/lib/logstash/codecs/multiline.rb
+++ b/lib/logstash/codecs/multiline.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/codecs/base"
 require "logstash/util/charset"
+require "logstash/timestamp"
 
 # The multiline codec will collapse multiline messages and merge them into a
 # single event.
@@ -159,13 +160,13 @@ def decode(text, &block)
   end # def decode
 
   def buffer(text)
-    @time = Time.now.utc if @buffer.empty?
+    @time = LogStash::Timestamp.now if @buffer.empty?
     @buffer << text
   end
 
   def flush(&block)
     if @buffer.any?
-      event = LogStash::Event.new("@timestamp" => @time, "message" => @buffer.join("\n"))
+      event = LogStash::Event.new(LogStash::Event::TIMESTAMP => @time, "message" => @buffer.join(NL))
       # Tag multiline events
       event.tag @multiline_tag if @multiline_tag && @buffer.size > 1
 
@@ -185,9 +186,9 @@ def do_previous(text, matched, &block)
   end
 
   public
-  def encode(data)
+  def encode(event)
     # Nothing to do.
-    @on_event.call(data)
+    @on_event.call(event)
   end # def encode
 
 end # class LogStash::Codecs::Plain
diff --git a/lib/logstash/codecs/netflow.rb b/lib/logstash/codecs/netflow.rb
index 9e2d99de1c2..360de0fd87f 100644
--- a/lib/logstash/codecs/netflow.rb
+++ b/lib/logstash/codecs/netflow.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/filters/base"
 require "logstash/namespace"
+require "logstash/timestamp"
 
 # The "netflow" codec is for decoding Netflow v5/v9 flows.
 class LogStash::Codecs::Netflow < LogStash::Codecs::Base
@@ -45,7 +46,7 @@ def register
     @templates = Vash.new()
 
     # Path to default Netflow v9 field definitions
-    filename = File.join(File.dirname(__FILE__), "netflow/netflow.yaml")
+    filename = LogStash::Environment.plugin_path("codecs/netflow/netflow.yaml")
 
     begin
       @fields = YAML.load_file(filename)
@@ -90,7 +91,7 @@ def decode(payload, &block)
         #
         # The flowset header gives us the UTC epoch seconds along with
         # residual nanoseconds so we can set @timestamp to that easily
-        event["@timestamp"] = Time.at(flowset.unix_sec, flowset.unix_nsec / 1000).utc
+        event.timestamp = LogStash::Timestamp.at(flowset.unix_sec, flowset.unix_nsec / 1000)
         event[@target] = {}
 
         # Copy some of the pertinent fields in the header to the event
@@ -162,7 +163,7 @@ def decode(payload, &block)
               # Purge any expired templates
               @templates.cleanup!
             end
-          end 
+          end
         when 256..65535
           # Data flowset
           #key = "#{flowset.source_id}|#{event["source"]}|#{record.flowset_id}"
@@ -180,7 +181,7 @@ def decode(payload, &block)
           # Template shouldn't be longer than the record and there should
           # be at most 3 padding bytes
           if template.num_bytes > length or ! (length % template.num_bytes).between?(0, 3)
-            @logger.warn("Template length doesn't fit cleanly into flowset", :template_id => record.flowset_id, :template_length => template.num_bytes, :record_length => length) 
+            @logger.warn("Template length doesn't fit cleanly into flowset", :template_id => record.flowset_id, :template_length => template.num_bytes, :record_length => length)
             next
           end
 
@@ -190,7 +191,7 @@ def decode(payload, &block)
 
           records.each do |r|
             event = LogStash::Event.new(
-              "@timestamp" => Time.at(flowset.unix_sec).utc,
+              LogStash::Event::TIMESTAMP => LogStash::Timestamp.at(flowset.unix_sec),
               @target => {}
             )
 
diff --git a/lib/logstash/codecs/noop.rb b/lib/logstash/codecs/noop.rb
index 8a7a0d7213a..ed74a685f2d 100644
--- a/lib/logstash/codecs/noop.rb
+++ b/lib/logstash/codecs/noop.rb
@@ -5,15 +5,15 @@ class LogStash::Codecs::Noop < LogStash::Codecs::Base
   config_name "noop"
 
   milestone 1
-  
+
   public
   def decode(data)
     yield data
   end # def decode
 
   public
-  def encode(data)
-    @on_event.call data
+  def encode(event)
+    @on_event.call event
   end # def encode
 
 end # class LogStash::Codecs::Noop
diff --git a/lib/logstash/codecs/oldlogstashjson.rb b/lib/logstash/codecs/oldlogstashjson.rb
index d815ece335e..0cd5ce74973 100644
--- a/lib/logstash/codecs/oldlogstashjson.rb
+++ b/lib/logstash/codecs/oldlogstashjson.rb
@@ -1,5 +1,6 @@
 # encoding: utf-8
 require "logstash/codecs/base"
+require "logstash/json"
 
 class LogStash::Codecs::OldLogStashJSON < LogStash::Codecs::Base
   config_name "oldlogstashjson"
@@ -14,8 +15,8 @@ class LogStash::Codecs::OldLogStashJSON < LogStash::Codecs::Base
   public
   def decode(data)
     begin
-      obj = JSON.parse(data.force_encoding(Encoding::UTF_8))
-    rescue JSON::ParserError => e
+      obj = LogStash::Json.load(data.force_encoding(Encoding::UTF_8))
+    rescue LogStash::Json::ParserError => e
       @logger.info("JSON parse failure. Falling back to plain-text", :error => e, :data => data)
       yield LogStash::Event.new("message" => data)
       return
@@ -33,24 +34,24 @@ def decode(data)
   end # def decode
 
   public
-  def encode(data)
+  def encode(event)
     h  = {}
 
     # Convert the new logstash schema to the old one.
     V0_TO_V1.each do |key, val|
-      h[key] = data[val] if data.include?(val)
+      h[key] = event[val] if event.include?(val)
     end
 
-    data.to_hash.each do |field, val|
+    event.to_hash.each do |field, val|
       # TODO: might be better to V1_TO_V0 = V0_TO_V1.invert during
       # initialization than V0_TO_V1.has_value? within loop
       next if field == "@version" or V0_TO_V1.has_value?(field)
-      h["@fields"] = {} if h["@fields"].nil?
+      h["@fields"] ||= {}
       h["@fields"][field] = val
     end
 
     # Tack on a \n because JSON outputs 1.1.x had them.
-    @on_event.call(h.to_json + "\n")
+    @on_event.call(LogStash::Json.dump(h) + NL)
   end # def encode
 
 end # class LogStash::Codecs::OldLogStashJSON
diff --git a/lib/logstash/codecs/plain.rb b/lib/logstash/codecs/plain.rb
index 40071f5addc..5fe86aacd28 100644
--- a/lib/logstash/codecs/plain.rb
+++ b/lib/logstash/codecs/plain.rb
@@ -37,11 +37,11 @@ def decode(data)
   end # def decode
 
   public
-  def encode(data)
-    if data.is_a? LogStash::Event and @format
-      @on_event.call(data.sprintf(@format))
+  def encode(event)
+    if event.is_a?(LogStash::Event) and @format
+      @on_event.call(event.sprintf(@format))
     else
-      @on_event.call(data.to_s)
+      @on_event.call(event.to_s)
     end
   end # def encode
 
diff --git a/lib/logstash/codecs/rubydebug.rb b/lib/logstash/codecs/rubydebug.rb
index 607131a29be..fa53a5ec6a4 100644
--- a/lib/logstash/codecs/rubydebug.rb
+++ b/lib/logstash/codecs/rubydebug.rb
@@ -18,8 +18,8 @@ def decode(data)
   end # def decode
 
   public
-  def encode(data)
-    @on_event.call(data.to_hash.awesome_inspect + "\n")
+  def encode(event)
+    @on_event.call(event.to_hash.awesome_inspect + NL)
   end # def encode
 
 end # class LogStash::Codecs::Dots
diff --git a/lib/logstash/codecs/spool.rb b/lib/logstash/codecs/spool.rb
index 67ff480b06d..04fdd05bde2 100644
--- a/lib/logstash/codecs/spool.rb
+++ b/lib/logstash/codecs/spool.rb
@@ -16,15 +16,15 @@ def decode(data)
   end # def decode
 
   public
-  def encode(data)
-    @buffer = [] if @buffer.nil?
-    #buffer size is hard coded for now until a 
+  def encode(event)
+    @buffer ||= []
+    #buffer size is hard coded for now until a
     #better way to pass args into codecs is implemented
     if @buffer.length >= @spool_size
       @on_event.call @buffer
       @buffer = []
     else
-      @buffer << data
+      @buffer << event
     end
   end # def encode
 
diff --git a/lib/logstash/environment.rb b/lib/logstash/environment.rb
index fd234644cf1..03399379c9c 100644
--- a/lib/logstash/environment.rb
+++ b/lib/logstash/environment.rb
@@ -24,12 +24,49 @@ def load_elasticsearch_jars!
       end
     end
 
+    def gem_target
+      "#{LOGSTASH_HOME}/vendor/bundle"
+    end
+
+    def set_gem_paths!
+      gemdir = "#{gem_target}/#{ruby_engine}/#{gem_ruby_version}/"
+      ENV["GEM_HOME"] = gemdir
+      ENV["GEM_PATH"] = gemdir
+    end
+
+    # @return [String] major.minor ruby version, ex 1.9
+    def ruby_abi_version
+      RUBY_VERSION[/(\d+\.\d+)(\.\d+)*/, 1]
+    end
+
+    # @return [String] the ruby version string bundler uses to craft its gem path
+    def gem_ruby_version
+      RbConfig::CONFIG["ruby_version"]
+    end
+
+    # @return [String] jruby, ruby, rbx, ...
+    def ruby_engine
+      RUBY_ENGINE
+    end
+
     def jruby?
-      RUBY_PLATFORM == "java"
+      @jruby ||= !!(RUBY_PLATFORM == "java")
     end
 
     def vendor_path(path)
       return ::File.join(LOGSTASH_HOME, "vendor", path)
     end
+
+    def plugin_path(path)
+      return ::File.join(LOGSTASH_HOME, "lib/logstash", path)
+    end
+
+    def pattern_path(path)
+      return ::File.join(LOGSTASH_HOME, "patterns", path)
+    end
+
+    def locales_path(path)
+      return ::File.join(LOGSTASH_HOME, "locales", path)
+    end
   end
 end
diff --git a/lib/logstash/event.rb b/lib/logstash/event.rb
index 1604ad60346..f9806a09c7a 100644
--- a/lib/logstash/event.rb
+++ b/lib/logstash/event.rb
@@ -1,23 +1,12 @@
 # encoding: utf-8
-require "json"
 require "time"
 require "date"
+require "cabin"
 require "logstash/namespace"
 require "logstash/util/fieldreference"
 require "logstash/util/accessors"
-require "logstash/time_addon"
-
-# Use a custom serialization for jsonifying Time objects.
-# TODO(sissel): Put this in a separate file.
-class Time
-  def to_json(*args)
-    return iso8601(3).to_json(*args)
-  end
-
-  def inspect
-    return to_json
-  end
-end
+require "logstash/timestamp"
+require "logstash/json"
 
 # the logstash event object.
 #
@@ -48,23 +37,17 @@ class DeprecatedMethod < StandardError; end
   TIMESTAMP = "@timestamp"
   VERSION = "@version"
   VERSION_ONE = "1"
+  TIMESTAMP_FAILURE_TAG = "_timestampparsefailure"
+  TIMESTAMP_FAILURE_FIELD = "_@timestamp"
 
   public
-  def initialize(data={})
+  def initialize(data = {})
+    @logger = Cabin::Channel.get(LogStash)
     @cancelled = false
-
     @data = data
     @accessors = LogStash::Util::Accessors.new(data)
-
-    data[VERSION] = VERSION_ONE if !@data.include?(VERSION)
-    if data.include?(TIMESTAMP)
-      t = data[TIMESTAMP]
-      if t.is_a?(String)
-        data[TIMESTAMP] = LogStash::Time.parse_iso8601(t)
-      end
-    else
-      data[TIMESTAMP] = ::Time.now.utc
-    end
+    @data[VERSION] ||= VERSION_ONE
+    @data[TIMESTAMP] = init_timestamp(@data[TIMESTAMP])
   end # def initialize
 
   public
@@ -101,7 +84,7 @@ def to_s
   else
     public
     def to_s
-      return self.sprintf("#{self["@timestamp"].iso8601} %{host} %{message}")
+      return self.sprintf("#{timestamp.to_iso8601} %{host} %{message}")
     end # def to_s
   end
 
@@ -119,23 +102,18 @@ def ruby_timestamp
 
   # field-related access
   public
-  def [](str)
-    if str[0,1] == CHAR_PLUS
-      # nothing?
-    else
-      # return LogStash::Util::FieldReference.exec(str, @data)
-      @accessors.get(str)
-    end
+  def [](fieldref)
+    @accessors.get(fieldref)
   end # def []
 
   public
   # keep []= implementation in sync with spec/test_utils.rb monkey patch
   # which redefines []= but using @accessors.strict_set
-  def []=(str, value)
-    if str == TIMESTAMP && !value.is_a?(Time)
-      raise TypeError, "The field '@timestamp' must be a Time, not a #{value.class} (#{value})"
+  def []=(fieldref, value)
+    if fieldref == TIMESTAMP && !value.is_a?(LogStash::Timestamp)
+      raise TypeError, "The field '@timestamp' must be a (LogStash::Timestamp, not a #{value.class} (#{value})"
     end
-    @accessors.set(str, value)
+    @accessors.set(fieldref, value)
   end # def []=
 
   public
@@ -144,12 +122,13 @@ def fields
   end
 
   public
-  def to_json(*args)
-    return @data.to_json(*args)
+  def to_json
+    LogStash::Json.dump(@data)
   end # def to_json
 
+  public
   def to_hash
-    return @data
+    @data
   end # def to_hash
 
   public
@@ -161,7 +140,7 @@ def overwrite(event)
 
     #convert timestamp if it is a String
     if @data[TIMESTAMP].is_a?(String)
-      @data[TIMESTAMP] = LogStash::Time.parse_iso8601(@data[TIMESTAMP])
+      @data[TIMESTAMP] = LogStash::Timestamp.parse_iso8601(@data[TIMESTAMP])
     end
   end
 
@@ -183,11 +162,8 @@ def append(event)
   # Remove a field or field reference. Returns the value of that field when
   # deleted
   public
-  def remove(str)
-    # return LogStash::Util::FieldReference.exec(str, @data) do |obj, key|
-    #   next obj.delete(key)
-    # end
-    @accessors.del(str)
+  def remove(fieldref)
+    @accessors.del(fieldref)
   end # def remove
 
   # sprintf. This could use a better method name.
@@ -219,9 +195,9 @@ def sprintf(format)
 
       if key == "+%s"
         # Got %{+%s}, support for unix epoch time
-        next @data["@timestamp"].to_i
+        next @data[TIMESTAMP].to_i
       elsif key[0,1] == "+"
-        t = @data["@timestamp"]
+        t = @data[TIMESTAMP]
         formatter = org.joda.time.format.DateTimeFormat.forPattern(key[1 .. -1])\
           .withZone(org.joda.time.DateTimeZone::UTC)
         #next org.joda.time.Instant.new(t.tv_sec * 1000 + t.tv_usec / 1000).toDateTime.toString(formatter)
@@ -238,7 +214,7 @@ def sprintf(format)
           when Array
             value.join(",") # Join by ',' if value is an array
           when Hash
-            value.to_json # Convert hashes to json
+            LogStash::Json.dump(value) # Convert hashes to json
           else
             value # otherwise return the value
         end # case value
@@ -251,4 +227,23 @@ def tag(value)
     self["tags"] ||= []
     self["tags"] << value unless self["tags"].include?(value)
   end
+
+  private
+
+  def init_timestamp(o)
+    begin
+      timestamp = o ? LogStash::Timestamp.coerce(o) : LogStash::Timestamp.now
+      return timestamp if timestamp
+
+      @logger.warn("Unrecognized #{TIMESTAMP} value, setting current time to #{TIMESTAMP}, original in #{TIMESTAMP_FAILURE_FIELD}field", :value => o.inspect)
+    rescue LogStash::TimestampParserError => e
+      @logger.warn("Error parsing #{TIMESTAMP} string, setting current time to #{TIMESTAMP}, original in #{TIMESTAMP_FAILURE_FIELD} field", :value => o.inspect, :exception => e.message)
+    end
+
+    @data["tags"] ||= []
+    @data["tags"] << TIMESTAMP_FAILURE_TAG unless @data["tags"].include?(TIMESTAMP_FAILURE_TAG)
+    @data[TIMESTAMP_FAILURE_FIELD] = o
+
+    LogStash::Timestamp.now
+  end
 end # class LogStash::Event
diff --git a/lib/logstash/filters/base.rb b/lib/logstash/filters/base.rb
index dba1848bdc1..652c31b8412 100644
--- a/lib/logstash/filters/base.rb
+++ b/lib/logstash/filters/base.rb
@@ -112,7 +112,7 @@ class LogStash::Filters::Base < LogStash::Plugin
   #
   #     filter {
   #       %PLUGIN% {
-  #         remove_field => [ "foo_%{somefield}" "my_extraneous_field" ]
+  #         remove_field => [ "foo_%{somefield}", "my_extraneous_field" ]
   #       }
   #     }
   #
diff --git a/lib/logstash/filters/date.rb b/lib/logstash/filters/date.rb
index 87663f4ee3e..937127a646b 100644
--- a/lib/logstash/filters/date.rb
+++ b/lib/logstash/filters/date.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/filters/base"
 require "logstash/namespace"
+require "logstash/timestamp"
 
 # The date filter is used for parsing dates from fields, and then using that
 # date or timestamp as the logstash timestamp for the event.
@@ -88,7 +89,7 @@ class LogStash::Filters::Date < LogStash::Filters::Base
   config :target, :validate => :string, :default => "@timestamp"
 
   # LOGSTASH-34
-  DATEPATTERNS = %w{ y d H m s S } 
+  DATEPATTERNS = %w{ y d H m s S }
 
   public
   def initialize(config = {})
@@ -111,7 +112,7 @@ def parseLocale(localeString)
   def register
     require "java"
     if @match.length < 2
-      raise LogStash::ConfigurationError, I18n.t("logstash.agent.configuration.invalid_plugin_register", 
+      raise LogStash::ConfigurationError, I18n.t("logstash.agent.configuration.invalid_plugin_register",
         :plugin => "filter", :type => "date",
         :error => "The match setting should contains first a field name and at least one date format, current value is #{@match}")
     end
@@ -137,16 +138,16 @@ def setupMatcher(field, locale, value)
           parser = lambda { |date| (date.to_f * 1000).to_i }
         when "UNIX_MS" # unix epoch in ms
           joda_instant = org.joda.time.Instant.java_class.constructor(Java::long).method(:new_instance)
-          parser = lambda do |date| 
+          parser = lambda do |date|
             #return joda_instant.call(date.to_i).to_java.toDateTime
             return date.to_i
           end
         when "TAI64N" # TAI64 with nanoseconds, -10000 accounts for leap seconds
           joda_instant = org.joda.time.Instant.java_class.constructor(Java::long).method(:new_instance)
-          parser = lambda do |date| 
+          parser = lambda do |date|
             # Skip leading "@" if it is present (common in tai64n times)
             date = date[1..-1] if date[0, 1] == "@"
-            #return joda_instant.call((date[1..15].hex * 1000 - 10000)+(date[16..23].hex/1000000)).to_java.toDateTime 
+            #return joda_instant.call((date[1..15].hex * 1000 - 10000)+(date[16..23].hex/1000000)).to_java.toDateTime
             return (date[1..15].hex * 1000 - 10000)+(date[16..23].hex/1000000)
           end
         else
@@ -204,8 +205,7 @@ def filter(event)
           raise last_exception unless success
 
           # Convert joda DateTime to a ruby Time
-          event[@target] = Time.at(epochmillis / 1000, (epochmillis % 1000) * 1000).utc
-          #event[@target] = Time.at(epochmillis / 1000.0).utc
+          event[@target] = LogStash::Timestamp.at(epochmillis / 1000, (epochmillis % 1000) * 1000)
 
           @logger.debug? && @logger.debug("Date parsing done", :value => value, :timestamp => event[@target])
           filter_matched(event)
@@ -217,7 +217,7 @@ def filter(event)
           # TODO(sissel): What do we do on a failure? Tag it like grok does?
           #raise e
         end # begin
-      end # fieldvalue.each 
+      end # fieldvalue.each
     end # @parsers.each
 
     return event
diff --git a/lib/logstash/filters/grok.rb b/lib/logstash/filters/grok.rb
index e992ccd5a2d..08a67688b64 100644
--- a/lib/logstash/filters/grok.rb
+++ b/lib/logstash/filters/grok.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/filters/base"
 require "logstash/namespace"
+require "logstash/environment"
 require "set"
 
 # Parse arbitrary text and structure it.
@@ -65,7 +66,7 @@
 #     }
 #     filter {
 #       grok {
-#         match => [ "message", "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}" ]
+#         match => { "message" => "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}" }
 #       }
 #     }
 #
@@ -99,7 +100,7 @@
 #
 #     (?<queue_id>[0-9A-F]{10,11})
 #
-# Alternately, you can create a custom patterns file. 
+# Alternately, you can create a custom patterns file.
 #
 # * Create a directory called `patterns` with a file in it called `extra`
 #   (the file name doesn't matter, but name it meaningfully for yourself)
@@ -119,7 +120,7 @@
 #     filter {
 #       grok {
 #         patterns_dir => "./patterns"
-#         match => [ "message", "%{SYSLOGBASE} %{POSTFIX_QUEUEID:queue_id}: %{GREEDYDATA:syslog_message}" ]
+#         match => { "message" => "%{SYSLOGBASE} %{POSTFIX_QUEUEID:queue_id}: %{GREEDYDATA:syslog_message}" }
 #       }
 #     }
 #
@@ -149,9 +150,13 @@ class LogStash::Filters::Grok < LogStash::Filters::Base
   # For example:
   #
   #     filter {
-  #       grok {
-  #         match => [ "message", "Duration: %{NUMBER:duration}" ]
-  #       }
+  #       grok { match => { "message" => "Duration: %{NUMBER:duration}" } }
+  #     }
+  #
+  # Alternatively, using the old array syntax:
+  #
+  #     filter {
+  #       grok { match => [ "message", "Duration: %{NUMBER:duration}" ] }
   #     }
   #
   config :match, :validate => :hash, :default => {}
@@ -202,10 +207,7 @@ class LogStash::Filters::Grok < LogStash::Filters::Base
   #
   #     filter {
   #       grok {
-  #         match => [ 
-  #           "message",
-  #           "%{SYSLOGBASE} %{DATA:message}"
-  #         ]
+  #         match => { "message" => "%{SYSLOGBASE} %{DATA:message}" }
   #         overwrite => [ "message" ]
   #       }
   #     }
@@ -216,7 +218,7 @@ class LogStash::Filters::Grok < LogStash::Filters::Base
 
   # Detect if we are running from a jarfile, pick the right path.
   @@patterns_path ||= Set.new
-  @@patterns_path += ["#{File.dirname(__FILE__)}/../../../patterns/*"]
+  @@patterns_path += [LogStash::Environment.pattern_path("*")]
 
   public
   def initialize(params)
@@ -255,16 +257,14 @@ def register
     @match.each do |field, patterns|
       patterns = [patterns] if patterns.is_a?(String)
 
-      if !@patterns.include?(field)
-        @patterns[field] = Grok::Pile.new
-        #@patterns[field].logger = @logger
-
-        add_patterns_from_files(@patternfiles, @patterns[field])
-      end
       @logger.info? and @logger.info("Grok compile", :field => field, :patterns => patterns)
       patterns.each do |pattern|
         @logger.debug? and @logger.debug("regexp: #{@type}/#{field}", :pattern => pattern)
-        @patterns[field].compile(pattern)
+        grok = Grok.new
+        grok.logger = @logger unless @logger.nil?
+        add_patterns_from_files(@patternfiles, grok)
+        grok.compile(pattern)
+        @patterns[field] << grok
       end
     end # @match.each
   end # def register
@@ -277,8 +277,8 @@ def filter(event)
     done = false
 
     @logger.debug? and @logger.debug("Running grok filter", :event => event);
-    @patterns.each do |field, grok|
-      if match(grok, field, event)
+    @patterns.each do |field, groks|
+      if match(groks, field, event)
         matched = true
         break if @break_on_match
       end
@@ -300,36 +300,38 @@ def filter(event)
   end # def filter
 
   private
-  def match(grok, field, event)
+  def match(groks, field, event)
     input = event[field]
     if input.is_a?(Array)
-      success = true
+      success = false
       input.each do |input|
-        grok, match = grok.match(input)
-        if match
-          match.each_capture do |capture, value|
-            handle(capture, value, event)
-          end
-        else
-          success = false
-        end
+        success |= match_against_groks(groks, input, event)
       end
       return success
-    #elsif input.is_a?(String)
     else
-      # Convert anything else to string (number, hash, etc)
-      grok, match = grok.match(input.to_s)
-      return false if !match
-
-      match.each_capture do |capture, value|
-        handle(capture, value, event)
-      end
-      return true
+      return match_against_groks(groks, input, event)
     end
   rescue StandardError => e
     @logger.warn("Grok regexp threw exception", :exception => e.message)
   end
 
+  private
+  def match_against_groks(groks, input, event)
+    matched = false
+    groks.each do |grok|
+      # Convert anything else to string (number, hash, etc)
+      match = grok.match(input.to_s)
+      if match
+        match.each_capture do |capture, value|
+          handle(capture, value, event)
+        end
+        matched = true
+        break if @break_on_match
+      end
+    end
+    return matched
+  end
+
   private
   def handle(capture, value, event)
     handler = @handlers[capture] ||= compile_capture_handler(capture)
@@ -342,7 +344,7 @@ def compile_capture_handler(capture)
     syntax, semantic, coerce = capture.split(":")
 
     # each_capture do |fullname, value|
-    #   capture_handlers[fullname].call(value, event) 
+    #   capture_handlers[fullname].call(value, event)
     # end
 
     code = []
@@ -350,7 +352,7 @@ def compile_capture_handler(capture)
     code << "lambda do |value, event|"
     #code << "  p :value => value, :event => event"
     if semantic.nil?
-      if @named_captures_only 
+      if @named_captures_only
         # Abort early if we are only keeping named (semantic) captures
         # and this capture has no semantic name.
         code << "  return"
@@ -390,12 +392,13 @@ def compile_capture_handler(capture)
   end # def compile_capture_handler
 
   private
-  def add_patterns_from_files(paths, pile)
-    paths.each { |path| add_patterns_from_file(path, pile) }
+  def add_patterns_from_files(paths, grok)
+    paths.each do |path|
+      if !File.exists?(path)
+        raise "Grok pattern file does not exist: #{path}"
+      end
+      grok.add_patterns_from_file(path)
+    end
   end # def add_patterns_from_files
 
-  private
-  def add_patterns_from_file(path, pile)
-    pile.add_patterns_from_file(path)
-  end # def add_patterns_from_file
 end # class LogStash::Filters::Grok
diff --git a/lib/logstash/filters/json.rb b/lib/logstash/filters/json.rb
index 2f6d471356b..688844ff707 100644
--- a/lib/logstash/filters/json.rb
+++ b/lib/logstash/filters/json.rb
@@ -1,10 +1,12 @@
 # encoding: utf-8
 require "logstash/filters/base"
 require "logstash/namespace"
+require "logstash/json"
+require "logstash/timestamp"
 
 # This is a JSON parsing filter. It takes an existing field which contains JSON and
 # expands it into an actual data structure within the Logstash event.
-# 
+#
 # By default it will place the parsed JSON in the root (top level) of the Logstash event, but this
 # filter can be configured to place the JSON into any arbitrary event field, using the
 # `target` configuration.
@@ -45,8 +47,6 @@ class LogStash::Filters::Json < LogStash::Filters::Base
   # NOTE: if the `target` field already exists, it will be overwritten!
   config :target, :validate => :string
 
-  TIMESTAMP = "@timestamp"
-
   public
   def register
     # Nothing to do here
@@ -60,6 +60,8 @@ def filter(event)
 
     return unless event.include?(@source)
 
+    # TODO(colin) this field merging stuff below should be handled in Event.
+
     source = event[@source]
     if @target.nil?
       # Default is to write to the root of the event.
@@ -75,18 +77,16 @@ def filter(event)
 
     begin
       # TODO(sissel): Note, this will not successfully handle json lists
-      # like your text is '[ 1,2,3 ]' JSON.parse gives you an array (correctly)
+      # like your text is '[ 1,2,3 ]' json parser gives you an array (correctly)
       # which won't merge into a hash. If someone needs this, we can fix it
       # later.
-      dest.merge!(JSON.parse(source))
+      dest.merge!(LogStash::Json.load(source))
 
       # If no target, we target the root of the event object. This can allow
-      # you to overwrite @timestamp. If so, let's parse it as a timestamp!
-      if !@target && event[TIMESTAMP].is_a?(String)
-        # This is a hack to help folks who are mucking with @timestamp during
-        # their json filter. You aren't supposed to do anything with
-        # "@timestamp" outside of the date filter, but nobody listens... ;)
-        event[TIMESTAMP] = Time.parse(event[TIMESTAMP]).utc
+      # you to overwrite @timestamp and this will typically happen for json
+      # LogStash Event deserialized here.
+      if !@target && event.timestamp.is_a?(String)
+        event.timestamp = LogStash::Timestamp.parse_iso8601(event.timestamp)
       end
 
       filter_matched(event)
diff --git a/lib/logstash/filters/metrics.rb b/lib/logstash/filters/metrics.rb
index e22431d74ba..9d27b4b4b53 100644
--- a/lib/logstash/filters/metrics.rb
+++ b/lib/logstash/filters/metrics.rb
@@ -158,7 +158,7 @@ def filter(event)
     return unless filter?(event)
 
     # TODO(piavlo): This should probably be moved to base filter class.
-    if @ignore_older_than > 0 && Time.now - event["@timestamp"] > @ignore_older_than
+    if @ignore_older_than > 0 && Time.now - event.timestamp.time > @ignore_older_than
       @logger.debug("Skipping metriks for old event", :event => event)
       return
     end
diff --git a/lib/logstash/filters/multiline.rb b/lib/logstash/filters/multiline.rb
index bcee5757aeb..76fcff420fd 100644
--- a/lib/logstash/filters/multiline.rb
+++ b/lib/logstash/filters/multiline.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/filters/base"
 require "logstash/namespace"
+require "logstash/environment"
 require "set"
 #
 # This filter will collapse multiline messages from a single source into one Logstash event.
@@ -102,7 +103,7 @@ class LogStash::Filters::Multiline < LogStash::Filters::Base
 
   # Detect if we are running from a jarfile, pick the right path.
   @@patterns_path = Set.new
-  @@patterns_path += ["#{File.dirname(__FILE__)}/../../../patterns/*"]
+  @@patterns_path += [LogStash::Environment.pattern_path("*")]
 
   public
   def initialize(config = {})
@@ -233,7 +234,7 @@ def flush
 
   def collapse_event!(event)
     event["message"] = event["message"].join("\n") if event["message"].is_a?(Array)
-    event["@timestamp"] = event["@timestamp"].first if event["@timestamp"].is_a?(Array)
+    event.timestamp = event.timestamp.first if event.timestamp.is_a?(Array)
     event
   end
 end # class LogStash::Filters::Multiline
diff --git a/lib/logstash/filters/mutate.rb b/lib/logstash/filters/mutate.rb
index 89075508c5c..50bcb949821 100644
--- a/lib/logstash/filters/mutate.rb
+++ b/lib/logstash/filters/mutate.rb
@@ -191,9 +191,10 @@ def register
         @logger.error("Invalid gsub configuration. gsub has to define 3 elements per config entry", :field => field, :needle => needle, :replacement => replacement)
         raise "Bad configuration, aborting."
       end
+
       @gsub_parsed << {
         :field        => field,
-        :needle       => Regexp.new(needle),
+        :needle       => (needle.index("%{").nil?? Regexp.new(needle): needle),
         :replacement  => replacement
       }
     end
@@ -303,7 +304,7 @@ def gsub(event)
                           "skipping", :field => field, :value => v)
             v
           else
-            v.gsub(needle, replacement)
+            gsub_dynamic_fields(event, v, needle, replacement)
           end
         end
       else
@@ -312,11 +313,21 @@ def gsub(event)
                         "skipping", :field => field, :value => event[field])
           next
         end
-        event[field] = event[field].gsub(needle, replacement)
+        event[field] = gsub_dynamic_fields(event, event[field], needle, replacement)
       end
     end # @gsub_parsed.each
   end # def gsub
 
+  private
+  def gsub_dynamic_fields(event, original, needle, replacement)
+    if needle.is_a? Regexp
+      original.gsub(needle, event.sprintf(replacement))
+    else
+      # we need to replace any dynamic fields
+      original.gsub(Regexp.new(event.sprintf(needle)), event.sprintf(replacement))
+    end
+  end
+
   private
   def uppercase(event)
     @uppercase.each do |field|
diff --git a/lib/logstash/filters/sleep.rb b/lib/logstash/filters/sleep.rb
index 3b83b644285..3309f86afdb 100644
--- a/lib/logstash/filters/sleep.rb
+++ b/lib/logstash/filters/sleep.rb
@@ -12,7 +12,7 @@ class LogStash::Filters::Sleep < LogStash::Filters::Base
 
   # The length of time to sleep, in seconds, for every event.
   #
-  # This can be a number (eg, 0.5), or a string (eg, "%{foo}") 
+  # This can be a number (eg, 0.5), or a string (eg, "%{foo}")
   # The second form (string with a field value) is useful if
   # you have an attribute of your event that you want to use
   # to indicate the amount of time to sleep.
@@ -33,7 +33,7 @@ class LogStash::Filters::Sleep < LogStash::Filters::Base
   #
   #     filter {
   #       sleep {
-  #         time => "1"   # Sleep 1 second 
+  #         time => "1"   # Sleep 1 second
   #         every => 10   # on every 10th event
   #       }
   #     }
@@ -89,7 +89,7 @@ def filter(event)
     end
 
     if @replay
-      clock = event["@timestamp"].to_f
+      clock = event.timestamp.to_f
       if @last_clock
         delay = clock - @last_clock
         time = delay/time
diff --git a/lib/logstash/inputs/base.rb b/lib/logstash/inputs/base.rb
index 68afca36f30..22bda363343 100644
--- a/lib/logstash/inputs/base.rb
+++ b/lib/logstash/inputs/base.rb
@@ -118,4 +118,20 @@ def decorate(event)
       event[field] = value
     end
   end
+
+  protected
+  def fix_streaming_codecs
+    require "logstash/codecs/plain"
+    require "logstash/codecs/line"
+    require "logstash/codecs/json"
+    require "logstash/codecs/json_lines"
+    case @codec
+      when LogStash::Codecs::Plain
+        @logger.info("Automatically switching from #{@codec.class.config_name} to line codec", :plugin => self.class.config_name)
+        @codec = LogStash::Codecs::Line.new("charset" => @codec.charset)
+      when LogStash::Codecs::JSON
+        @logger.info("Automatically switching from #{@codec.class.config_name} to json_lines codec", :plugin => self.class.config_name)
+        @codec = LogStash::Codecs::JSONLines.new("charset" => @codec.charset)
+    end
+  end
 end # class LogStash::Inputs::Base
diff --git a/lib/logstash/inputs/collectd.rb b/lib/logstash/inputs/collectd.rb
index f5f6afb5542..59eb3b4adfd 100644
--- a/lib/logstash/inputs/collectd.rb
+++ b/lib/logstash/inputs/collectd.rb
@@ -2,6 +2,7 @@
 require "date"
 require "logstash/inputs/base"
 require "logstash/namespace"
+require "logstash/timestamp"
 require "socket"
 require "tempfile"
 require "time"
@@ -107,7 +108,7 @@ class LogStash::Inputs::Collectd < LogStash::Inputs::Base
   def initialize(params)
     super
     BasicSocket.do_not_reverse_lookup = true
-    @timestamp = Time.now().utc
+    @timestamp = LogStash::Timestamp.now
     @collectd = {}
     @types = {}
   end # def initialize
diff --git a/lib/logstash/inputs/elasticsearch.rb b/lib/logstash/inputs/elasticsearch.rb
index d3e8b211e84..d34630eaf95 100644
--- a/lib/logstash/inputs/elasticsearch.rb
+++ b/lib/logstash/inputs/elasticsearch.rb
@@ -2,6 +2,7 @@
 require "logstash/inputs/base"
 require "logstash/namespace"
 require "logstash/util/socket_peer"
+require "logstash/json"
 
 # Read from an Elasticsearch cluster, based on search query results.
 # This is useful for replaying test logs, reindexing, etc.
@@ -55,15 +56,16 @@ class LogStash::Inputs::Elasticsearch < LogStash::Inputs::Base
   def register
     require "ftw"
     @agent = FTW::Agent.new
+
     params = {
       "q" => @query,
       "scroll" => @scroll,
       "size" => "#{@size}",
     }
-
     params['search_type'] = "scan" if @scan
 
-    @url = "http://#{@host}:#{@port}/#{@index}/_search?#{encode(params)}"
+    @search_url = "http://#{@host}:#{@port}/#{@index}/_search?#{encode(params)}"
+    @scroll_url = "http://#{@host}:#{@port}/_search/scroll?#{encode({"scroll" => @scroll})}"
   end # def register
 
   private
@@ -73,42 +75,41 @@ def encode(hash)
     end.join("&")
   end # def encode
 
-  public
-  def run(output_queue)
+  private
+  def execute_search_request
+    response = @agent.get!(@search_url)
+    json = ""
+    response.read_body { |c| json << c }
+    json
+  end
 
-    # Execute the search request
-    response = @agent.get!(@url)
+  private
+  def execute_scroll_request(scroll_id)
+    response = @agent.post!(@scroll_url, :body => scroll_id)
     json = ""
     response.read_body { |c| json << c }
-    result = JSON.parse(json)
+    json
+  end
+
+  public
+  def run(output_queue)
+    result = LogStash::Json.load(execute_search_request)
     scroll_id = result["_scroll_id"]
 
     # When using the search_type=scan we don't get an initial result set.
     # So we do it here.
     if @scan
-
-      scroll_params = {
-        "scroll" => @scroll
-      }
-
-      scroll_url = "http://#{@host}:#{@port}/_search/scroll?#{encode(scroll_params)}"
-      response = @agent.post!(scroll_url, :body => scroll_id)
-      json = ""
-      response.read_body { |c| json << c }
-      result = JSON.parse(json)
-
+      result = LogStash::Json.load(execute_scroll_request(scroll_id))
     end
 
-    while true
+    loop do
       break if result.nil?
       hits = result["hits"]["hits"]
       break if hits.empty?
 
       hits.each do |hit|
-        event = hit["_source"]
-
         # Hack to make codecs work
-        @codec.decode(event.to_json) do |event|
+        @codec.decode(LogStash::Json.dump(hit["_source"])) do |event|
           decorate(event)
           output_queue << event
         end
@@ -118,15 +119,7 @@ def run(output_queue)
       scroll_id = result["_scroll_id"]
 
       # Fetch the next result set
-      scroll_params = {
-        "scroll" => @scroll
-      }
-      scroll_url = "http://#{@host}:#{@port}/_search/scroll?#{encode(scroll_params)}"
-
-      response = @agent.post!(scroll_url, :body => scroll_id)
-      json = ""
-      response.read_body { |c| json << c }
-      result = JSON.parse(json)
+      result = LogStash::Json.load(execute_scroll_request(scroll_id))
 
       if result["error"]
         @logger.warn(result["error"], :request => scroll_url)
diff --git a/lib/logstash/inputs/eventlog.rb b/lib/logstash/inputs/eventlog.rb
index 92c0790fc00..ac6e7bd9303 100644
--- a/lib/logstash/inputs/eventlog.rb
+++ b/lib/logstash/inputs/eventlog.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/inputs/base"
 require "logstash/namespace"
+require "logstash/timestamp"
 require "socket"
 
 # This input will pull events from a (http://msdn.microsoft.com/en-us/library/windows/desktop/bb309026%28v=vs.85%29.aspx)[Windows Event Log].
@@ -60,14 +61,14 @@ def run(queue)
           "host" => @hostname,
           "path" => @logfile,
           "type" => @type,
-          "@timestamp" => timestamp
+          LogStash::Event::TIMESTAMP => timestamp
         )
 
         %w{Category CategoryString ComputerName EventCode EventIdentifier
             EventType Logfile Message RecordNumber SourceName
             TimeGenerated TimeWritten Type User
         }.each{
-            |property| e[property] = event.send property 
+            |property| e[property] = event.send property
         }
 
         if RUBY_PLATFORM == "java"
@@ -111,7 +112,7 @@ def to_timestamp(wmi_time)
     # parse the utc date string
     /(?<w_date>\d{8})(?<w_time>\d{6})\.\d{6}(?<w_sign>[\+-])(?<w_diff>\d{3})/ =~ wmi_time
     result = "#{w_date}T#{w_time}#{w_sign}"
-    # the offset is represented by the difference, in minutes, 
+    # the offset is represented by the difference, in minutes,
     # between the local time zone and Greenwich Mean Time (GMT).
     if w_diff.to_i > 0
       # calculate the timezone offset in hours and minutes
@@ -121,8 +122,8 @@ def to_timestamp(wmi_time)
     else
       result.concat("0000")
     end
-  
-    return DateTime.strptime(result, "%Y%m%dT%H%M%S%z").iso8601
+
+    return LogStash::Timestamp.new(DateTime.strptime(result, "%Y%m%dT%H%M%S%z").to_time)
   end
 end # class LogStash::Inputs::EventLog
 
diff --git a/lib/logstash/inputs/gelf.rb b/lib/logstash/inputs/gelf.rb
index 777c3ce87da..741c8b3a16a 100644
--- a/lib/logstash/inputs/gelf.rb
+++ b/lib/logstash/inputs/gelf.rb
@@ -2,6 +2,8 @@
 require "date"
 require "logstash/inputs/base"
 require "logstash/namespace"
+require "logstash/json"
+require "logstash/timestamp"
 require "socket"
 
 # This input will read GELF messages as events over the network,
@@ -89,10 +91,10 @@ def udp_listener(output_queue)
       # Gelfd parser outputs null if it received and cached a non-final chunk
       next if data.nil?
 
-      event = LogStash::Event.new(JSON.parse(data))
+      event = LogStash::Event.new(LogStash::Json.load(data))
       event["source_host"] = client[3]
       if event["timestamp"].is_a?(Numeric)
-        event["@timestamp"] = Time.at(event["timestamp"]).gmtime
+        event.timestamp = LogStash::Timestamp.at(event["timestamp"])
         event.remove("timestamp")
       end
       remap_gelf(event) if @remap
diff --git a/lib/logstash/inputs/generator.rb b/lib/logstash/inputs/generator.rb
index 18813f2b3fb..45f50ed77fe 100644
--- a/lib/logstash/inputs/generator.rb
+++ b/lib/logstash/inputs/generator.rb
@@ -39,7 +39,7 @@ class LogStash::Inputs::Generator < LogStash::Inputs::Threadable
   #       }
   #     }
   #
-  # The above will emit "line 1" then "line 2" then "line", then "line 1", etc... 
+  # The above will emit "line 1" then "line 2" then "line", then "line 1", etc...
   config :lines, :validate => :array
 
   # Set how many messages should be generated.
@@ -51,7 +51,6 @@ class LogStash::Inputs::Generator < LogStash::Inputs::Threadable
   def register
     @host = Socket.gethostname
     @count = @count.first if @count.is_a?(Array)
-    @lines = [@message] if @lines.nil?
   end # def register
 
   def run(queue)
@@ -62,6 +61,7 @@ def run(queue)
       @message = $stdin.readline
       @logger.debug("Generator line read complete", :message => @message)
     end
+    @lines = [@message] if @lines.nil?
 
     while !finished? && (@count <= 0 || number < @count)
       @lines.each do |line|
diff --git a/lib/logstash/inputs/graphite.rb b/lib/logstash/inputs/graphite.rb
index e590cc3bdee..c2f2f25f14c 100644
--- a/lib/logstash/inputs/graphite.rb
+++ b/lib/logstash/inputs/graphite.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/inputs/tcp"
 require "logstash/namespace"
+require "logstash/timestamp"
 
 # Receive graphite metrics. This plugin understands the text-based graphite
 # carbon protocol. Both 'N' and specific-timestamp forms are supported, example:
@@ -18,8 +19,6 @@ class LogStash::Inputs::Graphite < LogStash::Inputs::Tcp
   config_name "graphite"
   milestone 1
 
-  ISO8601_STRFTIME = "%04d-%02d-%02dT%02d:%02d:%02d.%06d%+03d:00".freeze
-
   public
   def run(output_queue)
     @queue = output_queue
@@ -33,7 +32,7 @@ def <<(event)
     event[name] = value.to_f
 
     if time != "N"
-      event["@timestamp"] = Time.at(time.to_i).gmtime
+      event.timestamp = LogStash::Timestamp.at(time.to_i)
     end
 
     @queue << event
diff --git a/lib/logstash/inputs/imap.rb b/lib/logstash/inputs/imap.rb
index ec2c9bccbe9..e980c9e9d4e 100644
--- a/lib/logstash/inputs/imap.rb
+++ b/lib/logstash/inputs/imap.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/inputs/base"
 require "logstash/namespace"
+require "logstash/timestamp"
 require "stud/interval"
 require "socket" # for Socket.gethostname
 
@@ -11,7 +12,6 @@
 class LogStash::Inputs::IMAP < LogStash::Inputs::Base
   config_name "imap"
   milestone 1
-  ISO8601_STRFTIME = "%04d-%02d-%02dT%02d:%02d:%02d.%06d%+03d:00".freeze
 
   default :codec, "plain"
 
@@ -106,7 +106,7 @@ def parse_mail(mail)
       # event = LogStash::Event.new("message" => message)
 
       # Use the 'Date' field as the timestamp
-      event["@timestamp"] = mail.date.to_time.gmtime
+      event.timestamp = LogStash::Timestamp.new(mail.date.to_time)
 
       # Add fields: Add message.header_fields { |h| h.name=> h.value }
       mail.header_fields.each do |header|
diff --git a/lib/logstash/inputs/pipe.rb b/lib/logstash/inputs/pipe.rb
index 143b8ef1418..933c602102c 100644
--- a/lib/logstash/inputs/pipe.rb
+++ b/lib/logstash/inputs/pipe.rb
@@ -46,6 +46,8 @@ def run(queue)
             queue << event
           end
         end
+      rescue LogStash::ShutdownSignal => e
+        break
       rescue Exception => e
         @logger.error("Exception while running command", :e => e, :backtrace => e.backtrace)
       end
diff --git a/lib/logstash/inputs/sqs.rb b/lib/logstash/inputs/sqs.rb
index c0d3e756083..8b0599aa6ef 100644
--- a/lib/logstash/inputs/sqs.rb
+++ b/lib/logstash/inputs/sqs.rb
@@ -1,12 +1,13 @@
 # encoding: utf-8
 require "logstash/inputs/threadable"
 require "logstash/namespace"
+require "logstash/timestamp"
 require "logstash/plugin_mixins/aws_config"
 require "digest/sha2"
 
 # Pull events from an Amazon Web Services Simple Queue Service (SQS) queue.
 #
-# SQS is a simple, scalable queue system that is part of the 
+# SQS is a simple, scalable queue system that is part of the
 # Amazon Web Services suite of tools.
 #
 # Although SQS is similar to other queuing systems like AMQP, it
@@ -52,7 +53,7 @@
 #           ]
 #         }
 #       ]
-#     } 
+#     }
 #
 # See http://aws.amazon.com/iam/ for more details on setting up AWS identities.
 #
@@ -124,7 +125,7 @@ def run(output_queue)
                 event[@md5_field] = message.md5
               end
               if @sent_timestamp_field
-                event[@sent_timestamp_field] = message.sent_timestamp.utc
+                event[@sent_timestamp_field] = LogStash::Timestamp.new(message.sent_timestamp).utc
               end
               @logger.debug? && @logger.debug("Processed SQS message", :message_id => message.id, :message_md5 => message.md5, :sent_timestamp => message.sent_timestamp, :queue => @queue)
               output_queue << event
diff --git a/lib/logstash/inputs/stdin.rb b/lib/logstash/inputs/stdin.rb
index bc9756fffd7..d065e2b09ee 100644
--- a/lib/logstash/inputs/stdin.rb
+++ b/lib/logstash/inputs/stdin.rb
@@ -16,6 +16,7 @@ class LogStash::Inputs::Stdin < LogStash::Inputs::Base
   public
   def register
     @host = Socket.gethostname
+    fix_streaming_codecs
   end # def register
 
   def run(queue) 
diff --git a/lib/logstash/inputs/tcp.rb b/lib/logstash/inputs/tcp.rb
index 07cdf46738b..f328be16022 100644
--- a/lib/logstash/inputs/tcp.rb
+++ b/lib/logstash/inputs/tcp.rb
@@ -62,6 +62,13 @@ def register
     require "socket"
     require "timeout"
     require "openssl"
+
+    # monkey patch TCPSocket and SSLSocket to include socket peer
+    TCPSocket.module_eval{include ::LogStash::Util::SocketPeer}
+    OpenSSL::SSL::SSLSocket.module_eval{include ::LogStash::Util::SocketPeer}
+
+    fix_streaming_codecs
+
     if @ssl_enable
       @ssl_context = OpenSSL::SSL::SSLContext.new
       @ssl_context.cert = OpenSSL::X509::Certificate.new(File.read(@ssl_cert))
@@ -85,8 +92,7 @@ def register
       begin
         @server_socket = TCPServer.new(@host, @port)
       rescue Errno::EADDRINUSE
-        @logger.error("Could not start TCP server: Address in use",
-                      :host => @host, :port => @port)
+        @logger.error("Could not start TCP server: Address in use", :host => @host, :port => @port)
         raise
       end
       if @ssl_enable
@@ -99,8 +105,7 @@ def register
   def handle_socket(socket, client_address, output_queue, codec)
     while true
       buf = nil
-      # NOTE(petef): the timeout only hits after the line is read
-      # or socket dies
+      # NOTE(petef): the timeout only hits after the line is read or socket dies
       # TODO(sissel): Why do we have a timeout here? What's the point?
       if @data_timeout == -1
         buf = read(socket)
@@ -115,14 +120,16 @@ def handle_socket(socket, client_address, output_queue, codec)
         decorate(event)
         output_queue << event
       end
-    end # loop do
+    end # loop
   rescue EOFError
-    @logger.debug("Connection closed", :client => socket.peer)
+    @logger.debug? && @logger.debug("Connection closed", :client => socket.peer)
+  rescue Errno::ECONNRESET
+    @logger.debug? && @logger.debug("Connection reset by peer", :client => socket.peer)
   rescue => e
-    @logger.debug("An error occurred. Closing connection",
-                  :client => socket.peer, :exception => e, :backtrace => e.backtrace)
+    @logger.error("An error occurred. Closing connection", :client => socket.peer, :exception => e, :backtrace => e.backtrace)
   ensure
-    socket.close rescue IOError nil
+    socket.close rescue nil
+
     codec.respond_to?(:flush) && codec.flush do |event|
       event["host"] ||= client_address
       event["sslsubject"] ||= socket.peer_cert.subject if @ssl_enable && @ssl_verify
@@ -131,6 +138,20 @@ def handle_socket(socket, client_address, output_queue, codec)
     end
   end
 
+  private
+  def client_thread(output_queue, socket)
+    Thread.new(output_queue, socket) do |q, s|
+      begin
+        @logger.debug? && @logger.debug("Accepted connection", :client => s.peer, :server => "#{@host}:#{@port}")
+        handle_socket(s, s.peer, q, @codec.clone)
+      rescue Interrupted
+        s.close rescue nil
+      ensure
+        @client_threads_lock.synchronize{@client_threads.delete(Thread.current)}
+      end
+    end
+  end
+
   private
   def server?
     @mode == "server"
@@ -153,36 +174,29 @@ def run(output_queue)
   def run_server(output_queue)
     @thread = Thread.current
     @client_threads = []
-    loop do
-      # Start a new thread for each connection.
+    @client_threads_lock = Mutex.new
+
+    while true
       begin
-        @client_threads << Thread.start(@server_socket.accept) do |s|
-          # TODO(sissel): put this block in its own method.
-
-          # monkeypatch a 'peer' method onto the socket.
-          s.instance_eval { class << self; include ::LogStash::Util::SocketPeer end }
-          @logger.debug("Accepted connection", :client => s.peer,
-                        :server => "#{@host}:#{@port}")
-          begin
-            handle_socket(s, s.peer, output_queue, @codec.clone)
-          rescue Interrupted
-            s.close rescue nil
-          end
-        end # Thread.start
+        socket = @server_socket.accept
+        # start a new thread for each connection.
+        @client_threads_lock.synchronize{@client_threads << client_thread(output_queue, socket)}
       rescue OpenSSL::SSL::SSLError => ssle
         # NOTE(mrichar1): This doesn't return a useful error message for some reason
-        @logger.error("SSL Error", :exception => ssle,
-                      :backtrace => ssle.backtrace)
+        @logger.error("SSL Error", :exception => ssle, :backtrace => ssle.backtrace)
       rescue IOError, LogStash::ShutdownSignal
         if @interrupted
-          # Intended shutdown, get out of the loop
-          @server_socket.close
-          @client_threads.each do |thread|
-            thread.raise(LogStash::ShutdownSignal)
+          @server_socket.close rescue nil
+
+          threads = @client_threads_lock.synchronize{@client_threads.dup}
+          threads.each do |thread|
+            thread.raise(LogStash::ShutdownSignal) if thread.alive?
           end
+
+          # intended shutdown, get out of the loop
           break
         else
-          # Else it was a genuine IOError caused by something else, so propagate it up..
+          # it was a genuine IOError, propagate it up
           raise
         end
       end
@@ -193,7 +207,7 @@ def run_server(output_queue)
     @server_socket.close rescue nil
   end # def run_server
 
-  def run_client(output_queue) 
+  def run_client(output_queue)
     @thread = Thread.current
     while true
       client_socket = TCPSocket.new(@host, @port)
@@ -202,19 +216,17 @@ def run_client(output_queue)
         begin
           client_socket.connect
         rescue OpenSSL::SSL::SSLError => ssle
-          @logger.error("SSL Error", :exception => ssle,
-                        :backtrace => ssle.backtrace)
+          @logger.error("SSL Error", :exception => ssle, :backtrace => ssle.backtrace)
           # NOTE(mrichar1): Hack to prevent hammering peer
           sleep(5)
           next
         end
       end
-      client_socket.instance_eval { class << self; include ::LogStash::Util::SocketPeer end }
       @logger.debug("Opened connection", :client => "#{client_socket.peer}")
       handle_socket(client_socket, client_socket.peer, output_queue, @codec.clone)
     end # loop
   ensure
-    client_socket.close
+    client_socket.close rescue nil
   end # def run
 
   public
diff --git a/lib/logstash/inputs/twitter.rb b/lib/logstash/inputs/twitter.rb
index fecd0755c7a..333b21a5661 100644
--- a/lib/logstash/inputs/twitter.rb
+++ b/lib/logstash/inputs/twitter.rb
@@ -1,7 +1,7 @@
 # encoding: utf-8
 require "logstash/inputs/base"
 require "logstash/namespace"
-require "json"
+require "logstash/timestamp"
 
 # Read events from the twitter streaming api.
 class LogStash::Inputs::Twitter < LogStash::Inputs::Base
@@ -32,7 +32,7 @@ class LogStash::Inputs::Twitter < LogStash::Inputs::Base
   # will create an oauth token and secret bound to your account and that
   # application.
   config :oauth_token, :validate => :string, :required => true
-  
+
   # Your oauth token secret.
   #
   # To get this, login to twitter with whatever account you want,
@@ -67,12 +67,11 @@ def run(queue)
     @client.filter(:track => @keywords.join(",")) do |tweet|
       @logger.info? && @logger.info("Got tweet", :user => tweet.user.screen_name, :text => tweet.text)
       if @full_tweet
-        event = LogStash::Event.new(
-          tweet.to_hash.merge("@timestamp" => tweet.created_at.gmtime)
-        )
+        event = LogStash::Event.new(tweet.to_hash)
+        event.timestamp = LogStash::Timestamp.new(tweet.created_at)
       else
         event = LogStash::Event.new(
-          "@timestamp" => tweet.created_at.gmtime,
+          LogStash::Event::TIMESTAMP => LogStash::Timestamp.new(tweet.created_at),
           "message" => tweet.full_text,
           "user" => tweet.user.screen_name,
           "client" => tweet.source,
diff --git a/lib/logstash/java_integration.rb b/lib/logstash/java_integration.rb
new file mode 100644
index 00000000000..2bfeb3e81d2
--- /dev/null
+++ b/lib/logstash/java_integration.rb
@@ -0,0 +1,41 @@
+require "java"
+
+# this is mainly for usage with JrJackson json parsing in :raw mode which genenerates
+# Java::JavaUtil::ArrayList and Java::JavaUtil::LinkedHashMap native objects for speed.
+# these object already quacks like their Ruby equivalents Array and Hash but they will
+# not test for is_a?(Array) or is_a?(Hash) and we do not want to include tests for
+# both classes everywhere. see LogStash::JSon.
+
+class Java::JavaUtil::ArrayList
+  # have ArrayList objects report is_a?(Array) == true
+  def is_a?(clazz)
+    return true if clazz == Array
+    super
+  end
+end
+
+class Java::JavaUtil::LinkedHashMap
+  # have LinkedHashMap objects report is_a?(Array) == true
+  def is_a?(clazz)
+    return true if clazz == Hash
+    super
+  end
+end
+
+class Array
+  # enable class equivalence between Array and ArrayList
+  # so that ArrayList will work with case o when Array ...
+  def self.===(other)
+    return true if other.is_a?(Java::JavaUtil::ArrayList)
+    super
+  end
+end
+
+class Hash
+  # enable class equivalence between Hash and LinkedHashMap
+  # so that LinkedHashMap will work with case o when Hash ...
+  def self.===(other)
+    return true if other.is_a?(Java::JavaUtil::LinkedHashMap)
+    super
+  end
+end
diff --git a/lib/logstash/json.rb b/lib/logstash/json.rb
new file mode 100644
index 00000000000..d7fce4397a6
--- /dev/null
+++ b/lib/logstash/json.rb
@@ -0,0 +1,53 @@
+# encoding: utf-8
+require "logstash/environment"
+require "logstash/errors"
+if LogStash::Environment.jruby?
+  require "jrjackson"
+  require "logstash/java_integration"
+else
+  require  "oj"
+end
+
+module LogStash
+  module Json
+    class ParserError < LogStash::Error; end
+    class GeneratorError < LogStash::Error; end
+
+    extend self
+
+    ### MRI
+
+    def mri_load(data)
+      Oj.load(data)
+    rescue Oj::ParseError => e
+      raise LogStash::Json::ParserError.new(e.message)
+    end
+
+    def mri_dump(o)
+      Oj.dump(o, :mode => :compat, :use_to_json => true)
+    rescue => e
+      raise LogStash::Json::GeneratorError.new(e.message)
+    end
+
+    ### JRuby
+
+    def jruby_load(data)
+      JrJackson::Raw.parse_raw(data)
+    rescue JrJackson::ParseError => e
+      raise LogStash::Json::ParserError.new(e.message)
+    end
+
+    def jruby_dump(o)
+      # test for enumerable here to work around an omission in JrJackson::Json.dump to
+      # also look for Java::JavaUtil::ArrayList, see TODO submit issue
+      o.is_a?(Enumerable) ? JrJackson::Raw.generate(o) : JrJackson::Json.dump(o)
+    rescue => e
+      raise LogStash::Json::GeneratorError.new(e.message)
+    end
+
+    prefix = LogStash::Environment.jruby? ? "jruby" : "mri"
+    alias_method :load, "#{prefix}_load".to_sym
+    alias_method :dump, "#{prefix}_dump".to_sym
+
+  end
+end
diff --git a/lib/logstash/outputs/csv.rb b/lib/logstash/outputs/csv.rb
index d35cb67c093..42daa155bce 100644
--- a/lib/logstash/outputs/csv.rb
+++ b/lib/logstash/outputs/csv.rb
@@ -1,6 +1,7 @@
 require "csv"
 require "logstash/namespace"
 require "logstash/outputs/file"
+require "logstash/json"
 
 # CSV output.
 #
@@ -17,8 +18,8 @@ class LogStash::Outputs::CSV < LogStash::Outputs::File
   # If a field does not exist on the event, an empty string will be written.
   # Supports field reference syntax eg: `fields => ["field1", "[nested][field]"]`.
   config :fields, :validate => :array, :required => true
-  
-  # Options for CSV output. This is passed directly to the Ruby stdlib to\_csv function. 
+
+  # Options for CSV output. This is passed directly to the Ruby stdlib to\_csv function.
   # Full documentation is available here: [http://ruby-doc.org/stdlib-2.0.0/libdoc/csv/rdoc/index.html].
   # A typical use case would be to use alternative column or row seperators eg: `csv_options => {"col_sep" => "\t" "row_sep" => "\r\n"}` gives tab seperated data with windows line endings
   config :csv_options, :validate => :hash, :required => false, :default => Hash.new
@@ -26,7 +27,7 @@ class LogStash::Outputs::CSV < LogStash::Outputs::File
   public
   def register
     super
-    @csv_options = Hash[@csv_options.map{|(k,v)|[k.to_sym, v]}]
+    @csv_options = Hash[@csv_options.map{|(k, v)|[k.to_sym, v]}]
   end
 
   public
@@ -44,12 +45,7 @@ def receive(event)
   private
   def get_value(name, event)
     val = event[name]
-    case val
-      when Hash
-        return val.to_json
-      else
-        return val
-    end
+    val.is_a?(Hash) ? LogStash::Json.dump(val) : val
   end
 end # class LogStash::Outputs::CSV
 
diff --git a/lib/logstash/outputs/elasticsearch.rb b/lib/logstash/outputs/elasticsearch.rb
index 6b92e346626..d42eb9eee4c 100644
--- a/lib/logstash/outputs/elasticsearch.rb
+++ b/lib/logstash/outputs/elasticsearch.rb
@@ -2,6 +2,7 @@
 require "logstash/namespace"
 require "logstash/environment"
 require "logstash/outputs/base"
+require "logstash/json"
 require "stud/buffer"
 require "socket" # for Socket.gethostname
 
@@ -269,20 +270,14 @@ def register
   public
   def get_template
     if @template.nil?
-      if File.exists?("elasticsearch-template.json")
-        @template = "elasticsearch-template.json"
-      else
-        path = File.join(File.dirname(__FILE__), "elasticsearch/elasticsearch-template.json")
-        if File.exists?(path)
-          @template = path
-        else
-          raise "You must specify 'template => ...' in your elasticsearch_http output"
-        end
+      @template = LogStash::Environment.plugin_path("outputs/elasticsearch/elasticsearch-template.json")
+      if !File.exists?(@template)
+        raise "You must specify 'template => ...' in your elasticsearch output (I looked for '#{@template}')"
       end
     end
     template_json = IO.read(@template).gsub(/\n/,'')
     @logger.info("Using mapping template", :template => template_json)
-    return JSON.parse(template_json)
+    return LogStash::Json.load(template_json)
   end # def get_template
 
   protected
diff --git a/lib/logstash/outputs/elasticsearch/protocol.rb b/lib/logstash/outputs/elasticsearch/protocol.rb
index 876598ec454..1a860e945c2 100644
--- a/lib/logstash/outputs/elasticsearch/protocol.rb
+++ b/lib/logstash/outputs/elasticsearch/protocol.rb
@@ -80,7 +80,7 @@ def bulk(actions)
           bulk_ftw(actions)
         end
       end
-      
+
       def bulk_esruby(actions)
         @client.bulk(:body => actions.collect do |action, args, source|
           if source
@@ -97,9 +97,9 @@ def bulk_ftw(actions)
         body = actions.collect do |action, args, source|
           header = { action => args }
           if source
-            next [ header.to_json, NEWLINE, source.to_json, NEWLINE ]
+            next [ LogStash::Json.dump(header), NEWLINE, LogStash::Json.dump(source), NEWLINE ]
           else
-            next [ header.to_json, NEWLINE ]
+            next [ LogStash::Json.dump(header), NEWLINE ]
           end
         end.flatten.join("")
         begin
@@ -170,7 +170,7 @@ def setup(options={})
 
         @settings.put("node.client", true)
         @settings.put("http.enabled", false)
-        
+
         if options[:client_settings]
           options[:client_settings].each do |key, value|
             @settings.put(key, value)
@@ -182,7 +182,7 @@ def setup(options={})
 
       def hosts(options)
         if options[:port].to_s =~ /^\d+-\d+$/
-          # port ranges are 'host[port1-port2]' according to 
+          # port ranges are 'host[port1-port2]' according to
           # http://www.elasticsearch.org/guide/reference/modules/discovery/zen/
           # However, it seems to only query the first port.
           # So generate our own list of unicast hosts to scan.
@@ -234,7 +234,7 @@ def template_exists?(name)
 
       def template_put(name, template)
         request = org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequestBuilder.new(@client.admin.indices, name)
-        request.setSource(template.to_json)
+        request.setSource(LogStash::Json.dump(template))
 
         # execute the request and get the response, if it fails, we'll get an exception.
         request.get
diff --git a/lib/logstash/outputs/elasticsearch_http.rb b/lib/logstash/outputs/elasticsearch_http.rb
index 32a85d0f27f..124b739050a 100644
--- a/lib/logstash/outputs/elasticsearch_http.rb
+++ b/lib/logstash/outputs/elasticsearch_http.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/namespace"
 require "logstash/outputs/base"
+require "logstash/json"
 require "stud/buffer"
 
 # This output lets you store logs in Elasticsearch.
@@ -123,7 +124,7 @@ def register
       elsif response.status == 200
         begin
           response.read_body { |c| json << c }
-          results = JSON.parse(json)
+          results = LogStash::Json.load(json)
         rescue Exception => e
           @logger.error("Error parsing JSON", :json => json, :results => results.to_s, :error => e.to_s)
           raise "Exception in parsing JSON", e
@@ -173,17 +174,14 @@ def template_action(command)
   public
   def get_template_json
     if @template.nil?
-      if File.exists?("elasticsearch-template.json")
-        @template = "elasticsearch-template.json"
-      elsif File.exists?("lib/logstash/outputs/elasticsearch/elasticsearch-template.json")
-        @template = "lib/logstash/outputs/elasticsearch/elasticsearch-template.json"
-      else
-        raise "You must specify 'template => ...' in your elasticsearch_http output"
+      @template = LogStash::Environment.plugin_path("outputs/elasticsearch/elasticsearch-template.json")
+      if !File.exists?(@template)
+        raise "You must specify 'template => ...' in your elasticsearch_http output (I looked for '#{@template}')"
       end
     end
     @template_json = IO.read(@template).gsub(/\n/,'')
     @logger.info("Using mapping template", :template => @template_json)
-  end # def get_template
+  end # def get_template_json
 
   public
   def receive(event)
@@ -207,7 +205,7 @@ def flush(events, teardown=false)
       header = { "index" => { "_index" => index, "_type" => type } }
       header["index"]["_id"] = event.sprintf(@document_id) if !@document_id.nil?
 
-      [ header.to_json, newline, event.to_json, newline ]
+      [ LogStash::Json.dump(header), newline, event.to_json, newline ]
     end.flatten
 
     post(body.join(""))
diff --git a/lib/logstash/outputs/elasticsearch_river.rb b/lib/logstash/outputs/elasticsearch_river.rb
index c3ac9c6b599..365d2b53ee9 100644
--- a/lib/logstash/outputs/elasticsearch_river.rb
+++ b/lib/logstash/outputs/elasticsearch_river.rb
@@ -2,7 +2,7 @@
 require "logstash/environment"
 require "logstash/namespace"
 require "logstash/outputs/base"
-require "json"
+require "logstash/json"
 require "uri"
 require "net/http"
 
@@ -146,7 +146,7 @@ def prepare_river
       @logger.info("ElasticSearch using river", :config => river_config)
       Net::HTTP.start(@es_host, @es_port) do |http|
         req = Net::HTTP::Put.new(api_path)
-        req.body = river_config.to_json
+        req.body = LogStash::Json.dump(river_config)
         response = http.request(req)
         response.value() # raise an exception if error
         @logger.info("River created: #{response.body}")
@@ -173,7 +173,7 @@ def check_river_status
           req = Net::HTTP::Get.new(@status_path)
           response = http.request(req)
           response.value
-          status = JSON.parse(response.body)
+          status = LogStash::Json.load(response.body)
           @logger.debug("Checking ES river status", :status => status)
           if status["_source"]["error"]
             reason = "ES river status: #{status["_source"]["error"]}"
@@ -201,6 +201,6 @@ def receive(event)
       header["index"]["_id"] = event.sprintf(@document_id)
     end
 
-    @mq.publish_serialized(header.to_json + "\n" + event.to_json + "\n")
+    @mq.publish_serialized(LogStash::Json.dump(header) + "\n" + event.to_json + "\n")
   end # def receive
 end # LogStash::Outputs::ElasticSearchRiver
diff --git a/lib/logstash/outputs/gelf.rb b/lib/logstash/outputs/gelf.rb
index e6c07255898..1d5dc161f99 100644
--- a/lib/logstash/outputs/gelf.rb
+++ b/lib/logstash/outputs/gelf.rb
@@ -202,7 +202,7 @@ def receive(event)
 
     @logger.debug(["Sending GELF event", m])
     begin
-      @gelf.notify!(m, :timestamp => event["@timestamp"].to_f)
+      @gelf.notify!(m, :timestamp => event.timestamp.to_f)
     rescue
       @logger.warn("Trouble sending GELF event", :gelf_event => m,
                    :event => event, :error => $!)
diff --git a/lib/logstash/outputs/http.rb b/lib/logstash/outputs/http.rb
index 029fc961a8e..8b955d16968 100644
--- a/lib/logstash/outputs/http.rb
+++ b/lib/logstash/outputs/http.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/outputs/base"
 require "logstash/namespace"
+require "logstash/json"
 
 class LogStash::Outputs::Http < LogStash::Outputs::Base
   # This output lets you `PUT` or `POST` events to a
@@ -102,7 +103,7 @@ def receive(event)
     else
       @logger.error("Unknown verb:", :verb => @http_method)
     end
-    
+
     if @headers
       @headers.each do |k,v|
         request.headers[k] = event.sprintf(v)
@@ -113,7 +114,7 @@ def receive(event)
 
     begin
       if @format == "json"
-        request.body = evt.to_json
+        request.body = LogStash::Json.dump(evt)
       elsif @format == "message"
         request.body = event.sprintf(@message)
       else
@@ -121,7 +122,7 @@ def receive(event)
       end
       #puts "#{request.port} / #{request.protocol}"
       #puts request
-      #puts 
+      #puts
       #puts request.body
       response = @agent.execute(request)
 
diff --git a/lib/logstash/outputs/juggernaut.rb b/lib/logstash/outputs/juggernaut.rb
index e41f8921726..f6985430bcd 100644
--- a/lib/logstash/outputs/juggernaut.rb
+++ b/lib/logstash/outputs/juggernaut.rb
@@ -2,6 +2,7 @@
 require "logstash/outputs/base"
 require "logstash/namespace"
 require "logstash/event"
+require "logstash/json"
 
 # Push messages to the juggernaut websockets server:
 #
@@ -85,7 +86,7 @@ def receive(event)
         "data" => event["message"]
       }
 
-      @redis.publish 'juggernaut', juggernaut_message.to_json
+      @redis.publish 'juggernaut', LogStash::Json.dump(juggernaut_message)
     rescue => e
       @logger.warn("Failed to send event to redis", :event => event,
                    :identity => identity, :exception => e,
diff --git a/lib/logstash/outputs/nagios_nsca.rb b/lib/logstash/outputs/nagios_nsca.rb
index 3e721de953f..90f761e0fa5 100644
--- a/lib/logstash/outputs/nagios_nsca.rb
+++ b/lib/logstash/outputs/nagios_nsca.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/outputs/base"
 require "logstash/namespace"
+require "open3"
 
 # The nagios_nsca output is used for sending passive check results to Nagios
 # through the NSCA protocol.
@@ -105,19 +106,27 @@ def receive(event)
     # build the command
     # syntax: echo '<server>!<nagios_service>!<status>!<text>'  | \
     #           /usr/sbin/send_nsca -H <nagios_host> -d '!' -c <nsca_config>"
-    cmd = %(echo '#{nagios_host}~#{nagios_service}~#{status}~#{msg}' |)
-    cmd << %( #{@send_nsca_bin} -H #{@host} -p #{@port} -d '~')
-    cmd << %( -c #{@send_nsca_config}) if @send_nsca_config
-    cmd << %( 2>/dev/null >/dev/null)
-    @logger.debug("Running send_nsca command", "nagios_nsca_command" => cmd)
+
+    cmd = [@send_nsca_bin, "-H", @host, "-p", @port, "-d", "~"]
+    cmd = cmd + ["-c", @send_nsca_config]  if @send_nsca_config
+    message = "#{nagios_host}~#{nagios_service}~#{status}~#{msg}"
+
+    @logger.debug("Running send_nsca command", :nagios_nsca_command => cmd.join(" "), :message => message)
 
     begin
-      system cmd
+      Open3.popen3(*cmd) do |i, o, e|
+        i.puts(message)
+        i.close
+      end
     rescue => e
-      @logger.warn("Skipping nagios_nsca output; error calling send_nsca",
-                   "error" => $!, "nagios_nsca_command" => cmd,
-                   "missed_event" => event)
-      @logger.debug("Backtrace", e.backtrace)
+      @logger.warn(
+        "Skipping nagios_nsca output; error calling send_nsca",
+        :error => $!,
+        :nagios_nsca_command => cmd.join(" "),
+        :message => message,
+        :missed_event => event
+      )
+      @logger.debug("Backtrace", :backtrace => e.backtrace)
     end
   end # def receive
 end # class LogStash::Outputs::NagiosNsca
diff --git a/lib/logstash/outputs/pagerduty.rb b/lib/logstash/outputs/pagerduty.rb
index 1c67e9a02e2..e4851986268 100644
--- a/lib/logstash/outputs/pagerduty.rb
+++ b/lib/logstash/outputs/pagerduty.rb
@@ -1,13 +1,14 @@
 # encoding: utf-8
 require "logstash/outputs/base"
 require "logstash/namespace"
+require "logstash/json"
 
-# The PagerDuty output will send notifications based on pre-configured services 
+# The PagerDuty output will send notifications based on pre-configured services
 # and escalation policies. Logstash can send "trigger", "acknowledge" and "resolve"
 # event types. In addition, you may configure custom descriptions and event details.
 # The only required field is the PagerDuty "Service API Key", which can be found on
 # the service's web page on pagerduty.com. In the default case, the description and
-# event details will be populated by Logstash, using `message`, `timestamp` and `host` data.  
+# event details will be populated by Logstash, using `message`, `timestamp` and `host` data.
 class LogStash::Outputs::PagerDuty < LogStash::Outputs::Base
   config_name "pagerduty"
   milestone 1
@@ -49,7 +50,7 @@ def register
   public
   def receive(event)
     return unless output?(event)
-   
+
     pd_event = Hash.new
     pd_event[:service_key] = "#{@service_key}"
     pd_event[:incident_key] = event.sprintf(@incident_key)
@@ -65,7 +66,7 @@ def receive(event)
     @logger.info("PD Event", :event => pd_event)
     begin
       request = Net::HTTP::Post.new(@pd_uri.path)
-      request.body = pd_event.to_json
+      request.body = LogStash::Json.dump(pd_event)
       @logger.debug("PD Request", :request => request.inspect)
       response = @client.request(request)
       @logger.debug("PD Response", :response => response.body)
diff --git a/lib/logstash/outputs/rabbitmq/bunny.rb b/lib/logstash/outputs/rabbitmq/bunny.rb
index 3a59234d23b..cc83eacc093 100644
--- a/lib/logstash/outputs/rabbitmq/bunny.rb
+++ b/lib/logstash/outputs/rabbitmq/bunny.rb
@@ -1,4 +1,7 @@
 # encoding: utf-8
+
+require "logstash/json"
+
 class LogStash::Outputs::RabbitMQ
   module BunnyImpl
 
@@ -24,7 +27,7 @@ def receive(event)
 
       begin
         publish_serialized(event.to_json, key)
-      rescue JSON::GeneratorError => e
+      rescue LogStash::Json::GeneratorError => e
         @logger.warn("Trouble converting event to JSON", :exception => e,
                      :event => event)
       end
diff --git a/lib/logstash/outputs/s3.rb b/lib/logstash/outputs/s3.rb
index e3a47b41bbf..c493d29b526 100644
--- a/lib/logstash/outputs/s3.rb
+++ b/lib/logstash/outputs/s3.rb
@@ -3,13 +3,13 @@
 require "logstash/namespace"
 require "socket" # for Socket.gethostname
 
-# TODO integrate aws_config in the future 
+# TODO integrate aws_config in the future
 #require "logstash/plugin_mixins/aws_config"
 
 # INFORMATION:
 
 # This plugin was created for store the logstash's events into Amazon Simple Storage Service (Amazon S3).
-# For use it you needs authentications and an s3 bucket. 
+# For use it you needs authentications and an s3 bucket.
 # Be careful to have the permission to write file on S3's bucket and run logstash with super user for establish connection.
 
 # S3 plugin allows you to do something complex, let's explain:)
@@ -23,9 +23,9 @@
 
 # "ip-10-228-27-95" : indicate you ip machine, if you have more logstash and writing on the same bucket for example.
 # "2013-04-18T10.00" : represents the time whenever you specify time_file.
-# "tag_hello" : this indicate the event's tag, you can collect events with the same tag. 
-# "part0" : this means if you indicate size_file then it will generate more parts if you file.size > size_file. 
-#           When a file is full it will pushed on bucket and will be deleted in temporary directory. 
+# "tag_hello" : this indicate the event's tag, you can collect events with the same tag.
+# "part0" : this means if you indicate size_file then it will generate more parts if you file.size > size_file.
+#           When a file is full it will pushed on bucket and will be deleted in temporary directory.
 #           If a file is empty is not pushed, but deleted.
 
 # This plugin have a system to restore the previous temporary files if something crash.
@@ -35,7 +35,7 @@
 ## If you specify size_file and time_file then it will create file for each tag (if specified), when time_file or
 ## their size > size_file, it will be triggered then they will be pushed on s3's bucket and will delete from local disk.
 
-## If you don't specify size_file, but time_file then it will create only one file for each tag (if specified). 
+## If you don't specify size_file, but time_file then it will create only one file for each tag (if specified).
 ## When time_file it will be triggered then the files will be pushed on s3's bucket and delete from local disk.
 
 ## If you don't specify time_file, but size_file  then it will create files for each tag (if specified),
@@ -46,15 +46,15 @@
 
 # INFORMATION ABOUT CLASS:
 
-# I tried to comment the class at best i could do. 
+# I tried to comment the class at best i could do.
 # I think there are much thing to improve, but if you want some points to develop here a list:
 
-# TODO Integrate aws_config in the future 
+# TODO Integrate aws_config in the future
 # TODO Find a method to push them all files when logtstash close the session.
 # TODO Integrate @field on the path file
-# TODO Permanent connection or on demand? For now on demand, but isn't a good implementation. 
+# TODO Permanent connection or on demand? For now on demand, but isn't a good implementation.
 #      Use a while or a thread to try the connection before break a time_out and signal an error.
-# TODO If you have bugs report or helpful advice contact me, but remember that this code is much mine as much as yours, 
+# TODO If you have bugs report or helpful advice contact me, but remember that this code is much mine as much as yours,
 #      try to work on it if you want :)
 
 
@@ -63,30 +63,30 @@
 # This is an example of logstash config:
 
 # output {
-#    s3{ 
+#    s3{
 #      access_key_id => "crazy_key"             (required)
 #      secret_access_key => "monkey_access_key" (required)
 #      endpoint_region => "eu-west-1"           (required)
-#      bucket => "boss_please_open_your_bucket" (required)         
+#      bucket => "boss_please_open_your_bucket" (required)
 #      size_file => 2048                        (optional)
 #      time_file => 5                           (optional)
-#      format => "plain"                        (optional) 
+#      format => "plain"                        (optional)
 #      canned_acl => "private"                  (optional. Options are "private", "public_read", "public_read_write", "authenticated_read". Defaults to "private" )
 #    }
 # }
 
 # We analize this:
 
-# access_key_id => "crazy_key" 
+# access_key_id => "crazy_key"
 # Amazon will give you the key for use their service if you buy it or try it. (not very much open source anyway)
 
 # secret_access_key => "monkey_access_key"
 # Amazon will give you the secret_access_key for use their service if you buy it or try it . (not very much open source anyway).
 
-# endpoint_region => "eu-west-1" 
+# endpoint_region => "eu-west-1"
 # When you make a contract with Amazon, you should know where the services you use.
 
-# bucket => "boss_please_open_your_bucket" 
+# bucket => "boss_please_open_your_bucket"
 # Be careful you have the permission to write on bucket and know the name.
 
 # size_file => 2048
@@ -95,7 +95,7 @@
 
 # time_file => 5
 # Means, in minutes, the time  before the files will be pushed on bucket. Is useful if you want to push the files every specific time.
- 
+
 # format => "plain"
 # Means the format of events you want to store in the files
 
@@ -105,7 +105,7 @@
 # LET'S ROCK AND ROLL ON THE CODE!
 
 class LogStash::Outputs::S3 < LogStash::Outputs::Base
- #TODO integrate aws_config in the future 
+ #TODO integrate aws_config in the future
  #  include LogStash::PluginMixins::AwsConfig
 
  config_name "s3"
@@ -113,7 +113,7 @@ class LogStash::Outputs::S3 < LogStash::Outputs::Base
 
  # Aws access_key.
  config :access_key_id, :validate => :string
- 
+
  # Aws secret_access_key
  config :secret_access_key, :validate => :string
 
@@ -125,24 +125,24 @@ class LogStash::Outputs::S3 < LogStash::Outputs::Base
                                         "eu-west-1", "ap-southeast-1", "ap-southeast-2",
                                         "ap-northeast-1", "sa-east-1", "us-gov-west-1"], :default => "us-east-1"
 
- # Set the size of file in KB, this means that files on bucket when have dimension > file_size, they are stored in two or more file. 
+ # Set the size of file in KB, this means that files on bucket when have dimension > file_size, they are stored in two or more file.
  # If you have tags then it will generate a specific size file for every tags
- ##NOTE: define size of file is the better thing, because generate a local temporary file on disk and then put it in bucket. 
+ ##NOTE: define size of file is the better thing, because generate a local temporary file on disk and then put it in bucket.
  config :size_file, :validate => :number, :default => 0
 
- # Set the time, in minutes, to close the current sub_time_section of bucket. 
+ # Set the time, in minutes, to close the current sub_time_section of bucket.
  # If you define file_size you have a number of files in consideration of the section and the current tag.
  # 0 stay all time on listerner, beware if you specific 0 and size_file 0, because you will not put the file on bucket,
  # for now the only thing this plugin can do is to put the file when logstash restart.
- config :time_file, :validate => :number, :default => 0 
- 
+ config :time_file, :validate => :number, :default => 0
+
  # The event format you want to store in files. Defaults to plain text.
  config :format, :validate => [ "json", "plain", "nil" ], :default => "plain"
 
  ## IMPORTANT: if you use multiple instance of s3, you should specify on one of them the "restore=> true" and on the others "restore => false".
- ## This is hack for not destroy the new files after restoring the initial files. 
+ ## This is hack for not destroy the new files after restoring the initial files.
  ## If you do not specify "restore => true" when logstash crashes or is restarted, the files are not sent into the bucket,
- ## for example if you have single Instance. 
+ ## for example if you have single Instance.
  config :restore, :validate => :boolean, :default => false
 
  # Aws canned ACL
@@ -161,7 +161,7 @@ def aws_s3_config
     :secret_access_key => @secret_access_key,
     :s3_endpoint => @endpoint_region
   )
-  @s3 = AWS::S3.new 
+  @s3 = AWS::S3.new
 
  end
 
@@ -181,9 +181,9 @@ def time_alert(interval)
 
  # this method is used for write files on bucket. It accept the file and the name of file.
  def write_on_bucket (file_data, file_basename)
- 
+
   # if you lose connection with s3, bad control implementation.
-  if ( @s3 == nil) 
+  if ( @s3 == nil)
     aws_s3_config
   end
 
@@ -195,31 +195,31 @@ def write_on_bucket (file_data, file_basename)
   # prepare for write the file
   object = bucket.objects[file_basename]
   object.write(:file => file_data, :acl => @canned_acl)
- 
+
   @logger.debug "S3: has written "+file_basename+" in bucket "+@bucket + " with canned ACL \"" + @canned_acl + "\""
 
  end
-  
+
  # this method is used for create new path for name the file
  def getFinalPath
-   
-   @pass_time = Time.now 
+
+   @pass_time = Time.now
    return @temp_directory+"ls.s3."+Socket.gethostname+"."+(@pass_time).strftime("%Y-%m-%dT%H.%M")
 
  end
 
- # This method is used for restore the previous crash of logstash or to prepare the files to send in bucket. 
- # Take two parameter: flag and name. Flag indicate if you want to restore or not, name is the name of file 
+ # This method is used for restore the previous crash of logstash or to prepare the files to send in bucket.
+ # Take two parameter: flag and name. Flag indicate if you want to restore or not, name is the name of file
  def upFile(flag, name)
-   
+
    Dir[@temp_directory+name].each do |file|
      name_file = File.basename(file)
-    
+
      if (flag == true)
       @logger.warn "S3: have found temporary file: "+name_file+", something has crashed before... Prepare for upload in bucket!"
      end
-    
-     if (!File.zero?(file))  
+
+     if (!File.zero?(file))
        write_on_bucket(file, name_file)
 
        if (flag == true)
@@ -236,7 +236,7 @@ def upFile(flag, name)
 
  # This method is used for create new empty temporary files for use. Flag is needed for indicate new subsection time_file.
  def newFile (flag)
-  
+
    if (flag == true)
      @current_final_path = getFinalPath
      @sizeCounter = 0
@@ -258,7 +258,7 @@ def register
    if (@tags.size != 0)
        @tag_path = ""
        for i in (0..@tags.size-1)
-          @tag_path += @tags[i].to_s+"." 
+          @tag_path += @tags[i].to_s+"."
        end
    end
 
@@ -267,16 +267,16 @@ def register
     Dir.mkdir(@temp_directory)
    else
     @logger.debug "S3: Directory "+@temp_directory+" exist, nothing to do"
-   end 
-   
+   end
+
    if (@restore == true )
      @logger.debug "S3: is attempting to verify previous crashes..."
-   
-     upFile(true, "*.txt")    
+
+     upFile(true, "*.txt")
    end
-   
+
    newFile(true)
-   
+
    if (time_file != 0)
       first_time = true
       @thread = time_alert(@time_file*60) do
@@ -289,14 +289,14 @@ def register
        end
      end
    end
- 
+
  end
- 
+
  public
  def receive(event)
   return unless output?(event)
-   
-  # Prepare format of Events 
+
+  # Prepare format of Events
   if (@format == "plain")
      message = self.class.format_message(event)
   elsif (@format == "json")
@@ -304,20 +304,20 @@ def receive(event)
   else
      message = event.to_s
   end
-  
+
   if(time_file !=0)
      @logger.debug "S3: trigger files after "+((@pass_time+60*time_file)-Time.now).to_s
   end
 
   # if specific the size
   if(size_file !=0)
-    
+
     if (@tempFile.size < @size_file )
 
        @logger.debug "S3: File have size: "+@tempFile.size.to_s+" and size_file is: "+ @size_file.to_s
        @logger.debug "S3: put event into: "+File.basename(@tempFile)
 
-       # Put the event in the file, now! 
+       # Put the event in the file, now!
        File.open(@tempFile, 'a') do |file|
          file.puts message
          file.write "\n"
@@ -331,8 +331,8 @@ def receive(event)
        newFile(false)
 
      end
-     
-  # else we put all in one file 
+
+  # else we put all in one file
   else
 
     @logger.debug "S3: put event into "+File.basename(@tempFile)
@@ -341,11 +341,11 @@ def receive(event)
       file.write "\n"
     end
   end
-    
+
  end
 
  def self.format_message(event)
-    message = "Date: #{event["@timestamp"]}\n"
+    message = "Date: #{event[LogStash::Event::TIMESTAMP]}\n"
     message << "Source: #{event["source"]}\n"
     message << "Tags: #{event["tags"].join(', ')}\n"
     message << "Fields: #{event.to_hash.inspect}\n"
diff --git a/lib/logstash/outputs/sns.rb b/lib/logstash/outputs/sns.rb
index ed91b6b2557..f882f65deca 100644
--- a/lib/logstash/outputs/sns.rb
+++ b/lib/logstash/outputs/sns.rb
@@ -112,7 +112,7 @@ def self.json_message(event)
   end
 
   def self.format_message(event)
-    message =  "Date: #{event["@timestamp"]}\n"
+    message =  "Date: #{event.timestamp}\n"
     message << "Source: #{event["source"]}\n"
     message << "Tags: #{event["tags"].join(', ')}\n"
     message << "Fields: #{event.to_hash.inspect}\n"
diff --git a/lib/logstash/plugin.rb b/lib/logstash/plugin.rb
index 418ccb8a9ca..ce8de95a5a0 100644
--- a/lib/logstash/plugin.rb
+++ b/lib/logstash/plugin.rb
@@ -8,6 +8,8 @@ class LogStash::Plugin
   attr_accessor :params
   attr_accessor :logger
 
+  NL = "\n"
+
   public
   def hash
     params.hash ^
diff --git a/lib/logstash/runner.rb b/lib/logstash/runner.rb
index 4a396b0802c..22ae31636a0 100644
--- a/lib/logstash/runner.rb
+++ b/lib/logstash/runner.rb
@@ -4,6 +4,9 @@
 $START = Time.now
 $DEBUGLIST = (ENV["DEBUG"] || "").split(",")
 
+require "logstash/environment"
+LogStash::Environment.set_gem_paths!
+
 Thread.abort_on_exception = true
 if ENV["PROFILE_BAD_LOG_CALLS"] || $DEBUGLIST.include?("log")
   # Set PROFILE_BAD_LOG_CALLS=1 in your environment if you want
@@ -43,11 +46,10 @@ module Cabin::Mixins::Logger
 require "logstash/monkeypatches-for-debugging"
 require "logstash/namespace"
 require "logstash/program"
-require "i18n" # gem 'i18n'
+
+require "i18n"
 I18n.enforce_available_locales = true
-I18n.load_path << File.expand_path(
-  File.join(File.dirname(__FILE__), "../../locales/en.yml")
-)
+I18n.load_path << LogStash::Environment.locales_path("en.yml")
 
 class LogStash::RSpecsRunner
   def initialize(args)
diff --git a/lib/logstash/time_addon.rb b/lib/logstash/time_addon.rb
deleted file mode 100644
index a5970332dc3..00000000000
--- a/lib/logstash/time_addon.rb
+++ /dev/null
@@ -1,25 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-
-module LogStash::Time
-  ISO8601_STRFTIME = "%04d-%02d-%02dT%02d:%02d:%02d.%06d%+03d:00".freeze
-  def self.now
-    return Time.new.utc
-  end
-
-  if RUBY_PLATFORM == "java"
-    JODA_ISO8601_PARSER = org.joda.time.format.ISODateTimeFormat.dateTimeParser
-    #JODA_ISO8601_PARSER = org.joda.time.format.DateTimeFormat.forPattern("yyyy-MM-dd'T'HH:mm:ss.SSSZ")
-    UTC = org.joda.time.DateTimeZone.forID("UTC")
-    def self.parse_iso8601(t)
-      millis = JODA_ISO8601_PARSER.parseMillis(t)
-      return Time.at(millis / 1000, (millis % 1000) * 1000)
-    end
-  else
-    def self.parse_iso8601(t)
-      # Warning, ruby's Time.parse is *really* terrible and slow.
-      return unless t.is_a?(String)
-      return Time.parse(t).gmtime
-    end
-  end
-end # module LogStash::Time
diff --git a/lib/logstash/timestamp.rb b/lib/logstash/timestamp.rb
new file mode 100644
index 00000000000..a96dfb02c1d
--- /dev/null
+++ b/lib/logstash/timestamp.rb
@@ -0,0 +1,92 @@
+# encoding: utf-8
+require "logstash/environment"
+require "logstash/json"
+require "forwardable"
+require "date"
+require "time"
+
+module LogStash
+  class TimestampParserError < StandardError; end
+
+  class Timestamp
+    extend Forwardable
+
+    def_delegators :@time, :tv_usec, :usec, :year, :iso8601, :to_i, :tv_sec, :to_f, :to_edn
+
+    attr_reader :time
+
+    ISO8601_STRFTIME = "%04d-%02d-%02dT%02d:%02d:%02d.%06d%+03d:00".freeze
+    ISO8601_PRECISION = 3
+
+    def initialize(time = Time.new)
+      @time = time.utc
+    end
+
+    def self.at(*args)
+      Timestamp.new(::Time.at(*args))
+    end
+
+    def self.parse(*args)
+      Timestamp.new(::Time.parse(*args))
+    end
+
+    def self.now
+      Timestamp.new(::Time.now)
+    end
+
+    # coerce tries different strategies based on the time object class to convert into a Timestamp.
+    # @param [String, Time, Timestamp] time the time object to try coerce
+    # @return [Timestamp, nil] Timestamp will be returned if successful otherwise nil
+    # @raise [TimestampParserError] on String with invalid format
+    def self.coerce(time)
+      case time
+      when String
+        LogStash::Timestamp.parse_iso8601(time)
+      when LogStash::Timestamp
+        time
+      when Time
+        LogStash::Timestamp.new(time)
+      else
+        nil
+      end
+    end
+
+    if LogStash::Environment.jruby?
+      JODA_ISO8601_PARSER = org.joda.time.format.ISODateTimeFormat.dateTimeParser
+      UTC = org.joda.time.DateTimeZone.forID("UTC")
+
+      def self.parse_iso8601(t)
+        millis = JODA_ISO8601_PARSER.parseMillis(t)
+        LogStash::Timestamp.at(millis / 1000, (millis % 1000) * 1000)
+      rescue => e
+        raise(TimestampParserError, "invalid timestamp string #{t.inspect}, error=#{e.inspect}")
+      end
+
+    else
+
+      def self.parse_iso8601(t)
+        # warning, ruby's Time.parse is *really* terrible and slow.
+        LogStash::Timestamp.new(::Time.parse(t))
+      rescue => e
+        raise(TimestampParserError, "invalid timestamp string #{t.inspect}, error=#{e.inspect}")
+      end
+    end
+
+    def utc
+      @time.utc # modifies the receiver
+      self
+    end
+    alias_method :gmtime, :utc
+
+    def to_json
+      LogStash::Json.dump(@time.iso8601(ISO8601_PRECISION))
+    end
+    alias_method :inspect, :to_json
+
+    def to_iso8601
+      @time.iso8601(ISO8601_PRECISION)
+    end
+    alias_method :to_s, :to_iso8601
+
+  end
+end
diff --git a/lib/logstash/util.rb b/lib/logstash/util.rb
index 76b5926b378..1ce6cd00ed6 100644
--- a/lib/logstash/util.rb
+++ b/lib/logstash/util.rb
@@ -1,5 +1,6 @@
 # encoding: utf-8
 require "logstash/namespace"
+require "logstash/environment"
 
 module LogStash::Util
   UNAME = case RbConfig::CONFIG["host_os"]
@@ -14,7 +15,7 @@ def self.set_thread_name(name)
       Java::java.lang.Thread.currentThread.setName(name)
     end
     Thread.current[:name] = name
-    
+
     if UNAME == "linux"
       require "logstash/util/prctl"
       # prctl PR_SET_NAME allows up to 16 bytes for a process name
@@ -34,7 +35,7 @@ def self.hash_merge(dst, src)
         dvalue = dst[name]
         if dvalue.is_a?(Hash) && svalue.is_a?(Hash)
           dvalue = hash_merge(dvalue, svalue)
-        elsif svalue.is_a?(Array) 
+        elsif svalue.is_a?(Array)
           if dvalue.is_a?(Array)
             # merge arrays without duplicates.
             dvalue |= svalue
@@ -58,7 +59,7 @@ def self.hash_merge(dst, src)
 
     return dst
   end # def self.hash_merge
- 
+
   # Merge hash 'src' into 'dst' nondestructively
   #
   # Duplicate keys will become array values
@@ -71,7 +72,7 @@ def self.hash_merge_with_dups(dst, src)
         dvalue = dst[name]
         if dvalue.is_a?(Hash) && svalue.is_a?(Hash)
           dvalue = hash_merge(dvalue, svalue)
-        elsif svalue.is_a?(Array) 
+        elsif svalue.is_a?(Array)
           if dvalue.is_a?(Array)
             # merge arrays without duplicates.
             dvalue += svalue
@@ -103,4 +104,37 @@ def self.hash_merge_many(*hashes)
     end
     return dst
   end # def hash_merge_many
+
+
+  # nomalize method definition based on platform.
+  # normalize is used to convert an object create through
+  # json deserialization from JrJackson in :raw mode to pure Ruby
+  # to support these pure Ruby object monkey patches.
+  # see logstash/json.rb and logstash/java_integration.rb
+
+  if LogStash::Environment.jruby?
+    require "java"
+
+    # recursively convert any Java LinkedHashMap and ArrayList to pure Ruby.
+    # will not recurse into pure Ruby objects. Pure Ruby object should never
+    # contain LinkedHashMap and ArrayList since these are only created at
+    # initial deserialization, anything after (deeper) will be pure Ruby.
+    def self.normalize(o)
+      case o
+      when Java::JavaUtil::LinkedHashMap
+        o.inject({}){|r, (k, v)| r[k] = normalize(v); r}
+      when Java::JavaUtil::ArrayList
+        o.map{|i| normalize(i)}
+      else
+        o
+      end
+    end
+
+  else
+
+    # identity function, pure Ruby object don't need normalization.
+    def self.normalize(o); o; end
+  end
+
+
 end # module LogStash::Util
diff --git a/locales/en.yml b/locales/en.yml
index 1ab0fdfb447..562da16eb7b 100644
--- a/locales/en.yml
+++ b/locales/en.yml
@@ -121,7 +121,7 @@ en:
         # them in an 80-character terminal
         config: |+
           Load the logstash config from a specific file
-          or directory.  If a direcory is given, all
+          or directory.  If a directory is given, all
           files in that directory will be concatonated
           in lexicographical order and then parsed as a
           single config file. You can also specify
diff --git a/logstash.gemspec b/logstash.gemspec
index 4917d83ed30..702c5129e76 100644
--- a/logstash.gemspec
+++ b/logstash.gemspec
@@ -17,7 +17,6 @@ Gem::Specification.new do |gem|
 
   # Core dependencies
   gem.add_runtime_dependency "cabin", [">=0.6.0"]   #(Apache 2.0 license)
-  gem.add_runtime_dependency "json"               #(ruby license)
   gem.add_runtime_dependency "minitest"           # for running the tests from the jar, (MIT license)
   gem.add_runtime_dependency "pry"                #(ruby license)
   gem.add_runtime_dependency "stud"               #(Apache 2.0 license)
@@ -68,9 +67,11 @@ Gem::Specification.new do |gem|
     gem.add_runtime_dependency "bouncy-castle-java", "1.5.0147"   #(MIT license)
     gem.add_runtime_dependency "jruby-openssl", "0.8.7"           #(CPL/GPL/LGPL license)
     gem.add_runtime_dependency "msgpack-jruby"                    #(Apache 2.0 license)
+    gem.add_runtime_dependency "jrjackson"                        #(Apache 2.0 license)
   else
     gem.add_runtime_dependency "excon"    #(MIT license)
     gem.add_runtime_dependency "msgpack"  #(Apache 2.0 license)
+    gem.add_runtime_dependency "oj"       #(MIT-style license)
   end
 
   if RUBY_PLATFORM != 'java'
diff --git a/patterns/java b/patterns/java
index 56233e13117..1d5a0e20e54 100644
--- a/patterns/java
+++ b/patterns/java
@@ -1,3 +1,7 @@
-JAVACLASS (?:[a-zA-Z0-9-]+\.)+[A-Za-z0-9$_]+
+JAVACLASS (?:[a-zA-Z$_][a-zA-Z$_0-9]*\.)*[a-zA-Z$_][a-zA-Z$_0-9]*
+#Space is an allowed character to match special cases like 'Native Method' or 'Unknown Source'
 JAVAFILE (?:[A-Za-z0-9_. -]+)
-JAVASTACKTRACEPART at %{JAVACLASS:class}\.%{WORD:method}\(%{JAVAFILE:file}:%{NUMBER:line}\)
+#Allow special <init> method
+JAVAMETHOD (?:(<init>)|[a-zA-Z$_][a-zA-Z$_0-9]*)
+#Line number is optional in special cases 'Native method' or 'Unknown source'
+JAVASTACKTRACEPART %{SPACE}at %{JAVACLASS:class}\.%{JAVAMETHOD:method}\(%{JAVAFILE:file}(?::%{NUMBER:line})?\)
diff --git a/pkg/logstash.sysv b/pkg/logstash.sysv
index bb7555e1ab1..fddc14d5ed9 100755
--- a/pkg/logstash.sysv
+++ b/pkg/logstash.sysv
@@ -48,6 +48,7 @@ start() {
 
 
   JAVA_OPTS=${LS_JAVA_OPTS}
+  HOME=${LS_HOME}
   export PATH HOME JAVA_OPTS LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
 
   # set ulimit as (root, presumably) first, before we drop privileges
diff --git a/spec/codecs/collectd.rb b/spec/codecs/collectd.rb
index 0233e4427b1..fb638a95090 100644
--- a/spec/codecs/collectd.rb
+++ b/spec/codecs/collectd.rb
@@ -56,8 +56,99 @@
       # One of these will fail because I altered the payload from the normal packet
       insist { counter } == 27
     end # it "should drop a part with an header length"
+
+    # This payload contains a NaN value
+    it "should replace a NaN with a zero and add tag '_collectdNaN' by default" do
+      payload = ["00000015746573742e6578616d706c652e636f6d000008000c14dc4c81831ef78b0009000c00000000400000000002000970696e67000004000970696e67000005001c70696e672d7461726765742e6578616d706c652e636f6d000006000f000101000000000000f87f"].pack('H*')
+      counter = 0
+      subject.decode(payload) do |event|
+        case counter
+        when 0
+          insist { event['host'] } == "test.example.com"
+          insist { event['plugin'] } == "ping"
+          insist { event['type_instance'] } == "ping-target.example.com"
+          insist { event['collectd_type'] } == "ping"
+          insist { event['value'] } == 0   # Not a NaN
+          insist { event['tags'] } == ["_collectdNaN"]
+        end
+        counter += 1
+      end
+      insist { counter } == 1
+    end # it "should replace a NaN with a zero and add tag '_collectdNaN' by default"
   end # context "None"
 
+  context "Replace nan_value and nan_tag with non-default values" do
+    subject do
+      next LogStash::Codecs::Collectd.new({"nan_value" => 1,
+                                           "nan_tag" => "NaN_encountered"})
+    end
+    # This payload contains a NaN value
+    it "should replace a NaN with the specified value and tag 'NaN_encountered'" do
+      payload = ["00000015746573742e6578616d706c652e636f6d000008000c14dc4c81831ef78b0009000c00000000400000000002000970696e67000004000970696e67000005001c70696e672d7461726765742e6578616d706c652e636f6d000006000f000101000000000000f87f"].pack('H*')
+      counter = 0
+      subject.decode(payload) do |event|
+        case counter
+        when 0
+          insist { event['host'] } == "test.example.com"
+          insist { event['plugin'] } == "ping"
+          insist { event['type_instance'] } == "ping-target.example.com"
+          insist { event['collectd_type'] } == "ping"
+          insist { event['value'] } == 1   # Not a NaN
+          insist { event['tags'] } == ["NaN_encountered"]
+        end
+        counter += 1
+      end
+      insist { counter } == 1
+    end # it "should replace a NaN with the specified value and tag 'NaN_encountered'"
+  end # context "Replace nan_value and nan_tag with non-default values"
+
+  context "Warn on NaN event" do
+    subject do
+      next LogStash::Codecs::Collectd.new({"nan_handling" => "warn"})
+    end
+    # This payload contains a NaN value
+    it "should replace a NaN with a zero and receive a warning when 'nan_handling' set to warn" do
+      payload = ["00000015746573742e6578616d706c652e636f6d000008000c14dc4c81831ef78b0009000c00000000400000000002000970696e67000004000970696e67000005001c70696e672d7461726765742e6578616d706c652e636f6d000006000f000101000000000000f87f"].pack('H*')
+      counter = 0
+      subject.logger.should_receive(:warn).with("NaN replaced by 0")
+      subject.decode(payload) do |event|
+        case counter
+        when 0
+          insist { event['host'] } == "test.example.com"
+          insist { event['plugin'] } == "ping"
+          insist { event['type_instance'] } == "ping-target.example.com"
+          insist { event['collectd_type'] } == "ping"
+          insist { event['value'] } == 0   # Not a NaN
+        end
+        counter += 1
+      end
+      insist { counter } == 1
+    end # it "should replace a NaN with a zero and receive a warning when 'nan_handling' set to warn"
+  end # context "Warn on NaN event"
+
+  context "Drop NaN event" do
+    subject do
+      next LogStash::Codecs::Collectd.new({"nan_handling" => "drop"})
+    end
+    # This payload contains a NaN value
+    it "should drop an event with a NaN value when 'nan_handling' set to drop" do
+      payload = ["00000015746573742e6578616d706c652e636f6d000008000c14dc4c81831ef78b0009000c00000000400000000002000970696e67000004000970696e67000005001c70696e672d7461726765742e6578616d706c652e636f6d000006000f000101000000000000f87f"].pack('H*')
+      counter = 0
+      subject.decode(payload) do |event|
+        case counter
+        when 0
+          insist { event['host'] } == "test.example.com"
+          insist { event['plugin'] } == "ping"
+          insist { event['type_instance'] } == "ping-target.example.com"
+          insist { event['collectd_type'] } == "ping"
+          insist { event['value'] } == NaN   # NaN
+        end
+        counter += 1 # Because we're dropping this, it should not increment
+      end
+      insist { counter } == 0 # We expect no increment
+    end # it "should drop an event with a NaN value when 'nan_handling' set to drop"
+  end # context "Drop NaN event"
+
   # Create an authfile for the next tests
   authfile = Tempfile.new('logstash-collectd-authfile')
   File.open(authfile.path, "a") do |fd|
diff --git a/spec/codecs/edn.rb b/spec/codecs/edn.rb
index e04cc659572..5fa49e58151 100644
--- a/spec/codecs/edn.rb
+++ b/spec/codecs/edn.rb
@@ -1,5 +1,6 @@
 require "logstash/codecs/edn"
 require "logstash/event"
+require "logstash/json"
 require "insist"
 require "edn"
 
@@ -10,26 +11,46 @@
 
   context "#decode" do
     it "should return an event from edn data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a", "b", "c"]}}
+      data = {"foo" => "bar", "baz" => {"bah" => ["a", "b", "c"]}, "@timestamp" => "2014-05-30T02:52:17.929Z"}
       subject.decode(data.to_edn) do |event|
         insist { event }.is_a?(LogStash::Event)
         insist { event["foo"] } == data["foo"]
         insist { event["baz"] } == data["baz"]
         insist { event["bah"] } == data["bah"]
+        insist { event["@timestamp"].to_iso8601 } == data["@timestamp"]
       end
     end
   end
 
   context "#encode" do
-    it "should return edn data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
+    it "should return edn data from pure ruby hash" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}, "@timestamp" => "2014-05-30T02:52:17.929Z"}
       event = LogStash::Event.new(data)
       got_event = false
       subject.on_event do |d|
-        insist { d.chomp } == LogStash::Event.new(data).to_hash.to_edn
         insist { EDN.read(d)["foo"] } == data["foo"]
         insist { EDN.read(d)["baz"] } == data["baz"]
         insist { EDN.read(d)["bah"] } == data["bah"]
+        insist { EDN.read(d)["@timestamp"] } == "2014-05-30T02:52:17.929Z"
+        got_event = true
+      end
+      subject.encode(event)
+      insist { got_event }
+    end
+
+    # this is to test the case where the event data has been produced by json
+    # deserialization using JrJackson in :raw mode which creates Java LinkedHashMap
+    # and not Ruby Hash which will not be monkey patched with the #to_edn method
+    it "should return edn data from deserialized json with normalization" do
+      data = LogStash::Json.load('{"foo": "bar", "baz": {"bah": ["a","b","c"]}, "@timestamp": "2014-05-30T02:52:17.929Z"}')
+      event = LogStash::Event.new(data)
+      got_event = false
+      subject.on_event do |d|
+        insist { EDN.read(d)["foo"] } == data["foo"]
+        insist { EDN.read(d)["baz"] } == data["baz"]
+        insist { EDN.read(d)["bah"] } == data["bah"]
+        insist { EDN.read(d)["@timestamp"] } == "2014-05-30T02:52:17.929Z"
+        insist { EDN.read(d)["@timestamp"] } == event["@timestamp"].to_iso8601
         got_event = true
       end
       subject.encode(event)
diff --git a/spec/codecs/edn_lines.rb b/spec/codecs/edn_lines.rb
index e5c1b711e19..79a25ba84a2 100644
--- a/spec/codecs/edn_lines.rb
+++ b/spec/codecs/edn_lines.rb
@@ -1,5 +1,6 @@
 require "logstash/codecs/edn_lines"
 require "logstash/event"
+require "logstash/json"
 require "insist"
 require "edn"
 
@@ -10,17 +11,18 @@
 
   context "#decode" do
     it "should return an event from edn data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a", "b", "c"]}}
+      data = {"foo" => "bar", "baz" => {"bah" => ["a", "b", "c"]}, "@timestamp" => "2014-05-30T02:52:17.929Z"}
       subject.decode(data.to_edn + "\n") do |event|
         insist { event }.is_a?(LogStash::Event)
         insist { event["foo"] } == data["foo"]
         insist { event["baz"] } == data["baz"]
         insist { event["bah"] } == data["bah"]
+        insist { event["@timestamp"].to_iso8601 } == data["@timestamp"]
       end
     end
 
     it "should return an event from edn data when a newline is recieved" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}, "@timestamp" => "2014-05-30T02:52:17.929Z"}
       subject.decode(data.to_edn) do |event|
         insist {false}
       end
@@ -29,21 +31,39 @@
         insist { event["foo"] } == data["foo"]
         insist { event["baz"] } == data["baz"]
         insist { event["bah"] } == data["bah"]
+        insist { event["@timestamp"].to_iso8601 } == data["@timestamp"]
       end
     end
   end
 
   context "#encode" do
-    it "should return edn data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
+    it "should return edn data from pure ruby hash" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}, "@timestamp" => "2014-05-30T02:52:17.929Z"}
       event = LogStash::Event.new(data)
       got_event = false
       subject.on_event do |d|
-        insist { d.chomp } == LogStash::Event.new(data).to_hash.to_edn
         insist { EDN.read(d)["foo"] } == data["foo"]
         insist { EDN.read(d)["baz"] } == data["baz"]
         insist { EDN.read(d)["bah"] } == data["bah"]
-        got_event = true
+        insist { EDN.read(d)["@timestamp"] } == "2014-05-30T02:52:17.929Z"
+        insist { EDN.read(d)["@timestamp"] } == event["@timestamp"].to_iso8601
+       got_event = true
+      end
+      subject.encode(event)
+      insist { got_event }
+    end
+
+    it "should return edn data rom deserialized json with normalization" do
+      data = LogStash::Json.load('{"foo": "bar", "baz": {"bah": ["a","b","c"]}, "@timestamp": "2014-05-30T02:52:17.929Z"}')
+      event = LogStash::Event.new(data)
+      got_event = false
+      subject.on_event do |d|
+        insist { EDN.read(d)["foo"] } == data["foo"]
+        insist { EDN.read(d)["baz"] } == data["baz"]
+        insist { EDN.read(d)["bah"] } == data["bah"]
+        insist { EDN.read(d)["@timestamp"] } == "2014-05-30T02:52:17.929Z"
+        insist { EDN.read(d)["@timestamp"] } == event["@timestamp"].to_iso8601
+       got_event = true
       end
       subject.encode(event)
       insist { got_event }
diff --git a/spec/codecs/graphite.rb b/spec/codecs/graphite.rb
index e5be954f149..8be9ef0af00 100644
--- a/spec/codecs/graphite.rb
+++ b/spec/codecs/graphite.rb
@@ -17,7 +17,7 @@
         insist { event[name] } == value
       end
     end
-    
+
     it "should return multiple events given multiple graphite formated lines" do
       total_count = Random.rand(20)
       names = Array.new(total_count) { Random.srand.to_s(36) }
@@ -32,7 +32,7 @@
       end
       insist { counter } == total_count
     end
-    
+
     it "should not return an event until newline is hit" do
       name = Random.srand.to_s(36)
       value = Random.rand*1000
@@ -50,7 +50,7 @@
       insist { event_returned }
     end
   end
-  
+
   context "#encode" do
     it "should emit an graphite formatted line" do
       name = Random.srand.to_s(36)
@@ -63,23 +63,23 @@
       end
       subject.encode(LogStash::Event.new("@timestamp" => timestamp))
     end
-    
+
     it "should treat fields as metrics if fields as metrics flag is set" do
       name = Random.srand.to_s(36)
       value = Random.rand*1000
-      timestamp = Time.now.gmtime.to_i
+      timestamp = Time.now.gmtime
       subject.fields_are_metrics = true
       subject.on_event do |event|
         insist { event.is_a? String }
         insist { event } == "#{name} #{value} #{timestamp.to_i}\n"
       end
       subject.encode(LogStash::Event.new({name => value, "@timestamp" => timestamp}))
-      
+
       #even if metrics param is set
       subject.metrics = {"foo" => 4}
       subject.encode(LogStash::Event.new({name => value, "@timestamp" => timestamp}))
     end
-    
+
     it "should change the metric name format when metrics_format is set" do
       name = Random.srand.to_s(36)
       value = Random.rand*1000
diff --git a/spec/codecs/json.rb b/spec/codecs/json.rb
index 11d879f3570..4cb128534c8 100644
--- a/spec/codecs/json.rb
+++ b/spec/codecs/json.rb
@@ -1,5 +1,6 @@
 require "logstash/codecs/json"
 require "logstash/event"
+require "logstash/json"
 require "insist"
 
 describe LogStash::Codecs::JSON do
@@ -10,7 +11,7 @@
   context "#decode" do
     it "should return an event from json data" do
       data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-      subject.decode(data.to_json) do |event|
+      subject.decode(LogStash::Json.dump(data)) do |event|
         insist { event.is_a? LogStash::Event }
         insist { event["foo"] } == data["foo"]
         insist { event["baz"] } == data["baz"]
@@ -70,9 +71,9 @@
       got_event = false
       subject.on_event do |d|
         insist { d.chomp } == LogStash::Event.new(data).to_json
-        insist { JSON.parse(d)["foo"] } == data["foo"]
-        insist { JSON.parse(d)["baz"] } == data["baz"]
-        insist { JSON.parse(d)["bah"] } == data["bah"]
+        insist { LogStash::Json.load(d)["foo"] } == data["foo"]
+        insist { LogStash::Json.load(d)["baz"] } == data["baz"]
+        insist { LogStash::Json.load(d)["bah"] } == data["bah"]
         got_event = true
       end
       subject.encode(event)
diff --git a/spec/codecs/json_lines.rb b/spec/codecs/json_lines.rb
index 40cdcba52a5..630e3fa7b6b 100644
--- a/spec/codecs/json_lines.rb
+++ b/spec/codecs/json_lines.rb
@@ -1,5 +1,6 @@
 require "logstash/codecs/json_lines"
 require "logstash/event"
+require "logstash/json"
 require "insist"
 
 describe LogStash::Codecs::JSONLines do
@@ -10,17 +11,17 @@
   context "#decode" do
     it "should return an event from json data" do
       data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-      subject.decode(data.to_json+"\n") do |event|
+      subject.decode(LogStash::Json.dump(data) + "\n") do |event|
         insist { event.is_a? LogStash::Event }
         insist { event["foo"] } == data["foo"]
         insist { event["baz"] } == data["baz"]
         insist { event["bah"] } == data["bah"]
       end
     end
-    
+
     it "should return an event from json data when a newline is recieved" do
       data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-      subject.decode(data.to_json) do |event|
+      subject.decode(LogStash::Json.dump(data)) do |event|
         insist {false}
       end
       subject.decode("\n") do |event|
@@ -64,10 +65,10 @@
       event = LogStash::Event.new(data)
       got_event = false
       subject.on_event do |d|
-        insist { d.chomp } == LogStash::Event.new(data).to_json
-        insist { JSON.parse(d)["foo"] } == data["foo"]
-        insist { JSON.parse(d)["baz"] } == data["baz"]
-        insist { JSON.parse(d)["bah"] } == data["bah"]
+        insist { d } == "#{LogStash::Event.new(data).to_json}\n"
+        insist { LogStash::Json.load(d)["foo"] } == data["foo"]
+        insist { LogStash::Json.load(d)["baz"] } == data["baz"]
+        insist { LogStash::Json.load(d)["bah"] } == data["bah"]
         got_event = true
       end
       subject.encode(event)
diff --git a/spec/codecs/json_spooler.rb b/spec/codecs/json_spooler.rb
index 7cb78da0b86..20aef79b596 100644
--- a/spec/codecs/json_spooler.rb
+++ b/spec/codecs/json_spooler.rb
@@ -1,43 +1,47 @@
 require "logstash/codecs/json_spooler"
 require "logstash/event"
+require "logstash/json"
 require "insist"
 
 describe LogStash::Codecs::JsonSpooler do
-  # subject do
-  #   next LogStash::Codecs::JsonSpooler.new
-  # end
+  subject do
+    # mute deprecation message
+    expect_any_instance_of(LogStash::Codecs::JsonSpooler).to receive(:register).and_return(nil)
 
-  # context "#decode" do
-  #   it "should return an event from spooled json data" do
-  #     data = {"a" => 1}
-  #     events = [LogStash::Event.new(data), LogStash::Event.new(data),
-  #       LogStash::Event.new(data)]
-  #     subject.decode(events.to_json) do |event|
-  #       insist { event.is_a? LogStash::Event }
-  #       insist { event["a"] } == data["a"]
-  #     end
-  #   end
-  # end
+    LogStash::Codecs::JsonSpooler.new
+  end
 
-  # context "#encode" do
-  #   it "should return spooled json data" do
-  #     data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-  #     subject.spool_size = 3
-  #     got_event = false
-  #     subject.on_event do |d|
-  #       events = JSON.parse(d)
-  #       insist { events.is_a? Array }
-  #       insist { events[0].is_a? LogStash::Event }
-  #       insist { events[0]["foo"] } == data["foo"]
-  #       insist { events[0]["baz"] } == data["baz"]
-  #       insist { events[0]["bah"] } == data["bah"]
-  #       insist { events.length } == 3
-  #       got_event = true
-  #     end
-  #     3.times do
-  #       subject.encode(LogStash::Event.new(data))
-  #     end
-  #     insist { got_event }
-  #   end
-  # end
+  context "#decode" do
+    it "should return an event from spooled json data" do
+      data = {"a" => 1}
+      events = [LogStash::Event.new(data), LogStash::Event.new(data),
+        LogStash::Event.new(data)]
+      subject.decode(LogStash::Json.dump(events)) do |event|
+        insist { event.is_a? LogStash::Event }
+        insist { event["a"] } == data["a"]
+      end
+    end
+  end
+
+  context "#encode" do
+    it "should return spooled json data" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
+      subject.spool_size = 3
+      got_event = false
+      subject.on_event do |d|
+        events = LogStash::Json.load(d)
+        insist { events.is_a? Array }
+        insist { events[0].is_a? LogStash::Event }
+        insist { events[0]["foo"] } == data["foo"]
+        insist { events[0]["baz"] } == data["baz"]
+        insist { events[0]["bah"] } == data["bah"]
+        insist { events.length } == 3
+        got_event = true
+      end
+      3.times do
+        subject.encode(LogStash::Event.new(data))
+      end
+      insist { got_event }
+    end
+  end
 end
diff --git a/spec/codecs/line.rb b/spec/codecs/line.rb
new file mode 100644
index 00000000000..e53a128b88b
--- /dev/null
+++ b/spec/codecs/line.rb
@@ -0,0 +1,51 @@
+# encoding: utf-8
+
+require "logstash/codecs/line"
+require "logstash/event"
+
+describe LogStash::Codecs::Line do
+  subject do
+    next LogStash::Codecs::Line.new
+  end
+
+  context "#encode" do
+    let (:event) {LogStash::Event.new({"message" => "hello world", "host" => "test"})}
+
+    it "should return a default date formatted line" do
+      expect(subject).to receive(:on_event).once.and_call_original
+      subject.on_event do |d|
+        insist {d} == event.to_s + "\n"
+      end
+      subject.encode(event)
+    end
+
+    it "should respect the supplied format" do
+      format = "%{host}"
+      subject.format = format
+      expect(subject).to receive(:on_event).once.and_call_original
+      subject.on_event do |d|
+        insist {d} == event.sprintf(format) + "\n"
+      end
+      subject.encode(event)
+    end
+  end
+
+  context "#decode" do
+    it "should return an event from an ascii string" do
+      decoded = false
+      subject.decode("hello world\n") do |e|
+        decoded = true
+        insist { e.is_a?(LogStash::Event) }
+        insist { e["message"] } == "hello world"
+      end
+      insist { decoded } == true
+    end
+
+    it "should return an event from a valid utf-8 string" do
+      subject.decode("München\n") do |e|
+        insist { e.is_a?(LogStash::Event) }
+        insist { e["message"] } == "München"
+      end
+    end
+  end
+end
diff --git a/spec/codecs/msgpack.rb b/spec/codecs/msgpack.rb
index fc36c1bab5e..ba0c451bd14 100644
--- a/spec/codecs/msgpack.rb
+++ b/spec/codecs/msgpack.rb
@@ -2,38 +2,56 @@
 require "logstash/event"
 require "insist"
 
-# Skip msgpack for now since Hash#to_msgpack seems to not be a valid method?
-describe LogStash::Codecs::Msgpack, :if => false  do
+describe LogStash::Codecs::Msgpack do
   subject do
     next LogStash::Codecs::Msgpack.new
   end
 
   context "#decode" do
     it "should return an event from msgpack data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-      subject.decode(data.to_msgpack) do |event|
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}, "@timestamp" => "2014-05-30T02:52:17.929Z"}
+      subject.decode(MessagePack.pack(data)) do |event|
         insist { event.is_a? LogStash::Event }
         insist { event["foo"] } == data["foo"]
         insist { event["baz"] } == data["baz"]
         insist { event["bah"] } == data["bah"]
+        insist { event["@timestamp"].to_iso8601 } == data["@timestamp"]
       end
     end
   end
 
   context "#encode" do
-    it "should return msgpack data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
+    it "should return msgpack data from pure ruby hash" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}, "@timestamp" => "2014-05-30T02:52:17.929Z"}
       event = LogStash::Event.new(data)
       got_event = false
       subject.on_event do |d|
-        insist { d } == LogStash::Event.new(data).to_hash.to_msgpack
         insist { MessagePack.unpack(d)["foo"] } == data["foo"]
         insist { MessagePack.unpack(d)["baz"] } == data["baz"]
         insist { MessagePack.unpack(d)["bah"] } == data["bah"]
+        insist { MessagePack.unpack(d)["@timestamp"] } == "2014-05-30T02:52:17.929Z"
+        insist { MessagePack.unpack(d)["@timestamp"] } == event["@timestamp"].to_iso8601
+        got_event = true
+      end
+      subject.encode(event)
+      insist { got_event }
+    end
+
+    it "should return msgpack data from deserialized json with normalization" do
+      data = LogStash::Json.load('{"foo": "bar", "baz": {"bah": ["a","b","c"]}, "@timestamp": "2014-05-30T02:52:17.929Z"}')
+      event = LogStash::Event.new(data)
+      got_event = false
+      subject.on_event do |d|
+        insist { MessagePack.unpack(d)["foo"] } == data["foo"]
+        insist { MessagePack.unpack(d)["baz"] } == data["baz"]
+        insist { MessagePack.unpack(d)["bah"] } == data["bah"]
+        insist { MessagePack.unpack(d)["@timestamp"] } == "2014-05-30T02:52:17.929Z"
+        insist { MessagePack.unpack(d)["@timestamp"] } == event["@timestamp"].to_iso8601
         got_event = true
       end
       subject.encode(event)
       insist { got_event }
     end
   end
+
 end
diff --git a/spec/codecs/oldlogstashjson.rb b/spec/codecs/oldlogstashjson.rb
index 163980637ec..3bb037b1a3b 100644
--- a/spec/codecs/oldlogstashjson.rb
+++ b/spec/codecs/oldlogstashjson.rb
@@ -1,5 +1,6 @@
 require "logstash/codecs/oldlogstashjson"
 require "logstash/event"
+require "logstash/json"
 require "insist"
 
 describe LogStash::Codecs::OldLogStashJSON do
@@ -11,7 +12,7 @@
     it "should return a new (v1) event from old (v0) json data" do
       data = {"@message" => "bar", "@source_host" => "localhost",
               "@tags" => ["a","b","c"]}
-      subject.decode(data.to_json) do |event|
+      subject.decode(LogStash::Json.dump(data)) do |event|
         insist { event.is_a? LogStash::Event }
         insist { event["@timestamp"] } != nil
         insist { event["type"] } == data["@type"]
@@ -38,14 +39,14 @@
       event = LogStash::Event.new(data)
       got_event = false
       subject.on_event do |d|
-        insist { JSON.parse(d)["@timestamp"] } != nil
-        insist { JSON.parse(d)["@type"] } == data["type"]
-        insist { JSON.parse(d)["@message"] } == data["message"]
-        insist { JSON.parse(d)["@source_host"] } == data["host"]
-        insist { JSON.parse(d)["@source_path"] } == data["path"]
-        insist { JSON.parse(d)["@tags"] } == data["tags"]
-        insist { JSON.parse(d)["@fields"]["bah"] } == "baz"
-        insist { JSON.parse(d)["@fields"]["@version"] } == nil
+        insist { LogStash::Json.load(d)["@timestamp"] } != nil
+        insist { LogStash::Json.load(d)["@type"] } == data["type"]
+        insist { LogStash::Json.load(d)["@message"] } == data["message"]
+        insist { LogStash::Json.load(d)["@source_host"] } == data["host"]
+        insist { LogStash::Json.load(d)["@source_path"] } == data["path"]
+        insist { LogStash::Json.load(d)["@tags"] } == data["tags"]
+        insist { LogStash::Json.load(d)["@fields"]["bah"] } == "baz"
+        insist { LogStash::Json.load(d)["@fields"]["@version"] } == nil
         got_event = true
       end
       subject.encode(event)
diff --git a/spec/event.rb b/spec/event.rb
index 17a283da038..e885c4f6e79 100644
--- a/spec/event.rb
+++ b/spec/event.rb
@@ -247,4 +247,51 @@
       end
     end
   end
+
+  context "timestamp initialization" do
+    let(:logger) { double("logger") }
+
+    it "should coerce timestamp" do
+      t = Time.iso8601("2014-06-12T00:12:17.114Z")
+      expect(LogStash::Timestamp).to receive(:coerce).exactly(3).times.and_call_original
+      insist{LogStash::Event.new("@timestamp" => t).timestamp.to_i} == t.to_i
+      insist{LogStash::Event.new("@timestamp" => LogStash::Timestamp.new(t)).timestamp.to_i} == t.to_i
+      insist{LogStash::Event.new("@timestamp" => "2014-06-12T00:12:17.114Z").timestamp.to_i} == t.to_i
+    end
+
+    it "should assign current time when no timestamp" do
+      ts = LogStash::Timestamp.now
+      expect(LogStash::Timestamp).to receive(:now).and_return(ts)
+      insist{LogStash::Event.new({}).timestamp.to_i} == ts.to_i
+    end
+
+    it "should tag and warn for invalid value" do
+      ts = LogStash::Timestamp.now
+      expect(LogStash::Timestamp).to receive(:now).twice.and_return(ts)
+      expect(Cabin::Channel).to receive(:get).twice.and_return(logger)
+      expect(logger).to receive(:warn).twice
+
+      event = LogStash::Event.new("@timestamp" => :foo)
+      insist{event.timestamp.to_i} == ts.to_i
+      insist{event["tags"]} == [LogStash::Event::TIMESTAMP_FAILURE_TAG]
+      insist{event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]} == :foo
+
+      event = LogStash::Event.new("@timestamp" => 666)
+      insist{event.timestamp.to_i} == ts.to_i
+      insist{event["tags"]} == [LogStash::Event::TIMESTAMP_FAILURE_TAG]
+      insist{event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]} == 666
+    end
+
+    it "should tag and warn for invalid string format" do
+      ts = LogStash::Timestamp.now
+      expect(LogStash::Timestamp).to receive(:now).and_return(ts)
+      expect(Cabin::Channel).to receive(:get).and_return(logger)
+      expect(logger).to receive(:warn)
+
+      event = LogStash::Event.new("@timestamp" => "foo")
+      insist{event.timestamp.to_i} == ts.to_i
+      insist{event["tags"]} == [LogStash::Event::TIMESTAMP_FAILURE_TAG]
+      insist{event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]} == "foo"
+    end
+  end
 end
diff --git a/spec/examples/graphite-input.rb b/spec/examples/graphite-input.rb
index fc86b09c49d..b1f4e96f571 100644
--- a/spec/examples/graphite-input.rb
+++ b/spec/examples/graphite-input.rb
@@ -37,7 +37,7 @@
 
     insist { subject["name"] } == "foo.bar.baz"
     insist { subject["value"] } == 4025.34
-    insist { subject["@timestamp"] } == Time.iso8601("2013-03-30T01:22:02.000Z")
+    insist { subject["@timestamp"].time } == Time.iso8601("2013-03-30T01:22:02.000Z")
 
   end
 end
diff --git a/spec/examples/parse-apache-logs.rb b/spec/examples/parse-apache-logs.rb
index c7e14537963..4407a95f29c 100644
--- a/spec/examples/parse-apache-logs.rb
+++ b/spec/examples/parse-apache-logs.rb
@@ -55,7 +55,7 @@
     insist { subject["agent"] } == "\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:14.0) Gecko/20100101 Firefox/14.0.1\""
 
     # Verify date parsing
-    insist { subject.timestamp } == Time.iso8601("2012-08-30T00:17:38.000Z")
+    insist { subject.timestamp.time } == Time.iso8601("2012-08-30T00:17:38.000Z")
   end
 
   sample '61.135.248.195 - - [26/Sep/2012:11:49:20 -0400] "GET /projects/keynav/ HTTP/1.1" 200 18985 "" "Mozilla/5.0 (compatible; YodaoBot/1.0; http://www.yodao.com/help/webmaster/spider/; )"' do
diff --git a/spec/filters/date.rb b/spec/filters/date.rb
index dc27d39d6e8..2419f4399ea 100644
--- a/spec/filters/date.rb
+++ b/spec/filters/date.rb
@@ -52,9 +52,9 @@
 
     times.each do |input, output|
       sample("mydate" => input) do
-        begin 
+        begin
           insist { subject["mydate"] } == input
-          insist { subject["@timestamp"] } == Time.iso8601(output).utc
+          insist { subject["@timestamp"].time } == Time.iso8601(output).utc
         rescue
           #require "pry"; binding.pry
           raise
@@ -83,7 +83,7 @@
     times.each do |input, output|
       sample("mydate" => input) do
         insist { subject["mydate"] } == input
-        insist { subject["@timestamp"] } == Time.iso8601(output).utc
+        insist { subject["@timestamp"].time } == Time.iso8601(output).utc
       end
     end # times.each
   end
@@ -109,7 +109,7 @@
     times.each do |input, output|
       sample("mydate" => input) do
         insist { subject["mydate"] } == input
-        insist { subject["@timestamp"] } == Time.iso8601(output).utc
+        insist { subject["@timestamp"].time } == Time.iso8601(output).utc
       end
     end # times.each
   end
@@ -126,7 +126,7 @@
 
     sample("mydate" => "1350414944.123456") do
       # Joda time only supports milliseconds :\
-      insist { subject.timestamp } == Time.iso8601("2012-10-16T12:15:44.123-07:00").utc
+      insist { subject.timestamp.time } == Time.iso8601("2012-10-16T12:15:44.123-07:00").utc
     end
   end
 
@@ -153,7 +153,7 @@
     times.each do |input, output|
       sample("mydate" => input) do
         insist { subject["mydate"] } == input
-        insist { subject["@timestamp"] } == Time.iso8601(output)
+        insist { subject["@timestamp"].time } == Time.iso8601(output)
       end
     end # times.each
   end
@@ -177,7 +177,7 @@
           locale => "en"
         }
       }
-      output { 
+      output {
         null { }
       }
     CONFIG
@@ -199,13 +199,13 @@
 
     # Try without leading "@"
     sample("t" => "4000000050d506482dbdf024") do
-      insist { subject.timestamp } == Time.iso8601("2012-12-22T01:00:46.767Z").utc
+      insist { subject.timestamp.time } == Time.iso8601("2012-12-22T01:00:46.767Z").utc
     end
 
     # Should still parse successfully if it's a full tai64n time (with leading
     # '@')
     sample("t" => "@4000000050d506482dbdf024") do
-      insist { subject.timestamp } == Time.iso8601("2012-12-22T01:00:46.767Z").utc
+      insist { subject.timestamp.time } == Time.iso8601("2012-12-22T01:00:46.767Z").utc
     end
   end
 
@@ -223,28 +223,28 @@
 
     sample("mydate" => time) do
       insist { subject["mydate"] } == time
-      insist { subject["@timestamp"] } == Time.iso8601(time).utc
+      insist { subject["@timestamp"].time } == Time.iso8601(time).utc
     end
   end
-  
+
   describe "support deep nested field access" do
     config <<-CONFIG
-      filter { 
+      filter {
         date {
           match => [ "[data][deep]", "ISO8601" ]
           locale => "en"
         }
       }
     CONFIG
-    
+
     sample("data" => { "deep" => "2013-01-01T00:00:00.000Z" }) do
-      insist { subject["@timestamp"] } == Time.iso8601("2013-01-01T00:00:00.000Z").utc
+      insist { subject["@timestamp"].time } == Time.iso8601("2013-01-01T00:00:00.000Z").utc
     end
   end
 
   describe "failing to parse should not throw an exception" do
     config <<-CONFIG
-      filter { 
+      filter {
         date {
           match => [ "thedate", "yyyy/MM/dd" ]
           locale => "en"
@@ -259,7 +259,7 @@
 
    describe "success to parse should apply on_success config(add_tag,add_field...)" do
     config <<-CONFIG
-      filter { 
+      filter {
         date {
           match => [ "thedate", "yyyy/MM/dd" ]
           add_tag => "tagged"
@@ -275,7 +275,7 @@
 
    describe "failing to parse should not apply on_success config(add_tag,add_field...)" do
     config <<-CONFIG
-      filter { 
+      filter {
         date {
           match => [ "thedate", "yyyy/MM/dd" ]
           add_tag => "tagged"
@@ -308,7 +308,7 @@
     times.each do |input, output|
       sample("mydate" => input) do
         insist { subject["mydate"] } == input
-        insist { subject["@timestamp"] } == Time.iso8601(output).utc
+        insist { subject["@timestamp"].time } == Time.iso8601(output).utc
       end
     end # times.each
   end
diff --git a/spec/filters/grok-patterns/java.rb b/spec/filters/grok-patterns/java.rb
new file mode 100644
index 00000000000..ce0eb3b64fc
--- /dev/null
+++ b/spec/filters/grok-patterns/java.rb
@@ -0,0 +1,158 @@
+# encoding: utf-8
+require "test_utils"
+
+# Test suite for the grok patterns defined in patterns/java
+# For each pattern:
+#  - a sample is considered valid i.e. "should match"  where message == result
+#  - a sample is considered invalid i.e. "should NOT match"  where message != result
+#
+describe "java grok pattern" do
+  extend LogStash::RSpec
+
+  describe "JAVACLASS" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{JAVACLASS:result}" }
+        }
+      }
+    CONFIG
+
+    context "should match" do
+      [
+        "package.Class",
+        "package.class", #camel case is not mandatory
+        "package.subpackage.Class",
+        "package._subpackage.Class",
+        "package._.Class",
+        "package.Class$InnerClass", #java inner class
+        "classWithNoPackage",
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} == message
+        end
+      end
+    end
+
+    context "should NOT match" do
+      [
+        "package.Illegal!Class",
+        "illegal.!package.Class",
+        "package-with-hyphen.Class",
+        "123package.Class",
+        "package.123Class",
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} != message
+        end
+      end
+    end
+  end
+
+  describe "JAVAMETHOD" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{JAVAMETHOD:result}" }
+        }
+      }
+    CONFIG
+
+    context "should match" do
+      [
+        "methodName",
+        "method_name",
+        "methodWithNumber0",
+        "_method",
+        "_",
+        "<init>", # Special constructor method
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} == message
+        end
+      end
+    end
+
+    context "should NOT match" do
+      [
+        "method-name",
+        "method!name",
+        "method.name",
+        "method>name",
+        "<notinit>",
+        "-",
+        "123method",
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} != message
+        end
+      end
+    end
+  end
+
+  describe "JAVAFILE" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{JAVAFILE:result}" }
+        }
+      }
+    CONFIG
+
+    context "should match" do
+      [
+        "Wombat.java",
+        "CacheAwareContextLoaderDelegate.java",
+        "Native Method",
+        "Unknown Source",
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} == message
+        end
+      end
+    end
+
+    context "should NOT match" do
+      [
+         #Sorry no idea
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} != message
+        end
+      end
+    end
+  end
+
+  describe "JAVASTACKTRACEPART" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{JAVASTACKTRACEPART:result}" }
+        }
+      }
+    CONFIG
+
+    context "should match" do
+      [
+        "at com.xyz.Wombat(Wombat.java:57)",
+        "at org.springframework.test.context.CacheAwareContextLoaderDelegate.loadContext(CacheAwareContextLoaderDelegate.java:91)",
+        "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
+        "at org.jnp.server.NamingServer_Stub.lookup(Unknown Source)",
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} == message
+        end
+      end
+    end
+
+    context "should NOT match" do
+      [
+        #Sorry no idea
+      ].each do |message|
+        sample message do 
+          insist {subject["result"]} != message
+        end
+      end
+    end
+  end
+end
diff --git a/spec/filters/grok.rb b/spec/filters/grok.rb
index 94dbbfaac9b..5d84105b690 100644
--- a/spec/filters/grok.rb
+++ b/spec/filters/grok.rb
@@ -12,7 +12,7 @@
     config <<-CONFIG
       filter {
         grok {
-          match => [ "message", "%{SYSLOGLINE}" ]
+          match => { "message" => "%{SYSLOGLINE}" }
           singles => true
           overwrite => [ "message" ]
         }
@@ -35,7 +35,7 @@
     config <<-CONFIG
       filter {
         grok {
-          match => [ "message",  "%{SYSLOG5424LINE}" ]
+          match => { "message" => "%{SYSLOG5424LINE}" }
           singles => true
         }
       }
@@ -163,7 +163,7 @@
     config <<-CONFIG
       filter {
         grok {
-          match => [ "message",  "(?:hello|world) %{NUMBER}" ]
+          match => { "message" => "(?:hello|world) %{NUMBER}" }
           named_captures_only => false
         }
       }
@@ -178,7 +178,7 @@
     config <<-CONFIG
       filter {
         grok {
-          match => [ "message",  "%{NUMBER:foo:int} %{NUMBER:bar:float}" ]
+          match => { "message" => "%{NUMBER:foo:int} %{NUMBER:bar:float}" }
           singles => true
         }
       }
@@ -196,7 +196,7 @@
     config <<-CONFIG
       filter {
         grok {
-          match => [ "message",  "%{FIZZLE=\\d+}" ]
+          match => { "message" => "%{FIZZLE=\\d+}" }
           named_captures_only => false
           singles => true
         }
@@ -212,8 +212,8 @@
     config <<-CONFIG
       filter {
         grok {
-          match => [ "message",  "%{WORD:word}" ]
-          match => [ "examplefield", "%{NUMBER:num}" ]
+          match => { "message" => "%{WORD:word}" }
+          match => { "examplefield" => "%{NUMBER:num}" }
           break_on_match => false
           singles => true
         }
@@ -230,7 +230,7 @@
     config <<-CONFIG
       filter {
         grok {
-          match => [ "message",  "matchme %{NUMBER:fancy}" ]
+          match => { "message" => "matchme %{NUMBER:fancy}" }
           singles => true
           add_field => [ "new_field", "%{fancy}" ]
         }
@@ -253,7 +253,7 @@
       config <<-CONFIG
         filter {
           grok {
-            match => [ "message",  "1=%{WORD:foo1} *(2=%{WORD:foo2})?" ]
+            match => { "message" => "1=%{WORD:foo1} *(2=%{WORD:foo2})?" }
           }
         }
       CONFIG
@@ -271,7 +271,7 @@
       config <<-CONFIG
         filter {
           grok {
-            match => [ "message",  "1=%{WORD:foo1} *(2=%{WORD:foo2})?" ]
+            match => { "message" => "1=%{WORD:foo1} *(2=%{WORD:foo2})?" }
             keep_empty_captures => true
           }
         }
@@ -292,7 +292,7 @@
     config <<-CONFIG
       filter {
         grok {
-          match => [ "message",  "Hello %{WORD}. %{WORD:foo}" ]
+          match => { "message" => "Hello %{WORD}. %{WORD:foo}" }
           named_captures_only => false
           singles => true
         }
@@ -313,7 +313,7 @@
         filter {
           grok {
             singles => true
-            match => [ "message",  "(?<foo>\w+)" ]
+            match => { "message" => "(?<foo>\w+)" }
           }
         }
       CONFIG
@@ -328,7 +328,7 @@
         filter {
           grok {
             singles => true
-            match => [ "message",  "(?<timestamp>%{DATE_EU} %{TIME})" ]
+            match => { "message" => "(?<timestamp>%{DATE_EU} %{TIME})" }
           }
         }
       CONFIG
@@ -344,7 +344,7 @@
     config <<-'CONFIG'
       filter {
         grok {
-          match => [ "status", "^403$" ]
+          match => { "status" => "^403$" }
           add_tag => "four_oh_three"
         }
       }
@@ -360,7 +360,7 @@
     config <<-'CONFIG'
       filter {
         grok {
-          match => [ "version", "^1.0$" ]
+          match => { "version" => "^1.0$" }
           add_tag => "one_point_oh"
         }
       }
@@ -404,7 +404,7 @@
     config <<-CONFIG
       filter {
         grok {
-          match => [ "message",  "matchme %{NUMBER:fancy}" ]
+          match => { "message" => "matchme %{NUMBER:fancy}" }
           tag_on_failure => false
         }
       }
@@ -423,7 +423,7 @@
     config <<-CONFIG
       filter {
         grok {
-          match => [ "message",  "%{DATE_EU:stimestamp}" ]
+          match => { "message" => "%{DATE_EU:stimestamp}" }
           singles => true
         }
       }
@@ -438,7 +438,7 @@
     config <<-CONFIG
       filter {
         grok {
-          match => [ "message",  "%{WORD:foo-bar}" ]
+          match => { "message" => "%{WORD:foo-bar}" }
           singles => true
         }
       }
@@ -464,7 +464,7 @@
       }
       filter {
         grok {
-          match => [ "message", "%{SYSLOGLINE}" ]
+          match => { "message" => "%{SYSLOGLINE}" }
           singles => true
           overwrite => [ "message" ]
         }
@@ -486,7 +486,7 @@
     config <<-CONFIG
       filter {
         grok {
-          match => [ "message", "%{INT:foo}|%{WORD:foo}" ]
+          match => { "message" => "%{INT:foo}|%{WORD:foo}" }
           singles => true
         }
       }
@@ -500,4 +500,133 @@
       insist { subject["foo"] }.is_a?(String)
     end
   end
+
+  describe "break_on_match default should be true and first match should exit filter" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{INT:foo}"
+                     "somefield" => "%{INT:bar}"}
+        }
+      }
+    CONFIG
+
+    sample("message" => "hello world 123", "somefield" => "testme abc 999") do
+      insist { subject["foo"] } == "123"
+      insist { subject["bar"] }.nil?
+    end
+  end
+
+  describe "break_on_match when set to false should try all patterns" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{INT:foo}"
+                     "somefield" => "%{INT:bar}"}
+          break_on_match => false
+        }
+      }
+    CONFIG
+
+    sample("message" => "hello world 123", "somefield" => "testme abc 999") do
+      insist { subject["foo"] } == "123"
+      insist { subject["bar"] } == "999"
+    end
+  end
+
+  describe "LOGSTASH-1547 - break_on_match should work on fields with multiple patterns" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => ["%{GREEDYDATA:name1}beard", "tree%{GREEDYDATA:name2}"] }
+          break_on_match => false
+        }
+      }
+    CONFIG
+
+    sample "treebranch" do
+      insist { subject["name2"] } == "branch"
+    end
+
+    sample "bushbeard" do
+      insist { subject["name1"] } == "bush"
+    end
+
+    sample "treebeard" do
+      insist { subject["name1"] } == "tree"
+      insist { subject["name2"] } == "beard"
+    end
+  end
+
+  describe "break_on_match default for array input with single grok pattern" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => "%{INT:foo}"}
+        }
+      }
+    CONFIG
+
+    # array input --
+    sample("message" => ["hello world 123", "line 23"]) do
+      insist { subject["foo"] } == ["123", "23"]
+      insist { subject["tags"] }.nil?
+    end
+
+    # array input, one of them matches
+    sample("message" => ["hello world 123", "abc"]) do
+      insist { subject["foo"] } == "123"
+      insist { subject["tags"] }.nil?
+    end
+  end
+
+  describe "break_on_match = true (default) for array input with multiple grok pattern" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => ["%{INT:foo}", "%{WORD:bar}"] }
+        }
+      }
+    CONFIG
+
+    # array input --
+    sample("message" => ["hello world 123", "line 23"]) do
+      insist { subject["foo"] } == ["123", "23"]
+      insist { subject["bar"] }.nil?
+      insist { subject["tags"] }.nil?
+    end
+
+    # array input, one of them matches
+    sample("message" => ["hello world", "line 23"]) do
+      insist { subject["bar"] } == "hello"
+      insist { subject["foo"] } == "23"
+      insist { subject["tags"] }.nil?
+    end
+  end
+
+  describe "break_on_match = false for array input with multiple grok pattern" do
+    config <<-CONFIG
+      filter {
+        grok {
+          match => { "message" => ["%{INT:foo}", "%{WORD:bar}"] }
+          break_on_match => false
+        }
+      }
+    CONFIG
+
+    # array input --
+    sample("message" => ["hello world 123", "line 23"]) do
+      insist { subject["foo"] } == ["123", "23"]
+      insist { subject["bar"] } == ["hello", "line"]
+      insist { subject["tags"] }.nil?
+    end
+
+    # array input, one of them matches
+    sample("message" => ["hello world", "line 23"]) do
+      insist { subject["bar"] } == ["hello", "line"]
+      insist { subject["foo"] } == "23"
+      insist { subject["tags"] }.nil?
+    end
+  end
+
 end
diff --git a/spec/filters/grok/timeout2.rb b/spec/filters/grok/timeout2.rb
index e4237e3648b..89c3a0cc712 100644
--- a/spec/filters/grok/timeout2.rb
+++ b/spec/filters/grok/timeout2.rb
@@ -9,7 +9,7 @@
     config <<-'CONFIG'
       filter {
         grok {
-         match  => [ "message", "%{SYSLOGBASE:ts1} \[\#\|%{TIMESTAMP_ISO8601:ts2}\|%{DATA} for %{PATH:url} = %{POSINT:delay} ms.%{GREEDYDATA}" ]
+         match  => { "message" => "%{SYSLOGBASE:ts1} \[\#\|%{TIMESTAMP_ISO8601:ts2}\|%{DATA} for %{PATH:url} = %{POSINT:delay} ms.%{GREEDYDATA}" }
         }
       }
     CONFIG
diff --git a/spec/filters/json.rb b/spec/filters/json.rb
index 7041a04bad5..b571b9b4023 100644
--- a/spec/filters/json.rb
+++ b/spec/filters/json.rb
@@ -1,5 +1,6 @@
 require "test_utils"
 require "logstash/filters/json"
+require "logstash/timestamp"
 
 describe LogStash::Filters::Json do
   extend LogStash::RSpec
@@ -16,7 +17,7 @@
 
     sample '{ "hello": "world", "list": [ 1, 2, 3 ], "hash": { "k": "v" } }' do
       insist { subject["hello"] } == "world"
-      insist { subject["list" ] } == [1,2,3]
+      insist { subject["list" ].to_a } == [1,2,3] # to_a for JRuby + JrJacksom which creates Java ArrayList
       insist { subject["hash"] } == { "k" => "v" }
     end
   end
@@ -34,7 +35,7 @@
 
     sample '{ "hello": "world", "list": [ 1, 2, 3 ], "hash": { "k": "v" } }' do
       insist { subject["data"]["hello"] } == "world"
-      insist { subject["data"]["list" ] } == [1,2,3]
+      insist { subject["data"]["list" ].to_a } == [1,2,3] # to_a for JRuby + JrJacksom which creates Java ArrayList
       insist { subject["data"]["hash"] } == { "k" => "v" }
     end
   end
@@ -65,8 +66,8 @@
     CONFIG
 
     sample "{ \"@timestamp\": \"2013-10-19T00:14:32.996Z\" }" do
-      insist { subject["@timestamp"] }.is_a?(Time)
-      insist { subject["@timestamp"].to_json } == "\"2013-10-19T00:14:32.996Z\""
+      insist { subject["@timestamp"] }.is_a?(LogStash::Timestamp)
+      insist { LogStash::Json.dump(subject["@timestamp"]) } == "\"2013-10-19T00:14:32.996Z\""
     end
   end
 
diff --git a/spec/filters/mutate.rb b/spec/filters/mutate.rb
index 9eb281fccb9..1c6ee2eacdd 100644
--- a/spec/filters/mutate.rb
+++ b/spec/filters/mutate.rb
@@ -136,7 +136,7 @@
     config <<-CONFIG
       filter {
         grok {
-          match => [ "message", "%{WORD:foo}" ]
+          match => { "message" => "%{WORD:foo}" }
         }
         mutate {
           lowercase => "foo"
@@ -178,5 +178,53 @@
       insist { subject["[foo][bar]"] }.is_a?(Fixnum)
     end
   end
+
+  #LOGSTASH-1529
+  describe "gsub on a String with dynamic fields (%{}) in pattern" do
+    config '
+      filter {
+        mutate {
+          gsub => [ "unicorns", "of type %{unicorn_type}", "green" ]
+        }
+      }'
+
+    sample("unicorns" => "Unicorns of type blue are common", "unicorn_type" => "blue") do
+      insist { subject["unicorns"] } == "Unicorns green are common"
+    end
+  end
+
+  #LOGSTASH-1529
+  describe "gsub on a String with dynamic fields (%{}) in pattern and replace" do
+    config '
+      filter {
+        mutate {
+          gsub => [ "unicorns2", "of type %{unicorn_color}", "%{unicorn_color} and green" ]
+        }
+      }'
+
+    sample("unicorns2" => "Unicorns of type blue are common", "unicorn_color" => "blue") do
+      insist { subject["unicorns2"] } == "Unicorns blue and green are common"
+    end
+  end
+
+  #LOGSTASH-1529
+  describe "gsub on a String array with dynamic fields in pattern" do
+    config '
+      filter {
+        mutate {
+          gsub => [ "unicorns_array", "of type %{color}", "blue and green" ]
+        }
+      }'
+
+    sample("unicorns_array" => [
+        "Unicorns of type blue are found in Alaska", "Unicorns of type blue are extinct" ],
+           "color" => "blue"
+    ) do
+      insist { subject["unicorns_array"] } == [
+          "Unicorns blue and green are found in Alaska",
+          "Unicorns blue and green are extinct"
+      ]
+    end
+  end
 end
 
diff --git a/spec/inputs/base.rb b/spec/inputs/base.rb
new file mode 100644
index 00000000000..bfb546343e0
--- /dev/null
+++ b/spec/inputs/base.rb
@@ -0,0 +1,13 @@
+# encoding: utf-8
+require "test_utils"
+
+describe "LogStash::Inputs::Base#fix_streaming_codecs" do
+  it "should carry the charset setting along when switching" do
+    require "logstash/inputs/tcp"
+    require "logstash/codecs/plain"
+    plain = LogStash::Codecs::Plain.new("charset" => "CP1252")
+    tcp = LogStash::Inputs::Tcp.new("codec" => plain, "port" => 3333)
+    tcp.instance_eval { fix_streaming_codecs }
+    insist { tcp.codec.charset } == "CP1252"
+  end
+end
diff --git a/spec/inputs/elasticsearch.rb b/spec/inputs/elasticsearch.rb
new file mode 100644
index 00000000000..d695a2ed871
--- /dev/null
+++ b/spec/inputs/elasticsearch.rb
@@ -0,0 +1,80 @@
+require "test_utils"
+require "logstash/inputs/elasticsearch"
+
+describe "inputs/elasticsearch" do
+  extend LogStash::RSpec
+
+  search_response = <<-RESPONSE
+    {
+      "_scroll_id":"xxx",
+      "took":5,
+      "timed_out":false,
+      "_shards":{"total":15,"successful":15,"failed":0},
+      "hits":{
+        "total":1000050,
+        "max_score":1.0,
+        "hits":[
+          {
+            "_index":"logstash2",
+            "_type":"logs",
+            "_id":"AmaqL7VuSWKF-F6N_Gz72g",
+            "_score":1.0,
+            "_source" : {
+              "message":"foobar",
+              "@version":"1",
+              "@timestamp":"2014-05-19T21:08:39.000Z",
+              "host":"colin-mbp13r"
+            }
+          }
+        ]
+      }
+    }
+  RESPONSE
+
+  scroll_response = <<-RESPONSE
+    {
+      "hits":{
+        "hits":[]
+      }
+    }
+  RESPONSE
+
+  config <<-CONFIG
+    input {
+      elasticsearch {
+        host => "localhost"
+        scan => false
+      }
+    }
+  CONFIG
+
+  it "should retrieve json event from elasticseach" do
+    # I somewhat duplicated our "input" rspec extension because I needed to add mocks for the the actual ES calls
+    # and rspec expectations need to be in "it" statement but the "input" extension defines the "it"
+    # TODO(colin) see how we can improve our rspec extension to better integrate in these scenarios
+
+    expect_any_instance_of(LogStash::Inputs::Elasticsearch).to receive(:execute_search_request).and_return(search_response)
+    expect_any_instance_of(LogStash::Inputs::Elasticsearch).to receive(:execute_scroll_request).with(any_args).and_return(scroll_response)
+
+    pipeline = LogStash::Pipeline.new(config)
+    queue = Queue.new
+    pipeline.instance_eval do
+      @output_func = lambda { |event| queue << event }
+    end
+    pipeline_thread = Thread.new { pipeline.run }
+    event = queue.pop
+
+    insist { event["message"] } == "foobar"
+
+    # do not call pipeline.shutdown here, as it will stop the plugin execution randomly
+    # and maybe kill input before calling execute_scroll_request.
+    # TODO(colin) we should rework the pipeliene shutdown to allow a soft/clean shutdown mecanism,
+    # using a shutdown event which can be fed into each plugin queue and when the plugin sees it
+    # exits after completing its processing.
+    #
+    # pipeline.shutdown
+    #
+    # instead, since our scroll_response will terminate the plugin, we can just join the pipeline thread
+    pipeline_thread.join
+  end
+end
diff --git a/spec/inputs/gelf.rb b/spec/inputs/gelf.rb
index e2cab136d36..458b34a647b 100644
--- a/spec/inputs/gelf.rb
+++ b/spec/inputs/gelf.rb
@@ -11,39 +11,39 @@
     gelfclient = GELF::Notifier.new(host,port,chunksize)
 
     config <<-CONFIG
-input {
-  gelf {
-    port => "#{port}"
-    host => "#{host}"
-  }
-}
+      input {
+        gelf {
+          port => "#{port}"
+          host => "#{host}"
+        }
+      }
     CONFIG
 
     input do |pipeline, queue|
       Thread.new { pipeline.run }
       sleep 0.1 while !pipeline.ready?
-      
-      # generate random characters (message is zipped!) from printable ascii ( SPACE till ~ )  
+
+      # generate random characters (message is zipped!) from printable ascii ( SPACE till ~ )
       # to trigger gelf chunking
       s = StringIO.new
-      for i in 1..2000 
-          s << 32 + rand(126-32)
+      for i in 1..2000
+        s << 32 + rand(126-32)
       end
       large_random = s.string
-      
-      [ "hello", 
-        "world", 
-        large_random, 
-        "we survived gelf!" 
-      ].each do |m| 
-  	gelfclient.notify!( "short_message" => m )
-        # poll at most 10 times 
-        waits = 0 
+
+      [ "hello",
+        "world",
+        large_random,
+        "we survived gelf!"
+      ].each do |m|
+  	    gelfclient.notify!( "short_message" => m )
+        # poll at most 10 times
+        waits = 0
         while waits < 10 and queue.size == 0
-           sleep 0.1 
-           waits += 1
+          sleep 0.1
+          waits += 1
         end
-        insist { queue.size } > 0  
+        insist { queue.size } > 0
         insist { queue.pop["message"] } == m
       end
 
diff --git a/spec/inputs/generator.rb b/spec/inputs/generator.rb
index 45579f620d2..b21ffaeb77f 100644
--- a/spec/inputs/generator.rb
+++ b/spec/inputs/generator.rb
@@ -1,9 +1,9 @@
 require "test_utils"
 
-describe "inputs/generator", :performance => true do
+describe "inputs/generator" do
   extend LogStash::RSpec
 
-  describe "generate events" do
+  context "performance", :performance => true do
     event_count = 100000 + rand(50000)
 
     config <<-CONFIG
@@ -27,4 +27,60 @@
       pipeline.shutdown
     end # input
   end
+
+  context "generate configured message" do
+    config <<-CONFIG
+      input {
+        generator {
+          count => 2
+          message => "foo"
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      event = queue.pop
+      insist { event["sequence"] } == 0
+      insist { event["message"] } == "foo"
+
+      event = queue.pop
+      insist { event["sequence"] } == 1
+      insist { event["message"] } == "foo"
+
+      insist { queue.size } == 0
+      pipeline.shutdown
+    end # input
+
+    context "generate message from stdin" do
+      config <<-CONFIG
+        input {
+          generator {
+            count => 2
+            message => "stdin"
+          }
+        }
+      CONFIG
+
+      input do |pipeline, queue|
+        saved_stdin = $stdin
+        stdin_mock = StringIO.new
+        $stdin = stdin_mock
+        stdin_mock.should_receive(:readline).once.and_return("bar")
+
+        Thread.new { pipeline.run }
+        event = queue.pop
+        insist { event["sequence"] } == 0
+        insist { event["message"] } == "bar"
+
+        event = queue.pop
+        insist { event["sequence"] } == 1
+        insist { event["message"] } == "bar"
+
+        insist { queue.size } == 0
+        pipeline.shutdown
+        $stdin = saved_stdin
+      end # input
+    end
+  end
 end
diff --git a/spec/inputs/pipe.rb b/spec/inputs/pipe.rb
new file mode 100644
index 00000000000..067937b4a75
--- /dev/null
+++ b/spec/inputs/pipe.rb
@@ -0,0 +1,60 @@
+# encoding: utf-8
+require "test_utils"
+require "tempfile"
+
+describe "inputs/pipe" do
+  extend LogStash::RSpec
+
+  describe "echo" do
+    event_count = 1
+    tmp_file = Tempfile.new('logstash-spec-input-pipe')
+
+    config <<-CONFIG
+    input {
+      pipe {
+        command => "echo ☹"
+      }
+    }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      events = event_count.times.collect { queue.pop }
+      event_count.times do |i|
+        insist { events[i]["message"] } == "☹"
+      end
+    end # input
+  end
+
+  describe "tail -f" do
+    event_count = 10
+    tmp_file = Tempfile.new('logstash-spec-input-pipe')
+
+    config <<-CONFIG
+    input {
+      pipe {
+        command => "tail -f #{tmp_file.path}"
+      }
+    }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      File.open(tmp_file, "a") do |fd|
+        event_count.times do |i|
+          # unicode smiley for testing unicode support!
+          fd.puts("#{i} ☹")
+        end
+      end
+      events = event_count.times.collect { queue.pop }
+      event_count.times do |i|
+        insist { events[i]["message"] } == "#{i} ☹"
+      end
+    end # input
+  end
+
+end
diff --git a/spec/inputs/stdin.rb b/spec/inputs/stdin.rb
new file mode 100644
index 00000000000..a3efea1e5e4
--- /dev/null
+++ b/spec/inputs/stdin.rb
@@ -0,0 +1,23 @@
+# encoding: utf-8
+require "test_utils"
+require "socket"
+require "logstash/inputs/stdin"
+
+describe LogStash::Inputs::Stdin do
+  context "codec (PR #1372)" do
+    it "switches from plain to line" do
+      require "logstash/codecs/plain"
+      require "logstash/codecs/line"
+      plugin = LogStash::Inputs::Stdin.new("codec" => LogStash::Codecs::Plain.new)
+      plugin.register
+      insist { plugin.codec }.is_a?(LogStash::Codecs::Line)
+    end
+    it "switches from json to json_lines" do
+      require "logstash/codecs/json"
+      require "logstash/codecs/json_lines"
+      plugin = LogStash::Inputs::Stdin.new("codec" => LogStash::Codecs::JSON.new)
+      plugin.register
+      insist { plugin.codec }.is_a?(LogStash::Codecs::JSONLines)
+    end
+  end
+end
diff --git a/spec/inputs/tcp.rb b/spec/inputs/tcp.rb
index e4ea8312aba..cbcccef404a 100644
--- a/spec/inputs/tcp.rb
+++ b/spec/inputs/tcp.rb
@@ -1,11 +1,31 @@
-# coding: utf-8
+# encoding: utf-8
 require "test_utils"
 require "socket"
+require "timeout"
+require "logstash/json"
+require "logstash/inputs/tcp"
 
-describe "inputs/tcp", :socket => true do
+describe LogStash::Inputs::Tcp do
   extend LogStash::RSpec
 
-  describe "read plain with unicode" do
+  context "codec (PR #1372)" do
+    it "switches from plain to line" do
+      require "logstash/codecs/plain"
+      require "logstash/codecs/line"
+      plugin = LogStash::Inputs::Tcp.new("codec" => LogStash::Codecs::Plain.new, "port" => 0)
+      plugin.register
+      insist { plugin.codec }.is_a?(LogStash::Codecs::Line)
+    end
+    it "switches from json to json_lines" do
+      require "logstash/codecs/json"
+      require "logstash/codecs/json_lines"
+      plugin = LogStash::Inputs::Tcp.new("codec" => LogStash::Codecs::JSON.new, "port" => 0)
+      plugin.register
+      insist { plugin.codec }.is_a?(LogStash::Codecs::JSONLines)
+    end
+  end
+
+  describe "read plain with unicode", :socket => true do
     event_count = 10
     port = 5511
     config <<-CONFIG
@@ -27,6 +47,9 @@
       end
       socket.close
 
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < event_count}
+
       events = event_count.times.collect { queue.pop }
       event_count.times do |i|
         insist { events[i]["message"] } == "#{i} ☹"
@@ -34,7 +57,7 @@
     end # input
   end
 
-  describe "read events with plain codec and ISO-8859-1 charset" do
+  describe "read events with plain codec and ISO-8859-1 charset", :socket => true do
     port = 5513
     charset = "ISO-8859-1"
     config <<-CONFIG
@@ -56,6 +79,9 @@
       socket.puts(text)
       socket.close
 
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < 1}
+
       event = queue.pop
       # Make sure the 0xA3 latin-1 code converts correctly to UTF-8.
       pending("charset conv broken") do
@@ -66,7 +92,7 @@
     end # input
   end
 
-  describe "read events with json codec" do
+  describe "read events with json codec", :socket => true do
     port = 5514
     config <<-CONFIG
       input {
@@ -89,12 +115,15 @@
       }
 
       socket = Stud.try(5.times) { TCPSocket.new("127.0.0.1", port) }
-      socket.puts(data.to_json)
+      socket.puts(LogStash::Json.dump(data))
       socket.close
 
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < 1}
+
       event = queue.pop
       insist { event["hello"] } == data["hello"]
-      insist { event["foo"] } == data["foo"]
+      insist { event["foo"].to_a } == data["foo"] # to_a to cast Java ArrayList produced by JrJackson
       insist { event["baz"] } == data["baz"]
 
       # Make sure the tcp input, w/ json codec, uses the event's 'host' value,
@@ -103,7 +132,7 @@
     end # input
   end
 
-  describe "read events with json codec (testing 'host' handling)" do
+  describe "read events with json codec (testing 'host' handling)", :socket => true do
     port = 5514
     config <<-CONFIG
       input {
@@ -123,16 +152,19 @@
       }
 
       socket = Stud.try(5.times) { TCPSocket.new("127.0.0.1", port) }
-      socket.puts(data.to_json)
+      socket.puts(LogStash::Json.dump(data))
       socket.close
 
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < 1}
+
       event = queue.pop
       insist { event["hello"] } == data["hello"]
       insist { event }.include?("host")
     end # input
   end
 
-  describe "read events with json_lines codec" do
+  describe "read events with json_lines codec", :socket => true do
     port = 5515
     config <<-CONFIG
       input {
@@ -157,20 +189,91 @@
       socket = Stud.try(5.times) { TCPSocket.new("127.0.0.1", port) }
       (1..5).each do |idx|
         data["idx"] = idx
-        socket.puts(data.to_json+"\n")
+        socket.puts(LogStash::Json.dump(data) + "\n")
       end # do
       socket.close
 
       (1..5).each do |idx|
         event = queue.pop
         insist { event["hello"] } == data["hello"]
-        insist { event["foo"] } == data["foo"]
+        insist { event["foo"].to_a } == data["foo"] # to_a to cast Java ArrayList produced by JrJackson
         insist { event["baz"] } == data["baz"]
         insist { event["idx"] } == idx
       end # do
     end # input
   end # describe
-end
 
+  describe "one message per connection" do
+    event_count = 10
+    port = 5515
+    config <<-CONFIG
+      input {
+        tcp {
+          port => #{port}
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      event_count.times do |i|
+        socket = Stud.try(5.times) { TCPSocket.new("127.0.0.1", port) }
+        socket.puts("#{i}")
+        socket.flush
+        socket.close
+      end
+
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < event_count}
+
+      # since each message is sent on its own tcp connection & thread, exact receiving order cannot be garanteed
+      events = event_count.times.collect{queue.pop}.sort_by{|event| event["message"]}
+
+      event_count.times do |i|
+        insist { events[i]["message"] } == "#{i}"
+      end
+    end # input
+  end
+
+  describe "connection threads are cleaned up when connection is closed" do
+    event_count = 10
+    port = 5515
+    config <<-CONFIG
+      input {
+        tcp {
+          port => #{port}
+        }
+      }
+    CONFIG
 
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      inputs = pipeline.instance_variable_get("@inputs")
+      insist { inputs.size } == 1
+
+      sockets = event_count.times.map do |i|
+        socket = Stud.try(5.times) { TCPSocket.new("127.0.0.1", port) }
+        socket.puts("#{i}")
+        socket.flush
+        socket
+      end
 
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < event_count}
+
+      # we should have "event_count" pending threads since sockets were not closed yet
+      client_threads = inputs[0].instance_variable_get("@client_threads")
+      insist { client_threads.size } == event_count
+
+      # close all sockets and make sure there is not more pending threads
+      sockets.each{|socket| socket.close}
+      Timeout.timeout(1) {sleep 0.1 while client_threads.size > 0}
+      insist { client_threads.size } == 0 # this check is actually useless per previous line
+
+    end # input
+  end
+end
diff --git a/spec/json.rb b/spec/json.rb
new file mode 100644
index 00000000000..147b6196d38
--- /dev/null
+++ b/spec/json.rb
@@ -0,0 +1,94 @@
+# encoding: utf-8
+require "logstash/json"
+require "logstash/environment"
+require "logstash/util"
+
+describe LogStash::Json do
+
+  let(:hash)   {{"a" => 1}}
+  let(:json_hash)   {"{\"a\":1}"}
+
+  let(:string) {"foobar"}
+  let(:json_string) {"\"foobar\""}
+
+  let(:array)  {["foo", "bar"]}
+  let(:json_array)  {"[\"foo\",\"bar\"]"}
+
+  let(:multi) {
+    [
+      {:ruby => "foo bar baz", :json => "\"foo bar baz\""},
+      {:ruby => "1", :json => "\"1\""},
+      {:ruby => {"a" => true}, :json => "{\"a\":true}"},
+      {:ruby => {"a" => nil}, :json => "{\"a\":null}"},
+      {:ruby => ["a", "b"], :json => "[\"a\",\"b\"]"},
+      {:ruby => [1, 2], :json => "[1,2]"},
+      {:ruby => [1, nil], :json => "[1,null]"},
+      {:ruby => {"a" => [1, 2]}, :json => "{\"a\":[1,2]}"},
+      {:ruby => {"a" => {"b" => 2}}, :json => "{\"a\":{\"b\":2}}"},
+      # {:ruby => , :json => },
+    ]
+  }
+
+  if LogStash::Environment.jruby?
+
+    ### JRuby specific
+
+    context "jruby deserialize" do
+      it "should respond to load and deserialize object" do
+        expect(JrJackson::Raw).to receive(:parse_raw).with(json_hash).and_call_original
+        expect(LogStash::Json.load(json_hash)).to eql(hash)
+      end
+    end
+
+    context "jruby serialize" do
+      it "should respond to dump and serialize object" do
+        expect(JrJackson::Json).to receive(:dump).with(string).and_call_original
+        expect(LogStash::Json.dump(string)).to eql(json_string)
+      end
+
+      it "should call JrJackson::Raw.generate for Hash" do
+        expect(JrJackson::Raw).to receive(:generate).with(hash).and_call_original
+        expect(LogStash::Json.dump(hash)).to eql(json_hash)
+      end
+
+      it "should call JrJackson::Raw.generate for Array" do
+        expect(JrJackson::Raw).to receive(:generate).with(array).and_call_original
+        expect(LogStash::Json.dump(array)).to eql(json_array)
+      end
+    end
+  else
+
+    ### MRI specific
+
+    it "should respond to load and deserialize object on mri" do
+      expect(Oj).to receive(:load).with(json).and_call_original
+      expect(LogStash::Json.load(json)).to eql(hash)
+    end
+
+    it "should respond to dump and serialize object on mri" do
+      expect(Oj).to receive(:dump).with(hash, anything).and_call_original
+      expect(LogStash::Json.dump(hash)).to eql(json)
+    end
+  end
+
+  ### non specific
+
+  it "should correctly deserialize" do
+    multi.each do |test|
+      # because JrJackson in :raw mode uses Java::JavaUtil::LinkedHashMap and
+      # Java::JavaUtil::ArrayList, we must cast to compare.
+      # other than that, they quack like their Ruby equivalent
+      expect(LogStash::Util.normalize(LogStash::Json.load(test[:json]))).to eql(test[:ruby])
+    end
+  end
+
+  it "should correctly serialize" do
+    multi.each do |test|
+      expect(LogStash::Json.dump(test[:ruby])).to eql(test[:json])
+    end
+  end
+
+  it "should raise Json::ParserError on invalid json" do
+    expect{LogStash::Json.load("abc")}.to raise_error LogStash::Json::ParserError
+  end
+end
diff --git a/spec/outputs/elasticsearch.rb b/spec/outputs/elasticsearch.rb
index a41955e778c..836c9ef56e1 100644
--- a/spec/outputs/elasticsearch.rb
+++ b/spec/outputs/elasticsearch.rb
@@ -1,6 +1,7 @@
 require "test_utils"
 require "ftw"
 require "logstash/plugin"
+require "logstash/json"
 
 describe "outputs/elasticsearch" do
   extend LogStash::RSpec
@@ -53,7 +54,7 @@
         data = ""
         response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
         response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
+        result = LogStash::Json.load(data)
         count = result["count"]
         insist { count } == event_count
       end
@@ -61,7 +62,7 @@
       response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
       data = ""
       response.read_body { |chunk| data << chunk }
-      result = JSON.parse(data)
+      result = LogStash::Json.load(data)
       result["hits"]["hits"].each do |doc|
         # With no 'index_type' set, the document type should be the type
         # set on the input
@@ -104,7 +105,7 @@
           data = ""
           response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
           response.read_body { |chunk| data << chunk }
-          result = JSON.parse(data)
+          result = LogStash::Json.load(data)
           count = result["count"]
           insist { count } == event_count
         end
@@ -112,7 +113,7 @@
         response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
         data = ""
         response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
+        result = LogStash::Json.load(data)
         result["hits"]["hits"].each do |doc|
           insist { doc["_type"] } == "logs"
         end
@@ -151,7 +152,7 @@
           data = ""
           response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
           response.read_body { |chunk| data << chunk }
-          result = JSON.parse(data)
+          result = LogStash::Json.load(data)
           count = result["count"]
           insist { count } == event_count
         end
@@ -159,7 +160,7 @@
         response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
         data = ""
         response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
+        result = LogStash::Json.load(data)
         result["hits"]["hits"].each do |doc|
           insist { doc["_type"] } == "generated"
         end
@@ -195,7 +196,7 @@
         data = ""
         response = ftw.get!("http://127.0.0.1:9200/#{index_name}/_count?q=*")
         response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
+        result = LogStash::Json.load(data)
         count = result["count"]
         insist { count } == 100
       end
@@ -203,7 +204,7 @@
       response = ftw.get!("http://127.0.0.1:9200/#{index_name}/_search?q=*&size=1000")
       data = ""
       response.read_body { |chunk| data << chunk }
-      result = JSON.parse(data)
+      result = LogStash::Json.load(data)
       result["hits"]["hits"].each do |doc|
         insist { doc["_type"] } == "logs"
       end
@@ -241,7 +242,7 @@
           data = ""
           response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
           response.read_body { |chunk| data << chunk }
-          result = JSON.parse(data)
+          result = LogStash::Json.load(data)
           count = result["count"]
           insist { count } == event_count
         end
@@ -249,7 +250,7 @@
         response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
         data = ""
         response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
+        result = LogStash::Json.load(data)
         result["hits"]["hits"].each do |doc|
           insist { doc["_type"] } == "generated"
         end
diff --git a/spec/outputs/elasticsearch_http.rb b/spec/outputs/elasticsearch_http.rb
index d1b1072e06a..f668b37191d 100644
--- a/spec/outputs/elasticsearch_http.rb
+++ b/spec/outputs/elasticsearch_http.rb
@@ -1,4 +1,5 @@
 require "test_utils"
+require "logstash/json"
 
 describe "outputs/elasticsearch_http", :elasticsearch => true do
   extend LogStash::RSpec
@@ -45,7 +46,7 @@
         data = ""
         response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
         response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
+        result = LogStash::Json.load(data)
         count = result["count"]
         insist { count } == event_count
       end
@@ -53,7 +54,7 @@
       response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
       data = ""
       response.read_body { |chunk| data << chunk }
-      result = JSON.parse(data)
+      result = LogStash::Json.load(data)
       result["hits"]["hits"].each do |doc|
         # With no 'index_type' set, the document type should be the type
         # set on the input
@@ -96,7 +97,7 @@
           data = ""
           response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
           response.read_body { |chunk| data << chunk }
-          result = JSON.parse(data)
+          result = LogStash::Json.load(data)
           count = result["count"]
           insist { count } == event_count
         end
@@ -104,7 +105,7 @@
         response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
         data = ""
         response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
+        result = LogStash::Json.load(data)
         result["hits"]["hits"].each do |doc|
           insist { doc["_type"] } == "logs"
         end
@@ -143,7 +144,7 @@
           data = ""
           response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
           response.read_body { |chunk| data << chunk }
-          result = JSON.parse(data)
+          result = LogStash::Json.load(data)
           count = result["count"]
           insist { count } == event_count
         end
@@ -151,7 +152,7 @@
         response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
         data = ""
         response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
+        result = LogStash::Json.load(data)
         result["hits"]["hits"].each do |doc|
           insist { doc["_type"] } == "generated"
         end
diff --git a/spec/outputs/file.rb b/spec/outputs/file.rb
index bdf6a769809..a49366bd4e3 100644
--- a/spec/outputs/file.rb
+++ b/spec/outputs/file.rb
@@ -1,5 +1,6 @@
 require "test_utils"
 require "logstash/outputs/file"
+require "logstash/json"
 require "tempfile"
 
 describe LogStash::Outputs::File do
@@ -28,7 +29,7 @@
       line_num = 0
       # Now check all events for order and correctness.
       File.foreach(tmp_file) do |line|
-        event = LogStash::Event.new(JSON.parse(line))
+        event = LogStash::Event.new(LogStash::Json.load(line))
         insist {event["message"]} == "hello world"
         insist {event["sequence"]} == line_num
         line_num += 1
@@ -61,7 +62,7 @@
       line_num = 0
       # Now check all events for order and correctness.
       Zlib::GzipReader.open(tmp_file.path).each_line do |line|
-        event = LogStash::Event.new(JSON.parse(line))
+        event = LogStash::Event.new(LogStash::Json.load(line))
         insist {event["message"]} == "hello world"
         insist {event["sequence"]} == line_num
         line_num += 1
diff --git a/spec/outputs/redis.rb b/spec/outputs/redis.rb
index 442d8b01734..3c4dbeb04d0 100644
--- a/spec/outputs/redis.rb
+++ b/spec/outputs/redis.rb
@@ -1,5 +1,6 @@
 require "test_utils"
 require "logstash/outputs/redis"
+require "logstash/json"
 require "redis"
 
 describe LogStash::Outputs::Redis, :redis => true do
@@ -36,7 +37,7 @@
       # Now check all events for order and correctness.
       event_count.times do |value|
         id, element = redis.blpop(key, 0)
-        event = LogStash::Event.new(JSON.parse(element))
+        event = LogStash::Event.new(LogStash::Json.load(element))
         insist { event["sequence"] } == value
         insist { event["message"] } == "hello world"
       end
@@ -84,7 +85,7 @@
       # Now check all events for order and correctness.
       event_count.times do |value|
         id, element = redis.blpop(key, 0)
-        event = LogStash::Event.new(JSON.parse(element))
+        event = LogStash::Event.new(LogStash::Json.load(element))
         insist { event["sequence"] } == value
         insist { event["message"] } == "hello world"
       end
diff --git a/spec/support/date-http.rb b/spec/support/date-http.rb
index f9fd883f1e8..a6ac07966fc 100644
--- a/spec/support/date-http.rb
+++ b/spec/support/date-http.rb
@@ -13,6 +13,6 @@
   CONFIG
 
   sample("timestamp" => "25/Mar/2013:20:33:56 +0000") do
-    insist { subject["@timestamp"] } == Time.iso8601("2013-03-25T20:33:56.000Z")
+    insist { subject["@timestamp"].time } == Time.iso8601("2013-03-25T20:33:56.000Z")
   end
 end
diff --git a/spec/test_utils.rb b/spec/test_utils.rb
index bb9f712c89e..f890552ef34 100644
--- a/spec/test_utils.rb
+++ b/spec/test_utils.rb
@@ -1,5 +1,8 @@
 # encoding: utf-8
 
+require "logstash/json"
+require "logstash/timestamp"
+
 if ENV['COVERAGE']
   require 'simplecov'
   require 'coveralls'
@@ -42,8 +45,8 @@
 # ugly, I know, but this avoids adding conditionals in performance critical section
 class LogStash::Event
   def []=(str, value)
-    if str == TIMESTAMP && !value.is_a?(Time)
-      raise TypeError, "The field '@timestamp' must be a Time, not a #{value.class} (#{value})"
+    if str == TIMESTAMP && !value.is_a?(LogStash::Timestamp)
+      raise TypeError, "The field '@timestamp' must be a LogStash::Timestamp, not a #{value.class} (#{value})"
     end
     @accessors.strict_set(str, value)
   end # def []=
@@ -69,7 +72,7 @@ def tags(*tags)
     end
 
     def sample(sample_event, &block)
-      name = sample_event.is_a?(String) ? sample_event : sample_event.to_json
+      name = sample_event.is_a?(String) ? sample_event : LogStash::Json.dump(sample_event)
       name = name[0..50] + "..." if name.length > 50
 
       describe "\"#{name}\"" do
diff --git a/spec/timestamp.rb b/spec/timestamp.rb
new file mode 100644
index 00000000000..f6e6a0ceeed
--- /dev/null
+++ b/spec/timestamp.rb
@@ -0,0 +1,35 @@
+require "logstash/timestamp"
+
+describe LogStash::Timestamp do
+
+  it "should parse its own iso8601 output" do
+    t = Time.now
+    ts = LogStash::Timestamp.new(t)
+    expect(LogStash::Timestamp.parse_iso8601(ts.to_iso8601).to_i).to eq(t.to_i)
+  end
+
+  it "should coerce iso8601 string" do
+    t = Time.now
+    ts = LogStash::Timestamp.new(t)
+    expect(LogStash::Timestamp.coerce(ts.to_iso8601).to_i).to eq(t.to_i)
+  end
+
+  it "should coerce Time" do
+    t = Time.now
+    expect(LogStash::Timestamp.coerce(t).to_i).to eq(t.to_i)
+  end
+
+  it "should coerce Timestamp" do
+    t = LogStash::Timestamp.now
+    expect(LogStash::Timestamp.coerce(t).to_i).to eq(t.to_i)
+  end
+
+  it "should raise on invalid string coerce" do
+    expect{LogStash::Timestamp.coerce("foobar")}.to raise_error LogStash::TimestampParserError
+  end
+
+  it "should return nil on invalid object coerce" do
+    expect(LogStash::Timestamp.coerce(:foobar)).to be_nil
+  end
+
+end
diff --git a/tools/Gemfile.jruby-1.9.lock b/tools/Gemfile.jruby-1.9.lock
index dc11fd5429f..f05fb0e2800 100644
--- a/tools/Gemfile.jruby-1.9.lock
+++ b/tools/Gemfile.jruby-1.9.lock
@@ -1,26 +1,28 @@
 GEM
   remote: https://rubygems.org/
   specs:
-    activesupport (3.2.17)
-      i18n (~> 0.6, >= 0.6.4)
-      multi_json (~> 1.0)
-    addressable (2.3.5)
-    atomic (1.1.15-java)
+    activesupport (4.1.1)
+      i18n (~> 0.6, >= 0.6.9)
+      json (~> 1.7, >= 1.7.7)
+      minitest (~> 5.1)
+      thread_safe (~> 0.1)
+      tzinfo (~> 1.1)
+    addressable (2.3.6)
+    atomic (1.1.16-java)
     avl_tree (1.1.3)
     awesome_print (1.2.0)
-    aws-sdk (1.35.0)
+    aws-sdk (1.41.0)
       json (~> 1.4)
       nokogiri (>= 1.4.4)
-      uuidtools (~> 2.1)
     backports (3.6.0)
     beefcake (0.3.7)
-    bindata (2.0.0)
+    bindata (2.1.0)
     blankslate (2.1.2.4)
     bouncy-castle-java (1.5.0147)
     buftok (0.1)
     builder (3.2.2)
     cabin (0.6.1)
-    ci_reporter (1.9.1)
+    ci_reporter (1.9.2)
       builder (>= 2.1.2)
     cinch (2.1.0)
     clamp (0.6.3)
@@ -33,20 +35,19 @@ GEM
       thor
     diff-lcs (1.2.5)
     docile (1.1.3)
-    edn (1.0.2)
+    edn (1.0.3)
       parslet (~> 1.4.0)
-    elasticsearch (1.0.1)
-      elasticsearch-api (= 1.0.1)
-      elasticsearch-transport (= 1.0.1)
-    elasticsearch-api (1.0.1)
+    elasticsearch (1.0.2)
+      elasticsearch-api (= 1.0.2)
+      elasticsearch-transport (= 1.0.2)
+    elasticsearch-api (1.0.2)
       multi_json
-    elasticsearch-transport (1.0.1)
+    elasticsearch-transport (1.0.2)
       faraday
       multi_json
     extlib (0.9.16)
     faraday (0.9.0)
       multipart-post (>= 1.2, < 3)
-    ffi (1.9.3)
     ffi (1.9.3-java)
     ffi-rzmq (1.0.0)
       ffi
@@ -59,23 +60,21 @@ GEM
     gelf (1.3.2)
       json
     gelfd (0.2.0)
-    geoip (1.3.5)
+    geoip (1.4.0)
     gmetric (0.1.3)
-    hitimes (1.2.1)
     hitimes (1.2.1-java)
     http (0.5.0)
       http_parser.rb
-    http_parser.rb (0.5.3)
     http_parser.rb (0.5.3-java)
     i18n (0.6.9)
     insist (1.0.0)
     jls-grok (0.10.12)
       cabin (>= 0.6.0)
     jls-lumberjack (0.0.20)
+    jrjackson (0.2.7)
     jruby-httpclient (1.1.1-java)
     jruby-openssl (0.8.7)
       bouncy-castle-java (>= 1.5.0147)
-    json (1.8.1)
     json (1.8.1-java)
     kramdown (1.3.3)
     mail (2.5.3)
@@ -90,16 +89,14 @@ GEM
       avl_tree (~> 1.1.2)
       hitimes (~> 1.1)
     mime-types (1.25.1)
-    mini_portile (0.5.2)
-    minitest (5.3.0)
-    mocha (1.0.0)
+    minitest (5.3.4)
+    mocha (1.1.0)
       metaclass (~> 0.0.1)
     msgpack-jruby (1.4.0-java)
-    multi_json (1.8.4)
+    multi_json (1.10.1)
     multipart-post (2.0.0)
     murmurhash3 (0.1.4)
-    nokogiri (1.6.1-java)
-      mini_portile (~> 0.5.0)
+    nokogiri (1.6.2.1-java)
     parslet (1.4.0)
       blankslate (~> 2.0)
     polyglot (0.3.4)
@@ -109,9 +106,9 @@ GEM
       slop (~> 3.4)
       spoon (~> 0.0)
     rack (1.5.2)
-    rack-protection (1.5.2)
+    rack-protection (1.5.3)
       rack
-    rbnacl (2.0.0)
+    rbnacl (3.1.0)
       ffi
     redis (3.0.7)
     rest-client (1.6.7)
@@ -120,7 +117,7 @@ GEM
       rspec-core (~> 2.14.0)
       rspec-expectations (~> 2.14.0)
       rspec-mocks (~> 2.14.0)
-    rspec-core (2.14.7)
+    rspec-core (2.14.8)
     rspec-expectations (2.14.5)
       diff-lcs (>= 1.1.3, < 2.0)
     rspec-mocks (2.14.6)
@@ -131,8 +128,8 @@ GEM
     shoulda (3.5.0)
       shoulda-context (~> 1.0, >= 1.0.1)
       shoulda-matchers (>= 1.4.1, < 3.0)
-    shoulda-context (1.1.6)
-    shoulda-matchers (2.5.0)
+    shoulda-context (1.2.1)
+    shoulda-matchers (2.6.1)
       activesupport (>= 3.0.0)
     simple_oauth (0.2.0)
     simplecov (0.8.2)
@@ -140,11 +137,11 @@ GEM
       multi_json
       simplecov-html (~> 0.8.0)
     simplecov-html (0.8.0)
-    sinatra (1.4.4)
+    sinatra (1.4.5)
       rack (~> 1.4)
       rack-protection (~> 1.4)
       tilt (~> 1.3, >= 1.3.4)
-    slop (3.4.7)
+    slop (3.5.0)
     snmp (1.1.1)
     spoon (0.0.4)
       ffi
@@ -154,11 +151,10 @@ GEM
       metriks
     term-ansicolor (1.3.0)
       tins (~> 1.0)
-    thor (0.18.1)
-    thread_safe (0.2.0-java)
-      atomic (>= 1.1.7, < 2)
+    thor (0.19.1)
+    thread_safe (0.3.3-java)
     tilt (1.4.1)
-    tins (1.0.0)
+    tins (1.3.0)
     treetop (1.4.15)
       polyglot
       polyglot (>= 0.3.1)
@@ -169,10 +165,9 @@ GEM
       http_parser.rb (~> 0.5.0)
       json (~> 1.8)
       simple_oauth (~> 0.2.0)
-    tzinfo (1.1.0)
+    tzinfo (1.2.0)
       thread_safe (~> 0.1)
-    user_agent_parser (2.1.2)
-    uuidtools (2.1.4)
+    user_agent_parser (2.1.5)
     xml-simple (1.1.3)
     xmpp4r (0.5)
 
@@ -206,9 +201,9 @@ DEPENDENCIES
   insist (= 1.0.0)
   jls-grok (= 0.10.12)
   jls-lumberjack (>= 0.0.20)
+  jrjackson
   jruby-httpclient
   jruby-openssl (= 0.8.7)
-  json
   kramdown
   mail
   march_hare (~> 2.1.0)
diff --git a/tools/release.sh b/tools/release.sh
index d667991d419..435196a95cb 100644
--- a/tools/release.sh
+++ b/tools/release.sh
@@ -38,8 +38,8 @@ docs() {
 }
 
 tests() {
-  USE_JRUBY=1 make -C $logstash test QUIET=
-  USE_JRUBY=1 make -C $logstash tarball test QUIET=
+  make -C $logstash test QUIET=
+  make -C $logstash tarball test QUIET=
 }
 
 packages() {
