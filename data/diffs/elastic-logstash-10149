diff --git a/Gemfile.template b/Gemfile.template
index b5b0fc65e58..dde07467aaf 100644
--- a/Gemfile.template
+++ b/Gemfile.template
@@ -101,7 +101,7 @@ gem "logstash-input-tcp"
 gem "logstash-input-twitter"
 gem "logstash-input-udp"
 gem "logstash-input-unix"
-gem "logstash-output-appsearch"
+gem "logstash-output-elastic_app_search"
 gem "logstash-output-cloudwatch"
 gem "logstash-output-csv"
 gem "logstash-output-elasticsearch"
diff --git a/README.md b/README.md
index 06293d4fd76..5aa6cace63f 100644
--- a/README.md
+++ b/README.md
@@ -28,14 +28,14 @@ These builds are created nightly and have undergone no formal QA, so they should
 | [deb-complete][]      | [deb-oss][]            |
 | [rpm-complete][]      | [rpm-oss][]            |
 
-[tar-complete]: https://snapshots.elastic.co/downloads/logstash/logstash-6.5.0-SNAPSHOT.tar.gz
-[zip-complete]: https://snapshots.elastic.co/downloads/logstash/logstash-6.5.0-SNAPSHOT.zip
-[deb-complete]: https://snapshots.elastic.co/downloads/logstash/logstash-6.5.0-SNAPSHOT.deb
-[rpm-complete]: https://snapshots.elastic.co/downloads/logstash/logstash-6.5.0-SNAPSHOT.rpm
-[tar-oss]: https://snapshots.elastic.co/downloads/logstash/logstash-oss-6.5.0-SNAPSHOT.tar.gz
-[zip-oss]: https://snapshots.elastic.co/downloads/logstash/logstash-oss-6.5.0-SNAPSHOT.zip
-[deb-oss]: https://snapshots.elastic.co/downloads/logstash/logstash-oss-6.5.0-SNAPSHOT.deb
-[rpm-oss]: https://snapshots.elastic.co/downloads/logstash/logstash-oss-6.5.0-SNAPSHOT.rpm
+[tar-complete]: https://snapshots.elastic.co/downloads/logstash/logstash-6.6.0-SNAPSHOT.tar.gz
+[zip-complete]: https://snapshots.elastic.co/downloads/logstash/logstash-6.6.0-SNAPSHOT.zip
+[deb-complete]: https://snapshots.elastic.co/downloads/logstash/logstash-6.6.0-SNAPSHOT.deb
+[rpm-complete]: https://snapshots.elastic.co/downloads/logstash/logstash-6.6.0-SNAPSHOT.rpm
+[tar-oss]: https://snapshots.elastic.co/downloads/logstash/logstash-oss-6.6.0-SNAPSHOT.tar.gz
+[zip-oss]: https://snapshots.elastic.co/downloads/logstash/logstash-oss-6.6.0-SNAPSHOT.zip
+[deb-oss]: https://snapshots.elastic.co/downloads/logstash/logstash-oss-6.6.0-SNAPSHOT.deb
+[rpm-oss]: https://snapshots.elastic.co/downloads/logstash/logstash-oss-6.6.0-SNAPSHOT.rpm
 
 ## Need Help?
 
diff --git a/config/log4j2.properties b/config/log4j2.properties
index 064064d4b49..77026ac6ec1 100644
--- a/config/log4j2.properties
+++ b/config/log4j2.properties
@@ -89,3 +89,6 @@ logger.slowlog.level = trace
 logger.slowlog.appenderRef.console_slowlog.ref = ${sys:ls.log.format}_console_slowlog
 logger.slowlog.appenderRef.rolling_slowlog.ref = ${sys:ls.log.format}_rolling_slowlog
 logger.slowlog.additivity = false
+
+logger.licensereader.name = logstash.licensechecker.licensereader
+logger.licensereader.level = error
diff --git a/config/logstash.yml b/config/logstash.yml
index 57b19b47376..0a853397c86 100644
--- a/config/logstash.yml
+++ b/config/logstash.yml
@@ -93,7 +93,7 @@
 # ------------ Module Settings ---------------
 # Define modules here.  Modules definitions must be defined as an array.
 # The simple way to see this is to prepend each `name` with a `-`, and keep
-# all associated variables under the `name` they are associated with, and 
+# all associated variables under the `name` they are associated with, and
 # above the next, like this:
 #
 # modules:
@@ -103,7 +103,7 @@
 #     var.PLUGINTYPE2.PLUGINNAME1.KEY1: VALUE
 #     var.PLUGINTYPE3.PLUGINNAME3.KEY1: VALUE
 #
-# Module variable names must be in the format of 
+# Module variable names must be in the format of
 #
 # var.PLUGIN_TYPE.PLUGIN_NAME.KEY
 #
diff --git a/docs/include/input.asciidoc b/docs/include/input.asciidoc
index 5290ba08635..9c6a7b802d5 100644
--- a/docs/include/input.asciidoc
+++ b/docs/include/input.asciidoc
@@ -11,7 +11,9 @@ ifeval::["{versioned_docs}"!="true"]
 |=======================================================================
 |Setting |Input type|Required
 | <<plugins-{type}s-{plugin}-add_field>> |{logstash-ref}/configuration-file-structure.html#hash[hash]|No
+ifndef::no_codec[]
 | <<plugins-{type}s-{plugin}-codec>> |{logstash-ref}/configuration-file-structure.html#codec[codec]|No
+endif::no_codec[]
 | <<plugins-{type}s-{plugin}-enable_metric>> |{logstash-ref}/configuration-file-structure.html#boolean[boolean]|No
 | <<plugins-{type}s-{plugin}-id>> |{logstash-ref}/configuration-file-structure.html#string[string]|No
 | <<plugins-{type}s-{plugin}-tags>> |{logstash-ref}/configuration-file-structure.html#array[array]|No
@@ -22,7 +24,9 @@ ifeval::["{versioned_docs}"=="true"]
 |=======================================================================
 |Setting |Input type|Required
 | <<{version}-plugins-{type}s-{plugin}-add_field>> |{logstash-ref}/configuration-file-structure.html#hash[hash]|No
+ifndef::no_codec[]
 | <<{version}-plugins-{type}s-{plugin}-codec>> |{logstash-ref}/configuration-file-structure.html#codec[codec]|No
+endif::no_codec[]
 | <<{version}-plugins-{type}s-{plugin}-enable_metric>> |{logstash-ref}/configuration-file-structure.html#boolean[boolean]|No
 | <<{version}-plugins-{type}s-{plugin}-id>> |{logstash-ref}/configuration-file-structure.html#string[string]|No
 | <<{version}-plugins-{type}s-{plugin}-tags>> |{logstash-ref}/configuration-file-structure.html#array[array]|No
@@ -47,6 +51,7 @@ endif::[]
 
 Add a field to an event
 
+ifndef::no_codec[]
 ifeval::["{versioned_docs}"!="true"]
 [id="plugins-{type}s-{plugin}-codec"]
 endif::[]
@@ -64,6 +69,8 @@ ifndef::default_codec[]
 endif::[]
 
 The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.
+endif::no_codec[]
+
 
 ifeval::["{versioned_docs}"!="true"]
 [id="plugins-{type}s-{plugin}-enable_metric"]
diff --git a/docs/include/output.asciidoc b/docs/include/output.asciidoc
index 0e2ec05c616..e546ce8ab77 100644
--- a/docs/include/output.asciidoc
+++ b/docs/include/output.asciidoc
@@ -10,7 +10,9 @@ ifeval::["{versioned_docs}"!="true"]
 [cols="<,<,<",options="header",]
 |=======================================================================
 |Setting |Input type|Required
+ifndef::no_codec[]
 | <<plugins-{type}s-{plugin}-codec>> |{logstash-ref}/configuration-file-structure.html#codec[codec]|No
+endif::no_codec[]
 | <<plugins-{type}s-{plugin}-enable_metric>> |{logstash-ref}/configuration-file-structure.html#boolean[boolean]|No
 | <<plugins-{type}s-{plugin}-id>> |{logstash-ref}/configuration-file-structure.html#string[string]|No
 |=======================================================================
@@ -19,12 +21,15 @@ ifeval::["{versioned_docs}"=="true"]
 [cols="<,<,<",options="header",]
 |=======================================================================
 |Setting |Input type|Required
+ifndef::no_codec[]
 | <<{version}-plugins-{type}s-{plugin}-codec>> |{logstash-ref}/configuration-file-structure.html#codec[codec]|No
+endif::no_codec[]
 | <<{version}-plugins-{type}s-{plugin}-enable_metric>> |{logstash-ref}/configuration-file-structure.html#boolean[boolean]|No
 | <<{version}-plugins-{type}s-{plugin}-id>> |{logstash-ref}/configuration-file-structure.html#string[string]|No
 |=======================================================================
 endif::[]
 
+ifndef::no_codec[]
 ifeval::["{versioned_docs}"!="true"]
 [id="plugins-{type}s-{plugin}-codec"]
 endif::[]
@@ -42,6 +47,7 @@ ifndef::default_codec[]
 endif::[]
 
 The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output without needing a separate filter in your Logstash pipeline.
+endif::no_codec[]
 
 ifeval::["{versioned_docs}"!="true"]
 [id="plugins-{type}s-{plugin}-enable_metric"]
diff --git a/docs/index.asciidoc b/docs/index.asciidoc
index 1e556dc58c9..594054ce875 100644
--- a/docs/index.asciidoc
+++ b/docs/index.asciidoc
@@ -8,9 +8,9 @@
 :plugins-repo-dir:      {docdir}/../../logstash-docs/docs
 :branch:                6.x
 :major-version:         6.x
-:logstash_version:      6.5.0
-:elasticsearch_version: 6.5.0
-:kibana_version:        6.5.0
+:logstash_version:      6.6.0
+:elasticsearch_version: 6.6.0
+:kibana_version:        6.6.0
 :docker-repo:           docker.elastic.co/logstash/logstash
 :docker-image:          {docker-repo}:{logstash_version}
 
diff --git a/docs/static/advanced-pipeline.asciidoc b/docs/static/advanced-pipeline.asciidoc
index da442a497d8..5403560be87 100644
--- a/docs/static/advanced-pipeline.asciidoc
+++ b/docs/static/advanced-pipeline.asciidoc
@@ -40,7 +40,7 @@ directory, and replace the contents with the following lines. Make sure `paths`
 
 [source,yaml]
 --------------------------------------------------------------------------------
-filebeat.prospectors:
+filebeat.input:
 - type: log
   paths:
     - /path/to/file/logstash-tutorial.log <1>
diff --git a/docs/static/arcsight-module.asciidoc b/docs/static/arcsight-module.asciidoc
index 5114ca55ae0..eb2c148c475 100644
--- a/docs/static/arcsight-module.asciidoc
+++ b/docs/static/arcsight-module.asciidoc
@@ -12,10 +12,10 @@ and is therefore free to use. Please contact
 mailto:arcsight@elastic.co[arcsight@elastic.co] for questions or more
 information.
 
-The Logstash ArcSight module enables you to easily integrate your ArcSight data with the Elastic Stack. 
-With a single command, the module taps directly into the ArcSight Smart Connector or the Event Broker, 
-parses and indexes the security events into Elasticsearch, and installs a suite of Kibana dashboards 
-to get you exploring your data immediately. 
+The Logstash ArcSight module enables you to easily integrate your ArcSight data with the Elastic Stack.
+With a single command, the module taps directly into the ArcSight Smart Connector or the Event Broker,
+parses and indexes the security events into Elasticsearch, and installs a suite of Kibana dashboards
+to get you exploring your data immediately.
 
 [[arcsight-prereqs]]
 ==== Prerequisites
@@ -51,10 +51,10 @@ the Smart Connector directly.
 
 image::static/images/arcsight-diagram-smart-connectors.svg[ArcSight Smart Connector architecture]
 
-Smart Connector has been configured to publish ArcSight data (to TCP port `5000`) using the CEF syslog 
+Smart Connector has been configured to publish ArcSight data (to TCP port `5000`) using the CEF syslog
 destination.
 
-NOTE: Logstash, Elasticsearch, and Kibana must run locally. Note that you can also run 
+NOTE: Logstash, Elasticsearch, and Kibana must run locally. Note that you can also run
 Elasticsearch, Kibana and Logstash on separate hosts to consume data from ArcSight.
 
 [[arcsight-instructions-smartconnector]]
@@ -66,9 +66,9 @@ Logstash install directory with your respective Smart Connector host and port:
 ["source","shell",subs="attributes"]
 -----
 bin/logstash --modules arcsight --setup \
-  -M "arcsight.var.input.smartconnector.bootstrap_servers={smart_connect_host}:{smart_connect_port}"  \
+  -M "arcsight.var.input.smartconnector.port={smart_connect_port}"  \
   -M "arcsight.var.elasticsearch.hosts=localhost:9200"  \
-  -M "arcsight.var.kibana.host=localhost:5601" 
+  -M "arcsight.var.kibana.host=localhost:5601"
 -----
 +
 --
@@ -105,9 +105,9 @@ image::static/images/arcsight-diagram-adp.svg[ArcSight Event Broker architecture
 
 By default, the Logstash ArcSight module consumes from the Event Broker "eb-cef" topic.
 For additional settings, see <<arcsight-module-config>>. Consuming from a
-secured Event Broker port is not currently available.
+secured Event Broker port is possible, see <<arcsight-module-config>>.
 
-NOTE: Logstash, Elasticsearch, and Kibana must run locally. Note that you can also run 
+NOTE: Logstash, Elasticsearch, and Kibana must run locally. Note that you can also run
 Elasticsearch, Kibana and Logstash on separate hosts to consume data from ArcSight.
 
 [[arcsight-instructions-eventbroker]]
@@ -204,7 +204,7 @@ like in the getting started. For more information about configuring modules, see
 As an example, the following settings can be appended to `logstash.yml` to
 configure your module:
 
-["source","yaml",subs="attributes"] 
+["source","yaml",subs="attributes"]
 -----
 modules:
   - name: arcsight
@@ -223,7 +223,7 @@ modules:
 
 The ArcSight module provides the following settings for configuring the behavior
 of the module. These settings include ArcSight-specific options plus common
-options that are supported by all Logstash modules. 
+options that are supported by all Logstash modules.
 
 When you override a setting at the command line, remember to prefix the setting
 with the module name, for example, `arcsight.var.inputs` instead of `var.inputs`.
@@ -243,6 +243,8 @@ Set the input(s) to expose for the Logstash ArcSight module. Valid settings are
 "eventbroker", "smartconnector", or "eventbroker,smartconnector" (exposes both
 inputs concurrently).
 
+*ArcSight Module Event Broker specific Options*
+
 *`var.input.eventbroker.bootstrap_servers`*::
 +
 --
@@ -265,6 +267,138 @@ servers. (You may want more than one in case a server is down.)
 +
 A list of Event Broker topics to subscribe to.
 
+*`var.input.eventbroker.security_protocol`*::
++
+--
+* Value can be any of: `PLAINTEXT`, `SSL`, `SASL_PLAINTEXT`, `SASL_SSL`
+* Default value is `"PLAINTEXT"`
+--
++
+Security protocol to use, which can be either of PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL. If you specify anything other than PLAINTEXT then you need to also specify some of the options listed below. When specifying `SSL` or `SASL_SSL` you should supply values for the options prefixed with `ssl_`, when specifying `SASL_PLAINTEXT` or `SASL_SSL` you should supply values for `jaas_path`, `kerberos_config`, `sasl_mechanism` and `sasl_kerberos_service_name`.
+
+*`var.input.eventbroker.ssl_key_password`*::
++
+--
+* Value type is <<password,password>>
+* There is no default value for this setting.
+--
++
+The password of the private key in the key store file.
+
+*`var.input.eventbroker.ssl_keystore_location`*::
++
+--
+* Value type is <<path,path>>
+* There is no default value for this setting.
+--
++
+If client authentication is required, this setting stores the keystore path.
+
+*`var.input.eventbroker.ssl_keystore_password`*::
++
+--
+* Value type is <<password,password>>
+* There is no default value for this setting.
+--
++
+If client authentication is required, this setting stores the keystore password.
+
+*`var.input.eventbroker.ssl_keystore_type`*::
++
+--
+* Value type is <<string,string>>
+* There is no default value for this setting.
+--
++
+The keystore type.
+
+*`var.input.eventbroker.ssl_truststore_location`*::
++
+--
+* Value type is <<path,path>>
+* There is no default value for this setting.
+--
++
+The JKS truststore path to validate the Kafka broker's certificate.
+
+*`var.input.eventbroker.ssl_truststore_password`*::
++
+--
+* Value type is <<password,password>>
+* There is no default value for this setting.
+--
++
+The truststore password.
+
+*`var.input.eventbroker.ssl_truststore_type`*::
++
+--
+* Value type is <<string,string>>
+* There is no default value for this setting.
+--
++
+The truststore type.
+
+*`var.input.eventbroker.sasl_kerberos_service_name`*::
++
+--
+* Value type is <<string,string>>
+* There is no default value for this setting.
+--
++
+The Kerberos principal name that Kafka broker runs as.
+This can be defined either in Kafka's JAAS config or in Kafka's config.
+
+*`var.input.eventbroker.sasl_mechanism`*::
++
+--
+* Value type is <<string,string>>
+* Default value is `"GSSAPI"`
+--
++
+http://kafka.apache.org/documentation.html#security_sasl[SASL mechanism] used for client connections.
+This may be any mechanism for which a security provider is available.
+GSSAPI is the default mechanism.
+
+*`var.input.eventbroker.jaas_path`*::
++
+--
+* Value type is <<path,path>>
+* There is no default value for this setting.
+--
++
+The Java Authentication and Authorization Service (JAAS) API supplies user authentication and authorization
+services for Kafka. This setting provides the path to the JAAS file. Sample JAAS file for Kafka client:
++
+[source,java]
+----------------------------------
+KafkaClient {
+  com.sun.security.auth.module.Krb5LoginModule required
+  useTicketCache=true
+  renewTicket=true
+  serviceName="kafka";
+  };
+----------------------------------
++
+
+Please note that specifying `jaas_path` and `kerberos_config` here will add these
+to the global JVM system properties. This means if you have multiple Kafka inputs,
+all of them would be sharing the same `jaas_path` and `kerberos_config`.
+If this is not desirable, you would have to run separate instances of Logstash on
+different JVM instances.
+
+
+*`var.input.eventbroker.kerberos_config`*::
++
+--
+* Value type is <<path,path>>
+* There is no default value for this setting.
+--
++
+Optional path to kerberos config file. This is krb5.conf style as detailed in https://web.mit.edu/kerberos/krb5-1.12/doc/admin/conf_files/krb5_conf.html
+
+*ArcSight Module Smart Connector specific Options*
+
 *`var.input.smartconnector.port`*::
 +
 --
@@ -274,6 +408,62 @@ A list of Event Broker topics to subscribe to.
 +
 The TCP port to listen on when receiving data from SCs.
 
+*`var.input.smartconnector.ssl_enable`*::
++
+--
+* Value type is <<boolean,boolean>>
+* Default value is `false`
+--
++
+Enable SSL (must be set for other `ssl_` options to take effect).
+
+*`var.input.smartconnector.ssl_cert`*::
++
+--
+* Value type is <<path,path>>
+* There is no default value for this setting.
+--
++
+SSL certificate path.
+
+*`var.input.smartconnector.ssl_extra_chain_certs`*::
++
+--
+* Value type is <<array,array>>
+* Default value is `[]`
+--
++
+An Array of paths to extra X509 certificates to be added to the certificate chain.
+Useful when the CA chain is not necessary in the system store.
+
+*`var.input.smartconnector.ssl_key`*::
++
+--
+* Value type is <<path,path>>
+* There is no default value for this setting.
+--
++
+SSL key path
+
+*`var.input.smartconnector.ssl_key_passphrase`*::
++
+--
+* Value type is <<password,password>>
+* Default value is `nil`
+--
++
+SSL key passphrase
+
+*`var.input.smartconnector.ssl_verify`*::
++
+--
+* Value type is <<boolean,boolean>>
+* Default value is `true`
+--
++
+Verify the identity of the other end of the SSL connection against the CA.
+For input, sets the field `sslsubject` to that of the client certificate.
+
 :smart_connect_host!:
 :smart_connect_port!:
 :event_broker_host!:
diff --git a/docs/static/logging.asciidoc b/docs/static/logging.asciidoc
index 9d568d53ff8..8e78ff284d8 100644
--- a/docs/static/logging.asciidoc
+++ b/docs/static/logging.asciidoc
@@ -5,87 +5,54 @@ Logstash emits internal logs during its operation, which are placed in `LS_HOME/
 DEB/RPM). The default logging level is `INFO`. Logstash's logging framework is based on
 http://logging.apache.org/log4j/2.x/[Log4j 2 framework], and much of its functionality is exposed directly to users.
 
-When debugging problems, particularly problems with plugins, it can be helpful to increase the logging level to `DEBUG`
-to get more verbose messages. Previously, you could only set a log level that applied to the entire Logstash product.
-Starting with 5.0, you can configure logging for a particular subsystem in Logstash. For example, if you are
-debugging issues with Elasticsearch Output, you can increase log levels just for that component. This way
-you can reduce noise due to excessive logging and focus on the problem area effectively.
+You can configure logging for a particular subsystem, module, or plugin.
 
-==== Log file location
-
-You can specify the log file location using `--path.logs` setting.
+When you need to debug problems, particularly problems with plugins, consider
+increasing the logging level to `DEBUG` to get more verbose messages. For
+example, if you are debugging issues with Elasticsearch Output, you can increase
+log levels just for that component. This approach reduces noise from
+excessive logging and helps you focus on the problem area.
 
-[[log4j2]]
-==== Log4j 2 Configuration
-
-Logstash ships with a `log4j2.properties` file with out-of-the-box settings. You can modify this file to change the
-rotation policy, type, and other https://logging.apache.org/log4j/2.x/manual/configuration.html#Loggers[log4j2 configuration].
-You must restart Logstash to apply any changes that you make to this file.
+You can configure logging using the `log4j2.properties` file or the Logstash API.
 
-==== Slowlog
+* *`log4j2.properties` file.*  Changes made through the `log4j2.properties`
+file require you to restart Logstash for the changes to take effect.  Changes *persist*
+through subsequent restarts. 
+* *Logging API.* Changes made through the Logging API are effective immediately 
+without a restart. The changes *do not persist* after Logstash
+is restarted.
 
-Slow-log for Logstash adds the ability to log when a specific event takes an abnormal amount of time to make its way
-through the pipeline. Just like the normal application log, you can find slow-logs in your `--path.logs` directory.
-Slowlog is configured in the `logstash.yml` settings file with the following options:
+[[log4j2]]
+==== Log4j2 configuration
 
-[source,yaml]
-------------------------------
-slowlog.threshold.warn (default: -1)
-slowlog.threshold.info (default: -1)
-slowlog.threshold.debug (default: -1)
-slowlog.threshold.trace (default: -1)
-------------------------------
+Logstash ships with a `log4j2.properties` file with out-of-the-box settings. You
+can modify this file to change the rotation policy, type, and other
+https://logging.apache.org/log4j/2.x/manual/configuration.html#Loggers[log4j2
+configuration]. 
 
-By default, these values are set to `-1nanos` to represent an infinite threshold where no slowlog will be invoked. These `slowlog.threshold`
-fields are configured using a time-value format which enables a wide range of trigger intervals. The positive numeric ranges
-can be specified using the following time units: `nanos` (nanoseconds), `micros` (microseconds), `ms` (milliseconds), `s` (second), `m` (minute),
-`h` (hour), `d` (day).
+You must restart Logstash to apply any changes that you make to
+this file.
+Changes to `log4j2.properties` persist after Logstash is restarted.
 
-Here is an example:
+Here's an example using `outputs.elasticsearch`:
 
 [source,yaml]
-------------------------------
-slowlog.threshold.warn: 2s
-slowlog.threshold.info: 1s
-slowlog.threshold.debug: 500ms
-slowlog.threshold.trace: 100ms
-------------------------------
-
-In the above configuration, events that take longer than two seconds to be processed within a filter will be logged.
-The logs will include the full event and filter configuration that are responsible for the slowness.
+--------------------------------------------------
+logger.elasticsearchoutput.name = logstash.outputs.elasticsearch
+logger.elasticsearchoutput.level = debug
+--------------------------------------------------
 
 ==== Logging APIs
 
-You could modify the `log4j2.properties` file and restart your Logstash, but that is both tedious and leads to unnecessary
+For temporary logging changes, modifying the `log4j2.properties` file and restarting Logstash leads to unnecessary
 downtime. Instead, you can dynamically update logging levels through the logging API. These settings are effective
-immediately and do not need a restart.
+immediately and do not need a restart. 
 
 NOTE: By default, the logging API attempts to bind to `tcp:9600`. If this port is already in use by another Logstash
 instance, you need to launch Logstash with the `--http.port` flag specified to bind to a different port. See
 <<command-line-flags>> for more information.
 
-To update logging levels, take the subsystem/module you are interested in and prepend
-`logger.` to it. For example:
-
-[source,js]
---------------------------------------------------
-curl -XPUT 'localhost:9600/_node/logging?pretty' -H 'Content-Type: application/json' -d'
-{
-    "logger.logstash.outputs.elasticsearch" : "DEBUG"
-}
-'
---------------------------------------------------
-
-While this setting is in effect, Logstash emits DEBUG-level logs for __all__ the Elasticsearch outputs
-specified in your configuration. Please note this new setting is transient and will not survive a restart.
-
-Persistent changes should be added to `log4j2.properties`. For example:
-
-[source,yaml]
---------------------------------------------------
-logger.elasticsearchoutput.name = logstash.outputs.elasticsearch
-logger.elasticsearchoutput.level = debug
---------------------------------------------------
+===== Retrieve list of logging configurations
 
 To retrieve a list of logging subsystems available at runtime, you can do a `GET` request to `_node/logging`
 
@@ -126,6 +93,28 @@ Example response:
 }
 --------------------------------------------------
 
+===== Update logging levels
+
+Prepend the name of the subsystem, module, or plugin with `logger.`. 
+
+Here is an example using `outputs.elasticsearch`:
+
+[source,js]
+--------------------------------------------------
+curl -XPUT 'localhost:9600/_node/logging?pretty' -H 'Content-Type: application/json' -d'
+{
+    "logger.logstash.outputs.elasticsearch" : "DEBUG"
+}
+'
+--------------------------------------------------
+
+While this setting is in effect, Logstash emits DEBUG-level logs for __all__ the Elasticsearch outputs
+specified in your configuration. Please note this new setting is transient and will not survive a restart.
+
+NOTE: If you want logging changes to persist after a restart, add them to `log4j2.properties` instead. 
+
+===== Reset dynamic logging levels
+
 To reset any logging levels that may have been dynamically changed via the logging API, send a `PUT` request to
 `_node/logging/reset`. All logging levels will revert to the values specified in the `log4j2.properties` file.
 
@@ -133,3 +122,53 @@ To reset any logging levels that may have been dynamically changed via the loggi
 --------------------------------------------------
 curl -XPUT 'localhost:9600/_node/logging/reset?pretty'
 --------------------------------------------------
+
+==== Log file location
+
+You can specify the log file location using `--path.logs` setting.
+
+==== Slowlog
+
+Slowlog for Logstash adds the ability to log when a specific event takes an abnormal amount of time to make its way
+through the pipeline. Just like the normal application log, you can find slowlogs in your `--path.logs` directory.
+Slowlog is configured in the `logstash.yml` settings file with the following options:
+
+[source,yaml]
+------------------------------
+slowlog.threshold.warn (default: -1)
+slowlog.threshold.info (default: -1)
+slowlog.threshold.debug (default: -1)
+slowlog.threshold.trace (default: -1)
+------------------------------
+
+Slowlog is disabled by default. The default threshold values are set to
+`-1nanos` to represent an infinite threshold. No slowlog will be invoked. 
+
+===== Enable slowlog
+
+The `slowlog.threshold` fields use a time-value format which enables a wide
+range of trigger intervals. You can specify ranges using the following time
+units: `nanos` (nanoseconds), `micros` (microseconds), `ms` (milliseconds), `s`
+(second), `m` (minute), `h` (hour), `d` (day).
+
+Slowlog becomes more sensitive and logs more events as you raise the log level. 
+
+Example:
+
+[source,yaml]
+------------------------------
+slowlog.threshold.warn: 2s
+slowlog.threshold.info: 1s
+slowlog.threshold.debug: 500ms
+slowlog.threshold.trace: 100ms
+------------------------------
+
+In this example:
+
+* If the log level is set to `warn`, the log shows events that took longer than 2 seconds to process.
+* If the log level is set to `info`, the log shows events that took longer than 1s to process.
+* If the log level is set to `trace`, the log shows events that took longer than 100ms to process.
+* If the log level is set to `debug`, the log shows events that took longer than 500ms to process.
+
+The logs include the full event and filter configuration that are responsible
+for the slowness.
diff --git a/docs/static/releasenotes.asciidoc b/docs/static/releasenotes.asciidoc
index 1eda4494440..52f0268f3b2 100644
--- a/docs/static/releasenotes.asciidoc
+++ b/docs/static/releasenotes.asciidoc
@@ -3,6 +3,11 @@
 
 This section summarizes the changes in the following releases:
 
+* <<logstash-6-5-0,Logstash 6.5.0>>
+* <<logstash-6-4-3,Logstash 6.4.3>>
+* <<logstash-6-4-2,Logstash 6.4.2>>
+* <<logstash-6-4-1,Logstash 6.4.1>>
+* <<logstash-6-4-0,Logstash 6.4.0>>
 * <<logstash-6-3-2,Logstash 6.3.2>>
 * <<logstash-6-3-1,Logstash 6.3.1>>
 * <<logstash-6-3-0,Logstash 6.3.0>>
@@ -16,6 +21,301 @@ This section summarizes the changes in the following releases:
 * <<logstash-6-1-1,Logstash 6.1.1>>
 * <<logstash-6-1-0,Logstash 6.1.0>>
 
+[[logstash-6-5-0]]
+=== Logstash 6.5.0 Release Notes
+
+* BUGFIX: Count unused space in page files towards current PQ size https://github.com/elastic/logstash/pull/10105[#10105]
+* BUGFIX: Handle equality checks on list and map types in Java execution https://github.com/elastic/logstash/pull/10074[#10074]
+* BUGFIX: Handle equality comparison where one or more fields are null in Java execution https://github.com/elastic/logstash/pull/10039[#10039]
+* Make cgroups more robust and provide the override similar to ES https://github.com/elastic/logstash/pull/10011[#10011]
+* BUGFIX: Pipeline reloading breaks with PQ enabled https://github.com/elastic/logstash/pull/9987[#9987]
+* BUGFIX: Avoid race condition when initializing event and pipeline metrics https://github.com/elastic/logstash/pull/9959[#9959]
+* Support port customization in cloud id https://github.com/elastic/logstash/pull/9877[#9877]
+* Support for integration plugins in plugin manager https://github.com/elastic/logstash/pull/9811[#9811]
+* Promote Java execution from experimental to beta https://github.com/elastic/logstash/pull/10063[#10063]
+
+==== Plugins
+
+*Elastic App Search output*
+
+* New: Added as default plugin
+
+*SNMP input*
+
+* New: Added as default plugin
+
+*Elasticsearch filter*
+
+* Adds [@metadata][total_hits] with total hits returned from the query https://github.com/logstash-plugins/logstash-filter-elasticsearch/pull/106[#106]
+* Improves error logging to fully inspect caught exceptions https://github.com/logstash-plugins/logstash-filter-elasticsearch/pull/105[#105]
+
+*Translate filter*
+
+* Fix to align with docs - looked-up values are always strings. Coerce better.
+* Fix bug in dictionary/file the always applied RegexExact, manifested when dictionary keys are not regex compatible
+* Added info to dictionary_path description to explain why integers must be quoted
+* Fix bug in csv_file when LS config has CSV filter plugin specified as well as a csv dictionary.
+* Updated formatting of examples in documentation for consistent rendering
+* Add iterate_on setting to support fields that are arrays, see the docs for detailed explanation.
+* Add Rufus::Scheduler to provide asynchronous loading of dictionary.
+* Re-organise code, yields performance improvement of around 360%
+
+*Useragent filter*
+
+* Update source mapping to latest from uap-core https://github.com/logstash-plugins/logstash-filter-useragent/issues/53[#53]
+
+*Kafka input*
+
+* Upgrade Kafka client to version 2.0.0
+* Docs: Correct list formatting for decorate_events
+* Docs: Add kafka default to partition_assignment_strategy
+
+*Redis input*
+
+* Added support for renamed redis commands
+* Add channel to the event
+
+*S3 input*
+
+* Docs: Fixed link formatting for input type
+* Skips objects that are archived to AWS Glacier with a helpful log message (previously they would log as matched, but then fail to load events) https://github.com/logstash-plugins/logstash-input-s3/pull/160[#160]
+* Added watch_for_new_files option, enabling single-batch imports https://github.com/logstash-plugins/logstash-input-s3/pull/159[#159]
+
+*TCP input*
+
+* Added support for pkcs1 and pkcs8 key formats https://github.com/logstash-plugins/logstash-input-tcp/issues/122[#122]
+* Changed server-mode SSL to run on top of Netty https://github.com/logstash-plugins/logstash-input-tcp/issues/122[#122]
+* Changed travis testing infra to use logstash tarballs https://github.com/logstash-plugins/logstash-input-tcp/issues/122[#122]
+* Fixed certificate chain handling and validation https://github.com/logstash-plugins/logstash-input-tcp/issues/124[#124]
+* Added new configuration option dns_reverse_lookup_enabled to allow users to disable costly DNS reverse lookups https://github.com/logstash-plugins/logstash-input-tcp/issues/100[#100]
+
+*Netflow codec*
+
+* Added Cisco ACI to list of known working Netflow v9 exporters
+* Added support for IXIA Packet Broker IPFIX
+* Fixed issue with Procera float fields
+* Fixed issue where TTL in template registry was not being respected.
+* Reduced complexity of creating, persisting, loading an retrieving template caches.
+* Added support for Netflow v9 devices with VarString fields (H3C Netstream)
+* Fixed incorrect parsing of zero-filled Netflow 9 packets from Palo Alto
+* Fixed IPFIX options template parsing for Juniper MX240 JunOS 15.1
+
+[[logstash-6-4-3]]
+=== Logstash 6.4.3 Release Notes
+
+* No changes to Logstash core for 6.4.3
+
+[float]
+==== Plugins
+
+*Tcp Input*
+
+* Added new configuration option dns_reverse_lookup_enabled to allow users to disable costly DNS reverse lookups https://github.com/logstash-plugins/logstash-input-tcp/issues/100[#100]
+
+*S3 Output*
+
+* Fixed leak of file handles that prevented temporary files from being cleaned up before pipeline restart https://github.com/logstash-plugins/logstash-output-s3/pull/193[#193]
+
+
+[[logstash-6-4-2]]
+=== Logstash 6.4.2 Release Notes
+
+* Make cgroups support more robust and provide the override similar to ES ({lsissue}/10012[#10012]).
+
+[float]
+==== Plugins
+*Cef Codec*
+
+* Added reverse_mapping option, which can be used to make encoder compliant to spec https://github.com/logstash-plugins/logstash-codec-cef/pull/51[#51]
+
+* Fix handling of malformed inputs that have illegal unescaped-equals characters in extension field values (restores behaviour from <= v5.0.3 in some edge-cases) https://github.com/logstash-plugins/logstash-codec-cef/issues/56[#56]
+
+*Heartbeat Input*
+
+* Fixed shutdown concurrency issues by simplifying shutdown signal handling https://github.com/logstash-plugins/logstash-input-heartbeat/pull/15[#15]
+
+[[logstash-6-4-1]]
+=== Logstash 6.4.1 Release Notes
+
+* Support ssl verification mode in monitoring and management ({lsissue}/9866[#9866]).
+* Extract kibana and elasticsearch client ssl config ({lsissue}/9945[#9945]).
+* Avoid race condition when initializing events and pipelines metrics ({lsissue}/9958[#9958]).
+* Make worker thread names visible to OS ({lsissue}/9973[#9973]).
+* Update logstash.bat to enable CLASSPATH with spaces ({lsissue}/9966[#9966]).
+* Pipeline reloading breaks with PQ enabled ({lsissue}/9986[#9986]).
+
+[float]
+==== Plugins
+
+*CEF Codec*
+
+* Fix bug in parsing extension values where a legal unescaped space in a field's value could be interpreted as a field separator https://github.com/logstash-plugins/logstash-codec-cef/issues/54[#54]
+* Fix bug in parsing headers where certain legal escape sequences could cause non-escaped pipe characters to be ignored https://github.com/logstash-plugins/logstash-codec-cef/pull/55[#55]
+* Add explicit handling for extension key names that use array-like syntax that isn't legal with the strict-mode field-reference parser (e.g., `fieldname[0]` becomes `[fieldname][0]`) https://github.com/logstash-plugins/logstash-codec-cef/pull/55[#55]
+
+*File Input*
+
+* Fixed Errno::ENOENT exception in Discoverer. https://github.com/logstash-plugins/logstash-input-file/issues/204[#204]
+
+*JDBC Input*
+
+* Added check to prevent count sql syntax errors when debug logging https://github.com/logstash-plugins/logstash-input-jdbc/pull/294[#294]
+* Changed documentation to generalize the PATH location https://github.com/logstash-plugins/logstash-input-jdbc/pull/297[#297]
+
+*Azure Event Hubs Input*
+
+* Added guidelines for setting number of threads https://github.com/logstash-plugins/logstash-input-azure_event_hubs/pull/17[#17]
+
+*HTTP Input*
+
+* Fix expensive SslContext creation per connection https://github.com/logstash-plugins/logstash-input-http/pull/93[#93]
+
+*UDP Input*
+
+* Fixed input workers exception handling and shutdown handling https://github.com/logstash-plugins/logstash-input-udp/pull/44[#44]
+
+*Ruby Filter*
+
+* Fixed path based scripting not calling filter\_matched https://github.com/logstash-plugins/logstash-filter-ruby/issues/45[#45]
+
+*Mutate Filter*
+
+* Changed documentation to clarify use of `replace` config option https://github.com/logstash-plugins/logstash-filter-mutate/pull/125[#125]
+
+*Clone Filter*
+
+* Added a warning when 'clones' is empty since that results in a no-op https://github.com/logstash-plugins/logstash-filter-clone/issues/14[#14]
+
+*KV Filter*
+
+* Fixes performance regression introduced in 4.1.0 https://github.com/logstash-plugins/logstash-filter-kv/issues/70[#70]
+
+*Elasticsearch Output*
+
+* Add text offering Elasticsearch Service hosted es https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/792[#792]
+
+*Kafka Output*
+
+* Fixed handling of receive buffer bytes setting https://github.com/logstash-plugins/logstash-output-kafka/pull/204[#204]
+
+*S3 Output*
+
+* Fixed bucket validation failures when bucket policy requires encryption https://github.com/logstash-plugins/logstash-output-s3/pull/191[#191]
+
+[[logstash-6-4-0]]
+=== Logstash 6.4.0 Release Notes
+
+[IMPORTANT]
+--
+**Attention users of Kafka Output in Logstash 6.4.0**
+
+If you are using Kafka output and have upgraded to Logstash 6.4.0, you will see pipeline startup errors:
+
+    Pipeline aborted due to error {:pipeline_id=>"pipeline1", :exception=>org.apache.kafka.common.config.ConfigException: Invalid value 32768 for configuration receive.buffer.bytes: Expected value to be a 32-bit integer, but it was a java.lang.Long
+
+This error was due to an incorrectly configured default value for the
+`receive_buffer_bytes` option (fixed in PR
+https://github.com/logstash-plugins/logstash-output-kafka/pull/205[logstash-output-kafka #205]),
+and false negative results on our CI due to incorrect exit code
+handling (fixed in
+https://github.com/logstash-plugins/logstash-output-kafka/pull/204[logstash-output-kafka#204]).
+
+Kafka output plugin version 7.1.3 has been released. You can upgrade
+using:
+
+[source,sh]
+-----
+bin/logstash-plugin update logstash-output-kafka
+-----
+
+This version will be included in the next 6.4.1 patch release.
+--
+
+* Adds the Azure Module for integrating Azure activity logs and SQL diagnostic logs with the Elastic Stack.
+* Adds the {logstash-ref}/plugins-inputs-azure_event_hubs.html[azure_event_hubs input plugin] as a default plugin.
+* Adds support for port customization in cloud id ({lsissue}/9877[#9877]).
+* Adds opt-in strict-mode for field reference ({lsissue}/9591[#9591]).
+* Adds syntax highlighting for expressions in Grok Debugger https://github.com/elastic/kibana/pull/18572[Kibana#18572]
+* Changes pipeline viewer visualization to use more tree like layout to express structure of pipeline configuration https://github.com/elastic/kibana/pull/18597[Kibana#18597]
+* Fixes incorrect pipeline shutdown logging ({lsissue}/9688[#9688]).
+* Fixes incorrect type handling between Java pipeline and Ruby pipeline ({lsissue}/9671[#9671]).
+* Fixes possible where Ensure separate output streams to avoid keystore corruption issue by ensuring separate output streams ({lsissue}/9582[#9582]).
+* Javafication to continue moving parts of Logstash core from Ruby to Java and some general code cleanup ({lsissue}/9414[#9414], {lsissue}/9415[#9415], {lsissue}/9416[#9416], {lsissue}/9422[#9422], {lsissue}/9482[#9482], {lsissue}/9486[#9486], {lsissue}/9489[#9489], {lsissue}/9490[#9490], {lsissue}/9491[#9491], {lsissue}/9496[#9496], {lsissue}/9520[#9520], {lsissue}/9587[#9587], {lsissue}/9574[#9574], {lsissue}/9610[#9610], {lsissue}/9620[#9620], {lsissue}/9631[#9631], {lsissue}/9632[#9632], {lsissue}/9633[#9633], {lsissue}/9661[#9661], {lsissue}/9662[#9662], {lsissue}/9665[#9665], {lsissue}/9667[#9667], {lsissue}/9668[#9668], {lsissue}/9670[#9670], {lsissue}/9676[#9676], {lsissue}/9687[#9687], {lsissue}/9693[#9693], {lsissue}/9697[#9697], {lsissue}/9699[#9699], {lsissue}/9717[#9717], {lsissue}/9723[#9723], {lsissue}/9731[#9731], {lsissue}/9740[#9740], {lsissue}/9742[#9742], {lsissue}/9743[#9743], {lsissue}/9751[#9751], {lsissue}/9752[#9752], {lsissue}/9765[#9765]).
+
+[float]
+==== Plugins
+
+*Rubydebug Codec*
+
+* Fixes crash that could occur on startup if `$HOME` was unset or if `${HOME}/.aprc` was unreadable by pinning awesome_print dependency to a release before the bug was introduced. https://github.com/logstash-plugins/logstash-codec-rubydebug/pull/5[#5]
+
+*Fingerprint Filter*
+
+* Adds support for non-keyed, regular hash functions. https://github.com/logstash-plugins/logstash-filter-fingerprint/issues/18[#18]
+
+*KV Filter*
+
+* Adds `whitespace => strict` mode, which allows the parser to behave more predictably when input is known to avoid unnecessary whitespace. https://github.com/logstash-plugins/logstash-filter-kv/pull/67[#67]
+* Adds error handling, which tags the event with `_kv_filter_error` if an exception is raised while handling an event instead of allowing the plugin to crash. https://github.com/logstash-plugins/logstash-filter-kv/pull/68[#68]
+
+*Azure Event Hubs Input*
+
+* Initial version of the {logstash-ref}/plugins-inputs-azure_event_hubs.html[azure_event_hubs input plugin], which supersedes logstash-input-azureeventhub.
+
+*Beats Input*
+
+* Adds `add_hostname` flag to enable/disable the population of the `host` field from the beats.hostname. field https://github.com/logstash-plugins/logstash-input-beats/pull/340[#340]
+* Fixes handling of batches where the sequence numbers do not start with 1. https://github.com/logstash-plugins/logstash-input-beats/pull/342[#342]
+* Changes project to use gradle version 4.8.1. https://github.com/logstash-plugins/logstash-input-beats/pull/334[#334]
+* Adds `ssl_peer_metadata` option. https://github.com/logstash-plugins/logstash-input-beats/pull/327[#327]
+* Fixes `ssl_verify_mode => peer`. https://github.com/logstash-plugins/logstash-input-beats/pull/326[#326]
+
+*Exec Input*
+
+* Fixes issue where certain log entries were incorrectly writing 'jdbc input' instead of 'exec input'. https://github.com/logstash-plugins/logstash-input-exec/pull/21[#21]
+
+*File Input*
+
+* Adds new feature: `mode` setting. Introduces two modes, `tail` mode is the existing behaviour for tailing, `read` mode is new behaviour that is optimized for the read complete content scenario. Please read the docs to fully appreciate the benefits of `read` mode.
+* Adds new feature: File completion actions. Settings `file_completed_action` and `file_completed_log_path` control what actions to do after a file is completely read. Applicable: `read` mode only.
+* Adds new feature: in `read` mode, compressed files can be processed, GZIP only.
+* Adds new feature: Files are sorted after being discovered. Settings `file_sort_by` and `file_sort_direction` control the sort order. Applicable: any mode.
+* Adds new feature: Banded or striped file processing. Settings: `file_chunk_size` and `file_chunk_count` control banded or striped processing. Applicable: any mode.
+* Adds new feature: `sincedb_clean_after` setting. Introduces expiry of sincedb records. The default is 14 days. If, after `sincedb_clean_after` days, no activity has been detected on a file (inode) the record expires and is not written to disk. The persisted record now includes the "last activity seen" timestamp. Applicable: any mode.
+* Moves Filewatch code into the plugin folder, rework Filewatch code to use Logstash facilities like logging and environment.
+* Adds much better support for file rotation schemes of copy/truncate and rename cascading. Applies to tail mode only.
+* Adds support for processing files over remote mounts e.g. NFS. Before, it was possible to read into memory allocated but not filled with data resulting in ASCII NUL (0) bytes in the message field. Now, files are read up to the size as given by the remote filesystem client. Applies to tail and read modes.
+* Fixes `read` mode of regular files sincedb write is requested in each read loop iteration rather than waiting for the end-of-file to be reached. Note: for gz files, the sincedb entry can only be updated at the end of the file as it is not possible to seek into a compressed file and begin reading from that position. https://github.com/logstash-plugins/logstash-input-file/pull/196[#196]
+* Adds support for String Durations in some settings e.g. `stat_interval => "750 ms"`. https://github.com/logstash-plugins/logstash-input-file/pull/194[#194]
+* Fixes `require winhelper` error in WINDOWS. https://github.com/logstash-plugins/logstash-input-file/issues/184[#184]
+* Fixes issue, where when no delimiter is found in a chunk, the chunk is reread - no forward progress is made in the file. https://github.com/logstash-plugins/logstash-input-file/issues/185[#185]
+* Fixes JAR_VERSION read problem, prevented Logstash from starting. https://github.com/logstash-plugins/logstash-input-file/issues/180[#180]
+* Fixes sincedb write error when using /dev/null, repeatedly causes a plugin restart. https://github.com/logstash-plugins/logstash-input-file/issues/182[#182]
+* Fixes a regression where files discovered after first discovery were not always read from the beginning. Applies to tail mode only. https://github.com/logstash-plugins/logstash-input-file/issues/198[#198]
+
+
+*Http Input*
+
+* Replaces Puma web server with Netty. https://github.com/logstash-plugins/logstash-input-http/pull/73[#73]
+* Adds `request_headers_target_field` and `remote_host_target_field` configuration options with default to host and headers respectively. https://github.com/logstash-plugins/logstash-input-http/pull/68[#68]
+* Sanitizes content-type header with getMimeType. https://github.com/logstash-plugins/logstash-input-http/pull/87[#87]
+* Moves most message handling code to Java. https://github.com/logstash-plugins/logstash-input-http/pull/85[#85]
+* Fixes issue to respond with correct http protocol version. https://github.com/logstash-plugins/logstash-input-http/pull/84[#84]
+* Adds support for crt/key certificates.
+* Deprecates jks support.
+
+*Jdbc Input*
+
+* Fixes crash that occurs when receiving string input that cannot be coerced to UTF-8 (such as BLOB data). https://github.com/logstash-plugins/logstash-input-jdbc/pull/291[#291]
+
+*S3 Input*
+
+* Adds ability to optionally include S3 object properties inside `@metadata`. https://github.com/logstash-plugins/logstash-input-s3/pull/155[#155]
+
+*Kafka Output*
+
+* Fixes handling of two settings that weren't wired to the kafka client. https://github.com/logstash-plugins/logstash-output-kafka/pull/198[#198]
+
 [[logstash-6-3-2]]
 === Logstash 6.3.2 Release Notes
 
diff --git a/logstash-core/lib/logstash/modules/logstash_config.rb b/logstash-core/lib/logstash/modules/logstash_config.rb
index c72e0029990..0e0345e723e 100644
--- a/logstash-core/lib/logstash/modules/logstash_config.rb
+++ b/logstash-core/lib/logstash/modules/logstash_config.rb
@@ -56,11 +56,25 @@ def setting(name, default)
         get_setting(LogStash::Setting::NullableString.new(name, default.to_s))
       when Numeric
         get_setting(LogStash::Setting::Numeric.new(name, default))
+      when true, false
+        get_setting(LogStash::Setting::Boolean.new(name, default))
       else
         get_setting(LogStash::Setting::NullableString.new(name, default.to_s))
       end
   end
 
+  def has_setting?(name)
+    @settings.key?(name)
+  end
+
+  def raw_setting(name)
+    @settings[name]
+  end
+
+  def fetch_raw_setting(name, default)
+    @settings.fetch(name, default)
+  end
+
   def elasticsearch_output_config(type_string = nil)
     hosts = array_to_string(get_setting(LogStash::Setting::SplittableStringArray.new("var.elasticsearch.hosts", String, ["localhost:9200"])))
     index = "#{@name}-#{setting("var.elasticsearch.index_suffix", "%{+YYYY.MM.dd}")}"
diff --git a/logstash-core/lib/logstash/runner.rb b/logstash-core/lib/logstash/runner.rb
index 714af908bb9..bc964a9dc13 100644
--- a/logstash-core/lib/logstash/runner.rb
+++ b/logstash-core/lib/logstash/runner.rb
@@ -348,6 +348,8 @@ def execute
     # lock path.data before starting the agent
     @data_path_lock = FileLockFactory.obtainLock(java.nio.file.Paths.get(setting("path.data")).to_absolute_path, ".lock")
 
+    logger.info("Starting Logstash", "logstash.version" => LOGSTASH_VERSION)
+
     @dispatcher.fire(:before_agent)
     @agent = create_agent(@settings, @source_loader)
     @dispatcher.fire(:after_agent)
@@ -357,8 +359,6 @@ def execute
     sigint_id = trap_sigint()
     sigterm_id = trap_sigterm()
 
-    logger.info("Starting Logstash", "logstash.version" => LOGSTASH_VERSION)
-
     @agent_task = Stud::Task.new { @agent.execute }
 
     # no point in enabling config reloading before the agent starts
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
index 10737a2bd50..ec8faac68c5 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
@@ -158,7 +158,7 @@ public void open() throws IOException {
 
                 logger.debug("No head checkpoint found at: {}, creating new head page", checkpointIO.headFileName());
 
-                this.ensureDiskAvailable(this.maxBytes);
+                this.ensureDiskAvailable(this.maxBytes, 0);
 
                 this.seqNum = 0;
                 headPageNum = 0;
@@ -172,7 +172,7 @@ public void open() throws IOException {
             // at this point we have a head checkpoint to figure queue recovery
 
             // as we load pages, compute actuall disk needed substracting existing pages size to the required maxBytes
-            long diskNeeded = this.maxBytes;
+            long pqSizeBytes = 0;
 
             // reconstruct all tail pages state upto but excluding the head page
             for (int pageNum = headCheckpoint.getFirstUnackedPageNum(); pageNum < headCheckpoint.getPageNum(); pageNum++) {
@@ -192,7 +192,7 @@ public void open() throws IOException {
                 } else {
                     pageIO.open(cp.getMinSeqNum(), cp.getElementCount());
                     addTailPage(PageFactory.newTailPage(cp, this, pageIO));
-                    diskNeeded -= (long)pageIO.getHead();
+                    pqSizeBytes += (long)pageIO.getCapacity();
                 }
 
                 // track the seqNum as we rebuild tail pages, prevent empty pages with a minSeqNum of 0 to reset seqNum
@@ -209,7 +209,8 @@ public void open() throws IOException {
             PageIO pageIO = new MmapPageIOV2(headCheckpoint.getPageNum(), this.pageCapacity, this.dirPath);
             pageIO.recover(); // optimistically recovers the head page data file and set minSeqNum and elementCount to the actual read/recovered data
 
-            ensureDiskAvailable(diskNeeded - (long)pageIO.getHead());
+            pqSizeBytes += (long)pageIO.getHead();
+            ensureDiskAvailable(this.maxBytes, pqSizeBytes);
 
             if (pageIO.getMinSeqNum() != headCheckpoint.getMinSeqNum() || pageIO.getElementCount() != headCheckpoint.getElementCount()) {
                 // the recovered page IO shows different minSeqNum or elementCount than the checkpoint, use the page IO attributes
@@ -784,8 +785,8 @@ private boolean isTailPage(Page p) {
         return !isHeadPage(p);
     }
 
-    private void ensureDiskAvailable(final long diskNeeded) throws IOException {
-        if (!FsUtil.hasFreeSpace(this.dirPath, diskNeeded)) {
+    private void ensureDiskAvailable(final long maxPqSize, final long currentPqSize) throws IOException {
+        if (!FsUtil.hasFreeSpace(this.dirPath, maxPqSize - currentPqSize)) {
             throw new IOException("Not enough free disk space available to allocate persisted queue.");
         }
     }
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/compiler/BaseDataset.java b/logstash-core/src/main/java/org/logstash/config/ir/compiler/BaseDataset.java
new file mode 100644
index 00000000000..7b4c2a011e7
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/compiler/BaseDataset.java
@@ -0,0 +1,21 @@
+package org.logstash.config.ir.compiler;
+
+/**
+ * Provides common behavior for Dataset classes generated by Java execution.
+ */
+public abstract class BaseDataset implements Dataset {
+    private boolean done;
+
+    protected void setDone() {
+        done = true;
+    }
+
+    protected void clearDone() {
+        done = false;
+    }
+
+    protected boolean isDone() {
+        return done;
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/compiler/ComputeStepSyntaxElement.java b/logstash-core/src/main/java/org/logstash/config/ir/compiler/ComputeStepSyntaxElement.java
index 898286eb239..2cf86c28402 100644
--- a/logstash-core/src/main/java/org/logstash/config/ir/compiler/ComputeStepSyntaxElement.java
+++ b/logstash-core/src/main/java/org/logstash/config/ir/compiler/ComputeStepSyntaxElement.java
@@ -107,7 +107,7 @@ private String generateCode(final String name) {
         try {
             return REDUNDANT_SEMICOLON.matcher(new Formatter().formatSource(
                 String.format(
-                    "package org.logstash.generated;\npublic final class %s implements %s { %s }",
+                    "package org.logstash.generated;\npublic final class %s extends org.logstash.config.ir.compiler.BaseDataset implements %s { %s }",
                     name,
                     type.getName(),
                     SyntaxFactory.join(
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/compiler/DatasetCompiler.java b/logstash-core/src/main/java/org/logstash/config/ir/compiler/DatasetCompiler.java
index 50b8d30f003..b277671414d 100644
--- a/logstash-core/src/main/java/org/logstash/config/ir/compiler/DatasetCompiler.java
+++ b/logstash-core/src/main/java/org/logstash/config/ir/compiler/DatasetCompiler.java
@@ -208,16 +208,8 @@ private static Closure conditionalLoop(final VariableDefinition event,
         final ValueSyntaxElement ifData, final ValueSyntaxElement elseData) {
         final ValueSyntaxElement eventVal = event.access();
         return Closure.wrap(
-            SyntaxFactory.forLoop(
-                event, inputBuffer,
-                Closure.wrap(
-                    SyntaxFactory.ifCondition(
-                        condition.call("fulfilled", eventVal),
-                        Closure.wrap(ifData.call("add", eventVal)),
-                        Closure.wrap(elseData.call("add", eventVal))
-                    )
-                )
-            )
+                SyntaxFactory.value("org.logstash.config.ir.compiler.Utils")
+                        .call("filterEvents", inputBuffer, condition, ifData, elseData)
         );
     }
 
@@ -245,22 +237,10 @@ private static ComputeStepSyntaxElement<Dataset> prepare(final DatasetCompiler.C
      */
     private static Closure withInputBuffering(final Closure compute,
         final Collection<ValueSyntaxElement> parents, final ValueSyntaxElement inputBuffer) {
-        final VariableDefinition event =
-            new VariableDefinition(JrubyEventExtLibrary.RubyEvent.class, "e");
-        final ValueSyntaxElement eventVar = event.access();
         return Closure.wrap(
-            parents.stream().map(par ->
-                SyntaxFactory.forLoop(
-                    event, computeDataset(par),
-                    Closure.wrap(
-                        SyntaxFactory.ifCondition(
-                            SyntaxFactory.not(
-                                eventVar.call("getEvent").call("isCancelled")
-                            ), Closure.wrap(inputBuffer.call("add", eventVar))
-                        )
-                    )
-                )
-            ).toArray(MethodLevelSyntaxElement[]::new)
+                parents.stream().map(par -> SyntaxFactory.value("org.logstash.config.ir.compiler.Utils")
+                        .call("copyNonCancelledEvents", computeDataset(par), inputBuffer)
+                ).toArray(MethodLevelSyntaxElement[]::new)
         ).add(compute).add(clear(inputBuffer));
     }
 
@@ -278,18 +258,18 @@ event, computeDataset(par),
      */
     private static DatasetCompiler.ComputeAndClear withOutputBuffering(final Closure compute,
         final Closure clear, final ValueSyntaxElement outputBuffer, final ClassFields fields) {
-        final ValueSyntaxElement done = fields.add(boolean.class);
+        final SyntaxFactory.MethodCallReturnValue done = new SyntaxFactory.MethodCallReturnValue(SyntaxFactory.value("this"), "isDone");
         return computeAndClear(
             Closure.wrap(
                 SyntaxFactory.ifCondition(done, Closure.wrap(SyntaxFactory.ret(outputBuffer)))
             ).add(compute)
-                .add(SyntaxFactory.assignment(done, SyntaxFactory.identifier("true")))
+                .add(new SyntaxFactory.MethodCallReturnValue(SyntaxFactory.value("this"), "setDone"))
                 .add(SyntaxFactory.ret(outputBuffer)),
             Closure.wrap(
                 SyntaxFactory.ifCondition(
                     done, Closure.wrap(
                         clear.add(clear(outputBuffer)),
-                        SyntaxFactory.assignment(done, SyntaxFactory.identifier("false"))
+                        new SyntaxFactory.MethodCallReturnValue(SyntaxFactory.value("this"), "clearDone")
                     )
                 )
             ), fields
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/compiler/EventCondition.java b/logstash-core/src/main/java/org/logstash/config/ir/compiler/EventCondition.java
index dee0ab4387d..d15f66cbb00 100644
--- a/logstash-core/src/main/java/org/logstash/config/ir/compiler/EventCondition.java
+++ b/logstash-core/src/main/java/org/logstash/config/ir/compiler/EventCondition.java
@@ -437,7 +437,7 @@ private static EventCondition rubyFieldEquals(final Comparable<IRubyObject> left
             final String field) {
             final FieldReference reference = FieldReference.from(field);
             return event ->
-                left.equals((IRubyObject) event.getEvent().getUnconvertedField(reference));
+                    left.equals(Rubyfier.deep(RubyUtil.RUBY, event.getEvent().getUnconvertedField(reference)));
         }
 
         private static EventCondition constant(final boolean value) {
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/compiler/Utils.java b/logstash-core/src/main/java/org/logstash/config/ir/compiler/Utils.java
new file mode 100644
index 00000000000..8165258e568
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/compiler/Utils.java
@@ -0,0 +1,33 @@
+package org.logstash.config.ir.compiler;
+
+import org.logstash.ext.JrubyEventExtLibrary;
+
+import java.util.Collection;
+import java.util.List;
+
+/**
+ * Static utility methods that replace common blocks of generated code in the Java execution.
+ */
+public class Utils {
+
+    // has field1.compute(batchArg, flushArg, shutdownArg) passed as input
+    public static void copyNonCancelledEvents(Collection<JrubyEventExtLibrary.RubyEvent> input, List output) {
+        for (JrubyEventExtLibrary.RubyEvent e : input) {
+            if (!(e.getEvent().isCancelled())) {
+                output.add(e);
+            }
+        }
+    }
+
+    public static void filterEvents(Collection<JrubyEventExtLibrary.RubyEvent> input, EventCondition filter,
+                                    List fulfilled, List unfulfilled) {
+        for (JrubyEventExtLibrary.RubyEvent e : input) {
+            if (filter.fulfilled(e)) {
+                fulfilled.add(e);
+            } else {
+                unfulfilled.add(e);
+            }
+        }
+    }
+
+}
diff --git a/logstash-core/src/test/java/org/logstash/config/ir/CompiledPipelineTest.java b/logstash-core/src/test/java/org/logstash/config/ir/CompiledPipelineTest.java
index ae9f7c4f0f8..6d434aed6c2 100644
--- a/logstash-core/src/test/java/org/logstash/config/ir/CompiledPipelineTest.java
+++ b/logstash-core/src/test/java/org/logstash/config/ir/CompiledPipelineTest.java
@@ -1,15 +1,6 @@
 package org.logstash.config.ir;
 
 import com.google.common.base.Strings;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.LinkedTransferQueue;
-import java.util.concurrent.atomic.AtomicLong;
-import java.util.function.Consumer;
-import java.util.function.Supplier;
 import org.hamcrest.CoreMatchers;
 import org.hamcrest.MatcherAssert;
 import org.jruby.RubyInteger;
@@ -18,6 +9,8 @@
 import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
+import org.logstash.ConvertedList;
+import org.logstash.ConvertedMap;
 import org.logstash.Event;
 import org.logstash.RubyUtil;
 import org.logstash.common.IncompleteSourceWithMetadataException;
@@ -26,6 +19,17 @@
 import org.logstash.config.ir.compiler.RubyIntegration;
 import org.logstash.ext.JrubyEventExtLibrary;
 
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.LinkedTransferQueue;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.function.Consumer;
+import java.util.function.Supplier;
+
 /**
  * Tests for {@link CompiledPipeline}.
  */
@@ -240,6 +244,37 @@ public void correctlyCompilesGreaterOrEqualThan() throws Exception {
         assertCorrectFieldToFieldComparison(gte, 7, 8, false);
     }
 
+    @Test
+    public void equalityCheckOnCompositeField() throws Exception {
+        final PipelineIR pipelineIR = ConfigCompiler.configToPipelineIR(
+                "input {mockinput{}} filter { if 4 == [list] { mockaddfilter {} } if 5 == [map] { mockaddfilter {} } } output {mockoutput{} }",
+                false
+        );
+        final Collection<String> s = new ArrayList<>();
+        s.add("foo");
+        final Map<String, Object> m = new HashMap<>();
+        m.put("foo", "bar");
+        final JrubyEventExtLibrary.RubyEvent testEvent =
+                JrubyEventExtLibrary.RubyEvent.newRubyEvent(RubyUtil.RUBY, new Event());
+        testEvent.getEvent().setField("list", ConvertedList.newFromList(s));
+        testEvent.getEvent().setField("map", ConvertedMap.newFromMap(m));
+
+        final Map<String, Supplier<IRubyObject>> filters = new HashMap<>();
+        filters.put("mockaddfilter", () -> ADD_FIELD_FILTER);
+        new CompiledPipeline(
+                pipelineIR,
+                new CompiledPipelineTest.MockPluginFactory(
+                        Collections.singletonMap("mockinput", () -> null),
+                        filters,
+                        Collections.singletonMap("mockoutput", mockOutputSupplier())
+                )
+        ).buildExecution().compute(RubyUtil.RUBY.newArray(testEvent), false, false);
+        final Collection<JrubyEventExtLibrary.RubyEvent> outputEvents = EVENT_SINKS.get(runId);
+        MatcherAssert.assertThat(outputEvents.size(), CoreMatchers.is(1));
+        MatcherAssert.assertThat(outputEvents.contains(testEvent), CoreMatchers.is(true));
+        MatcherAssert.assertThat(testEvent.getEvent().getField("foo"), CoreMatchers.nullValue());
+    }
+
     @Test
     public void conditionalWithNullField() throws Exception {
         final PipelineIR pipelineIR = ConfigCompiler.configToPipelineIR(
diff --git a/rakelib/plugins-metadata.json b/rakelib/plugins-metadata.json
index 37646a0503c..7d916b446ad 100644
--- a/rakelib/plugins-metadata.json
+++ b/rakelib/plugins-metadata.json
@@ -468,7 +468,7 @@
 		"test-vendor-plugins": false,
 		"skip-list": false
 	},
-	"logstash-output-appsearch": {
+	"logstash-output-elastic_app_search": {
 		"default-plugins": true,
 		"core-specs": false,
 		"test-jar-dependencies": false,
diff --git a/versions.yml b/versions.yml
index f902886925a..729464547fd 100644
--- a/versions.yml
+++ b/versions.yml
@@ -1,6 +1,6 @@
 ---
-logstash: 6.5.0
-logstash-core: 6.5.0
+logstash: 6.6.0
+logstash-core: 6.6.0
 logstash-core-plugin-api: 2.1.16
 
 # jruby must reference a *released* version of jruby which can be downloaded from the official download url
diff --git a/x-pack/lib/config_management/elasticsearch_source.rb b/x-pack/lib/config_management/elasticsearch_source.rb
index 9d738e32228..f8a860fbbce 100644
--- a/x-pack/lib/config_management/elasticsearch_source.rb
+++ b/x-pack/lib/config_management/elasticsearch_source.rb
@@ -142,7 +142,13 @@ def config_path
       end
 
       def populate_license_state(xpack_info)
-        if !xpack_info.installed?
+        if xpack_info.failed?
+          {
+              :state => :error,
+              :log_level => :error,
+              :log_message => "Failed to fetch X-Pack information from Elasticsearch. This is likely due to failure to reach a live Elasticsearch cluster."
+          }
+        elsif !xpack_info.installed?
           {
               :state => :error,
               :log_level => :error,
@@ -193,4 +199,4 @@ def client
       end
     end
   end
-end
\ No newline at end of file
+end
diff --git a/x-pack/lib/license_checker/license_manager.rb b/x-pack/lib/license_checker/license_manager.rb
index 414e3e6e411..076570041aa 100644
--- a/x-pack/lib/license_checker/license_manager.rb
+++ b/x-pack/lib/license_checker/license_manager.rb
@@ -39,9 +39,6 @@ def current_xpack_info
       def fetch_xpack_info
         xpack_info = @license_reader.fetch_xpack_info
 
-        # TODO: we should be more lenient when we're having issues
-        xpack_info ||= XPackInfo.xpack_not_installed
-
         update_xpack_info(xpack_info)
       end
 
diff --git a/x-pack/lib/license_checker/license_reader.rb b/x-pack/lib/license_checker/license_reader.rb
index 1fb74f1093f..be978ae1d06 100644
--- a/x-pack/lib/license_checker/license_reader.rb
+++ b/x-pack/lib/license_checker/license_reader.rb
@@ -18,6 +18,7 @@ def initialize(settings, feature, options)
         @namespace = "xpack.#{feature}"
         @settings = settings
         @es_options = options
+        @es_options.merge!("resurrect_delay" => 30)
       end
 
       ##
@@ -37,8 +38,12 @@ def fetch_xpack_info
           XPackInfo.xpack_not_installed
         end
       rescue => e
-        logger.error('Unable to retrieve license information from license server', :message => e.message, :class => e.class.name, :backtrace => e.backtrace)
-        nil
+        if logger.debug?
+          logger.error('Unable to retrieve license information from license server', :message => e.message, :class => e.class.name, :backtrace => e.backtrace)
+        else
+          logger.error('Unable to retrieve license information from license server', :message => e.message)
+        end
+        XPackInfo.failed_to_fetch
       end
 
       ##
diff --git a/x-pack/lib/license_checker/x_pack_info.rb b/x-pack/lib/license_checker/x_pack_info.rb
index ee16990ee02..e2461f56e7f 100644
--- a/x-pack/lib/license_checker/x_pack_info.rb
+++ b/x-pack/lib/license_checker/x_pack_info.rb
@@ -14,10 +14,11 @@ class XPackInfo
 
       LICENSE_TYPES = :trial, :basic, :standard, :gold, :platinum
 
-      def initialize(license, features = nil, installed=true)
+      def initialize(license, features = nil, installed=true, failed = false)
         @license = license
         @installed = installed
         @features = features
+        @failed = failed
 
         freeze
       end
@@ -31,6 +32,10 @@ def method_missing(meth)
         end
       end
 
+      def failed?
+        @failed
+      end
+
       def installed?
         @installed
       end
@@ -86,7 +91,11 @@ def self.from_es_response(es_response)
       end
 
       def self.xpack_not_installed
-        XPackInfo.new(nil, nil,false)
+        XPackInfo.new(nil, nil, false)
+      end
+
+      def self.failed_to_fetch
+        XPackInfo.new(nil, nil, false, true)
       end
     end
   end
diff --git a/x-pack/lib/modules/module_license_checker.rb b/x-pack/lib/modules/module_license_checker.rb
index 1bab2f34492..3e3ea15c29a 100644
--- a/x-pack/lib/modules/module_license_checker.rb
+++ b/x-pack/lib/modules/module_license_checker.rb
@@ -37,7 +37,13 @@ def setup(settings)
       end
 
       def populate_license_state(xpack_info)
-        if !xpack_info.installed?
+        if xpack_info.failed?
+          {
+              :state => :error,
+              :log_level => :error,
+              :log_message => "Failed to fetch X-Pack information from Elasticsearch. This is likely due to failure to reach a live Elasticsearch cluster."
+          }
+        elsif !xpack_info.installed?
           {
               :state => :error,
               :log_level => :error,
diff --git a/x-pack/lib/monitoring/inputs/metrics.rb b/x-pack/lib/monitoring/inputs/metrics.rb
index 442ce926674..c05733e433a 100644
--- a/x-pack/lib/monitoring/inputs/metrics.rb
+++ b/x-pack/lib/monitoring/inputs/metrics.rb
@@ -5,7 +5,6 @@
 require "logstash/event"
 require "logstash/inputs/base"
 require "logstash/instrument/collector"
-require 'license_checker/licensed'
 require 'helpers/elasticsearch_options'
 require "concurrent"
 require "thread"
@@ -17,17 +16,12 @@ module LogStash module Inputs
   # This input further transform it into a `Logstash::Event`, which can be consumed by the shipper and
   # shipped to Elasticsearch
   class Metrics < LogStash::Inputs::Base
-    include LogStash::LicenseChecker::Licensed, LogStash::Helpers::ElasticsearchOptions
-
     require "monitoring/inputs/metrics/state_event_factory"
     require "monitoring/inputs/metrics/stats_event_factory"
     
     @pipelines_mutex = Mutex.new
     @pipelines = {}
 
-    VALID_LICENSES = %w(basic trial standard gold platinum)
-    FEATURE = 'monitoring'
-
     require "monitoring/inputs/timer_task_logger"
     
     attr_reader :queue, :agent
@@ -52,16 +46,12 @@ def register
       @agent = nil
       @settings = LogStash::SETTINGS.clone
       @last_updated_pipeline_hashes = []
-      @es_options = es_options_from_settings_or_modules(FEATURE, @settings)
-      setup_license_checker(FEATURE)
+      @agent = execution_context.agent if execution_context
     end
 
     def pipeline_started(agent, pipeline)
       @agent = agent
-
-      with_license_check do
-        update_pipeline_state(pipeline)
-      end
+      update_pipeline_state(pipeline)
     end
 
     def configure_snapshot_poller
@@ -104,10 +94,8 @@ def stop
     end
 
     def update(snapshot)
-      with_license_check do
-        update_stats(snapshot)
-        update_states
-      end
+      update_stats(snapshot)
+      update_states
     end
 
     def update_stats(snapshot)
@@ -166,40 +154,6 @@ def emit_event(event)
       queue << event
     end
 
-    def populate_license_state(xpack_info)
-      if !xpack_info.installed?
-        {
-            :state => :error,
-            :log_level => :error,
-            :log_message => "X-Pack is installed on Logstash but not on Elasticsearch. Please install X-Pack on Elasticsearch to use the monitoring feature. Other features may be available."
-        }
-      elsif !xpack_info.license_available?
-        {
-            :state => :error,
-            :log_level => :error,
-            :log_message => 'Monitoring is not available: License information is currently unavailable. Please make sure you have added your production elasticsearch connection info in the xpack.monitoring.elasticsearch settings.'
-        }
-      elsif !xpack_info.license_one_of?(VALID_LICENSES)
-        {
-            :state => :error,
-            :log_level => :error,
-            :log_message => "Monitoring is not available: #{xpack_info.license_type} is not a valid license for this feature."
-        }
-      elsif !xpack_info.license_active?
-        {
-            :state => :ok,
-            :log_level => :warn,
-            :log_message => 'Monitoring requires a valid license. You can continue to monitor Logstash, but please contact your administrator to update your license'
-        }
-      else
-        unless xpack_info.feature_enabled?(FEATURE)
-          logger.warn('Monitoring installed and enabled in Logstash, but not enabled in Elasticsearch')
-        end
-
-        { :state => :ok, :log_level => :info, :log_message => 'Monitoring License OK' }
-      end
-    end
-
     private
     def remove_reserved_fields(event)
       event.remove("@timestamp")
diff --git a/x-pack/lib/monitoring/internal_pipeline_source.rb b/x-pack/lib/monitoring/internal_pipeline_source.rb
index 9aa03e97786..403617960e8 100644
--- a/x-pack/lib/monitoring/internal_pipeline_source.rb
+++ b/x-pack/lib/monitoring/internal_pipeline_source.rb
@@ -3,20 +3,91 @@
 # you may not use this file except in compliance with the Elastic License.
 
 require "logstash/config/source/base"
+require 'license_checker/licensed'
+require 'helpers/elasticsearch_options'
 
 module LogStash module Monitoring
   class InternalPipelineSource < LogStash::Config::Source::Base
-    def initialize(pipeline_config)
+    include LogStash::LicenseChecker::Licensed
+    include LogStash::Helpers::ElasticsearchOptions
+    include LogStash::Util::Loggable
+    VALID_LICENSES = %w(basic trial standard gold platinum)
+    FEATURE = 'monitoring'
+
+    def initialize(pipeline_config, agent)
       super(pipeline_config.settings)
       @pipeline_config = pipeline_config
+      @settings = LogStash::SETTINGS.clone
+      @agent = agent
+      @es_options = es_options_from_settings_or_modules(FEATURE, @settings)
+      setup_license_checker(FEATURE)
     end
 
     def pipeline_configs
-      return @pipeline_config
+      @pipeline_config
     end
 
     def match?
-      true
+      valid_basic_license?
+    end
+
+    def update_license_state(xpack_info)
+      return if valid_basic_license?
+      super(xpack_info) if xpack_info
+      if valid_basic_license?
+        logger.info("Validated license for monitoring. Enabling monitoring pipeline.")
+        enable_monitoring()
+      end
     end
+
+    private
+    def valid_basic_license?
+      @license_state ? license_check : false
+    end
+
+    def enable_monitoring
+      @agent.converge_state_and_update
+    end
+
+    def populate_license_state(xpack_info)
+      if xpack_info.failed?
+        {
+            :state => :error,
+            :log_level => :error,
+            :log_message => "Failed to fetch X-Pack information from Elasticsearch. This is likely due to failure to reach a live Elasticsearch cluster."
+        }
+      elsif !xpack_info.installed?
+        {
+            :state => :error,
+            :log_level => :error,
+            :log_message => "X-Pack is installed on Logstash but not on Elasticsearch. Please install X-Pack on Elasticsearch to use the monitoring feature. Other features may be available."
+        }
+      elsif !xpack_info.license_available?
+        {
+            :state => :error,
+            :log_level => :error,
+            :log_message => 'Monitoring is not available: License information is currently unavailable. Please make sure you have added your production elasticsearch connection info in the xpack.monitoring.elasticsearch settings.'
+        }
+      elsif !xpack_info.license_one_of?(VALID_LICENSES)
+        {
+            :state => :error,
+            :log_level => :error,
+            :log_message => "Monitoring is not available: #{xpack_info.license_type} is not a valid license for this feature."
+        }
+      elsif !xpack_info.license_active?
+        {
+            :state => :ok,
+            :log_level => :warn,
+            :log_message => 'Monitoring requires a valid license. You can continue to monitor Logstash, but please contact your administrator to update your license'
+        }
+      else
+        unless xpack_info.feature_enabled?(FEATURE)
+          logger.warn('Monitoring installed and enabled in Logstash, but not enabled in Elasticsearch')
+        end
+
+        { :state => :ok, :log_level => :info, :log_message => 'Monitoring License OK' }
+      end
+    end
+
   end
 end end
diff --git a/x-pack/lib/monitoring/monitoring.rb b/x-pack/lib/monitoring/monitoring.rb
index f209539b3cd..bfbd40f6800 100644
--- a/x-pack/lib/monitoring/monitoring.rb
+++ b/x-pack/lib/monitoring/monitoring.rb
@@ -95,7 +95,7 @@ def after_agent(runner)
 
         logger.trace("registering the metrics pipeline")
         LogStash::SETTINGS.set("node.uuid", runner.agent.id)
-        internal_pipeline_source = LogStash::Monitoring::InternalPipelineSource.new(setup_metrics_pipeline)
+        internal_pipeline_source = LogStash::Monitoring::InternalPipelineSource.new(setup_metrics_pipeline, runner.agent)
         runner.source_loader.add_source(internal_pipeline_source)
       rescue => e
         logger.error("Failed to set up the metrics pipeline", :message => e.message, :backtrace => e.backtrace)
diff --git a/x-pack/lib/x-pack/logstash_registry.rb b/x-pack/lib/x-pack/logstash_registry.rb
index 202108af579..afa20d9e792 100644
--- a/x-pack/lib/x-pack/logstash_registry.rb
+++ b/x-pack/lib/x-pack/logstash_registry.rb
@@ -3,7 +3,10 @@
 # you may not use this file except in compliance with the Elastic License.
 
 require "logstash/runner" # needed for LogStash::XPACK_PATH
-$LOAD_PATH << File.join(LogStash::XPACK_PATH, "modules", "azure", "lib")
+xpack_modules = ["azure", "arcsight"]
+xpack_modules.each do |name|
+  $LOAD_PATH << File.join(LogStash::XPACK_PATH, "modules", name, "lib")
+end
 require "logstash/plugins/registry"
 require "logstash/modules/util"
 require "monitoring/monitoring"
@@ -15,15 +18,14 @@
 LogStash::PLUGIN_REGISTRY.add(:input, "metrics", LogStash::Inputs::Metrics)
 LogStash::PLUGIN_REGISTRY.add(:universal, "monitoring", LogStash::MonitoringExtension)
 LogStash::PLUGIN_REGISTRY.add(:universal, "config_management", LogStash::ConfigManagement::Extension)
-LogStash::PLUGIN_REGISTRY.add(:modules, "arcsight",
-                              LogStash::Modules::XpackScaffold.new("arcsight",
-                                                                   File.join(File.dirname(__FILE__), "..", "..", "modules", "arcsight", "configuration"),
-                                                                   ["basic", "trial", "standard", "gold", "platinum"]
-                              ))
 
-LogStash::PLUGIN_REGISTRY.add(:modules, "azure",
-                              LogStash::Modules::XpackScaffold.new("azure",
-                                                                   File.join(File.dirname(__FILE__), "..", "..", "modules", "azure", "configuration"),
-                                                                   ["basic", "trial", "standard", "gold", "platinum"]
-                              ))
-LogStash::PLUGIN_REGISTRY.add(:filter, "azure_event", LogStash::Filters::AzureEvent)
\ No newline at end of file
+license_levels = Hash.new
+license_levels.default = ["basic", "trial", "standard", "gold", "platinum"]
+
+xpack_modules.each do |name|
+  path = File.join(File.dirname(__FILE__), "..", "..", "modules", name, "configuration")
+  LogStash::PLUGIN_REGISTRY.add(:modules, name,
+    LogStash::Modules::XpackScaffold.new(name, path, license_levels[name]))
+end
+
+LogStash::PLUGIN_REGISTRY.add(:filter, "azure_event", LogStash::Filters::AzureEvent)
diff --git a/x-pack/modules/arcsight/configuration/logstash/arcsight.conf.erb b/x-pack/modules/arcsight/configuration/logstash/arcsight.conf.erb
index 65df6e9a1bd..36bae36075e 100644
--- a/x-pack/modules/arcsight/configuration/logstash/arcsight.conf.erb
+++ b/x-pack/modules/arcsight/configuration/logstash/arcsight.conf.erb
@@ -11,6 +11,7 @@ alias_settings_keys!(
         "var.input.kafka" => "var.input.eventbroker",
         "var.input.tcp"   => "var.input.smartconnector"
     })
+require 'arcsight_module_config_helper'
 %>
 
 input {
@@ -19,6 +20,7 @@ input {
     codec => cef
     bootstrap_servers => <%= csv_string(get_setting(LogStash::Setting::SplittableStringArray.new("var.input.kafka.bootstrap_servers", String, "localhost:9092"))) %>
     topics => <%= array_to_string(get_setting(LogStash::Setting::SplittableStringArray.new("var.input.kafka.topics", String, ["eb-cef"]))) %>
+    <%= LogStash::Arcsight::ConfigHelper.kafka_input_ssl_sasl_config(self) %>
     type => syslog
   }
   <% end %>
@@ -28,6 +30,7 @@ input {
     # The delimiter config used is for TCP interpretation
     codec => cef { delimiter => "\r\n" }
     port => <%= setting("var.input.tcp.port", 5000) %>
+    <%= LogStash::Arcsight::ConfigHelper.tcp_input_ssl_config(self) %>
     type => syslog
   }
   <% end %>
diff --git a/x-pack/modules/arcsight/lib/arcsight_module_config_helper.rb b/x-pack/modules/arcsight/lib/arcsight_module_config_helper.rb
new file mode 100644
index 00000000000..c5a5ef3eafe
--- /dev/null
+++ b/x-pack/modules/arcsight/lib/arcsight_module_config_helper.rb
@@ -0,0 +1,67 @@
+# Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
+# or more contributor license agreements. Licensed under the Elastic License;
+# you may not use this file except in compliance with the Elastic License.
+
+require 'logstash/namespace'
+
+module LogStash
+  module Arcsight
+    module ConfigHelper
+      extend self
+      def kafka_input_ssl_sasl_config(bound_scope)
+        security_protocol = bound_scope.setting("var.input.kafka.security_protocol", "unset")
+        return "" if security_protocol == "unset"
+        lines = ["security_protocol => '#{security_protocol}'"]
+        lines.push("ssl_truststore_type => '#{bound_scope.setting("var.input.kafka.ssl_truststore_type", "")}'")
+
+        ssl_truststore_location = bound_scope.setting("var.input.kafka.ssl_truststore_location","")
+        lines.push("ssl_truststore_location => '#{ssl_truststore_location}'") unless ssl_truststore_location.empty?
+
+        ["ssl_truststore_password", "ssl_keystore_password", "ssl_key_password"].each do |name|
+          full_name = "var.input.kafka.".concat(name)
+          lines.push("#{name} => '#{bound_scope.raw_setting(full_name)}'") if bound_scope.has_setting?(full_name)
+        end
+
+        lines.push("ssl_keystore_type => '#{bound_scope.setting("var.input.kafka.ssl_keystore_type", "")}'")
+
+        ssl_keystore_location = bound_scope.setting("var.input.kafka.ssl_keystore_location","")
+        lines.push("ssl_keystore_location => '#{ssl_keystore_location}'") unless ssl_keystore_location.empty?
+
+        lines.push("sasl_mechanism => '#{bound_scope.setting("var.input.kafka.sasl_mechanism", "")}'")
+        lines.push("sasl_kerberos_service_name => '#{bound_scope.setting("var.input.kafka.sasl_kerberos_service_name", "")}'")
+
+        jaas_path = bound_scope.setting("var.input.kafka.jaas_path","")
+        lines.push("jaas_path => '#{jaas_path}'") unless jaas_path.empty?
+
+        kerberos_config = bound_scope.setting("var.input.kafka.kerberos_config","")
+        lines.push("kerberos_config => '#{kerberos_config}'") unless kerberos_config.empty?
+
+        lines.compact.join("\n    ")
+      end
+
+      def tcp_input_ssl_config(bound_scope)
+        ssl_enabled = bound_scope.setting("var.input.tcp.ssl_enable", false)
+        return "" unless ssl_enabled
+        lines = ["ssl_enable => true"]
+
+        verify_enabled = bound_scope.setting("var.input.tcp.ssl_verify", true)
+        lines.push("ssl_verify => #{verify_enabled}")
+
+        ssl_cert = bound_scope.setting("var.input.tcp.ssl_cert","")
+        lines.push("ssl_cert => '#{ssl_cert}'") unless ssl_cert.empty?
+
+        ssl_key = bound_scope.setting("var.input.tcp.ssl_key","")
+        lines.push("ssl_key => '#{ssl_key}'") unless ssl_key.empty?
+
+        lines.push("ssl_key_passphrase => '#{ bound_scope.setting("var.input.tcp.ssl_key_passphrase", "")}'")
+
+        certs_array_as_string = bound_scope.array_to_string(
+          bound_scope.get_setting(LogStash::Setting::SplittableStringArray.new("var.input.tcp.ssl_extra_chain_certs", String, []))
+        )
+        lines.push("ssl_extra_chain_certs => #{certs_array_as_string}")
+
+        lines.compact.join("\n    ")
+      end
+    end
+  end
+end
diff --git a/x-pack/spec/license_checker/license_reader_spec.rb b/x-pack/spec/license_checker/license_reader_spec.rb
index 5199ce2b75f..0112a7a8840 100644
--- a/x-pack/spec/license_checker/license_reader_spec.rb
+++ b/x-pack/spec/license_checker/license_reader_spec.rb
@@ -64,8 +64,8 @@
       before(:each) do
         expect(mock_client).to receive(:get).with('_xpack').and_raise(Puma::ConnectionError)
       end
-      it 'returns nil' do
-        expect(subject.fetch_xpack_info).to be_nil
+      it 'returns failed to fetch' do
+        expect(subject.fetch_xpack_info.failed?).to be_truthy
       end
     end
     context 'when client raises a 5XX' do
@@ -74,7 +74,7 @@
         expect(mock_client).to receive(:get).with('_xpack').and_raise(exception_500)
       end
       it 'returns nil' do
-        expect(subject.fetch_xpack_info).to be_nil
+        expect(subject.fetch_xpack_info.failed?).to be_truthy
       end
     end
     context 'when client raises a 404' do
diff --git a/x-pack/spec/modules/arcsight/arcsight_module_config_helper_spec.rb b/x-pack/spec/modules/arcsight/arcsight_module_config_helper_spec.rb
new file mode 100644
index 00000000000..854c25a7167
--- /dev/null
+++ b/x-pack/spec/modules/arcsight/arcsight_module_config_helper_spec.rb
@@ -0,0 +1,81 @@
+# encoding: utf-8
+
+# Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
+# or more contributor license agreements. Licensed under the Elastic License;
+# you may not use this file except in compliance with the Elastic License.
+
+# require "logstash/devutils/rspec/spec_helper"
+
+require 'x-pack/logstash_registry'
+require 'logstash-core'
+require 'logstash/settings'
+require 'logstash/util/modules_setting_array'
+require 'logstash/modules/scaffold'
+require 'arcsight_module_config_helper'
+
+describe LogStash::Arcsight::ConfigHelper do
+
+  let(:sample_yaml_folder) { ::File.join(File.dirname(__FILE__), "yaml") }
+  let(:settings) { settings = LogStash::SETTINGS.clone }
+  let(:module_name) { "arcsight" }
+  let(:module_hash) { Hash.new }
+  let(:scaffolding) { LogStash::Modules::Scaffold.new(module_name, ::File.join(File.dirname(__FILE__), '..', '..', '..', 'modules', 'arcsight', 'configuration')) }
+  let(:module_config) { LogStash::Modules::LogStashConfig.new(scaffolding, module_hash) }
+
+  describe "`tcp_input_ssl_config` method" do
+    context "when ssl_enabled is not enabled" do
+      it "returns an empty string" do
+        expect(described_class.tcp_input_ssl_config(module_config)).to be_empty
+      end
+    end
+
+    context "when ssl_enabled is enabled" do
+      before do
+        settings.from_yaml(sample_yaml_folder, "smart_connector_with_ssl.yml")
+      end
+      let(:module_hash) { settings.get("modules").find {|m| m["name"] == module_name} }
+
+      it "returns a tcp input ssl settings snippet" do
+        actual = described_class.tcp_input_ssl_config(module_config)
+        expect(actual).not_to be_empty
+        expect(actual).to include("ssl_enable => true")
+        expect(actual).to include("ssl_verify => false")
+        expect(actual).to include("ssl_cert => '/some/path/to/a/cert.p12'")
+        expect(actual).to include("ssl_key => '/some/path/to/a/cert.p12'")
+        expect(actual).to include("ssl_key_passphrase => 'foobar'")
+        expect(actual).to include("ssl_extra_chain_certs => ['/some/path/to/a/cert.p12']")
+      end
+    end
+  end
+
+  describe "`kafka_input_ssl_sasl_config` method" do
+    context "when security_protocol is not set" do
+      it "returns an empty string" do
+        expect(described_class.kafka_input_ssl_sasl_config(module_config)).to be_empty
+      end
+    end
+
+    context "when security_protocol is set" do
+      before do
+        settings.from_yaml(sample_yaml_folder, "event_broker_with_security.yml")
+      end
+      let(:module_hash) { settings.get("modules").find {|m| m["name"] == module_name} }
+      it "returns a kafka input security settings snippet" do
+        actual = described_class.kafka_input_ssl_sasl_config(module_config)
+        expect(actual).not_to be_empty
+        expect(actual).to include("security_protocol => 'SASL_SSL'")
+        expect(actual).to include("ssl_truststore_type => 'foo'")
+        expect(actual).to include("ssl_truststore_location => '/some/path/to/a/file'")
+        expect(actual).to include("ssl_truststore_password => '<password>'")
+        expect(actual).to include("ssl_keystore_password => '<password>'")
+        expect(actual).to include("ssl_key_password => '<password>'")
+        expect(actual).to include("ssl_keystore_type => 'bar'")
+        expect(actual).to include("ssl_keystore_location => '/some/path/to/a/file'")
+        expect(actual).to include("sasl_mechanism => 'GSSAPI'")
+        expect(actual).to include("sasl_kerberos_service_name => 'baz'")
+        expect(actual).to include("jaas_path => '/some/path/to/a/file'")
+        expect(actual).to include("kerberos_config => '/some/path/to/a/file'")
+      end
+    end
+  end
+end
diff --git a/x-pack/spec/modules/arcsight/yaml/event_broker_with_security.yml b/x-pack/spec/modules/arcsight/yaml/event_broker_with_security.yml
new file mode 100644
index 00000000000..6e1d7cc52c8
--- /dev/null
+++ b/x-pack/spec/modules/arcsight/yaml/event_broker_with_security.yml
@@ -0,0 +1,22 @@
+modules:
+  - name: arcsight
+    var.inputs: "kafka"
+    var.input.kafka.security_protocol: "SASL_SSL"
+    var.input.kafka.ssl_truststore_type: "foo"
+    var.input.kafka.ssl_truststore_location: "/some/path/to/a/file"
+    var.input.kafka.ssl_truststore_password: "foobar"
+    var.input.kafka.ssl_keystore_type: "bar"
+    var.input.kafka.ssl_keystore_location: "/some/path/to/a/file"
+    var.input.kafka.ssl_keystore_password: "barfoo"
+    var.input.kafka.ssl_key_password: "foobaz"
+    var.input.kafka.sasl_mechanism: "GSSAPI"
+    var.input.kafka.sasl_kerberos_service_name: "baz"
+    var.input.kafka.jaas_path: "/some/path/to/a/file"
+    var.input.kafka.kerberos_config: "/some/path/to/a/file"
+    var.input.kafka.bootstrap_servers: "kcluster.local:1234"
+    var.input.kafka.topics: ["foo", "bar", "baz"]
+
+# although this file is named eventbroker..., the arcsight config ERB
+# file duplicates the eventbroker based settings to be kafka based ones
+# plus, the arcsight_module_config_helper's module methods expect
+# kafka based settings
diff --git a/x-pack/spec/modules/arcsight/yaml/smart_connector_with_ssl.yml b/x-pack/spec/modules/arcsight/yaml/smart_connector_with_ssl.yml
new file mode 100644
index 00000000000..0f2777ea17a
--- /dev/null
+++ b/x-pack/spec/modules/arcsight/yaml/smart_connector_with_ssl.yml
@@ -0,0 +1,15 @@
+modules:
+  - name: arcsight
+    var.inputs: "tcp"
+    var.input.tcp.port: 5050
+    var.input.tcp.ssl_enable: true
+    var.input.tcp.ssl_cert: "/some/path/to/a/cert.p12"
+    var.input.tcp.ssl_extra_chain_certs: ["/some/path/to/a/cert.p12"]
+    var.input.tcp.ssl_key: "/some/path/to/a/cert.p12"
+    var.input.tcp.ssl_key_passphrase: "foobar"
+    var.input.tcp.ssl_verify: false
+
+# although this file is named smartconnector..., the arcsight config ERB
+# file duplicates the smartconnector based settings to be tcp based ones
+# plus, the arcsight_module_config_helper's module methods expect
+# tcp based settings
diff --git a/x-pack/spec/modules/module_license_checker_spec.rb b/x-pack/spec/modules/module_license_checker_spec.rb
index b1ffd19e60c..b22fa2ebdfa 100644
--- a/x-pack/spec/modules/module_license_checker_spec.rb
+++ b/x-pack/spec/modules/module_license_checker_spec.rb
@@ -4,6 +4,7 @@
 
 require "modules/module_license_checker"
 require "logstash/modules/settings_merger"
+require 'license_checker/x_pack_info'
 
 describe LogStash::LicenseChecker::ModuleLicenseChecker do
 
@@ -15,7 +16,8 @@
 
     before(:each) {
       expect(subject).to receive(:license_reader).and_return(mock_reader)
-      expect(mock_reader).to receive(:fetch_xpack_info).and_return(nil)
+      expect(mock_reader).to receive(:fetch_xpack_info).and_return(LogStash::LicenseChecker::XPackInfo.failed_to_fetch)
+
     }
     let(:mock_reader) {double("reader")}
 
diff --git a/x-pack/spec/monitoring/inputs/metrics_spec.rb b/x-pack/spec/monitoring/inputs/metrics_spec.rb
index 1e9b988e9eb..0cda3ea423f 100644
--- a/x-pack/spec/monitoring/inputs/metrics_spec.rb
+++ b/x-pack/spec/monitoring/inputs/metrics_spec.rb
@@ -9,7 +9,6 @@
 require 'spec_helper'
 require "json"
 require "json-schema"
-require 'license_checker/x_pack_info'
 require 'monitoring/monitoring'
 
 describe LogStash::Inputs::Metrics do
@@ -121,15 +120,8 @@ def main_pipeline
       agent.shutdown
     end
 
-    let(:license_state_ok) do
-      {:state => :ok, :log_level => :info, :log_message => 'Monitoring License OK'}
-    end
-
     context 'after the pipeline is setup' do
       before do
-        allow(subject).to receive(:setup_license_checker)
-        allow(subject).to receive(:es_options_from_settings_or_modules).and_return(es_options)
-        allow(subject).to receive(:get_current_license_state).and_return(license_state_ok)
         allow(subject).to receive(:exec_timer_task)
         allow(subject).to receive(:sleep_till_stop)
         setup_pipeline
@@ -142,9 +134,6 @@ def main_pipeline
     describe "#update" do
       before :each do
         allow(subject).to receive(:fetch_global_stats).and_return({"uuid" => "00001" })
-        allow(subject).to receive(:setup_license_checker)
-        allow(subject).to receive(:es_options_from_settings_or_modules).and_return(es_options)
-        allow(subject).to receive(:get_current_license_state).and_return(license_state_ok)
         allow(subject).to receive(:exec_timer_task)
         allow(subject).to receive(:sleep_till_stop)
         setup_pipeline
@@ -175,127 +164,6 @@ def main_pipeline
         end
       end
     end
-
-    context 'license testing' do
-      let(:elasticsearch_url) { ["https://localhost:9898"] }
-      let(:elasticsearch_username) { "elastictest" }
-      let(:elasticsearch_password) { "testchangeme" }
-      let(:mock_license_client) { double("es_client")}
-      let(:license_subject) {   subject { described_class.new(options) }}
-      let(:license_reader) { LogStash::LicenseChecker::LicenseReader.new(system_settings, 'monitoring', es_options)}
-      let(:extension) {  LogStash::MonitoringExtension.new }
-      let(:system_settings) { LogStash::Runner::SYSTEM_SETTINGS.clone }
-      let(:license_status) { 'active'}
-      let(:license_type) { 'trial' }
-      let(:license_expiry_date) { Time.now + (60 * 60 * 24)}
-      let(:license_expiry_in_millis) { license_expiry_date.to_i * 1000 }
-
-      let(:xpack_response) {
-        LogStash::Json.load("{
-          \"license\": {
-            \"status\": \"#{license_status}\",
-            \"uid\": \"9a48c67c-ce2c-4169-97bf-37d324b8ab80\",
-            \"type\": \"#{license_type}\",
-            \"expiry_date_in_millis\": #{license_expiry_in_millis}
-          }
-        }")
-      }
-
-
-      let(:no_xpack_response) {
-        LogStash::Json.load("{
-          \"error\": {
-            \"root_cause\": [
-              {
-                \"type\": \"index_not_found_exception\",
-                \"reason\": \"no such index\",
-                \"resource.type\": \"index_or_alias\",
-                \"resource.id\": \"_xpack\",
-                \"index_uuid\": \"_na_\",
-                \"index\": \"_xpack\"
-              }],
-            \"type\": \"index_not_found_exception\",
-            \"reason\": \"no such index\",
-            \"resource.type\": \"index_or_alias\",
-            \"resource.id\": \"_xpack\",
-            \"index_uuid\": \"_na_\",
-            \"index\": \"_xpack\"
-          },
-          \"status\": 404
-        }")
-      }
-
-      let(:settings) do
-        {
-            "xpack.monitoring.enabled" => true,
-            "xpack.monitoring.elasticsearch.url" => elasticsearch_url,
-            "xpack.monitoring.elasticsearch.username" => elasticsearch_username,
-            "xpack.monitoring.elasticsearch.password" => elasticsearch_password,
-        }
-      end
-
-      before :each do
-        extension.additionals_settings(system_settings)
-        apply_settings(settings, system_settings)
-        allow(subject).to receive(:fetch_global_stats).and_return({"uuid" => "00001" })
-        allow(subject).to receive(:es_options_from_settings_or_modules).and_return(es_options)
-        allow(subject).to receive(:exec_timer_task)
-        allow(subject).to receive(:sleep_till_stop)
-
-        allow(subject).to receive(:license_reader).and_return(license_reader)
-        allow(license_reader).to receive(:build_client).and_return(mock_license_client)
-      end
-
-      describe 'with licensing' do
-        context 'when xpack has not been installed' do
-
-          before :each do
-            expect(mock_license_client).to receive(:get).with('_xpack').and_return(no_xpack_response)
-            setup_pipeline
-            subject.update(collector.snapshot_metric)
-          end
-
-          it_behaves_like 'events are not added to the queue'
-
-        end
-
-        context 'when the license has expired' do
-          let(:license_status) { 'expired'}
-          let(:license_expiry_date) { Time.now - (60 * 60 * 24)}
-
-          before :each do
-            expect(mock_license_client).to receive(:get).with('_xpack').and_return(xpack_response)
-            setup_pipeline
-            subject.update(collector.snapshot_metric)
-          end
-
-          it_behaves_like 'events are added to the queue'
-        end
-
-        context 'when the license server is not available' do
-          let(:mock_license_client) { double('license_client')}
-          before :each do
-            expect(mock_license_client).to receive(:get).and_raise("An error is here")
-            setup_pipeline
-            subject.update(collector.snapshot_metric)
-          end
-
-          it_behaves_like 'events are not added to the queue'
-        end
-
-        %w(basic standard trial standard gold platinum).sample(1).each  do |license_type|
-          context "With a valid #{license_type} license" do
-            let(:license_type) { license_type }
-            before :each do
-              expect(mock_license_client).to receive(:get).with('_xpack').and_return(xpack_response)
-              setup_pipeline
-              subject.update(collector.snapshot_metric)
-            end
-            it_behaves_like 'events are added to the queue'
-          end
-        end
-      end
-    end
   end
 
   context "unit tests" do
@@ -311,7 +179,6 @@ def main_pipeline
 
       describe "system pipelines" do
         before(:each) do
-          allow(subject).to receive(:valid_license?).and_return(true)
           allow(pipeline).to receive(:system?).and_return(true)
           allow(subject).to receive(:emit_event)
           subject.update_pipeline_state(pipeline)
@@ -324,7 +191,6 @@ def main_pipeline
 
       describe "normal pipelines" do
         before(:each) do
-          allow(subject).to receive(:valid_license?).and_return(true)
           allow(pipeline).to receive(:system?).and_return(false)
           allow(subject).to receive(:state_event_for).with(pipeline).and_return(state_event)
           allow(subject).to receive(:emit_event)
diff --git a/x-pack/spec/monitoring/internal_pipeline_source_spec.rb b/x-pack/spec/monitoring/internal_pipeline_source_spec.rb
new file mode 100644
index 00000000000..db61e8a484c
--- /dev/null
+++ b/x-pack/spec/monitoring/internal_pipeline_source_spec.rb
@@ -0,0 +1,122 @@
+# Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
+# or more contributor license agreements. Licensed under the Elastic License;
+# you may not use this file except in compliance with the Elastic License.
+
+require "logstash-core"
+require "logstash/agent"
+require "logstash/agent"
+require "monitoring/inputs/metrics"
+require "logstash/config/pipeline_config"
+require "logstash/config/source/local"
+require 'license_checker/x_pack_info'
+require "rspec/wait"
+require 'spec_helper'
+require "json"
+require "json-schema"
+require 'license_checker/x_pack_info'
+require 'monitoring/monitoring'
+
+
+describe LogStash::Monitoring::InternalPipelineSource do
+  context 'license testing' do
+    let(:xpack_monitoring_interval) { 1 }
+    let(:options) { { "collection_interval" => xpack_monitoring_interval,
+                        "collection_timeout_interval" => 600 } }
+
+    subject { described_class.new(pipeline_config, mock_agent) }
+    let(:mock_agent) { double("agent")}
+    let(:mock_license_client) { double("es_client")}
+    let(:license_reader) { LogStash::LicenseChecker::LicenseReader.new(system_settings, 'monitoring', es_options)}
+    let(:system_settings) { LogStash::Runner::SYSTEM_SETTINGS.clone }
+    let(:license_status) { 'active'}
+    let(:license_type) { 'trial' }
+    let(:license_expiry_date) { Time.now + (60 * 60 * 24)}
+    let(:source) { LogStash::Config::Source::Local }
+    let(:pipeline_id) { :main }
+    let(:ordered_config_parts) do
+      [
+        org.logstash.common.SourceWithMetadata.new("file", "/tmp/1", 0, 0, "input { generator1 }"),
+        org.logstash.common.SourceWithMetadata.new("file", "/tmp/2", 0, 0,  "input { generator2 }"),
+        org.logstash.common.SourceWithMetadata.new("file", "/tmp/3", 0, 0, "input { generator3 }"),
+        org.logstash.common.SourceWithMetadata.new("file", "/tmp/4", 0, 0, "input { generator4 }"),
+        org.logstash.common.SourceWithMetadata.new("file", "/tmp/5", 0, 0, "input { generator5 }"),
+        org.logstash.common.SourceWithMetadata.new("file", "/tmp/6", 0, 0, "input { generator6 }"),
+        org.logstash.common.SourceWithMetadata.new("string", "config_string", 0, 0, "input { generator1 }"),
+      ]
+    end
+
+    let(:unordered_config_parts) { ordered_config_parts.shuffle }
+
+    let(:pipeline_config) { LogStash::Config::PipelineConfig.new(source, pipeline_id, unordered_config_parts, system_settings) }
+
+    let(:es_options) do
+      {
+          'url' => elasticsearch_url,
+          'user' => elasticsearch_username,
+          'password' => elasticsearch_password
+      }
+    end
+    let(:elasticsearch_url) { ["https://localhost:9898"] }
+    let(:elasticsearch_username) { "elastictest" }
+    let(:elasticsearch_password) { "testchangeme" }
+
+    let(:settings) do
+      {
+        "xpack.monitoring.enabled" => true,
+        "xpack.monitoring.elasticsearch.url" => elasticsearch_url,
+        "xpack.monitoring.elasticsearch.username" => elasticsearch_username,
+        "xpack.monitoring.elasticsearch.password" => elasticsearch_password,
+      }
+    end
+
+    before :each do
+      allow(subject).to receive(:es_options_from_settings_or_modules).and_return(es_options)
+      allow(subject).to receive(:license_reader).and_return(license_reader)
+      allow(license_reader).to receive(:build_client).and_return(mock_license_client)
+    end
+
+    describe 'with licensing' do
+      context 'when xpack has not been installed on es 6' do
+        let(:xpack_info) { LogStash::LicenseChecker::XPackInfo.xpack_not_installed }
+        it "does not start the pipeline" do
+          expect(subject).to_not receive(:enable_monitoring)
+          subject.update_license_state(xpack_info)
+        end
+      end
+      context 'when the license has expired' do
+        let(:license) do
+          { "status" => "inactive", "type" => license_type }
+        end
+        let(:xpack_info) { LogStash::LicenseChecker::XPackInfo.new(license, nil) }
+        it "still starts the pipeline" do
+          expect(subject).to receive(:enable_monitoring)
+          subject.update_license_state(xpack_info)
+        end
+      end
+      context 'when the license server is not available' do
+        let(:xpack_info) { LogStash::LicenseChecker::XPackInfo.new(nil, nil, nil, true) }
+        it "does not start the pipeline" do
+          expect(subject).to_not receive(:enable_monitoring)
+          subject.update_license_state(xpack_info)
+        end
+      end
+
+      %w(basic standard trial gold platinum).each  do |license_type|
+        context "With a valid #{license_type} license" do
+          let(:license_type) { license_type }
+          let(:license) do
+            { "status" => "active", "type" => license_type }
+          end
+          let(:features) do
+            { "monitoring" => { "enabled" => true } }
+          end
+          let(:xpack_info) { LogStash::LicenseChecker::XPackInfo.new(license, features) }
+          it "starts the pipeline" do
+            expect(subject).to receive(:enable_monitoring)
+            subject.update_license_state(xpack_info)
+          end
+        end
+      end
+    end
+  end
+end
