diff --git a/CONTRIBUTORS b/CONTRIBUTORS
index e14761e28fe..c045c8f1c25 100644
--- a/CONTRIBUTORS
+++ b/CONTRIBUTORS
@@ -74,6 +74,7 @@ Contributors:
 * Andrea Forni (andreaforni)
 * Leandro Moreira (leandromoreira)
 * Hao Chen (haoch)
+* Ryan O'Keeffe (danielredoak)
 
 Note: If you've sent me patches, bug reports, or otherwise contributed to
 logstash, and you aren't on the list above and want to be, please let me know
diff --git a/lib/logstash/inputs/s3.rb b/lib/logstash/inputs/s3.rb
index 403aaebd349..8ab8b481bda 100644
--- a/lib/logstash/inputs/s3.rb
+++ b/lib/logstash/inputs/s3.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/inputs/base"
 require "logstash/namespace"
+require "logstash/plugin_mixins/aws_config"
 
 require "time"
 require "tmpdir"
@@ -10,6 +11,7 @@
 # Each line from each file generates an event.
 # Files ending in '.gz' are handled as gzip'ed files.
 class LogStash::Inputs::S3 < LogStash::Inputs::Base
+  include LogStash::PluginMixins::AwsConfig
   config_name "s3"
   milestone 1
 
@@ -17,26 +19,20 @@ class LogStash::Inputs::S3 < LogStash::Inputs::Base
   # support and readline usage). Support gzip through a gzip codec! ;)
   default :codec, "plain"
 
-  # The credentials of the AWS account used to access the bucket.
+  # DEPRECATED: The credentials of the AWS account used to access the bucket.
   # Credentials can be specified:
   # - As an ["id","secret"] array
   # - As a path to a file containing AWS_ACCESS_KEY_ID=... and AWS_SECRET_ACCESS_KEY=...
   # - In the environment, if not set (using variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY)
-  config :credentials, :validate => :array, :default => []
+  config :credentials, :validate => :array, :default => [], :deprecated => "This only exists to be backwards compatible. This plugin now uses the AwsConfig from PluginMixins"
 
   # The name of the S3 bucket.
   config :bucket, :validate => :string, :required => true
 
-  # The AWS region for your bucket.
-  config :region, :validate => ["us-east-1", "us-west-1", "us-west-2",
-                                "eu-west-1", "ap-southeast-1", "ap-southeast-2",
-                                "ap-northeast-1", "sa-east-1", "us-gov-west-1"],
-                                :deprecated => "'region' has been deprecated in favor of 'region_endpoint'"
-
   # The AWS region for your bucket.
   config :region_endpoint, :validate => ["us-east-1", "us-west-1", "us-west-2",
                                 "eu-west-1", "ap-southeast-1", "ap-southeast-2",
-                                "ap-northeast-1", "sa-east-1", "us-gov-west-1"], :default => "us-east-1"
+                                "ap-northeast-1", "sa-east-1", "us-gov-west-1"], :default => "us-east-1", :deprecated => "This only exists to be backwards compatible. This plugin now uses the AwsConfig from PluginMixins"
 
   # If specified, the prefix the filenames in the bucket must match (not a regexp)
   config :prefix, :validate => :string, :default => nil
@@ -49,6 +45,11 @@ class LogStash::Inputs::S3 < LogStash::Inputs::Base
   # Name of a S3 bucket to backup processed files to.
   config :backup_to_bucket, :validate => :string, :default => nil
 
+  # Append a prefix to the key (full path including file name in s3) after processing.
+  # If backing up to another (or the same) bucket, this effectively lets you
+  # choose a new 'folder' to place the files in
+  config :backup_add_prefix, :validate => :string, :default => nil
+
   # Path of a local directory to backup processed files to.
   config :backup_to_dir, :validate => :string, :default => nil
 
@@ -59,19 +60,20 @@ class LogStash::Inputs::S3 < LogStash::Inputs::Base
   # Value is in seconds.
   config :interval, :validate => :number, :default => 60
 
+  # Ruby style regexp of keys to exclude from the bucket
+  config :exclude_pattern, :validate => :string, :default => nil
+
   public
   def register
     require "digest/md5"
     require "aws-sdk"
 
-    @region_endpoint = @region if @region && !@region.empty?
+    @region = @region_endpoint if @region_endpoint && !@region_endpoint.empty? && !@region
 
-    @logger.info("Registering s3 input", :bucket => @bucket, :region_endpoint => @region_endpoint)
+    @logger.info("Registering s3 input", :bucket => @bucket, :region => @region)
 
-    if @credentials.length == 0
-      @access_key_id = ENV['AWS_ACCESS_KEY_ID']
-      @secret_access_key = ENV['AWS_SECRET_ACCESS_KEY']
-    elsif @credentials.length == 1
+    # Deprecated
+    if @credentials.length == 1
       File.open(@credentials[0]) { |f| f.each do |line|
         unless (/^\#/.match(line))
           if(/\s*=\s*/.match(line))
@@ -90,30 +92,24 @@ def register
     elsif @credentials.length == 2
       @access_key_id = @credentials[0]
       @secret_access_key = @credentials[1]
-    else
-      raise ArgumentError.new('Credentials must be of the form "/path/to/file" or ["id", "secret"]')
-    end
-
-    if @access_key_id.nil? or @secret_access_key.nil?
-      raise ArgumentError.new('Missing AWS credentials')
-    end
-
-    if @bucket.nil?
-      raise ArgumentError.new('Missing AWS bucket')
     end
 
     if @sincedb_path.nil?
       if ENV['HOME'].nil?
-        raise ArgumentError.new('No HOME or sincedb_path set')
+        raise ConfigurationError.new('No HOME or sincedb_path set')
       end
       @sincedb_path = File.join(ENV["HOME"], ".sincedb_" + Digest::MD5.hexdigest("#{@bucket}+#{@prefix}"))
     end
 
-    s3 = AWS::S3.new(
+    if @credentials
+      s3 = AWS::S3.new(
       :access_key_id => @access_key_id,
       :secret_access_key => @secret_access_key,
-      :region => @region_endpoint
-    )
+      :region => @region
+      )
+    else
+      s3 = AWS::S3.new(aws_options_hash)
+    end
 
     @s3bucket = s3.buckets[@bucket]
 
@@ -156,7 +152,7 @@ def process_new(queue, since=nil)
 
   end # def process_new
 
-  private
+  public
   def list_new(since=nil)
 
     if since.nil?
@@ -165,18 +161,33 @@ def list_new(since=nil)
 
     objects = {}
     @s3bucket.objects.with_prefix(@prefix).each do |log|
-      if log.last_modified > since
-        objects[log.key] = log.last_modified
+      @logger.debug("Found key: #{log.key}")
+
+      unless ignore_filename?(log.key)
+        if log.last_modified > since
+          objects[log.key] = log.last_modified
+          @logger.debug("Adding to objects[]: #{log.key}")
+        end
       end
     end
-
     return sorted_objects = objects.keys.sort {|a,b| objects[a] <=> objects[b]}
-
   end # def list_new
 
   private
-  def process_log(queue, key)
+  def ignore_filename?(filename)
+    if (@backup_add_prefix && @backup_to_bucket == @bucket && filename =~ /^#{backup_add_prefix}/)
+      return true
+    elsif @exclude_pattern.nil?
+      return false
+    elsif filename =~ Regexp.new(@exclude_pattern)
+      return true
+    else
+      return false
+    end
+  end
 
+  private
+  def process_log(queue, key)
     object = @s3bucket.objects[key]
     tmp = Dir.mktmpdir("logstash-")
     begin
@@ -186,29 +197,48 @@ def process_log(queue, key)
           s3file.write(chunk)
         end
       end
+
       process_local_log(queue, filename)
-      unless @backup_to_bucket.nil?
-        backup_object = @backup_bucket.objects[key]
-        backup_object.write(Pathname.new(filename))
-      end
-      unless @backup_to_dir.nil?
-        FileUtils.cp(filename, @backup_to_dir)
-      end
-      if @delete
-        object.delete()
-      end
+      process_backup_to_bucket(object, key)
+      process_backup_to_dir(filename)
+      delete_file_from_bucket()
     end
     FileUtils.remove_entry_secure(tmp, force=true)
 
   end # def process_log
 
+  public
+  def process_backup_to_bucket(object, key)
+    unless @backup_to_bucket.nil?
+      backup_key = "#{@backup_add_prefix}#{key}"
+      if @delete
+        object.move_to(backup_key, :bucket => @backup_bucket)
+      else
+        object.copy_to(backup_key, :bucket => @backup_bucket)
+      end
+    end
+  end
+
+  public
+  def process_backup_to_dir(filename)
+    unless @backup_to_dir.nil?
+      FileUtils.cp(filename, @backup_to_dir)
+    end
+  end
+
+  def delete_file_from_bucket
+    if @delete and @backup_to_bucket.nil?
+      object.delete()
+    end
+  end
+
   private
   def process_local_log(queue, filename)
-
     metadata = {
       :version => nil,
       :format => nil,
     }
+
     File.open(filename) do |file|
       if filename.end_with?('.gz')
         gz = Zlib::GzipReader.new(file)
@@ -274,5 +304,4 @@ def sincedb_write(since=nil)
     File.open(@sincedb_path, 'w') { |file| file.write(since.to_s) }
 
   end # def sincedb_write
-
 end # class LogStash::Inputs::S3
diff --git a/spec/inputs/s3_spec.rb b/spec/inputs/s3_spec.rb
new file mode 100644
index 00000000000..c0588625baa
--- /dev/null
+++ b/spec/inputs/s3_spec.rb
@@ -0,0 +1,144 @@
+require "spec_helper"
+require "logstash/inputs/s3"
+require "logstash/errors"
+require "aws-sdk"
+require "tempfile"
+
+describe LogStash::Inputs::S3 do
+  before { AWS.stub! }
+  let(:day) { 3600 * 24 }
+  let(:settings) {
+    {
+      "access_key_id" => "1234",
+      "secret_access_key" => "secret",
+      "bucket" => "logstash-test"
+    }
+  }
+
+  describe "#list_new" do
+    let(:present_object) { double(:key => 'this-should-be-present', :last_modified => Time.now) }
+    let(:objects_list) {
+      [
+        double(:key => 'exclude-this-file-1', :last_modified => Time.now - 2 * day),
+        double(:key => 'exclude/logstash', :last_modified => Time.now - 2 * day),
+        present_object
+      ]
+    }
+
+    it 'should allow user to exclude files from the s3 bucket' do
+      AWS::S3::ObjectCollection.any_instance.stub(:with_prefix).with(nil) { objects_list }
+
+      config = LogStash::Inputs::S3.new(settings.merge({ "exclude_pattern" => "^exclude" }))
+      config.register
+      config.list_new.should == [present_object.key]
+    end
+
+    it 'should support not providing a exclude pattern' do
+      AWS::S3::ObjectCollection.any_instance.stub(:with_prefix).with(nil) { objects_list }
+
+      config = LogStash::Inputs::S3.new(settings)
+      config.register
+      config.list_new.should == objects_list.map(&:key)
+    end
+
+    context "If the bucket is the same as the backup bucket" do
+      it 'should ignore files from the bucket if they match the backup prefix' do
+        objects_list = [
+          double(:key => 'mybackup-log-1', :last_modified => Time.now),
+          present_object
+        ]
+
+        AWS::S3::ObjectCollection.any_instance.stub(:with_prefix).with(nil) { objects_list }
+
+        config = LogStash::Inputs::S3.new(settings.merge({ 'backup_add_prefix' => 'mybackup',
+                                                           'backup_to_bucket' => settings['bucket']}))
+        config.register
+        config.list_new.should == [present_object.key]
+      end
+    end
+
+    it 'should ignore files older than X' do
+      AWS::S3::ObjectCollection.any_instance.stub(:with_prefix).with(nil) { objects_list }
+
+      config = LogStash::Inputs::S3.new(settings.merge({ 'backup_add_prefix' => 'exclude-this-file'}))
+      config.register
+      config.list_new(Time.now - day).should == [present_object.key]
+    end
+
+    it 'should sort return object sorted by last_modification date with older first' do
+      objects = [
+        double(:key => 'YESTERDAY', :last_modified => Time.now - day),
+        double(:key => 'TODAY', :last_modified => Time.now),
+        double(:key => 'TWO_DAYS_AGO', :last_modified => Time.now - 2 * day)
+      ]
+
+      AWS::S3::ObjectCollection.any_instance.stub(:with_prefix).with(nil) { objects }
+
+
+      config = LogStash::Inputs::S3.new(settings)
+      config.register
+      config.list_new.should == ['TWO_DAYS_AGO', 'YESTERDAY', 'TODAY']
+    end
+
+    describe "when doing backup on the s3" do
+      it 'should copy to another s3 bucket when keeping the original file' do
+        config = LogStash::Inputs::S3.new(settings.merge({ "backup_to_bucket" => "mybackup"}))
+        config.register
+
+        s3object = double()
+        s3object.stub(:copy_to).with('test-file', :bucket => an_instance_of(AWS::S3::Bucket))
+
+        config.process_backup_to_bucket(s3object, 'test-file')
+      end
+
+      it 'should move to another s3 bucket when deleting the original file' do
+        config = LogStash::Inputs::S3.new(settings.merge({ "backup_to_bucket" => "mybackup", "delete" => true }))
+        config.register
+
+        s3object = double()
+        s3object.stub(:move_to).with('test-file', :bucket => an_instance_of(AWS::S3::Bucket))
+
+        config.process_backup_to_bucket(s3object, 'test-file')
+      end
+
+      it 'should add the specified prefix to the backup file' do
+        config = LogStash::Inputs::S3.new(settings.merge({ "backup_to_bucket" => "mybackup",
+                                                           "backup_add_prefix" => 'backup-' }))
+        config.register
+
+        s3object = double()
+        s3object.stub(:copy_to).with('backup-test-file', :bucket => an_instance_of(AWS::S3::Bucket))
+
+        config.process_backup_to_bucket(s3object, 'test-file')
+      end
+    end
+
+    it 'should support doing local backup of files' do
+      backup_dir = Dir.mktmpdir
+
+      source_file = Tempfile.new('tmp-logstash-file')
+      backup_file = File.join(backup_dir.to_s, Pathname.new(source_file.path).basename.to_s)
+
+      begin
+        config = LogStash::Inputs::S3.new(settings.merge({ "backup_to_dir" => backup_dir }))
+
+        config.process_backup_to_dir(source_file)
+
+        File.exists?(backup_file).should be_true
+      ensure
+        FileUtils.remove_entry_secure(backup_file, :force => true)
+        FileUtils.remove_entry_secure(backup_dir)
+      end
+    end
+
+    it 'should accepts a list of credentials for the aws-sdk, this is deprecated' do
+      old_credentials_settings = {
+        "credentials" => ['1234', 'secret'],
+        "bucket" => "logstash-test"
+      }
+
+      config = LogStash::Inputs::S3.new(settings.merge({ "backup_to_dir" => "/tmp/mybackup" }))
+      config.register
+    end
+  end
+end
diff --git a/tools/Gemfile.jruby-1.9.lock b/tools/Gemfile.jruby-1.9.lock
index 172b1472108..eae9a236261 100644
--- a/tools/Gemfile.jruby-1.9.lock
+++ b/tools/Gemfile.jruby-1.9.lock
@@ -1,5 +1,5 @@
 PATH
-  remote: /Users/colin/dev/src/elasticsearch/logstash
+  remote: /Users/ph/es/logstash
   specs:
     logstash (1.5.0.dev-java)
       addressable
@@ -67,18 +67,18 @@ GEM
       tzinfo (~> 1.1)
     addressable (2.3.6)
     atomic (1.1.16-java)
-    avl_tree (1.1.3)
+    avl_tree (1.2.0)
     awesome_print (1.2.0)
-    aws-sdk (1.54.0)
-      aws-sdk-v1 (= 1.54.0)
-    aws-sdk-v1 (1.54.0)
+    aws-sdk (1.55.0)
+      aws-sdk-v1 (= 1.55.0)
+    aws-sdk-v1 (1.55.0)
       json (~> 1.4)
       nokogiri (>= 1.4.4)
     axiom-types (0.1.1)
       descendants_tracker (~> 0.0.4)
       ice_nine (~> 0.11.0)
       thread_safe (~> 0.3, >= 0.3.1)
-    backports (3.6.1)
+    backports (3.6.3)
     beefcake (0.3.7)
     bindata (2.1.0)
     buftok (0.1)
@@ -114,7 +114,7 @@ GEM
     extlib (0.9.16)
     faraday (0.9.0)
       multipart-post (>= 1.2, < 3)
-    ffi (1.9.5-java)
+    ffi (1.9.6)
     ffi-rzmq (1.0.0)
       ffi
     filewatch (0.5.1)
@@ -156,9 +156,9 @@ GEM
       virtus (~> 1.0)
     metaclass (0.0.4)
     method_source (0.8.2)
-    metriks (0.9.9.6)
+    metriks (0.9.9.7)
       atomic (~> 1.0)
-      avl_tree (~> 1.1.2)
+      avl_tree (~> 1.2.0)
       hitimes (~> 1.1)
     mime-types (1.25.1)
     minitest (5.4.2)
@@ -233,97 +233,6 @@ GEM
     tins (1.3.3)
     treetop (1.4.15)
       polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
       polyglot (>= 0.3.1)
     twitter (5.0.0.rc.1)
       buftok (~> 0.1.0)
