diff --git a/docs/asciidoc/static/command-line-flags.asciidoc b/docs/asciidoc/static/command-line-flags.asciidoc
index 797fd67c52d..4c7f919ee63 100644
--- a/docs/asciidoc/static/command-line-flags.asciidoc
+++ b/docs/asciidoc/static/command-line-flags.asciidoc
@@ -3,7 +3,7 @@
 [float]
 === Agent
 
-The Logstash agent has the following flags (also try using the '--help' flag)
+The Logstash agent has the following flags. (You can use the '--help' flag to display this information.)
 
 [source,js]
 ----------------------------------
diff --git a/docs/asciidoc/static/configuration.asciidoc b/docs/asciidoc/static/configuration.asciidoc
index 8c63b150006..a9a99d1b793 100644
--- a/docs/asciidoc/static/configuration.asciidoc
+++ b/docs/asciidoc/static/configuration.asciidoc
@@ -1,12 +1,13 @@
-== Logstash Config Language
-[float]
-=== Basic Layout
+== Logstash Configuration Language
 
-The Logstash config language aims to be simple.
+Logstash config files enable you to specify which plugins you want to use and settings for each plugin.
+You can reference event fields in a configuration and use conditionals to process events when they meet certain
+criteria.
 
-There are 3 main sections: inputs, filters, outputs. Each section has configurations for each plugin available in that section.
+[float]
+=== Structure of a Config File
 
-Example:
+A Logstash config file has a separate section for each type of plugin you want to add to the event processing pipeline. For example:
 
 [source,js]
 ----------------------------------
@@ -24,30 +25,17 @@ output {
   ...
 }
 ----------------------------------
-[float]
-=== Filters and Ordering
 
-For a given event, filters are applied in the order of appearance in the configuration file.
-[float]
-=== Comments
+Each section contains the configuration options for one or more plugins. If you specify 
+multiple filters, they are applied in the order of their appearance in the configuration file.
 
-Comments are the same as in perl, ruby, and python. A comment starts with a '#' character, and does not need to be at the beginning of a line. For example:
 
-[source,js]
-----------------------------------
-# this is a comment
-
-input { # comments can appear at the end of a line, too
-  # ...
-}
-----------------------------------
 [float]
 [[plugin_configuration]]
 === Plugins
 
-The input, filter and output sections all let you configure plugins. Plugin
-configuration consists of the plugin name followed by a block of settings for
-that plugin. For example, how about two file inputs:
+The configuration of a plugin consists of the plugin name followed 
+by a block of settings for that plugin. For example, this input section configures two file inputs:
 
 [source,js]
 ----------------------------------
@@ -64,74 +52,96 @@ input {
 }
 ----------------------------------
 
-The above configures two file separate inputs. Both set two configuration settings each: 'path' and 'type'. Each plugin has different settings for configuring it; seek the documentation for your plugin to learn what settings are available and what they mean. For example, the [file input][fileinput] documentation will explain the meanings of the path and type settings.
+In this example, two settings are configured for each of the file inputs: 'path' and 'type'. 
+
+The settings you can configure vary according to the plugin type. For information about each plugin, see <<input-plugins,Input Plugins>>, <<output-plugins, Output Plugins>>, <<filter-plugins,Filter Plugins>>, and <<codec-plugins,Codec Plugins>>.
 
 [float]
 === Value Types
 
-The documentation for a plugin may enforce a configuration field having a
-certain type.  Examples include boolean, string, array, number, hash,
-etc.
-[[boolean]]
+A plugin can require that the value for a setting be a
+certain type, such as boolean or hash. The following value
+types are supported.
+
+[[array]]
 [float]
-==== Boolean
+==== Array
 
-A boolean must be either `true` or `false`. Note the lack of quotes around `true` and `false`.
+An array can be a single string value or multiple values. If you specify the same
+setting multiple times, it appends to the array.
 
-Examples:
+Example:
 
 [source,js]
 ----------------------------------
-  ssl_enable => true
+  path => [ "/var/log/messages", "/var/log/*.log" ]
+  path => "/data/mysql/mysql.log"
 ----------------------------------
-[[string]]
+
+This example configures `path` to be an array that contains an element for each of the three strings.
+
+
+[[boolean]]
 [float]
-==== String
+==== Boolean
 
-A string must be a single value.
+A boolean must be either `true` or `false`. Note that the `true` and `false` keywords
+are not enclosed in quotes.
 
 Example:
 
 [source,js]
 ----------------------------------
-  name => "Hello world"
+  ssl_enable => true
 ----------------------------------
 
-You should use quotes around string values.
-[[number]]
+[[bytes]]
 [float]
-==== Number
+==== Bytes
 
-Numbers must be valid numerics (floating point or integer are OK).
+A bytes field is a string field that represents a valid unit of bytes. It is a 
+convenient way to declare specific sizes in your plugin options. Both SI (k M G T P E Z Y)
+and Binary (Ki Mi Gi Ti Pi Ei Zi Yi) units are supported. Binary units are in 
+base-1024 and SI units are in base-1000. This field is case-insensitive 
+and accepts space between the value and the unit. If no unit is specified, the integer string
+represents the number of bytes.
 
-Example:
+Examples:
 
 [source,js]
 ----------------------------------
-  port => 33
+  my_bytes => "1113"   # 1113 bytes
+  my_bytes => "10MiB"  # 10485760 bytes
+  my_bytes => "100kib" # 102400 bytes
+  my_bytes => "180 mb" # 180000000 bytes
 ----------------------------------
-[[array]]
+
+[[codec]]
 [float]
-==== Array
+==== Codec
 
-An array can be a single string value or multiple. If you specify the same
-field multiple times, it appends to the array.
+A codec is the name of Logstash codec used to represent the data. Codecs can be
+used in both inputs and outputs.
 
-Examples:
+Input codecs provide a convenient way to decode your data before it enters the input.
+Output codecs provide a convenient way to encode your data before it leaves the output.
+Using an input or output codec eliminates the need for a separate filter in your Logstash pipeline.
+
+Example:
 
 [source,js]
 ----------------------------------
-  path => [ "/var/log/messages", "/var/log/*.log" ]
-  path => "/data/mysql/mysql.log"
+  codec => "json"
 ----------------------------------
 
-The above makes 'path' a 3-element array including all 3 strings.
 [[hash]]
 [float]
 ==== Hash
 
-A hash is basically the same syntax as Ruby hashes.
-The key and value are simply pairs, such as:
+A hash is a collection of key value pairs specified in the format `"field1" => "value1"`. 
+Note that mutliple key value entries are separated by spaces rather than commas. 
+
+Example:
 
 [source,js]
 ----------------------------------
@@ -142,12 +152,24 @@ match => {
 }
 ----------------------------------
 
+[[number]]
+[float]
+==== Number
+
+Numbers must be valid numeric values (floating point or integer).
+
+Example:
+
+[source,js]
+----------------------------------
+  port => 33
+----------------------------------
+
 [[password]]
 [float]
 ==== Password
 
-A password field is basically a String field with a single value, but it will
-not be logged or printed
+A password is a string with a single value that is not logged or printed.
 
 Example:
 
@@ -156,12 +178,11 @@ Example:
   my_password => "password"
 ----------------------------------
 
-
 [[path]]
 [float]
 ==== Path
 
-A path field is a String field which represents a valid operating system path
+A path is a string that represents a valid operating system path.
 
 Example:
 
@@ -170,57 +191,50 @@ Example:
   my_path => "/tmp/logstash"
 ----------------------------------
 
-[[codec]]
+[[string]]
 [float]
-==== Codec
+==== String
 
-A codec is the name of Logstash codec used to represent the data. Codec can be
-used in both inputs and outputs.
-Input codecs are a convenient method for decoding your data before it enters the input,
-without needing a separate filter in your Logstash pipeline.
-Output codecs are a convenient method for encoding your data before it leaves the output,
-without needing a separate filter in your Logstash pipeline.
+A string must be a single character sequence. Note that string values are enclosed in quotes.
 
 Example:
 
 [source,js]
 ----------------------------------
-  codec => "json"
+  name => "Hello world"
 ----------------------------------
 
-[[bytes]]
 [float]
-==== Bytes
-
-A bytes field is a String field which represents a valid unit of bytes. It is a 
-convenient method for declaring specific sizes in your plugin options. Both SI(k M G T P E Z Y)
-and Binary(Ki Mi Gi Ti Pi Ei Zi Yi) units are supported. Binary units are in 
-base-1024, while SI units are in base-1000. This field is case-insensitive 
-and accepts space between the value and the unit. When no unit is specified, the integer string
-represents the number of bytes.
+=== Comments
 
-Examples:
+Comments are the same as in perl, ruby, and python. A comment starts with a '#' character, and does not need to be at the beginning of a line. For example:
 
 [source,js]
 ----------------------------------
-  my_bytes => "1113"   # 1113 bytes
-  my_bytes => "10MiB"  # 10485760 bytes
-  my_bytes => "100kib" # 102400 bytes
-  my_bytes => "180 mb" # 180000000 bytes
-----------------------------------
+# this is a comment
 
+input { # comments can appear at the end of a line, too
+  # ...
+}
+----------------------------------
 
 [float]
 === Field References
 
-All events have properties. For example, an apache access log would have things
+All events have properties. For example, an apache access log has things
 like status code (200, 404), request path ("/", "index.html"), HTTP verb (GET, POST),
-client IP address, etc. Logstash calls these properties "fields."
+and client IP address. Logstash calls these properties fields.
 
-In many cases, it is useful to be able to refer to a field by name. To do this,
+It is often useful to be able to refer to a field by name. To do this,
 you can use the Logstash field reference syntax.
 
-By way of example, let us suppose we have this event:
+The syntax to access a field is `[fieldname]`. If you are referring to a 
+**top-level field**, you can omit the `[]` and simply use `fieldname`.
+To refer to a **nested field**, you specify
+the full path to that field: `[top-level field][nested field]`.
+
+For example, the following event has five top-level fields (agent, ip, request, response, 
+ua) and three nested fields (status, bytes, os). 
 
 [source,js]
 ----------------------------------
@@ -239,18 +253,15 @@ By way of example, let us suppose we have this event:
 
 ----------------------------------
 
-- the syntax to access fields is `[fieldname]`.
-- if you are only referring to a **top-level field**, you can omit the `[]` and
-simply say `fieldname`.
-- in the case of **nested fields**, like the "os" field above, you need
-the full path to that field: `[ua][os]`.
+To reference the `os` field, you specify `[ua][os]`. To reference a top-level
+field such as `request`, you can simply specify the field name.
 
 [float]
 ==== sprintf format
 
-This syntax is also used in what Logstash calls 'sprintf format'. This format
-allows you to refer to field values from within other strings. For example, the
-statsd output has an 'increment' setting, to allow you to keep a count of
+The field reference format is also used in what Logstash calls 'sprintf format'. This format
+enables you to refer to field values from within other strings. For example, the
+statsd output has an 'increment' setting that enables you to keep a count of
 apache logs by status code:
 
 [source,js]
@@ -262,7 +273,7 @@ output {
 }
 ----------------------------------
 
-You can also do time formatting in this sprintf format. Instead of specifying a field name, use the `+FORMAT` syntax where `FORMAT` is a [time format](http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html).
+You can also format times using in this sprintf format. Instead of specifying a field name, use the `+FORMAT` syntax where `FORMAT` is a http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html[time format].
 
 For example, if you want to use the file output to write to logs based on the
 hour and the 'type' field:
@@ -279,14 +290,14 @@ output {
 [float]
 === Conditionals
 
-Sometimes you only want a filter or output to process an event under
-certain conditions. For that, you'll want to use a conditional!
+Sometimes you only want to filter or output an event under
+certain conditions. For that, you can use a conditional.
 
 Conditionals in Logstash look and act the same way they do in programming
-languages. You have `if`, `else if` and `else` statements. Conditionals may be
-nested if you need that.
+languages. Conditionals support `if`, `else if` and `else` statements
+and can be nested.
 
-The syntax is follows:
+The conditional syntax is:
 
 [source,js]
 ----------------------------------
@@ -299,27 +310,26 @@ if EXPRESSION {
 }
 ----------------------------------
 
-What's an expression? Comparison tests, boolean logic, etc!
+What's an expression? Comparison tests, boolean logic, and so on!
 
-The following comparison operators  are supported:
+You can use the following comparison operators:
 
-* equality, etc: ==,  !=,  <,  >,  <=,  >=
+* equality: ==,  !=,  <,  >,  <=,  >=
 * regexp: =~, !~
 * inclusion: in, not in
 
-The following boolean operators are supported:
+The supported boolean operators are:
 
 * and, or, nand, xor
 
-The following unary operators are supported:
+The supported unary operators are:
 
 * !
 
-Expressions may contain expressions. Expressions may be negated with `!`.
-Expressions may be grouped with parentheses `(...)`. Expressions can be long
-and complex.
+Expressions can be long and complex. Expressions can contain other expressions,
+you can negate expressions with `!`, and you can group them with parentheses `(...)`. 
 
-For example, if we want to remove the field `secret` if the field
+For example, the following conditional uses the mutate filter to remove the field `secret` if the field
 `action` has a value of `login`:
 
 [source,js]
@@ -331,17 +341,17 @@ filter {
 }
 ----------------------------------
 
-The above uses the field reference syntax to get the value of the
-`action` field. It is compared against the text `login` and, if equal,
-allows the mutate filter to delete the field named `secret`.
-
 How about a more complex example?
 
 * alert nagios of any apache events with status 5xx
 * record any 4xx status to elasticsearch
 * record all status code hits via statsd
 
-How about telling nagios of any http event that has a status code of 5xx?
+To tell nagios about any http event that has a 5xx status code, you
+first need to check the value of the `type` field. If it's apache, then you can 
+check to see if the `status` field contains a 5xx error. If it is, send it to nagios. If it isn't 
+a 5xx error, check to see if the `status` field contains a 4xx error. If so, send it to Elasticsearch. 
+Finally, send all apache status codes to statsd no matter what the `status` field contains:
 
 [source,js]
 ----------------------------------
@@ -357,7 +367,7 @@ output {
 }
 ----------------------------------
 
-You can also do multiple expressions in a single condition:
+You can specify multiple expressions in a single condition:
 
 [source,js]
 ----------------------------------
@@ -371,7 +381,7 @@ output {
 }
 ----------------------------------
 
-Here are some examples for testing with the in conditional:
+Here are some examples for testing with the `in` conditional:
 
 [source,js]
 ----------------------------------
@@ -407,8 +417,3 @@ output {
   }
 }
 ----------------------------------
-
-[float]
-=== Further Reading
-
-For more information, see [the plugin docs index](index)
diff --git a/docs/asciidoc/static/example-add-a-new-filter.asciidoc b/docs/asciidoc/static/example-add-a-new-filter.asciidoc
index 267c61e3aaf..7251d13a1bc 100644
--- a/docs/asciidoc/static/example-add-a-new-filter.asciidoc
+++ b/docs/asciidoc/static/example-add-a-new-filter.asciidoc
@@ -1,17 +1,19 @@
-== Adding a sample filter to Logstash
+== Adding a filter to Logstash
 
-This document shows you how to add a new filter to Logstash.
+To add a filter to Logstash, you need to:
 
-For a general overview of how to add a new plugin, see [the extending Logstash](.) overview.
+. Write code to extend the  LogStash::Filters::Base class.
+. Tell Logstash about your new filter.
+
+For a general overview of how to add plugins, see [the extending Logstash](.) overview.
 
 [float]
 === Write code.
 
-Let's write a 'hello world' filter. This filter will replace the 'message' in the event with "Hello world!"
-
-First, Logstash expects plugins in a certain directory structure: `logstash/TYPE/PLUGIN_NAME.rb`
+Let's write a 'hello world' filter. This filter replaces the 'message' in an event with "Hello world!"
 
-Since we're creating a filter, let's mkdir this:
+Logstash expects plugins to be in a certain directory structure: `logstash/TYPE/PLUGIN_NAME.rb`, 
+so first create a directory where you can store the new filter.
 
 [source,js]
 ----------------------------------
@@ -19,12 +21,10 @@ mkdir -p logstash/filters/
 cd logstash/filters
 ----------------------------------
 
-Now add the code:
+Now save the following code to a file called `foo.rb` in the `logstash/filters` directory.
 
 [source,js]
 ----------------------------------
-# Call this file 'foo.rb' (in logstash/filters, as above)
-
 require "logstash/filters/base"
 require "logstash/namespace"
 
@@ -66,8 +66,8 @@ class LogStash::Filters::Foo < LogStash::Filters::Base
   ## Add it to your configuration
 ----------------------------------
 
-For this simple example, let's just use stdin input and stdout output.
-The config file looks like this:
+Create a configuration file called `example.conf` that uses the new filter. You
+can just use stdin input and stdout output.
 
 [source,js]
 ----------------------------------
@@ -86,34 +86,23 @@ output {
 }
 ----------------------------------
 
-Call this file 'example.conf'
-
 [float]
 === Tell Logstash about it.
 
-Depending on how you installed Logstash, you have a few ways of including this
-plugin.
-
-You can use the agent flag --pluginpath flag to specify where the root of your
-plugin tree is. In our case, it's the current directory.
+You can use the agent flag `--pluginpath` flag to specify where the root of your
+plugin tree is. In this example, it's the current directory.
 
 [source,js]
 ----------------------------------
 % bin/logstash --pluginpath your/plugin/root -f example.conf
 ----------------------------------
 
-## Example running
-
-In the example below, I typed in "the quick brown fox" after running the java
-command.
+Now, the message text you input is transformed by the `foo` filter. For example: 
 
 [source,js]
 ----------------------------------
-% bin/logstash --pluginpath your/plugin/root -f example.conf
 the quick brown fox   
 2011-05-12T01:05:09.495000Z mylocalhost: Hello world!
 ----------------------------------
 
-The output is the standard Logstash stdout output, but in this case our "the quick brown fox" message was replaced with "Hello world!"
-
-All done! :)
+The output is the standard Logstash stdout output, but the "the quick brown fox" message is replaced with "Hello world!"
diff --git a/docs/asciidoc/static/getting-started-with-logstash.asciidoc b/docs/asciidoc/static/getting-started-with-logstash.asciidoc
index 46b06a8cb24..68828990e2e 100644
--- a/docs/asciidoc/static/getting-started-with-logstash.asciidoc
+++ b/docs/asciidoc/static/getting-started-with-logstash.asciidoc
@@ -1,14 +1,14 @@
 == Getting Started with Logstash
 
-[float]
-=== Introduction
-Logstash is a tool for receiving, processing and outputting logs. All kinds of logs. System logs, webserver logs, error logs, application logs and just about anything you can throw at it. Sounds great, eh?
+Logstash is a tool for receiving, processing and outputting logs. All kinds of logs. System logs, webserver logs, error logs, application logs, and just about anything you can throw at it. Sounds great, eh?
 
-Using Elasticsearch as a backend datastore, and kibana as a frontend reporting tool, Logstash acts as the workhorse, creating a powerful pipeline for storing, querying and analyzing your logs. With an arsenal of built-in inputs, filters, codecs and outputs, you can harness some powerful functionality with a small amount of effort. So, let's get started!
+Logstash provides a powerful pipeline for storing, querying, and analyzing your logs. When using Elasticsearch as a backend data store and Kibana as a frontend reporting tool, Logstash acts as the workhorse.  It includes an arsenal of built-in inputs, filters, codecs, and outputs, enabling you to harness some powerful functionality with a small amount of effort. So, let's get started!
 
 [float]
 ==== Prerequisite: Java
-The only prerequisite required by Logstash is a Java runtime. You can check that you have it installed by running the  command `java -version` in your shell. Here's something similar to what you might see:
+A Java runtime is required to run Logstash. We recommend running the latest version of Java. At a minimum, you need Java 7. You can use the http://www.oracle.com/technetwork/java/javase/downloads/index.html[official Oracle distribution], or an open-source distribution such as http://openjdk.java.net/[OpenJDK].
+
+You can verify that you have Java installed by running the  command `java -version` in your shell. Here's something similar to what you might see:
 [source,java]
 ----------------------------------
 > java -version
@@ -16,54 +16,51 @@ java version "1.7.0_45"
 Java(TM) SE Runtime Environment (build 1.7.0_45-b18)
 Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode)
 ----------------------------------
-It is recommended to run a recent version of Java in order to ensure the greatest success in running Logstash.
-
-It's fine to run an open-source version such as OpenJDK: +
-http://openjdk.java.net/
-
-Or you can use the official Oracle version: +
-http://www.oracle.com/technetwork/java/index.html
 
 Once you have verified the existence of Java on your system, we can move on!
 
 [float]
 === Up and Running!
 
-[float]
-==== Logstash in two commands
-First, we're going to download the 'logstash' binary and run it with a very simple configuration.
-["source","sh",subs="attributes,callouts"]
+To get started, download and extract the 'logstash' binary and run 
+it with a very simple configuration.
+
+First, download the Logstash tar file.
+
+["source","sh"]
 ----------------------------------
 curl -O https://download.elasticsearch.org/logstash/logstash/logstash-{logstash_version}.tar.gz
 ----------------------------------
-Now you should have the file named 'logstash-{logstash_version}.tar.gz' on your local filesystem. Let's unpack it:
+Then, unpack 'logstash-{logstash_version}.tar.gz' on your local filesystem. 
+
 ["source","sh",subs="attributes,callouts"]
 ----------------------------------
-tar zxvf logstash-{logstash_version}.tar.gz
-cd logstash-{logstash_version}
+tar -zxvf logstash-{logstash_version}.tar.gz
 ----------------------------------
-Now let's run it:
+Now, you can run Logstash with a basic configuration:
 [source,js]
 ----------------------------------
+cd logstash-{logstash_version}
 bin/logstash -e 'input { stdin { } } output { stdout {} }'
 ----------------------------------
 
-Now type something into your command prompt, and you will see it output by Logstash:
+This simply takes input from stdin and outputs it to stdout.  
+Type something at the command prompt, and you will see it output by Logstash:
 [source,js]
 ----------------------------------
 hello world
 2013-11-21T01:22:14.405+0000 0.0.0.0 hello world
 ----------------------------------
 
-OK, that's interesting... We ran Logstash with an input called `stdin`, and an output named `stdout`, and Logstash basically echoed back whatever we typed in some sort of structured format. Note that specifying the `-e` command line flag allows Logstash to accept a configuration directly from the command line. This is especially useful for quickly testing configurations without having to edit a file between iterations.
+OK, that's interesting... By running Logstash with the input called `stdin` and the output named `stdout`, Logstash echoes whatever you type in a structured format. The `-e` flag enables you to specify a configuration directly from the command line. This is especially useful for quickly testing configurations without having to edit a file between iterations.
 
-Let's try a slightly fancier example. First, you should exit Logstash by issuing a `CTRL-C` command in the shell in which it is running. Now run Logstash again with the following command:
+Let's try a slightly fancier example. First, exit Logstash by issuing a `CTRL-C` command in the shell in which it is running. Then, start Logstash again with the following command:
 [source,ruby]
 ----------------------------------
 bin/logstash -e 'input { stdin { } } output { stdout { codec => rubydebug } }'
 ----------------------------------
 
-And then try another test input, typing the text "goodnight moon":
+Now, enter some more test input:
 [source,ruby]
 ----------------------------------
 goodnight moon
@@ -75,7 +72,7 @@ goodnight moon
 }
 ----------------------------------
 
-So, by re-configuring the `stdout` output (adding a "codec"), we can change the output of Logstash. By adding inputs, outputs and filters to your configuration, it's possible to massage the log data in many ways, in order to maximize flexibility of the stored data when you are querying it.
+Re-configuring the `stdout` output by adding a "codec" enables you to change what Logstash outputs. By adding inputs, outputs, and filters to your configuration, you can massage the log data and maximize the flexibility of the stored data when you query it.
 
 [float]
 === Storing logs with Elasticsearch
@@ -84,37 +81,37 @@ Now, you're probably saying, "that's all fine and dandy, but typing all my logs
 ["source","sh",subs="attributes,callouts"]
 ----------------------------------
 curl -O https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-{elasticsearch_version}.tar.gz
-tar zxvf elasticsearch-{elasticsearch_version}.tar.gz
+tar -zxvf elasticsearch-{elasticsearch_version}.tar.gz
 cd elasticsearch-{elasticsearch_version}/
 ./bin/elasticsearch
 ----------------------------------
 
-NOTE: This tutorial is running running Logstash {logstash_version} with Elasticsearch {elasticsearch_version}, although you can use it with a cluster running 1.0.0 or later. Each release of Logstash has a *recommended* version of Elasticsearch to pair with. Make sure they match based on the http://www.elasticsearch.org/overview/logstash[Logstash version] you're running!
+NOTE: This tutorial runs Logstash {logstash_version} with Elasticsearch {elasticsearch_version}, although you can use it with a cluster running 1.0.0 or later. Each release of Logstash has a *recommended* version of Elasticsearch you should use. Make sure they match based on the http://www.elasticsearch.org/overview/logstash[Logstash version] you're running!
 
-More detailed information on installing and configuring Elasticsearch can be found on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index.html[The Elasticsearch reference pages]. However, for the purposes of Getting Started with Logstash, the default installation and configuration of Elasticsearch should be sufficient.
+You can get started with Logstash using the default Elasticsearch installation and configuration. See the http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index.html[Elasticsearch Reference] for more  information about installing and running Elasticsearch. 
 
-Now that we have Elasticsearch running on port 9200 (we do, right?), Logstash can be simply configured to use Elasticsearch as its backend. The defaults for both Logstash and Elasticsearch are fairly sane and well thought out, so we can omit the optional configurations within the elasticsearch output:
+Now that you have Elasticsearch running on port 9200 (you do, right?), you can easily configure Logstash to use Elasticsearch as its backend. The defaults for both Logstash and Elasticsearch are fairly sane and well thought out, so you can omit the optional configurations within the elasticsearch output:
 
 [source,js]
 ----------------------------------
 bin/logstash -e 'input { stdin { } } output { elasticsearch { host => localhost } }'
 ----------------------------------
 
-Type something, and Logstash will process it as before (this time you won't see any output, since we don't have the stdout output configured)
+Type something and Logstash processes it as before. However, this time you won't see any output, since the stdout output isn't configured.
 
 [source,js]
 ----------------------------------
 you know, for logs
 ----------------------------------
 
-You can confirm that ES actually received the data by making a curl request and inspecting the return:
+You can confirm that Elasticsearch actually received the data by submitting a curl request:
 
 [source,js]
 ----------------------------------
 curl 'http://localhost:9200/_search?pretty'
 ----------------------------------
 
-which should return something like this:
+This should return something like the following:
 
 [source,js]
 ----------------------------------
@@ -143,78 +140,87 @@ Congratulations! You've successfully stashed logs in Elasticsearch via Logstash.
 
 [float]
 ==== Elasticsearch Plugins (an aside)
-Another very useful tool for querying your Logstash data (and Elasticsearch in general) is the Elasticearch-kopf plugin. Here is more information on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html[Elasticsearch plugins]. To install elasticsearch-kopf, simply issue the following command in your Elasticsearch directory (the same one in which you ran Elasticsearch earlier):
+Another very useful tool for querying your Logstash data (and Elasticsearch in general) is the Elasticearch-kopf plugin. (For more information about Elasticsearch plugins, see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html[Elasticsearch plugins].) To install elasticsearch-kopf,  issue the following command from your Elasticsearch directory (the same one from which you started Elasticsearch):
 
 [source,js]
 ----------------------------------
 bin/plugin -install lmenezes/elasticsearch-kopf
 ----------------------------------
-Now you can browse to http://localhost:9200/_plugin/kopf/[http://localhost:9200/_plugin/kopf/] to browse your Elasticsearch data, settings and mappings!
+Now you can go to http://localhost:9200/_plugin/kopf/[http://localhost:9200/_plugin/kopf/] to browse your Elasticsearch data, settings, and mappings!
 
 [float]
 ==== Multiple Outputs
-As a quick exercise in configuring multiple Logstash outputs, let's invoke Logstash again, using both the 'stdout' as well as the 'elasticsearch' output:
+As a quick exercise in configuring multiple Logstash outputs, let's invoke Logstash again, using both  'stdout' and 'elasticsearch' as outputs:
 
 [source,js]
 ----------------------------------
 bin/logstash -e 'input { stdin { } } output { elasticsearch { host => localhost } stdout { } }'
 ----------------------------------
-Typing a phrase will now echo back to your terminal, as well as save in Elasticsearch! (Feel free to verify this using curl or elasticsearch-kopf).
+Now when you enter a phrase, it is echoed to the terminal and saved in Elasticsearch! (You can verify this using curl or elasticsearch-kopf).
 
 [float]
 ==== Default - Daily Indices
-You might notice that Logstash was smart enough to create a new index in Elasticsearch... The default index name is in the form of `logstash-YYYY.MM.DD`, which essentially creates one index per day. At midnight (UTC), Logstash will automagically rotate the index to a fresh new one, with the new current day's timestamp. This allows you to keep windows of data, based on how far retroactively you'd like to query your log data. Of course, you can always archive (or re-index) your data to an alternate location, where you are able to query further into the past. If you'd like to simply delete old indices after a certain time period, you can use the https://github.com/elasticsearch/curator[Elasticsearch Curator tool].
+You might have noticed that Logstash is smart enough to create a new index in Elasticsearch. The default index name is in the form of `logstash-YYYY.MM.DD`, which essentially creates one index per day. At midnight (UTC), Logstash automagically rotates the index to a fresh one, with the new current day's timestamp. This allows you to keep windows of data, based on how far retroactively you'd like to query your log data. Of course, you can always archive (or re-index) your data to an alternate location so you can query further into the past. If you want to delete old indices after a certain time period, you can use the https://github.com/elasticsearch/curator[Elasticsearch Curator tool].
 
 [float]
 === Moving On
-Now you're ready for more advanced configurations. At this point, it makes sense for a quick discussion of some of the core features of Logstash, and how they interact with the Logstash engine.
+Before we talk about more advanced configurations, let's take a quick look at some of the core features of Logstash and how they interact with the Logstash engine.
 [float]
 ==== The Life of an Event
 
-Inputs, Outputs, Codecs and Filters are at the heart of the Logstash configuration. By creating a pipeline of event processing, Logstash is able to extract the relevant data from your logs and make it available to elasticsearch, in order to efficiently query your data. To get you thinking about the various options available in Logstash, let's discuss some of the more common configurations currently in use. For more details, read about http://logstash.net/docs/latest/life-of-an-event[the Logstash event pipeline].
+Inputs, Outputs, Codecs and Filters are at the heart of the Logstash configuration. By creating an event processing pipeline, Logstash can extract the relevant data from your logs and make it available to Elasticsearch so you can efficiently query your data. To get you thinking about the various options available in Logstash, let's discuss some of the more common configurations currently in use. For more information, see <<pipeline, the life of an event>>.
 
 [float]
 ===== Inputs
-Inputs are the mechanism for passing log data to Logstash. Some of the more useful, commonly-used ones are:
+Inputs are the mechanism for passing log data to Logstash. Some of the more commonly-used inputs are:
 
 * *file*: reads from a file on the filesystem, much like the UNIX command `tail -0a`
-* *syslog*: listens on the well-known port 514 for syslog messages and parses according to RFC3164 format
-* *redis*: reads from a redis server, using both redis channels and also redis lists. Redis is often used as a "broker" in a centralized Logstash installation, which queues Logstash events from remote Logstash "shippers".
+* *syslog*: listens on the well-known port 514 for syslog messages and parses according to the RFC3164 format
+* *redis*: reads from a redis server, using both redis channels and redis lists. Redis is often used as a "broker" in a centralized Logstash installation, which queues Logstash events from remote Logstash "shippers".
 * *lumberjack*: processes events sent in the lumberjack protocol. Now called https://github.com/elasticsearch/logstash-forwarder[logstash-forwarder].
 
+For more information about the available inputs, see <<input-plugins,Input Plugins>>.
+
 [float]
 ===== Filters
-Filters are used as intermediary processing devices in the Logstash chain. They are often combined with conditionals in order to perform a certain action on an event, if it matches particular criteria. Some useful filters:
+Filters are used as intermediary processing devices in the Logstash pipeline. They are often combined with conditionals to perform an action on an event if it matches particular criteria. Some useful filters:
 
-* *grok*: parses arbitrary text and structure it. Grok is currently the best way in Logstash to parse unstructured log data into something structured and queryable. With 120 patterns shipped built-in to Logstash, it's more than likely you'll find one that meets your needs!
-* *mutate*: The mutate filter allows you to do general mutations to fields. You can rename, remove, replace, and modify fields in your events.
+* *grok*: parse and structure arbitrary text. Grok is currently the best way in Logstash to parse unstructured log data into something structured and queryable. With 120 patterns built-in to Logstash, it's more than likely you'll find one that meets your needs!
+* *mutate*: perform general transformations on event fields. You can rename, remove, replace, and modify fields in your events.
 * *drop*: drop an event completely, for example, 'debug' events.
 * *clone*: make a copy of an event, possibly adding or removing fields.
-* *geoip*: adds information about geographical location of IP addresses (and displays amazing charts in kibana)
+* *geoip*: add information about geographical location of IP addresses (also displays amazing charts in Kibana!)
+
+For more information about the available filters, see <<filter-plugins,Filter Plugins>>.
+
 [float]
 ===== Outputs
-Outputs are the final phase of the Logstash pipeline. An event may pass through multiple outputs during processing, but once all outputs are complete, the event has finished its execution. Some commonly used outputs include:
+Outputs are the final phase of the Logstash pipeline. An event can pass through multiple outputs, but once all output processing is complete, the event has finished its execution. Some commonly used outputs include:
+
+* *elasticsearch*: send event data to Elasticsearch. If you're planning to save your data in an efficient, convenient, and easily queryable format... Elasticsearch is the way to go. Period. Yes, we're biased :)
+* *file*: write event data to a file on disk.
+* *graphite*: send event data to graphite, a popular open source tool for storing and graphing metrics. http://graphite.wikidot.com/
+* *statsd*: send event data to statsd, a service that "listens for statistics, like counters and timers, sent over UDP and sends aggregates to one or more pluggable backend services". If you're already using statsd, this could be useful for you!
+
+For more information about the available outputs, see <<output-plugins,Output Plugins>>.
 
-* *elasticsearch*: If you're planning to save your data in an efficient, convenient and easily queryable format... Elasticsearch is the way to go. Period. Yes, we're biased :)
-* *file*: writes event data to a file on disk.
-* *graphite*: sends event data to graphite, a popular open source tool for storing and graphing metrics. http://graphite.wikidot.com/
-* *statsd*: a service which "listens for statistics, like counters and timers, sent over UDP and sends aggregates to one or more pluggable backend services". If you're already using statsd, this could be useful for you!
 [float]
 ===== Codecs
-Codecs are basically stream filters which can operate as part of an input, or an output. Codecs allow you to easily separate the transport of your messages from the serialization process. Popular codecs include `json`, `msgpack` and `plain` (text).
-
-* *json*: encode / decode data in JSON format
-* *multiline*: Takes multiple-line text events and merge them into a single event, e.g. java exception and stacktrace messages
+Codecs are basically stream filters that can operate as part of an input or output. Codecs allow you to easily separate the transport of your messages from the serialization process. Popular codecs include `json`, `msgpack`, and `plain` (text).
 
-For the complete list of (current) configurations, visit the Logstash <<plugin_configuration, plugin configuration>> section of the http://www.elasticsearch.org/guide/en/logstash/current/[Logstash documentation page].
+* *json*: encode or decode data in the JSON format.
+* *multiline*: merge multiple-line text events such as java exception and stacktrace messages into a single event.  
 
+For more information about the available codecs, see <<codec-plugins,Codec Plugins>>.
 
 [float]
 === More fun with Logstash
 [float]
 ==== Persistent Configuration files
 
-Specifying configurations on the command line using '-e' is only so helpful, and more advanced setups will require more lengthy, long-lived configurations. First, let's create a simple configuration file, and invoke Logstash using it. Create a file named "logstash-simple.conf" and save it in the same directory as Logstash.
+While specifying configurations on the command line is convenient for experimenting with different configurations, for more advanced setups with more complex configurations you'll want to use a configuration file. 
+
+Let's create a simple configuration file and use it to run Logstash. Create a file named "logstash-simple.conf" and save it in the same directory as Logstash.
 
 [source,ruby]
 ----------------------------------
@@ -225,18 +231,18 @@ output {
 }
 ----------------------------------
 
-Then, run this command:
+Then, run logstash and specify the configuration file with the -f flag.
 
 [source,ruby]
 ----------------------------------
 bin/logstash -f logstash-simple.conf
 ----------------------------------
 
-Et voilà! Logstash will read in the configuration file you just created and run as in the example we saw earlier. Note that we used the '-f' to read in the file, rather than the '-e' to read the configuration from the command line. This is a very simple case, of course, so let's move on to some more complex examples.
+Et voilà! Logstash reads  the specified configuration file and outputs to both Elasticsearch and stdout. This is a very simple case, of course, so let's move on to some more complex examples.
 
 [float]
 ==== Filters
-Filters are an in-line processing mechanism which provide the flexibility to slice and dice your data to fit your needs. Let's see one in action, namely the *grok filter*.
+Filters are an in-line processing mechanism that provide the flexibility to slice and dice your data to fit your needs. Let's take a look at some filters in action. The following configuration file sets up the *grok* and *date* filters.
 
 [source,ruby]
 ----------------------------------
@@ -256,6 +262,7 @@ output {
   stdout { codec => rubydebug }
 }
 ----------------------------------
+
 Run Logstash with this configuration:
 
 [source,ruby]
@@ -263,13 +270,13 @@ Run Logstash with this configuration:
 bin/logstash -f logstash-filter.conf
 ----------------------------------
 
-Now paste this line into the terminal (so it will be processed by the stdin input):
+Now, paste the following line into your terminal so it will be processed by the stdin input:
 [source,ruby]
 ----------------------------------
 127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] "GET /xampp/status.php HTTP/1.1" 200 3891 "http://cadenza/xampp/navi.php" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0"
 ----------------------------------
 
-You should see something returned to STDOUT which looks like this:
+You should see something returned to stdout that looks like this:
 
 [source,ruby]
 ----------------------------------
@@ -292,16 +299,16 @@ You should see something returned to STDOUT which looks like this:
 }
 ----------------------------------
 
-As you can see, Logstash (with help from the *grok* filter) was able to parse the log line (which happens to be in Apache "combined log" format) and break it up into many different discrete bits of information. This will be extremely useful later when we start querying and analyzing our log data... for example, we'll be able to run reports on HTTP response codes, IP addresses, referrers, etc. very easily. There are quite a few grok patterns included with Logstash out-of-the-box, so it's quite likely if you're attempting to parse a fairly common log format, someone has already done the work for you. For more details, see the list of https://github.com/logstash/logstash/blob/master/patterns/[logstash grok patterns] on github.
+As you can see, Logstash (with help from the *grok* filter) was able to parse the log line (which happens to be in Apache "combined log" format) and break it up into many different discrete bits of information. This is extremely useful once you start querying and analyzing our log data. For example, you'll be able to easily run reports on HTTP response codes, IP addresses, referrers, and so on. There are quite a few grok patterns included with Logstash out-of-the-box, so it's quite likely if you need to parse a common log format, someone has already done the work for you. For more information, see the list of https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns[Logstash grok patterns] on GitHub.
 
-The other filter used in this example is the *date* filter. This filter parses out a timestamp and uses it as the timestamp for the event (regardless of when you're ingesting the log data). You'll notice that the @timestamp field in this example is set to December 11, 2013, even though Logstash is ingesting the event at some point afterwards. This is handy when backfilling logs, for example... the ability to tell Logstash "use this value as the timestamp for this event".
+The other filter used in this example is the *date* filter. This filter parses out a timestamp and uses it as the timestamp for the event (regardless of when you're ingesting the log data). You'll notice that the @timestamp field in this example is set to December 11, 2013, even though Logstash is ingesting the event at some point afterwards. This is handy when backfilling logs. It gives you the ability to tell Logstash "use this value as the timestamp for this event".
 
 [float]
 === Useful Examples
 
 [float]
 ==== Apache logs (from files)
-Now, let's configure something actually *useful*... apache2 access log files! We are going to read the input from a file on the localhost, and use a *conditional* to process the event according to our needs. First, create a file called something like 'logstash-apache.conf' with the following contents (you'll need to change the log's file path to suit your needs):
+Now, let's configure something actually *useful*... apache2 access log files! We are going to read the input from a file on the localhost, and use a *conditional* to process the event according to our needs. First, create a file called something like 'logstash-apache.conf' with the following contents (you can change the log's file path to suit your needs):
 
 [source,js]
 ----------------------------------
@@ -333,7 +340,7 @@ output {
 
 ----------------------------------
 
-Then, create the file you configured above (in this example, "/tmp/access_log") with the following log lines as contents (or use some from your own webserver):
+Then, create the input file you configured above (in this example, "/tmp/access_log") with the following log entries (or use some from your own webserver):
 
 [source,js]
 ----------------------------------
@@ -342,16 +349,16 @@ Then, create the file you configured above (in this example, "/tmp/access_log")
 98.83.179.51 - - [18/May/2011:19:35:08 -0700] "GET /css/main.css HTTP/1.1" 200 1837 "http://www.safesand.com/information.htm" "Mozilla/5.0 (Windows NT 6.0; WOW64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1"
 ----------------------------------
 
-Now run it with the -f flag as in the last example:
+Now, run Logstash with the -f flag to pass in the configuration file:
 
 [source,js]
 ----------------------------------
 bin/logstash -f logstash-apache.conf
 ----------------------------------
 
-You should be able to see your apache log data in Elasticsearch now! You'll notice that Logstash opened the file you configured, and read through it, processing any events it encountered. Any additional lines logged to this file will also be captured, processed by Logstash as events and stored in Elasticsearch. As an added bonus, they will be stashed with the field "type" set to "apache_access" (this is done by the type => "apache_access" line in the input configuration).
+Now you should see your apache log data in Elasticsearch! Logstash opened and read the specified input file, processing each event it encountered. Any additional lines logged to this file will also be captured, processed by Logstash as events, and stored in Elasticsearch. As an added bonus, they are stashed with the field "type" set to "apache_access" (this is done by the type => "apache_access" line in the input configuration).
 
-In this configuration, Logstash is only watching the apache access_log, but it's easy enough to watch both the access_log and the error_log (actually, any file matching '*log'), by changing one line in the above configuration, like this:
+In this configuration, Logstash is only watching the apache access_log, but it's easy enough to watch both the access_log and the error_log (actually, any file matching '*log'), by changing one line in the above configuration:
 
 [source,js]
 ----------------------------------
@@ -361,13 +368,13 @@ input {
 ...
 ----------------------------------
 
-Now, rerun Logstash, and you will see both the error and access logs processed via Logstash. However, if you inspect your data (using elasticsearch-kopf, perhaps), you will see that the access_log was broken up into discrete fields, but not the error_log. That's because we used a "grok" filter to match the standard combined apache log format and automatically split the data into separate fields. Wouldn't it be nice *if* we could control how a line was parsed, based on its format? Well, we can...
+When you restart Logstash, it will process both the error and access logs. However, if you inspect your data (using elasticsearch-kopf, perhaps), you'll see that the access_log is broken up into discrete fields, but the error_log isn't. That's because we used a "grok" filter to match the standard combined apache log format and automatically split the data into separate fields. Wouldn't it be nice *if* we could control how a line was parsed, based on its format? Well, we can...
 
-Also, you might have noticed that Logstash did not reprocess the events which were already seen in the access_log file. Logstash is able to save its position in files, only processing new lines as they are added to the file. Neat!
+Note that Logstash did not reprocess the events that were already seen in the access_log file. When reading from a file, Logstash saves its position and only processes new lines as they are added. Neat!
 
 [float]
 ==== Conditionals
-Now we can build on the previous example, where we introduced the concept of a *conditional*. A conditional should be familiar to most Logstash users, in the general sense. You may use 'if', 'else if' and 'else' statements, as in many other programming languages. Let's label each event according to which file it appeared in (access_log, error_log and other random files which end with "log").
+In the previous example, we introduced the concept of a *conditional*. As in many other programming languages, you can use 'if', 'else if' and 'else' statements to control what events are processed. For example, you could label each event according to which file it appeared in (access_log, error_log, and other random files that end with "log").
 
 [source,ruby]
 ----------------------------------
@@ -399,11 +406,11 @@ output {
 }
 ----------------------------------
 
-You'll notice we've labeled all events using the "type" field, but we didn't actually parse the "error" or "random" files... There are so many types of error logs that it's better left as an exercise for you, depending on the logs you're seeing.
+This example labels all events using the "type" field, but doesn't actually parse the "error" or "random" files. There are so many types of error logs that how they should be labeled really depends on what logs you're working with.
 
 [float]
 ==== Syslog
-OK, now we can move on to another incredibly useful example: *syslog*. Syslog is one of the most common use cases for Logstash, and one it handles exceedingly well (as long as the log lines conform roughly to RFC3164 :). Syslog is the de facto UNIX networked logging standard, sending messages from client machines to a local file, or to a centralized log server via rsyslog. For this example, you won't need a functioning syslog instance; we'll fake it from the command line, so you can get a feel for what happens.
+Let's move on to another incredibly useful example: *syslog*. Syslog is one of the most common use cases for Logstash, and one it handles exceedingly well (as long as the log lines conform roughly to RFC3164). Syslog is the de facto UNIX networked logging standard, sending messages from client machines to a local file, or to a centralized log server via rsyslog. For this example, you won't need a functioning syslog instance; we'll fake it from the command line so you can get a feel for what happens.
 
 First, let's make a simple configuration file for Logstash + syslog, called 'logstash-syslog.conf'.
 
@@ -439,21 +446,22 @@ output {
   stdout { codec => rubydebug }
 }
 ----------------------------------
-Run it as normal:
+
+Run Logstash with this new configuration:
 
 [source,ruby]
 ----------------------------------
 bin/logstash -f logstash-syslog.conf
 ----------------------------------
 
-Normally, a client machine would connect to the Logstash instance on port 5000 and send its message. In this simplified case, we're simply going to telnet to Logstash and enter a log line (similar to how we entered log lines into STDIN earlier). First, open another shell window to interact with the Logstash syslog input and type the following command:
+Normally, a client machine would connect to the Logstash instance on port 5000 and send its message. For this example, we'll just telnet to Logstash and enter a log line (similar to how we entered log lines into STDIN earlier). Open another shell window to interact with the Logstash syslog input and enter the following command:
 
 [source,ruby]
 ----------------------------------
 telnet localhost 5000
 ----------------------------------
 
-You can copy and paste the following lines as samples (feel free to try some of your own, but keep in mind they might not parse if the grok filter is not correct for your data):
+Copy and paste the following lines as samples. (Feel free to try some of your own, but keep in mind they might not parse if the grok filter is not correct for your data).
 
 [source,ruby]
 ----------------------------------
diff --git a/docs/asciidoc/static/life-of-an-event.asciidoc b/docs/asciidoc/static/life-of-an-event.asciidoc
index 45b0eb34431..ca417b01da1 100644
--- a/docs/asciidoc/static/life-of-an-event.asciidoc
+++ b/docs/asciidoc/static/life-of-an-event.asciidoc
@@ -1,30 +1,24 @@
+[[pipeline]]
 == the life of an event
 
-The Logstash agent is an event pipeline.
+The Logstash agent is an event processing pipeline that has three stages: inputs -> filters -> outputs. Inputs generate events, filters modify them, and outputs ship them elsewhere.
 
-[float]
-=== The Pipeline
-
-The Logstash agent is a processing pipeline with 3 stages: inputs -> filters -> outputs. Inputs generate events, filters modify them, outputs ship them elsewhere.
-
-Internal to Logstash, events are passed from each phase using internal queues. It is implemented with a 'SizedQueue' in Ruby. SizedQueue allows a bounded maximum of items in the queue such that any writes to the queue will block if the queue is full at maximum capacity.
+Events are passed from stage to stage using internal queues implemented with a Ruby `SizedQueue`. A `SizedQueue` has a maximum number of items it can contain.  When the queue is at maximum capacity, all writes to the queue are blocked.
 
-Logstash sets each queue size to 20. This means only 20 events can be pending into the next phase - this helps reduce any data loss and in general avoids Logstash trying to act as a data storage system. These internal queues are not for storing messages long-term.
+Logstash sets the size of each queue to 20. This means a maximum of 20 events can be pending for the next stage, which helps prevent data loss and keeps Logstash from acting as a data storage system. These internal queues are not intended for storing messages long-term.
 
 [float]
 === Fault Tolerance
 
-Starting at outputs, here's what happens when things break.
+An output can fail or have problems due to downstream issues, such as a full disk, permissions problems, temporary network failures, or service outages. Most outputs keep retrying to ship events affected by the failure.
 
-An output can fail or have problems because of some downstream cause, such as full disk, permissions problems, temporary network failures, or service outages. Most outputs should keep retrying to ship any events that were involved in the failure.
+If an output is failing, the output thread waits until the output is able to successfully send the message. The output stops reading from the output queue, which means the queue can fill up with events. 
 
-If an output is failing, the output thread will wait until this output is healthy again and able to successfully send the message. Therefore, the output queue will stop being read from by this output and will eventually fill up with events and block new events from being written to this queue.
+When the output queue is full, filters are blocked because they cannot write new events to the output queue. While they are blocked from writing to the output queue, filters stop reading from the filter queue. Eventually, this can cause the filter queue (input -> filter) to fill up.
 
-A full output queue means filters will block trying to write to the output queue. Because filters will be stuck, blocked writing to the output queue, they will stop reading from the filter queue which will eventually cause the filter queue (input -> filter) to fill up.
+A full filter queue blocks inputs from writing to the filters. This causes all inputs to stop processing data from wherever they're getting new events.
 
-A full filter queue will cause inputs to block when writing to the filters. This will cause each input to block, causing each input to stop processing new data from wherever that input is getting new events.
-
-In ideal circumstances, this will behave similarly to when the tcp window closes to 0, no new data is sent because the receiver hasn't finished processing the current queue of data, but as soon as the downstream (output) problem is resolved, messages will begin flowing again..
+In ideal circumstances, this behaves similarly to when the tcp window closes to 0. No new data is sent because the receiver hasn't finished processing the current queue of data, but as soon as the downstream (output) problem is resolved, messages start flowing again.
 
 [float]
 === Thread Model
@@ -36,34 +30,29 @@ The thread model in Logstash is currently:
 input threads | filter worker threads | output worker
 ----------------------------------
 
-Filters are optional, so you will have this model if you have no filters defined:
+Filters are optional, so if you have no filters defined it is simply:
 
 [source,js]
 ----------------------------------
 input threads | output worker
 ----------------------------------
 
-Each input runs in a thread by itself. This allows busier inputs to not be blocked by slower ones, etc. It also allows for easier containment of scope because each input has a thread.
+Each input runs in a thread by itself. This prevents busier inputs from being blocked by slower ones. It also allows for easier containment of scope because each input has a thread.
 
-The filter thread model is a 'worker' model where each worker receives an event and applies all filters, in order, before emitting that to the output queue. This allows scalability across CPUs because many filters are CPU intensive (permitting that we have thread safety). 
+The filter thread model is a 'worker' model where each worker receives an event and applies all filters, in order, before sending it on to the output queue. This allows scalability across CPUs because many filters are CPU intensive (permitting that we have thread safety). 
 
-The default number of filter workers is 1, but you can increase this number with the '-w' flag on the agent.
+The default number of filter workers is 1, but you can increase this number by specifying the '-w' flag when you run the Logstash agent.
 
-The output worker model is currently a single thread. Outputs will receive events in the order they are defined in the config file. 
+The output worker model is currently a single thread. Outputs receive events in the order the outputs are defined in the config file. 
 
-Outputs may decide to buffer events temporarily before publishing them, possibly in a separate thread. One example of this is the elasticsearch output
-which will buffer events and flush them all at once, in a separate thread. This mechanism (buffering many events + writing in a separate thread) can improve performance so the Logstash pipeline isn't stalled waiting for a response from elasticsearch.
+Outputs might decide to temporarily buffer events before publishing them. One example of this is the `elasticsearch` output, which buffers events and flushes them all at once using a separate thread. This mechanism (buffering many events and writing in a separate thread) can improve performance because it prevents the Logstash pipeline from being stalled waiting for a response from elasticsearch.
 
 [float]
 === Consequences and Expectations
 
-Small queue sizes mean that Logstash simply blocks and stalls safely during times of load or other temporary pipeline problems. There are two alternatives to this - unlimited queue length and dropping messages. Unlimited queues grow grow unbounded and eventually exceed memory causing a crash which loses all of those messages. Dropping messages is also an undesirable behavior in most cases.
-
-At a minimum, Logstash will have probably 3 threads (2 if you have no filters). One input, one filter worker, and one output thread each.
-
-If you see Logstash using multiple CPUs, this is likely why. If you want to know more about what each thread is doing, you should read this: <http://www.semicomplete.com/blog/geekery/debugging-java-performance.html>.
+The small queue sizes mean that Logstash simply blocks and stalls safely when there's a heavy load or temporary pipeline problems. The alternatives would be to either have an unlimited queue or drop messages when there's a problem. An unlimited queue can grow unbounded and eventually exceed memory, causing a crash that loses all of the queued messages. In most cases, dropping messages outright is undesirable.
 
-Threads in java have names, and you can use jstack and top to figure out who is using what resources. The URL above will help you learn how to do this.
+Logstash typically has at least 3 threads (2 if you have no filters). One input thread, one filter worker thread, and one output thread. If you see Logstash using multiple CPUs, this is likely why. If you want to know more about what each thread is doing, you should read this article: http://www.semicomplete.com/blog/geekery/debugging-java-performance.html[Debugging Java Performance]. Threads in Java have names and you can use `jstack` and `top` to figure out who is using what resources. 
 
-On Linux platforms, Logstash will label all the threads it can with something descriptive. Inputs will show up as "<inputname" and filter workers as "|worker" and outputs as ">outputworker" (or something similar).  Other threads may be labeled as well, and are intended to help you identify their purpose should you wonder why they are consuming resources!
+On Linux platforms, Logstash labels all the threads it can with something descriptive. For example, inputs show up as `<inputname`, filter workers show up as `|worker`, and outputs show up as `>outputworker`.  Where possible, other threads are also labeled to help you identify their purpose should you wonder why they are consuming resources!
 
diff --git a/docs/index-codecs.asciidoc.erb b/docs/index-codecs.asciidoc.erb
index c87562f4218..528ee1d39d0 100644
--- a/docs/index-codecs.asciidoc.erb
+++ b/docs/index-codecs.asciidoc.erb
@@ -1,7 +1,7 @@
 [[codec-plugins]]
 == Codec plugins
 
-The plugins in this section change the data representation of an input or output in Logstash.
+A codec plugin changes the data representation of an event. Codecs are essentially stream filters that can operate as part of an input or output.
 
 The following codec plugins are available:
 
diff --git a/docs/index-filters.asciidoc.erb b/docs/index-filters.asciidoc.erb
index 5f9e5363c9e..bcfab03925c 100644
--- a/docs/index-filters.asciidoc.erb
+++ b/docs/index-filters.asciidoc.erb
@@ -1,7 +1,7 @@
 [[filter-plugins]]
 == Filter plugins
 
-The plugins in this section apply intermediary processing to the information from a given source in Logstash.
+A filter plugin performs intermediary processing on an event. Filters are often applied conditionally depending on the characteristics of the event. 
 
 The following filter plugins are available:
 
diff --git a/docs/index-outputs.asciidoc.erb b/docs/index-outputs.asciidoc.erb
index 60744157fda..2cab51fa478 100644
--- a/docs/index-outputs.asciidoc.erb
+++ b/docs/index-outputs.asciidoc.erb
@@ -1,7 +1,7 @@
 [[output-plugins]]
 == Output plugins
 
-Output plugins manage the final disposition of event data.
+An output plugin sends event data to a particular destination. Outputs are the final stage in the event pipeline.
 
 The following output plugins are available:
 
