diff --git a/.gitignore b/.gitignore
index 2715c52735f..29c78e9bf7f 100644
--- a/.gitignore
+++ b/.gitignore
@@ -39,3 +39,4 @@ qa/Gemfile.lock
 .idea
 logs
 qa/integration/services/installed/
+**/.gradle
diff --git a/Gemfile b/Gemfile
index 9e2c0c8d835..3dd811e8bc6 100644
--- a/Gemfile
+++ b/Gemfile
@@ -3,6 +3,7 @@
 
 source "https://rubygems.org"
 gem "logstash-core", :path => "./logstash-core"
+gem "logstash-core-queue-jruby", :path => "./logstash-core-queue-jruby"
 gem "logstash-core-event-java", :path => "./logstash-core-event-java"
 gem "logstash-core-plugin-api", :path => "./logstash-core-plugin-api"
 gem "file-dependencies", "0.1.6"
@@ -113,3 +114,4 @@ gem "logstash-output-stdout"
 gem "logstash-output-tcp"
 gem "logstash-output-udp"
 gem "logstash-output-webhdfs"
+gem "logstash-filter-multiline"
diff --git a/Gemfile.jruby-1.9.lock b/Gemfile.jruby-1.9.lock
new file mode 100644
index 00000000000..cf29142672f
--- /dev/null
+++ b/Gemfile.jruby-1.9.lock
@@ -0,0 +1,722 @@
+PATH
+  remote: ./logstash-core
+  specs:
+    logstash-core (6.0.0.pre.alpha1-java)
+      chronic_duration (= 0.10.6)
+      clamp (~> 0.6.5)
+      concurrent-ruby (= 1.0.0)
+      filesize (= 0.0.4)
+      gems (~> 0.8.3)
+      i18n (= 0.6.9)
+      jar-dependencies (~> 0.3.4)
+      jrjackson (~> 0.4.0)
+      jrmonitor (~> 0.4.2)
+      jruby-openssl (= 0.9.16)
+      logstash-core-event-java (= 6.0.0.pre.alpha1)
+      minitar (~> 0.5.4)
+      pry (~> 0.10.1)
+      puma (~> 2.16)
+      rubyzip (~> 1.1.7)
+      sinatra (~> 1.4, >= 1.4.6)
+      stud (~> 0.0.19)
+      thread_safe (~> 0.3.5)
+      treetop (< 1.5.0)
+
+PATH
+  remote: ./logstash-core-event-java
+  specs:
+    logstash-core-event-java (6.0.0.pre.alpha1-java)
+      jar-dependencies
+      ruby-maven (~> 3.3.9)
+
+PATH
+  remote: ./logstash-core-plugin-api
+  specs:
+    logstash-core-plugin-api (2.1.16-java)
+      logstash-core (= 6.0.0.pre.alpha1)
+
+GEM
+  remote: https://rubygems.org/
+  specs:
+    addressable (2.3.8)
+    arr-pm (0.0.10)
+      cabin (> 0)
+    atomic (1.1.99-java)
+    avl_tree (1.2.1)
+      atomic (~> 1.1)
+    awesome_print (1.7.0)
+    aws-sdk (2.3.22)
+      aws-sdk-resources (= 2.3.22)
+    aws-sdk-core (2.3.22)
+      jmespath (~> 1.0)
+    aws-sdk-resources (2.3.22)
+      aws-sdk-core (= 2.3.22)
+    aws-sdk-v1 (1.66.0)
+      json (~> 1.4)
+      nokogiri (>= 1.4.4)
+    backports (3.6.8)
+    benchmark-ips (2.7.2)
+    bindata (2.3.3)
+    buftok (0.2.0)
+    builder (3.2.2)
+    cabin (0.9.0)
+    childprocess (0.5.9)
+      ffi (~> 1.0, >= 1.0.11)
+    chronic_duration (0.10.6)
+      numerizer (~> 0.1.1)
+    ci_reporter (2.0.0)
+      builder (>= 2.1.2)
+    ci_reporter_rspec (1.0.0)
+      ci_reporter (~> 2.0)
+      rspec (>= 2.14, < 4)
+    cinch (2.3.2)
+    clamp (0.6.5)
+    coderay (1.1.1)
+    concurrent-ruby (1.0.0-java)
+    coveralls (0.8.15)
+      json (>= 1.8, < 3)
+      simplecov (~> 0.12.0)
+      term-ansicolor (~> 1.3)
+      thor (~> 0.19.1)
+      tins (>= 1.6.0, < 2)
+    diff-lcs (1.2.5)
+    docile (1.1.5)
+    domain_name (0.5.20160826)
+      unf (>= 0.0.5, < 1.0.0)
+    edn (1.1.1)
+    elasticsearch (1.1.0)
+      elasticsearch-api (= 1.1.0)
+      elasticsearch-transport (= 1.1.0)
+    elasticsearch-api (1.1.0)
+      multi_json
+    elasticsearch-transport (1.1.0)
+      faraday
+      multi_json
+    equalizer (0.0.10)
+    faraday (0.9.2)
+      multipart-post (>= 1.2, < 3)
+    ffi (1.9.14-java)
+    file-dependencies (0.1.6)
+      minitar
+    filesize (0.0.4)
+    filewatch (0.9.0)
+    fivemat (1.3.2)
+    flores (0.0.6)
+    fpm (1.3.3)
+      arr-pm (~> 0.0.9)
+      backports (>= 2.6.2)
+      cabin (>= 0.6.0)
+      childprocess
+      clamp (~> 0.6)
+      ffi
+      json (>= 1.7.7)
+    gelfd (0.2.0)
+    gem_publisher (1.5.0)
+    gems (0.8.3)
+    hitimes (1.2.4-java)
+    http (0.9.9)
+      addressable (~> 2.3)
+      http-cookie (~> 1.0)
+      http-form_data (~> 1.0.1)
+      http_parser.rb (~> 0.6.0)
+    http-cookie (1.0.2)
+      domain_name (~> 0.5)
+    http-form_data (1.0.1)
+    http_parser.rb (0.6.0-java)
+    i18n (0.6.9)
+    insist (1.0.0)
+    jar-dependencies (0.3.5)
+    jls-grok (0.11.3)
+      cabin (>= 0.6.0)
+    jls-lumberjack (0.0.26)
+      concurrent-ruby
+    jmespath (1.3.1)
+    jrjackson (0.4.0-java)
+    jrmonitor (0.4.2)
+    jruby-openssl (0.9.16-java)
+    jruby-stdin-channel (0.2.0-java)
+    json (1.8.3-java)
+    kramdown (1.12.0)
+    logstash-codec-collectd (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-dots (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-edn (3.0.2)
+      edn
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-edn_lines (3.0.2)
+      edn
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-es_bulk (3.0.2)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-fluent (3.0.2-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      msgpack-jruby
+    logstash-codec-graphite (3.0.2)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-json (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-json_lines (3.0.2)
+      logstash-codec-line (>= 2.1.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-line (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-msgpack (3.0.2-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      msgpack-jruby
+    logstash-codec-multiline (3.0.2)
+      jls-grok (~> 0.11.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-patterns-core
+    logstash-codec-netflow (3.1.2)
+      bindata (>= 1.5.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-plain (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-rubydebug (3.0.2)
+      awesome_print
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-devutils (1.1.0-java)
+      fivemat
+      gem_publisher
+      insist (= 1.0.0)
+      kramdown
+      logstash-core-plugin-api (~> 2.0)
+      minitar
+      rake
+      rspec (~> 3.0)
+      rspec-wait
+      stud (>= 0.0.20)
+    logstash-filter-clone (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-csv (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-date (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-dns (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      lru_redux (~> 1.1.0)
+    logstash-filter-drop (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-fingerprint (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      murmurhash3
+    logstash-filter-geoip (4.0.3-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-grok (3.2.1)
+      jls-grok (~> 0.11.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-patterns-core
+    logstash-filter-json (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-kv (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-metrics (4.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      metriks
+      thread_safe
+    logstash-filter-mutate (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-ruby (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-filter-date
+    logstash-filter-sleep (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-split (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-syslog_pri (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-throttle (4.0.0)
+      atomic
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      thread_safe
+    logstash-filter-urldecode (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-useragent (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      lru_redux (~> 1.1.0)
+      user_agent_parser (>= 2.0.0)
+    logstash-filter-uuid (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-xml (4.0.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      nokogiri
+      xml-simple
+    logstash-input-beats (3.1.3-java)
+      concurrent-ruby (>= 0.9.2, <= 1.0.0)
+      jar-dependencies (~> 0.3.4)
+      logstash-codec-multiline (>= 2.0.5)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      thread_safe (~> 0.3.5)
+    logstash-input-couchdb_changes (3.0.2)
+      json
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (>= 0.0.22)
+    logstash-input-elasticsearch (3.0.2)
+      elasticsearch (~> 1.0, >= 1.0.6)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-exec (3.1.1)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-file (3.1.1)
+      addressable
+      filewatch (~> 0.8, >= 0.8.1)
+      logstash-codec-multiline (~> 3.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-ganglia (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-gelf (3.0.2)
+      gelfd (= 0.2.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-generator (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-graphite (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-input-tcp
+    logstash-input-heartbeat (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud
+    logstash-input-http (3.0.3)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      puma (~> 2.16, >= 2.16.0)
+      rack (~> 1)
+      stud
+    logstash-input-http_poller (3.1.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-http_client (>= 2.2.4, < 5.0.0)
+      rufus-scheduler (~> 3.0.9)
+      stud (~> 0.0.22)
+    logstash-input-imap (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      mail (~> 2.6.3)
+      mime-types (= 2.6.2)
+      stud (~> 0.0.22)
+    logstash-input-irc (3.0.2)
+      cinch
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-jdbc (4.1.1)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      rufus-scheduler
+      sequel
+      tzinfo
+      tzinfo-data
+    logstash-input-kafka (5.0.4)
+      logstash-codec-json
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (>= 0.0.22, < 0.1.0)
+    logstash-input-log4j (3.0.3-java)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-lumberjack (3.1.1)
+      concurrent-ruby
+      jls-lumberjack (~> 0.0.26)
+      logstash-codec-multiline (~> 3.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-pipe (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-rabbitmq (5.1.1)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-rabbitmq_connection (>= 4.1.1, < 5.0.0)
+    logstash-input-redis (3.1.1)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      redis
+    logstash-input-s3 (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws
+      stud (~> 0.0.18)
+    logstash-input-snmptrap (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      snmp
+    logstash-input-sqs (3.0.2)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+    logstash-input-stdin (3.2.0)
+      concurrent-ruby
+      jruby-stdin-channel
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-syslog (3.0.2)
+      concurrent-ruby
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-filter-date
+      logstash-filter-grok
+      stud (>= 0.0.22, < 0.1.0)
+      thread_safe
+    logstash-input-tcp (4.0.3)
+      logstash-codec-json
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-twitter (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (>= 0.0.22, < 0.1)
+      twitter (= 5.15.0)
+    logstash-input-udp (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-unix (3.0.2)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-xmpp (3.1.1)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      xmpp4r (= 0.5)
+    logstash-mixin-aws (4.2.0)
+      aws-sdk (~> 2.3.0)
+      aws-sdk-v1 (>= 1.61.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-mixin-http_client (4.0.3)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      manticore (>= 0.5.2, < 1.0.0)
+    logstash-mixin-rabbitmq_connection (4.1.2-java)
+      march_hare (~> 2.15.0)
+      stud (~> 0.0.22)
+    logstash-output-cloudwatch (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+      rufus-scheduler (~> 3.0.9)
+    logstash-output-csv (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-filter-json
+      logstash-input-generator
+      logstash-output-file
+    logstash-output-elasticsearch (5.1.1-java)
+      cabin (~> 0.6)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      manticore (>= 0.5.4, < 1.0.0)
+      stud (~> 0.0, >= 0.0.17)
+    logstash-output-file (4.0.0)
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-core-plugin-api (>= 2.0.0, < 2.99)
+    logstash-output-graphite (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-http (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-http_client (>= 2.2.1, < 5.0.0)
+    logstash-output-irc (3.0.2)
+      cinch
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-kafka (5.0.4)
+      logstash-codec-json
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-nagios (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-null (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-pagerduty (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-pipe (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-rabbitmq (4.0.4-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-rabbitmq_connection (>= 4.1.1, < 5.0.0)
+    logstash-output-redis (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      redis
+      stud
+    logstash-output-s3 (3.2.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws
+      stud (~> 0.0.22)
+    logstash-output-sns (4.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+    logstash-output-sqs (3.0.2)
+      aws-sdk
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+      stud
+    logstash-output-statsd (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-input-generator
+      statsd-ruby (= 1.2.0)
+    logstash-output-stdout (3.1.0)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60.1, < 2.99)
+    logstash-output-tcp (4.0.0)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud
+    logstash-output-udp (3.0.2)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-webhdfs (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      snappy (= 0.0.12)
+      webhdfs
+    logstash-output-xmpp (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      xmpp4r (= 0.5)
+    logstash-patterns-core (4.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    lru_redux (1.1.0)
+    mail (2.6.4)
+      mime-types (>= 1.16, < 4)
+    manticore (0.6.0-java)
+    march_hare (2.15.0-java)
+    memoizable (0.4.2)
+      thread_safe (~> 0.3, >= 0.3.1)
+    method_source (0.8.2)
+    metriks (0.9.9.7)
+      atomic (~> 1.0)
+      avl_tree (~> 1.2.0)
+      hitimes (~> 1.1)
+    mime-types (2.6.2)
+    minitar (0.5.4)
+    msgpack-jruby (1.4.1-java)
+    multi_json (1.12.1)
+    multipart-post (2.0.0)
+    murmurhash3 (0.1.6-java)
+    mustache (0.99.8)
+    naught (1.1.0)
+    nokogiri (1.6.8-java)
+    numerizer (0.1.1)
+    octokit (3.8.0)
+      sawyer (~> 0.6.0, >= 0.5.3)
+    pleaserun (0.0.24)
+      cabin (> 0)
+      clamp
+      insist
+      mustache (= 0.99.8)
+      stud
+    polyglot (0.3.5)
+    pry (0.10.4-java)
+      coderay (~> 1.1.0)
+      method_source (~> 0.8.1)
+      slop (~> 3.4)
+      spoon (~> 0.0)
+    puma (2.16.0-java)
+    rack (1.6.4)
+    rack-protection (1.5.3)
+      rack
+    rack-test (0.6.3)
+      rack (>= 1.0)
+    rake (11.2.2)
+    redis (3.3.1)
+    rspec (3.1.0)
+      rspec-core (~> 3.1.0)
+      rspec-expectations (~> 3.1.0)
+      rspec-mocks (~> 3.1.0)
+    rspec-core (3.1.7)
+      rspec-support (~> 3.1.0)
+    rspec-expectations (3.1.2)
+      diff-lcs (>= 1.2.0, < 2.0)
+      rspec-support (~> 3.1.0)
+    rspec-mocks (3.1.3)
+      rspec-support (~> 3.1.0)
+    rspec-support (3.1.2)
+    rspec-wait (0.0.9)
+      rspec (>= 3, < 4)
+    ruby-maven (3.3.12)
+      ruby-maven-libs (~> 3.3.9)
+    ruby-maven-libs (3.3.9)
+    rubyzip (1.1.7)
+    rufus-scheduler (3.0.9)
+      tzinfo
+    sawyer (0.6.0)
+      addressable (~> 2.3.5)
+      faraday (~> 0.8, < 0.10)
+    sequel (4.38.0)
+    simple_oauth (0.3.1)
+    simplecov (0.12.0)
+      docile (~> 1.1.0)
+      json (>= 1.8, < 3)
+      simplecov-html (~> 0.10.0)
+    simplecov-html (0.10.0)
+    sinatra (1.4.7)
+      rack (~> 1.5)
+      rack-protection (~> 1.4)
+      tilt (>= 1.3, < 3)
+    slop (3.6.0)
+    snappy (0.0.12-java)
+      snappy-jars (~> 1.1.0)
+    snappy-jars (1.1.0.1.2-java)
+    snmp (1.2.0)
+    spoon (0.0.6)
+      ffi
+    statsd-ruby (1.2.0)
+    stud (0.0.22)
+    term-ansicolor (1.3.2)
+      tins (~> 1.0)
+    thor (0.19.1)
+    thread_safe (0.3.5-java)
+    tilt (2.0.5)
+    tins (1.6.0)
+    treetop (1.4.15)
+      polyglot
+      polyglot (>= 0.3.1)
+    twitter (5.15.0)
+      addressable (~> 2.3)
+      buftok (~> 0.2.0)
+      equalizer (= 0.0.10)
+      faraday (~> 0.9.0)
+      http (>= 0.4, < 0.10)
+      http_parser.rb (~> 0.6.0)
+      json (~> 1.8)
+      memoizable (~> 0.4.0)
+      naught (~> 1.0)
+      simple_oauth (~> 0.3.0)
+    tzinfo (1.2.2)
+      thread_safe (~> 0.1)
+    tzinfo-data (1.2016.6)
+      tzinfo (>= 1.0.0)
+    unf (0.1.4-java)
+    user_agent_parser (2.3.0)
+    webhdfs (0.8.0)
+      addressable
+    xml-simple (1.1.5)
+    xmpp4r (0.5)
+
+PLATFORMS
+  java
+
+DEPENDENCIES
+  benchmark-ips
+  ci_reporter_rspec (= 1.0.0)
+  coveralls
+  file-dependencies (= 0.1.6)
+  flores (~> 0.0.6)
+  fpm (~> 1.3.3)
+  gems (~> 0.8.3)
+  logstash-codec-collectd
+  logstash-codec-dots
+  logstash-codec-edn
+  logstash-codec-edn_lines
+  logstash-codec-es_bulk
+  logstash-codec-fluent
+  logstash-codec-graphite
+  logstash-codec-json
+  logstash-codec-json_lines
+  logstash-codec-line
+  logstash-codec-msgpack
+  logstash-codec-multiline
+  logstash-codec-netflow
+  logstash-codec-plain
+  logstash-codec-rubydebug
+  logstash-core!
+  logstash-core-event-java!
+  logstash-core-plugin-api!
+  logstash-devutils (~> 1.1)
+  logstash-filter-clone
+  logstash-filter-csv
+  logstash-filter-date
+  logstash-filter-dns
+  logstash-filter-drop
+  logstash-filter-fingerprint
+  logstash-filter-geoip
+  logstash-filter-grok
+  logstash-filter-json
+  logstash-filter-kv
+  logstash-filter-metrics
+  logstash-filter-mutate
+  logstash-filter-ruby
+  logstash-filter-sleep
+  logstash-filter-split
+  logstash-filter-syslog_pri
+  logstash-filter-throttle
+  logstash-filter-urldecode
+  logstash-filter-useragent
+  logstash-filter-uuid
+  logstash-filter-xml
+  logstash-input-beats
+  logstash-input-couchdb_changes
+  logstash-input-elasticsearch
+  logstash-input-exec
+  logstash-input-file
+  logstash-input-ganglia
+  logstash-input-gelf
+  logstash-input-generator
+  logstash-input-graphite
+  logstash-input-heartbeat
+  logstash-input-http
+  logstash-input-http_poller
+  logstash-input-imap
+  logstash-input-irc
+  logstash-input-jdbc
+  logstash-input-kafka
+  logstash-input-log4j
+  logstash-input-lumberjack
+  logstash-input-pipe
+  logstash-input-rabbitmq
+  logstash-input-redis
+  logstash-input-s3
+  logstash-input-snmptrap
+  logstash-input-sqs
+  logstash-input-stdin
+  logstash-input-syslog
+  logstash-input-tcp
+  logstash-input-twitter
+  logstash-input-udp
+  logstash-input-unix
+  logstash-input-xmpp
+  logstash-output-cloudwatch
+  logstash-output-csv
+  logstash-output-elasticsearch
+  logstash-output-file
+  logstash-output-graphite
+  logstash-output-http
+  logstash-output-irc
+  logstash-output-kafka
+  logstash-output-nagios
+  logstash-output-null
+  logstash-output-pagerduty
+  logstash-output-pipe
+  logstash-output-rabbitmq
+  logstash-output-redis
+  logstash-output-s3
+  logstash-output-sns
+  logstash-output-sqs
+  logstash-output-statsd
+  logstash-output-stdout
+  logstash-output-tcp
+  logstash-output-udp
+  logstash-output-webhdfs
+  logstash-output-xmpp
+  octokit (= 3.8.0)
+  pleaserun
+  rack-test
+  rspec (~> 3.1.0)
+  rubyzip (~> 1.1.7)
+  simplecov
+  stud (~> 0.0.22)
+  tins (= 1.6)
diff --git a/Gemfile.lock b/Gemfile.lock
new file mode 100644
index 00000000000..47685567665
--- /dev/null
+++ b/Gemfile.lock
@@ -0,0 +1,726 @@
+PATH
+  remote: ./logstash-core-event-java
+  specs:
+    logstash-core-event-java (6.0.0.pre.alpha1-java)
+      jar-dependencies
+      ruby-maven (~> 3.3.9)
+
+PATH
+  remote: ./logstash-core-plugin-api
+  specs:
+    logstash-core-plugin-api (2.1.16-java)
+      logstash-core (= 6.0.0.pre.alpha1)
+
+PATH
+  remote: ./logstash-core
+  specs:
+    logstash-core (6.0.0.pre.alpha1-java)
+      chronic_duration (= 0.10.6)
+      clamp (~> 0.6.5)
+      concurrent-ruby (= 1.0.0)
+      filesize (= 0.0.4)
+      gems (~> 0.8.3)
+      i18n (= 0.6.9)
+      jar-dependencies (~> 0.3.4)
+      jrjackson (~> 0.4.0)
+      jrmonitor (~> 0.4.2)
+      jruby-openssl (= 0.9.16)
+      logstash-core-event-java (= 6.0.0.pre.alpha1)
+      minitar (~> 0.5.4)
+      pry (~> 0.10.1)
+      puma (~> 2.16)
+      rubyzip (~> 1.1.7)
+      sinatra (~> 1.4, >= 1.4.6)
+      stud (~> 0.0.19)
+      term-ansicolor (~> 1.3.2)
+      thread_safe (~> 0.3.5)
+      treetop (< 1.5.0)
+
+GEM
+  remote: https://rubygems.org/
+  specs:
+    addressable (2.3.8)
+    arr-pm (0.0.10)
+      cabin (> 0)
+    atomic (1.1.99-java)
+    avl_tree (1.2.1)
+      atomic (~> 1.1)
+    awesome_print (1.7.0)
+    aws-sdk (2.3.22)
+      aws-sdk-resources (= 2.3.22)
+    aws-sdk-core (2.3.22)
+      jmespath (~> 1.0)
+    aws-sdk-resources (2.3.22)
+      aws-sdk-core (= 2.3.22)
+    aws-sdk-v1 (1.66.0)
+      json (~> 1.4)
+      nokogiri (>= 1.4.4)
+    backports (3.6.8)
+    benchmark-ips (2.7.2)
+    bindata (2.3.3)
+    buftok (0.2.0)
+    builder (3.2.2)
+    cabin (0.9.0)
+    childprocess (0.5.9)
+      ffi (~> 1.0, >= 1.0.11)
+    chronic_duration (0.10.6)
+      numerizer (~> 0.1.1)
+    ci_reporter (2.0.0)
+      builder (>= 2.1.2)
+    ci_reporter_rspec (1.0.0)
+      ci_reporter (~> 2.0)
+      rspec (>= 2.14, < 4)
+    cinch (2.3.2)
+    clamp (0.6.5)
+    coderay (1.1.1)
+    concurrent-ruby (1.0.0-java)
+    coveralls (0.8.15)
+      json (>= 1.8, < 3)
+      simplecov (~> 0.12.0)
+      term-ansicolor (~> 1.3)
+      thor (~> 0.19.1)
+      tins (>= 1.6.0, < 2)
+    diff-lcs (1.2.5)
+    docile (1.1.5)
+    domain_name (0.5.20160826)
+      unf (>= 0.0.5, < 1.0.0)
+    edn (1.1.1)
+    elasticsearch (1.1.0)
+      elasticsearch-api (= 1.1.0)
+      elasticsearch-transport (= 1.1.0)
+    elasticsearch-api (1.1.0)
+      multi_json
+    elasticsearch-transport (1.1.0)
+      faraday
+      multi_json
+    equalizer (0.0.10)
+    faraday (0.9.2)
+      multipart-post (>= 1.2, < 3)
+    ffi (1.9.14-java)
+    file-dependencies (0.1.6)
+      minitar
+    filesize (0.0.4)
+    filewatch (0.9.0)
+    fivemat (1.3.2)
+    flores (0.0.6)
+    fpm (1.3.3)
+      arr-pm (~> 0.0.9)
+      backports (>= 2.6.2)
+      cabin (>= 0.6.0)
+      childprocess
+      clamp (~> 0.6)
+      ffi
+      json (>= 1.7.7)
+    gelfd (0.2.0)
+    gem_publisher (1.5.0)
+    gems (0.8.3)
+    hitimes (1.2.4-java)
+    http (0.9.9)
+      addressable (~> 2.3)
+      http-cookie (~> 1.0)
+      http-form_data (~> 1.0.1)
+      http_parser.rb (~> 0.6.0)
+    http-cookie (1.0.2)
+      domain_name (~> 0.5)
+    http-form_data (1.0.1)
+    http_parser.rb (0.6.0-java)
+    i18n (0.6.9)
+    insist (1.0.0)
+    jar-dependencies (0.3.5)
+    jls-grok (0.11.4)
+      cabin (>= 0.6.0)
+    jls-lumberjack (0.0.26)
+      concurrent-ruby
+    jmespath (1.3.1)
+    jrjackson (0.4.0-java)
+    jrmonitor (0.4.2)
+    jruby-openssl (0.9.16-java)
+    jruby-stdin-channel (0.2.0-java)
+    json (1.8.3-java)
+    kramdown (1.12.0)
+    logstash-codec-collectd (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-dots (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-edn (3.0.2)
+      edn
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-edn_lines (3.0.2)
+      edn
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-es_bulk (3.0.2)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-fluent (3.0.2-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      msgpack-jruby
+    logstash-codec-graphite (3.0.2)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-json (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-json_lines (3.0.2)
+      logstash-codec-line (>= 2.1.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-line (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-msgpack (3.0.2-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      msgpack-jruby
+    logstash-codec-multiline (3.0.3)
+      jls-grok (~> 0.11.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-patterns-core
+    logstash-codec-netflow (3.1.2)
+      bindata (>= 1.5.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-plain (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-rubydebug (3.0.2)
+      awesome_print
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-devutils (1.1.0-java)
+      fivemat
+      gem_publisher
+      insist (= 1.0.0)
+      kramdown
+      logstash-core-plugin-api (~> 2.0)
+      minitar
+      rake
+      rspec (~> 3.0)
+      rspec-wait
+      stud (>= 0.0.20)
+    logstash-filter-clone (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-csv (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-date (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-dns (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      lru_redux (~> 1.1.0)
+    logstash-filter-drop (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-fingerprint (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      murmurhash3
+    logstash-filter-geoip (4.0.3-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-grok (3.2.2)
+      jls-grok (~> 0.11.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-patterns-core
+    logstash-filter-json (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-kv (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-metrics (4.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      metriks
+      thread_safe
+    logstash-filter-mutate (3.1.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-ruby (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-filter-date
+    logstash-filter-sleep (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-split (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-syslog_pri (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-throttle (4.0.1)
+      atomic
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      thread_safe
+    logstash-filter-urldecode (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-useragent (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      lru_redux (~> 1.1.0)
+      user_agent_parser (>= 2.0.0)
+    logstash-filter-uuid (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-xml (4.0.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      nokogiri
+      xml-simple
+    logstash-input-beats (3.1.6-java)
+      concurrent-ruby (>= 0.9.2, <= 1.0.0)
+      jar-dependencies (~> 0.3.4)
+      logstash-codec-multiline (>= 2.0.5)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      thread_safe (~> 0.3.5)
+    logstash-input-couchdb_changes (3.0.2)
+      json
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (>= 0.0.22)
+    logstash-input-elasticsearch (3.0.2)
+      elasticsearch (~> 1.0, >= 1.0.6)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-exec (3.1.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-file (3.1.2)
+      addressable
+      filewatch (~> 0.8, >= 0.8.1)
+      logstash-codec-multiline (~> 3.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-ganglia (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-gelf (3.0.2)
+      gelfd (= 0.2.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-generator (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-graphite (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-input-tcp
+    logstash-input-heartbeat (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud
+    logstash-input-http (3.0.3)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      puma (~> 2.16, >= 2.16.0)
+      rack (~> 1)
+      stud
+    logstash-input-http_poller (3.1.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-http_client (>= 2.2.4, < 5.0.0)
+      rufus-scheduler (~> 3.0.9)
+      stud (~> 0.0.22)
+    logstash-input-imap (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      mail (~> 2.6.3)
+      mime-types (= 2.6.2)
+      stud (~> 0.0.22)
+    logstash-input-irc (3.0.2)
+      cinch
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-jdbc (4.1.1)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      rufus-scheduler
+      sequel
+      tzinfo
+      tzinfo-data
+    logstash-input-kafka (5.0.5)
+      logstash-codec-json
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (>= 0.0.22, < 0.1.0)
+    logstash-input-log4j (3.0.3-java)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-lumberjack (3.1.1)
+      concurrent-ruby
+      jls-lumberjack (~> 0.0.26)
+      logstash-codec-multiline (~> 3.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-pipe (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-rabbitmq (5.1.1)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-rabbitmq_connection (>= 4.1.1, < 5.0.0)
+    logstash-input-redis (3.1.1)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      redis
+    logstash-input-s3 (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws
+      stud (~> 0.0.18)
+    logstash-input-snmptrap (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      snmp
+    logstash-input-sqs (3.0.2)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+    logstash-input-stdin (3.2.0)
+      concurrent-ruby
+      jruby-stdin-channel
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-syslog (3.1.1)
+      concurrent-ruby
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-filter-date
+      logstash-filter-grok
+      stud (>= 0.0.22, < 0.1.0)
+      thread_safe
+    logstash-input-tcp (4.0.3)
+      logstash-codec-json
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-twitter (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (>= 0.0.22, < 0.1)
+      twitter (= 5.15.0)
+    logstash-input-udp (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-unix (3.0.2)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-xmpp (3.1.1)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      xmpp4r (= 0.5)
+    logstash-mixin-aws (4.2.0)
+      aws-sdk (~> 2.3.0)
+      aws-sdk-v1 (>= 1.61.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-mixin-http_client (4.0.3)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      manticore (>= 0.5.2, < 1.0.0)
+    logstash-mixin-rabbitmq_connection (4.1.2-java)
+      march_hare (~> 2.15.0)
+      stud (~> 0.0.22)
+    logstash-output-cloudwatch (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+      rufus-scheduler (~> 3.0.9)
+    logstash-output-csv (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-filter-json
+      logstash-input-generator
+      logstash-output-file
+    logstash-output-elasticsearch (5.1.1-java)
+      cabin (~> 0.6)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      manticore (>= 0.5.4, < 1.0.0)
+      stud (~> 0.0, >= 0.0.17)
+    logstash-output-file (4.0.1)
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-core-plugin-api (>= 2.0.0, < 2.99)
+    logstash-output-graphite (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-http (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-http_client (>= 2.2.1, < 5.0.0)
+    logstash-output-irc (3.0.2)
+      cinch
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-kafka (5.0.4)
+      logstash-codec-json
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-nagios (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-null (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-pagerduty (3.0.3)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-pipe (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-rabbitmq (4.0.4-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-rabbitmq_connection (>= 4.1.1, < 5.0.0)
+    logstash-output-redis (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      redis
+      stud
+    logstash-output-s3 (3.2.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws
+      stud (~> 0.0.22)
+    logstash-output-sns (4.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+    logstash-output-sqs (3.0.2)
+      aws-sdk
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+      stud
+    logstash-output-statsd (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-input-generator
+      statsd-ruby (= 1.2.0)
+    logstash-output-stdout (3.1.0)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60.1, < 2.99)
+    logstash-output-tcp (4.0.0)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud
+    logstash-output-udp (3.0.2)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-webhdfs (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      snappy (= 0.0.12)
+      webhdfs
+    logstash-output-xmpp (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      xmpp4r (= 0.5)
+    logstash-patterns-core (4.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    lru_redux (1.1.0)
+    mail (2.6.4)
+      mime-types (>= 1.16, < 4)
+    manticore (0.6.0-java)
+    march_hare (2.15.0-java)
+    memoizable (0.4.2)
+      thread_safe (~> 0.3, >= 0.3.1)
+    method_source (0.8.2)
+    metriks (0.9.9.7)
+      atomic (~> 1.0)
+      avl_tree (~> 1.2.0)
+      hitimes (~> 1.1)
+    mime-types (2.6.2)
+    minitar (0.5.4)
+    msgpack-jruby (1.4.1-java)
+    multi_json (1.12.1)
+    multipart-post (2.0.0)
+    murmurhash3 (0.1.6-java)
+    mustache (0.99.8)
+    naught (1.1.0)
+    nokogiri (1.6.8-java)
+    numerizer (0.1.1)
+    octokit (3.8.0)
+      sawyer (~> 0.6.0, >= 0.5.3)
+    pleaserun (0.0.24)
+      cabin (> 0)
+      clamp
+      insist
+      mustache (= 0.99.8)
+      stud
+    polyglot (0.3.5)
+    pry (0.10.4-java)
+      coderay (~> 1.1.0)
+      method_source (~> 0.8.1)
+      slop (~> 3.4)
+      spoon (~> 0.0)
+    puma (2.16.0-java)
+    rack (1.6.4)
+    rack-protection (1.5.3)
+      rack
+    rack-test (0.6.3)
+      rack (>= 1.0)
+    rake (11.3.0)
+    redis (3.3.1)
+    rspec (3.1.0)
+      rspec-core (~> 3.1.0)
+      rspec-expectations (~> 3.1.0)
+      rspec-mocks (~> 3.1.0)
+    rspec-core (3.1.7)
+      rspec-support (~> 3.1.0)
+    rspec-expectations (3.1.2)
+      diff-lcs (>= 1.2.0, < 2.0)
+      rspec-support (~> 3.1.0)
+    rspec-mocks (3.1.3)
+      rspec-support (~> 3.1.0)
+    rspec-support (3.1.2)
+    rspec-wait (0.0.9)
+      rspec (>= 3, < 4)
+    ruby-maven (3.3.12)
+      ruby-maven-libs (~> 3.3.9)
+    ruby-maven-libs (3.3.9)
+    rubyzip (1.1.7)
+    rufus-scheduler (3.0.9)
+      tzinfo
+    sawyer (0.6.0)
+      addressable (~> 2.3.5)
+      faraday (~> 0.8, < 0.10)
+    sequel (4.38.0)
+    simple_oauth (0.3.1)
+    simplecov (0.12.0)
+      docile (~> 1.1.0)
+      json (>= 1.8, < 3)
+      simplecov-html (~> 0.10.0)
+    simplecov-html (0.10.0)
+    sinatra (1.4.7)
+      rack (~> 1.5)
+      rack-protection (~> 1.4)
+      tilt (>= 1.3, < 3)
+    slop (3.6.0)
+    snappy (0.0.12-java)
+      snappy-jars (~> 1.1.0)
+    snappy-jars (1.1.0.1.2-java)
+    snmp (1.2.0)
+    spoon (0.0.6)
+      ffi
+    statsd-ruby (1.2.0)
+    stud (0.0.22)
+    term-ansicolor (1.3.2)
+      tins (~> 1.0)
+    thor (0.19.1)
+    thread_safe (0.3.5-java)
+    tilt (2.0.5)
+    tins (1.6.0)
+    treetop (1.4.15)
+      polyglot
+      polyglot (>= 0.3.1)
+    twitter (5.15.0)
+      addressable (~> 2.3)
+      buftok (~> 0.2.0)
+      equalizer (= 0.0.10)
+      faraday (~> 0.9.0)
+      http (>= 0.4, < 0.10)
+      http_parser.rb (~> 0.6.0)
+      json (~> 1.8)
+      memoizable (~> 0.4.0)
+      naught (~> 1.0)
+      simple_oauth (~> 0.3.0)
+    tzinfo (1.2.2)
+      thread_safe (~> 0.1)
+    tzinfo-data (1.2016.7)
+      tzinfo (>= 1.0.0)
+    unf (0.1.4-java)
+    user_agent_parser (2.3.0)
+    webhdfs (0.8.0)
+      addressable
+    xml-simple (1.1.5)
+    xmpp4r (0.5)
+
+PLATFORMS
+  java
+
+DEPENDENCIES
+  benchmark-ips
+  ci_reporter_rspec (= 1.0.0)
+  coveralls
+  file-dependencies (= 0.1.6)
+  flores (~> 0.0.6)
+  fpm (~> 1.3.3)
+  gems (~> 0.8.3)
+  logstash-codec-collectd
+  logstash-codec-dots
+  logstash-codec-edn
+  logstash-codec-edn_lines
+  logstash-codec-es_bulk
+  logstash-codec-fluent
+  logstash-codec-graphite
+  logstash-codec-json
+  logstash-codec-json_lines
+  logstash-codec-line
+  logstash-codec-msgpack
+  logstash-codec-multiline
+  logstash-codec-netflow
+  logstash-codec-plain
+  logstash-codec-rubydebug
+  logstash-core!
+  logstash-core-event-java!
+  logstash-core-plugin-api!
+  logstash-devutils (~> 1.1)
+  logstash-filter-clone
+  logstash-filter-csv
+  logstash-filter-date
+  logstash-filter-dns
+  logstash-filter-drop
+  logstash-filter-fingerprint
+  logstash-filter-geoip
+  logstash-filter-grok
+  logstash-filter-json
+  logstash-filter-kv
+  logstash-filter-metrics
+  logstash-filter-mutate
+  logstash-filter-ruby
+  logstash-filter-sleep
+  logstash-filter-split
+  logstash-filter-syslog_pri
+  logstash-filter-throttle
+  logstash-filter-urldecode
+  logstash-filter-useragent
+  logstash-filter-uuid
+  logstash-filter-xml
+  logstash-input-beats
+  logstash-input-couchdb_changes
+  logstash-input-elasticsearch
+  logstash-input-exec
+  logstash-input-file
+  logstash-input-ganglia
+  logstash-input-gelf
+  logstash-input-generator
+  logstash-input-graphite
+  logstash-input-heartbeat
+  logstash-input-http
+  logstash-input-http_poller
+  logstash-input-imap
+  logstash-input-irc
+  logstash-input-jdbc
+  logstash-input-kafka
+  logstash-input-log4j
+  logstash-input-lumberjack
+  logstash-input-pipe
+  logstash-input-rabbitmq
+  logstash-input-redis
+  logstash-input-s3
+  logstash-input-snmptrap
+  logstash-input-sqs
+  logstash-input-stdin
+  logstash-input-syslog
+  logstash-input-tcp
+  logstash-input-twitter
+  logstash-input-udp
+  logstash-input-unix
+  logstash-input-xmpp
+  logstash-output-cloudwatch
+  logstash-output-csv
+  logstash-output-elasticsearch
+  logstash-output-file
+  logstash-output-graphite
+  logstash-output-http
+  logstash-output-irc
+  logstash-output-kafka
+  logstash-output-nagios
+  logstash-output-null
+  logstash-output-pagerduty
+  logstash-output-pipe
+  logstash-output-rabbitmq
+  logstash-output-redis
+  logstash-output-s3
+  logstash-output-sns
+  logstash-output-sqs
+  logstash-output-statsd
+  logstash-output-stdout
+  logstash-output-tcp
+  logstash-output-udp
+  logstash-output-webhdfs
+  logstash-output-xmpp
+  octokit (= 3.8.0)
+  pleaserun
+  rack-test
+  rspec (~> 3.1.0)
+  rubyzip (~> 1.1.7)
+  simplecov
+  stud (~> 0.0.22)
+  tins (= 1.6)
+
+BUNDLED WITH
+   1.12.5
diff --git a/config/logstash.yml b/config/logstash.yml
index bce9f417e36..0910bac807c 100644
--- a/config/logstash.yml
+++ b/config/logstash.yml
@@ -85,6 +85,28 @@
 #
 # config.debug: false
 #
+# ------------ Queuing Settings --------------
+#
+# Internal queuing model, "memory" for legacy in-memory based queuing and
+# "persisted" for disk-based acked queueing. Defaults is memory
+#
+# queue.type: memory
+#
+# If using queue.type: persisted, the directory path where the data files will be stored.
+# Default is path.data/queue
+#
+# path.queue:
+#
+# If using queue.type: persisted, the page data files size. The queue data consists of
+# append-only data files separated into pages. Default is 250mb
+#
+# queue.page_capacity: 250mb
+#
+# If using queue.type: persisted, the maximum number of unread events in the queue.
+# Default is 0 (unlimited)
+#
+# queue.max_events: 0
+#
 # ------------ Metrics Settings --------------
 #
 # Bind address for the metrics REST endpoint
diff --git a/dripmain.rb b/dripmain.rb
index 0324150871a..60bfc0435b5 100644
--- a/dripmain.rb
+++ b/dripmain.rb
@@ -3,6 +3,7 @@
 
 require_relative "lib/bootstrap/environment"
 LogStash::Bundler.setup!({:without => [:build]})
+require "logstash-core"
 
 # typical required gems and libs
 require "logstash/environment"
diff --git a/lib/bootstrap/rspec.rb b/lib/bootstrap/rspec.rb
index 4c95f3bfc76..d24c9559553 100755
--- a/lib/bootstrap/rspec.rb
+++ b/lib/bootstrap/rspec.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require_relative "environment"
 LogStash::Bundler.setup!({:without => [:build]})
+require "logstash-core"
 require "logstash/environment"
 
 $LOAD_PATH.unshift(File.join(LogStash::Environment::LOGSTASH_CORE, "spec"))
diff --git a/logstash-core-event-java/build.gradle b/logstash-core-event-java/build.gradle
index d99d30476b4..1ff40a0938a 100644
--- a/logstash-core-event-java/build.gradle
+++ b/logstash-core-event-java/build.gradle
@@ -1,18 +1,13 @@
-buildscript {
-    repositories {
-        mavenLocal()
-        mavenCentral()
-        jcenter()
-    }
-    dependencies {
-        classpath 'net.saliman:gradle-cobertura-plugin:2.2.8'
-    }
-}
+group = 'org.logstash'
+version = '6.0.0-alpha1'
+
+description = "Logstash Core Event Java"
+
+apply plugin: 'java'
+apply plugin: 'idea'
 
 repositories {
-    mavenLocal()
     mavenCentral()
-    jcenter()
 }
 
 gradle.projectsEvaluated {
@@ -22,12 +17,8 @@ gradle.projectsEvaluated {
     }
 }
 
-apply plugin: 'java'
-apply plugin: 'idea'
-
-group = 'org.logstash'
-
 project.sourceCompatibility = 1.8
+project.targetCompatibility = 1.8
 
 task sourcesJar(type: Jar, dependsOn: classes) {
     from sourceSets.main.allSource
@@ -41,9 +32,11 @@ task javadocJar(type: Jar, dependsOn: javadoc) {
     extension 'jar'
 }
 
+// copy jar file into the gem lib dir but without the version number in filename
 task copyGemjar(type: Copy, dependsOn: sourcesJar) {
     from project.jar
     into project.file('lib/logstash-core-event-java/')
+    rename(/(.+)-${project.version}.jar/, '$1.jar')
 }
 
 task cleanGemjar {
@@ -55,6 +48,19 @@ task cleanGemjar {
 clean.dependsOn(cleanGemjar)
 jar.finalizedBy(copyGemjar)
 
+task gemspec_jars << {
+    File gemspec_jars = file("./gemspec_jars.rb")
+    gemspec_jars.newWriter().withWriter { w ->
+        w << "# This file is generated by Gradle as part of the build process. It extracts the build.gradle\n"
+        w << "# runtime dependencies to generate this gemspec dependencies file to be eval'ed by the gemspec\n"
+        w << "# for the jar-dependencies requirements.\n\n"
+        configurations.runtime.allDependencies.each { dependency ->
+            w << "gem.requirements << \"jar ${dependency.group}:${dependency.name}, ${dependency.version}\"\n"
+        }
+    }
+}
+build.finalizedBy(gemspec_jars)
+
 configurations.create('sources')
 configurations.create('javadoc')
 configurations.archives {
@@ -94,10 +100,13 @@ idea {
 dependencies {
     compile 'com.fasterxml.jackson.core:jackson-core:2.7.3'
     compile 'com.fasterxml.jackson.core:jackson-databind:2.7.3'
+    compile 'com.fasterxml.jackson.module:jackson-module-afterburner:2.7.3'
+    compile 'com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:2.7.3'
     compile 'org.apache.logging.log4j:log4j-api:2.6.2'
-    provided 'org.jruby:jruby-core:1.7.25'
     testCompile 'junit:junit:4.12'
     testCompile 'net.javacrumbs.json-unit:json-unit:1.9.0'
+    provided 'org.jruby:jruby-core:1.7.25'
+    provided files('../logstash-core/lib/logstash-core/logstash-core.jar')
 }
 
 // See http://www.gradle.org/docs/current/userguide/gradle_wrapper.html
diff --git a/logstash-core-event-java/gemspec_jars.rb b/logstash-core-event-java/gemspec_jars.rb
new file mode 100644
index 00000000000..9c3acd02195
--- /dev/null
+++ b/logstash-core-event-java/gemspec_jars.rb
@@ -0,0 +1,9 @@
+# This file is generated by Gradle as part of the build process. It extracts the build.gradle
+# runtime dependencies to generate this gemspec dependencies file to be eval'ed by the gemspec
+# for the jar-dependencies requirements.
+
+gem.requirements << "jar com.fasterxml.jackson.core:jackson-core, 2.7.3"
+gem.requirements << "jar com.fasterxml.jackson.core:jackson-databind, 2.7.3"
+gem.requirements << "jar com.fasterxml.jackson.module:jackson-module-afterburner, 2.7.3"
+gem.requirements << "jar com.fasterxml.jackson.dataformat:jackson-dataformat-cbor, 2.7.3"
+gem.requirements << "jar org.apache.logging.log4j:log4j-api, 2.6.2"
diff --git a/logstash-core-event-java/lib/logstash-core-event-java_jars.rb b/logstash-core-event-java/lib/logstash-core-event-java_jars.rb
index eda40d431f0..068f1826201 100644
--- a/logstash-core-event-java/lib/logstash-core-event-java_jars.rb
+++ b/logstash-core-event-java/lib/logstash-core-event-java_jars.rb
@@ -3,12 +3,18 @@
   require 'jar_dependencies'
 rescue LoadError
   require 'com/fasterxml/jackson/core/jackson-databind/2.7.3/jackson-databind-2.7.3.jar'
+  require 'org/apache/logging/log4j/log4j-api/2.6.2/log4j-api-2.6.2.jar'
   require 'com/fasterxml/jackson/core/jackson-annotations/2.7.0/jackson-annotations-2.7.0.jar'
+  require 'com/fasterxml/jackson/module/jackson-module-afterburner/2.7.3/jackson-module-afterburner-2.7.3.jar'
+  require 'com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.7.3/jackson-dataformat-cbor-2.7.3.jar'
   require 'com/fasterxml/jackson/core/jackson-core/2.7.3/jackson-core-2.7.3.jar'
 end
 
 if defined? Jars
   require_jar( 'com.fasterxml.jackson.core', 'jackson-databind', '2.7.3' )
+  require_jar( 'org.apache.logging.log4j', 'log4j-api', '2.6.2' )
   require_jar( 'com.fasterxml.jackson.core', 'jackson-annotations', '2.7.0' )
+  require_jar( 'com.fasterxml.jackson.module', 'jackson-module-afterburner', '2.7.3' )
+  require_jar( 'com.fasterxml.jackson.dataformat', 'jackson-dataformat-cbor', '2.7.3' )
   require_jar( 'com.fasterxml.jackson.core', 'jackson-core', '2.7.3' )
 end
diff --git a/logstash-core-event-java/lib/logstash/event.rb b/logstash-core-event-java/lib/logstash/event.rb
index 38c5d206938..27766aca97f 100644
--- a/logstash-core-event-java/lib/logstash/event.rb
+++ b/logstash-core-event-java/lib/logstash/event.rb
@@ -25,10 +25,14 @@ def flush?; true; end;
     def shutdown?; false; end;
   end
 
-  FLUSH = FlushEvent.new
+  class NoSignal < SignalEvent
+    def flush?; false; end;
+    def shutdown?; false; end;
+  end
 
-  # LogStash::SHUTDOWN is used by plugins
+  FLUSH = FlushEvent.new
   SHUTDOWN = ShutdownEvent.new
+  NO_SIGNAL = NoSignal.new
 
   class Event
     MSG_BRACKETS_METHOD_MISSING = "Direct event field references (i.e. event['field']) have been disabled in favor of using event get and set methods (e.g. event.get('field')). Please consult the Logstash 5.0 breaking changes documentation for more details.".freeze
diff --git a/logstash-core-event-java/logstash-core-event-java.gemspec b/logstash-core-event-java/logstash-core-event-java.gemspec
index 42c190b5f28..89d0bf10577 100644
--- a/logstash-core-event-java/logstash-core-event-java.gemspec
+++ b/logstash-core-event-java/logstash-core-event-java.gemspec
@@ -11,7 +11,7 @@ Gem::Specification.new do |gem|
   gem.homepage      = "http://www.elastic.co/guide/en/logstash/current/index.html"
   gem.license       = "Apache License (2.0)"
 
-  gem.files         = Dir.glob(["logstash-core-event-java.gemspec", "lib/**/*.jar", "lib/**/*.rb", "spec/**/*.rb"])
+  gem.files         = Dir.glob(["logstash-core-event-java.gemspec", "gemspec_jars.rb", "lib/**/*.jar", "lib/**/*.rb", "spec/**/*.rb"])
   gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
   gem.name          = "logstash-core-event-java"
   gem.require_paths = ["lib"]
@@ -26,6 +26,5 @@ Gem::Specification.new do |gem|
   # which does not have this problem.
   gem.add_runtime_dependency "ruby-maven", "~> 3.3.9"
 
-  gem.requirements << "jar com.fasterxml.jackson.core:jackson-core, 2.7.3"
-  gem.requirements << "jar com.fasterxml.jackson.core:jackson-databind, 2.7.3"
+  eval(File.read(File.expand_path("../gemspec_jars.rb", __FILE__)))
 end
diff --git a/logstash-core-event-java/spec/event_spec.rb b/logstash-core-event-java/spec/event_spec.rb
index 0b7d174e782..3402270c92b 100644
--- a/logstash-core-event-java/spec/event_spec.rb
+++ b/logstash-core-event-java/spec/event_spec.rb
@@ -146,6 +146,17 @@
       expect(e.get("[proxy][array][1]")).to eql("baz")
       expect(e.get("[proxy][hash][string]")).to eql("quux")
     end
+
+    it "should fail on non UTF-8 encoding" do
+      # e = LogStash::Event.new
+      # s1 = "\xE0 Montr\xE9al".force_encoding("ISO-8859-1")
+      # expect(s1.encoding.name).to eq("ISO-8859-1")
+      # expect(s1.valid_encoding?).to eq(true)
+      # e.set("test", s1)
+      # s2 = e.get("test")
+      # expect(s2.encoding.name).to eq("UTF-8")
+      # expect(s2.valid_encoding?).to eq(true)
+    end
   end
 
   context "timestamp" do
diff --git a/logstash-core-event-java/src/main/java/org/logstash/Event.java b/logstash-core-event-java/src/main/java/org/logstash/Event.java
index b54806db3dc..a0ad214daf5 100644
--- a/logstash-core-event-java/src/main/java/org/logstash/Event.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/Event.java
@@ -10,6 +10,7 @@
 import org.apache.logging.log4j.Logger;
 import org.joda.time.DateTime;
 import org.jruby.RubySymbol;
+import org.logstash.ackedqueue.Queueable;
 
 import java.io.IOException;
 import java.io.Serializable;
@@ -19,8 +20,11 @@
 import java.util.List;
 import java.util.Map;
 
+import static org.logstash.ObjectMappers.CBOR_MAPPER;
+import static org.logstash.ObjectMappers.JSON_MAPPER;
 
-public class Event implements Cloneable, Serializable {
+
+public class Event implements Cloneable, Serializable, Queueable {
 
     private boolean cancelled;
     private Map<String, Object> data;
@@ -36,6 +40,10 @@ public class Event implements Cloneable, Serializable {
     public static final String TIMESTAMP_FAILURE_FIELD = "_@timestamp";
     public static final String VERSION = "@version";
     public static final String VERSION_ONE = "1";
+    private static final String DATA_MAP_KEY = "DATA";
+    private static final String META_MAP_KEY = "META";
+    private static final String SEQNUM_MAP_KEY = "SEQUENCE_NUMBER";
+
 
     private static final Logger logger = LogManager.getLogger(Event.class);
     private static final ObjectMapper mapper = new ObjectMapper();
@@ -165,10 +173,53 @@ public boolean includes(String reference) {
         }
     }
 
+    public byte[] toBinary() throws IOException {
+        return toBinaryFromMap(toSerializableMap());
+    }
+
+    private Map<String, Map<String, Object>> toSerializableMap() {
+        HashMap<String, Map<String, Object>> hashMap = new HashMap<>();
+        hashMap.put(DATA_MAP_KEY, this.data);
+        hashMap.put(META_MAP_KEY, this.metadata);
+        return hashMap;
+    }
+
+    private byte[] toBinaryFromMap(Map<String, Map<String, Object>> representation) throws IOException {
+        return CBOR_MAPPER.writeValueAsBytes(representation);
+    }
+
+    private static Event fromSerializableMap(Map<String, Map<String, Object>> representation) throws IOException{
+        if (!representation.containsKey(DATA_MAP_KEY)) {
+            throw new IOException("The deserialized Map must contain the \"DATA\" key");
+        }
+        if (!representation.containsKey(META_MAP_KEY)) {
+            throw new IOException("The deserialized Map must contain the \"META\" key");
+        }
+        Map<String, Object> dataMap = representation.get(DATA_MAP_KEY);
+        dataMap.put(METADATA, representation.get(META_MAP_KEY));
+        return new Event(dataMap);
+    }
+
+    public static Event fromBinary(byte[] source) throws IOException {
+        if (source == null || source.length == 0) {
+            return new Event();
+        }
+        return fromSerializableMap(fromBinaryToMap(source));
+    }
+
+    private static Map<String, Map<String, Object>> fromBinaryToMap(byte[] source) throws IOException {
+        Object o = CBOR_MAPPER.readValue(source, HashMap.class);
+        if (o instanceof Map) {
+            return (HashMap<String, Map<String, Object>>) o;
+        } else {
+            throw new IOException("incompatible from binary object type=" + o.getClass().getName() + " , only HashMap is supported");
+        }
+    }
+
     public String toJson()
             throws IOException
     {
-        return mapper.writeValueAsString(this.data);
+        return JSON_MAPPER.writeValueAsString(this.data);
     }
 
     public static Event[] fromJson(String json)
@@ -180,7 +231,7 @@ public static Event[] fromJson(String json)
         }
 
         Event[] result;
-        Object o = mapper.readValue(json, Object.class);
+        Object o = JSON_MAPPER.readValue(json, Object.class);
         // we currently only support Map or Array json objects
         if (o instanceof Map) {
             result = new Event[]{ new Event((Map)o) };
@@ -304,4 +355,21 @@ public void tag(String tag) {
             tags.add(tag);
         }
     }
+
+    public byte[] serialize() throws IOException {
+        Map<String, Map<String, Object>> dataMap = toSerializableMap();
+        return toBinaryFromMap(dataMap);
+    }
+
+    public byte[] serializeWithoutSeqNum() throws IOException {
+        return toBinary();
+    }
+
+    public static Event deserialize(byte[] data) throws IOException {
+        if (data == null || data.length == 0) {
+            return new Event();
+        }
+        Map<String, Map<String, Object>> dataMap = fromBinaryToMap(data);
+        return fromSerializableMap(dataMap);
+    }
 }
diff --git a/logstash-core-event-java/src/main/java/org/logstash/ObjectMappers.java b/logstash-core-event-java/src/main/java/org/logstash/ObjectMappers.java
new file mode 100644
index 00000000000..55cc633b685
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/ObjectMappers.java
@@ -0,0 +1,19 @@
+package org.logstash;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.dataformat.cbor.CBORFactory;
+import com.fasterxml.jackson.dataformat.cbor.CBORGenerator;
+import com.fasterxml.jackson.module.afterburner.AfterburnerModule;
+
+public class ObjectMappers {
+    public static final ObjectMapper JSON_MAPPER = new ObjectMapper();
+    public static final ObjectMapper CBOR_MAPPER = new ObjectMapper(new CBORFactory());
+
+    static {
+        JSON_MAPPER.registerModule(new AfterburnerModule());
+
+        CBORFactory cborf = (CBORFactory) CBOR_MAPPER.getFactory();
+        cborf.configure(CBORGenerator.Feature.WRITE_MINIMAL_INTS, false);
+        CBOR_MAPPER.registerModule(new AfterburnerModule());
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/Timestamp.java b/logstash-core-event-java/src/main/java/org/logstash/Timestamp.java
index f3cea5b560b..15a86d1c240 100644
--- a/logstash-core-event-java/src/main/java/org/logstash/Timestamp.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/Timestamp.java
@@ -3,14 +3,14 @@
 import com.fasterxml.jackson.databind.annotation.JsonSerialize;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
-import org.joda.time.LocalDateTime;
 import org.joda.time.Duration;
+import org.joda.time.LocalDateTime;
 import org.joda.time.format.DateTimeFormatter;
 import org.joda.time.format.ISODateTimeFormat;
 
 import java.util.Date;
 
-@JsonSerialize(using = TimestampSerializer.class)
+@JsonSerialize(using = org.logstash.json.TimestampSerializer.class)
 public class Timestamp implements Cloneable {
 
     // all methods setting the time object must set it in the UTC timezone
diff --git a/logstash-core-event-java/src/main/java/org/logstash/json/TimestampSerializer.java b/logstash-core-event-java/src/main/java/org/logstash/json/TimestampSerializer.java
new file mode 100644
index 00000000000..cc61e45c6e1
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/json/TimestampSerializer.java
@@ -0,0 +1,18 @@
+package org.logstash.json;
+
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.databind.JsonSerializer;
+import com.fasterxml.jackson.databind.SerializerProvider;
+import org.logstash.Timestamp;
+
+import java.io.IOException;
+
+public class TimestampSerializer extends JsonSerializer<Timestamp> {
+
+    @Override
+    public void serialize(Timestamp value, JsonGenerator jgen, SerializerProvider provider)
+            throws IOException
+    {
+        jgen.writeString(value.toIso8601());
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/org/logstash/EventTest.java b/logstash-core-event-java/src/test/java/org/logstash/EventTest.java
index a2714761623..08a330e2b00 100644
--- a/logstash-core-event-java/src/test/java/org/logstash/EventTest.java
+++ b/logstash-core-event-java/src/test/java/org/logstash/EventTest.java
@@ -14,6 +14,69 @@
 import static org.junit.Assert.assertFalse;
 
 public class EventTest {
+    @Test
+    public void queueableInterfaceWithoutSeqNumRoundTrip() throws Exception {
+        Event e = new Event();
+        e.setField("foo", 42L);
+        e.setField("bar", 42);
+        HashMap inner = new HashMap(2);
+        inner.put("innerFoo", 42L);
+        inner.put("innerQuux", 42.42);
+        e.setField("baz", inner);
+        e.setField("[@metadata][foo]", 42L);
+        byte[] binary = e.serializeWithoutSeqNum();
+        Event er = Event.deserialize(binary);
+        assertEquals(42L, er.getField("foo"));
+        assertEquals(42, er.getField("bar"));
+        assertEquals(42L, er.getField("[baz][innerFoo]"));
+        assertEquals(42.42, er.getField("[baz][innerQuux]"));
+        assertEquals(42L, er.getField("[@metadata][foo]"));
+
+        assertEquals(e.getTimestamp().toIso8601(), er.getTimestamp().toIso8601());
+    }
+
+    @Test
+    public void queueableInterfaceRoundTrip() throws Exception {
+        Event e = new Event();
+        e.setField("foo", 42L);
+        e.setField("bar", 42);
+        HashMap inner = new HashMap(2);
+        inner.put("innerFoo", 42L);
+        inner.put("innerQuux", 42.42);
+        e.setField("baz", inner);
+        e.setField("[@metadata][foo]", 42L);
+        byte[] binary = e.serialize();
+        Event er = Event.deserialize(binary);
+        assertEquals(42L, er.getField("foo"));
+        assertEquals(42, er.getField("bar"));
+        assertEquals(42L, er.getField("[baz][innerFoo]"));
+        assertEquals(42.42, er.getField("[baz][innerQuux]"));
+        assertEquals(42L, er.getField("[@metadata][foo]"));
+
+        assertEquals(e.getTimestamp().toIso8601(), er.getTimestamp().toIso8601());
+    }
+
+    @Test
+    public void toBinaryRoundtrip() throws Exception {
+        Event e = new Event();
+        e.setField("foo", 42L);
+        e.setField("bar", 42);
+        HashMap inner = new HashMap(2);
+        inner.put("innerFoo", 42L);
+        inner.put("innerQuux", 42.42);
+        e.setField("baz", inner);
+        e.setField("[@metadata][foo]", 42L);
+        byte[] binary = e.toBinary();
+        Event er = Event.fromBinary(binary);
+        assertEquals(42L, er.getField("foo"));
+        assertEquals(42, er.getField("bar"));
+        assertEquals(42L, er.getField("[baz][innerFoo]"));
+        assertEquals(42.42, er.getField("[baz][innerQuux]"));
+        assertEquals(42L, er.getField("[@metadata][foo]"));
+
+        assertEquals(e.getTimestamp().toIso8601(), er.getTimestamp().toIso8601());
+    }
+
     @Test
     public void testBareToJson() throws Exception {
         Event e = new Event();
diff --git a/logstash-core-queue-jruby/build.gradle b/logstash-core-queue-jruby/build.gradle
new file mode 100644
index 00000000000..4f0ed72692e
--- /dev/null
+++ b/logstash-core-queue-jruby/build.gradle
@@ -0,0 +1,123 @@
+import org.yaml.snakeyaml.Yaml
+
+apply plugin: 'java'
+apply plugin: 'idea'
+
+// fetch version from Logstash's master versions.yml file
+def versionMap = (Map) (new Yaml()).load(new File("$projectDir/../versions.yml").text)
+
+description = "Logstash Core Queue JRuby"
+group 'org.logstash'
+version = versionMap['logstash-core-queue-jruby']
+
+buildscript {
+    repositories {
+        mavenCentral()
+    }
+    dependencies {
+        classpath 'org.yaml:snakeyaml:1.17'
+    }
+}
+
+repositories {
+    mavenCentral()
+}
+
+gradle.projectsEvaluated {
+    tasks.withType(JavaCompile) {
+        options.compilerArgs << "-Xlint:unchecked" << "-Xlint:deprecation"
+    }
+}
+
+project.sourceCompatibility = 1.8
+project.targetCompatibility = 1.8
+
+task sourcesJar(type: org.gradle.api.tasks.bundling.Jar, dependsOn: classes) {
+    from sourceSets.main.allSource
+    classifier 'sources'
+    extension 'jar'
+}
+
+task javadocJar(type: org.gradle.api.tasks.bundling.Jar, dependsOn: javadoc) {
+    from javadoc.destinationDir
+    classifier 'javadoc'
+    extension 'jar'
+}
+
+// copy jar file into the gem lib dir but without the version number in filename
+task copyGemjar(type: org.gradle.api.tasks.Copy, dependsOn: sourcesJar) {
+    from project.jar
+    into project.file('lib/logstash-core-queue-jruby/')
+    rename(/(.+)-${project.version}.jar/, '$1.jar')
+}
+
+task cleanGemjar {
+    delete fileTree(project.file('lib/logstash-core-queue-jruby/')) {
+        include '*.jar'
+    }
+}
+
+clean.dependsOn(cleanGemjar)
+jar.finalizedBy(copyGemjar)
+
+task gemspec_jars << {
+    File gemspec_jars = file("./gemspec_jars.rb")
+    gemspec_jars.newWriter().withWriter { w ->
+        w << "# This file is generated by Gradle as part of the build process. It extracts the build.gradle\n"
+        w << "# runtime dependencies to generate this gemspec dependencies file to be eval'ed by the gemspec\n"
+        w << "# for the jar-dependencies requirements.\n\n"
+        configurations.runtime.allDependencies.each { dependency ->
+            w << "gem.requirements << \"jar ${dependency.group}:${dependency.name}, ${dependency.version}\"\n"
+        }
+    }
+}
+build.finalizedBy(gemspec_jars)
+
+configurations.create('sources')
+configurations.create('javadoc')
+configurations.archives {
+    extendsFrom configurations.sources
+    extendsFrom configurations.javadoc
+}
+
+artifacts {
+    sources(sourcesJar) {
+        // Weird Gradle quirk where type will be used for the extension, but only for sources
+        type 'jar'
+    }
+
+    javadoc(javadocJar) {
+        type 'javadoc'
+    }
+}
+
+configurations {
+    provided
+}
+
+project.sourceSets {
+    main.compileClasspath += project.configurations.provided
+    main.runtimeClasspath += project.configurations.provided
+    test.compileClasspath += project.configurations.provided
+    test.runtimeClasspath += project.configurations.provided
+}
+project.javadoc.classpath += project.configurations.provided
+
+idea {
+    module {
+        scopes.PROVIDED.plus += [project.configurations.provided]
+    }
+}
+
+dependencies {
+    testCompile group: 'junit', name: 'junit', version: '4.12'
+    provided group: 'org.jruby', name: 'jruby-core', version: '1.7.25'
+    provided files('../logstash-core-event-java/lib/logstash-core-event-java/logstash-core-event-java.jar')
+    provided files('../logstash-core/lib/logstash-core/logstash-core.jar')
+}
+
+// See http://www.gradle.org/docs/current/userguide/gradle_wrapper.html
+task wrapper(type: Wrapper) {
+    description = 'Install Gradle wrapper'
+    gradleVersion = '2.8'
+}
diff --git a/logstash-core-queue-jruby/gemspec_jars.rb b/logstash-core-queue-jruby/gemspec_jars.rb
new file mode 100644
index 00000000000..80046e4e63a
--- /dev/null
+++ b/logstash-core-queue-jruby/gemspec_jars.rb
@@ -0,0 +1,4 @@
+# This file is generated by Gradle as part of the build process. It extracts the build.gradle
+# runtime dependencies to generate this gemspec dependencies file to be eval'ed by the gemspec
+# for the jar-dependencies requirements.
+
diff --git a/logstash-core-queue-jruby/lib/logstash-core-queue-jruby/logstash-core-queue-jruby.rb b/logstash-core-queue-jruby/lib/logstash-core-queue-jruby/logstash-core-queue-jruby.rb
new file mode 100644
index 00000000000..25c471f6842
--- /dev/null
+++ b/logstash-core-queue-jruby/lib/logstash-core-queue-jruby/logstash-core-queue-jruby.rb
@@ -0,0 +1,24 @@
+# encoding: utf-8
+
+require "java"
+
+module LogStash
+end
+
+# local dev setup
+classes_dir = File.expand_path("../../../build/classes/main", __FILE__)
+
+if File.directory?(classes_dir)
+  # if in local dev setup, add target to classpath
+  $CLASSPATH << classes_dir unless $CLASSPATH.include?(classes_dir)
+else
+  # otherwise use included jar
+  begin
+    require "logstash-core-queue-jruby/logstash-core-queue-jruby.jar"
+  rescue Exception => e
+    raise("Error loading logstash-core-queue-jruby/logstash-core-queue-jruby.jar file, cause: #{e.message}")
+  end
+end
+
+require "jruby_acked_queue_ext"
+require "jruby_acked_batch_ext"
diff --git a/logstash-core-queue-jruby/lib/logstash-core-queue-jruby/version.rb b/logstash-core-queue-jruby/lib/logstash-core-queue-jruby/version.rb
new file mode 100644
index 00000000000..62f138e2ba6
--- /dev/null
+++ b/logstash-core-queue-jruby/lib/logstash-core-queue-jruby/version.rb
@@ -0,0 +1,3 @@
+# encoding: utf-8
+
+LOGSTASH_CORE_QUEUE_JRUBY_VERSION = "0.0.1"
diff --git a/logstash-core-queue-jruby/logstash-core-queue-jruby.gemspec b/logstash-core-queue-jruby/logstash-core-queue-jruby.gemspec
new file mode 100644
index 00000000000..463d1dc3454
--- /dev/null
+++ b/logstash-core-queue-jruby/logstash-core-queue-jruby.gemspec
@@ -0,0 +1,23 @@
+# -*- encoding: utf-8 -*-
+lib = File.expand_path('../lib', __FILE__)
+$LOAD_PATH.unshift(lib) unless $LOAD_PATH.include?(lib)
+require 'logstash-core-queue-jruby/version'
+
+Gem::Specification.new do |gem|
+  gem.authors       = ["Elastic"]
+  gem.email         = ["info@elastic.co"]
+  gem.description   = %q{The core event component of logstash, the scalable log and event management tool}
+  gem.summary       = %q{logstash-core-event-java - The core event component of logstash}
+  gem.homepage      = "http://www.elastic.co/guide/en/logstash/current/index.html"
+  gem.license       = "Apache License (2.0)"
+
+  gem.files         = Dir.glob(["logstash-core-queue-jruby.gemspec", "gemspec_jars.rb", "lib/**/*.jar", "lib/**/*.rb", "spec/**/*.rb"])
+  gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
+  gem.name          = "logstash-core-queue-jruby"
+  gem.require_paths = ["lib"]
+  gem.version       = LOGSTASH_CORE_QUEUE_JRUBY_VERSION
+
+  gem.platform = "java"
+
+  eval(File.read(File.expand_path("../gemspec_jars.rb", __FILE__)))
+end
diff --git a/logstash-core-queue-jruby/settings.gradle b/logstash-core-queue-jruby/settings.gradle
new file mode 100644
index 00000000000..31c56eb1ad0
--- /dev/null
+++ b/logstash-core-queue-jruby/settings.gradle
@@ -0,0 +1 @@
+rootProject.name = 'logstash-core-queue-jruby'
diff --git a/logstash-core-queue-jruby/src/main/java/JrubyAckedBatchExtService.java b/logstash-core-queue-jruby/src/main/java/JrubyAckedBatchExtService.java
new file mode 100644
index 00000000000..f31aa6089c1
--- /dev/null
+++ b/logstash-core-queue-jruby/src/main/java/JrubyAckedBatchExtService.java
@@ -0,0 +1,14 @@
+import org.jruby.Ruby;
+import org.jruby.runtime.load.BasicLibraryService;
+import org.logstash.ackedqueue.ext.JrubyAckedBatchExtLibrary;
+
+import java.io.IOException;
+
+public class JrubyAckedBatchExtService implements BasicLibraryService {
+    public boolean basicLoad(final Ruby runtime)
+            throws IOException
+    {
+        new JrubyAckedBatchExtLibrary().load(runtime, false);
+        return true;
+    }
+}
diff --git a/logstash-core-queue-jruby/src/main/java/JrubyAckedQueueExtService.java b/logstash-core-queue-jruby/src/main/java/JrubyAckedQueueExtService.java
new file mode 100644
index 00000000000..8b349646e2d
--- /dev/null
+++ b/logstash-core-queue-jruby/src/main/java/JrubyAckedQueueExtService.java
@@ -0,0 +1,16 @@
+import org.jruby.Ruby;
+import org.jruby.runtime.load.BasicLibraryService;
+import org.logstash.ackedqueue.ext.JrubyAckedQueueExtLibrary;
+import org.logstash.ackedqueue.ext.JrubyAckedQueueMemoryExtLibrary;
+
+import java.io.IOException;
+
+public class JrubyAckedQueueExtService implements BasicLibraryService {
+    public boolean basicLoad(final Ruby runtime)
+            throws IOException
+    {
+        new JrubyAckedQueueExtLibrary().load(runtime, false);
+        new JrubyAckedQueueMemoryExtLibrary().load(runtime, false);
+        return true;
+    }
+}
diff --git a/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedBatchExtLibrary.java b/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedBatchExtLibrary.java
new file mode 100644
index 00000000000..cd858b5faa5
--- /dev/null
+++ b/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedBatchExtLibrary.java
@@ -0,0 +1,88 @@
+package org.logstash.ackedqueue.ext;
+
+import org.jruby.Ruby;
+import org.jruby.RubyClass;
+import org.jruby.RubyModule;
+import org.jruby.RubyObject;
+import org.jruby.RubyArray;
+import org.jruby.anno.JRubyClass;
+import org.jruby.anno.JRubyMethod;
+import org.jruby.runtime.ObjectAllocator;
+import org.jruby.runtime.ThreadContext;
+import org.jruby.runtime.builtin.IRubyObject;
+import org.jruby.runtime.load.Library;
+import org.logstash.ackedqueue.Batch;
+import org.logstash.Event;
+import org.logstash.ext.JrubyEventExtLibrary;
+
+import java.io.IOException;
+
+public class JrubyAckedBatchExtLibrary implements Library {
+
+    public void load(Ruby runtime, boolean wrap) throws IOException {
+        RubyModule module = runtime.defineModule("LogStash");
+
+        RubyClass clazz = runtime.defineClassUnder("AckedBatch", runtime.getObject(), new ObjectAllocator() {
+            public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
+                return new RubyAckedBatch(runtime, rubyClass);
+            }
+        }, module);
+
+        clazz.defineAnnotatedMethods(RubyAckedBatch.class);
+    }
+
+    @JRubyClass(name = "AckedBatch", parent = "Object")
+    public static class RubyAckedBatch extends RubyObject {
+        private Batch batch;
+
+        public RubyAckedBatch(Ruby runtime, RubyClass klass) {
+            super(runtime, klass);
+            this.batch = null;
+        }
+
+        public RubyAckedBatch(Ruby runtime, Batch batch) {
+            super(runtime, runtime.getModule("LogStash").getClass("AckedBatch"));
+            this.batch = batch;
+        }
+
+        @SuppressWarnings("unchecked") // for the getList() calls
+        @JRubyMethod(name = "initialize", required = 3)
+        public IRubyObject ruby_initialize(ThreadContext context, IRubyObject events,  IRubyObject seqNums,  IRubyObject queue)
+        {
+            if (! (events instanceof RubyArray)) {
+                context.runtime.newArgumentError("expected events array");
+            }
+            if (! (seqNums instanceof RubyArray)) {
+                context.runtime.newArgumentError("expected seqNums array");
+            }
+            if (! (queue instanceof JrubyAckedQueueExtLibrary.RubyAckedQueue)) {
+                context.runtime.newArgumentError("expected queue AckedQueue");
+            }
+
+            this.batch = new Batch(((RubyArray)events).getList(), ((RubyArray)seqNums).getList(), ((JrubyAckedQueueExtLibrary.RubyAckedQueue)queue).getQueue());
+
+            return context.nil;
+        }
+
+        @JRubyMethod(name = "get_elements")
+        public IRubyObject ruby_get_elements(ThreadContext context)
+        {
+            RubyArray result = context.runtime.newArray();
+            this.batch.getElements().forEach(e -> result.add(new JrubyEventExtLibrary.RubyEvent(context.runtime, (Event)e)));
+
+            return result;
+        }
+
+        @JRubyMethod(name = "close")
+        public IRubyObject ruby_close(ThreadContext context)
+        {
+            try {
+                this.batch.close();
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            return context.nil;
+        }
+    }
+}
diff --git a/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java b/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java
new file mode 100644
index 00000000000..8b73d837c59
--- /dev/null
+++ b/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java
@@ -0,0 +1,140 @@
+package org.logstash.ackedqueue.ext;
+
+import org.logstash.Event;
+import org.logstash.ext.JrubyEventExtLibrary;
+import org.jruby.Ruby;
+import org.jruby.RubyClass;
+import org.jruby.RubyFixnum;
+import org.jruby.RubyModule;
+import org.jruby.RubyObject;
+import org.jruby.anno.JRubyClass;
+import org.jruby.anno.JRubyMethod;
+import org.jruby.runtime.Arity;
+import org.jruby.runtime.ObjectAllocator;
+import org.jruby.runtime.ThreadContext;
+import org.jruby.runtime.builtin.IRubyObject;
+import org.jruby.runtime.load.Library;
+import org.logstash.ackedqueue.Batch;
+import org.logstash.ackedqueue.FileSettings;
+import org.logstash.ackedqueue.Queue;
+import org.logstash.ackedqueue.Settings;
+import org.logstash.common.io.CheckpointIOFactory;
+import org.logstash.common.io.FileCheckpointIO;
+import org.logstash.common.io.MmapPageIO;
+import org.logstash.common.io.PageIOFactory;
+
+import java.io.IOException;
+
+public class JrubyAckedQueueExtLibrary implements Library {
+
+    public void load(Ruby runtime, boolean wrap) throws IOException {
+        RubyModule module = runtime.defineModule("LogStash");
+
+        RubyClass clazz = runtime.defineClassUnder("AckedQueue", runtime.getObject(), new ObjectAllocator() {
+            public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
+                return new RubyAckedQueue(runtime, rubyClass);
+            }
+        }, module);
+
+        clazz.defineAnnotatedMethods(RubyAckedQueue.class);
+    }
+
+    // TODO:
+    // as a simplified first prototyping implementation, the Settings class is not exposed and the queue elements
+    // are assumed to be logstash Event.
+
+
+    @JRubyClass(name = "AckedQueue", parent = "Object")
+    public static class RubyAckedQueue extends RubyObject {
+        private Queue queue;
+
+        public RubyAckedQueue(Ruby runtime, RubyClass klass) {
+            super(runtime, klass);
+        }
+
+        public Queue getQueue() {
+            return this.queue;
+        }
+
+        // def initialize
+        @JRubyMethod(name = "initialize", optional = 3)
+        public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args)
+        {
+            args = Arity.scanArgs(context.runtime, args, 3, 0);
+
+            int capacity = RubyFixnum.num2int(args[1]);
+            int maxUnread = RubyFixnum.num2int(args[2]);
+
+            Settings s = new FileSettings(args[0].asJavaString());
+            PageIOFactory pageIOFactory = (pageNum, size, path) -> new MmapPageIO(pageNum, size, path);
+            CheckpointIOFactory checkpointIOFactory = (source) -> new FileCheckpointIO(source);
+            s.setCapacity(capacity);
+            s.setMaxUnread(maxUnread);
+            s.setElementIOFactory(pageIOFactory);
+            s.setCheckpointIOFactory(checkpointIOFactory);
+            s.setElementClass(Event.class);
+
+            this.queue = new Queue(s);
+
+            return context.nil;
+        }
+
+        @JRubyMethod(name = "open")
+        public IRubyObject ruby_open(ThreadContext context)
+        {
+            try {
+                this.queue.open();
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            return context.nil;
+        }
+
+        @JRubyMethod(name = {"write", "<<"}, required = 1)
+        public IRubyObject ruby_write(ThreadContext context, IRubyObject event)
+        {
+            if (!(event instanceof JrubyEventExtLibrary.RubyEvent)) {
+                throw context.runtime.newTypeError("wrong argument type " + event.getMetaClass() + " (expected LogStash::Event)");
+            }
+
+            long seqNum;
+            try {
+                seqNum = this.queue.write(((JrubyEventExtLibrary.RubyEvent) event).getEvent());
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            return context.runtime.newFixnum(seqNum);
+        }
+
+        @JRubyMethod(name = "read_batch", required = 2)
+        public IRubyObject ruby_read_batch(ThreadContext context, IRubyObject limit, IRubyObject timeout)
+        {
+            Batch b;
+
+            try {
+                b = this.queue.readBatch(RubyFixnum.num2int(limit), RubyFixnum.num2int(timeout));
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            // TODO: return proper Batch object
+            return (b == null) ? context.nil : new JrubyAckedBatchExtLibrary.RubyAckedBatch(context.runtime, b);
+        }
+
+
+        @JRubyMethod(name = "close")
+        public IRubyObject ruby_close(ThreadContext context)
+        {
+            try {
+                this.queue.close();
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            return context.nil;
+        }
+
+    }
+}
diff --git a/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueMemoryExtLibrary.java b/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueMemoryExtLibrary.java
new file mode 100644
index 00000000000..3ba29210867
--- /dev/null
+++ b/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueMemoryExtLibrary.java
@@ -0,0 +1,141 @@
+package org.logstash.ackedqueue.ext;
+
+import org.logstash.Event;
+import org.logstash.ext.JrubyEventExtLibrary;
+import org.jruby.Ruby;
+import org.jruby.RubyClass;
+import org.jruby.RubyFixnum;
+import org.jruby.RubyModule;
+import org.jruby.RubyObject;
+import org.jruby.anno.JRubyClass;
+import org.jruby.anno.JRubyMethod;
+import org.jruby.runtime.Arity;
+import org.jruby.runtime.ObjectAllocator;
+import org.jruby.runtime.ThreadContext;
+import org.jruby.runtime.builtin.IRubyObject;
+import org.jruby.runtime.load.Library;
+import org.logstash.ackedqueue.Batch;
+import org.logstash.ackedqueue.MemorySettings;
+import org.logstash.ackedqueue.Queue;
+import org.logstash.ackedqueue.Settings;
+import org.logstash.common.io.ByteBufferPageIO;
+import org.logstash.common.io.CheckpointIOFactory;
+import org.logstash.common.io.MemoryCheckpointIO;
+import org.logstash.common.io.PageIOFactory;
+
+import java.io.IOException;
+
+public class JrubyAckedQueueMemoryExtLibrary implements Library {
+
+    public void load(Ruby runtime, boolean wrap) throws IOException {
+        RubyModule module = runtime.defineModule("LogStash");
+
+        RubyClass clazz = runtime.defineClassUnder("AckedMemoryQueue", runtime.getObject(), new ObjectAllocator() {
+            public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
+                return new RubyAckedMemoryQueue(runtime, rubyClass);
+            }
+        }, module);
+
+        clazz.defineAnnotatedMethods(RubyAckedMemoryQueue.class);
+    }
+
+    // TODO:
+    // as a simplified first prototyping implementation, the Settings class is not exposed and the queue elements
+    // are assumed to be logstash Event.
+
+
+    @JRubyClass(name = "AckedMemoryQueue", parent = "Object")
+    public static class RubyAckedMemoryQueue extends RubyObject {
+        private Queue queue;
+
+        public RubyAckedMemoryQueue(Ruby runtime, RubyClass klass) {
+            super(runtime, klass);
+        }
+
+        public Queue getQueue() {
+            return this.queue;
+        }
+
+        // def initialize
+        @JRubyMethod(name = "initialize", optional = 3)
+        public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args)
+        {
+            args = Arity.scanArgs(context.runtime, args, 3, 0);
+
+            int capacity = RubyFixnum.num2int(args[1]);
+            int maxUnread = RubyFixnum.num2int(args[2]);
+
+            Settings s = new MemorySettings(args[0].asJavaString());
+            PageIOFactory pageIOFactory = (pageNum, size, path) -> new ByteBufferPageIO(pageNum, size, path);
+            CheckpointIOFactory checkpointIOFactory = (source) -> new MemoryCheckpointIO(source);
+            s.setCapacity(capacity);
+            s.setMaxUnread(maxUnread);
+            s.setElementIOFactory(pageIOFactory);
+            s.setCheckpointIOFactory(checkpointIOFactory);
+            s.setElementClass(Event.class);
+
+            this.queue = new Queue(s);
+
+            return context.nil;
+        }
+
+        @JRubyMethod(name = "open")
+        public IRubyObject ruby_open(ThreadContext context)
+        {
+            try {
+                this.queue.getCheckpointIO().purge();
+                this.queue.open();
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            return context.nil;
+        }
+
+        @JRubyMethod(name = {"write", "<<"}, required = 1)
+        public IRubyObject ruby_write(ThreadContext context, IRubyObject event)
+        {
+            if (!(event instanceof JrubyEventExtLibrary.RubyEvent)) {
+                throw context.runtime.newTypeError("wrong argument type " + event.getMetaClass() + " (expected LogStash::Event)");
+            }
+
+            long seqNum;
+            try {
+                seqNum = this.queue.write(((JrubyEventExtLibrary.RubyEvent) event).getEvent());
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            return context.runtime.newFixnum(seqNum);
+        }
+
+        @JRubyMethod(name = "read_batch", required = 2)
+        public IRubyObject ruby_read_batch(ThreadContext context, IRubyObject limit, IRubyObject timeout)
+        {
+            Batch b;
+
+            try {
+                b = this.queue.readBatch(RubyFixnum.num2int(limit), RubyFixnum.num2int(timeout));
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            // TODO: return proper Batch object
+            return (b == null) ? context.nil : new JrubyAckedBatchExtLibrary.RubyAckedBatch(context.runtime, b);
+        }
+
+
+        @JRubyMethod(name = "close")
+        public IRubyObject ruby_close(ThreadContext context)
+        {
+            try {
+                this.queue.close();
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            return context.nil;
+        }
+
+    }
+}
diff --git a/logstash-core/build.gradle b/logstash-core/build.gradle
index c45ad34920c..ab442f40b75 100644
--- a/logstash-core/build.gradle
+++ b/logstash-core/build.gradle
@@ -1,32 +1,98 @@
-import java.nio.file.Files
 import org.yaml.snakeyaml.Yaml
-import static java.nio.file.StandardCopyOption.REPLACE_EXISTING
 
 apply plugin: 'java'
 apply plugin: 'idea'
 
-group = 'org.logstash'
-description = """Logstash Core Java"""
-
-sourceCompatibility = 1.8
-targetCompatibility = 1.8
-
 // fetch version from Logstash's master versions.yml file
 def versionMap = (Map) (new Yaml()).load(new File("$projectDir/../versions.yml").text)
+
+group = 'org.logstash'
+description = """Logstash Core Java"""
 version = versionMap['logstash-core']
 
+repositories {
+    mavenCentral()
+}
+
 buildscript {
     repositories {
         mavenCentral()
     }
-
     dependencies {
         classpath 'org.yaml:snakeyaml:1.17'
     }
 }
 
+gradle.projectsEvaluated {
+    tasks.withType(JavaCompile) {
+        options.compilerArgs << "-Xlint:deprecation"
+//        options.compilerArgs << "-Xlint:unchecked" << "-Xlint:deprecation"
+    }
+}
+
+project.sourceCompatibility = 1.8
+project.targetCompatibility = 1.8
+
+task sourcesJar(type: org.gradle.api.tasks.bundling.Jar, dependsOn: classes) {
+    from sourceSets.main.allSource
+    classifier 'sources'
+    extension 'jar'
+}
+
+task javadocJar(type: org.gradle.api.tasks.bundling.Jar, dependsOn: javadoc) {
+    from javadoc.destinationDir
+    classifier 'javadoc'
+    extension 'jar'
+}
+
+// copy jar file into the gem lib dir but without the version number in filename
+task copyGemjar(type: org.gradle.api.tasks.Copy, dependsOn: sourcesJar) {
+    from project.jar
+    into project.file('lib/logstash-core/')
+    rename(/(.+)-${project.version}.jar/, '$1.jar')
+}
+
+task cleanGemjar {
+    delete fileTree(project.file('lib/logstash-core/')) {
+        include '*.jar'
+    }
+}
+
+clean.dependsOn(cleanGemjar)
+jar.finalizedBy(copyGemjar)
+
+task gemspec_jars << {
+    File gemspec_jars = file("./gemspec_jars.rb")
+    gemspec_jars.newWriter().withWriter { w ->
+        w << "# This file is generated by Gradle as part of the build process. It extracts the build.gradle\n"
+        w << "# runtime dependencies to generate this gemspec dependencies file to be eval'ed by the gemspec\n"
+        w << "# for the jar-dependencies requirements.\n\n"
+        configurations.runtime.allDependencies.each { dependency ->
+            w << "gem.requirements << \"jar ${dependency.group}:${dependency.name}, ${dependency.version}\"\n"
+        }
+    }
+}
+build.finalizedBy(gemspec_jars)
+
+configurations.create('sources')
+configurations.create('javadoc')
+configurations.archives {
+    extendsFrom configurations.sources
+    extendsFrom configurations.javadoc
+}
+
+artifacts {
+    sources(sourcesJar) {
+        // Weird Gradle quirk where type will be used for the extension, but only for sources
+        type 'jar'
+    }
+    javadoc(javadocJar) {
+        type 'javadoc'
+    }
+}
+
 configurations {
-  provided
+    provided
 }
 
 project.sourceSets {
@@ -37,52 +103,27 @@ project.sourceSets {
 }
 project.javadoc.classpath += project.configurations.provided
 
-repositories {
-  mavenCentral()
-}
-
-dependencies {
-  runtime 'org.apache.logging.log4j:log4j-1.2-api:2.6.2'
-  compile 'org.apache.logging.log4j:log4j-api:2.6.2'
-  compile 'org.apache.logging.log4j:log4j-core:2.6.2'
-  compile 'com.fasterxml.jackson.core:jackson-core:2.7.4'
-  compile 'com.fasterxml.jackson.core:jackson-databind:2.7.4'
-  testCompile 'org.apache.logging.log4j:log4j-core:2.6.2:tests'
-  testCompile 'org.apache.logging.log4j:log4j-api:2.6.2:tests'
-  testCompile 'junit:junit:4.12'
-  provided 'org.jruby:jruby-core:1.7.25'
-}
-
 idea {
     module {
         scopes.PROVIDED.plus += [project.configurations.provided]
     }
 }
 
-task generateGemJarRequiresFile << {
-  File jars_file = file('lib/jars.rb')
-  jars_file.newWriter().withWriter { w ->
-    w << "require \'jar_dependencies\'\n"
-    configurations.runtime.allDependencies.each {
-      w << "require_jar(\'${it.group}\', \'${it.name}\', \'${it.version}\')\n"
-    }
-    w << "require_jar(\'${project.group}\', \'${project.name}\', \'${project.version}\')\n"
-  }
+dependencies {
+    runtime 'org.apache.logging.log4j:log4j-1.2-api:2.6.2'
+    compile 'org.apache.logging.log4j:log4j-api:2.6.2'
+    compile 'org.apache.logging.log4j:log4j-core:2.6.2'
+    compile 'com.fasterxml.jackson.core:jackson-core:2.7.4'
+    compile 'com.fasterxml.jackson.core:jackson-databind:2.7.4'
+    testCompile 'org.apache.logging.log4j:log4j-core:2.6.2:tests'
+    testCompile 'org.apache.logging.log4j:log4j-api:2.6.2:tests'
+    testCompile 'junit:junit:4.12'
+    provided 'org.jruby:jruby-core:1.7.25'
+    //provided files('../logstash-core-event-java/build/libs/logstash-core-event-java.jar')
 }
 
-task vendor << {
-    String vendorPathPrefix = "vendor/jars"
-    configurations.runtime.allDependencies.each { dep ->
-      File f = configurations.runtime.filter { it.absolutePath.contains("${dep.group}/${dep.name}/${dep.version}") }.singleFile
-      String groupPath = dep.group.replaceAll('\\.', '/')
-      File newJarFile = file("${vendorPathPrefix}/${groupPath}/${dep.name}/${dep.version}/${dep.name}-${dep.version}.jar")
-      newJarFile.mkdirs()
-      Files.copy(f.toPath(), newJarFile.toPath(), REPLACE_EXISTING)
-    }
-    String projectGroupPath = project.group.replaceAll('\\.', '/')
-    File projectJarFile = file("${vendorPathPrefix}/${projectGroupPath}/${project.name}/${project.version}/${project.name}-${project.version}.jar")
-    projectJarFile.mkdirs()
-    Files.copy(file("$buildDir/libs/${project.name}-${project.version}.jar").toPath(), projectJarFile.toPath(), REPLACE_EXISTING)
+// See http://www.gradle.org/docs/current/userguide/gradle_wrapper.html
+task wrapper(type: Wrapper) {
+    description = 'Install Gradle wrapper'
+    gradleVersion = '2.8'
 }
-
-vendor.dependsOn(jar, generateGemJarRequiresFile)
diff --git a/logstash-core/gemspec_jars.rb b/logstash-core/gemspec_jars.rb
new file mode 100644
index 00000000000..15f2967a4b6
--- /dev/null
+++ b/logstash-core/gemspec_jars.rb
@@ -0,0 +1,9 @@
+# This file is generated by Gradle as part of the build process. It extracts the build.gradle
+# runtime dependencies to generate this gemspec dependencies file to be eval'ed by the gemspec
+# for the jar-dependencies requirements.
+
+gem.requirements << "jar org.apache.logging.log4j:log4j-1.2-api, 2.6.2"
+gem.requirements << "jar org.apache.logging.log4j:log4j-api, 2.6.2"
+gem.requirements << "jar org.apache.logging.log4j:log4j-core, 2.6.2"
+gem.requirements << "jar com.fasterxml.jackson.core:jackson-core, 2.7.4"
+gem.requirements << "jar com.fasterxml.jackson.core:jackson-databind, 2.7.4"
diff --git a/logstash-core/lib/jars.rb b/logstash-core/lib/jars.rb
deleted file mode 100644
index b55fbe05c49..00000000000
--- a/logstash-core/lib/jars.rb
+++ /dev/null
@@ -1,7 +0,0 @@
-require 'jar_dependencies'
-require_jar('org.apache.logging.log4j', 'log4j-1.2-api', '2.6.2')
-require_jar('org.apache.logging.log4j', 'log4j-api', '2.6.2')
-require_jar('org.apache.logging.log4j', 'log4j-core', '2.6.2')
-require_jar('com.fasterxml.jackson.core', 'jackson-core', '2.7.4')
-require_jar('com.fasterxml.jackson.core', 'jackson-databind', '2.7.4')
-require_jar('org.logstash', 'logstash-core', '6.0.0-alpha1')
diff --git a/logstash-core/lib/logstash-core/logstash-core.rb b/logstash-core/lib/logstash-core/logstash-core.rb
index 74f073326eb..1d39ad9e80f 100644
--- a/logstash-core/lib/logstash-core/logstash-core.rb
+++ b/logstash-core/lib/logstash-core/logstash-core.rb
@@ -1,3 +1,23 @@
 # encoding: utf-8
+
+require "java"
+
 module LogStash
 end
+
+require "logstash-core_jars"
+
+# local dev setup
+classes_dir = File.expand_path("../../../build/classes/main", __FILE__)
+
+if File.directory?(classes_dir)
+  # if in local dev setup, add target to classpath
+  $CLASSPATH << classes_dir unless $CLASSPATH.include?(classes_dir)
+else
+  # otherwise use included jar
+  begin
+    require "logstash-core/logstash-core.jar"
+  rescue Exception => e
+    raise("Error loading logstash-core/logstash-core.jar file, cause: #{e.message}")
+  end
+end
diff --git a/logstash-core/lib/logstash-core_jars.rb b/logstash-core/lib/logstash-core_jars.rb
new file mode 100644
index 00000000000..f548c9fd35c
--- /dev/null
+++ b/logstash-core/lib/logstash-core_jars.rb
@@ -0,0 +1,20 @@
+# this is a generated file, to avoid over-writing it just delete this comment
+begin
+  require 'jar_dependencies'
+rescue LoadError
+  require 'org/apache/logging/log4j/log4j-core/2.6.2/log4j-core-2.6.2.jar'
+  require 'org/apache/logging/log4j/log4j-api/2.6.2/log4j-api-2.6.2.jar'
+  require 'com/fasterxml/jackson/core/jackson-core/2.7.4/jackson-core-2.7.4.jar'
+  require 'org/apache/logging/log4j/log4j-1.2-api/2.6.2/log4j-1.2-api-2.6.2.jar'
+  require 'com/fasterxml/jackson/core/jackson-annotations/2.7.0/jackson-annotations-2.7.0.jar'
+  require 'com/fasterxml/jackson/core/jackson-databind/2.7.4/jackson-databind-2.7.4.jar'
+end
+
+if defined? Jars
+  require_jar( 'org.apache.logging.log4j', 'log4j-core', '2.6.2' )
+  require_jar( 'org.apache.logging.log4j', 'log4j-api', '2.6.2' )
+  require_jar( 'com.fasterxml.jackson.core', 'jackson-core', '2.7.4' )
+  require_jar( 'org.apache.logging.log4j', 'log4j-1.2-api', '2.6.2' )
+  require_jar( 'com.fasterxml.jackson.core', 'jackson-annotations', '2.7.0' )
+  require_jar( 'com.fasterxml.jackson.core', 'jackson-databind', '2.7.4' )
+end
diff --git a/logstash-core/lib/logstash/certs/cacert.pem b/logstash-core/lib/logstash/certs/cacert.pem
index 99b310bce91..e26e9e4d6c7 100644
--- a/logstash-core/lib/logstash/certs/cacert.pem
+++ b/logstash-core/lib/logstash/certs/cacert.pem
@@ -5,8 +5,8 @@
 ##
 ## This is a bundle of X.509 certificates of public Certificate Authorities
 ## (CA). These were automatically extracted from Mozilla's root certificates
-## file (certdata.txt).  This file can be found in the mozilla source tree:
-## http://mxr.mozilla.org/mozilla/source/security/nss/lib/ckfw/builtins/certdata.txt?raw=1
+## file (certdata.txt).  This file can be found in the mozilla producer tree:
+## http://mxr.mozilla.org/mozilla/producer/security/nss/lib/ckfw/builtins/certdata.txt?raw=1
 ##
 ## It contains the certificates in PEM format and therefore
 ## can be directly used with curl / libcurl / php_curl, or with
diff --git a/logstash-core/lib/logstash/compiler.rb b/logstash-core/lib/logstash/compiler.rb
new file mode 100644
index 00000000000..35b0c798e14
--- /dev/null
+++ b/logstash-core/lib/logstash/compiler.rb
@@ -0,0 +1,199 @@
+require 'logstash/util/loggable'
+require 'logstash/output_delegator_strategy_registry'
+
+java_import org.logstash.config.pipeline.Pipeline
+java_import org.logstash.config.pipeline.PipelineRunner;
+java_import org.logstash.config.pipeline.pipette.PipetteExecutionException;
+java_import org.logstash.config.pipeline.pipette.PipetteSourceEmitter;
+java_import org.logstash.config.ir.graph.Graph;
+java_import org.logstash.config.ir.graph.PluginVertex;
+java_import java.util.concurrent.SynchronousQueue;
+java_import org.logstash.config.compiler.RubyExpressionCompiler;
+java_import org.logstash.config.pipeline.pipette.PipetteSourceEmitter
+
+module LogStash; class Compiler
+  include ::LogStash::Util::Loggable
+
+  class CompiledRubyInput
+    include org.logstash.config.compiler.compiled.ICompiledInputPlugin
+
+    class QueueEmitterAdapter
+      def initialize(emitter)
+        @emitter = emitter
+      end
+
+      def push(event)
+        push_batch([event])
+      end
+      alias_method(:<<, :push)
+
+      def push_batch(events)
+        @emitter.emit(events)
+      end
+    end
+
+    def initialize(plugin, vertex)
+      @plugin = plugin
+      @vertex = vertex
+    end
+
+    def register
+      @plugin.register
+    end
+
+    def start
+      if @emitter.nil?
+        raise "Could not start! Emitter not defined! #{self}"
+      end
+      @plugin.run(QueueEmitterAdapter.new(@emitter))
+    end
+
+    def stop
+      @plugin.do_stop
+    end
+
+    def onEvents(emitter)
+      @emitter = emitter
+    end
+  end
+
+  class CompiledRubyProcessor
+    include org.logstash.config.compiler.compiled.ICompiledProcessor
+
+    def initialize(plugin, vertex)
+      @plugin = plugin
+      @vertex = vertex
+      # We run this a lot, so it's easier to just clone this on every
+      # Invocation rather than worry about jruby efficiently invoking this method
+      @outgoing_edges = vertex.getOutgoingEdges()
+    end
+
+    def register
+      @plugin.register
+    end
+
+    def stop
+      @plugin.do_close
+    end
+
+    def map_processed(processed)
+      result_map = {}
+      @outgoing_edges.each {|edge| result_map[edge] = processed}
+      result_map
+    end
+  end
+
+  class CompiledRubyFilter < CompiledRubyProcessor
+    def process(events)
+      map_processed(@plugin.multi_filter(events))
+    end
+  end
+
+  class CompiledRubyOutput < CompiledRubyProcessor
+    def process(events)
+      map_processed(@plugin.multi_receive(events))
+    end
+  end
+
+  class StandardPluginCompiler
+    include ::LogStash::Util::Loggable
+    include org.logstash.config.compiler.IPluginCompiler;
+
+    attr_reader :pipeline_id, :pipeline_metric
+
+    def initialize(pipeline_id, pipeline_metric)
+      @pipeline_id = pipeline_id
+      @pipeline_metric = pipeline_metric
+    end
+
+    def compileInput(vertex)
+      definition = vertex.getPluginDefinition
+      klass = lookup_plugin_class(definition)
+
+      input_plugin = klass.new(definition.getArguments)
+      input_plugin.metric = plugin_scoped_metric(definition)
+
+      CompiledRubyInput.new(input_plugin, vertex)
+    end
+
+    def compileFilter(vertex)
+      definition = vertex.getPluginDefinition
+      klass = lookup_plugin_class(definition)
+      delegator = FilterDelegator.new(logger, klass, plugin_scoped_metric(definition), definition.getArguments)
+      CompiledRubyFilter.new(delegator, vertex)
+    end
+
+    def compileOutput(vertex)
+      definition = vertex.getPluginDefinition
+      klass = lookup_plugin_class(definition)
+      delegator = OutputDelegator.new(logger, klass, plugin_scoped_metric(definition),
+                                      OutputDelegatorStrategyRegistry.instance,
+                                      definition.getArguments)
+      CompiledRubyOutput.new(delegator, vertex)
+    end
+
+    def plugin_scoped_metric(definition)
+      pipeline_metric.namespace([(definition_type_string(definition) + "s").to_sym,
+                                  definition.getId()])
+    end
+
+
+    def lookup_plugin_class(definition)
+      Plugin.lookup(definition_type_string(definition), definition.getName)
+    end
+
+    def definition_type_string(definition)
+      definition.getType.to_s.downcase
+    end
+  end
+
+  def self.compile(config_str, source_file=nil, pipeline_id=nil, metric=nil)
+    pipeline_id ||= :main
+    metric ||= LogStash::Instrument::NullMetric.new
+
+    pipeline = compile_pipeline(config_str, source_file)
+    pipeline_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :plugins])
+    plugin_compiler = StandardPluginCompiler.new(pipeline_id, pipeline_metric)
+
+    runner = org.logstash.config.pipeline.PipelineRunner.new(
+      pipeline, queue, expression_compiler, plugin_compiler, nil
+    )
+  end
+
+  def self.compile_pipeline(config_str, source_file=nil)
+    graph_sections = self.compile_graph(config_str, source_file)
+    pipeline = org.logstash.config.pipeline.Pipeline.new(
+      graph_sections[:input],
+      graph_sections[:filter],
+      graph_sections[:output]
+    )
+  end
+
+  def self.queue
+    queue = java.util.concurrent.SynchronousQueue.new
+  end
+
+  def self.expression_compiler
+    org.logstash.config.compiler.RubyExpressionCompiler.new
+  end
+
+
+  def self.compile_ast(config_str, source_file=nil)
+    grammar = LogStashConfigParser.new
+    config = grammar.parse(config_str)
+
+    if config.nil?
+      raise ConfigurationError, grammar.failure_reason
+    end
+
+    config
+  end
+
+  def self.compile_imperative(config_str, source_file=nil)
+    compile_ast(config_str, source_file).compile(source_file)
+  end
+
+  def self.compile_graph(config_str, source_file=nil)
+    Hash[compile_imperative(config_str, source_file).map {|section,icompiled| [section, icompiled.toGraph]}]
+  end
+end; end
diff --git a/logstash-core/lib/logstash/config/config_ast.rb b/logstash-core/lib/logstash/config/config_ast.rb
index ce6295a6933..c40a5e44cd9 100644
--- a/logstash-core/lib/logstash/config/config_ast.rb
+++ b/logstash-core/lib/logstash/config/config_ast.rb
@@ -1,15 +1,54 @@
 # encoding: utf-8
 require 'logstash/errors'
 require "treetop"
+java_import Java::OrgLogstashConfigIr::DSL
+java_import Java::OrgLogstashConfigIr::SourceMetadata
 
-class Treetop::Runtime::SyntaxNode
+module LogStash; module Config; module AST
+  module JDSL
+    attr_writer :source_file
+    def source_meta
+      line, column = line_and_column
+      Java::OrgLogstashConfigIr::SourceMetadata.new(source_file, line, column, self.text_value)
+    end
+
+    def source_file
+      return @source_file if @source_file
+      return self.parent.source_file if self.parent
+      nil
+    end
+
+    def compose(*statements)
+      compose_for(section_type.to_sym).call(source_meta, *statements)
+    end
+
+    def compose_for(section_sym)
+      if section_sym == :filter
+        jdsl.method(:iComposeSequence)
+      else
+        jdsl.method(:iComposeParallel)
+      end
+    end
+
+    def line_and_column
+      start = self.interval.first
+      [self.input.line_of(start), self.input.column_of(start)]
+    end
 
-  def compile
-    return "" if elements.nil?
-    return elements.collect(&:compile).reject(&:empty?).join("")
+    def empty_source_meta()
+      Java::OrgLogstashConfigIr::SourceMetadata.new()
+    end
+
+    def jdsl
+      Java::OrgLogstashConfigIr::DSL
+    end
   end
+end; end; end
 
-  # Traverse the syntax tree recursively.
+class Treetop::Runtime::SyntaxNode
+  include LogStash::Config::AST::JDSL
+
+  # Traverse the sourceComponent tree recursively.
   # The order should respect the order of the configuration file as it is read
   # and written by humans (and the order in which it is parsed).
   def recurse(e, depth=0, &block)
@@ -37,8 +76,8 @@ def recursive_inject(results=[], &block)
   # Some of theses node will point to our concrete class.
   # To fetch a specific types of object we need to follow each branch
   # and ignore the empty nodes.
-  def recursive_select(klass)
-    return recursive_inject { |e| e.is_a?(klass) }
+  def recursive_select(*klasses)
+    return recursive_inject { |e| klasses.any? {|k| e.is_a?(k)} }
   end
 
   def recursive_inject_parent(results=[], &block)
@@ -59,296 +98,150 @@ def recursive_select_parent(results=[], klass)
 
 
 module LogStash; module Config; module AST
-
-  def self.defered_conditionals=(val)
-    @defered_conditionals = val
-  end
-
-  def self.defered_conditionals
-    @defered_conditionals
-  end
-
-  def self.defered_conditionals_index
-    @defered_conditionals_index
-  end
-
-  def self.defered_conditionals_index=(val)
-    @defered_conditionals_index = val
-  end
-
-  def self.plugin_instance_index
-    @plugin_instance_index
-  end
-
-  def self.plugin_instance_index=(val)
-    @plugin_instance_index = val
-  end
-
   class Node < Treetop::Runtime::SyntaxNode
-    def text_value_for_comments
-      text_value.gsub(/[\r\n]/, " ")
+    def section_type
+      if recursive_select_parent(Plugin).any?
+        return "codec"
+      else
+        section = recursive_select_parent(PluginSection)
+        return section.first.plugin_type.text_value
+      end
     end
   end
 
   class Config < Node
-    def compile
-      LogStash::Config::AST.defered_conditionals = []
-      LogStash::Config::AST.defered_conditionals_index = 0
-      LogStash::Config::AST.plugin_instance_index = 0
-      code = []
-
-      code << <<-CODE
-        @inputs = []
-        @filters = []
-        @outputs = []
-        @periodic_flushers = []
-        @shutdown_flushers = []
-        @generated_objects = {}
-      CODE
+    def compile(source_file=nil)
+      # There is no way to move vars across nodes in treetop :(
+      self.source_file = source_file
 
       sections = recursive_select(LogStash::Config::AST::PluginSection)
-      sections.each do |s|
-        code << s.compile_initializer
-      end
-
-      # start inputs
-      definitions = []
 
-      ["filter", "output"].each do |type|
-        # defines @filter_func and @output_func
-
-        # This need to be defined as a singleton method
-        # so each instance of the pipeline has his own implementation
-        # of the output/filter function
-        definitions << "define_singleton_method :#{type}_func do |event|"
-        definitions << "  targeted_outputs = []" if type == "output"
-        definitions << "  events = [event]" if type == "filter"
-        definitions << "  @logger.debug? && @logger.debug(\"#{type} received\", \"event\" => event.to_hash)"
-
-        sections.select { |s| s.plugin_type.text_value == type }.each do |s|
-          definitions << s.compile.split("\n", -1).map { |e| "  #{e}" }
+      section_map = {
+        :input  => [],
+        :filter => [],
+        :output => []
+      }
+
+      sections.each do |section|
+        section_name = section.plugin_type.text_value.to_sym
+        section_expr = section.expr
+        raise "Unknown section name #{section_name}!" if ![:input, :output, :filter].include?(section_name)
+        # Don't include nil section exprs!
+        ::Array[section_expr].each do |se|
+          section_map[section_name].concat se
         end
-
-        definitions << "  events" if type == "filter"
-        definitions << "  targeted_outputs" if type == "output"
-        definitions << "end"
       end
 
-      code += definitions.join("\n").split("\n", -1).collect { |l| "  #{l}" }
-
-      code += LogStash::Config::AST.defered_conditionals
+      # Represent filter / output blocks as a single composed statement
+      section_map.keys.each do |key|
+        section_map[key] = compose_for(key).call(empty_source_meta, *section_map[key])
+      end
 
-      return code.join("\n")
+      section_map
     end
   end
 
-  class Comment < Node; end
-  class Whitespace < Node; end
-  class PluginSection < Node
-    # Global plugin numbering for the janky instance variable naming we use
-    # like @filter_<name>_1
-    def initialize(*args)
-      super(*args)
+  class Comment < Node
+    def significant
+      false
     end
 
-    # Generate ruby code to initialize all the plugins.
-    def compile_initializer
-      generate_variables
-      code = []
-      @variables.each do |plugin, name|
-
-
-        code << <<-CODE
-          @generated_objects[:#{name}] = #{plugin.compile_initializer}
-          @#{plugin.plugin_type}s << @generated_objects[:#{name}]
-        CODE
-
-        # The flush method for this filter.
-        if plugin.plugin_type == "filter"
-
-          code << <<-CODE
-            @generated_objects[:#{name}_flush] = lambda do |options, &block|
-              @logger.debug? && @logger.debug(\"Flushing\", :plugin => @generated_objects[:#{name}])
-
-              events = @generated_objects[:#{name}].flush(options)
-
-              return if events.nil? || events.empty?
-
-              @logger.debug? && @logger.debug(\"Flushing\", :plugin => @generated_objects[:#{name}], :events => events.map { |x| x.to_hash  })
-
-              #{plugin.compile_starting_here.gsub(/^/, "  ")}
-
-              events.each{|e| block.call(e)}
-            end
-
-            if @generated_objects[:#{name}].respond_to?(:flush)
-              @periodic_flushers << @generated_objects[:#{name}_flush] if @generated_objects[:#{name}].periodic_flush
-              @shutdown_flushers << @generated_objects[:#{name}_flush]
-            end
-          CODE
-
-        end
-      end
-      return code.join("\n")
-    end
+  end
 
-    def variable(object)
-      generate_variables
-      return @variables[object]
+  class Whitespace < Node
+    def significant
+      false
     end
-
-    def generate_variables
-      return if !@variables.nil?
-      @variables = {}
-      plugins = recursive_select(Plugin)
-
-      plugins.each do |plugin|
-        # Unique number for every plugin.
-        LogStash::Config::AST.plugin_instance_index += 1
-        # store things as ivars, like @filter_grok_3
-        var = :"#{plugin.plugin_type}_#{plugin.plugin_name}_#{LogStash::Config::AST.plugin_instance_index}"
-        # puts("var=#{var.inspect}")
-        @variables[plugin] = var
-      end
-      return @variables
+  end
+  class PluginSection < Node
+    # Global plugin numbering for the janky instance variable naming we use
+    # like @filter_<name>_1
+    def expr
+      [*recursive_select(Branch, Plugin).map(&:expr)]
     end
-
   end
 
   class Plugins < Node; end
   class Plugin < Node
-    def plugin_type
-      if recursive_select_parent(Plugin).any?
-        return "codec"
-      else
-        return recursive_select_parent(PluginSection).first.plugin_type.text_value
-      end
-    end
-
-    def plugin_name
-      return name.text_value
+    def expr
+      jdsl.iPlugin(source_meta, plugin_type_enum, self.plugin_name, self.expr_attributes)
     end
 
-    def variable_name
-      return recursive_select_parent(PluginSection).first.variable(self)
-    end
-
-    def compile_initializer
-      # If any parent is a Plugin, this must be a codec.
-
-      if attributes.elements.nil?
-        return "plugin(#{plugin_type.inspect}, #{plugin_name.inspect})" << (plugin_type == "codec" ? "" : "\n")
-      else
-        settings = attributes.recursive_select(Attribute).collect(&:compile).reject(&:empty?)
-
-        attributes_code = "LogStash::Util.hash_merge_many(#{settings.map { |c| "{ #{c} }" }.join(", ")})"
-        return "plugin(#{plugin_type.inspect}, #{plugin_name.inspect}, #{attributes_code})" << (plugin_type == "codec" ? "" : "\n")
-      end
-    end
-
-    def compile
-      case plugin_type
+    def plugin_type_enum
+      case section_type
       when "input"
-        return "start_input(@generated_objects[:#{variable_name}])"
+        Java::OrgLogstashConfigIr::PluginDefinition::Type::INPUT
+      when "codec"
+        Java::OrgLogstashConfigIr::PluginDefinition::Type::CODEC
       when "filter"
-        return <<-CODE
-          events = @generated_objects[:#{variable_name}].multi_filter(events)
-        CODE
+        Java::OrgLogstashConfigIr::PluginDefinition::Type::FILTER
       when "output"
-        return "targeted_outputs << @generated_objects[:#{variable_name}]\n"
-      when "codec"
-        settings = attributes.recursive_select(Attribute).collect(&:compile).reject(&:empty?)
-        attributes_code = "LogStash::Util.hash_merge_many(#{settings.map { |c| "{ #{c} }" }.join(", ")})"
-        return "plugin(#{plugin_type.inspect}, #{plugin_name.inspect}, #{attributes_code})"
+        Java::OrgLogstashConfigIr::PluginDefinition::Type::OUTPUT
       end
     end
 
-    def compile_starting_here
-      return unless plugin_type == "filter" # only filter supported.
-
-      expressions = [
-        LogStash::Config::AST::Branch,
-        LogStash::Config::AST::Plugin
-      ]
-      code = []
 
-      # Find the branch we are in, if any (the 'if' statement, etc)
-      self_branch = recursive_select_parent(LogStash::Config::AST::BranchEntry).first
-
-      # Find any siblings to our branch so we can skip them later.  For example,
-      # if we are in an 'else if' we want to skip any sibling 'else if' or
-      # 'else' blocks.
-      branch_siblings = []
-      if self_branch
-        branch_siblings = recursive_select_parent(LogStash::Config::AST::Branch).first \
-          .recursive_select(LogStash::Config::AST::BranchEntry) \
-          .reject { |b| b == self_branch }
-      end
+    def plugin_name
+      return name.text_value
+    end
 
-      #ast = recursive_select_parent(LogStash::Config::AST::PluginSection).first
-      ast = recursive_select_parent(LogStash::Config::AST::Config).first
+    def variable_name
+      return recursive_select_parent(PluginSection).first.variable(self)
+    end
 
-      found = false
-      recurse(ast) do |element, depth|
-        next false if element.is_a?(LogStash::Config::AST::PluginSection) && element.plugin_type.text_value != "filter"
-        if element == self
-          found = true
-          next false
-        end
-        if found && expressions.include?(element.class)
-          code << element.compile
-          next false
+    def expr_attributes
+      # Turn attributes into a hash map
+      self.attributes.recursive_select(Attribute).map(&:expr).map {|k,v|
+        if v.java_kind_of?(Java::OrgLogstashConfigIrExpression::ValueExpression)
+          [k, v.get]
+        else
+          [k,v]
         end
-        next false if branch_siblings.include?(element)
-        next true
+      }.reduce({}) do |hash,kv|
+        k,v = kv
+        hash[k] = v
+        hash
       end
 
-      return code.collect { |l| "#{l}\n" }.join("")
-    end # def compile_starting_here
+    end
   end
 
   class Name < Node
-    def compile
-      return text_value.inspect
+    def expr
+      return text_value
     end
   end
   class Attribute < Node
-    def compile
-      return %Q(#{name.compile} => #{value.compile})
+    def expr
+      [name.text_value, value.expr]
     end
   end
   class RValue < Node; end
   class Value < RValue; end
 
-  module Unicode
-    def self.wrap(text)
-      return "(" + text.force_encoding(Encoding::UTF_8).inspect + ")"
-    end
-  end
-
   class Bareword < Value
-    def compile
-      return Unicode.wrap(text_value)
+    def expr
+      jdsl.eValue(source_meta, text_value)
     end
   end
   class String < Value
-    def compile
-      return Unicode.wrap(text_value[1...-1])
+    def expr
+      jdsl.eValue(source_meta, text_value[1...-1])
     end
   end
   class RegExp < Value
-    def compile
-      return "Regexp.new(" + Unicode.wrap(text_value[1...-1]) + ")"
+    def expr
+      jdsl.eRegex(text_value[1..-2])
     end
   end
   class Number < Value
-    def compile
-      return text_value
+    def expr
+      jdsl.eValue(source_meta, text_value.include?(".") ? text_value.to_f : text_value.to_i)
     end
   end
   class Array < Value
-    def compile
-      return "[" << recursive_select(Value).collect(&:compile).reject(&:empty?).join(", ") << "]"
+    def expr
+      jdsl.eValue(source_meta, recursive_select(Value).map(&:expr).map(&:get))
     end
   end
   class Hash < Value
@@ -373,9 +266,9 @@ def find_duplicate_keys
       values.find_all { |v| values.count(v) > 1 }.uniq
     end
 
-    def compile
+    def expr
       validate!
-      return "{" << recursive_select(HashEntry).collect(&:compile).reject(&:empty?).join(", ") << "}"
+      ::Hash[recursive_select(HashEntry).map(&:expr)]
     end
   end
 
@@ -383,111 +276,229 @@ class HashEntries < Node
   end
 
   class HashEntry < Node
-    def compile
-      return %Q(#{name.compile} => #{value.compile})
+    def expr
+      return [name.expr.get, value.expr.get()]
     end
   end
 
-  class BranchOrPlugin < Node; end
-
   class Branch < Node
-    def compile
-
-      # this construct is non obvious. we need to loop through each event and apply the conditional.
-      # each branch of a conditional will contain a construct (a filter for example) that also loops through
-      # the events variable so we have to initialize it to [event] for the branch code.
-      # at the end, events is returned to handle the case where no branch match and no branch code is executed
-      # so we must make sure to return the current event.
-
-      type = recursive_select_parent(PluginSection).first.plugin_type.text_value
-
-      if type == "filter"
-        i = LogStash::Config::AST.defered_conditionals_index += 1
-        source = <<-CODE
-          @generated_objects[:cond_func_#{i}] = lambda do |input_events|
-            result = []
-            input_events.each do |event|
-              events = [event]
-              #{super}
-              end
-              result += events
-            end
-            result
-          end
-        CODE
-        LogStash::Config::AST.defered_conditionals << source
-
-        <<-CODE
-          events = @generated_objects[:cond_func_#{i}].call(events)
-        CODE
-      else # Output
-        <<-CODE
-          #{super}
+    def expr
+      # Build this stuff as s-expressions for convenience at first (they're mutable)
+
+      exprs = []
+      else_stack = [] # For turning if / elsif / else into nested ifs
+
+      self.recursive_select(Plugin, If, Elsif, Else).each do |node|
+        if node.is_a?(If)
+          exprs << :if
+          exprs << expr_cond(node)
+          exprs << expr_body(node)
+        elsif node.is_a?(Elsif)
+          condition = expr_cond(node)
+          body = expr_body(node)
+
+          else_stack << [:if, condition, body]
+        elsif node.is_a?(Else)
+          body = expr_body(node)
+          if else_stack.size >= 1
+            else_stack.last << body
+          else
+            exprs << body
           end
-        CODE
+        end
+      end
+
+      else_stack.reverse.each_cons(2) do |cons|
+        later,earlier = cons
+        earlier << later
       end
+      exprs << else_stack.first
+
+      # Then convert to the imperative java IR
+      javaify_sexpr(exprs)
+    end
+
+    def javaify_sexpr(sexpr)
+      return nil if sexpr.nil?
+
+      head, tail = sexpr.first
+      tail = sexpr[1..-1]
+
+      if head == :if
+        condition, t_branch, f_branch = tail
+
+        java_t_branch = t_branch && javaify_sexpr(t_branch)
+        java_f_branch = f_branch && javaify_sexpr(f_branch)
+
+        if java_t_branch || java_f_branch
+          # Invert the expression and make the f_branch the t_branch
+
+          jdsl.iIf(condition, java_t_branch || jdsl.noop, java_f_branch || jdsl.noop)
+        else
+          jdsl.noop()
+        end
+      elsif head == :compose
+        tail && tail.size > 0 ? compose(*tail) : jdsl.noop
+      else
+        raise "Unknown expression #{head}!"
+      end
+    end
+
+    def expr_cond(node)
+      node.elements.find {|e| e.is_a?(Condition)}.expr
+    end
+
+    def expr_body(node)
+      [:compose, *node.recursive_select(Plugin, Branch).map(&:expr)]
     end
   end
 
   class BranchEntry < Node; end
 
   class If < BranchEntry
-    def compile
-      children = recursive_inject { |e| e.is_a?(Branch) || e.is_a?(Plugin) }
-      return "if #{condition.compile} # if #{condition.text_value_for_comments}\n" \
-        << children.collect(&:compile).map { |s| s.split("\n", -1).map { |l| "  " + l }.join("\n") }.join("") << "\n"
-    end
   end
   class Elsif < BranchEntry
-    def compile
-      children = recursive_inject { |e| e.is_a?(Branch) || e.is_a?(Plugin) }
-      return "elsif #{condition.compile} # else if #{condition.text_value_for_comments}\n" \
-        << children.collect(&:compile).map { |s| s.split("\n", -1).map { |l| "  " + l }.join("\n") }.join("") << "\n"
-    end
   end
   class Else < BranchEntry
-    def compile
-      children = recursive_inject { |e| e.is_a?(Branch) || e.is_a?(Plugin) }
-      return "else\n" \
-        << children.collect(&:compile).map { |s| s.split("\n", -1).map { |l| "  " + l }.join("\n") }.join("") << "\n"
-    end
   end
 
   class Condition < Node
-    def compile
-      return "(#{super})"
+    def expr
+      first_element = elements.first
+      rest_elements = elements.size > 1 ? elements[1].recursive_select(BooleanOperator, Expression, SelectorElement) : []
+
+      all_elements = [first_element, *rest_elements]
+
+      if all_elements.size == 1
+        elem = all_elements.first
+        if elem.is_a?(Selector)
+          eventValue = elem.recursive_select(SelectorElement).first.expr
+          jdsl.eTruthy(eventValue)
+        elsif elem.is_a?(RegexpExpression)
+          elem.value
+        else
+          join_conditions(all_elements)
+        end
+      else
+
+        join_conditions(all_elements) # Is this necessary?
+      end
+    end
+
+    def precedence(op)
+      #  Believe this is right for logstash?
+      case op
+      when :and
+        2
+      when :or
+        1
+      else
+        raise ArgumentError, "Unexpected operator #{op}"
+      end
+    end
+
+    def jconvert(sexpr)
+      return sexpr if sexpr.java_kind_of?(Java::OrgLogstashConfigIrExpression::BooleanExpression)
+
+      op, left, right = sexpr
+
+      left_c = jconvert(left)
+      right_c = jconvert(right)
+
+       case op
+       when :and
+         return jdsl.eAnd(left, right);
+       when :or
+         return jdsl.eOr(left, right);
+       else
+         raise "Unknown op #{jop}"
+       end
+    end
+
+    def join_conditions(all_elements)
+      # Use Dijkstra's shunting yard algorithm
+      out = []
+      operators = []
+
+      all_elements.each do |e|
+        e_exp = e.expr
+
+        if e.is_a?(BooleanOperator)
+          if operators.last && precedence(operators.last) > precedence(e_exp)
+            out << operators.pop
+          end
+          operators << e_exp
+        else
+          out << e_exp
+        end
+      end
+      operators.reverse.each {|o| out << o}
+
+      stack = []
+      expr = []
+      x = false
+      out.each do |e|
+        if e.is_a?(Symbol)
+          x = 1
+          rval, lval = stack.pop, stack.pop
+          stack << jconvert([e, lval, rval])
+        else
+          stack << e
+        end
+      end
+
+      if stack.size > 1
+        raise "Stack size should never be > than 1!"
+      end
+      return stack.first
     end
   end
 
   module Expression
-    def compile
-      return "(#{super})"
+    def expr
+      return self.value if self.respond_to?(:value)
+
+      self.recursive_select(Condition, Expression).map {|e| e.respond_to?(:value) ? e.value : e.expr }.first
     end
   end
 
   module NegativeExpression
-    def compile
-      return "!(#{super})"
+    include JDSL
+
+    def value
+      jdsl.eNot(source_meta, self.recursive_select(Condition).map(&:expr).first)
     end
   end
 
-  module ComparisonExpression; end
+  module ComparisonExpression
+    include JDSL
+
+    def value
+      lval, comparison_method, rval = self.recursive_select(Selector, ComparisonOperator, Number, String).map(&:expr)
+      comparison_method.call(source_meta, lval, rval)
+    end
+  end
 
   module InExpression
-    def compile
+    include JDSL
+
+    def value # Because this is somehow higher up the inheritance chain than Expression
       item, list = recursive_select(LogStash::Config::AST::RValue)
-      return "(x = #{list.compile}; x.respond_to?(:include?) && x.include?(#{item.compile}))"
+      jdsl.eIn(source_meta, item.expr, list.expr)
     end
   end
 
   module NotInExpression
-    def compile
+    include JDSL
+
+    def value
       item, list = recursive_select(LogStash::Config::AST::RValue)
-      return "(x = #{list.compile}; !x.respond_to?(:include?) || !x.include?(#{item.compile}))"
+      jdsl.eNot(source_meta, jdsl.eIn(item.expr, list.expr))
     end
   end
 
   class MethodCall < Node
+    # TBD: Can we delete this? Who uses the method call syntax?
     def compile
       arguments = recursive_inject { |e| [String, Number, Selector, Array, MethodCall].any? { |c| e.is_a?(c) } }
       return "#{method.text_value}(" << arguments.collect(&:compile).join(", ") << ")"
@@ -495,40 +506,67 @@ def compile
   end
 
   class RegexpExpression < Node
-    def compile
-      operator = recursive_select(LogStash::Config::AST::RegExpOperator).first.text_value
-      item, regexp = recursive_select(LogStash::Config::AST::RValue)
-      # Compile strings to regexp's
-      if regexp.is_a?(LogStash::Config::AST::String)
-        regexp = "/#{regexp.text_value[1..-2]}/"
-      else
-        regexp = regexp.compile
-      end
-      return "(#{item.compile} #{operator} #{regexp})"
+    def value
+      selector, operator_method, regexp = recursive_select(Selector, LogStash::Config::AST::RegExpOperator, LogStash::Config::AST::RegExp).map(&:expr)
+
+      raise "Expected a selector #{text_value}!" unless selector
+      raise "Expected a regexp #{text_value}!" unless regexp
+
+
+      operator_method.call(source_meta, selector, regexp);
     end
   end
 
+  module BranchOrPlugin; end
+
   module ComparisonOperator
-    def compile
-      return " #{text_value} "
+    include JDSL
+
+    def expr
+      case self.text_value
+      when "=="
+        jdsl.method(:eEq)
+      when "!="
+        jdsl.method(:eNeq)
+      when ">"
+        jdsl.method(:eGt)
+      when "<"
+        jdsl.method(:eLt)
+      when ">="
+        jdsl.method(:eGte)
+      when "<="
+        jdsl.method(:eLte)
+      else
+        raise "Unknown operator #{self.text_value}"
+      end
     end
   end
   module RegExpOperator
-    def compile
-      return " #{text_value} "
+    def expr
+      if self.text_value == '!~'
+        jdsl.method(:eRegexNeq)
+      elsif self.text_value == '=~'
+        jdsl.method(:eRegexEq)
+      else
+        raise "Unknown regex operator #{self.text_value}"
+      end
     end
   end
   module BooleanOperator
-    def compile
-      return " #{text_value} "
+    def expr
+      self.text_value.to_sym
     end
   end
   class Selector < RValue
-    def compile
-      return "event.get(#{text_value.inspect})"
+    def expr
+      jdsl.eEventValue(source_meta, text_value)
+    end
+  end
+  class SelectorElement < Node;
+    def expr
+      jdsl.eEventValue(source_meta, text_value)
     end
   end
-  class SelectorElement < Node; end
 end; end; end
 
 
diff --git a/logstash-core/lib/logstash/environment.rb b/logstash-core/lib/logstash/environment.rb
index ddef42918cd..88b3527a438 100644
--- a/logstash-core/lib/logstash/environment.rb
+++ b/logstash-core/lib/logstash/environment.rb
@@ -40,8 +40,15 @@ module Environment
             Setting::String.new("http.host", "127.0.0.1"),
             Setting::PortRange.new("http.port", 9600..9700),
             Setting::String.new("http.environment", "production"),
+            Setting::String.new("queue.type", "memory", true, ["persisted", "memory", "memory_acked"]),
+            Setting::Bytes.new("queue.page_capacity", "250mb"),
+            Setting::Numeric.new("queue.max_events", 0), # 0 is unlimited
   ].each {|setting| SETTINGS.register(setting) }
 
+  # Compute the default queue path based on `path.data`
+  default_queue_file_path = ::File.join(SETTINGS.get("path.data"), "queue")
+  SETTINGS.register Setting::WritableDirectory.new("path.queue", default_queue_file_path)
+
   module Environment
     extend self
 
diff --git a/logstash-core/lib/logstash/filters/base.rb b/logstash-core/lib/logstash/filters/base.rb
index 35bf49e46ab..5276e08981d 100644
--- a/logstash-core/lib/logstash/filters/base.rb
+++ b/logstash-core/lib/logstash/filters/base.rb
@@ -23,13 +23,13 @@ class LogStash::Filters::Base < LogStash::Plugin
   # syntax.
   #
   # Example:
-  # [source,ruby]
+  # [producer,ruby]
   #     filter {
   #       %PLUGIN% {
   #         add_tag => [ "foo_%{somefield}" ]
   #       }
   #     }
-  # [source,ruby]
+  # [producer,ruby]
   #     # You can also add multiple tags at once:
   #     filter {
   #       %PLUGIN% {
@@ -46,13 +46,13 @@ class LogStash::Filters::Base < LogStash::Plugin
   # syntax.
   #
   # Example:
-  # [source,ruby]
+  # [producer,ruby]
   #     filter {
   #       %PLUGIN% {
   #         remove_tag => [ "foo_%{somefield}" ]
   #       }
   #     }
-  # [source,ruby]
+  # [producer,ruby]
   #     # You can also remove multiple tags at once:
   #     filter {
   #       %PLUGIN% {
@@ -69,13 +69,13 @@ class LogStash::Filters::Base < LogStash::Plugin
   # Field names can be dynamic and include parts of the event using the `%{field}`.
   #
   # Example:
-  # [source,ruby]
+  # [producer,ruby]
   #     filter {
   #       %PLUGIN% {
   #         add_field => { "foo_%{somefield}" => "Hello world, from %{host}" }
   #       }
   #     }
-  # [source,ruby]
+  # [producer,ruby]
   #     # You can also add multiple fields at once:
   #     filter {
   #       %PLUGIN% {
@@ -95,13 +95,13 @@ class LogStash::Filters::Base < LogStash::Plugin
   # If this filter is successful, remove arbitrary fields from this event.
   # Fields names can be dynamic and include parts of the event using the %{field}
   # Example:
-  # [source,ruby]
+  # [producer,ruby]
   #     filter {
   #       %PLUGIN% {
   #         remove_field => [ "foo_%{somefield}" ]
   #       }
   #     }
-  # [source,ruby]
+  # [producer,ruby]
   #     # You can also remove multiple fields at once:
   #     filter {
   #       %PLUGIN% {
diff --git a/logstash-core/lib/logstash/instrument/metric_store.rb b/logstash-core/lib/logstash/instrument/metric_store.rb
index c440e2524d2..6a3af5837bc 100644
--- a/logstash-core/lib/logstash/instrument/metric_store.rb
+++ b/logstash-core/lib/logstash/instrument/metric_store.rb
@@ -1,6 +1,6 @@
 # encoding: utf-8
 require "concurrent"
-require "logstash/event"
+# require "logstash/event"
 require "logstash/instrument/metric_type"
 require "thread"
 
diff --git a/logstash-core/lib/logstash/instrument/metric_type/base.rb b/logstash-core/lib/logstash/instrument/metric_type/base.rb
index 206f175c753..06d39a3c921 100644
--- a/logstash-core/lib/logstash/instrument/metric_type/base.rb
+++ b/logstash-core/lib/logstash/instrument/metric_type/base.rb
@@ -1,5 +1,5 @@
 # encoding: utf-8
-require "logstash/event"
+# require "logstash/event"
 require "logstash/util"
 
 module LogStash module Instrument module MetricType
diff --git a/logstash-core/lib/logstash/instrument/snapshot.rb b/logstash-core/lib/logstash/instrument/snapshot.rb
index f46068439ad..56b1020af3e 100644
--- a/logstash-core/lib/logstash/instrument/snapshot.rb
+++ b/logstash-core/lib/logstash/instrument/snapshot.rb
@@ -1,6 +1,6 @@
 # encoding: utf-8
 require "logstash/util/loggable"
-require "logstash/event"
+# require "logstash/event"
 
 module LogStash module Instrument
   class Snapshot
diff --git a/logstash-core/lib/logstash/java_integration.rb b/logstash-core/lib/logstash/java_integration.rb
index 26f9eb546e0..670ceaae650 100644
--- a/logstash-core/lib/logstash/java_integration.rb
+++ b/logstash-core/lib/logstash/java_integration.rb
@@ -1,6 +1,5 @@
 # encoding: utf-8
 require "java"
-require "jars"
 
 # this is mainly for usage with JrJackson json parsing in :raw mode which genenerates
 # Java::JavaUtil::ArrayList and Java::JavaUtil::LinkedHashMap native objects for speed.
diff --git a/logstash-core/lib/logstash/outputs/base.rb b/logstash-core/lib/logstash/outputs/base.rb
index 453d0cbdf98..5e3ce1ad1a3 100644
--- a/logstash-core/lib/logstash/outputs/base.rb
+++ b/logstash-core/lib/logstash/outputs/base.rb
@@ -84,7 +84,7 @@ def receive(event)
   end # def receive
 
   public
-  # To be overriden in implementations
+  # To be overridden in implementations
   def multi_receive(events)
     if @receives_encoded
       self.multi_receive_encoded(codec.multi_encode(events))
diff --git a/logstash-core/lib/logstash/patches/profile_require_calls.rb b/logstash-core/lib/logstash/patches/profile_require_calls.rb
index 0eb2f2fcc0e..7e681fdb3c2 100644
--- a/logstash-core/lib/logstash/patches/profile_require_calls.rb
+++ b/logstash-core/lib/logstash/patches/profile_require_calls.rb
@@ -22,17 +22,17 @@ def require(path)
       # Only print require() calls that did actual work.
       # require() returns true on load, false if already loaded.
       if result
-        source = caller[0]
-        #p source.include?("/lib/polyglot.rb:63:in `require'") => source
-        if source.include?("/lib/polyglot.rb:63:in `require'")
-          source = caller[1]
+        producer = caller[0]
+        #p producer.include?("/lib/polyglot.rb:63:in `require'") => producer
+        if producer.include?("/lib/polyglot.rb:63:in `require'")
+          producer = caller[1]
         end
 
         #target = $LOADED_FEATURES.grep(/#{path}/).first
         #puts path
         #puts caller.map { |c| "  #{c}" }.join("\n")
         #fontsize = [10, duration * 48].max
-        puts "#{duration},#{path},#{source}"
+        puts "#{duration},#{path},#{producer}"
       end
       #puts caller.map { |c| " => #{c}" }.join("\n")
     end
diff --git a/logstash-core/lib/logstash/pipeline.rb b/logstash-core/lib/logstash/pipeline.rb
index ad1297cba1f..d0a0e822706 100644
--- a/logstash-core/lib/logstash/pipeline.rb
+++ b/logstash-core/lib/logstash/pipeline.rb
@@ -4,13 +4,15 @@
 require "concurrent"
 require "logstash/namespace"
 require "logstash/errors"
+require "logstash-core/logstash-core"
+require "logstash/util/wrapped_acked_queue"
+require "logstash/util/wrapped_synchronous_queue"
 require "logstash/event"
 require "logstash/config/file"
 require "logstash/filters/base"
 require "logstash/inputs/base"
 require "logstash/outputs/base"
 require "logstash/shutdown_watcher"
-require "logstash/util/wrapped_synchronous_queue"
 require "logstash/pipeline_reporter"
 require "logstash/instrument/metric"
 require "logstash/instrument/namespaced_metric"
@@ -18,6 +20,7 @@
 require "logstash/instrument/collector"
 require "logstash/output_delegator"
 require "logstash/filter_delegator"
+require "logstash/compiler"
 
 module LogStash; class Pipeline
   include LogStash::Util::Loggable
@@ -91,16 +94,17 @@ def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
     rescue => e
       raise
     end
-
-    queue = Util::WrappedSynchronousQueue.new
+    queue = build_queue_from_settings
     @input_queue_client = queue.write_client
     @filter_queue_client = queue.read_client
-    # Note that @inflight_batches as a central mechanism for tracking inflight
+    @signal_queue = Queue.new
+    # Note that @infilght_batches as a central mechanism for tracking inflight
     # batches will fail if we have multiple read clients here.
     @filter_queue_client.set_events_metric(metric.namespace([:stats, :events]))
     @filter_queue_client.set_pipeline_metric(
         metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :events])
     )
+
     @events_filtered = Concurrent::AtomicFixnum.new(0)
     @events_consumed = Concurrent::AtomicFixnum.new(0)
 
@@ -111,6 +115,28 @@ def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
     @flushing = Concurrent::AtomicReference.new(false)
   end # def initialize
 
+  def build_queue_from_settings
+    queue_type = settings.get("queue.type")
+    queue_page_capacity = settings.get("queue.page_capacity")
+    max_events = settings.get("queue.max_events")
+
+    if queue_type == "memory_acked"
+      # memory_acked is used in tests/specs
+      LogStash::Util::WrappedAckedQueue.create_memory_based("", queue_page_capacity, max_events)
+    elsif queue_type == "memory"
+      # memory is the legacy and default setting
+      LogStash::Util::WrappedSynchronousQueue.new()
+    elsif queue_type == "persisted"
+      # persisted is the disk based acked queue
+      queue_path = settings.get("path.queue")
+      LogStash::Util::WrappedAckedQueue.create_file_based(queue_path, queue_page_capacity, max_events)
+    else
+      raise(ConfigurationError, "invalid queue.type setting")
+    end
+  end
+
+  private :build_queue_from_settings
+
   def ready?
     @ready.value
   end
@@ -167,6 +193,8 @@ def run
     shutdown_flusher
     shutdown_workers
 
+    @filter_queue_client.close
+
     @logger.debug("Pipeline #{@pipeline_id} has been shutdown")
 
     # exit code
@@ -241,12 +269,15 @@ def worker_loop(batch_size, batch_delay)
 
     while running
       batch = @filter_queue_client.take_batch
+      signal = @signal_queue.empty? ? NO_SIGNAL : @signal_queue.pop
+      running = !signal.shutdown?
+
       @events_consumed.increment(batch.size)
-      running = false if batch.shutdown_signal_received?
+
       filter_batch(batch)
 
-      if batch.shutdown_signal_received? || batch.flush_signal_received?
-        flush_filters_to_batch(batch)
+      if signal.flush? || signal.shutdown?
+        flush_filters_to_batch(batch, :final => signal.shutdown?)
       end
 
       output_batch(batch)
@@ -256,11 +287,9 @@ def worker_loop(batch_size, batch_delay)
 
   def filter_batch(batch)
     batch.each do |event|
-      if event.is_a?(Event)
-        filter_func(event).each do |e|
-          # these are both original and generated events
-          batch.merge(e) unless e.cancelled?
-        end
+      filter_func(event).each do |e|
+        #these are both original and generated events
+        batch.merge(e) unless e.cancelled?
       end
     end
     @filter_queue_client.add_filtered_metrics(batch)
@@ -296,7 +325,7 @@ def output_batch(batch)
     output_events_map.each do |output, events|
       output.multi_receive(events)
     end
-    
+
     @filter_queue_client.add_output_metrics(batch)
   end
 
@@ -382,7 +411,7 @@ def shutdown_workers
     # Each worker thread will receive this exactly once!
     @worker_threads.each do |t|
       @logger.debug("Pushing shutdown", :thread => t.inspect)
-      @input_queue_client.push(SHUTDOWN)
+      @signal_queue.push(SHUTDOWN)
     end
 
     @worker_threads.each do |t|
@@ -407,7 +436,7 @@ def plugin(plugin_type, name, *args)
          end
 
     raise ConfigurationError, "Two plugins have the id '#{id}', please fix this conflict" if @plugins_by_id[id]
-    
+
     pipeline_scoped_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :plugins])
 
     klass = Plugin.lookup(plugin_type, name)
@@ -425,7 +454,7 @@ def plugin(plugin_type, name, *args)
                input_plugin.metric = type_scoped_metric.namespace(id)
                input_plugin
              end
-    
+
     @plugins_by_id[id] = plugin
   end
 
@@ -467,7 +496,7 @@ def shutdown_flusher
   def flush
     if @flushing.compare_and_set(false, true)
       @logger.debug? && @logger.debug("Pushing flush onto pipeline")
-      @input_queue_client.push(FLUSH)
+      @signal_queue.push(FLUSH)
     end
   end
 
@@ -485,7 +514,6 @@ def uptime
   # @param batch [ReadClient::ReadBatch]
   # @param options [Hash]
   def flush_filters_to_batch(batch, options = {})
-    options[:final] = batch.shutdown_signal_received?
     flush_filters(options) do |event|
       unless event.cancelled?
         @logger.debug? and @logger.debug("Pushing flushed events", :event => event)
diff --git a/logstash-core/lib/logstash/runner.rb b/logstash-core/lib/logstash/runner.rb
index 1ea15138c81..942059cff9b 100644
--- a/logstash-core/lib/logstash/runner.rb
+++ b/logstash-core/lib/logstash/runner.rb
@@ -10,6 +10,8 @@
 LogStash::Environment.load_locale!
 
 require "logstash/namespace"
+require "logstash-core/logstash-core"
+
 require "logstash/agent"
 require "logstash/config/defaults"
 require "logstash/shutdown_watcher"
diff --git a/logstash-core/lib/logstash/settings.rb b/logstash-core/lib/logstash/settings.rb
index 10ec1f5f7d3..2039303ad8a 100644
--- a/logstash-core/lib/logstash/settings.rb
+++ b/logstash-core/lib/logstash/settings.rb
@@ -1,5 +1,7 @@
 # encoding: utf-8
 require "logstash/util/loggable"
+require "fileutils"
+require "logstash/util/byte_value"
 
 module LogStash
   class Settings
@@ -434,6 +436,33 @@ def value
         end
       end
     end
+
+    class Bytes < Coercible
+      def initialize(name, default=nil, strict=true)
+        super(name, ::Fixnum, default, strict=true) { |value| valid?(value) }
+      end
+
+      def valid?(value)
+        value.is_a?(Fixnum) && value >= 0
+      end
+
+      def coerce(value)
+        case value
+        when ::Numeric
+          value
+        when ::String
+          LogStash::Util::ByteValue.parse(value)
+        else
+          raise ArgumentError.new("Could not coerce '#{value}' into a bytes value")
+        end
+      end
+
+      def validate(value)
+        unless valid?(value)
+          raise ArgumentError.new("Invalid byte value \"#{value}\".")
+        end
+      end
+    end
   end
 
   SETTINGS = Settings.new
diff --git a/logstash-core/lib/logstash/util/buftok.rb b/logstash-core/lib/logstash/util/buftok.rb
index a093874483a..fd6491cc7f3 100644
--- a/logstash-core/lib/logstash/util/buftok.rb
+++ b/logstash-core/lib/logstash/util/buftok.rb
@@ -16,7 +16,7 @@
 # Distributed under the Ruby license (http://www.ruby-lang.org/en/LICENSE.txt)
 
 # BufferedTokenizer takes a delimiter upon instantiation, or acts line-based
-# by default.  It allows input to be spoon-fed from some outside source which
+# by default.  It allows input to be spoon-fed from some outside producer which
 # receives arbitrary length datagrams which may-or-may-not contain the token
 # by which entities are delimited.
 #
diff --git a/logstash-core/lib/logstash/util/byte_value.rb b/logstash-core/lib/logstash/util/byte_value.rb
new file mode 100644
index 00000000000..bc189a438a1
--- /dev/null
+++ b/logstash-core/lib/logstash/util/byte_value.rb
@@ -0,0 +1,61 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/util"
+
+module LogStash; module Util; module ByteValue
+  module_function
+
+  B = 1
+  KB = B << 10
+  MB = B << 20
+  GB = B << 30
+  TB = B << 40
+  PB = B << 50
+
+  def parse(text)
+    if !text.is_a?(String)
+      raise ArgumentError, "ByteValue::parse takes a String, got a `#{text.class.name}`"
+    end
+    number = text.to_f
+    factor = multiplier(text)
+
+    (number * factor).to_i
+  end
+
+  def multiplier(text)
+    case text
+      when /(?:k|kb)$/ 
+        KB
+      when /(?:m|mb)$/
+        MB
+      when /(?:g|gb)$/
+        GB
+      when /(?:t|tb)$/
+        TB
+      when /(?:p|pb)$/
+        PB
+      when /(?:b)$/
+        B
+      else 
+        raise ArgumentError, "Unknown bytes value '#{text}'"
+    end
+  end
+
+  def human_readable(number)
+    value, unit = if number > PB
+      [number / PB, "pb"]
+    elsif number > TB
+      [number / TB, "tb"]
+    elsif number > GB
+      [number / GB, "gb"]
+    elsif number > MB
+      [number / MB, "mb"]
+    elsif number > KB
+      [number / KB, "kb"]
+    else
+      [number, "b"]
+    end
+
+    format("%.2d%s", value, unit)
+  end
+end end end
diff --git a/logstash-core/lib/logstash/util/wrapped_acked_queue.rb b/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
new file mode 100644
index 00000000000..07d1978ac3a
--- /dev/null
+++ b/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
@@ -0,0 +1,347 @@
+# encoding: utf-8
+
+require "logstash-core-queue-jruby/logstash-core-queue-jruby"
+require "concurrent"
+# This is an adapted copy of the wrapped_synchronous_queue file
+# ideally this should be moved to Java/JRuby
+
+module LogStash; module Util
+  # Some specialized constructors. The calling code *does* need to know what kind it creates but
+  # not the internal implementation e.g. LogStash::AckedMemoryQueue etc.
+  # Note the use of allocate - this is what new does before it calls initialize.
+  # Note that the new method has been made private this is because there is no
+  # default queue implementation.
+  # It would be expensive to create a persistent queue in the new method
+  # to then throw it away in favor of a memory based one directly after.
+  # Especially in terms of (mmap) memory allocation and proper close sequencing.
+
+  class WrappedAckedQueue
+    class QueueClosedError < ::StandardError; end
+    class NotImplementedError < ::StandardError; end
+
+    def self.create_memory_based(path, capacity, size)
+      self.allocate.with_queue(
+        LogStash::AckedMemoryQueue.new(path, capacity, size)
+      )
+    end
+
+    def self.create_file_based(path, capacity, size)
+      self.allocate.with_queue(
+        LogStash::AckedQueue.new(path, capacity, size)
+      )
+    end
+
+    private_class_method :new
+
+    def with_queue(queue)
+      @queue = queue
+      @queue.open
+      @closed = Concurrent::AtomicBoolean.new(false)
+      self
+    end
+
+    def closed?
+      @closed.true?
+    end
+
+    # Push an object to the queue if the queue is full
+    # it will block until the object can be added to the queue.
+    #
+    # @param [Object] Object to add to the queue
+    def push(obj)
+      check_closed("write")
+      @queue.write(obj)
+    end
+    alias_method(:<<, :push)
+
+    # TODO - fix doc for this noop method
+    # Offer an object to the queue, wait for the specified amount of time.
+    # If adding to the queue was successful it will return true, false otherwise.
+    #
+    # @param [Object] Object to add to the queue
+    # @param [Integer] Time in milliseconds to wait before giving up
+    # @return [Boolean] True if adding was successfull if not it return false
+    def offer(obj, timeout_ms)
+      raise NotImplementedError.new("The offer method is not implemented. There is no non blocking write operation yet.")
+    end
+
+    # Blocking
+    def take
+      check_closed("read a batch")
+      # TODO - determine better arbitrary timeout millis
+      @queue.read_batch(1, 200).get_elements.first
+    end
+
+    # Block for X millis
+    def poll(millis)
+      check_closed("read")
+      @queue.read_batch(1, millis).get_elements.first
+    end
+
+    def read_batch(size, wait)
+      check_closed("read a batch")
+      @queue.read_batch(size, wait)
+    end
+
+    def write_client
+      WriteClient.new(self)
+    end
+
+    def read_client()
+      ReadClient.new(self)
+    end
+
+    def check_closed(action)
+      if closed?
+        raise QueueClosedError.new("Attempted to #{action} on a closed AckedQueue")
+      end
+    end
+
+    def close
+      @queue.close
+      @closed.make_true
+    end
+
+    class ReadClient
+      # We generally only want one thread at a time able to access pop/take/poll operations
+      # from this queue. We also depend on this to be able to block consumers while we snapshot
+      # in-flight buffers
+
+      def initialize(queue, batch_size = 125, wait_for = 250)
+        @queue = queue
+        @mutex = Mutex.new
+        # Note that @inflight_batches as a central mechanism for tracking inflight
+        # batches will fail if we have multiple read clients in the pipeline.
+        @inflight_batches = {}
+        # allow the worker thread to report the execution time of the filter + output
+        @inflight_clocks = {}
+        @batch_size = batch_size
+        @wait_for = wait_for
+      end
+
+      def close
+        @queue.close
+      end
+
+      def set_batch_dimensions(batch_size, wait_for)
+        @batch_size = batch_size
+        @wait_for = wait_for
+      end
+
+      def set_events_metric(metric)
+        @event_metric = metric
+      end
+
+      def set_pipeline_metric(metric)
+        @pipeline_metric = metric
+      end
+
+      def inflight_batches
+        @mutex.synchronize do
+          yield(@inflight_batches)
+        end
+      end
+
+      def current_inflight_batch
+        @inflight_batches.fetch(Thread.current, [])
+      end
+
+      def take_batch
+        if @queue.closed?
+          raise QueueClosedError.new("Attempt to take a batch from a closed AckedQueue")
+        end
+        @mutex.synchronize do
+          batch = ReadBatch.new(@queue, @batch_size, @wait_for)
+          add_starting_metrics(batch)
+          set_current_thread_inflight_batch(batch)
+          start_clock
+          batch
+        end
+      end
+
+      def set_current_thread_inflight_batch(batch)
+        @inflight_batches[Thread.current] = batch
+      end
+
+      def close_batch(batch)
+        @mutex.synchronize do
+          batch.close
+          @inflight_batches.delete(Thread.current)
+          stop_clock
+        end
+      end
+
+      def start_clock
+        @inflight_clocks[Thread.current] = [
+        @event_metric.time(:duration_in_millis),
+        @pipeline_metric.time(:duration_in_millis)
+        ]
+      end
+
+      def stop_clock
+        @inflight_clocks[Thread.current].each(&:stop)
+        @inflight_clocks.delete(Thread.current)
+      end
+
+      def add_starting_metrics(batch)
+        return if @event_metric.nil? || @pipeline_metric.nil?
+        @event_metric.increment(:in, batch.starting_size)
+        @pipeline_metric.increment(:in, batch.starting_size)
+      end
+
+      def add_filtered_metrics(batch)
+        @event_metric.increment(:filtered, batch.filtered_size)
+        @pipeline_metric.increment(:filtered, batch.filtered_size)
+      end
+
+      def add_output_metrics(batch)
+        @event_metric.increment(:out, batch.filtered_size)
+        @pipeline_metric.increment(:out, batch.filtered_size)
+      end
+    end
+
+    class ReadBatch
+      def initialize(queue, size, wait)
+        @originals = Hash.new
+
+        # TODO: disabled for https://github.com/elastic/logstash/issues/6055 - will have to properly refactor
+        # @cancelled = Hash.new
+
+        @generated = Hash.new
+        @iterating_temp = Hash.new
+        @iterating = false # Atomic Boolean maybe? Although batches are not shared across threads
+        take_originals_from_queue(queue, size, wait) # this sets a reference to @acked_batch
+      end
+
+      def close
+        # this will ack the whole batch, regardless of whether some
+        # events were cancelled or failed
+        return if @acked_batch.nil?
+        @acked_batch.close
+      end
+
+      def merge(event)
+        return if event.nil? || @originals.key?(event)
+        # take care not to cause @generated to change during iteration
+        # @iterating_temp is merged after the iteration
+        if iterating?
+          @iterating_temp[event] = true
+        else
+          # the periodic flush could generate events outside of an each iteration
+          @generated[event] = true
+        end
+      end
+
+      def cancel(event)
+        # TODO: disabled for https://github.com/elastic/logstash/issues/6055 - will have to properly refactor
+        raise("cancel is unsupported")
+        # @cancelled[event] = true
+      end
+
+      def each(&blk)
+        # take care not to cause @originals or @generated to change during iteration
+
+        # below the checks for @cancelled.include?(e) have been replaced by e.cancelled?
+        # TODO: for https://github.com/elastic/logstash/issues/6055 = will have to properly refactor
+        @iterating = true
+        @originals.each do |e, _|
+          blk.call(e) unless e.cancelled?
+        end
+        @generated.each do |e, _|
+          blk.call(e) unless e.cancelled?
+        end
+        @iterating = false
+        update_generated
+      end
+
+      def size
+        filtered_size
+      end
+
+      def starting_size
+        @originals.size
+      end
+
+      def filtered_size
+        @originals.size + @generated.size
+      end
+
+      def cancelled_size
+        # TODO: disabled for https://github.com/elastic/logstash/issues/6055 = will have to properly refactor
+        raise("cancelled_size is unsupported ")
+        # @cancelled.size
+      end
+
+      def shutdown_signal_received?
+        false
+      end
+
+      def flush_signal_received?
+        false
+      end
+
+      private
+
+      def iterating?
+        @iterating
+      end
+
+      def update_generated
+        @generated.update(@iterating_temp)
+        @iterating_temp.clear
+      end
+
+      def take_originals_from_queue(queue, size, wait)
+        @acked_batch = queue.read_batch(size, wait)
+        return if @acked_batch.nil?
+        @acked_batch.get_elements.each do |e|
+          @originals[e] = true
+        end
+      end
+    end
+
+    class WriteClient
+      def initialize(queue)
+        @queue = queue
+      end
+
+      def get_new_batch
+        WriteBatch.new
+      end
+
+      def push(event)
+        if @queue.closed?
+          raise QueueClosedError.new("Attempted to write an event to a closed AckedQueue")
+        end
+        @queue.push(event)
+      end
+      alias_method(:<<, :push)
+
+      def push_batch(batch)
+        if @queue.closed?
+          raise QueueClosedError.new("Attempted to write a batch to a closed AckedQueue")
+        end
+        batch.each do |event|
+          push(event)
+        end
+      end
+    end
+
+    class WriteBatch
+      def initialize
+        @events = []
+      end
+
+      def push(event)
+        @events.push(event)
+      end
+      alias_method(:<<, :push)
+
+      def each(&blk)
+        @events.each do |e|
+          blk.call(e)
+        end
+      end
+    end
+  end
+end end
diff --git a/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
index 55bc66c237e..6e401527696 100644
--- a/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
+++ b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
@@ -5,8 +5,8 @@ class WrappedSynchronousQueue
     java_import java.util.concurrent.SynchronousQueue
     java_import java.util.concurrent.TimeUnit
 
-    def initialize()
-      @queue = java.util.concurrent.SynchronousQueue.new()
+    def initialize
+      @queue = java.util.concurrent.SynchronousQueue.new
     end
 
     # Push an object to the queue if the queue is full
@@ -30,7 +30,7 @@ def offer(obj, timeout_ms)
 
     # Blocking
     def take
-      @queue.take()
+      @queue.take
     end
 
     # Block for X millis
@@ -42,7 +42,7 @@ def write_client
       WriteClient.new(self)
     end
 
-    def read_client()
+    def read_client
       ReadClient.new(self)
     end
 
@@ -51,7 +51,7 @@ class ReadClient
       # from this queue. We also depend on this to be able to block consumers while we snapshot
       # in-flight buffers
 
-      def initialize(queue, batch_size = 125, wait_for = 5)
+      def initialize(queue, batch_size = 125, wait_for = 250)
         @queue = queue
         @mutex = Mutex.new
         # Note that @infilght_batches as a central mechanism for tracking inflight
@@ -64,6 +64,10 @@ def initialize(queue, batch_size = 125, wait_for = 5)
         @wait_for = wait_for
       end
 
+      def close
+        # noop, compat with acked queue read client
+      end
+
       def set_batch_dimensions(batch_size, wait_for)
         @batch_size = batch_size
         @wait_for = wait_for
@@ -145,8 +149,6 @@ def add_output_metrics(batch)
 
     class ReadBatch
       def initialize(queue, size, wait)
-        @shutdown_signal_received = false
-        @flush_signal_received = false
         @originals = Hash.new
 
         # TODO: disabled for https://github.com/elastic/logstash/issues/6055 - will have to properly refactor
@@ -210,14 +212,6 @@ def cancelled_size
         # @cancelled.size
       end
 
-      def shutdown_signal_received?
-        @shutdown_signal_received
-      end
-
-      def flush_signal_received?
-        @flush_signal_received
-      end
-
       private
 
       def iterating?
@@ -231,24 +225,10 @@ def update_generated
 
       def take_originals_from_queue(queue, size, wait)
         size.times do |t|
-          event = (t == 0) ? queue.take : queue.poll(wait)
-          if event.nil?
-            # queue poll timed out
-            next
-          elsif event.is_a?(LogStash::SignalEvent)
-            # We MUST break here. If a batch consumes two SHUTDOWN events
-            # then another worker may have its SHUTDOWN 'stolen', thus blocking
-            # the pipeline.
-            @shutdown_signal_received = event.shutdown?
-
-            # See comment above
-            # We should stop doing work after flush as well.
-            @flush_signal_received = event.flush?
-
-            break
-          else
-            @originals[event] = true
-          end
+          event = queue.poll(wait)
+          return if event.nil? # queue poll timed out
+
+          @originals[event] = true
         end
       end
     end
diff --git a/logstash-core/locales/en.yml b/logstash-core/locales/en.yml
index 144f579d3b5..20fe330cafe 100644
--- a/logstash-core/locales/en.yml
+++ b/logstash-core/locales/en.yml
@@ -188,7 +188,7 @@ en:
         http_host: Web API binding host
         http_port: Web API http port
         pipeline-workers: |+
-          Sets the number of pipeline workers to run.
+          Sets the number of pipeline workers to start.
         pipeline-batch-size: |+
           Size of batches the pipeline is to work in.
         pipeline-batch-delay: |+
@@ -257,7 +257,7 @@ en:
           WARNING: This will include any 'password' options passed to plugin configs as plaintext, and may result
           in plaintext passwords appearing in your logs!
         log_format: |+
-          Specify if Logstash should write its own logs in JSON form (one
+          Specify if Logstash should emit its own logs in JSON form (one
           event per line) or in plain text (using Ruby's Object#inspect)
         debug: |+
           Set the log level to debug.
diff --git a/logstash-core/logstash-core.gemspec b/logstash-core/logstash-core.gemspec
index 3b1ee5c62b3..f59d397b75e 100644
--- a/logstash-core/logstash-core.gemspec
+++ b/logstash-core/logstash-core.gemspec
@@ -11,13 +11,16 @@ Gem::Specification.new do |gem|
   gem.homepage      = "http://www.elastic.co/guide/en/logstash/current/index.html"
   gem.license       = "Apache License (2.0)"
 
-  gem.files         = Dir.glob(["logstash-core.gemspec", "lib/**/*.rb", "spec/**/*.rb", "locales/*", "lib/logstash/api/init.ru", "vendor/jars/**/*.jar"])
+  gem.files         = Dir.glob(["logstash-core.gemspec", "gemspec_jars.rb", "lib/**/*.rb", "spec/**/*.rb", "locales/*", "lib/logstash/api/init.ru"])
   gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
   gem.name          = "logstash-core"
-  gem.require_paths = ["lib", "vendor/jars"]
+  gem.require_paths = ["lib"]
   gem.version       = LOGSTASH_CORE_VERSION
 
-  gem.add_runtime_dependency "logstash-core-event-java", "6.0.0-alpha1"
+  gem.platform = "java"
+
+  gem.add_runtime_dependency "logstash-core-event-java"
+  gem.add_runtime_dependency "logstash-core-queue-jruby"
 
   gem.add_runtime_dependency "pry", "~> 0.10.1"  #(Ruby license)
   gem.add_runtime_dependency "stud", "~> 0.0.19" #(Apache 2.0 license)
@@ -43,21 +46,13 @@ Gem::Specification.new do |gem|
   gem.add_runtime_dependency "rubyzip", "~> 1.1.7"
   gem.add_runtime_dependency "thread_safe", "~> 0.3.5" #(Apache 2.0 license)
 
-  if RUBY_PLATFORM == 'java'
-    gem.platform = RUBY_PLATFORM
-    gem.add_runtime_dependency "jrjackson", "~> 0.4.0" #(Apache 2.0 license)
-  else
-    gem.add_runtime_dependency "oj" #(MIT-style license)
-  end
-
-  if RUBY_ENGINE == "rbx"
-    # rubinius puts the ruby stdlib into gems.
-    gem.add_runtime_dependency "rubysl"
+  gem.add_runtime_dependency "jrjackson", "~> 0.4.0" #(Apache 2.0 license)
 
-    # Include racc to make the xml tests pass.
-    # https://github.com/rubinius/rubinius/issues/2632#issuecomment-26954565
-    gem.add_runtime_dependency "racc"
-  end
+  gem.add_runtime_dependency "jar-dependencies"
+  # as of Feb 3rd 2016, the ruby-maven gem is resolved to version 3.3.3 and that version
+  # has an rdoc problem that causes a bundler exception. 3.3.9 is the current latest version
+  # which does not have this problem.
+  gem.add_runtime_dependency "ruby-maven", "~> 3.3.9"
 
-  gem.add_runtime_dependency 'jar-dependencies', '~> 0.3.4'
+  eval(File.read(File.expand_path("../gemspec_jars.rb", __FILE__)))
 end
diff --git a/logstash-core/logstash-core.iml b/logstash-core/logstash-core.iml
new file mode 100644
index 00000000000..e2b6c5a46ef
--- /dev/null
+++ b/logstash-core/logstash-core.iml
@@ -0,0 +1,76 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<module external.linked.project.id="logstash-core" external.linked.project.path="$MODULE_DIR$" external.root.project.path="$MODULE_DIR$" external.system.id="GRADLE" external.system.module.group="org.logstash" external.system.module.version="6.0.0-alpha1" type="JAVA_MODULE" version="4">
+  <component name="NewModuleRootManager" inherit-compiler-output="false">
+    <output url="file://$MODULE_DIR$/build/classes/main" />
+    <output-test url="file://$MODULE_DIR$/build/classes/test" />
+    <exclude-output />
+    <content url="file://$MODULE_DIR$">
+      <sourceFolder url="file://$MODULE_DIR$/src/main/java" isTestSource="false" />
+      <sourceFolder url="file://$MODULE_DIR$/src/test/java" isTestSource="true" />
+      <sourceFolder url="file://$MODULE_DIR$/src/main/resources" type="java-resource" />
+      <sourceFolder url="file://$MODULE_DIR$/src/test/resources" type="java-test-resource" />
+      <excludeFolder url="file://$MODULE_DIR$/.gradle" />
+      <excludeFolder url="file://$MODULE_DIR$/build" />
+    </content>
+    <orderEntry type="inheritedJdk" />
+    <orderEntry type="sourceFolder" forTests="false" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: org.jruby:jruby-core:1.7.25" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: org.ow2.asm:asm:5.0.3" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: org.ow2.asm:asm-commons:5.0.3" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: org.ow2.asm:asm-analysis:5.0.3" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: org.ow2.asm:asm-util:5.0.3" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: org.jruby.joni:joni:2.1.10" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: com.github.jnr:jnr-netdb:1.1.2" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: com.github.jnr:jnr-enxio:0.12" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: com.github.jnr:jnr-x86asm:1.0.2" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: com.github.jnr:jnr-unixsocket:0.12" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: com.github.jnr:jnr-posix:3.0.29" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: org.jruby.extras:bytelist:1.0.11" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: com.github.jnr:jnr-constants:0.9.1" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: org.jruby.jcodings:jcodings:1.0.16" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: com.github.jnr:jnr-ffi:2.0.9" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: com.github.jnr:jffi:1.2.11" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: com.github.jnr:jffi:1.2.11:native" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: org.yaml:snakeyaml:1.13" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: com.jcraft:jzlib:1.1.3" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: com.headius:invokebinder:1.2" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: com.martiansoftware:nailgun-server:0.9.1" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: org.jruby:yecht:1.1:jruby" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: joda-time:joda-time:2.8.2" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: com.headius:options:1.3" level="project" />
+    <orderEntry type="library" scope="PROVIDED" name="Gradle: org.ow2.asm:asm-tree:5.0.3" level="project" />
+    <orderEntry type="library" name="Gradle: org.apache.logging.log4j:log4j-api:2.6.2" level="project" />
+    <orderEntry type="library" name="Gradle: org.apache.logging.log4j:log4j-core:2.6.2" level="project" />
+    <orderEntry type="library" name="Gradle: com.fasterxml.jackson.core:jackson-core:2.7.4" level="project" />
+    <orderEntry type="library" name="Gradle: com.fasterxml.jackson.core:jackson-databind:2.7.4" level="project" />
+    <orderEntry type="library" name="Gradle: com.fasterxml.jackson.core:jackson-annotations:2.7.0" level="project" />
+    <orderEntry type="library" scope="RUNTIME" name="Gradle: org.apache.logging.log4j:log4j-1.2-api:2.6.2" level="project" />
+    <orderEntry type="library" scope="TEST" name="Gradle: org.apache.logging.log4j:log4j-api:2.6.2:tests" level="project" />
+    <orderEntry type="library" scope="TEST" name="Gradle: org.apache.logging.log4j:log4j-core:2.6.2:tests" level="project" />
+    <orderEntry type="library" scope="TEST" name="Gradle: junit:junit:4.12" level="project" />
+    <orderEntry type="library" scope="TEST" name="Gradle: org.hamcrest:hamcrest-core:1.3" level="project" />
+    <orderEntry type="module-library">
+      <library name="Gradle: logstash-core-event-java">
+        <CLASSES>
+          <root url="jar://$MODULE_DIR$/../logstash-core-event-java/build/libs/logstash-core-event-java.jar!/" />
+        </CLASSES>
+        <JAVADOC />
+        <SOURCES />
+      </library>
+    </orderEntry>
+    <orderEntry type="module-library">
+      <library>
+        <CLASSES>
+          <root url="file://$MODULE_DIR$/../logstash-core-event-java" />
+        </CLASSES>
+        <JAVADOC>
+          <root url="file://$MODULE_DIR$/../logstash-core-event-java/build/docs/javadoc" />
+        </JAVADOC>
+        <SOURCES>
+          <root url="file://$MODULE_DIR$/../logstash-core-event-java/src/main/java" />
+          <root url="file://$MODULE_DIR$/../logstash-core-event-java/src/test/java" />
+        </SOURCES>
+      </library>
+    </orderEntry>
+  </component>
+</module>
\ No newline at end of file
diff --git a/logstash-core/settings.gradle b/logstash-core/settings.gradle
new file mode 100644
index 00000000000..4da77f25813
--- /dev/null
+++ b/logstash-core/settings.gradle
@@ -0,0 +1 @@
+rootProject.name = 'logstash-core'
diff --git a/logstash-core/spec/logstash/compiler/compiler_spec.rb b/logstash-core/spec/logstash/compiler/compiler_spec.rb
new file mode 100644
index 00000000000..2e00160b18e
--- /dev/null
+++ b/logstash-core/spec/logstash/compiler/compiler_spec.rb
@@ -0,0 +1,616 @@
+require "spec_helper"
+java_import Java::OrgLogstashConfigIr::DSL
+
+describe LogStash::Compiler do
+  def j
+    Java::OrgLogstashConfigIr::DSL
+  end
+
+  # Static import of these useful enums
+  INPUT = Java::OrgLogstashConfigIr::PluginDefinition::Type::INPUT
+  FILTER = Java::OrgLogstashConfigIr::PluginDefinition::Type::FILTER
+  OUTPUT = Java::OrgLogstashConfigIr::PluginDefinition::Type::OUTPUT
+  CODEC = Java::OrgLogstashConfigIr::PluginDefinition::Type::OUTPUT
+
+
+  describe "compiling to a PipelineRunner" do
+    subject(:source_file) { "fake_sourcefile" }
+    subject(:compiled) { described_class.compile(producer, source_file) }
+
+    describe "a simple config" do
+      let(:producer) do
+        <<-EOC
+          input {
+            stdin {}
+          }
+          filter {
+            mutate { add_field => {"something" => "else"} }
+          }
+          output {
+            stdout { codec => rubydebug }
+          }
+        EOC
+      end
+
+      it "should run correctly" do
+          compiled.start(1)
+        puts "SLEEP"
+        sleep 100
+      end
+    end
+  end
+
+  describe "compiling to Pipeline" do
+    subject(:source_file) { "fake_sourcefile" }
+    subject(:compiled) { described_class.compile_graph(producer, source_file) }
+
+    describe "a complex config" do
+      let(:producer) do
+        <<-EOC
+        input {
+          foo {a => 'b'}
+          bar {}
+        }
+
+        filter {
+          if [type] == "hostbytes" {
+            grok {
+              match => { data => "%{WORD:hostname} %{NUMBER:bytes}" }
+            }
+            mutate {
+              lowercase => ["hostname"]
+            }
+            if [hostname] != "localhost" and [hostname] != "127.0.0.1" or [hostname] == "special" {
+              mutate {
+                add_field => {"notlocalhost" => "true"}
+              }
+            } else {
+              drop {}
+            }
+          } else if [type] == "crazy" {
+            mutate {
+              add_tag => "thisiscrazy"
+            }
+          } else if [type] =~ /(in)?sane/ {
+            mutate { add_tag => "thisissane" }
+          } else {
+            drop {}
+          }
+        }
+
+        output {
+          stdout { codec => rubydebug }
+          if [size] > 1000 {
+            s3 {}
+          } else {
+            elasticsearch {}
+          }
+        }
+        EOC
+      end
+
+      it "should compile" do
+        puts "\n!!!SOURCE!!!"
+        puts producer
+        puts "\n!!!AST!!!"
+        puts described_class.compile_ast(producer).inspect
+        puts "\n!!!IMPERATIVE!!!"
+        imp = described_class.compile_imperative(producer)
+        imp.each {|s,v| puts "\n!#{s}\n"; puts v.to_s}
+        puts "\n!!!GRAPH!!!\n"
+        puts g = described_class.compile_graph(producer)
+        g.each {|s,v| puts "\n!#{s}\n"; puts v.to_s}
+        puts "\n!!!PIPELINE!!!"
+        puts compiled.to_s
+      end
+    end
+  end
+
+  describe "compiling imperative" do
+    subject(:source_file) { "fake_sourcefile" }
+    subject(:compiled) { described_class.compile_imperative(producer, source_file) }
+
+    describe "an empty file" do
+      let(:producer) { "input {} output {}" }
+
+      it "should have an empty input block" do
+        expect(compiled[:input]).to ir_eql(j.noop)
+      end
+
+      it "should have an empty filter block" do
+        expect(compiled[:filter]).to ir_eql(j.noop)
+      end
+
+      it "should have an empty output block" do
+        expect(compiled[:output]).to ir_eql(j.noop)
+      end
+    end
+
+    describe "SourceMetadata" do
+      let(:producer) { "input { generator {} } output { }" }
+
+      it "should attach correct producer text for components" do
+        expect(compiled[:input].get_meta.getSourceText).to eql("generator {}")
+      end
+    end
+
+    context "plugins" do
+      subject(:c_plugin) { compiled[:input] }
+      let(:producer) { "input { #{plugin_source} } " }
+
+      describe "a simple plugin" do
+        let(:plugin_source) { "generator {}" }
+
+        it "should contain the plugin" do
+          expect(c_plugin).to ir_eql(j.iPlugin(INPUT, "generator"))
+        end
+      end
+
+      describe "a plugin with mixed parameter types" do
+        let(:plugin_source) { "generator { aarg => [1] hasharg => {foo => bar} iarg => 123 farg => 123.123 sarg => 'hello'}" }
+
+        it "should contain the plugin" do
+          expect(c_plugin).to ir_eql(j.iPlugin(INPUT, "generator", {"aarg" => [1],
+                                                                "hasharg" => {"foo" => "bar"},
+                                                                "iarg" => 123,
+                                                                "farg" => 123.123,
+                                                                "sarg" => 'hello'}))
+        end
+      end
+    end
+
+    context "inputs" do
+      subject(:input) { compiled[:input] }
+
+      describe "a single input" do
+        let(:producer) { "input { generator {} }" }
+
+        it "should contain the single input" do
+          expect(input).to ir_eql(j.iPlugin(INPUT, "generator"))
+        end
+      end
+
+      describe "two inputs" do
+        let(:producer) { "input { generator { count => 1 } generator { count => 2 } } output { }" }
+
+        it "should contain both inputs" do
+          expect(input).to ir_eql(j.iComposeParallel(
+                                j.iPlugin(INPUT, "generator", {"count" => 1}),
+                                j.iPlugin(INPUT, "generator", {"count" => 2})
+                              ))
+        end
+      end
+    end
+
+    shared_examples_for "complex grammar" do |section|
+      let(:section_name_enum) {
+        case section
+        when :input
+          INPUT
+        when :filter
+          FILTER
+        when :output
+          OUTPUT
+        else
+          raise "Unknown section"
+        end
+      }
+
+      let(:section) { section }
+      let (:c_section) { compiled[section] }
+
+      def splugin(*args)
+        j.iPlugin(section_name_enum, *args)
+      end
+
+      def compose(*statements)
+        if section == :filter
+          j.iComposeSequence(*statements)
+        else
+          j.iComposeParallel(*statements)
+        end
+      end
+
+      describe "two plugins" do
+        let(:producer) do
+          # We care about line/column for this test, hence the indentation
+          <<-EOS
+          #{section} {
+            aplugin { count => 1 }
+            aplugin { count => 2 }
+            }
+          EOS
+        end
+
+        it "should contain both" do
+          expect(c_section).to ir_eql(compose(
+                                        splugin("aplugin", {"count" => 1}),
+                                        splugin("aplugin", {"count" => 2})
+                                      ))
+        end
+
+        it "should attach source_metadata with correct info to the statements" do
+          meta = c_section.statements.first.meta
+          expect(meta.getSourceText).to eql("aplugin { count => 1 }")
+          expect(meta.getSourceLine).to eql(2)
+          expect(meta.getSourceColumn).to eql(13)
+          expect(meta.getSourceFile).to eql(source_file)
+          expect(c_section.statements.first.meta)
+          expect(c_section)
+        end
+      end
+
+      describe "if conditions" do
+        describe "conditional expressions" do
+          let(:producer) { "#{section} { if (#{expression}) { aplugin {} } }" }
+          let(:c_expression) { c_section.getBooleanExpression }
+
+          describe "logical expressions" do
+            describe "simple and" do
+              let(:expression) { "2 > 1 and 1 < 2" }
+
+              it "should compile correctly" do
+                expect(c_expression).to ir_eql(
+                                          j.eAnd(
+                                            j.eGt(j.eValue(2), j.eValue(1)),
+                                            j.eLt(j.eValue(1), j.eValue(2))
+                                          ))
+              end
+            end
+
+            describe "'in' array" do
+              let(:expression) { "'foo' in ['foo', 'bar']" }
+
+              it "should compile correctly" do
+                expect(c_expression).to ir_eql(
+                                          j.eIn(
+                                            j.eValue('foo'),
+                                            j.eValue(['foo', 'bar'])
+                                          ))
+              end
+            end
+
+            describe "'not in' array" do
+              let(:expression) { "'foo' not in ['foo', 'bar']" }
+
+              it "should compile correctly" do
+                expect(c_expression).to ir_eql(
+                                          j.eNot(
+                                            j.eIn(
+                                              j.eValue('foo'),
+                                              j.eValue(['foo', 'bar'])
+                                            )))
+              end
+            end
+
+            describe "'not'" do
+              let(:expression) { "!(1 > 2)" }
+
+              it "should compile correctly" do
+                expect(c_expression).to ir_eql(j.eNot(j.eGt(j.eValue(1), j.eValue(2))))
+              end
+            end
+
+            describe "and or precedence" do
+              let(:expression) { "2 > 1 and 1 < 2 or 3 < 2" }
+
+              it "should compile correctly" do
+                expect(c_expression).to ir_eql(
+                                          j.eOr(
+                                            j.eAnd(
+                                              j.eGt(j.eValue(2), j.eValue(1)),
+                                              j.eLt(j.eValue(1), j.eValue(2))
+                                            ),
+                                            j.eLt(j.eValue(3), j.eValue(2))
+                                          )
+                                        )
+              end
+
+              describe "multiple or" do
+                let(:expression) { "2 > 1 or 1 < 2 or 3 < 2" }
+
+                it "should compile correctly" do
+                  expect(c_expression).to ir_eql(
+                                            j.eOr(
+                                              j.eGt(j.eValue(2), j.eValue(1)),
+                                              j.eOr(
+                                                j.eLt(j.eValue(1), j.eValue(2)),
+                                                j.eLt(j.eValue(3), j.eValue(2))
+                                              )
+                                            )
+                                          )
+                end
+              end
+
+              describe "a complex expression" do
+                let(:expression) { "1 > 2 and 3 > 4 or 6 > 7 and 8 > 9" }
+                false and false or true and true
+
+                it "should compile correctly" do
+                  expect(c_expression).to ir_eql(
+                                            j.eOr(
+                                              j.eAnd(
+                                                j.eGt(j.eValue(1), j.eValue(2)),
+                                                j.eGt(j.eValue(3), j.eValue(4))
+                                              ),
+                                              j.eAnd(
+                                                j.eGt(j.eValue(6), j.eValue(7)),
+                                                j.eGt(j.eValue(8), j.eValue(9))
+                                              )
+                                            )
+                                          )
+                end
+              end
+
+              describe "a complex nested expression" do
+                let(:expression) { "1 > 2 and (1 > 2 and 3 > 4 or 6 > 7 and 8 > 9) or 6 > 7 and 8 > 9" }
+                false and false or true and true
+
+                it "should compile correctly" do
+                  expect(c_expression).to ir_eql(
+                                            j.eOr(
+                                              j.eAnd(
+                                                j.eGt(j.eValue(1), j.eValue(2)),
+                                                j.eOr(
+                                                  j.eAnd(
+                                                    j.eGt(j.eValue(1), j.eValue(2)),
+                                                    j.eGt(j.eValue(3), j.eValue(4))
+                                                  ),
+                                                  j.eAnd(
+                                                    j.eGt(j.eValue(6), j.eValue(7)),
+                                                    j.eGt(j.eValue(8), j.eValue(9))
+                                                  )
+                                                )
+                                              ),
+                                              j.eAnd(
+                                                j.eGt(j.eValue(6), j.eValue(7)),
+                                                j.eGt(j.eValue(8), j.eValue(9))
+                                              )
+                                            )
+                                          )
+                end
+              end
+            end
+          end
+
+          describe "comparisons" do
+            describe "field not null" do
+              let(:expression) { "[foo]"}
+
+              it "should compile correctly" do
+                expect(c_expression).to ir_eql(j.eTruthy(j.eEventValue("[foo]")))
+              end
+            end
+
+            describe "'=='" do
+              let(:expression) { "[foo] == 5"}
+
+              it "should compile correctly" do
+                expect(c_expression).to ir_eql(j.eEq(j.eEventValue("[foo]"), j.eValue(5.to_java)))
+              end
+            end
+
+            describe "'!='" do
+              let(:expression) { "[foo] != 5"}
+
+              it "should compile correctly" do
+                expect(c_expression).to ir_eql(j.eNeq(j.eEventValue("[foo]"), j.eValue(5.to_java)))
+              end
+            end
+
+            describe "'>'" do
+              let(:expression) { "[foo] > 5"}
+
+              it "should compile correctly" do
+                expect(c_expression).to ir_eql(j.eGt(j.eEventValue("[foo]"), j.eValue(5.to_java)))
+              end
+            end
+
+            describe "'<'" do
+              let(:expression) { "[foo] < 5"}
+
+              it "should compile correctly" do
+                expect(c_expression).to ir_eql(j.eLt(j.eEventValue("[foo]"), j.eValue(5.to_java)))
+              end
+            end
+
+            describe "'>='" do
+              let(:expression) { "[foo] >= 5"}
+
+              it "should compile correctly" do
+                expect(c_expression).to ir_eql(j.eGte(j.eEventValue("[foo]"), j.eValue(5.to_java)))
+              end
+            end
+
+            describe "'<='" do
+              let(:expression) { "[foo] <= 5"}
+
+              it "should compile correctly" do
+                expect(c_expression).to ir_eql(j.eLte(j.eEventValue("[foo]"), j.eValue(5.to_java)))
+              end
+            end
+
+            describe "'=~'" do
+              let(:expression) { "[foo] =~ /^abc$/"}
+
+              it "should compile correctly" do
+                expect(c_expression).to ir_eql(j.eRegexEq(j.eEventValue("[foo]"), j.eRegex('^abc$')))
+              end
+            end
+
+            describe "'!~'" do
+              let(:expression) { "[foo] !~ /^abc$/"}
+
+              it "should compile correctly" do
+                expect(c_expression).to ir_eql(j.eRegexNeq(j.eEventValue("[foo]"), j.eRegex('^abc$')))
+              end
+            end
+          end
+        end
+
+        describe "only true branch" do
+          let (:producer) { "#{section} { if [foo] == [bar] { grok {} } }" }
+
+          it "should compile correctly" do
+            expect(c_section).to ir_eql(j.iIf(
+                                            j.eEq(j.eEventValue("[foo]"), j.eEventValue("[bar]")),
+                                            splugin("grok")
+                                          )
+                                       )
+          end
+        end
+
+        describe "only false branch" do
+          let (:producer) { "#{section} { if [foo] == [bar] { } else { fplugin {} } }" }
+
+          it "should compile correctly" do
+            expect(c_section).to ir_eql(j.iIf(
+                                          j.eEq(j.eEventValue("[foo]"), j.eEventValue("[bar]")),
+                                          j.noop,
+                                          splugin("fplugin"),
+                                        )
+                                       )
+          end
+        end
+
+        describe "empty if statement" do
+          let (:producer) { "#{section} { if [foo] == [bar] { } }" }
+
+          it "should compile correctly" do
+            expect(c_section).to ir_eql(j.iIf(
+                                          j.eEq(j.eEventValue("[foo]"), j.eEventValue("[bar]")),
+                                          j.noop,
+                                          j.noop
+                                        )
+                                       )
+          end
+        end
+
+        describe "if else" do
+          let (:producer) { "#{section} { if [foo] == [bar] { tplugin {} } else { fplugin {} } }" }
+
+          it "should compile correctly" do
+            expect(c_section).to ir_eql(j.iIf(
+                                          j.eEq(j.eEventValue("[foo]"), j.eEventValue("[bar]")),
+                                          splugin("tplugin"),
+                                          splugin("fplugin")
+                                        )
+                                       )
+          end
+        end
+
+        describe "if elsif else" do
+          let (:producer) { "#{section} { if [foo] == [bar] { tplugin {} } else if [bar] == [baz] { eifplugin {} } else { fplugin {} } }" }
+
+          it "should compile correctly" do
+            expect(c_section).to ir_eql(j.iIf(
+                                          j.eEq(j.eEventValue("[foo]"), j.eEventValue("[bar]")),
+                                          splugin("tplugin"),
+                                          j.iIf(
+                                            j.eEq(j.eEventValue("[bar]"), j.eEventValue("[baz]")),
+                                            splugin("eifplugin"),
+                                            splugin("fplugin")
+                                          )
+                                        )
+                                       )
+          end
+        end
+
+        describe "if elsif elsif else" do
+          let (:producer) do
+            <<-EOS
+              #{section} {
+                if [foo] == [bar] { tplugin {} }
+                else if [bar] == [baz] { eifplugin {} }
+                else if [baz] == [bot] { eeifplugin {} }
+                else { fplugin {} }
+              }
+            EOS
+          end
+
+          it "should compile correctly" do
+            expect(c_section).to ir_eql(j.iIf(
+                                          j.eEq(j.eEventValue("[foo]"), j.eEventValue("[bar]")),
+                                          splugin("tplugin"),
+                                          j.iIf(
+                                            j.eEq(j.eEventValue("[bar]"), j.eEventValue("[baz]")),
+                                            splugin("eifplugin"),
+                                            j.iIf(
+                                              j.eEq(j.eEventValue("[baz]"), j.eEventValue("[bot]")),
+                                              splugin("eeifplugin"),
+                                              splugin("fplugin")
+                                            )
+                                          )
+                                        )
+                                       )
+          end
+
+          describe "nested ifs" do
+let (:producer) do
+            <<-EOS
+              #{section} {
+                if [foo] == [bar] {
+                  if [bar] == [baz] { aplugin {} }
+                } else {
+                  if [bar] == [baz] { bplugin {} }
+                  else if [baz] == [bot] { cplugin {} }
+                  else { dplugin {} }
+                }
+              }
+            EOS
+
+          end
+
+          it "should compile correctly" do
+            expect(c_section).to ir_eql(j.iIf(
+                                          j.eEq(j.eEventValue("[foo]"), j.eEventValue("[bar]")),
+                                          j.iIf(j.eEq(j.eEventValue("[bar]"), j.eEventValue("[baz]")),
+                                                   splugin("aplugin"),
+                                                   j.noop
+                                                  ),
+                                          j.iIf(
+                                            j.eEq(j.eEventValue("[bar]"), j.eEventValue("[baz]")),
+                                            splugin("bplugin"),
+                                            j.iIf(
+                                              j.eEq(j.eEventValue("[baz]"), j.eEventValue("[bot]")),
+                                              splugin("cplugin"),
+                                              splugin("dplugin")
+                                            )
+                                          )
+                                        )
+                                       )
+          end
+          end
+        end
+      end
+    end
+
+    context "filters" do
+      subject(:filter) { compiled[:filter] }
+
+      describe "a single filter" do
+        let(:producer) { "input { } filter { grok {} } output { }" }
+
+        it "should contain the single input" do
+          expect(filter).to ir_eql(j.iPlugin(FILTER, "grok"))
+        end
+      end
+
+      it_should_behave_like "complex grammar", :filter
+    end
+
+    context "outputs" do
+      subject(:output) { compiled[:output] }
+
+      describe "a single output" do
+        let(:producer) { "input { } output { stdout {} }" }
+
+        it "should contain the single input" do
+          expect(output).to ir_eql(j.iPlugin(OUTPUT, "stdout"))
+        end
+      end
+
+      it_should_behave_like "complex grammar", :output
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/pipeline_pq_file_spec.rb b/logstash-core/spec/logstash/pipeline_pq_file_spec.rb
new file mode 100644
index 00000000000..9349868eac6
--- /dev/null
+++ b/logstash-core/spec/logstash/pipeline_pq_file_spec.rb
@@ -0,0 +1,128 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/inputs/generator"
+require "logstash/filters/multiline"
+
+class PipelinePqFileOutput < LogStash::Outputs::Base
+  config_name "pipelinepqfileoutput"
+  milestone 2
+
+  attr_reader :num_closes, :event_count
+
+  def self.make_shared
+    @concurrency = :shared
+  end
+
+  def initialize(params={})
+    super
+    @num_closes = 0
+    @event_count = 0
+    @mutex = Mutex.new
+  end
+
+  def register
+    self.class.make_shared
+  end
+
+  def receive(event)
+    @mutex.synchronize do
+      @event_count = @event_count.succ
+    end
+  end
+
+  def close
+    @num_closes = 1
+  end
+end
+
+describe LogStash::Pipeline do
+  let(:pipeline_settings_obj) { LogStash::SETTINGS }
+  let(:pipeline_id) { "main" }
+
+  let(:multiline_id) { "my-multiline" }
+  let(:output_id) { "my-pipelinepqfileoutput" }
+  let(:generator_id) { "my-generator" }
+  let(:config) do
+    <<-EOS
+    input {
+      generator {
+        count => #{number_of_events}
+        id => "#{generator_id}"
+      }
+    }
+    filter {
+      multiline {
+        id => "#{multiline_id}"
+        pattern => "hello"
+        what => next
+      }
+    }
+    output {
+      pipelinepqfileoutput {
+        id => "#{output_id}"
+      }
+    }
+    EOS
+  end
+
+   let(:pipeline_settings) { { "queue.type" => queue_type, "pipeline.workers" => worker_thread_count, "pipeline.id" => pipeline_id} }
+
+  subject { described_class.new(config, pipeline_settings_obj, metric) }
+
+  let(:counting_output) { PipelinePqFileOutput.new({ "id" => output_id }) }
+  let(:metric_store) { subject.metric.collector.snapshot_metric.metric_store }
+  let(:metric) { LogStash::Instrument::Metric.new(LogStash::Instrument::Collector.new) }
+  let(:base_queue_path) { pipeline_settings_obj.get("path.queue") }
+  let(:this_queue_folder) { File.join(base_queue_path, SecureRandom.hex(8)) }
+
+  let(:worker_thread_count) { 8 } # 1 4 8
+  let(:number_of_events) { 100_000 }
+  let(:page_capacity) { 1 * 1024 * 512 } # 1 128
+  let(:queue_type) { "persisted" } #  "memory" "memory_acked"
+  let(:times) { [] }
+
+  before :each do
+    FileUtils.mkdir_p(this_queue_folder)
+
+    pipeline_settings_obj.set("path.queue", this_queue_folder)
+    allow(PipelinePqFileOutput).to receive(:new).with(any_args).and_return(counting_output)
+    allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
+    allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
+    allow(LogStash::Plugin).to receive(:lookup).with("filter", "multiline").and_return(LogStash::Filters::Multiline)
+    allow(LogStash::Plugin).to receive(:lookup).with("output", "pipelinepqfileoutput").and_return(PipelinePqFileOutput)
+
+    pipeline_workers_setting = LogStash::SETTINGS.get_setting("pipeline.workers")
+    allow(pipeline_workers_setting).to receive(:default).and_return(worker_thread_count)
+    pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
+    pipeline_settings_obj.set("queue.page_capacity", page_capacity)
+    Thread.new do
+      # make sure we have received all the generated events
+      while counting_output.event_count < number_of_events do
+        sleep 1
+      end
+      subject.shutdown
+    end
+    times.push(Time.now.to_f)
+    subject.run
+    times.unshift(Time.now.to_f - times.first)
+  end
+
+  after :each do
+    # Dir.rm_rf(this_queue_folder)
+  end
+
+  let(:collected_metric) { metric_store.get_with_path("stats/pipelines/") }
+
+  it "populates the pipelines core metrics" do
+    _metric = collected_metric[:stats][:pipelines][:main][:events]
+    expect(_metric[:duration_in_millis].value).not_to be_nil
+    expect(_metric[:in].value).to eq(number_of_events)
+    expect(_metric[:filtered].value).to eq(number_of_events)
+    expect(_metric[:out].value).to eq(number_of_events)
+    STDOUT.puts "  queue.type: #{pipeline_settings_obj.get("queue.type")}"
+    STDOUT.puts "  queue.page_capacity: #{pipeline_settings_obj.get("queue.page_capacity") / 1024}KB"
+    STDOUT.puts "  workers: #{worker_thread_count}"
+    STDOUT.puts "  events: #{number_of_events}"
+    STDOUT.puts "  took: #{times.first}s"
+  end
+end
diff --git a/logstash-core/spec/logstash/pipeline_spec.rb b/logstash-core/spec/logstash/pipeline_spec.rb
index 197f0631bd8..06007ecb1de 100644
--- a/logstash-core/spec/logstash/pipeline_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_spec.rb
@@ -447,6 +447,9 @@ class TestPipeline < LogStash::Pipeline
       allow(settings).to receive(:get_value).with("pipeline.id").and_return("main")
       allow(settings).to receive(:get_value).with("metric.collect").and_return(false)
       allow(settings).to receive(:get_value).with("config.debug").and_return(false)
+      allow(settings).to receive(:get).with("queue.type").and_return("memory")
+      allow(settings).to receive(:get).with("queue.page_capacity").and_return(1024 * 1024)
+      allow(settings).to receive(:get).with("queue.max_events").and_return(250)
 
       pipeline = LogStash::Pipeline.new(config, settings)
       expect(pipeline.metric).to be_kind_of(LogStash::Instrument::NullMetric)
@@ -509,7 +512,7 @@ class TestPipeline < LogStash::Pipeline
       pipeline = LogStash::Pipeline.new(config, pipeline_settings_obj)
       Thread.new { pipeline.run }
       sleep 0.1 while !pipeline.ready?
-      wait(5).for do
+      wait(3).for do
         # give us a bit of time to flush the events
         output.events.empty?
       end.to be_falsey
@@ -549,10 +552,11 @@ class TestPipeline < LogStash::Pipeline
   end
 
   context "#started_at" do
+    # use a run limiting count to shutdown the pipeline automatically
     let(:config) do
       <<-EOS
       input {
-        generator {}
+        generator { count => 10 }
       }
       EOS
     end
@@ -564,8 +568,7 @@ class TestPipeline < LogStash::Pipeline
     end
 
     it "return when the pipeline started working" do
-      t = Thread.new { subject.run }
-      sleep(0.1)
+      subject.run
       expect(subject.started_at).to be < Time.now
       subject.shutdown
     end
@@ -604,7 +607,7 @@ class TestPipeline < LogStash::Pipeline
 
     let(:pipeline_settings) { { "pipeline.id" => pipeline_id } }
     let(:pipeline_id) { "main" }
-    let(:number_of_events) { 1000 }
+    let(:number_of_events) { 420 }
     let(:multiline_id) { "my-multiline" }
     let(:multiline_id_other) { "my-multiline_other" }
     let(:dummy_output_id) { "my-dummyoutput" }
@@ -648,13 +651,10 @@ class TestPipeline < LogStash::Pipeline
 
       Thread.new { subject.run }
       # make sure we have received all the generated events
-
-      times = 0
-      while dummyoutput.events.size < number_of_events
-        times += 1
-        sleep 0.25
-        raise "Waited too long" if times > 4
-      end
+      wait(3).for do
+        # give us a bit of time to flush the events
+        dummyoutput.events.size < number_of_events
+      end.to be_falsey
     end
 
     after :each do
diff --git a/logstash-core/spec/logstash/settings/bytes_spec.rb b/logstash-core/spec/logstash/settings/bytes_spec.rb
new file mode 100644
index 00000000000..b4fe0aab765
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/bytes_spec.rb
@@ -0,0 +1,53 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Setting::Bytes do
+  let(:multipliers) do
+    {
+      "b" => 1,
+      "kb" => 1 << 10,
+      "mb" => 1 << 20,
+      "gb" => 1 << 30,
+      "tb" => 1 << 40,
+      "pb" => 1 << 50,
+    }
+  end
+
+  let(:number) { Flores::Random.number(0..1000) }
+  let(:unit) { Flores::Random.item(multipliers.keys) }
+  let(:default) { "0b" }
+
+  subject { described_class.new("a byte value", default, false) }
+
+  describe "#set" do
+
+    # Hard-coded test just to make sure at least one known case is working
+    context "when given '10mb'" do
+      it "returns 10485760" do
+        expect(subject.set("10mb")).to be == 10485760
+      end
+    end
+
+    context "when given a string" do
+      context "which is a valid byte unit" do
+        let(:text) { "#{number}#{unit}" }
+
+        before { subject.set(text) }
+
+        it "should coerce it to a Fixnum" do
+          expect(subject.value).to be_a(Fixnum)
+        end
+      end
+
+      context "which is not a valid byte unit" do
+        values = [ "hello world", "1234", "", "-__-" ]
+        values.each do |value|
+          it "should fail" do
+            expect { subject.set(value) }.to raise_error
+          end
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/util/byte_value_spec.rb b/logstash-core/spec/logstash/util/byte_value_spec.rb
new file mode 100644
index 00000000000..57c3f645ee1
--- /dev/null
+++ b/logstash-core/spec/logstash/util/byte_value_spec.rb
@@ -0,0 +1,32 @@
+require "logstash/util/byte_value"
+require "flores/random"
+
+describe LogStash::Util::ByteValue do
+  let(:multipliers) do
+    {
+      "b" => 1,
+      "kb" => 1 << 10,
+      "mb" => 1 << 20,
+      "gb" => 1 << 30,
+      "tb" => 1 << 40,
+      "pb" => 1 << 50,
+    }
+  end
+
+  let(:number) { Flores::Random.number(0..100000000000) }
+  let(:unit) { Flores::Random.item(multipliers.keys) }
+  let(:text) { "#{number}#{unit}" }
+
+  describe "#parse" do
+    let(:expected) { number * multipliers[unit] }
+    subject { described_class.parse(text) }
+
+    it "should return a Numeric" do
+      expect(subject).to be_a(Numeric)
+    end
+
+    it "should have an expected byte value" do
+      expect(subject).to be == expected
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/util/charset_spec.rb b/logstash-core/spec/logstash/util/charset_spec.rb
index 5111a0c7e34..c816c22af08 100644
--- a/logstash-core/spec/logstash/util/charset_spec.rb
+++ b/logstash-core/spec/logstash/util/charset_spec.rb
@@ -5,7 +5,7 @@
 describe LogStash::Util::Charset do
   let(:logger) { double("logger") }
 
-  context "with valid UTF-8 source encoding" do
+  context "with valid UTF-8 producer encoding" do
     subject {LogStash::Util::Charset.new("UTF-8")}
 
     it "should return untouched data" do
@@ -17,7 +17,7 @@
     end
   end
 
-  context "with invalid UTF-8 source encoding" do
+  context "with invalid UTF-8 producer encoding" do
     subject do
       LogStash::Util::Charset.new("UTF-8").tap do |charset|
         charset.logger = logger
@@ -37,7 +37,7 @@
 
   end
 
-  context "with valid non UTF-8 source encoding" do
+  context "with valid non UTF-8 producer encoding" do
     subject {LogStash::Util::Charset.new("ISO-8859-1")}
 
     it "should encode to UTF-8" do
@@ -55,7 +55,7 @@
     end
   end
 
-  context "with invalid non UTF-8 source encoding" do
+  context "with invalid non UTF-8 producer encoding" do
     subject {LogStash::Util::Charset.new("ASCII-8BIT")}
 
     it "should encode to UTF-8 and replace invalid chars" do
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java
new file mode 100644
index 00000000000..7ff83a1c75d
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java
@@ -0,0 +1,44 @@
+package org.logstash.ackedqueue;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.List;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+public class Batch implements Closeable {
+
+    private final List<Queueable> elements;
+
+    private final List<Long> seqNums;
+    private final Queue queue;
+    private final AtomicBoolean closed;
+
+    public Batch(List<Queueable> elements, List<Long> seqNums, Queue q) {
+        this.elements = elements;
+        this.seqNums = seqNums;
+        this.queue = q;
+        this.closed = new AtomicBoolean(false);
+    }
+
+    // close acks the batch ackable events
+    public void close() throws IOException {
+        if (closed.getAndSet(true) == false) {
+              this.queue.ack(this.seqNums);
+        } else {
+            // TODO: how should we handle double-closing?
+            throw new IOException("double closing batch");
+        }
+    }
+
+    public int size() {
+        return elements.size();
+    }
+
+    public List<? extends Queueable> getElements() {
+        return elements;
+    }
+
+    public Queue getQueue() {
+        return queue;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/BeheadedPage.java b/logstash-core/src/main/java/org/logstash/ackedqueue/BeheadedPage.java
new file mode 100644
index 00000000000..044e5c59ccf
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/BeheadedPage.java
@@ -0,0 +1,48 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.common.io.CheckpointIO;
+import org.logstash.common.io.PageIO;
+
+import java.io.IOException;
+import java.util.BitSet;
+
+public class BeheadedPage extends Page {
+
+    // create a new BeheadedPage object from a HeadPage object
+    public BeheadedPage(HeadPage page) {
+        super(page.pageNum, page.queue, page.minSeqNum, page.elementCount, page.firstUnreadSeqNum, page.ackedSeqNums, page.pageIO);
+    }
+
+    // create a new BeheadedPage object for an exiting Checkpoint and data file
+    public BeheadedPage(Checkpoint checkpoint, Queue queue, PageIO pageIO) throws IOException {
+        super(checkpoint.getPageNum(), queue, checkpoint.getMinSeqNum(), checkpoint.getElementCount(), checkpoint.getFirstUnackedSeqNum(), new BitSet(), pageIO);
+
+        // open the data file and reconstruct the IO object internal state
+        pageIO.open(checkpoint.getMinSeqNum(), checkpoint.getElementCount());
+
+        // this page ackedSeqNums bitset is a new empty bitset, if we have some acked elements, set them in the bitset
+        if (checkpoint.getFirstUnackedSeqNum() > checkpoint.getMinSeqNum()) {
+            this.ackedSeqNums.flip(0, (int) (checkpoint.getFirstUnackedSeqNum() - checkpoint.getMinSeqNum()));
+        }
+    }
+
+    public void checkpoint() throws IOException {
+        // TODO: not concurrent for first iteration:
+
+        // since this is a tail page and no write can happen in this page, there is no point in performing a fsync on this page, just stamp checkpoint
+        CheckpointIO io = queue.getCheckpointIO();
+        this.lastCheckpoint = io.write(io.tailFileName(this.pageNum), this.pageNum, this.queue.firstUnackedPageNum(), firstUnackedSeqNum(), this.minSeqNum, this.elementCount);
+    }
+
+    // delete all IO files associated with this page
+    public void purge() throws IOException {
+        this.pageIO.purge();
+        CheckpointIO io = queue.getCheckpointIO();
+        io.purge(io.tailFileName(this.pageNum));
+    }
+
+    public void close() throws IOException {
+        checkpoint();
+        this.pageIO.close();
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Checkpoint.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Checkpoint.java
new file mode 100644
index 00000000000..4cee0611d83
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Checkpoint.java
@@ -0,0 +1,47 @@
+package org.logstash.ackedqueue;
+
+public class Checkpoint {
+//    Checkpoint file structure see FileCheckpointIO
+
+    public static final int VERSION = 1;
+
+    private final int pageNum;             // local per-page page number
+    private final int firstUnackedPageNum; // queue-wide global pointer, only valid in the head checkpoint
+    private final long firstUnackedSeqNum; // local per-page unacknowledged tracking
+    private final long minSeqNum;          // local per-page minimum seqNum
+    private final int elementCount;        // local per-page element count
+
+
+    public Checkpoint(int pageNum, int firstUnackedPageNum, long firstUnackedSeqNum, long minSeqNum, int elementCount) {
+        this.pageNum = pageNum;
+        this.firstUnackedPageNum = firstUnackedPageNum;
+        this.firstUnackedSeqNum = firstUnackedSeqNum;
+        this.minSeqNum = minSeqNum;
+        this.elementCount = elementCount;
+    }
+
+    public int getPageNum() {
+        return this.pageNum;
+    }
+
+    public int getFirstUnackedPageNum() {
+        return this.firstUnackedPageNum;
+    }
+
+    public long getFirstUnackedSeqNum() {
+        return this.firstUnackedSeqNum;
+    }
+
+    public long getMinSeqNum() {
+        return this.minSeqNum;
+    }
+
+    public int getElementCount() {
+        return this.elementCount;
+    }
+
+    public String toString() {
+        return "pageNum=" + this.pageNum + ", firstUnackedPageNum=" + this.firstUnackedPageNum + ", firstUnackedSeqNum=" + this.firstUnackedSeqNum + ", minSeqNum=" + this.minSeqNum + ", elementCount=" + this.elementCount;
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/FileSettings.java b/logstash-core/src/main/java/org/logstash/ackedqueue/FileSettings.java
new file mode 100644
index 00000000000..cb0024e28e6
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/FileSettings.java
@@ -0,0 +1,79 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.common.io.CheckpointIOFactory;
+import org.logstash.common.io.PageIOFactory;
+
+public class FileSettings implements Settings {
+    private String dirForFiles;
+    private CheckpointIOFactory checkpointIOFactory;
+    private PageIOFactory pageIOFactory;
+    private Class elementClass;
+    private int capacity;
+    private int maxUnread;
+
+    private FileSettings() { this(""); }
+
+    public FileSettings(String dirPath) {
+        this.dirForFiles = dirPath;
+        this.maxUnread = 0;
+    }
+
+    @Override
+    public Settings setCheckpointIOFactory(CheckpointIOFactory factory) {
+        this.checkpointIOFactory = factory;
+        return this;
+    }
+
+    @Override
+    public Settings setElementIOFactory(PageIOFactory factory) {
+        this.pageIOFactory = factory;
+        return this;
+    }
+
+    @Override
+    public Settings setElementClass(Class elementClass) {
+        this.elementClass = elementClass;
+        return this;
+    }
+
+    @Override
+    public Settings setCapacity(int capacity) {
+        this.capacity = capacity;
+        return this;
+    }
+
+    @Override
+    public Settings setMaxUnread(int maxUnread) {
+        this.maxUnread = maxUnread;
+        return this;
+    }
+
+    @Override
+    public CheckpointIOFactory getCheckpointIOFactory() {
+        return checkpointIOFactory;
+    }
+
+    public PageIOFactory getPageIOFactory() {
+        return pageIOFactory;
+    }
+
+    @Override
+    public Class getElementClass()  {
+        return this.elementClass;
+    }
+
+    @Override
+    public String getDirPath() {
+        return dirForFiles;
+    }
+
+    @Override
+    public int getCapacity() {
+        return capacity;
+    }
+
+    @Override
+    public int getMaxUnread() {
+        return this.maxUnread;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java b/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java
new file mode 100644
index 00000000000..5c92ee0e0b8
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java
@@ -0,0 +1,93 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.common.io.CheckpointIO;
+import org.logstash.common.io.PageIO;
+
+import java.io.IOException;
+import java.util.BitSet;
+
+public class HeadPage extends Page {
+
+    // create a new HeadPage object and new page.{pageNum} empty valid data file
+    public HeadPage(int pageNum, Queue queue, PageIO pageIO) throws IOException {
+        super(pageNum, queue, 0, 0, 0, new BitSet(), pageIO);
+        pageIO.create();
+    }
+
+    // verify if data size plus overhead is not greater than the page capacity
+    public boolean hasCapacity(int byteSize) {
+        return this.pageIO.persistedByteCount(byteSize) <= this.pageIO.getCapacity();
+    }
+
+    public boolean hasSpace(int byteSize) {
+        return this.pageIO.hasSpace((byteSize));
+    }
+
+    // NOTE:
+    // we have a page concern inconsistency where readBatch() takes care of the
+    // deserialization and returns a Batch object which contains the deserialized
+    // elements objects of the proper elementClass but HeadPage.write() deals with
+    // a serialized element byte[] and serialization is done at the Queue level to
+    // be able to use the Page.hasSpace() method with the serialized element byte size.
+    //
+    public void write(byte[] bytes, long seqNum) throws IOException {
+        this.pageIO.write(bytes, seqNum);
+
+        if (this.minSeqNum <= 0) {
+            this.minSeqNum = seqNum;
+            this.firstUnreadSeqNum = seqNum;
+        }
+        this.elementCount++;
+    }
+
+    public void ensurePersistedUpto(long seqNum) throws IOException {
+        long lastCheckpointUptoSeqNum = this.lastCheckpoint.getMinSeqNum() + this.lastCheckpoint.getElementCount();
+
+        // if the last checkpoint for this headpage already included the given seqNum, no need to fsync/checkpoint
+        if (seqNum > lastCheckpointUptoSeqNum) {
+            // head page checkpoint does a data file fsync
+            checkpoint();
+        }
+    }
+
+
+    public BeheadedPage behead() throws IOException {
+        // first do we need to checkpoint+fsync the headpage a last time?
+        if (this.elementCount > this.lastCheckpoint.getElementCount()) {
+            checkpoint();
+        }
+
+        BeheadedPage beheadedPage = new BeheadedPage(this);
+
+        // first thing that must be done after beheading is to create a new checkpoint for that new tail page
+        // tail page checkpoint does NOT includes a fsync
+        beheadedPage.checkpoint();
+
+        // TODO: should we have a better deactivation strategy to avoid too rapid reactivation scenario?
+        Page firstUnreadPage = queue.firstUnreadPage();
+        if (firstUnreadPage == null || (beheadedPage.getPageNum() > firstUnreadPage.getPageNum())) {
+            // deactivate if this new beheadedPage is not where the read is occuring
+            beheadedPage.getPageIO().deactivate();
+        }
+
+        return beheadedPage;
+    }
+
+    public void checkpoint() throws IOException {
+        // TODO: not concurrent for first iteration:
+
+        // first fsync data file
+        this.pageIO.ensurePersisted();
+
+        // then write new checkpoint
+
+        CheckpointIO io = queue.getCheckpointIO();
+        this.lastCheckpoint = io.write(io.headFileName(), this.pageNum, this.queue.firstUnackedPageNum(), firstUnackedSeqNum(), this.minSeqNum, this.elementCount);
+     }
+
+    public void close() throws IOException {
+        checkpoint();
+        this.pageIO.close();
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/MemorySettings.java b/logstash-core/src/main/java/org/logstash/ackedqueue/MemorySettings.java
new file mode 100644
index 00000000000..c7e3dd8c4b0
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/MemorySettings.java
@@ -0,0 +1,81 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.common.io.CheckpointIOFactory;
+import org.logstash.common.io.PageIOFactory;
+
+public class MemorySettings implements Settings {
+    private CheckpointIOFactory checkpointIOFactory;
+    private PageIOFactory pageIOFactory;
+    private Class elementClass;
+    private int capacity;
+    private final String dirPath;
+    private int maxUnread;
+
+    public MemorySettings() {
+        this("");
+    }
+
+    public MemorySettings(String dirPath) {
+        this.dirPath = dirPath;
+        this.maxUnread = 0;
+    }
+
+    @Override
+    public Settings setCheckpointIOFactory(CheckpointIOFactory factory) {
+        this.checkpointIOFactory = factory;
+        return this;
+    }
+
+    @Override
+    public Settings setElementIOFactory(PageIOFactory factory) {
+        this.pageIOFactory = factory;
+        return this;
+    }
+
+    @Override
+    public Settings setElementClass(Class elementClass) {
+        this.elementClass = elementClass;
+        return this;
+    }
+
+    @Override
+    public Settings setCapacity(int capacity) {
+        this.capacity = capacity;
+        return this;
+    }
+
+    @Override
+    public Settings setMaxUnread(int maxUnread) {
+        this.maxUnread = maxUnread;
+        return this;
+    }
+
+    @Override
+    public CheckpointIOFactory getCheckpointIOFactory() {
+        return checkpointIOFactory;
+    }
+
+    public PageIOFactory getPageIOFactory() {
+        return pageIOFactory;
+    }
+
+    @Override
+    public Class getElementClass()  {
+        return this.elementClass;
+    }
+
+    @Override
+    public String getDirPath() {
+        return this.dirPath;
+    }
+
+    @Override
+    public int getCapacity() {
+        return this.capacity;
+    }
+
+    @Override
+    public int getMaxUnread() {
+        return this.maxUnread;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
new file mode 100644
index 00000000000..ab3f0611b88
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
@@ -0,0 +1,151 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.common.io.PageIO;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.BitSet;
+import java.util.List;
+import java.util.stream.Collectors;
+
+public abstract class Page implements Closeable {
+    protected final int pageNum;
+    protected long minSeqNum; // TODO: see if we can meke it final?
+    protected int elementCount;
+    protected long firstUnreadSeqNum;
+    protected final Queue queue;
+    protected PageIO pageIO;
+
+    // bit 0 is minSeqNum
+    // TODO: go steal LocalCheckpointService in feature/seq_no from ES
+    // TODO: https://github.com/elastic/elasticsearch/blob/feature/seq_no/core/src/main/java/org/elasticsearch/index/seqno/LocalCheckpointService.java
+    protected BitSet ackedSeqNums;
+    protected Checkpoint lastCheckpoint;
+
+    public Page(int pageNum, Queue queue, long minSeqNum, int elementCount, long firstUnreadSeqNum, BitSet ackedSeqNums, PageIO pageIO) {
+        this.pageNum = pageNum;
+        this.queue = queue;
+
+        this.minSeqNum = minSeqNum;
+        this.elementCount = elementCount;
+        this.firstUnreadSeqNum = firstUnreadSeqNum;
+        this.ackedSeqNums = ackedSeqNums;
+        this.lastCheckpoint = new Checkpoint(0, 0, 0, 0, 0);
+        this.pageIO = pageIO;
+    }
+
+    public String toString() {
+        return "pageNum=" + this.pageNum + ", minSeqNum=" + this.minSeqNum + ", elementCount=" + this.elementCount + ", firstUnreadSeqNum=" + this.firstUnreadSeqNum;
+    }
+
+    // NOTE:
+    // we have a page concern inconsistency where readBatch() takes care of the
+    // deserialization and returns a Batch object which contains the deserialized
+    // elements objects of the proper elementClass but HeadPage.write() deals with
+    // a serialized element byte[] and serialization is done at the Queue level to
+    // be able to use the Page.hasSpace() method with the serialized element byte size.
+    //
+    // @param limit the batch size limit
+    // @param elementClass the concrete element class for deserialization
+    // @return Batch batch of elements read when the number of elements can be <= limit
+    public Batch readBatch(int limit) throws IOException {
+
+        // first make sure this page is activated, activating previously activated is harmless
+        this.pageIO.activate();
+
+        SequencedList<byte[]> serialized = this.pageIO.read(this.firstUnreadSeqNum, limit);
+        List<Queueable> deserialized = serialized.getElements().stream().map(e -> this.queue.deserialize(e)).collect(Collectors.toList());
+
+        assert serialized.getSeqNums().get(0) == this.firstUnreadSeqNum :
+            String.format("firstUnreadSeqNum=%d != first result seqNum=%d", this.firstUnreadSeqNum, serialized.getSeqNums().get(0));
+
+        Batch batch = new Batch(deserialized, serialized.getSeqNums(), this.queue);
+
+        this.firstUnreadSeqNum += deserialized.size();
+
+        return batch;
+    }
+
+    public boolean isFullyRead() {
+        return unreadCount() <= 0;
+//        return this.elementCount <= 0 || this.firstUnreadSeqNum > maxSeqNum();
+    }
+
+    public boolean isFullyAcked() {
+        // TODO: it should be something similar to this when we use a proper bitset class like ES
+        // this.ackedSeqNum.firstUnackedBit >= this.elementCount;
+        // TODO: for now use a naive & inneficient mechanism with a simple Bitset
+        return this.elementCount > 0 && this.ackedSeqNums.cardinality() >= this.elementCount;
+    }
+
+    public long unreadCount() {
+        return this.elementCount <= 0 ? 0 : Math.max(0, (maxSeqNum() - this.firstUnreadSeqNum) + 1);
+    }
+
+    public void ack(List<Long> seqNums) throws IOException {
+        for (long seqNum : seqNums) {
+            // TODO: eventually refactor to use new bit handling class
+
+            assert seqNum >= this.minSeqNum :
+                    String.format("seqNum=%d is smaller than minSeqnum=%d", seqNum, this.minSeqNum);
+
+            assert seqNum < this.minSeqNum + this.elementCount:
+                    String.format("seqNum=%d is greater than minSeqnum=%d + elementCount=%d = %d", seqNum, this.minSeqNum, this.elementCount, this.minSeqNum + this.elementCount);
+            int index = (int)(seqNum - this.minSeqNum);
+
+            this.ackedSeqNums.set(index);
+        }
+
+        // checkpoint if totally acked or we acked more than 1024 elements in this page since last checkpoint
+        long firstUnackedSeqNum = firstUnackedSeqNum();
+
+        if (isFullyAcked()) {
+            // TODO: here if consumer is faster than producer, the head page may be always fully acked and we may end up fsync'ing too ofter?
+            checkpoint();
+
+            assert firstUnackedSeqNum >= this.minSeqNum + this.elementCount - 1:
+                    String.format("invalid firstUnackedSeqNum=%d for minSeqNum=%d and elementCount=%d and cardinality=%d", firstUnackedSeqNum, this.minSeqNum, this.elementCount, this.ackedSeqNums.cardinality());
+
+        } else if (firstUnackedSeqNum > this.lastCheckpoint.getFirstUnackedSeqNum() + 1024) {
+            // did we acked more that 1024 elements? if so we should checkpoint now
+            checkpoint();
+        }
+    }
+
+    public abstract void checkpoint() throws IOException;
+
+    public abstract void close() throws IOException;
+
+    public int getPageNum() {
+        return pageNum;
+    }
+
+    public long getMinSeqNum() {
+        return this.minSeqNum;
+    }
+
+    public int getElementCount() {
+        return elementCount;
+    }
+
+    public Queue getQueue() {
+        return queue;
+    }
+
+    public PageIO getPageIO() {
+        return pageIO;
+    }
+
+    protected long maxSeqNum() {
+        return this.minSeqNum + this.elementCount - 1;
+    }
+
+    protected long firstUnackedSeqNum() {
+        // TODO: eventually refactor to use new bithandling class
+        return this.ackedSeqNums.nextClearBit(0) + this.minSeqNum;
+    }
+
+    protected int firstUnackedPageNumFromQueue() {
+        return queue.firstUnackedPageNum();
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
new file mode 100644
index 00000000000..1296b30c7e4
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
@@ -0,0 +1,516 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.common.io.CheckpointIO;
+import org.logstash.common.io.PageIO;
+import org.logstash.common.io.PageIOFactory;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
+import java.nio.file.NoSuchFileException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+
+// TODO: Notes
+//
+// - time-based fsync
+//
+// - tragic errors handling
+//   - what errors cause whole queue to be broken
+//   - where to put try/catch for these errors
+
+
+public class Queue implements Closeable {
+    protected long seqNum;
+    protected HeadPage headPage;
+
+    // complete list of all non fully acked pages. note that exact sequenciality by pageNum cannot be assumed
+    // because any fully acked page will be removed from this list potentially creating pageNum gaps in the list.
+    protected final List<BeheadedPage> beheadedPages;
+
+    // this list serves the only purpose of quickly retrieving the first unread page, operation necessary on every read
+    // reads will simply remove the first page from the list when fully read and writes will append new pages upon beheading
+    protected final List<BeheadedPage> unreadBeheadedPages;
+
+    protected volatile long unreadCount;
+
+    private final CheckpointIO checkpointIO;
+    private final PageIOFactory pageIOFactory;
+    private final int pageCapacity;
+    private final String dirPath;
+    private final int maxUnread;
+
+    private final AtomicBoolean closed;
+
+    // deserialization
+    private final Class elementClass;
+    private final Method deserializeMethod;
+
+    // thread safety
+    final Lock lock = new ReentrantLock();
+    final Condition notFull  = lock.newCondition();
+    final Condition notEmpty = lock.newCondition();
+
+    public Queue(Settings settings) {
+        this(settings.getDirPath(), settings.getCapacity(), settings.getCheckpointIOFactory().build(settings.getDirPath()), settings.getPageIOFactory(), settings.getElementClass(), settings.getMaxUnread());
+    }
+
+    public Queue(String dirPath, int pageCapacity, CheckpointIO checkpointIO, PageIOFactory pageIOFactory, Class elementClass, int maxUnread) {
+        this.dirPath = dirPath;
+        this.pageCapacity = pageCapacity;
+        this.checkpointIO = checkpointIO;
+        this.pageIOFactory = pageIOFactory;
+        this.elementClass = elementClass;
+        this.beheadedPages = new ArrayList<>();
+        this.unreadBeheadedPages = new ArrayList<>();
+        this.closed = new AtomicBoolean(true); // not yet opened
+        this.maxUnread = maxUnread;
+        this.unreadCount = 0;
+
+        // retrieve the deserialize method
+        try {
+            Class[] cArg = new Class[1];
+            cArg[0] = byte[].class;
+            this.deserializeMethod = this.elementClass.getDeclaredMethod("deserialize", cArg);
+        } catch (NoSuchMethodException e) {
+            throw new QueueRuntimeException("cannot find deserialize method on class " + this.elementClass.getName(), e);
+        }
+    }
+
+    // moved queue opening logic in open() method until we have something in place to used in-memory checkpoints for testing
+    // because for now we need to pass a Queue instance to the Page and we don't want to trigger a Queue recovery when
+    // testing Page
+    public void open() throws IOException {
+        final int headPageNum;
+
+        if (!this.closed.get()) {
+            throw new IOException("queue already opened");
+        }
+
+        Checkpoint headCheckpoint;
+        try {
+            headCheckpoint = checkpointIO.read(checkpointIO.headFileName());
+        } catch (NoSuchFileException e) {
+            headCheckpoint = null;
+        }
+
+        if (headCheckpoint == null) {
+            this.seqNum = 0;
+            headPageNum = 0;
+        } else {
+
+            // reconstruct all tail pages state upto but excluding the head page
+            for (int pageNum = headCheckpoint.getFirstUnackedPageNum(); pageNum < headCheckpoint.getPageNum(); pageNum++) {
+                Checkpoint tailCheckpoint = checkpointIO.read(checkpointIO.tailFileName(pageNum));
+
+                if (tailCheckpoint == null) {
+                    throw new IOException(checkpointIO.tailFileName(pageNum) + " not found");
+                }
+
+                PageIO pageIO = this.pageIOFactory.build(pageNum, this.pageCapacity, this.dirPath);
+                BeheadedPage beheadedPage = new BeheadedPage(tailCheckpoint, this, pageIO);
+
+                // if this page is not the first tail page, deactivate it
+                // we keep the first tail page activated since we know the next read operation will be in that one
+                if (pageNum > headCheckpoint.getFirstUnackedPageNum()) {
+                    pageIO.deactivate();
+                }
+
+                // track the seqNum as we rebuild tail pages
+                if (beheadedPage.maxSeqNum() > this.seqNum) {
+                    // prevent empty pages with a minSeqNum of 0 to reset seqNum
+                    this.seqNum = beheadedPage.maxSeqNum();
+                }
+
+                // the systematic beheading on open below can create empty/fully acked tail pages
+                // TODO: add test harness for this and see if/how we should purge empty/fully acked pages
+                if (!beheadedPage.isFullyAcked()) {
+                    this.beheadedPages.add(beheadedPage);
+                    if (! beheadedPage.isFullyRead()) {
+                        this.unreadBeheadedPages.add(beheadedPage);
+                        this.unreadCount += beheadedPage.unreadCount();
+                    }
+                }
+            }
+
+            // transform the head page into a beheaded tail page
+            PageIO pageIO = this.pageIOFactory.build(headCheckpoint.getPageNum(), this.pageCapacity, this.dirPath);
+            BeheadedPage beheadedHeadPage = new BeheadedPage(headCheckpoint, this, pageIO);
+
+            // track the seqNum as we rebuild tail pages
+            if (beheadedHeadPage.maxSeqNum() > this.seqNum) {
+                // prevent empty beheadedHeadPage with a minSeqNum of 0 to reset seqNum
+                this.seqNum = beheadedHeadPage.maxSeqNum();
+            }
+
+            // the systematic beheading on open above can create empty/fully acked tail pages
+            // TODO: add test harness for this and see if/how we should purge empty/fully acked pages
+            if (beheadedHeadPage.isFullyAcked()) {
+                this.beheadedPages.add(beheadedHeadPage);
+                if (! beheadedHeadPage.isFullyRead()) {
+                    this.unreadBeheadedPages.add(beheadedHeadPage);
+                    this.unreadCount += beheadedHeadPage.unreadCount();
+                }
+
+                beheadedHeadPage.checkpoint();
+            }
+
+            headPageNum = headCheckpoint.getPageNum() + 1;
+        }
+
+        // create new head page
+        PageIO pageIO = this.pageIOFactory.build(headPageNum, this.pageCapacity, this.dirPath);
+        this.headPage = new HeadPage(headPageNum, this, pageIO);
+        this.headPage.checkpoint();
+
+        // TODO: here do directory traversal and cleanup lingering pages? could be a background operations to not delay queue start?
+
+        this.closed.set(false);
+    }
+
+    // @param element the Queueable object to write to the queue
+    // @return long written sequence number
+    public long write(Queueable element) throws IOException {
+        long seqNum = nextSeqNum();
+        byte[] data = element.serialize();
+
+        if (! this.headPage.hasCapacity(data.length)) {
+            throw new IOException("data to be written is bigger than page capacity");
+        }
+
+        // the write strategy with regard to the isFull() state is to assume there is space for this element
+        // and write it, then after write verify if we just filled the queue and wait on the notFull condition
+        // *after* the write which is both safer for a crash condition, and the queue closing sequence. In the former case
+        // holding an element in memory while wainting for the notFull condition would mean always having the current write
+        // element at risk in the always-full queue state. In the later, when closing a full queue, it would be impossible
+        // to write the current element.
+
+        lock.lock();
+        try {
+            boolean wasEmpty = (firstUnreadPage() == null);
+
+            // create a new head page if the current does not have suffient space left for data to be written
+            if (! this.headPage.hasSpace(data.length)) {
+                // beheading includes checkpoint+fsync if required
+                BeheadedPage beheadedPage = this.headPage.behead();
+
+                this.beheadedPages.add(beheadedPage);
+                if (! beheadedPage.isFullyRead()) {
+                    this.unreadBeheadedPages.add(beheadedPage);
+                }
+
+                // create new head page
+                int headPageNum = beheadedPage.pageNum + 1;
+                PageIO pageIO = this.pageIOFactory.build(headPageNum, this.pageCapacity, this.dirPath);
+                this.headPage = new HeadPage(headPageNum, this, pageIO);
+                this.headPage.checkpoint();
+            }
+
+            this.headPage.write(data, seqNum);
+            this.unreadCount++;
+
+            // if the queue was empty before write, signal non emptiness
+            if (wasEmpty) { notEmpty.signal(); }
+
+            // now check if we reached a queue full state and block here until it is not full
+            // for the next write or the queue was closed.
+            while (isFull() && !isClosed()) {
+                try {
+                    notFull.await();
+                } catch (InterruptedException e) {
+                    // the thread interrupt() has been called while in the await() blocking call.
+                    // at this point the interrupted flag is reset and Thread.interrupted() will return false
+                    // to any upstream calls on it. for now our choice is to return normally and set back
+                    // the Thread.interrupted() flag so it can be checked upstream.
+
+                    // this is a bit tricky in the case of the queue full condition blocking state.
+                    // TODO: we will want to avoid initiating a new write operation if Thread.interrupted() was called.
+
+                    // set back the interrupted flag
+                    Thread.currentThread().interrupt();
+
+                    return seqNum;
+                }
+            }
+
+            return seqNum;
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    // @return true if the queue is deemed at full capacity
+    public boolean isFull() {
+        // TODO: I am not sure if having unreadCount as volatile is sufficient here. all unreadCount updates are done inside syncronized
+        // TODO: sections, I believe that to only read the value here, having it as volatile is sufficient?
+        return (this.maxUnread > 0) ? this.unreadCount >= this.maxUnread : false;
+    }
+
+    // @param seqNum the element sequence number upper bound for which persistence should be garanteed (by fsync'ing)
+    public void ensurePersistedUpto(long seqNum) throws IOException{
+        lock.lock();
+        try {
+            this.headPage.ensurePersistedUpto(seqNum);
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    // non-blockin queue read
+    // @param limit read the next bach of size up to this limit. the returned batch size can be smaller than than the requested limit if fewer elements are available
+    // @return Batch the batch containing 1 or more element up to the required limit or null of no elements were available
+    public Batch nonBlockReadBatch(int limit) throws IOException {
+        lock.lock();
+        try {
+            Page p = firstUnreadPage();
+            if (p == null) {
+                return null;
+            }
+
+            return _readPageBatch(p, limit);
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    // blocking readBatch notes:
+    //   the queue close() notifies all pending blocking read so that they unblock if the queue is being closed.
+    //   this means that all blocking read methods need to verify for the queue close condition.
+    //
+    // blocking queue read until elements are available for read
+    // @param limit read the next bach of size up to this limit. the returned batch size can be smaller than than the requested limit if fewer elements are available
+    // @return Batch the batch containing 1 or more element up to the required limit or null if no elements were available or the blocking call was interrupted
+    public Batch readBatch(int limit) throws IOException {
+        Page p;
+
+        lock.lock();
+        try {
+            while ((p = firstUnreadPage()) == null && !isClosed()) {
+                try {
+                    notEmpty.await();
+                } catch (InterruptedException e) {
+                    // the thread interrupt() has been called while in the await() blocking call.
+                    // at this point the interrupted flag is reset and Thread.interrupted() will return false
+                    // to any upstream calls on it. for now our choice is to simply return null and set back
+                    // the Thread.interrupted() flag so it can be checked upstream.
+
+                    // set back the interrupted flag
+                    Thread.currentThread().interrupt();
+
+                    return null;
+                }
+            }
+
+            // need to check for close since it is a condition for exiting the while loop
+            if (isClosed()) { return null; }
+
+            return _readPageBatch(p, limit);
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    // blocking queue read until elements are available for read or the given timeout is reached.
+    // @param limit read the next batch of size up to this limit. the returned batch size can be smaller than than the requested limit if fewer elements are available
+    // @param timeout the maximum time to wait in milliseconds
+    // @return Batch the batch containing 1 or more element up to the required limit or null if no elements were available or the blocking call was interrupted
+    public Batch readBatch(int limit, long timeout) throws IOException {
+        Page p;
+
+        lock.lock();
+        try {
+            // wait only if queue is empty
+            if ((p = firstUnreadPage()) == null) {
+                try {
+                    notEmpty.await(timeout, TimeUnit.MILLISECONDS);
+                } catch (InterruptedException e) {
+                    // the thread interrupt() has been called while in the await() blocking call.
+                    // at this point the interrupted flag is reset and Thread.interrupted() will return false
+                    // to any upstream calls on it. for now our choice is to simply return null and set back
+                    // the Thread.interrupted() flag so it can be checked upstream.
+
+                    // set back the interrupted flag
+                    Thread.currentThread().interrupt();
+
+                    return null;
+                }
+
+                // if after returnining from wait queue is still empty, or the queue was closed return null
+                if ((p = firstUnreadPage()) == null || isClosed()) { return null; }
+            }
+
+            return _readPageBatch(p, limit);
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    private Batch _readPageBatch(Page p, int limit) throws IOException {
+        boolean wasFull = isFull();
+
+        Batch b = p.readBatch(limit);
+        this.unreadCount -= b.size();
+
+        if (p.isFullyRead()) {
+            removeUnreadPage(p);
+        }
+
+        if (wasFull) { notFull.signal(); }
+
+        return b;
+    }
+
+    private static class BeheadedPageResult {
+        public BeheadedPage page;
+        public int index;
+
+        public BeheadedPageResult(BeheadedPage page, int index) {
+            this.page = page;
+            this.index = index;
+        }
+    }
+
+    // perform a binary search through tail pages to find in which page this seqNum falls into
+    private BeheadedPageResult binaryFindPageForSeqnum(long seqNum) {
+        int lo = 0;
+        int hi = this.beheadedPages.size() - 1;
+        while (lo <= hi) {
+            int mid = lo + (hi - lo) / 2;
+            BeheadedPage p = this.beheadedPages.get(mid);
+
+            if (seqNum < p.getMinSeqNum()) {
+                hi = mid - 1;
+            } else if (seqNum >= (p.getMinSeqNum() + p.getElementCount())) {
+                lo = mid + 1;
+            } else {
+                return new BeheadedPageResult(p, mid);
+            }
+        }
+        return null;
+    }
+
+    // perform a linear search through tail pages to find in which page this seqNum falls into
+    private BeheadedPageResult linearFindPageForSeqnum(long seqNum) {
+        for (int i = 0; i < this.beheadedPages.size(); i++) {
+            BeheadedPage p = this.beheadedPages.get(i);
+            if (p.getMinSeqNum() > 0 && seqNum >= p.getMinSeqNum() && seqNum < p.getMinSeqNum() + p.getElementCount()) {
+                return new BeheadedPageResult(p, i);
+            }
+        }
+        return null;
+    }
+
+    public void ack(List<Long> seqNums) throws IOException {
+        // as a first implementation we assume that all batches are created from the same page
+        // so we will avoid multi pages acking here for now
+
+        // find the page to ack by travesing from oldest tail page
+        long firstAckSeqNum = seqNums.get(0);
+
+        lock.lock();
+        try {
+            // dual search strategy: if few tail pages search linearily otherwise perform binary search
+            BeheadedPageResult result = (this.beheadedPages.size() > 3) ? binaryFindPageForSeqnum(firstAckSeqNum) : linearFindPageForSeqnum(firstAckSeqNum);
+
+            if (result == null) {
+                // if not found then it is in head page
+                assert this.headPage.getMinSeqNum() > 0 && firstAckSeqNum >= this.headPage.getMinSeqNum() && firstAckSeqNum < this.headPage.getMinSeqNum() + this.headPage.getElementCount():
+                        String.format("seqNum=%d is not in head page with minSeqNum=%d", firstAckSeqNum, this.headPage.getMinSeqNum());
+                this.headPage.ack(seqNums);
+            } else {
+                result.page.ack(seqNums);
+
+                // cleanup fully acked tail page
+                if (result.page.isFullyAcked()) {
+                    this.beheadedPages.remove(result.index);
+                    this.headPage.checkpoint();
+                    result.page.purge();
+                }
+            }
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    public CheckpointIO getCheckpointIO() {
+        return this.checkpointIO;
+    }
+
+    // deserialize a byte array into the required element class.
+    // @param bytes the byte array to deserialize
+    // @return Queueable the deserialized byte array into the required Queuable interface implementation concrete class
+    public Queueable deserialize(byte[] bytes) {
+        try {
+            return (Queueable)this.deserializeMethod.invoke(this.elementClass, bytes);
+        } catch (IllegalAccessException|InvocationTargetException e) {
+            throw new QueueRuntimeException("deserialize invocation error", e);
+        }
+    }
+
+    public void close() throws IOException {
+        // TODO: review close strategy and exception handling and resiliency of first closing tail pages if crash in the middle
+
+        if (closed.getAndSet(true) == false) {
+            lock.lock();
+            try {
+                // TODO: not sure if we need to do this here since the headpage close will also call ensurePersited
+                ensurePersistedUpto(this.seqNum);
+
+                for (BeheadedPage p : this.beheadedPages) { p.close(); }
+                this.headPage.close();
+
+                // release all referenced objects
+                this.beheadedPages.clear();
+                this.unreadBeheadedPages.clear();
+                this.headPage = null;
+
+                // unblock blocked reads which will return null by checking of isClosed()
+                // no data will be lost because the actual read has not been performed
+                notEmpty.signalAll();
+
+
+                // unblock blocked writes. a write is blocked *after* the write has been performed so
+                // unblocking is safe and will return from the write call
+                notFull.signalAll();
+            } finally {
+                lock.unlock();
+            }
+        }
+    }
+
+    protected Page firstUnreadPage() throws IOException {
+        // look at head page if no unreadBeheadedPages
+        return (this.unreadBeheadedPages.isEmpty()) ? (this.headPage.isFullyRead() ? null : this.headPage) : this.unreadBeheadedPages.get(0);
+    }
+
+    private void removeUnreadPage(Page p) {
+        // HeadPage is not part of the unreadBeheadedPages, just ignore
+        if (p instanceof BeheadedPage){
+            // the page to remove should always be the first one
+            assert this.unreadBeheadedPages.get(0) == p : String.format("unread page is not first in unreadBeheadedPages list");
+            this.unreadBeheadedPages.remove(0);
+        }
+    }
+
+    protected int firstUnackedPageNum() {
+        if (this.beheadedPages.isEmpty()) {
+            return this.headPage.getPageNum();
+        }
+        return this.beheadedPages.get(0).getPageNum();
+    }
+
+    protected long nextSeqNum() {
+        return this.seqNum += 1;
+    }
+
+    protected boolean isClosed() {
+        return this.closed.get();
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/QueueRuntimeException.java b/logstash-core/src/main/java/org/logstash/ackedqueue/QueueRuntimeException.java
new file mode 100644
index 00000000000..06b8639d5b0
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/QueueRuntimeException.java
@@ -0,0 +1,29 @@
+package org.logstash.ackedqueue;
+
+public class QueueRuntimeException extends RuntimeException {
+
+    public static QueueRuntimeException newFormatMessage(String fmt, Object... args) {
+        return new QueueRuntimeException(
+                String.format(fmt, args)
+        );
+    }
+
+    public QueueRuntimeException() {
+    }
+
+    public QueueRuntimeException(String message) {
+        super(message);
+    }
+
+    public QueueRuntimeException(String message, Throwable cause) {
+        super(message, cause);
+    }
+
+    public QueueRuntimeException(Throwable cause) {
+        super(cause);
+    }
+
+    public QueueRuntimeException(String message, Throwable cause, boolean enableSuppression, boolean writableStackTrace) {
+        super(message, cause, enableSuppression, writableStackTrace);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Queueable.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Queueable.java
new file mode 100644
index 00000000000..2becec11d3b
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Queueable.java
@@ -0,0 +1,10 @@
+package org.logstash.ackedqueue;
+
+import java.io.IOException;
+
+public interface Queueable {
+
+    byte[] serialize() throws IOException;
+
+    static Object deserialize(byte[] bytes) { throw new RuntimeException("please implement deserialize"); };
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/SequencedList.java b/logstash-core/src/main/java/org/logstash/ackedqueue/SequencedList.java
new file mode 100644
index 00000000000..8bb580fe053
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/SequencedList.java
@@ -0,0 +1,21 @@
+package org.logstash.ackedqueue;
+
+import java.util.List;
+
+public class SequencedList<E> {
+    private final List<E> elements;
+    private final List<Long> seqNums;
+
+    public SequencedList(List<E> elements, List<Long> seqNums) {
+        this.elements = elements;
+        this.seqNums = seqNums;
+    }
+
+    public List<E> getElements() {
+        return elements;
+    }
+
+    public List<Long> getSeqNums() {
+        return seqNums;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Settings.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Settings.java
new file mode 100644
index 00000000000..1b847e0fc2c
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Settings.java
@@ -0,0 +1,28 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.common.io.CheckpointIOFactory;
+import org.logstash.common.io.PageIOFactory;
+
+public interface Settings {
+    Settings setCheckpointIOFactory(CheckpointIOFactory factory);
+
+    Settings setElementIOFactory(PageIOFactory factory);
+
+    Settings setElementClass(Class elementClass);
+
+    Settings setCapacity(int capacity);
+
+    Settings setMaxUnread(int maxUnread);
+
+    CheckpointIOFactory getCheckpointIOFactory();
+
+    PageIOFactory getPageIOFactory();
+
+    Class getElementClass();
+
+    String getDirPath();
+
+    int getCapacity();
+
+    int getMaxUnread();
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksum.java b/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksum.java
new file mode 100644
index 00000000000..79bbd7af306
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksum.java
@@ -0,0 +1,67 @@
+package org.logstash.common.io;
+
+import java.util.zip.Checksum;
+
+/**
+ * Wraps another {@link Checksum} with an internal buffer
+ * to speed up checksum calculations.
+ */
+public class BufferedChecksum implements Checksum {
+    private final Checksum in;
+    private final byte buffer[];
+    private int upto;
+    /** Default buffer size: 256 */
+    public static final int DEFAULT_BUFFERSIZE = 256;
+
+    /** Create a new BufferedChecksum with {@link #DEFAULT_BUFFERSIZE} */
+    public BufferedChecksum(Checksum in) {
+        this(in, DEFAULT_BUFFERSIZE);
+    }
+
+    /** Create a new BufferedChecksum with the specified bufferSize */
+    public BufferedChecksum(Checksum in, int bufferSize) {
+        this.in = in;
+        this.buffer = new byte[bufferSize];
+    }
+
+    @Override
+    public void update(int b) {
+        if (upto == buffer.length) {
+            flush();
+        }
+        buffer[upto++] = (byte) b;
+    }
+
+    @Override
+    public void update(byte[] b, int off, int len) {
+        if (len >= buffer.length) {
+            flush();
+            in.update(b, off, len);
+        } else {
+            if (upto + len > buffer.length) {
+                flush();
+            }
+            System.arraycopy(b, off, buffer, upto, len);
+            upto += len;
+        }
+    }
+
+    @Override
+    public long getValue() {
+        flush();
+        return in.getValue();
+    }
+
+    @Override
+    public void reset() {
+        upto = 0;
+        in.reset();
+    }
+
+    private void flush() {
+        if (upto > 0) {
+            in.update(buffer, 0, upto);
+        }
+        upto = 0;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksumStreamInput.java b/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksumStreamInput.java
new file mode 100644
index 00000000000..beed5238738
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksumStreamInput.java
@@ -0,0 +1,104 @@
+package org.logstash.common.io;
+
+import java.io.IOException;
+import java.util.zip.CRC32;
+import java.util.zip.Checksum;
+
+/**
+ * Similar to Lucene's BufferedChecksumIndexInput, however this wraps a
+ * {@link StreamInput} so anything read will update the checksum
+ */
+public final class BufferedChecksumStreamInput extends StreamInput {
+    private static final int SKIP_BUFFER_SIZE = 1024;
+    private byte[] skipBuffer;
+    private final StreamInput in;
+    private final Checksum digest;
+
+    public BufferedChecksumStreamInput(StreamInput in) {
+        this.in = in;
+        this.digest = new BufferedChecksum(new CRC32());
+    }
+
+    public BufferedChecksumStreamInput(StreamInput in, BufferedChecksumStreamInput reuse) {
+        this.in = in;
+        if (reuse == null ) {
+            this.digest = new BufferedChecksum(new CRC32());
+        } else {
+            this.digest = reuse.digest;
+            digest.reset();
+            this.skipBuffer = reuse.skipBuffer;
+        }
+    }
+
+    public long getChecksum() {
+        return this.digest.getValue();
+    }
+
+    @Override
+    public byte readByte() throws IOException {
+        final byte b = in.readByte();
+        digest.update(b);
+        return b;
+    }
+
+    @Override
+    public void readBytes(byte[] b, int offset, int len) throws IOException {
+        in.readBytes(b, offset, len);
+        digest.update(b, offset, len);
+    }
+
+    @Override
+    public void reset() throws IOException {
+        in.reset();
+        digest.reset();
+    }
+
+    @Override
+    public int read() throws IOException {
+        return readByte() & 0xFF;
+    }
+
+    @Override
+    public void close() throws IOException {
+        in.close();
+    }
+
+    @Override
+    public boolean markSupported() {
+        return in.markSupported();
+    }
+
+
+    @Override
+    public long skip(long numBytes) throws IOException {
+        if (numBytes < 0) {
+            throw new IllegalArgumentException("numBytes must be >= 0, got " + numBytes);
+        }
+        if (skipBuffer == null) {
+            skipBuffer = new byte[SKIP_BUFFER_SIZE];
+        }
+        assert skipBuffer.length == SKIP_BUFFER_SIZE;
+        long skipped = 0;
+        for (; skipped < numBytes; ) {
+            final int step = (int) Math.min(SKIP_BUFFER_SIZE, numBytes - skipped);
+            readBytes(skipBuffer, 0, step);
+            skipped += step;
+        }
+        return skipped;
+    }
+
+    @Override
+    public int available() throws IOException {
+        return in.available();
+    }
+
+    @Override
+    public synchronized void mark(int readlimit) {
+        in.mark(readlimit);
+    }
+
+    public void resetDigest() {
+        digest.reset();
+    }
+}
+
diff --git a/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksumStreamOutput.java b/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksumStreamOutput.java
new file mode 100644
index 00000000000..f37b71f92bf
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksumStreamOutput.java
@@ -0,0 +1,57 @@
+package org.logstash.common.io;
+
+
+import java.io.IOException;
+import java.util.zip.CRC32;
+import java.util.zip.Checksum;
+
+/**
+ * Similar to Lucene's BufferedChecksumIndexOutput, however this wraps a
+ * {@link StreamOutput} so anything written will update the checksum
+ */
+public final class BufferedChecksumStreamOutput extends StreamOutput {
+    private final StreamOutput out;
+    private final Checksum digest;
+
+    public BufferedChecksumStreamOutput(StreamOutput out) {
+        this.out = out;
+        this.digest = new BufferedChecksum(new CRC32());
+    }
+
+    public long getChecksum() {
+        return this.digest.getValue();
+    }
+
+    @Override
+    public void writeByte(byte b) throws IOException {
+        out.writeByte(b);
+        digest.update(b);
+    }
+
+    @Override
+    public void writeBytes(byte[] b, int offset, int length) throws IOException {
+        out.writeBytes(b, offset, length);
+        digest.update(b, offset, length);
+    }
+
+    @Override
+    public void flush() throws IOException {
+        out.flush();
+    }
+
+    @Override
+    public void close() throws IOException {
+        out.close();
+    }
+
+    @Override
+    public void reset() throws IOException {
+        out.reset();
+        digest.reset();
+    }
+
+    public void resetDigest() {
+        digest.reset();
+    }
+}
+
diff --git a/logstash-core/src/main/java/org/logstash/common/io/ByteArrayStreamOutput.java b/logstash-core/src/main/java/org/logstash/common/io/ByteArrayStreamOutput.java
new file mode 100644
index 00000000000..6f49581e9cc
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/ByteArrayStreamOutput.java
@@ -0,0 +1,65 @@
+package org.logstash.common.io;
+
+import java.nio.ByteBuffer;
+
+public class ByteArrayStreamOutput extends StreamOutput {
+    private byte[] bytes;
+
+    private int pos;
+    private int limit;
+
+    public ByteArrayStreamOutput(byte[] bytes) {
+        reset(bytes);
+    }
+
+    public ByteArrayStreamOutput(ByteBuffer bytebuffer) {
+        reset(bytebuffer.array());
+    }
+
+    public ByteArrayStreamOutput(ByteBuffer bytebuffer, int offset, int len) {
+        reset(bytebuffer.array(), offset, len);
+    }
+
+    public ByteArrayStreamOutput(byte[] bytes, int offset, int len) {
+        reset(bytes, offset, len);
+    }
+
+    public void reset(byte[] bytes) {
+        reset(bytes, 0, bytes.length);
+    }
+
+    public void reset(byte[] bytes, int offset, int len) {
+        this.bytes = bytes;
+        pos = offset;
+        limit = offset + len;
+    }
+
+    public void setWriteWindow(int offset, int len) {
+        pos = offset;
+        limit = offset + len;
+    }
+
+    public void reset() {
+    }
+
+    public void reset(int offset) {
+        pos = offset;
+    }
+
+    public int getPosition() {
+        return pos;
+    }
+
+    @Override
+    public void writeByte(byte b) {
+        assert pos < limit :  String.format("ByteArrayStreamOutput#writeByte pos=%d !< limit=%d", pos, limit);
+        bytes[pos++] = b;
+    }
+
+    @Override
+    public void writeBytes(byte[] b, int offset, int length) {
+        assert pos + length <= limit;
+        System.arraycopy(b, offset, bytes, pos, length);
+        pos += length;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/ByteBufferPageIO.java b/logstash-core/src/main/java/org/logstash/common/io/ByteBufferPageIO.java
new file mode 100644
index 00000000000..26c3ae56da0
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/ByteBufferPageIO.java
@@ -0,0 +1,248 @@
+package org.logstash.common.io;
+
+import org.logstash.ackedqueue.Queueable;
+import org.logstash.ackedqueue.SequencedList;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.zip.CRC32;
+import java.util.zip.Checksum;
+
+// TODO: currently assuming continuous seqNum is the byte buffer where we can deduct the maxSeqNum from the min + count.
+// TODO: we could change this and support non-continuous seqNums but I am not sure we should.
+// TODO: checksum is not currently computed.
+
+public class ByteBufferPageIO implements PageIO {
+    public static final byte VERSION = 1;
+    public static final int CHECKSUM_SIZE = Integer.BYTES;
+    public static final int LENGTH_SIZE = Integer.BYTES;
+    public static final int SEQNUM_SIZE = Long.BYTES;
+    public static final int MIN_RECORD_SIZE = SEQNUM_SIZE + LENGTH_SIZE + CHECKSUM_SIZE;
+    public static final int HEADER_SIZE = 1;     // version byte
+    static final List<byte[]> EMPTY_READ = new ArrayList<>(0);
+
+    private final int capacity;
+    private final List<Integer> offsetMap; // has to be extendable
+    private final ByteBuffer buffer;
+    private long minSeqNum; // TODO: to make minSeqNum final we have to pass in the minSeqNum in the constructor and not set it on first write
+    private int elementCount;
+    private int head;
+    private byte version;
+    private Checksum checkSummer;
+
+    public ByteBufferPageIO(int pageNum, int capacity, String path) throws IOException {
+        this(capacity, new byte[0]);
+    }
+
+    public ByteBufferPageIO(int capacity) throws IOException {
+        this(capacity, new byte[0]);
+    }
+
+    public ByteBufferPageIO(int capacity, byte[] initialBytes) throws IOException {
+        this.capacity = capacity;
+        if (initialBytes.length > capacity) {
+            throw new IOException("initial bytes greater than capacity");
+        }
+
+        this.buffer = ByteBuffer.allocate(capacity);
+        this.buffer.put(initialBytes);
+
+        this.offsetMap = new ArrayList<>();
+        this.checkSummer = new CRC32();
+    }
+
+    @Override
+    public void open(long minSeqNum, int elementCount) throws IOException {
+        this.minSeqNum = minSeqNum;
+        this.elementCount = elementCount;
+
+        this.buffer.position(0);
+        this.version = this.buffer.get();
+        this.head = 1;
+
+        if (this.elementCount > 0) {
+
+            // TODO: refactor the read logic below to DRY with the read() method.
+
+            // set head by skipping over all elements
+            for (int i = 0; i < this.elementCount; i++) {
+                if (this.head + SEQNUM_SIZE + LENGTH_SIZE > capacity) {
+                    throw new IOException(String.format("cannot read seqNum and length bytes past buffer capacity"));
+                }
+
+                long seqNum = this.buffer.getLong();
+
+                if (i == 0 && seqNum != this.minSeqNum) {
+                    throw new IOException(String.format("first seqNum=%d is different than minSeqNum=%d", seqNum, this.minSeqNum));
+                }
+
+                this.offsetMap.add(head);
+                this.head += SEQNUM_SIZE;
+
+
+                int length = this.buffer.getInt();
+                this.head += LENGTH_SIZE;
+
+                if (this.head + length + CHECKSUM_SIZE > capacity) {
+                    throw new IOException(String.format("cannot read element payload and checksum past buffer capacity"));
+                }
+
+                // skip over data
+                this.head += length;
+                this.head += CHECKSUM_SIZE;
+
+                this.buffer.position(head);
+            }
+        }
+    }
+
+    @Override
+    public void create() throws IOException {
+        this.buffer.position(0);
+        this.buffer.put(VERSION);
+        this.head = 1;
+        this.minSeqNum = 0L;
+        this.elementCount = 0;
+    }
+
+    @Override
+    public int getCapacity() {
+        return this.capacity;
+    }
+
+    public long getMinSeqNum() {
+        return this.minSeqNum;
+    }
+
+    @Override
+    public boolean hasSpace(int bytes) {
+        int bytesLeft = this.capacity - this.head;
+        return persistedByteCount(bytes) <= bytesLeft;
+    }
+
+    @Override
+    public void write(byte[] bytes, long seqNum) throws IOException {
+        // since writes always happen at head, we can just append head to the offsetMap
+        assert this.offsetMap.size() == this.elementCount :
+                String.format("offsetMap size=%d != elementCount=%d", this.offsetMap.size(), this.elementCount);
+
+        int initialHead = this.head;
+
+        this.buffer.position(this.head);
+        this.buffer.putLong(seqNum);
+        this.buffer.putInt(bytes.length);
+        this.buffer.put(bytes);
+        this.buffer.putInt(checksum(bytes));
+        this.head += persistedByteCount(bytes.length);
+        assert this.head == this.buffer.position() :
+                String.format("head=%d != buffer position=%d", this.head, this.buffer.position());
+
+        if (this.elementCount <= 0) {
+            this.minSeqNum = seqNum;
+        }
+        this.offsetMap.add(initialHead);
+        this.elementCount++;
+    }
+
+    @Override
+    public SequencedList<byte[]> read(long seqNum, int limit) throws IOException {
+        assert seqNum >= this.minSeqNum :
+                String.format("seqNum=%d < minSeqNum=%d", seqNum, this.minSeqNum);
+        assert seqNum <= maxSeqNum() :
+                String.format("seqNum=%d is > maxSeqNum=%d", seqNum, maxSeqNum());
+
+        List<byte[]> elements = new ArrayList<>();
+        List<Long> seqNums = new ArrayList<>();
+
+        int offset = this.offsetMap.get((int)(seqNum - this.minSeqNum));
+
+        this.buffer.position(offset);
+
+        for (int i = 0; i < limit; i++) {
+            long readSeqNum = this.buffer.getLong();
+
+            assert readSeqNum == (seqNum + i) :
+                    String.format("unmatched seqNum=%d to readSeqNum=%d", seqNum + i, readSeqNum);
+
+            int readLength = this.buffer.getInt();
+            byte[] readBytes = new byte[readLength];
+            this.buffer.get(readBytes);
+            int checksum = this.buffer.getInt();
+            int computedChecksum = checksum(readBytes);
+            if (computedChecksum != checksum) {
+                throw new IOException(String.format("computed checksum=%d != checksum for file=%d", computedChecksum, checksum));
+            }
+
+            elements.add(readBytes);
+            seqNums.add(readSeqNum);
+
+            if (seqNum + i >= maxSeqNum()) {
+                break;
+            }
+        }
+
+        return new SequencedList<>(elements, seqNums);
+    }
+
+    @Override
+    public void deactivate() {
+        // nothing to do
+    }
+
+    @Override
+    public void activate() {
+        // nothing to do
+    }
+
+    @Override
+    public void ensurePersisted() {
+        // nothing to do
+    }
+
+    @Override
+    public void purge() throws IOException {
+        // do nothing
+    }
+
+    @Override
+    public void close() throws IOException {
+        // TBD
+    }
+
+    private int checksum(byte[] bytes) {
+        checkSummer.reset();
+        checkSummer.update(bytes, 0, bytes.length);
+        return (int) checkSummer.getValue();
+    }
+
+    // TODO: static method for tests - should refactor
+    public static int _persistedByteCount(int byteCount) {
+        return SEQNUM_SIZE + LENGTH_SIZE + byteCount + CHECKSUM_SIZE;
+    }
+
+    @Override
+    public int persistedByteCount(int byteCount) {
+        return ByteBufferPageIO._persistedByteCount(byteCount);
+    }
+
+    private long maxSeqNum() {
+        return this.minSeqNum + this.elementCount - 1;
+    }
+
+
+    // below public methods only used by tests
+
+    public int getWritePosition() {
+        return this.head;
+    }
+
+    public int getElementCount() {
+        return this.elementCount;
+    }
+
+    public byte[] dump() {
+        return this.buffer.array();
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/ByteBufferStreamInput.java b/logstash-core/src/main/java/org/logstash/common/io/ByteBufferStreamInput.java
new file mode 100644
index 00000000000..8afeb4eef2c
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/ByteBufferStreamInput.java
@@ -0,0 +1,93 @@
+package org.logstash.common.io;
+
+import java.io.EOFException;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+public class ByteBufferStreamInput extends StreamInput {
+
+    private final ByteBuffer buffer;
+
+    public ByteBufferStreamInput(ByteBuffer buffer) {
+        this.buffer = buffer;
+    }
+
+    @Override
+    public int read() throws IOException {
+        if (!buffer.hasRemaining()) {
+            return -1;
+        }
+        return buffer.get() & 0xFF;
+    }
+
+    @Override
+    public byte readByte() throws IOException {
+        if (!buffer.hasRemaining()) {
+            throw new EOFException();
+        }
+        return buffer.get();
+    }
+
+    @Override
+    public int read(byte[] b, int off, int len) throws IOException {
+        if (!buffer.hasRemaining()) {
+            return -1;
+        }
+
+        len = Math.min(len, buffer.remaining());
+        buffer.get(b, off, len);
+        return len;
+    }
+
+    @Override
+    public long skip(long n) throws IOException {
+        if (n > buffer.remaining()) {
+            int ret = buffer.position();
+            buffer.position(buffer.limit());
+            return ret;
+        }
+        buffer.position((int) (buffer.position() + n));
+        return n;
+    }
+
+    @Override
+    public void readBytes(byte[] b, int offset, int len) throws IOException {
+        if (buffer.remaining() < len) {
+            throw new EOFException();
+        }
+        buffer.get(b, offset, len);
+    }
+
+    @Override
+    public void reset() throws IOException {
+        buffer.reset();
+    }
+
+    public void movePosition(int position) {
+        buffer.position(position);
+    }
+
+    public void rewind() throws IOException {
+        buffer.rewind();
+    }
+
+    @Override
+    public int available() throws IOException {
+        return buffer.remaining();
+    }
+
+    @Override
+    public void mark(int readlimit) {
+        buffer.mark();
+    }
+
+    @Override
+    public boolean markSupported() {
+        return true;
+    }
+
+    @Override
+    public void close() throws IOException {
+    }
+}
+
diff --git a/logstash-core/src/main/java/org/logstash/common/io/CheckpointIO.java b/logstash-core/src/main/java/org/logstash/common/io/CheckpointIO.java
new file mode 100644
index 00000000000..cd28cf66e3b
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/CheckpointIO.java
@@ -0,0 +1,22 @@
+package org.logstash.common.io;
+
+import org.logstash.ackedqueue.Checkpoint;
+import java.io.IOException;
+
+public interface CheckpointIO {
+
+    // @return Checkpoint the written checkpoint object
+    Checkpoint write(String fileName, int pageNum, int firstUnackedPageNum, long firstUnackedSeqNum, long minSeqNum, int elementCount) throws IOException;
+
+    Checkpoint read(String fileName) throws IOException;
+
+    void purge(String fileName) throws IOException;
+
+    void purge() throws IOException;
+
+    // @return the head page checkpoint file name
+    String headFileName();
+
+    // @return the tail page checkpoint file name for given page number
+    String tailFileName(int pageNum);
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/CheckpointIOFactory.java b/logstash-core/src/main/java/org/logstash/common/io/CheckpointIOFactory.java
new file mode 100644
index 00000000000..574ca42d328
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/CheckpointIOFactory.java
@@ -0,0 +1,6 @@
+package org.logstash.common.io;
+
+@FunctionalInterface
+public interface CheckpointIOFactory {
+    CheckpointIO build(String dirPath);
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/FileCheckpointIO.java b/logstash-core/src/main/java/org/logstash/common/io/FileCheckpointIO.java
new file mode 100644
index 00000000000..12e8e197772
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/FileCheckpointIO.java
@@ -0,0 +1,108 @@
+package org.logstash.common.io;
+
+import org.logstash.ackedqueue.Checkpoint;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+
+public class FileCheckpointIO  implements CheckpointIO {
+//    Checkpoint file structure
+//
+//    byte version;
+//    int pageNum;
+//    int firstUnackedPageNum;
+//    long firstUnackedSeqNum;
+//    long minSeqNum;
+//    int elementCount;
+
+    public static final int BUFFER_SIZE = Short.BYTES // version
+            + Integer.BYTES  // pageNum
+            + Integer.BYTES  // firstUnackedPageNum
+            + Long.BYTES     // firstUnackedSeqNum
+            + Long.BYTES     // minSeqNum
+            + Integer.BYTES  // eventCount
+            + Integer.BYTES;    // checksum
+
+    private final String dirPath;
+    private final String HEAD_CHECKPOINT = "checkpoint.head";
+    private final String TAIL_CHECKPOINT = "checkpoint.";
+
+    public FileCheckpointIO(String dirPath) {
+        this.dirPath = dirPath;
+    }
+
+    @Override
+    public Checkpoint read(String fileName) throws IOException {
+        Path path = Paths.get(dirPath, fileName);
+        InputStream is = Files.newInputStream(path);
+        return read(new BufferedChecksumStreamInput(new InputStreamStreamInput(is)));
+    }
+
+    @Override
+    public Checkpoint write(String fileName, int pageNum, int firstUnackedPageNum, long firstUnackedSeqNum, long minSeqNum, int elementCount) throws IOException {
+        Path path = Paths.get(dirPath, fileName);
+        Checkpoint checkpoint = new Checkpoint(pageNum, firstUnackedPageNum, firstUnackedSeqNum, minSeqNum, elementCount);
+        final byte[] buffer = new byte[BUFFER_SIZE];
+        write(checkpoint, buffer);
+        Files.write(path, buffer);
+        return checkpoint;
+    }
+
+    @Override
+    public void purge(String fileName) throws IOException {
+        Path path = Paths.get(dirPath, fileName);
+        Files.delete(path);
+    }
+
+    @Override
+    public void purge() throws IOException {
+        // TODO: dir traversal and delete all checkpoints?
+    }
+
+    // @return the head page checkpoint file name
+    @Override
+    public String headFileName() {
+         return HEAD_CHECKPOINT;
+    }
+
+    // @return the tail page checkpoint file name for given page number
+    @Override
+    public String tailFileName(int pageNum) {
+        return TAIL_CHECKPOINT + pageNum;
+    }
+
+    private Checkpoint read(BufferedChecksumStreamInput crcsi) throws IOException {
+        int version = (int) crcsi.readShort();
+        // TODO - build reader for this version
+        int pageNum = crcsi.readInt();
+        int firstUnackedPageNum = crcsi.readInt();
+        long firstUnackedSeqNum = crcsi.readLong();
+        long minSeqNum = crcsi.readLong();
+        int elementCount = crcsi.readInt();
+
+        int calcCrc32 = (int)crcsi.getChecksum();
+        int readCrc32 = crcsi.readInt();
+        if (readCrc32 != calcCrc32) {
+            throw new IOException(String.format("Checkpoint checksum mismatch, expected: %d, actual: %d", calcCrc32, readCrc32));
+        }
+        if (version != Checkpoint.VERSION) {
+            throw new IOException("Unknown file format version: " + version);
+        }
+
+        return new Checkpoint(pageNum, firstUnackedPageNum, firstUnackedSeqNum, minSeqNum, elementCount);
+    }
+
+    private void write(Checkpoint checkpoint, byte[] buf) throws IOException {
+        BufferedChecksumStreamOutput output = new BufferedChecksumStreamOutput(new ByteArrayStreamOutput(buf));
+        output.writeShort((short)Checkpoint.VERSION);
+        output.writeInt(checkpoint.getPageNum());
+        output.writeInt(checkpoint.getFirstUnackedPageNum());
+        output.writeLong(checkpoint.getFirstUnackedSeqNum());
+        output.writeLong(checkpoint.getMinSeqNum());
+        output.writeInt(checkpoint.getElementCount());
+        output.writeInt((int)output.getChecksum());
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/InputStreamStreamInput.java b/logstash-core/src/main/java/org/logstash/common/io/InputStreamStreamInput.java
new file mode 100644
index 00000000000..712e42b87b1
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/InputStreamStreamInput.java
@@ -0,0 +1,77 @@
+package org.logstash.common.io;
+
+import java.io.EOFException;
+import java.io.IOException;
+import java.io.InputStream;
+
+public class InputStreamStreamInput extends StreamInput {
+
+    private final InputStream is;
+
+    public InputStreamStreamInput(InputStream is) {
+        this.is = is;
+    }
+
+    @Override
+    public byte readByte() throws IOException {
+        int ch = is.read();
+        if (ch < 0)
+            throw new EOFException();
+        return (byte) (ch);
+    }
+
+    @Override
+    public void readBytes(byte[] b, int offset, int len) throws IOException {
+        if (len < 0)
+            throw new IndexOutOfBoundsException();
+        final int read = Streams.readFully(is, b, offset, len);
+        if (read != len) {
+            throw new EOFException();
+        }
+    }
+
+    @Override
+    public void reset() throws IOException {
+        is.reset();
+    }
+
+    @Override
+    public boolean markSupported() {
+        return is.markSupported();
+    }
+
+    @Override
+    public void mark(int readlimit) {
+        is.mark(readlimit);
+    }
+
+    @Override
+    public void close() throws IOException {
+        is.close();
+    }
+
+    @Override
+    public int available() throws IOException {
+        return is.available();
+    }
+
+    @Override
+    public int read() throws IOException {
+        return is.read();
+    }
+
+    @Override
+    public int read(byte[] b) throws IOException {
+        return is.read(b);
+    }
+
+    @Override
+    public int read(byte[] b, int off, int len) throws IOException {
+        return is.read(b, off, len);
+    }
+
+    @Override
+    public long skip(long n) throws IOException {
+        return is.skip(n);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/MemoryCheckpointIO.java b/logstash-core/src/main/java/org/logstash/common/io/MemoryCheckpointIO.java
new file mode 100644
index 00000000000..0dc5b8b6310
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/MemoryCheckpointIO.java
@@ -0,0 +1,60 @@
+package org.logstash.common.io;
+
+import org.logstash.ackedqueue.Checkpoint;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+public class MemoryCheckpointIO implements CheckpointIO {
+
+    private final String HEAD_CHECKPOINT = "checkpoint.head";
+    private final String TAIL_CHECKPOINT = "checkpoint.";
+
+    private static final Map<String, Checkpoint> sources = new HashMap<>();
+
+    private final String dirPath;
+
+    public static void clearSources() {
+        sources.clear();
+    }
+
+    public MemoryCheckpointIO(String dirPath) {
+        this.dirPath = dirPath;
+    }
+
+    @Override
+    public Checkpoint read(String fileName) throws IOException {
+        return this.sources.get(fileName);
+    }
+
+    @Override
+    public Checkpoint write(String fileName, int pageNum, int firstUnackedPageNum, long firstUnackedSeqNum, long minSeqNum, int elementCount) throws IOException {
+        Checkpoint checkpoint = new Checkpoint(pageNum, firstUnackedPageNum, firstUnackedSeqNum, minSeqNum, elementCount);
+        this.sources.put(fileName, checkpoint);
+        return checkpoint;
+    }
+
+    @Override
+    public void purge(String fileName) {
+        this.sources.remove(fileName);
+    }
+
+    @Override
+    public void purge() {
+        this.sources.clear();
+    }
+
+    // @return the head page checkpoint file name
+    @Override
+    public String headFileName() {
+        return HEAD_CHECKPOINT;
+    }
+
+    // @return the tail page checkpoint file name for given page number
+    @Override
+    public String tailFileName(int pageNum) {
+        return TAIL_CHECKPOINT + pageNum;
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/MemoryPageIOStream.java b/logstash-core/src/main/java/org/logstash/common/io/MemoryPageIOStream.java
new file mode 100644
index 00000000000..197753232ad
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/MemoryPageIOStream.java
@@ -0,0 +1,274 @@
+package org.logstash.common.io;
+
+import org.logstash.ackedqueue.Checkpoint;
+import org.logstash.ackedqueue.Queueable;
+import org.logstash.ackedqueue.SequencedList;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+
+public class MemoryPageIOStream implements PageIO {
+    static final int CHECKSUM_SIZE = Integer.BYTES;
+    static final int LENGTH_SIZE = Integer.BYTES;
+    static final int SEQNUM_SIZE = Long.BYTES;
+    static final int MIN_RECORD_SIZE = SEQNUM_SIZE + LENGTH_SIZE + CHECKSUM_SIZE;
+    static final int VERSION_SIZE = Integer.BYTES;
+
+    private final byte[] buffer;
+    private final int capacity;
+    private int writePosition;
+    private int readPosition;
+    private int elementCount;
+    private long minSeqNum;
+    private ByteBufferStreamInput streamedInput;
+    private ByteArrayStreamOutput streamedOutput;
+    private BufferedChecksumStreamOutput crcWrappedOutput;
+    private final List<Integer> offsetMap;
+    private String dirPath = "";
+    private String headerDetails = "";
+
+    public int persistedByteCount(byte[] data) {
+        return persistedByteCount(data.length);
+    }
+
+    @Override
+    public int persistedByteCount(int length) {
+        return MIN_RECORD_SIZE + length;
+    }
+
+    public MemoryPageIOStream(int pageNum, int capacity, String dirPath) throws IOException {
+        this(capacity, new byte[capacity]);
+        this.dirPath = dirPath;
+    }
+
+    public MemoryPageIOStream(int capacity, String dirPath) throws IOException {
+        this(capacity, new byte[capacity]);
+        this.dirPath = dirPath;
+    }
+
+    public MemoryPageIOStream(int capacity) throws IOException {
+        this(capacity, new byte[capacity]);
+    }
+
+    public MemoryPageIOStream(int capacity, byte[] initialBytes) throws IOException {
+        this.capacity = capacity;
+        if (initialBytes.length > capacity) {
+            throw new IOException("initial bytes greater than capacity");
+        }
+        buffer = initialBytes;
+        offsetMap = new ArrayList<>();
+        streamedInput = new ByteBufferStreamInput(ByteBuffer.wrap(buffer));
+        streamedOutput = new ByteArrayStreamOutput(buffer);
+        crcWrappedOutput = new BufferedChecksumStreamOutput(streamedOutput);
+    }
+
+    @Override
+    public void open(long minSeqNum, int elementCount) throws IOException {
+        this.minSeqNum = minSeqNum;
+        this.elementCount = elementCount;
+        writePosition = verifyHeader();
+        readPosition = writePosition;
+        if (elementCount > 0) {
+            long seqNumRead;
+            BufferedChecksumStreamInput in = new BufferedChecksumStreamInput(streamedInput);
+            for (int i = 0; i < this.elementCount; i++) {
+                if (writePosition + SEQNUM_SIZE + LENGTH_SIZE > capacity) {
+                    throw new IOException(String.format("cannot read seqNum and length bytes past buffer capacity"));
+                }
+
+                seqNumRead = in.readLong();
+
+                //verify that the buffer starts with the min sequence number
+                if (i == 0 && seqNumRead != this.minSeqNum) {
+                    String msg = String.format("Page minSeqNum mismatch, expected: %d, actual: %d", this.minSeqNum, seqNumRead);
+                    throw new IOException(msg);
+                }
+
+                in.resetDigest();
+                byte[] bytes = in.readByteArray();
+                int actualChecksum = (int) in.getChecksum();
+                int expectedChecksum = in.readInt();
+
+                if (actualChecksum != expectedChecksum) {
+                    // explode with tragic error
+                }
+
+                offsetMap.add(writePosition);
+                writePosition += persistedByteCount(bytes);
+            }
+            setReadPoint(this.minSeqNum);
+        }
+    }
+
+    @Override
+    public void create() throws IOException {
+        writePosition = addHeader();
+        readPosition = writePosition;
+        this.minSeqNum = 1L;
+        this.elementCount = 0;
+    }
+
+    @Override
+    public boolean hasSpace(int byteSize) {
+        return this.capacity >= writePosition + persistedByteCount(byteSize);
+    }
+
+    @Override
+    public void write(byte[] bytes, long seqNum) throws IOException {
+        int pos = this.writePosition;
+        int writeLength = persistedByteCount(bytes);
+        writeToBuffer(seqNum, bytes, writeLength);
+        writePosition += writeLength;
+        assert writePosition == streamedOutput.getPosition() :
+                String.format("writePosition=%d != streamedOutput position=%d", writePosition, streamedOutput.getPosition());
+        if (elementCount <= 0) {
+            this.minSeqNum = seqNum;
+        }
+        this.offsetMap.add(pos);
+        elementCount++;
+    }
+
+    @Override
+    public SequencedList<byte[]> read(long seqNum, int limit) throws IOException {
+        if (elementCount == 0) {
+            return new SequencedList<>(new ArrayList<>(), new ArrayList<>());
+        }
+        setReadPoint(seqNum);
+        return read(limit);
+    }
+
+    @Override
+    public int getCapacity() {
+        return capacity;
+    }
+
+    @Override
+    public void deactivate() {
+        // do nothing
+    }
+
+    @Override
+    public void activate() {
+        // do nothing
+    }
+
+    @Override
+    public void ensurePersisted() {
+        // do nothing
+    }
+
+    @Override
+    public void purge() throws IOException {
+        // do nothing
+    }
+
+    @Override
+    public void close() throws IOException {
+        // TBD
+    }
+
+    //@Override
+    public void setPageHeaderDetails(String details) {
+        headerDetails = details;
+    }
+
+    public int getWritePosition() {
+        return writePosition;
+    }
+
+    public int getElementCount() {
+        return elementCount;
+    }
+
+    public long getMinSeqNum() {
+        return minSeqNum;
+    }
+
+    // used in tests
+    public byte[] getBuffer() {
+        return buffer;
+    }
+
+    // used in tests
+    public String readHeaderDetails() throws IOException {
+        int tempPosition = readPosition;
+        streamedInput.movePosition(0);
+        int ver = streamedInput.readInt();
+        String details = new String(streamedInput.readByteArray());
+        streamedInput.movePosition(tempPosition);
+        return details;
+    }
+
+    private void setReadPoint(long seqNum) throws IOException {
+        int readPosition = offsetMap.get(calcRelativeSeqNum(seqNum));
+        streamedInput.movePosition(readPosition);
+    }
+
+    private int calcRelativeSeqNum(long seqNum) {
+        return (int) (seqNum - minSeqNum);
+    }
+
+    private int addHeader() throws IOException {
+        streamedOutput.writeInt(Checkpoint.VERSION);
+        byte[] details = headerDetails.getBytes();
+        streamedOutput.writeByteArray(details);
+        return VERSION_SIZE + LENGTH_SIZE + details.length;
+    }
+
+    private int verifyHeader() throws IOException {
+        int ver = streamedInput.readInt();
+        if (ver != Checkpoint.VERSION) {
+            String msg = String.format("Page version mismatch, expecting: %d, this version: %d", Checkpoint.VERSION, ver);
+            throw new IOException(msg);
+        }
+        int len = streamedInput.readInt();
+        streamedInput.skip(len);
+        return VERSION_SIZE + LENGTH_SIZE + len;
+    }
+
+    private void writeToBuffer(long seqNum, byte[] data, int len) throws IOException {
+        streamedOutput.setWriteWindow(writePosition, len);
+        crcWrappedOutput.writeLong(seqNum);
+        crcWrappedOutput.resetDigest();
+        crcWrappedOutput.writeByteArray(data);
+        long checksum = crcWrappedOutput.getChecksum();
+        crcWrappedOutput.writeInt((int) checksum);
+        crcWrappedOutput.flush();
+        crcWrappedOutput.close();
+    }
+
+    private SequencedList<byte[]> read(int limit) throws IOException {
+        List<byte[]> elements = new ArrayList<>();
+        List<Long> seqNums = new ArrayList<>();
+
+        int upto = available(limit);
+        for (int i = 0; i < upto; i++) {
+            long seqNum = readSeqNum();
+            byte[] data = readData();
+            skipChecksum();
+            elements.add(data);
+            seqNums.add(seqNum);
+        }
+        return new SequencedList<>(elements, seqNums);
+    }
+
+    private long readSeqNum() throws IOException {
+        return streamedInput.readLong();
+    }
+
+    private byte[] readData() throws IOException {
+        return streamedInput.readByteArray();
+    }
+
+    private void skipChecksum() throws IOException {
+        streamedInput.skip(CHECKSUM_SIZE);
+    }
+
+    private int available(int sought) {
+        if (elementCount < 1) return 0;
+        if (elementCount < sought) return elementCount;
+        return sought;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/MmapPageIO.java b/logstash-core/src/main/java/org/logstash/common/io/MmapPageIO.java
new file mode 100644
index 00000000000..617e444acd8
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/MmapPageIO.java
@@ -0,0 +1,255 @@
+package org.logstash.common.io;
+
+import org.logstash.ackedqueue.Queueable;
+import org.logstash.ackedqueue.SequencedList;
+
+import java.io.File;
+import java.io.IOException;
+import java.io.RandomAccessFile;
+import java.nio.MappedByteBuffer;
+import java.nio.channels.FileChannel;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.zip.CRC32;
+import java.util.zip.Checksum;
+
+// TODO: this essentially a copy of ByteBufferPageIO and should be DRY'ed - temp impl to test file based stress test
+
+public class MmapPageIO implements PageIO {
+    public static final byte VERSION = 1;
+    public static final int CHECKSUM_SIZE = Integer.BYTES;
+    public static final int LENGTH_SIZE = Integer.BYTES;
+    public static final int SEQNUM_SIZE = Long.BYTES;
+    public static final int MIN_RECORD_SIZE = SEQNUM_SIZE + CHECKSUM_SIZE;
+    public static final int HEADER_SIZE = 1;     // version byte
+    static final List<byte[]> EMPTY_READ = new ArrayList<>(0);
+
+    private final int capacity;
+    private final String dirPath;
+    private final int pageNum;
+    private final List<Integer> offsetMap; // has to be extendable
+
+    private MappedByteBuffer buffer;
+    private File file;
+    private FileChannel channel;
+
+    private long minSeqNum; // TODO: to make minSeqNum final we have to pass in the minSeqNum in the constructor and not set it on first write
+    private int elementCount;
+    private int head;
+    private byte version;
+    private Checksum checkSummer;
+
+    public MmapPageIO(int pageNum, int capacity, String dirPath) throws IOException {
+        this.pageNum = pageNum;
+        this.capacity = capacity;
+        this.dirPath = dirPath;
+        this.offsetMap = new ArrayList<>();
+        this.checkSummer = new CRC32();
+    }
+
+    @Override
+    public void open(long minSeqNum, int elementCount) throws IOException {
+        this.minSeqNum = minSeqNum;
+        this.elementCount = elementCount;
+
+        this.file = buildPath().toFile();
+        RandomAccessFile raf = new RandomAccessFile(this.file, "rw");
+        this.channel = raf.getChannel();
+        this.buffer = this.channel.map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
+        raf.close();
+        this.buffer.load();
+
+        this.buffer.position(0);
+        this.version = this.buffer.get();
+        this.head = 1;
+
+        if (this.elementCount > 0) {
+
+            // TODO: refactor the read logic below to DRY with the read() method.
+
+            // set head by skipping over all elements
+            for (int i = 0; i < this.elementCount; i++) {
+                if (this.head + SEQNUM_SIZE + LENGTH_SIZE > capacity) {
+                    throw new IOException(String.format("cannot read seqNum and length bytes past buffer capacity"));
+                }
+
+                long seqNum = this.buffer.getLong();
+
+                if (i == 0 && seqNum != this.minSeqNum) {
+                    throw new IOException(String.format("first seqNum=%d is different than minSeqNum=%d", seqNum, this.minSeqNum));
+                }
+
+                this.offsetMap.add(head);
+                this.head += SEQNUM_SIZE;
+
+
+                int length = this.buffer.getInt();
+                this.head += LENGTH_SIZE;
+
+                if (this.head + length + CHECKSUM_SIZE > capacity) {
+                    throw new IOException(String.format("cannot read element payload and checksum past buffer capacity"));
+                }
+
+                // skip over data
+                this.head += length;
+                this.head += CHECKSUM_SIZE;
+
+                this.buffer.position(head);
+            }
+        }
+    }
+
+    @Override
+    public void create() throws IOException {
+        this.file = buildPath().toFile();
+        RandomAccessFile raf = new RandomAccessFile(this.file, "rw");
+        this.channel = raf.getChannel();
+        this.buffer = this.channel.map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
+        raf.close();
+
+        this.buffer.position(0);
+        this.buffer.put(VERSION);
+        this.head = 1;
+        this.minSeqNum = 0;
+        this.elementCount = 0;
+    }
+
+    @Override
+    public int getCapacity() {
+        return this.capacity;
+    }
+
+    public long getMinSeqNum() {
+        return this.minSeqNum;
+    }
+
+    @Override
+    public boolean hasSpace(int bytes) {
+        int bytesLeft = this.capacity - this.head;
+        return persistedByteCount(bytes) <= bytesLeft;
+    }
+
+    @Override
+    public void write(byte[] bytes, long seqNum) throws IOException {
+        // since writes always happen at head, we can just append head to the offsetMap
+        assert this.offsetMap.size() == this.elementCount :
+                String.format("offsetMap size=%d != elementCount=%d", this.offsetMap.size(), this.elementCount);
+
+        int initialHead = this.head;
+
+        this.buffer.position(this.head);
+        this.buffer.putLong(seqNum);
+        this.buffer.putInt(bytes.length);
+        this.buffer.put(bytes);
+        this.buffer.putInt(checksum(bytes));
+        this.head += persistedByteCount(bytes.length);
+        assert this.head == this.buffer.position() :
+                String.format("head=%d != buffer position=%d", this.head, this.buffer.position());
+
+        if (this.elementCount <= 0) {
+            this.minSeqNum = seqNum;
+        }
+        this.offsetMap.add(initialHead);
+        this.elementCount++;
+    }
+
+    @Override
+    public SequencedList<byte[]> read(long seqNum, int limit) throws IOException {
+        assert seqNum >= this.minSeqNum :
+                String.format("seqNum=%d < minSeqNum=%d", seqNum, this.minSeqNum);
+        assert seqNum <= maxSeqNum() :
+                String.format("seqNum=%d is > maxSeqNum=%d", seqNum, maxSeqNum());
+
+        List<byte[]> elements = new ArrayList<>();
+        List<Long> seqNums = new ArrayList<>();
+
+        int offset = this.offsetMap.get((int)(seqNum - this.minSeqNum));
+
+        this.buffer.position(offset);
+
+        for (int i = 0; i < limit; i++) {
+            long readSeqNum = this.buffer.getLong();
+
+            assert readSeqNum == (seqNum + i) :
+                    String.format("unmatched seqNum=%d to readSeqNum=%d", seqNum + i, readSeqNum);
+
+            int readLength = this.buffer.getInt();
+            byte[] readBytes = new byte[readLength];
+            this.buffer.get(readBytes);
+            int checksum = this.buffer.getInt();
+            int computedChecksum = checksum(readBytes);
+            if (computedChecksum != checksum) {
+                throw new IOException(String.format("computed checksum=%d != checksum for file=%d", computedChecksum, checksum));
+            }
+
+            elements.add(readBytes);
+            seqNums.add(readSeqNum);
+
+            if (seqNum + i >= maxSeqNum()) {
+                break;
+            }
+        }
+
+        return new SequencedList<byte[]>(elements, seqNums);
+    }
+
+    @Override
+    public void deactivate() throws IOException {
+        close(); // close can be called multiple times
+    }
+
+    @Override
+    public void activate() throws IOException {
+        if (this.channel == null) {
+            RandomAccessFile raf = new RandomAccessFile(this.file, "rw");
+            this.channel = raf.getChannel();
+            this.buffer = this.channel.map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
+            raf.close();
+            this.buffer.load();
+        } else {
+//            assert this.channel.isOpen() : String.format("closed channel");
+        }
+    }
+
+    @Override
+    public void ensurePersisted() {
+        this.buffer.force();
+    }
+
+    @Override
+    public void purge() throws IOException {
+        close();
+        Files.delete(buildPath());
+    }
+
+    @Override
+    public void close() throws IOException {
+        if (this.channel != null && this.channel.isOpen()) {
+            this.channel.close();
+        }
+        this.channel = null;
+        this.buffer = null;
+    }
+
+    private int checksum(byte[] bytes) {
+        checkSummer.reset();
+        checkSummer.update(bytes, 0, bytes.length);
+        return (int) checkSummer.getValue();
+    }
+
+    @Override
+    public int persistedByteCount(int byteCount) {
+        return SEQNUM_SIZE + LENGTH_SIZE + byteCount + CHECKSUM_SIZE;
+    }
+
+    private long maxSeqNum() {
+        return this.minSeqNum + this.elementCount - 1;
+    }
+
+    private Path buildPath() {
+        return Paths.get(this.dirPath, "page." + this.pageNum);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/PageIO.java b/logstash-core/src/main/java/org/logstash/common/io/PageIO.java
new file mode 100644
index 00000000000..a9960a45c60
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/PageIO.java
@@ -0,0 +1,50 @@
+package org.logstash.common.io;
+
+import org.logstash.ackedqueue.Queueable;
+import org.logstash.ackedqueue.SequencedList;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.List;
+
+public interface PageIO extends Closeable {
+
+    // the concrete class should be constructed with the pageNum, capacity and dirPath attributes
+
+    // open an existing data container and reconstruct internal state if required
+    void open(long minSeqNum, int elementCount) throws IOException;
+
+    // create a new empty data file
+    void create() throws IOException;
+
+    // verify if the data container has space for the given number of bytes
+    boolean hasSpace(int bytes);
+
+    // write the given bytes to the data container
+    void write(byte[] bytes, long seqNum) throws IOException;
+
+    // read up to limit number of items starting at give seqNum
+    SequencedList<byte[]> read(long seqNum, int limit) throws IOException;
+
+    // @return the data container total capacity in bytes
+    int getCapacity();
+
+    // @return the actual persisted byte count (with overhead) for the given data bytes
+    int persistedByteCount(int bytes);
+
+    // signal that this data page is not active and resources can be released
+    void deactivate() throws IOException;
+
+    // signal that this data page is active will be read or written to
+    // should do nothing if page is aready active
+    void activate() throws IOException;
+
+    // issue the proper data container "fsync" sematic
+    void ensurePersisted();
+
+    // delete/unlink/remove data file
+    void purge() throws IOException;
+
+    // add debugging details, to be added when creating a new page
+    // void setPageHeaderDetails(String details);
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/PageIOFactory.java b/logstash-core/src/main/java/org/logstash/common/io/PageIOFactory.java
new file mode 100644
index 00000000000..ca9d7fa2067
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/PageIOFactory.java
@@ -0,0 +1,8 @@
+package org.logstash.common.io;
+
+import java.io.IOException;
+
+@FunctionalInterface
+public interface PageIOFactory {
+    PageIO build(int pageNum, int capacity, String dirPath) throws IOException;
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/StreamInput.java b/logstash-core/src/main/java/org/logstash/common/io/StreamInput.java
new file mode 100644
index 00000000000..c387657a309
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/StreamInput.java
@@ -0,0 +1,86 @@
+package org.logstash.common.io;
+
+import java.io.IOException;
+import java.io.InputStream;
+
+public abstract class StreamInput extends InputStream {
+    /**
+     * Reads and returns a single byte.
+     */
+    public abstract byte readByte() throws IOException;
+
+    /**
+     * Reads a specified number of bytes into an array at the specified offset.
+     *
+     * @param b      the array to read bytes into
+     * @param offset the offset in the array to start storing bytes
+     * @param len    the number of bytes to read
+     */
+    public abstract void readBytes(byte[] b, int offset, int len) throws IOException;
+
+    /**
+     * Reads four bytes and returns an int.
+     */
+    public int readInt() throws IOException {
+        return ((readByte() & 0xFF) << 24) | ((readByte() & 0xFF) << 16)
+                | ((readByte() & 0xFF) << 8) | (readByte() & 0xFF);
+    }
+    
+    /**
+     * Reads an int stored in variable-length format.  Reads between one and
+     * five bytes.  Smaller values take fewer bytes.  Negative numbers
+     * will always use all 5 bytes and are therefore better serialized
+     * using {@link #readInt}
+     */
+    public int readVInt() throws IOException {
+        byte b = readByte();
+        int i = b & 0x7F;
+        if ((b & 0x80) == 0) {
+            return i;
+        }
+        b = readByte();
+        i |= (b & 0x7F) << 7;
+        if ((b & 0x80) == 0) {
+            return i;
+        }
+        b = readByte();
+        i |= (b & 0x7F) << 14;
+        if ((b & 0x80) == 0) {
+            return i;
+        }
+        b = readByte();
+        i |= (b & 0x7F) << 21;
+        if ((b & 0x80) == 0) {
+            return i;
+        }
+        b = readByte();
+        assert (b & 0x80) == 0;
+        return i | ((b & 0x7F) << 28);
+    }
+
+    /**
+     * Reads two bytes and returns a short.
+     */
+    public short readShort() throws IOException {
+        int i = ((readByte() & 0xFF) <<  8);
+        int j = (readByte() & 0xFF);
+        return (short) (i | j);
+    }
+
+    /**
+     * Reads eight bytes and returns a long.
+     */
+    public long readLong() throws IOException {
+        return (((long) readInt()) << 32) | (readInt() & 0xFFFFFFFFL);
+    }
+
+    public byte[] readByteArray() throws IOException {
+        int length = readInt();
+        byte[] values = new byte[length];
+        for (int i = 0; i < length; i++) {
+            values[i] = readByte();
+        }
+        return values;
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/StreamOutput.java b/logstash-core/src/main/java/org/logstash/common/io/StreamOutput.java
new file mode 100644
index 00000000000..76c4271e4bf
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/StreamOutput.java
@@ -0,0 +1,74 @@
+package org.logstash.common.io;
+
+import java.io.IOException;
+import java.io.OutputStream;
+
+public abstract class StreamOutput extends OutputStream {
+    @Override
+    public void write(int b) throws IOException {
+        writeByte((byte) b);
+    }
+
+    public abstract void writeByte(byte b) throws IOException;
+
+    public abstract void writeBytes(byte[] b, int offset, int length) throws IOException;
+
+    public abstract void reset() throws IOException;
+
+    /**
+     * Writes an int in a variable-length format.  Writes between one and
+     * five bytes.  Smaller values take fewer bytes.  Negative numbers
+     * will always use all 5 bytes and are therefore better serialized
+     * using {@link #writeInt}
+     */
+    public void writeVInt(int i) throws IOException {
+        while ((i & ~0x7F) != 0) {
+            writeByte((byte) ((i & 0x7f) | 0x80));
+            i >>>= 7;
+        }
+        writeByte((byte) i);
+    }
+
+    /**
+     * Writes a short as two bytes.
+     */
+    public void writeShort(short i) throws IOException {
+        writeByte((byte)(i >>  8));
+        writeByte((byte) i);
+    }
+
+    /**
+     * Writes an int as four bytes.
+     */
+    public void writeInt(int i) throws IOException {
+        writeByte((byte) (i >> 24));
+        writeByte((byte) (i >> 16));
+        writeByte((byte) (i >> 8));
+        writeByte((byte) i);
+    }
+
+    public void writeIntArray(int[] values) throws IOException {
+        writeVInt(values.length);
+        for (int value : values) {
+            writeInt(value);
+        }
+    }
+
+    /**
+     * Writes a long as eight bytes.
+     */
+    public void writeLong(long i) throws IOException {
+        writeInt((int) (i >> 32));
+        writeInt((int) i);
+    }
+
+    /**
+     * Writes an array of bytes.
+     *
+     * @param b the bytes to write
+     */
+    public void writeByteArray(byte[] b) throws IOException {
+        writeInt(b.length);
+        writeBytes(b, 0, b.length);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/Streams.java b/logstash-core/src/main/java/org/logstash/common/io/Streams.java
new file mode 100644
index 00000000000..2e237fb4d9d
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/Streams.java
@@ -0,0 +1,61 @@
+package org.logstash.common.io;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.Reader;
+/**
+ */
+public abstract class Streams {
+
+    public static int readFully(Reader reader, char[] dest) throws IOException {
+        return readFully(reader, dest, 0, dest.length);
+    }
+
+    public static int readFully(Reader reader, char[] dest, int offset, int len) throws IOException {
+        int read = 0;
+        while (read < len) {
+            final int r = reader.read(dest, offset + read, len - read);
+            if (r == -1) {
+                break;
+            }
+            read += r;
+        }
+        return read;
+    }
+
+    public static int readFully(InputStream reader, byte[] dest) throws IOException {
+        return readFully(reader, dest, 0, dest.length);
+    }
+
+    public static int readFully(InputStream reader, byte[] dest, int offset, int len) throws IOException {
+        int read = 0;
+        while (read < len) {
+            final int r = reader.read(dest, offset + read, len - read);
+            if (r == -1) {
+                break;
+            }
+            read += r;
+        }
+        return read;
+    }
+}
+
diff --git a/logstash-core/src/main/java/org/logstash/config/compiler/CompilationError.java b/logstash-core/src/main/java/org/logstash/config/compiler/CompilationError.java
new file mode 100644
index 00000000000..b640a37991c
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/compiler/CompilationError.java
@@ -0,0 +1,16 @@
+package org.logstash.config.compiler;
+
+import org.logstash.config.ir.InvalidIRException;
+
+/**
+ * Created by andrewvc on 9/22/16.
+ */
+public class CompilationError extends Exception {
+    public CompilationError(String s, InvalidIRException e) {
+        super(s,e);
+    }
+
+    public CompilationError(String s) {
+        super(s);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/compiler/IExpressionCompiler.java b/logstash-core/src/main/java/org/logstash/config/compiler/IExpressionCompiler.java
new file mode 100644
index 00000000000..fe09782bd4a
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/compiler/IExpressionCompiler.java
@@ -0,0 +1,11 @@
+package org.logstash.config.compiler;
+
+import org.logstash.config.compiler.compiled.ICompiledExpression;
+import org.logstash.config.ir.expression.Expression;
+
+/**
+ * Created by andrewvc on 9/22/16.
+ */
+public interface IExpressionCompiler {
+    ICompiledExpression compile(Expression expression) throws CompilationError;
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/compiler/IPluginCompiler.java b/logstash-core/src/main/java/org/logstash/config/compiler/IPluginCompiler.java
new file mode 100644
index 00000000000..51ee352783a
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/compiler/IPluginCompiler.java
@@ -0,0 +1,14 @@
+package org.logstash.config.compiler;
+
+import org.logstash.config.compiler.compiled.ICompiledInputPlugin;
+import org.logstash.config.compiler.compiled.ICompiledProcessor;
+import org.logstash.config.ir.graph.PluginVertex;
+
+/**
+ * Created by andrewvc on 9/22/16.
+ */
+public interface IPluginCompiler {
+   ICompiledInputPlugin compileInput(PluginVertex vertex) throws CompilationError;
+   ICompiledProcessor compileFilter(PluginVertex vertex) throws CompilationError;
+   ICompiledProcessor compileOutput(PluginVertex vertex) throws CompilationError;
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/compiler/IfCompiler.java b/logstash-core/src/main/java/org/logstash/config/compiler/IfCompiler.java
new file mode 100644
index 00000000000..66be4b94b74
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/compiler/IfCompiler.java
@@ -0,0 +1,71 @@
+package org.logstash.config.compiler;
+
+import org.logstash.Event;
+import org.logstash.config.compiler.compiled.ICompiledExpression;
+import org.logstash.config.compiler.compiled.ICompiledProcessor;
+import org.logstash.config.ir.graph.BooleanEdge;
+import org.logstash.config.ir.graph.Edge;
+import org.logstash.config.ir.graph.IfVertex;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Created by andrewvc on 10/13/16.
+ */
+public class IfCompiler {
+    public class CompiledIf implements ICompiledProcessor {
+        private final ICompiledExpression compiledExpression;
+        private final List<BooleanEdge> trueEdges;
+        private final List<BooleanEdge> falseEdges;
+
+        public CompiledIf(ICompiledExpression compiledExpression, List<BooleanEdge> trueEdges, List<BooleanEdge> falseEdges) {
+            this.compiledExpression = compiledExpression;
+            this.trueEdges = trueEdges;
+            this.falseEdges = falseEdges;
+        }
+
+        @Override
+        public Map<Edge, List<Event>> process(List<Event> events) {
+            List<Boolean> booleans = compiledExpression.execute(events);
+
+            ArrayList<Event> trueEvents = new ArrayList<>(events.size());
+            ArrayList<Event> falseEvents = new ArrayList<>(events.size());
+            for (int i = 0; i < events.size(); i++) {
+                if (booleans.get(i)) {
+                    trueEvents.add(events.get(i));
+                } else {
+                    falseEvents.add(events.get(i));
+                }
+            }
+
+            Map<Edge, List<Event>> out = new HashMap<>();
+            trueEdges.stream().forEach(e -> out.put(e, trueEvents));
+            falseEdges.stream().forEach(e -> out.put(e, falseEvents));
+            return out;
+        }
+
+        @Override
+        public void register() {
+            // Nothing to do!
+        }
+
+        @Override
+        public void stop() {
+            // Nothing to do!
+        }
+    }
+
+    private final IExpressionCompiler expressionCompiler;
+
+    public IfCompiler(IExpressionCompiler expressionCompiler) {
+        this.expressionCompiler = expressionCompiler;
+    }
+
+    public ICompiledProcessor compile(IfVertex vertex) throws CompilationError {
+        ICompiledExpression compiledExpression = expressionCompiler.compile(vertex.getBooleanExpression());
+        return new CompiledIf(compiledExpression, vertex.getOutgoingBooleanEdgesByType(true), vertex.getOutgoingBooleanEdgesByType(false));
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/compiler/RubyExpressionCompiler.java b/logstash-core/src/main/java/org/logstash/config/compiler/RubyExpressionCompiler.java
new file mode 100644
index 00000000000..2dd4619bab8
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/compiler/RubyExpressionCompiler.java
@@ -0,0 +1,51 @@
+package org.logstash.config.compiler;
+
+import org.jruby.RubyInstanceConfig;
+import org.jruby.embed.ScriptingContainer;
+import org.logstash.Event;
+import org.logstash.config.compiler.compiled.ICompiledExpression;
+import org.logstash.config.ir.expression.Expression;
+
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+
+/**
+ * Created by andrewvc on 10/11/16.
+ */
+public class RubyExpressionCompiler implements IExpressionCompiler {
+    private final ScriptingContainer container;
+    private long expressionCounter;
+
+    private class CompiledRubyExpression implements ICompiledExpression {
+        private final String expressionSource;
+        private final long expressionId;
+        private final String methodName;
+
+        public CompiledRubyExpression(long expressionId, String expressionSource) {
+            this.expressionId = expressionId;
+            this.methodName = "condition_" + expressionId;
+            container.runScriptlet("def " + methodName + "(events); events.map {|event| " + expressionSource + " }; end");
+            this.expressionSource = expressionSource;
+        }
+
+        @Override
+        public List<Boolean> execute(Collection<Event> events) {
+            // Unchecked cast because we enforce this ourselves. No need to take a speed hit here
+            List<Boolean> result = (List<Boolean>) container.callMethod(null, methodName, events);
+            return result;
+        }
+    }
+
+    public RubyExpressionCompiler() {
+        this.expressionCounter = 0l;
+        this.container = new ScriptingContainer();
+        this.container.setCompileMode(RubyInstanceConfig.CompileMode.FORCE);
+    }
+
+    @Override
+    public ICompiledExpression compile(Expression expression) throws CompilationError {
+        long expressionId = expressionCounter += 1;
+        return new CompiledRubyExpression(expressionId, expression.toRubyString());
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/compiler/compiled/ICompiledExpression.java b/logstash-core/src/main/java/org/logstash/config/compiler/compiled/ICompiledExpression.java
new file mode 100644
index 00000000000..2a86cbd69e5
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/compiler/compiled/ICompiledExpression.java
@@ -0,0 +1,18 @@
+package org.logstash.config.compiler.compiled;
+
+import org.logstash.Event;
+
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+
+/**
+ * Created by andrewvc on 9/22/16.
+ */
+public interface ICompiledExpression {
+    List<Boolean> execute(Collection<Event> events);
+
+    default Boolean execute(Event event) {
+        return execute(Collections.singletonList(event)).get(0);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/compiler/compiled/ICompiledInputPlugin.java b/logstash-core/src/main/java/org/logstash/config/compiler/compiled/ICompiledInputPlugin.java
new file mode 100644
index 00000000000..13ab6c0aa3c
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/compiler/compiled/ICompiledInputPlugin.java
@@ -0,0 +1,9 @@
+package org.logstash.config.compiler.compiled;
+
+import org.logstash.config.pipeline.pipette.IPipetteProducer;
+
+/**
+ * Created by andrewvc on 9/22/16.
+ */
+public interface ICompiledInputPlugin extends ICompiledVertex, IPipetteProducer {
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/compiler/compiled/ICompiledProcessor.java b/logstash-core/src/main/java/org/logstash/config/compiler/compiled/ICompiledProcessor.java
new file mode 100644
index 00000000000..ed2da972e22
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/compiler/compiled/ICompiledProcessor.java
@@ -0,0 +1,14 @@
+package org.logstash.config.compiler.compiled;
+
+import org.logstash.Event;
+import org.logstash.config.ir.graph.Edge;
+
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Created by andrewvc on 9/22/16.
+ */
+public interface ICompiledProcessor extends ICompiledVertex {
+    Map<Edge, List<Event>> process(List<Event> events);
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/compiler/compiled/ICompiledVertex.java b/logstash-core/src/main/java/org/logstash/config/compiler/compiled/ICompiledVertex.java
new file mode 100644
index 00000000000..8f69e8aa0c2
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/compiler/compiled/ICompiledVertex.java
@@ -0,0 +1,9 @@
+package org.logstash.config.compiler.compiled;
+
+/**
+ * Created by andrewvc on 10/14/16.
+ */
+public interface ICompiledVertex {
+    void register();
+    void stop();
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/DSL.java b/logstash-core/src/main/java/org/logstash/config/ir/DSL.java
new file mode 100644
index 00000000000..56e5c642e36
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/DSL.java
@@ -0,0 +1,285 @@
+package org.logstash.config.ir;
+
+import org.logstash.config.ir.expression.*;
+import org.logstash.config.ir.expression.binary.*;
+import org.logstash.config.ir.expression.unary.Not;
+import org.logstash.config.ir.expression.unary.Truthy;
+import org.logstash.config.ir.graph.Graph;
+import org.logstash.config.ir.graph.IfVertex;
+import org.logstash.config.ir.graph.PluginVertex;
+import org.logstash.config.ir.imperative.*;
+
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ * Created by andrewvc on 9/15/16.
+ */
+public class DSL {
+    public static EventValueExpression eEventValue(SourceMetadata meta, String fieldName) {
+        return new EventValueExpression(meta, fieldName);
+    }
+
+    public static EventValueExpression eEventValue(String fieldName) {
+        return eEventValue(new SourceMetadata(), fieldName);
+    }
+
+    public static ValueExpression eValue(SourceMetadata meta, Object value) throws InvalidIRException {
+        return new ValueExpression(meta, value);
+    }
+
+    public static ValueExpression eValue(Object value) throws InvalidIRException {
+        return eValue(new SourceMetadata(), value);
+    }
+
+    public static ValueExpression eRegex(SourceMetadata meta, String pattern) throws InvalidIRException {
+       return new RegexValueExpression(meta, pattern);
+    }
+
+    public static ValueExpression eRegex(String pattern) throws InvalidIRException {
+        return eRegex(new SourceMetadata(), pattern);
+    }
+
+    public static ValueExpression eValue(long value) {
+        try {
+            return eValue(new SourceMetadata(), value);
+        } catch (InvalidIRException e) {
+            e.printStackTrace(); // Can't happen with an int
+            return null;
+        }
+    }
+
+    public static ValueExpression eValue(double value) {
+        try {
+            return eValue(new SourceMetadata(), value);
+        } catch (InvalidIRException e) {
+            e.printStackTrace(); // Can't happen with an int
+            return null;
+        }
+    }
+
+    public static Gt eGt(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        return new Gt(meta, left, right);
+    }
+
+    public static Gt eGt(Expression left, Expression right) throws InvalidIRException {
+        return new Gt(null, left, right);
+    }
+
+    public static Gte eGte(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        return new Gte(meta, left, right);
+    }
+
+    public static Gte eGte(Expression left, Expression right) throws InvalidIRException {
+        return new Gte(null, left, right);
+    }
+
+    public static Lt eLt(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        return new Lt(meta, left, right);
+    }
+
+    public static Lt eLt(Expression left, Expression right) throws InvalidIRException {
+        return new Lt(null, left, right);
+    }
+
+    public static Lte eLte(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        return new Lte(meta, left, right);
+    }
+    public static Lte eLte(Expression left, Expression right) throws InvalidIRException {
+        return new Lte(null, left, right);
+    }
+
+    public static Eq eEq(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        return new Eq(meta, left, right);
+    }
+
+    public static Eq eEq(Expression left, Expression right) throws InvalidIRException {
+        return new Eq(null, left, right);
+    }
+
+    public static And eAnd(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        return new And(meta, left, right);
+    }
+
+    public static And eAnd(Expression left, Expression right) throws InvalidIRException {
+        return new And(null, left, right);
+    }
+
+    public static Or eOr(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        return new Or(meta, left, right);
+    }
+
+    public static Or eOr(Expression left, Expression right) throws InvalidIRException {
+        return new Or(null, left, right);
+    }
+
+    public static RegexEq eRegexEq(SourceMetadata meta, Expression left, ValueExpression right) throws InvalidIRException {
+        return new RegexEq(meta, left, right);
+    }
+
+    public static RegexEq eRegexEq(Expression left, ValueExpression right) throws InvalidIRException {
+        return new RegexEq(null, left, right);
+    }
+
+    public static Expression eRegexNeq(SourceMetadata meta, Expression left, ValueExpression right) throws InvalidIRException {
+        return eNot(eRegexEq(meta, left, right));
+    }
+
+    public static Expression eRegexNeq(Expression left, ValueExpression right) throws InvalidIRException {
+        return eNot(eRegexEq(left, right));
+    }
+
+    public static Neq eNeq(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        return new Neq(meta, left, right);
+    }
+    public static Neq eNeq(Expression left, Expression right) throws InvalidIRException {
+        return new Neq(null, left, right);
+    }
+
+    public static In eIn(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        return new In(meta, left, right);
+    }
+
+    public static In eIn(Expression left, Expression right) throws InvalidIRException {
+        return new In(null, left, right);
+    }
+
+    public static Not eNot(SourceMetadata meta, Expression expr) throws InvalidIRException {
+        return new Not(meta, expr);
+    }
+
+    public static Not eNot(Expression expr) throws InvalidIRException {
+        return new Not(null, expr);
+    }
+
+    public static Truthy eTruthy(SourceMetadata meta, Expression expr) throws InvalidIRException {
+        return new Truthy(meta, expr);
+    }
+    public static Truthy eTruthy(Expression expr) throws InvalidIRException {
+        return new Truthy(null, expr);
+    }
+
+    public static Statement iCompose(ComposedStatement.IFactory factory, SourceMetadata meta, Statement... statements) throws InvalidIRException {
+        if (statements.length == 0 ) {
+            return new NoopStatement(meta);
+        } else if (statements.length == 1 ) {
+            return statements[0];
+        } else {
+            return factory.make(meta, Arrays.asList(statements));
+        }
+    }
+
+    public static Statement iComposeSequence(SourceMetadata meta, Statement... statements) throws InvalidIRException {
+        return iCompose(ComposedSequenceStatement::new, meta, statements);
+    }
+
+    public static Statement iComposeSequence(Statement... statements) throws InvalidIRException {
+        return iComposeSequence(null, statements);
+    }
+
+    public static Statement iComposeParallel(SourceMetadata meta, Statement... statements) throws InvalidIRException {
+        return iCompose(ComposedParallelStatement::new, meta, statements);
+    }
+
+    public static Statement iComposeParallel(Statement... statements) throws InvalidIRException {
+        return iComposeParallel(null, statements);
+    }
+
+    public static NoopStatement noop(SourceMetadata meta) {
+        return new NoopStatement(meta);
+    }
+
+    public static NoopStatement noop() {
+        return new NoopStatement(new SourceMetadata());
+    }
+
+    public static PluginStatement iPlugin(SourceMetadata meta, PluginDefinition.Type pluginType,  String pluginName, Map<String, Object> pluginArguments) {
+        return new PluginStatement(meta, new PluginDefinition(pluginType, pluginName, pluginArguments));
+    }
+
+    public static PluginStatement iPlugin(PluginDefinition.Type type, String pluginName, Map<String, Object> pluginArguments) {
+        return iPlugin(new SourceMetadata(), type, pluginName, pluginArguments);
+    }
+
+    public static PluginStatement iPlugin(PluginDefinition.Type type, String pluginName, MapBuilder<String, Object> argBuilder) {
+        return iPlugin(type, pluginName, argBuilder.build());
+    }
+
+    public static PluginStatement iPlugin(PluginDefinition.Type type, String pluginName) {
+        return iPlugin(type, pluginName, pargs());
+    }
+
+    public static IfStatement iIf(SourceMetadata meta,
+                                  BooleanExpression condition,
+                                  Statement ifTrue,
+                                  Statement ifFalse) throws InvalidIRException {
+        return new IfStatement(meta, condition, ifTrue, ifFalse);
+    }
+
+    public static IfStatement iIf(BooleanExpression condition,
+                                  Statement ifTrue,
+                                  Statement ifFalse) throws InvalidIRException {
+        return iIf(new SourceMetadata(), condition, ifTrue, ifFalse);
+    }
+
+    public static IfStatement iIf(BooleanExpression condition,
+                                  Statement ifTrue) throws InvalidIRException {
+        return iIf(new SourceMetadata(), condition, ifTrue, noop());
+    }
+
+
+    public static class MapBuilder<K,V> {
+        private final HashMap<K, V> map;
+
+        public MapBuilder() {
+            this.map = new HashMap<>();
+        }
+
+        public MapBuilder<K, V> put(K k, V v) {
+            map.put(k, v);
+            return this;
+        }
+
+        public Map<K, V> build() {
+            return map;
+        }
+    }
+
+    static <K,V> MapBuilder<K,V> mapBuilder() {
+        return new MapBuilder<>();
+    }
+
+    public static MapBuilder<String, Object> argumentBuilder() {
+        return mapBuilder();
+    }
+
+    public static MapBuilder<String, Object> pargs() {
+        return argumentBuilder();
+    }
+
+    public static Graph graph() {
+        return new Graph();
+    }
+
+    public static PluginVertex gPlugin(SourceMetadata sourceMetadata, PluginDefinition.Type pluginType, String pluginName, Map<String, Object> pluginArgs) {
+       return new PluginVertex(sourceMetadata, new PluginDefinition(pluginType, pluginName, pluginArgs));
+    }
+
+    public static PluginVertex gPlugin(PluginDefinition.Type type, String pluginName, Map<String, Object> pluginArgs) {
+        return gPlugin(new SourceMetadata(), type, pluginName, pluginArgs);
+    }
+
+    public static PluginVertex gPlugin(PluginDefinition.Type type, String pluginName) {
+        return gPlugin(new SourceMetadata(), type, pluginName, new HashMap<>());
+    }
+
+
+    public static IfVertex gIf(SourceMetadata meta, BooleanExpression expression) {
+       return new IfVertex(meta, expression);
+    }
+
+    public static IfVertex gIf(BooleanExpression expression) {
+       return new IfVertex(new SourceMetadata(), expression);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/ISourceComponent.java b/logstash-core/src/main/java/org/logstash/config/ir/ISourceComponent.java
new file mode 100644
index 00000000000..af954dd09e2
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/ISourceComponent.java
@@ -0,0 +1,9 @@
+package org.logstash.config.ir;
+
+/**
+ * Created by andrewvc on 9/16/16.
+ */
+public interface ISourceComponent {
+    public boolean sourceComponentEquals(ISourceComponent sourceComponent);
+    public SourceMetadata getMeta();
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/InvalidIRException.java b/logstash-core/src/main/java/org/logstash/config/ir/InvalidIRException.java
new file mode 100644
index 00000000000..7c4a30bf208
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/InvalidIRException.java
@@ -0,0 +1,10 @@
+package org.logstash.config.ir;
+
+/**
+ * Created by andrewvc on 9/6/16.
+ */
+public class InvalidIRException extends Exception {
+    public InvalidIRException(String s) {
+        super(s);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/PluginDefinition.java b/logstash-core/src/main/java/org/logstash/config/ir/PluginDefinition.java
new file mode 100644
index 00000000000..bba5392f49d
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/PluginDefinition.java
@@ -0,0 +1,60 @@
+package org.logstash.config.ir;
+
+import java.util.Map;
+import java.util.Objects;
+
+/**
+ * Created by andrewvc on 9/20/16.
+ */
+public class PluginDefinition {
+    public enum Type {
+        INPUT,
+        FILTER,
+        OUTPUT,
+        CODEC
+    }
+
+    private final Type type;
+    private final String name;
+    private final Map<String,Object> arguments;
+
+    public Type getType() {
+        return type;
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public String getId() {
+        return (String) arguments.get("id");
+    }
+
+    public Map<String, Object> getArguments() {
+        return arguments;
+    }
+
+    public PluginDefinition(Type type, String name, Map<String, Object> arguments) {
+        this.type = type;
+        this.name = name;
+        this.arguments = arguments;
+    }
+
+    public String toString() {
+        return type.toString().toLowerCase() + "-" + name + arguments;
+    }
+
+    public int hashCode() {
+        return Objects.hash(type, name, arguments);
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (o == null) return false;
+        if (o instanceof PluginDefinition) {
+            PluginDefinition oPlugin = (PluginDefinition) o;
+            return type.equals(oPlugin.type) && name.equals(oPlugin.name) && arguments.equals(oPlugin.arguments);
+        }
+        return false;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/SourceComponent.java b/logstash-core/src/main/java/org/logstash/config/ir/SourceComponent.java
new file mode 100644
index 00000000000..70374314fae
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/SourceComponent.java
@@ -0,0 +1,24 @@
+package org.logstash.config.ir;
+
+/**
+ * Created by andrewvc on 9/6/16.
+ */
+public abstract class SourceComponent implements ISourceComponent {
+    private final SourceMetadata meta;
+
+    public SourceComponent(SourceMetadata meta) {
+        this.meta = meta;
+    }
+
+    public SourceMetadata getMeta() {
+        return meta;
+    }
+
+    public abstract boolean sourceComponentEquals(ISourceComponent sourceComponent);
+
+    public String toString(int indent) {
+        return "toString(int indent) should be implemented for " + this.getClass().getName();
+    }
+
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/SourceMetadata.java b/logstash-core/src/main/java/org/logstash/config/ir/SourceMetadata.java
new file mode 100644
index 00000000000..e45969ed30e
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/SourceMetadata.java
@@ -0,0 +1,46 @@
+package org.logstash.config.ir;
+
+/**
+ * Created by andrewvc on 9/6/16.
+ */
+public class SourceMetadata {
+    private final String sourceFile;
+
+    public String getSourceFile() {
+        return sourceFile;
+    }
+
+    public Integer getSourceLine() {
+        return sourceLine;
+    }
+
+    public Integer getSourceColumn() {
+        return sourceColumn;
+    }
+
+    public String getSourceText() {
+        return sourceText;
+    }
+
+    private final Integer sourceLine;
+    private final Integer sourceColumn;
+    private final String sourceText;
+
+    public SourceMetadata(String sourceFile, Integer sourceLine, Integer sourceChar, String sourceText) {
+        this.sourceFile = sourceFile;
+        this.sourceLine = sourceLine;
+        this.sourceColumn = sourceChar;
+        this.sourceText = sourceText;
+    }
+
+    public SourceMetadata() {
+        this.sourceFile = null;
+        this.sourceLine = null;
+        this.sourceColumn = null;
+        this.sourceText = null;
+    }
+
+    public String toString() {
+        return sourceFile + ":" + sourceLine + ":" + sourceColumn + ":```\n" + sourceText + "\n```";
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/BinaryBooleanExpression.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/BinaryBooleanExpression.java
new file mode 100644
index 00000000000..5c2e26ba22e
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/BinaryBooleanExpression.java
@@ -0,0 +1,48 @@
+package org.logstash.config.ir.expression;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+
+/**
+ * Created by andrewvc on 9/6/16.
+ */
+public abstract class BinaryBooleanExpression extends BooleanExpression {
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent sourceComponent) {
+        if (sourceComponent == null) return false;
+        if (this == sourceComponent) return true;
+        if (this.getClass().equals(sourceComponent.getClass())) {
+            BinaryBooleanExpression other = (BinaryBooleanExpression) sourceComponent;
+            return (this.getLeft().sourceComponentEquals(other.getLeft()) &&
+                    this.getRight().sourceComponentEquals(other.getRight()));
+        }
+        return false;
+    }
+
+    private final Expression left;
+    private final Expression right;
+
+    public Expression getRight() {
+        return right;
+    }
+
+    public Expression getLeft() {
+        return left;
+    }
+
+    public BinaryBooleanExpression(SourceMetadata meta,
+                                   Expression left,
+                                   Expression right) throws InvalidIRException {
+        super(meta);
+        this.left = left;
+        this.right = right;
+    }
+
+    public abstract String rubyOperator();
+
+    @Override
+    public String toRubyString() {
+        return "(" + getLeft().toRubyString() + rubyOperator() + getRight().toRubyString() + ")";
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/BooleanExpression.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/BooleanExpression.java
new file mode 100644
index 00000000000..533d20f5c56
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/BooleanExpression.java
@@ -0,0 +1,13 @@
+package org.logstash.config.ir.expression;
+
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.expression.Expression;
+
+/**
+ * Created by andrewvc on 9/14/16.
+ */
+public abstract class BooleanExpression extends Expression {
+    public BooleanExpression(SourceMetadata meta) {
+        super(meta);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/EventValueExpression.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/EventValueExpression.java
new file mode 100644
index 00000000000..eeff014b6cb
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/EventValueExpression.java
@@ -0,0 +1,41 @@
+package org.logstash.config.ir.expression;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.SourceMetadata;
+
+/**
+ * Created by andrewvc on 9/13/16.
+ */
+public class EventValueExpression extends Expression {
+    private final String fieldName;
+
+    public EventValueExpression(SourceMetadata meta, String fieldName) {
+        super(meta);
+        this.fieldName = fieldName;
+    }
+
+    public String getFieldName() {
+        return fieldName;
+    }
+
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent sourceComponent) {
+        if (sourceComponent == null) return false;
+        if (this == sourceComponent) return true;
+        if (sourceComponent instanceof EventValueExpression) {
+            EventValueExpression other = (EventValueExpression) sourceComponent;
+            return (this.getFieldName().equals(other.getFieldName()));
+        }
+        return false;
+    }
+
+    @Override
+    public String toString() {
+        return "event.get('" + fieldName + "')";
+    }
+
+    @Override
+    public String toRubyString() {
+        return "event.getField('" + fieldName + "')";
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/Expression.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/Expression.java
new file mode 100644
index 00000000000..8a6b3a53e0f
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/Expression.java
@@ -0,0 +1,48 @@
+package org.logstash.config.ir.expression;
+
+import org.jruby.RubyInstanceConfig;
+import org.jruby.embed.AttributeName;
+import org.jruby.embed.EmbedEvalUnit;
+import org.jruby.embed.ScriptingContainer;
+import org.logstash.config.ir.SourceComponent;
+import org.logstash.config.ir.SourceMetadata;
+
+/**
+ * [foo] == "foostr" eAnd [bar] > 10
+ * eAnd(eEq(eventValueExpr("foo"), value("foostr")), eEq(eEventValue("bar"), value(10)))
+ *
+ * if [foo]
+ * notnull(eEventValue("foo"))
+ * Created by andrewvc on 9/6/16.
+ */
+public abstract class Expression extends SourceComponent {
+    private Object compiled;
+    private ScriptingContainer container;
+
+    public Expression(SourceMetadata meta) {
+        super(meta);
+    }
+
+    public boolean eval() {
+        return true;
+    }
+
+    public void compile() {
+        container = new ScriptingContainer();
+        container.setCompileMode(RubyInstanceConfig.CompileMode.JIT);
+        container.setAttribute(AttributeName.SHARING_VARIABLES, false);
+        container.runScriptlet("def start(event)\n" + this.toString() + "\nend");
+    }
+
+    @Override
+    public String toString(int indent) {
+        return toString();
+    }
+
+    @Override
+    public String toString() {
+        return toRubyString();
+    }
+
+    public abstract String toRubyString();
+}
\ No newline at end of file
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/RegexValueExpression.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/RegexValueExpression.java
new file mode 100644
index 00000000000..bbe8ae4756b
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/RegexValueExpression.java
@@ -0,0 +1,53 @@
+package org.logstash.config.ir.expression;
+
+import org.joni.Option;
+import org.joni.Regex;
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+
+/**
+ * Created by andrewvc on 9/15/16.
+ */
+public class RegexValueExpression extends ValueExpression {
+    private final Regex regex;
+
+    public RegexValueExpression(SourceMetadata meta, Object value) throws InvalidIRException {
+        super(meta, value);
+
+        if (!(value instanceof String)) {
+            throw new InvalidIRException("Regex value expressions can only take strings!");
+        }
+
+        byte[] patternBytes = getSource().getBytes();
+        this.regex = new Regex(patternBytes, 0, patternBytes.length, Option.NONE);
+    }
+
+    @Override
+    public Object get() {
+        return this.regex;
+    }
+
+    public String getSource() {
+        return (String) value;
+    }
+
+    @Override
+    public String toString() {
+        return this.value.toString();
+    }
+
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent other) {
+        if (other == null) return false;
+        if (other instanceof RegexValueExpression) {
+            return (((RegexValueExpression) other).getSource().equals(getSource()));
+        }
+        return false;
+    }
+
+    @Override
+    public String toRubyString() {
+       return (String) value;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/UnaryBooleanExpression.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/UnaryBooleanExpression.java
new file mode 100644
index 00000000000..3c1c84572df
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/UnaryBooleanExpression.java
@@ -0,0 +1,23 @@
+package org.logstash.config.ir.expression;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+
+/**
+ * Created by andrewvc on 9/13/16.
+ */
+public abstract class UnaryBooleanExpression extends BooleanExpression {
+    private final Expression expression;
+
+    public Expression getExpression() {
+        return expression;
+    }
+
+    public UnaryBooleanExpression(SourceMetadata meta,
+                                   Expression expression) throws InvalidIRException {
+        super(meta);
+        if (expression == null) throw new InvalidIRException("Unary expressions cannot operate on null!");
+        this.expression = expression;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/ValueExpression.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/ValueExpression.java
new file mode 100644
index 00000000000..db8b0324863
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/ValueExpression.java
@@ -0,0 +1,66 @@
+package org.logstash.config.ir.expression;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+
+import java.math.BigDecimal;
+import java.util.List;
+
+/**
+ * Created by andrewvc on 9/13/16.
+ */
+public class ValueExpression extends Expression {
+    protected final Object value;
+
+    public ValueExpression(SourceMetadata meta, Object value) throws InvalidIRException {
+        super(meta);
+
+        if (!(value == null ||
+                value instanceof Short ||
+                value instanceof Long ||
+                value instanceof Integer ||
+                value instanceof Float ||
+                value instanceof Double ||
+                value instanceof BigDecimal ||
+                value instanceof String ||
+                value instanceof List ||
+                value instanceof java.time.Instant
+        )) {
+            throw new InvalidIRException("Invalid eValue " + value + " with class " + value.getClass().getName());
+        }
+
+        this.value = value;
+    }
+
+    public Object get() {
+        return value;
+    }
+
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent sourceComponent) {
+        if (sourceComponent == null) return false;
+        if (this == sourceComponent) return true;
+        if (sourceComponent instanceof ValueExpression) {
+            ValueExpression other = (ValueExpression) sourceComponent;
+            if (this.get() == null) {
+                return (other.get() == null);
+            } else {
+                return (this.get().equals(other.get()));
+            }
+        }
+        return false;
+    }
+
+    @Override
+    public String toRubyString() {
+        if (value == null) {
+            return "null";
+        }
+        if (value instanceof String) {
+            return "'" + get() + "'";
+        }
+
+        return get().toString();
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/And.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/And.java
new file mode 100644
index 00000000000..434cea5c159
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/And.java
@@ -0,0 +1,20 @@
+package org.logstash.config.ir.expression.binary;
+
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.expression.BinaryBooleanExpression;
+import org.logstash.config.ir.expression.Expression;
+
+/**
+ * Created by andrewvc on 9/21/16.
+ */
+public class And extends BinaryBooleanExpression {
+    public And(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        super(meta, left, right);
+    }
+
+    @Override
+    public String rubyOperator() {
+        return "&&";
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Eq.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Eq.java
new file mode 100644
index 00000000000..3ddbd50d3d0
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Eq.java
@@ -0,0 +1,21 @@
+package org.logstash.config.ir.expression.binary;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.expression.BinaryBooleanExpression;
+import org.logstash.config.ir.expression.Expression;
+
+/**
+ * Created by andrewvc on 9/21/16.
+ */
+public class Eq extends BinaryBooleanExpression {
+    public Eq(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        super(meta, left, right);
+    }
+
+    @Override
+    public String rubyOperator() {
+        return "==";
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Gt.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Gt.java
new file mode 100644
index 00000000000..e611a771790
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Gt.java
@@ -0,0 +1,20 @@
+package org.logstash.config.ir.expression.binary;
+
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.expression.BinaryBooleanExpression;
+import org.logstash.config.ir.expression.Expression;
+
+/**
+ * Created by andrewvc on 9/21/16.
+ */
+public class Gt extends BinaryBooleanExpression {
+    public Gt(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        super(meta, left, right);
+    }
+
+    @Override
+    public String rubyOperator() {
+        return ">";
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Gte.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Gte.java
new file mode 100644
index 00000000000..3e9840c6eef
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Gte.java
@@ -0,0 +1,20 @@
+package org.logstash.config.ir.expression.binary;
+
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.expression.BinaryBooleanExpression;
+import org.logstash.config.ir.expression.Expression;
+
+/**
+ * Created by andrewvc on 9/21/16.
+ */
+public class Gte extends BinaryBooleanExpression {
+    public Gte(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        super(meta, left, right);
+    }
+
+    @Override
+    public String rubyOperator() {
+        return ">=";
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/In.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/In.java
new file mode 100644
index 00000000000..0a4302c4608
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/In.java
@@ -0,0 +1,20 @@
+package org.logstash.config.ir.expression.binary;
+
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.expression.BinaryBooleanExpression;
+import org.logstash.config.ir.expression.Expression;
+
+/**
+ * Created by andrewvc on 9/21/16.
+ */
+public class In extends BinaryBooleanExpression {
+    public In(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        super(meta, left, right);
+    }
+
+    @Override
+    public String rubyOperator() {
+        return ".include?";
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Lt.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Lt.java
new file mode 100644
index 00000000000..ea453b9a433
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Lt.java
@@ -0,0 +1,20 @@
+package org.logstash.config.ir.expression.binary;
+
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.expression.BinaryBooleanExpression;
+import org.logstash.config.ir.expression.Expression;
+
+/**
+ * Created by andrewvc on 9/21/16.
+ */
+public class Lt extends BinaryBooleanExpression {
+    public Lt(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        super(meta, left, right);
+    }
+
+    @Override
+    public String rubyOperator() {
+        return "<";
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Lte.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Lte.java
new file mode 100644
index 00000000000..0a165023ddc
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Lte.java
@@ -0,0 +1,20 @@
+package org.logstash.config.ir.expression.binary;
+
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.expression.BinaryBooleanExpression;
+import org.logstash.config.ir.expression.Expression;
+
+/**
+ * Created by andrewvc on 9/21/16.
+ */
+public class Lte extends BinaryBooleanExpression {
+    public Lte(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        super(meta, left, right);
+    }
+
+    @Override
+    public String rubyOperator() {
+        return "<=";
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Neq.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Neq.java
new file mode 100644
index 00000000000..5358a123948
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Neq.java
@@ -0,0 +1,20 @@
+package org.logstash.config.ir.expression.binary;
+
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.expression.BinaryBooleanExpression;
+import org.logstash.config.ir.expression.Expression;
+
+/**
+ * Created by andrewvc on 9/21/16.
+ */
+public class Neq extends BinaryBooleanExpression {
+    public Neq(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        super(meta, left, right);
+    }
+
+    @Override
+    public String rubyOperator() {
+        return "!=";
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Or.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Or.java
new file mode 100644
index 00000000000..290200e2bc2
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/Or.java
@@ -0,0 +1,20 @@
+package org.logstash.config.ir.expression.binary;
+
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.expression.BinaryBooleanExpression;
+import org.logstash.config.ir.expression.Expression;
+
+/**
+ * Created by andrewvc on 9/21/16.
+ */
+public class Or extends BinaryBooleanExpression {
+    public Or(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        super(meta, left, right);
+    }
+
+    @Override
+    public String rubyOperator() {
+        return "||";
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/RegexEq.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/RegexEq.java
new file mode 100644
index 00000000000..043d84a3ad3
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/binary/RegexEq.java
@@ -0,0 +1,26 @@
+package org.logstash.config.ir.expression.binary;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.expression.BinaryBooleanExpression;
+import org.logstash.config.ir.expression.Expression;
+import org.logstash.config.ir.expression.RegexValueExpression;
+
+/**
+ * Created by andrewvc on 9/21/16.
+ */
+public class RegexEq extends BinaryBooleanExpression {
+    public RegexEq(SourceMetadata meta, Expression left, Expression right) throws InvalidIRException {
+        super(meta, left, right);
+
+        if (!(right instanceof RegexValueExpression)) {
+            throw new InvalidIRException("You must use a regexp operator with a regexp rval!" + right);
+        }
+    }
+
+    @Override
+    public String rubyOperator() {
+        return "=~";
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/unary/Not.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/unary/Not.java
new file mode 100644
index 00000000000..968db43a5f2
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/unary/Not.java
@@ -0,0 +1,28 @@
+package org.logstash.config.ir.expression.unary;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.expression.Expression;
+import org.logstash.config.ir.expression.UnaryBooleanExpression;
+
+/**
+ * Created by andrewvc on 9/21/16.
+ */
+public class Not extends UnaryBooleanExpression {
+    public Not(SourceMetadata meta, Expression expression) throws InvalidIRException {
+        super(meta, expression);
+    }
+
+    @Override
+    public String toRubyString() {
+        return "!(" + getExpression().toRubyString() + ")";
+    }
+
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent sourceComponent) {
+        return sourceComponent != null &&
+                (sourceComponent instanceof Not &&
+                        ((Not) sourceComponent).getExpression().sourceComponentEquals(getExpression()));
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/expression/unary/Truthy.java b/logstash-core/src/main/java/org/logstash/config/ir/expression/unary/Truthy.java
new file mode 100644
index 00000000000..6ef1550fa9f
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/expression/unary/Truthy.java
@@ -0,0 +1,28 @@
+package org.logstash.config.ir.expression.unary;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.expression.Expression;
+import org.logstash.config.ir.expression.UnaryBooleanExpression;
+
+/**
+ * Created by andrewvc on 9/21/16.
+ */
+public class Truthy extends UnaryBooleanExpression {
+    public Truthy(SourceMetadata meta, Expression expression) throws InvalidIRException {
+        super(meta, expression);
+    }
+
+    @Override
+    public String toRubyString() {
+        return "!!(" + this.getExpression() + ")";
+    }
+
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent sourceComponent) {
+        return sourceComponent != null &&
+                sourceComponent instanceof Truthy &&
+                ((Truthy) sourceComponent).getExpression().sourceComponentEquals(this.getExpression());
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/graph/BooleanEdge.java b/logstash-core/src/main/java/org/logstash/config/ir/graph/BooleanEdge.java
new file mode 100644
index 00000000000..0cd31080d9b
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/graph/BooleanEdge.java
@@ -0,0 +1,63 @@
+package org.logstash.config.ir.graph;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.InvalidIRException;
+
+import java.util.Collection;
+
+/**
+ * Created by andrewvc on 9/15/16.
+ */
+public class BooleanEdge extends Edge {
+    public static class BooleanEdgeFactory extends EdgeFactory {
+        private final Boolean edgeType;
+
+        public BooleanEdgeFactory(Boolean edgeType) {
+            this.edgeType = edgeType;
+        }
+
+        public BooleanEdge make(Vertex in, Vertex out) throws InvalidIRException {
+            return new BooleanEdge(edgeType, in, out);
+        }
+    }
+    public static BooleanEdge.BooleanEdgeFactory trueFactory = new BooleanEdge.BooleanEdgeFactory(true);
+    public static BooleanEdge.BooleanEdgeFactory falseFactory = new BooleanEdge.BooleanEdgeFactory(false);
+
+    public static Collection<Edge> trueThreadVertices(Vertex... vertices) throws InvalidIRException {
+        return threadVertices(new BooleanEdgeFactory(true), vertices);
+    }
+
+    public static Collection<Edge> falseThreadVertices(Vertex... vertices) throws InvalidIRException {
+        return threadVertices(new BooleanEdgeFactory(false), vertices);
+    }
+
+    private final Boolean edgeType;
+
+    public Boolean getEdgeType() {
+        return edgeType;
+    }
+
+    public BooleanEdge(Boolean edgeType, Vertex outVertex, Vertex inVertex) throws InvalidIRException {
+        super(outVertex, inVertex);
+        this.edgeType = edgeType;
+    }
+
+    public String toString() {
+        return getFrom() + " -|" + this.edgeType + "|-> " + getTo();
+    }
+
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent sourceComponent) {
+        if (sourceComponent == null) return false;
+        if (sourceComponent == this) return true;
+        if (sourceComponent instanceof BooleanEdge) {
+            BooleanEdge otherE = (BooleanEdge) sourceComponent;
+
+            return this.getFrom().sourceComponentEquals(otherE.getFrom()) &&
+                    this.getTo().sourceComponentEquals(otherE.getTo()) &&
+                    this.getEdgeType().equals(otherE.getEdgeType());
+        }
+        return false;
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/graph/Edge.java b/logstash-core/src/main/java/org/logstash/config/ir/graph/Edge.java
new file mode 100644
index 00000000000..ae801aeb819
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/graph/Edge.java
@@ -0,0 +1,76 @@
+package org.logstash.config.ir.graph;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+/**
+ * Created by andrewvc on 9/15/16.
+ */
+public abstract class Edge implements ISourceComponent {
+    public static abstract class EdgeFactory {
+        public abstract Edge make(Vertex out, Vertex in) throws InvalidIRException;
+    }
+
+    private final Vertex to;
+    private final Vertex from;
+
+    public static Collection<Edge> threadVertices(EdgeFactory edgeFactory, Vertex... vertices) throws InvalidIRException {
+        Collection<Edge> edges = new ArrayList<>();
+
+        for (int i = 0; i < vertices.length-1; i++) {
+            Vertex from = vertices[i];
+            Vertex to = vertices[i+1];
+
+            Edge edge = edgeFactory.make(from, to);
+            to.addInEdge(edge);
+            from.addOutEdge(edge);
+            edges.add(edge);
+        }
+
+        return edges;
+    }
+
+    public Edge(Vertex from, Vertex to) throws InvalidIRException {
+        this.from = from;
+        this.to = to;
+
+        if (this.from == this.to) {
+            throw new InvalidIRException("Cannot create a cyclic vertex!" + to);
+        }
+    }
+
+    public Vertex getTo() {
+        return to;
+    }
+
+    public Vertex getFrom() {
+        return from;
+    }
+
+    public String toString() {
+        return getFrom() + " -> " + getTo();
+    }
+
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent sourceComponent) {
+        if (sourceComponent == null) return false;
+        if (sourceComponent == this) return true;
+        if (sourceComponent.getClass() == sourceComponent.getClass()) {
+            Edge otherE = (Edge) sourceComponent;
+
+            return this.getFrom().sourceComponentEquals(otherE.getFrom()) &&
+                    this.getTo().sourceComponentEquals(otherE.getTo());
+        }
+        return false;
+    }
+
+
+    @Override
+    public SourceMetadata getMeta() {
+        return null;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/graph/Graph.java b/logstash-core/src/main/java/org/logstash/config/ir/graph/Graph.java
new file mode 100644
index 00000000000..30692ff2772
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/graph/Graph.java
@@ -0,0 +1,345 @@
+package org.logstash.config.ir.graph;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.PluginDefinition;
+import org.logstash.config.ir.SourceMetadata;
+
+import java.util.*;
+import java.util.function.Consumer;
+import java.util.stream.Collectors;
+import java.util.stream.Stream;
+
+/**
+ * Created by andrewvc on 9/15/16.
+ */
+public class Graph implements ISourceComponent {
+    private final Set<Vertex> vertices = new HashSet<>();
+    private final Set<Edge> edges = new HashSet<>();
+
+    public Graph(Collection<Vertex> vertices, Collection<Edge> edges) throws InvalidIRException {
+        this.vertices.addAll(vertices);
+        this.edges.addAll(edges);
+        validate();
+    }
+
+    public Graph() {
+    }
+
+    public static Graph empty() {
+        return new Graph();
+    }
+
+    public Graph addVertex(Vertex v) {
+        this.vertices.add(v);
+        return this;
+    }
+
+    public void merge(Graph otherGraph) throws InvalidIRException {
+        this.vertices.addAll(otherGraph.getVertices());
+        this.edges.addAll(otherGraph.edges);
+        refresh();
+    }
+
+    /*
+      Attach another graph's nodes to this one by connection this graph's leaves to
+      the other graph's root
+    */
+    public Graph extend(Graph otherGraph) throws InvalidIRException {
+        if (otherGraph.getVertices().size() == 0) return this;
+
+        if (this.isEmpty()) {
+            this.merge(otherGraph);
+            return this;
+        }
+
+        for (Vertex otherRoot : otherGraph.getRoots()) {
+            extend(otherRoot);
+        }
+
+        return this;
+    }
+
+    public Graph extend(Vertex otherVertex) throws InvalidIRException {
+        for (Vertex leaf : this.getPartialLeaves()) {
+            for (Edge.EdgeFactory unusedEf : leaf.getUnusedOutgoingEdgeFactories()) {
+                this.threadVertices(unusedEf, leaf, otherVertex);
+            }
+        }
+        return this;
+    }
+
+    public Graph threadVertices(Edge.EdgeFactory edgeFactory, Vertex... argVertices) throws InvalidIRException {
+        Collection<Edge> newEdges = Edge.threadVertices(edgeFactory, argVertices);
+        addEdges(newEdges);
+
+        return this;
+    }
+
+    public Graph threadVertices(boolean bool, Vertex... vertices) throws InvalidIRException {
+        Edge.EdgeFactory factory = new BooleanEdge.BooleanEdgeFactory(bool);
+        return threadVertices(factory, vertices);
+    }
+
+    public Graph threadVertices(Vertex... vertices) throws InvalidIRException {
+        return threadVertices(new PlainEdge.PlainEdgeFactory(), vertices);
+    }
+
+    private void addEdge(Edge e) throws InvalidIRException {
+        this.getEdges().add(e);
+        refresh();
+    }
+
+    public void refresh() throws InvalidIRException {
+        walk(e -> {
+            this.edges.add(e);
+            this.vertices.add(e.getTo());
+            this.vertices.add(e.getFrom());
+        });
+
+        this.validate();
+    }
+
+    public void validate() throws InvalidIRException {
+        if (this.getVertices().stream().noneMatch(Vertex::isLeaf)) {
+            throw new InvalidIRException("Graph has no leaf vertices!" + this.toString());
+        }
+
+        this.getSortedVertices();
+    }
+
+    public void walk(Consumer<Edge> consumer) {
+        // avoid stream interface to avoid concurrency issues if a new root is added
+        for (Vertex root : this.getRoots()) {
+            walk(consumer, root);
+        }
+    }
+
+    private void walk(Consumer<Edge> consumer, Vertex vertex) {
+       vertex.outgoingEdges().forEach(e -> {
+           consumer.accept(e);
+           walk(consumer,e.getTo());
+       });
+    }
+
+    public Graph addEdges(Collection<Edge> edges) throws InvalidIRException {
+        this.edges.addAll(edges);
+
+        this.edges.stream().forEach(edge -> {
+            this.vertices.add(edge.getTo());
+            this.vertices.add(edge.getFrom());
+        });
+
+        refresh();
+
+        return this;
+    }
+
+    public Stream<Vertex> roots() {
+        return vertices.stream().filter(Vertex::isRoot);
+    }
+
+    public List<Vertex> getRoots() {
+        return roots().collect(Collectors.toList());
+    }
+
+    // Vertices which are partially leaves in that they support multiple
+    // outgoing edge types but only have one or fewer attached
+    public Stream<Vertex> partialLeaves() {
+        return vertices.stream().filter(Vertex::isPartialLeaf);
+    }
+
+    public Collection<Vertex> getPartialLeaves() {
+        return partialLeaves().collect(Collectors.toList());
+    }
+
+    public Stream<Vertex> leaves() {
+        return vertices.stream().filter(Vertex::isLeaf);
+    }
+
+    public Collection<Vertex> getLeaves() {
+        return leaves().collect(Collectors.toList());
+    }
+
+    public Set<Vertex> getVertices() {
+        return vertices;
+    }
+
+    public Set<Edge> getEdges() {
+        return edges;
+    }
+
+    public String toString() {
+        Stream<Edge> edgesToFormat;
+        try {
+            edgesToFormat = getSortedEdges().stream();
+        } catch (InvalidIRException e) {
+            edgesToFormat = edges.stream();
+        }
+
+        String edgelessVerticesStr;
+        if (this.isolatedVertices().count() > 0) {
+            edgelessVerticesStr = "\n== Vertices Without Edges ==\n" +
+                    this.isolatedVertices().map(Vertex::toString).collect(Collectors.joining("\n"));
+        } else {
+            edgelessVerticesStr = "";
+        }
+
+        return "<<< GRAPH >>>\n" +
+                edgesToFormat.map(Edge::toString).collect(Collectors.joining("\n")) +
+                edgelessVerticesStr +
+                "\n<<< /GRAPH >>>";
+    }
+
+    public Stream<Vertex> isolatedVertices() {
+        return this.getVertices().stream().filter(v -> v.getOutgoingEdges().isEmpty() && v.getIncomingEdges().isEmpty());
+    }
+
+    // Uses Kahn's algorithm to do a topological sort and detect cycles
+    public List<Vertex> getSortedVertices() throws InvalidIRException {
+        if (this.edges.size() == 0) return new ArrayList(this.vertices);
+
+        List<Vertex> sorted = new ArrayList<>(this.vertices.size());
+
+        Deque<Vertex> pending = new LinkedList<>();
+        pending.addAll(this.getRoots());
+
+        Set<Edge> traversedEdges = new HashSet<>();
+
+        while (!pending.isEmpty()) {
+            Vertex currentVertex = pending.removeFirst();
+            sorted.add(currentVertex);
+
+            currentVertex.getOutgoingEdges().forEach(edge -> {
+                traversedEdges.add(edge);
+                Vertex toVertex = edge.getTo();
+                if (toVertex.getIncomingEdges().stream().allMatch(traversedEdges::contains)) {
+                    pending.add(toVertex);
+                }
+            });
+        }
+
+        // Check for cycles
+        if (this.edges.stream().noneMatch(traversedEdges::contains)) {
+            throw new InvalidIRException("Graph has cycles, is not a DAG! " + this.edges);
+        }
+
+        return sorted;
+    }
+
+    public List<Edge> getSortedEdges() throws InvalidIRException {
+        return getSortedVertices().stream().
+                flatMap(Vertex::outgoingEdges).
+                collect(Collectors.toList());
+    }
+
+    public List<Vertex> getSortedVerticesBefore(Vertex end) throws InvalidIRException {
+        return getSortedVerticesBetween(null, end);
+    }
+
+    public List<Vertex> getSortedVerticesAfter(Vertex start) throws InvalidIRException {
+        return getSortedVerticesBetween(start, null);
+    }
+
+    public List<Vertex> getSortedVerticesBetween(Vertex start, Vertex end) throws InvalidIRException {
+        List<Vertex> sortedVertices = getSortedVertices();
+
+        int startIndex = start == null ? 0 : sortedVertices.indexOf(start);
+        int endIndex = end == null ? sortedVertices.size() : sortedVertices.indexOf(end);
+
+        return sortedVertices.subList(startIndex+1, endIndex);
+    }
+
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent sourceComponent) {
+        if (sourceComponent == this) return true;
+        if (sourceComponent instanceof Graph) {
+            Graph otherG = (Graph) sourceComponent;
+            if (otherG.getVertices().size() != this.getVertices().size()) return false;
+
+            boolean edgesEqual = this.getEdges().stream().
+                    allMatch(e -> otherG.getEdges().stream().anyMatch(oe -> oe.sourceComponentEquals(e)));
+
+            // We need to check vertices separately because there may be unconnected vertices
+            boolean verticesEqual = this.getVertices().stream().
+                    allMatch(v -> otherG.getVertices().stream().anyMatch(ov -> ov.sourceComponentEquals(v)));
+
+            return edgesEqual && verticesEqual;
+        }
+        return false;
+    }
+
+    // returns true if this graph has a .sourceComponentEquals equivalent edge
+    public boolean hasEquivalentEdge(Edge otherE) {
+        return this.getEdges().stream().anyMatch(e -> e.sourceComponentEquals(otherE));
+    }
+
+
+    public class DiffResult {
+        public Collection<Edge> getRemovedEdges() {
+            return removedEdges;
+        }
+
+        public Collection<Edge> getAddedEdges() {
+            return addedEdges;
+        }
+
+        private final Collection<Edge> removedEdges;
+        private final Collection<Edge> addedEdges;
+
+        public DiffResult(Collection<Edge> removed, Collection<Edge> added) {
+            this.removedEdges = removed;
+            this.addedEdges = added;
+        }
+
+        public String toString() {
+            return "Diff Result (-" + removedEdges.size() + ",+" + addedEdges.size() + ")\n" +
+                    removedEdges.stream().map(e -> "-" + e.toString()).collect(Collectors.joining("\n")) +
+                    "\n" +
+                    addedEdges.stream().map(e -> "+" + e.toString()).collect(Collectors.joining("\n"));
+        }
+    }
+
+    public DiffResult diff(Graph o) {
+       List<Edge> removedEdges = this.getEdges().stream().filter(e -> !o.hasEquivalentEdge(e)).collect(Collectors.toList());
+       List<Edge> addedEdges = o.getEdges().stream().filter(e -> !o.hasEquivalentEdge(e)).collect(Collectors.toList());
+        return new DiffResult(removedEdges, addedEdges);
+    }
+
+    @Override
+    public SourceMetadata getMeta() {
+        return null;
+    }
+
+    public boolean isEmpty() {
+        return (this.getVertices().size() == 0);
+    }
+
+    public Graph threadToGraph(BooleanEdge.BooleanEdgeFactory edgeFactory, Vertex v, Graph otherGraph) throws InvalidIRException {
+        if (otherGraph.getVertices().size() == 0) return this;
+
+        for (Vertex otherRoot : otherGraph.getRoots()) {
+            this.threadVertices(edgeFactory, v, otherRoot);
+        }
+
+        return this;
+    }
+
+    // Return plugin vertices by type
+    public List<PluginVertex> getPluginVertices(PluginDefinition.Type type) {
+       return pluginVertices()
+               .filter(v -> v.getPluginDefinition().getType().equals(type))
+               .collect(Collectors.toList());
+    }
+
+    public List<PluginVertex> getPluginVertices() {
+        return pluginVertices().collect(Collectors.toList());
+    }
+
+    public Stream<PluginVertex> pluginVertices() {
+        return this.vertices.stream()
+               .filter(v -> v instanceof PluginVertex)
+               .map(v -> (PluginVertex) v);
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/graph/IfVertex.java b/logstash-core/src/main/java/org/logstash/config/ir/graph/IfVertex.java
new file mode 100644
index 00000000000..ffe49078bad
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/graph/IfVertex.java
@@ -0,0 +1,69 @@
+package org.logstash.config.ir.graph;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.expression.BooleanExpression;
+
+import java.util.*;
+import java.util.stream.Collectors;
+
+/**
+ * Created by andrewvc on 9/15/16.
+ */
+public class IfVertex extends Vertex {
+
+    public BooleanExpression getBooleanExpression() {
+        return booleanExpression;
+    }
+
+    private final BooleanExpression booleanExpression;
+
+    public IfVertex(SourceMetadata meta, BooleanExpression booleanExpression) {
+        super(meta);
+        this.booleanExpression = booleanExpression;
+    }
+
+    public String toString() {
+        return "[if " + booleanExpression.toString(0) + "]";
+    }
+
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent other) {
+        if (other == null) return false;
+        if (other == this) return true;
+        if (other instanceof IfVertex) {
+            IfVertex otherV = (IfVertex) other;
+            // We don't check the ID because that's randomly generated
+            return otherV.booleanExpression.sourceComponentEquals(this.booleanExpression);
+        }
+        return false;
+    }
+
+    public boolean hasEdgeType(boolean type) {
+        for (Edge e : getOutgoingEdges()) {
+            BooleanEdge bEdge = (BooleanEdge) e; // There should only  be boolean edges here!
+            if (bEdge.getEdgeType() == type) return true;
+        }
+        return false;
+    }
+
+    public Collection<Edge.EdgeFactory> getUnusedOutgoingEdgeFactories() {
+        List<Edge.EdgeFactory> l = new LinkedList<>();
+        if (!hasEdgeType(true)) l.add(BooleanEdge.trueFactory);
+        if (!hasEdgeType(false)) l.add(BooleanEdge.falseFactory);
+        return l;
+    }
+
+    public boolean acceptsOutgoingEdge(Edge e) {
+        return (e instanceof BooleanEdge);
+    }
+
+    public List<BooleanEdge> getOutgoingBooleanEdges() {
+        // Wish there was a way to do this as a java a cast without an operation
+        return getOutgoingEdges().stream().map(e -> (BooleanEdge) e).collect(Collectors.toList());
+    }
+
+    public List<BooleanEdge> getOutgoingBooleanEdgesByType(Boolean edgeType) {
+        return getOutgoingBooleanEdges().stream().filter(e -> e.getEdgeType().equals(edgeType)).collect(Collectors.toList());
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/graph/PlainEdge.java b/logstash-core/src/main/java/org/logstash/config/ir/graph/PlainEdge.java
new file mode 100644
index 00000000000..8237ceb8fc2
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/graph/PlainEdge.java
@@ -0,0 +1,23 @@
+package org.logstash.config.ir.graph;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.InvalidIRException;
+
+/**
+ * Created by andrewvc on 9/19/16.
+ */
+public class PlainEdge extends Edge {
+    public static class PlainEdgeFactory extends Edge.EdgeFactory {
+        @Override
+        public Edge make(Vertex from, Vertex to) throws InvalidIRException {
+           return new PlainEdge(from, to);
+        }
+    }
+
+    public PlainEdge(Vertex from, Vertex to) throws InvalidIRException {
+        super(from, to);
+    }
+
+
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/graph/PluginVertex.java b/logstash-core/src/main/java/org/logstash/config/ir/graph/PluginVertex.java
new file mode 100644
index 00000000000..3d67670be83
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/graph/PluginVertex.java
@@ -0,0 +1,60 @@
+package org.logstash.config.ir.graph;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.PluginDefinition;
+import org.logstash.config.ir.SourceMetadata;
+
+import java.util.Map;
+import java.util.Objects;
+import java.util.UUID;
+
+/**
+ * Created by andrewvc on 9/15/16.
+ */
+public class PluginVertex extends Vertex {
+    private final SourceMetadata meta;
+
+    public String getId() {
+        return id;
+    }
+
+    @Override
+    public SourceMetadata getMeta() {
+        return meta;
+    }
+
+    private final String id;
+
+    public PluginDefinition getPluginDefinition() {
+        return pluginDefinition;
+    }
+
+    private final PluginDefinition pluginDefinition;
+
+    public PluginVertex(SourceMetadata meta, PluginDefinition pluginDefinition) {
+        super(meta);
+        this.meta = meta;
+
+        this.pluginDefinition = pluginDefinition;
+
+        Object argId = this.pluginDefinition.getArguments().get("id");
+        this.id = argId != null ? argId.toString() : UUID.randomUUID().toString();
+        this.pluginDefinition.getArguments().put("id", this.id);
+    }
+
+    public String toString() {
+        return "P[" + pluginDefinition + "]";
+    }
+
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent other) {
+        if (other == null) return false;
+        if (other == this) return true;
+        if (other instanceof PluginVertex) {
+            PluginVertex otherV = (PluginVertex) other;
+            // We don't check the ID because that's randomly generated
+            return otherV.getPluginDefinition().equals(this.getPluginDefinition());
+        }
+        return false;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/graph/SpecialVertex.java b/logstash-core/src/main/java/org/logstash/config/ir/graph/SpecialVertex.java
new file mode 100644
index 00000000000..0986ac15a20
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/graph/SpecialVertex.java
@@ -0,0 +1,54 @@
+package org.logstash.config.ir.graph;
+
+import org.logstash.config.ir.ISourceComponent;
+
+/**
+ * Created by andrewvc on 9/15/16.
+ */
+public class SpecialVertex extends Vertex {
+    private final Type type;
+
+    public SpecialVertex() {
+        super(null);
+        this.type = Type.QUEUE;
+    }
+
+    public SpecialVertex(Type type) {
+        super(null);
+        this.type = type;
+
+    }
+
+    public enum Type {
+        FILTER_IN ("FILTER_IN"),
+        FILTER_OUT ("FILTER OUT"),
+        OUTPUT_IN ("OUTPUT IN"),
+        OUTPUT_OUT ("OUTPUT OUT"),
+        QUEUE ("QUEUE");
+
+        private final String name;
+
+        Type(String s) {
+            this.name = s;
+        }
+
+        public String toString() {
+            return this.name;
+        }
+    }
+
+    public String toString() {
+        return "S[" + this.type + "]";
+    }
+
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent other) {
+        if (other == null) return false;
+        if (other == this) return true;
+        if (other instanceof SpecialVertex) {
+            SpecialVertex otherV = (SpecialVertex) other;
+            return otherV.type.equals(this.type);
+        }
+        return false;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/graph/Vertex.java b/logstash-core/src/main/java/org/logstash/config/ir/graph/Vertex.java
new file mode 100644
index 00000000000..a6e7b51dee8
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/graph/Vertex.java
@@ -0,0 +1,104 @@
+package org.logstash.config.ir.graph;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+
+import java.util.*;
+import java.util.stream.Stream;
+
+/**
+ * Created by andrewvc on 9/15/16.
+ */
+public abstract class Vertex implements ISourceComponent {
+    private final Collection<Edge> incoming = new HashSet<Edge>();
+    private final Collection<Edge> outgoingEdges = new HashSet<Edge>();
+    private final SourceMetadata sourceMetadata;
+
+    public Vertex() {
+        this.sourceMetadata = null;
+    }
+
+    public Vertex(SourceMetadata sourceMetadata) {
+        this.sourceMetadata = sourceMetadata;
+    }
+
+    public Vertex(Collection<Edge> incoming, Collection<Edge> outgoingEdges, SourceMetadata sourceMetadata) {
+        this.sourceMetadata = sourceMetadata;
+        this.incoming.addAll(incoming);
+        this.outgoingEdges.addAll(outgoingEdges);
+    }
+
+    public Vertex addInEdge(Edge e) throws InvalidIRException {
+        if (!this.acceptsIncomingEdge(e)) throw new InvalidIRException("Invalid incoming edge!" + e + " for " + this);
+        this.incoming.add(e);
+        return this;
+    }
+
+    public Vertex addOutEdge(Edge e) throws InvalidIRException {
+        if (!this.acceptsOutgoingEdge(e)) {
+            throw new InvalidIRException(
+                "Invalid outgoing edge!" +
+                e + " for " + this +
+                " existing outgoing edges: " + this.getOutgoingEdges());
+        }
+        this.outgoingEdges.add(e);
+        return this;
+    }
+
+    public boolean isRoot() {
+        return incoming.size() == 0;
+    }
+
+    public boolean isLeaf() {
+        return outgoingEdges.size() == 0;
+    }
+
+    public boolean hasIncomingEdges() {
+        return incoming.size() > 0;
+    }
+
+    public boolean hasOutgoingEdges() {
+        return outgoingEdges.size() > 0;
+    }
+
+    public Collection<Edge> getIncomingEdges() {
+        return incoming;
+    }
+
+    public Collection<Edge> getOutgoingEdges() {
+        return outgoingEdges;
+    }
+
+    public Stream<Edge> incomingEdges() {
+        return getIncomingEdges().stream();
+    }
+
+    public Stream<Edge> outgoingEdges() {
+        return outgoingEdges.stream();
+    }
+
+    @Override
+    public SourceMetadata getMeta() {
+        return null;
+    }
+
+    public Collection<Edge.EdgeFactory> getUnusedOutgoingEdgeFactories() {
+       if (!this.hasOutgoingEdges()) {
+           return Collections.singletonList(new PlainEdge.PlainEdgeFactory());
+       }
+       return Collections.emptyList();
+    }
+
+    public boolean isPartialLeaf() {
+       return getUnusedOutgoingEdgeFactories().size() > 0;
+    }
+
+    public boolean acceptsIncomingEdge(Edge e) {
+        return true;
+    }
+
+    public boolean acceptsOutgoingEdge(Edge e) {
+        return true;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/imperative/ComposedParallelStatement.java b/logstash-core/src/main/java/org/logstash/config/ir/imperative/ComposedParallelStatement.java
new file mode 100644
index 00000000000..7c504a696e3
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/imperative/ComposedParallelStatement.java
@@ -0,0 +1,32 @@
+package org.logstash.config.ir.imperative;
+
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.graph.Graph;
+
+import java.util.List;
+
+/**
+ * Created by andrewvc on 9/22/16.
+ */
+public class ComposedParallelStatement extends ComposedStatement {
+    public ComposedParallelStatement(SourceMetadata meta, List<Statement> statements) throws InvalidIRException {
+        super(meta, statements);
+    }
+
+    @Override
+    protected String composeTypeString() {
+        return "composed-parallel";
+    }
+
+    @Override
+    public Graph toGraph() throws InvalidIRException {
+        Graph g = Graph.empty();
+
+        for (Statement s : getStatements()) {
+            g.merge(s.toGraph());
+        }
+
+        return g;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/imperative/ComposedSequenceStatement.java b/logstash-core/src/main/java/org/logstash/config/ir/imperative/ComposedSequenceStatement.java
new file mode 100644
index 00000000000..69dad74d277
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/imperative/ComposedSequenceStatement.java
@@ -0,0 +1,34 @@
+package org.logstash.config.ir.imperative;
+
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.graph.Graph;
+
+import java.util.List;
+import java.util.stream.Collectors;
+
+/**
+ * Created by andrewvc on 9/22/16.
+ */
+public class ComposedSequenceStatement extends ComposedStatement {
+    public ComposedSequenceStatement(SourceMetadata meta, List<Statement> statements) throws InvalidIRException {
+        super(meta, statements);
+    }
+
+    @Override
+    protected String composeTypeString() {
+        return "do-sequence";
+    }
+
+    @Override
+    public Graph toGraph() throws InvalidIRException {
+        Graph g = Graph.empty();
+
+        for (Statement statement : getStatements()) {
+            Graph sg = statement.toGraph();
+            g.extend(sg);
+        }
+
+        return g;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/imperative/ComposedStatement.java b/logstash-core/src/main/java/org/logstash/config/ir/imperative/ComposedStatement.java
new file mode 100644
index 00000000000..3b1ee61822f
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/imperative/ComposedStatement.java
@@ -0,0 +1,67 @@
+package org.logstash.config.ir.imperative;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.graph.Graph;
+import org.logstash.config.ir.graph.Vertex;
+
+import java.util.*;
+import java.util.stream.Collectors;
+
+/**
+ * Created by andrewvc on 9/6/16.
+ */
+public abstract class ComposedStatement extends Statement {
+    public interface IFactory {
+        ComposedStatement make(SourceMetadata meta, List<Statement> statements) throws InvalidIRException;
+    }
+
+    private final List<Statement> statements;
+
+    public ComposedStatement(SourceMetadata meta, List<Statement> statements) throws InvalidIRException {
+        super(meta);
+        if (statements == null || statements.stream().anyMatch(s -> s == null)) {
+            throw new InvalidIRException("Nulls eNot allowed for list eOr in statement list");
+        }
+        this.statements = statements;
+    }
+
+    public List<Statement> getStatements() {
+        return this.statements;
+    }
+
+    public int size() {
+        return getStatements().size();
+    }
+
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent sourceComponent) {
+        if (sourceComponent == null) return false;
+        if (this == sourceComponent) return true;
+        if (sourceComponent.getClass().equals(this.getClass())) {
+            ComposedStatement other = (ComposedStatement) sourceComponent;
+            if (this.size() != other.size()) {
+                return false;
+            }
+            for (int i = 0; i < size(); i++) {
+                Statement s = this.getStatements().get(i);
+                Statement os = other.getStatements().get(i);
+                if (!(s.sourceComponentEquals(os))) return false;
+            }
+            return true;
+        }
+        return false;
+    }
+
+    @Override
+    public String toString(int indent) {
+        return "(" + this.composeTypeString() + "\n" +
+                getStatements().stream().
+                  map(s -> s.toString(indent+2)).
+                  collect(Collectors.joining("\n")) +
+                "\n";
+    }
+
+    protected abstract String composeTypeString();
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/imperative/IfStatement.java b/logstash-core/src/main/java/org/logstash/config/ir/imperative/IfStatement.java
new file mode 100644
index 00000000000..31bf6a9a19c
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/imperative/IfStatement.java
@@ -0,0 +1,99 @@
+package org.logstash.config.ir.imperative;
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.expression.BooleanExpression;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.graph.*;
+
+import java.util.List;
+import java.util.Optional;
+import java.util.stream.Collectors;
+
+/**
+ * Created by andrewvc on 9/6/16.
+ * if 5 {
+ *
+ * }
+ */
+
+public class IfStatement extends Statement {
+    private final BooleanExpression booleanExpression;
+    private final Statement trueStatement;
+    private final Statement falseStatement;
+
+    public BooleanExpression getBooleanExpression() {
+        return booleanExpression;
+    }
+
+    public Statement getTrueStatement() {
+        return trueStatement;
+    }
+
+    public Statement getFalseStatement() {
+        return falseStatement;
+    }
+
+    public IfStatement(SourceMetadata meta,
+                       BooleanExpression booleanExpression,
+                       Statement trueStatement,
+                       Statement falseStatement
+    ) throws InvalidIRException {
+        super(meta);
+
+        if (booleanExpression == null) throw new InvalidIRException("Boolean expr must eNot be null!");
+        if (trueStatement == null) throw new InvalidIRException("If Statement needs true statement!");
+        if (falseStatement == null) throw new InvalidIRException("If Statement needs false statement!");
+
+        this.booleanExpression = booleanExpression;
+        this.trueStatement = trueStatement;
+        this.falseStatement = falseStatement;
+    }
+
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent sourceComponent) {
+        if (sourceComponent == null) return false;
+        if (sourceComponent == this) return true;
+        if (sourceComponent instanceof IfStatement) {
+            IfStatement other = (IfStatement) sourceComponent;
+
+
+            return (this.booleanExpression.sourceComponentEquals(other.getBooleanExpression()) &&
+                    this.trueStatement.sourceComponentEquals(other.trueStatement) &&
+                    this.falseStatement.sourceComponentEquals(other.falseStatement));
+        }
+        return false;
+    }
+
+    @Override
+    public String toString(int indent) {
+        return indentPadding(indent) +
+                    "(if " + booleanExpression.toString(0) +
+                    "\n" +
+                    this.trueStatement +
+                    "\n" +
+                    this.falseStatement +
+                    ")";
+    }
+
+
+    @Override
+    public Graph toGraph() throws InvalidIRException {
+        Graph graph = new Graph();
+        Vertex ifVertex = new IfVertex(this.getMeta(), this.booleanExpression);
+        graph.addVertex(ifVertex);
+
+        if (!(getTrueStatement() instanceof NoopStatement)) {
+            Statement ts = this.getTrueStatement();
+            Graph tsg = ts.toGraph();
+            graph.threadToGraph(BooleanEdge.trueFactory, ifVertex, tsg);
+        }
+
+        if (!(getFalseStatement() instanceof NoopStatement)) {
+            Statement fs = this.getFalseStatement();
+            Graph fsg = fs.toGraph();
+            graph.threadToGraph(BooleanEdge.falseFactory, ifVertex, fsg);
+        }
+
+        return graph;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/imperative/NoopStatement.java b/logstash-core/src/main/java/org/logstash/config/ir/imperative/NoopStatement.java
new file mode 100644
index 00000000000..ac3d26997fa
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/imperative/NoopStatement.java
@@ -0,0 +1,33 @@
+package org.logstash.config.ir.imperative;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.graph.Graph;
+
+/**
+ * Created by andrewvc on 9/15/16.
+ */
+public class NoopStatement extends Statement {
+
+    public NoopStatement(SourceMetadata meta) {
+        super(meta);
+    }
+
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent sourceComponent) {
+        if (sourceComponent == null) return false;
+        if (sourceComponent instanceof NoopStatement) return true;
+        return false;
+    }
+
+    @Override
+    public String toString(int indent) {
+        return indentPadding(indent) + "(Noop)";
+    }
+
+    @Override
+    public Graph toGraph() {
+        return Graph.empty();
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/imperative/PluginStatement.java b/logstash-core/src/main/java/org/logstash/config/ir/imperative/PluginStatement.java
new file mode 100644
index 00000000000..9b2542f9184
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/imperative/PluginStatement.java
@@ -0,0 +1,44 @@
+package org.logstash.config.ir.imperative;
+
+import org.logstash.config.ir.ISourceComponent;
+import org.logstash.config.ir.PluginDefinition;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.graph.Graph;
+import org.logstash.config.ir.graph.PluginVertex;
+import org.logstash.config.ir.graph.Vertex;
+
+import java.util.Map;
+
+/**
+ * Created by andrewvc on 9/6/16.
+ */
+public class PluginStatement extends Statement {
+    private final PluginDefinition pluginDefinition;
+
+    public PluginStatement(SourceMetadata meta, PluginDefinition pluginDefinition) {
+        super(meta);
+        this.pluginDefinition = pluginDefinition;
+    }
+
+    @Override
+    public boolean sourceComponentEquals(ISourceComponent sourceComponent) {
+        if (sourceComponent == null) return false;
+        if (sourceComponent == this) return true;
+        if (sourceComponent instanceof PluginStatement) {
+            PluginStatement other = (PluginStatement) sourceComponent;
+            return this.pluginDefinition.equals(other.pluginDefinition);
+        }
+        return false;
+    }
+
+    @Override
+    public String toString(int indent) {
+        return indentPadding(indent) + this.pluginDefinition;
+    }
+
+    @Override
+    public Graph toGraph() {
+        Vertex pluginVertex = new PluginVertex(getMeta(), pluginDefinition);
+        return Graph.empty().addVertex(pluginVertex);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/imperative/Statement.java b/logstash-core/src/main/java/org/logstash/config/ir/imperative/Statement.java
new file mode 100644
index 00000000000..1051ff77acb
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/ir/imperative/Statement.java
@@ -0,0 +1,31 @@
+package org.logstash.config.ir.imperative;
+
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceComponent;
+import org.logstash.config.ir.SourceMetadata;
+import org.logstash.config.ir.graph.Edge;
+import org.logstash.config.ir.graph.Graph;
+import org.logstash.config.ir.graph.Vertex;
+
+import java.util.Collection;
+
+/**
+ * Created by andrewvc on 9/6/16.
+ */
+public abstract class Statement extends SourceComponent {
+    public Statement(SourceMetadata meta) {
+        super(meta);
+    }
+
+    public abstract Graph toGraph() throws InvalidIRException;
+
+    public String toString() {
+        return toString(2);
+    }
+
+    public abstract String toString(int indent);
+
+    public String indentPadding(int length) {
+        return new String(new char[length]).replace("\0", " ");
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/pipeline/IPipelineTransferer.java b/logstash-core/src/main/java/org/logstash/config/pipeline/IPipelineTransferer.java
new file mode 100644
index 00000000000..108179d5128
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/pipeline/IPipelineTransferer.java
@@ -0,0 +1,14 @@
+package org.logstash.config.pipeline;
+
+import org.logstash.config.pipeline.pipette.IPipetteConsumer;
+import org.logstash.config.pipeline.pipette.IPipetteProducer;
+
+/**
+ * Created by andrewvc on 10/18/16.
+ */
+public interface IPipelineTransferer {
+    void start();
+    IPipetteConsumer makeConsumer();
+    IPipetteProducer makeProducer();
+    void stop();
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/pipeline/PassthroughProcessor.java b/logstash-core/src/main/java/org/logstash/config/pipeline/PassthroughProcessor.java
new file mode 100644
index 00000000000..f8c7d83e9fd
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/pipeline/PassthroughProcessor.java
@@ -0,0 +1,40 @@
+package org.logstash.config.pipeline;
+
+import org.logstash.Event;
+import org.logstash.config.compiler.compiled.ICompiledProcessor;
+import org.logstash.config.ir.graph.Edge;
+import org.logstash.config.ir.graph.Vertex;
+
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Created by andrewvc on 9/23/16.
+ */
+public class PassthroughProcessor implements ICompiledProcessor {
+    private final Vertex vertex;
+
+    public PassthroughProcessor(Vertex vertex) {
+        this.vertex = vertex;
+    }
+
+    @Override
+    public Map<Edge, List<Event>> process(List<Event> events) {
+        HashMap<Edge, List<Event>> out = new HashMap<>(vertex.getOutgoingEdges().size());
+        for (Edge e : vertex.getOutgoingEdges()) {
+            out.put(e, events);
+        }
+        return out;
+    }
+
+    @Override
+    public void register() {
+
+    }
+
+    @Override
+    public void stop() {
+
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/pipeline/Pipeline.java b/logstash-core/src/main/java/org/logstash/config/pipeline/Pipeline.java
new file mode 100644
index 00000000000..8ebda9630b5
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/pipeline/Pipeline.java
@@ -0,0 +1,90 @@
+package org.logstash.config.pipeline;
+
+import org.logstash.config.ir.DSL;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.PluginDefinition;
+import org.logstash.config.ir.graph.Graph;
+import org.logstash.config.ir.graph.PluginVertex;
+import org.logstash.config.ir.graph.SpecialVertex;
+import org.logstash.config.ir.graph.Vertex;
+
+import java.util.List;
+
+/**
+ * Created by andrewvc on 9/20/16.
+ */
+public class Pipeline {
+    public Graph getGraph() {
+        return graph;
+    }
+
+    public SpecialVertex getQueue() {
+        return queue;
+    }
+
+    public SpecialVertex getFilterOut() {
+        return filterOut;
+    }
+
+    private final Graph graph;
+    private final SpecialVertex queue;
+    private final SpecialVertex filterOut;
+
+    public Pipeline(Graph inputSection, Graph filterSection, Graph outputSection) throws InvalidIRException {
+        graph = DSL.graph();
+
+        // We don't really care about the edges in the input section, we just want the vertices as roots
+        for (Vertex inV : inputSection.getVertices()) {
+            if (inV instanceof PluginVertex) {
+                graph.addVertex(inV);
+            } else {
+                throw new InvalidIRException("Only plugin vertices are allowed in input sections!");
+            }
+        }
+
+        // Connect all the input vertices out to the queue
+        queue = new SpecialVertex(SpecialVertex.Type.QUEUE);
+        graph.extend(queue);
+
+        // Now we connect the queue to the root of the filter section
+        graph.extend(filterSection);
+
+        // Now we connect the leaves (and partial leaves) of the graph
+        // which should all be filters (unless no filters are defined)
+        // to the special filterOut node
+        filterOut = new SpecialVertex(SpecialVertex.Type.FILTER_OUT);
+        graph.extend(filterOut);
+
+        // Finally, connect the filter out node to all the outputs
+        graph.extend(outputSection);
+    }
+
+    public List<Vertex> getPostQueue() throws InvalidIRException {
+    return graph.getSortedVerticesAfter(queue);
+}
+
+    public List<PluginVertex> getInputPluginVertices() {
+        return graph.getPluginVertices(PluginDefinition.Type.INPUT);
+    }
+
+    public List<PluginVertex> getFilterPluginVertices() {
+        return graph.getPluginVertices(PluginDefinition.Type.FILTER);
+    }
+
+    public List<PluginVertex> getOutputPluginVertices() {
+        return graph.getPluginVertices(PluginDefinition.Type.OUTPUT);
+    }
+
+    @Override
+    public String toString() {
+        String summary = String.format("[Pipeline] Inputs: %d Filters: %d Outputs %d",
+                getInputPluginVertices().size(),
+                getFilterPluginVertices().size(),
+                getOutputPluginVertices().size());
+        return summary + "\n" + graph.toString();
+    }
+
+    public String toRubyString() {
+        return "";
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/pipeline/PipelineRunner.java b/logstash-core/src/main/java/org/logstash/config/pipeline/PipelineRunner.java
new file mode 100644
index 00000000000..ddee4145147
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/pipeline/PipelineRunner.java
@@ -0,0 +1,232 @@
+package org.logstash.config.pipeline;
+
+import org.logstash.Event;
+import org.logstash.config.compiler.*;
+import org.logstash.config.compiler.compiled.ICompiledInputPlugin;
+import org.logstash.config.compiler.compiled.ICompiledProcessor;
+import org.logstash.config.pipeline.pipette.*;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.PluginDefinition;
+import org.logstash.config.ir.graph.IfVertex;
+import org.logstash.config.ir.graph.PluginVertex;
+import org.logstash.config.ir.graph.SpecialVertex;
+import org.logstash.config.ir.graph.Vertex;
+
+import java.util.*;
+import java.util.function.Function;
+import java.util.stream.Collectors;
+
+/**
+ * Created by andrewvc on 9/22/16.
+ */
+public class PipelineRunner {
+    private final Pipeline pipeline;
+    private final IPluginCompiler pluginCompiler;
+    private final IExpressionCompiler expressionCompiler;
+    private final HashMap<PluginVertex, ICompiledInputPlugin> inputVerticesToCompiled;
+    private final Collection<Pipette> inputPipettes;
+    private final HashMap<Pipette, Thread> pipettesThreads;
+    private final Map<Vertex, ICompiledProcessor> processorVerticesToCompiled;
+    private final List<Vertex> orderedPostQueue;
+    private final Collection<Pipette> processorPipettes;
+    private final IfCompiler ifCompiler;
+    private final PipelineRunnerObserver observer;
+    private final IPipelineTransferer pipelineTransferer;
+
+    public PipelineRunner(Pipeline pipeline, IPipelineTransferer pipelineTransferer, IExpressionCompiler expressionCompiler, IPluginCompiler pluginCompiler, PipelineRunnerObserver observer) throws CompilationError, InvalidIRException {
+        this.pipeline = pipeline;
+        this.expressionCompiler = expressionCompiler;
+        this.observer = observer != null ? observer : new PipelineRunnerObserver();
+        this.ifCompiler = new IfCompiler(expressionCompiler);
+        this.pluginCompiler = pluginCompiler;
+        this.pipettesThreads = new HashMap<>();
+        this.pipelineTransferer = pipelineTransferer;
+
+        // Handle stuff before the queue, e.g. inputs
+        this.inputVerticesToCompiled = compileInputs();
+        this.inputPipettes = new ArrayList<>();
+
+        // Handle stuff after the queue, e.g. processors
+        this.orderedPostQueue = pipeline.getPostQueue();
+        this.processorVerticesToCompiled = this.compileProcessors();
+        this.processorPipettes = new ArrayList<>();
+        this.observer.initialize(this);
+    }
+
+    public void start(int workers) throws PipetteExecutionException, CompilationError {
+        pipelineTransferer.start();
+        observer.beforeStart(this);
+        startInputs();
+        startProcessors(workers);
+        observer.afterStart(this);
+    }
+
+    public void join() throws InterruptedException, PipetteExecutionException {
+        // Join on input threads
+        for (Map.Entry<Pipette,Thread> entry : pipettesThreads(inputPipettes).entrySet()) {
+            entry.getValue().join();
+            System.out.println("Input thread finished: " + entry.getKey());
+        }
+
+        // Join on processors / close them out
+        for (Map.Entry<Pipette,Thread> entry : pipettesThreads(processorPipettes).entrySet()) {
+            Pipette pipette = entry.getKey();
+            Thread thread = entry.getValue();
+
+            pipette.stop();
+            thread.join();
+
+            System.out.println("Processor thread finished: " + pipette);
+        }
+
+    }
+
+    public Map<Pipette,Thread> pipettesThreads(Collection<Pipette> pipettes) {
+        return pipettes.stream().collect(Collectors.toMap(Function.identity(), pipettesThreads::get));
+    }
+
+    public void stop(int workers) throws PipetteExecutionException {
+        observer.beforeStop(this);
+        stopInputs();
+        observer.inputsStopped(this);
+
+        pipelineTransferer.stop();
+
+        stopProcessors();
+        observer.afterStop(this);
+    }
+
+    private Pipette createProcessorPipette(String name) {
+        OrderedVertexPipetteConsumer consumer = new OrderedVertexPipetteConsumer(this.orderedPostQueue, this.processorVerticesToCompiled, pipeline.getQueue(), observer);
+        return new Pipette(name, pipelineTransferer.makeProducer(), consumer);
+    }
+
+    private Map<Vertex, ICompiledProcessor> compileProcessors() throws CompilationError {
+        Map<Vertex, ICompiledProcessor> vertexToCompiled = new HashMap<>(orderedPostQueue.size());
+
+        for (Vertex v : orderedPostQueue) {
+            if (v instanceof PluginVertex) {
+                PluginDefinition pluginDefinition = ((PluginVertex) v).getPluginDefinition();
+
+                ICompiledProcessor compiled;
+                switch (pluginDefinition.getType()) {
+                    case FILTER:
+                        compiled = pluginCompiler.compileFilter((PluginVertex) v);
+                        break;
+                    case OUTPUT:
+                        compiled = pluginCompiler.compileOutput((PluginVertex) v);
+                        break;
+                    default:
+                        throw new CompilationError("Invariant violated! only filter / output plugins expected in filter/output stage");
+                }
+
+                if (compiled == null) {
+                    throw new CompilationError("Compilation of vertex returned null! Vertex:" + v);
+                }
+
+                compiled.register();
+                vertexToCompiled.put(v, compiled);
+            } else if (v instanceof SpecialVertex) {
+                vertexToCompiled.put(v, new PassthroughProcessor((SpecialVertex) v));
+            } else if (v instanceof IfVertex) {
+                vertexToCompiled.put(v, ifCompiler.compile((IfVertex) v));
+            } else {
+                throw new CompilationError("Invariant violated!, only plugin / special vertices were expected! Got: " + v);
+            }
+        }
+
+        return vertexToCompiled;
+    }
+
+    private HashMap<PluginVertex, ICompiledInputPlugin> compileInputs() throws CompilationError {
+        HashMap<PluginVertex, ICompiledInputPlugin> compiled = new HashMap<>();
+        for (PluginVertex pv : pipeline.getInputPluginVertices()) {
+            ICompiledInputPlugin c = pluginCompiler.compileInput(pv);
+            c.register();
+            compiled.put(pv, c);
+        }
+        return compiled;
+    }
+
+    private Collection<Pipette> createInputPipettes() throws CompilationError {
+        List<Pipette> inputPipettes = new ArrayList<>(this.inputVerticesToCompiled.size());
+        for (Map.Entry<PluginVertex, ICompiledInputPlugin> entry : this.inputVerticesToCompiled.entrySet()) {
+            String name = "Input[" + entry.getKey().getId() + "]";
+            Pipette inputPipette = new Pipette(name, entry.getValue(), pipelineTransferer.makeConsumer(), observer);
+            inputPipettes.add(inputPipette);
+        }
+        return inputPipettes;
+    }
+
+    public void startInputs() throws PipetteExecutionException, CompilationError {
+        observer.beforeInputsStart(this);
+        stopInputs();
+        this.inputPipettes.clear();
+        Collection<Pipette> newPipettes = createInputPipettes();
+        this.inputPipettes.addAll(newPipettes);
+        startPipettes(inputPipettes);
+        observer.afterInputsStart(this);
+    }
+
+    public void stopInputs() throws PipetteExecutionException {
+        stopPipettes(this.inputPipettes);
+    }
+
+    public void startProcessors(int workers) throws PipetteExecutionException {
+        observer.beforeProcessorsStart(this);
+        stopProcessors();
+        this.processorPipettes.clear();
+
+        for (int i = 0; i < workers; i++) {
+            String name = "> PostQueue Processor[" + i + "]";
+            this.processorPipettes.add(createProcessorPipette(name));
+        }
+
+        startPipettes(this.processorPipettes);
+        observer.afterProcessorsStart(this);
+    }
+
+    private void stopProcessors() throws PipetteExecutionException {
+        stopPipettes(this.processorPipettes);
+    }
+
+    private void stopPipettes(Collection<Pipette> pipettes) throws PipetteExecutionException {
+        for (Pipette pipette : pipettes) {
+            stopPipette(pipette);
+        }
+    }
+
+    private void startPipettes(Collection<Pipette> pipettes) {
+        for (Pipette pipette : pipettes) {
+            startPipette(pipette);
+        }
+    }
+
+    private Thread startPipette(Pipette pipette) {
+        Thread thread = new Thread(() -> {
+            try {
+                pipette.start();
+            } catch (PipetteExecutionException e) {
+                //TODO: We need some failure handling here
+                e.printStackTrace();
+            }
+        });
+        thread.start();
+
+        this.pipettesThreads.put(pipette, thread);
+
+        return thread;
+    }
+
+    private void stopPipette(Pipette pipette) throws PipetteExecutionException {
+        pipette.stop();
+        Thread thread = pipettesThreads.get(pipette);
+        if (thread.isAlive()) {
+            throw new PipetteExecutionException(
+                    "Pipette stop succeeded, but its thread is still alive! " +
+                            "Pipette: "  + pipette + " Thread " + thread
+            );
+        }
+        pipettesThreads.remove(pipette);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/pipeline/PipelineRunnerObserver.java b/logstash-core/src/main/java/org/logstash/config/pipeline/PipelineRunnerObserver.java
new file mode 100644
index 00000000000..4080b1bb06c
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/pipeline/PipelineRunnerObserver.java
@@ -0,0 +1,53 @@
+package org.logstash.config.pipeline;
+
+import org.logstash.Event;
+import org.logstash.config.pipeline.pipette.OrderedVertexPipetteConsumer;
+import org.logstash.config.ir.graph.Edge;
+
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Created by andrewvc on 10/14/16.
+ *
+ * By default this class is a noop, override any methods you want to implement
+ */
+public class PipelineRunnerObserver {
+    public void postExecutionStep(OrderedVertexPipetteConsumer.ExecutionStep executionStep, List<Event> incomingEvents, Map<Edge, List<Event>> outgoingEvents) {
+
+    }
+
+    public void initialize(PipelineRunner pipelineRunner) {
+        
+    }
+
+    public void beforeStart(PipelineRunner pipelineRunner) {
+    }
+
+    public void afterStart(PipelineRunner pipelineRunner) {
+    }
+
+    public void beforeStop(PipelineRunner pipelineRunner) {
+        
+    }
+
+    public void inputsStopped(PipelineRunner pipelineRunner) {
+    }
+
+    public void afterStop(PipelineRunner pipelineRunner) {
+    }
+
+    public void beforeInputsStart(PipelineRunner pipelineRunner) {
+
+    }
+
+    public void afterInputsStart(PipelineRunner pipelineRunner) {
+        
+    }
+
+    public void beforeProcessorsStart(PipelineRunner pipelineRunner) {
+    }
+
+    public void afterProcessorsStart(PipelineRunner pipelineRunner) {
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/IPipetteConsumer.java b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/IPipetteConsumer.java
new file mode 100644
index 00000000000..0b556527881
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/IPipetteConsumer.java
@@ -0,0 +1,13 @@
+package org.logstash.config.pipeline.pipette;
+
+import org.logstash.Event;
+
+import java.util.List;
+
+/**
+ * Created by andrewvc on 9/30/16.
+ */
+public interface IPipetteConsumer {
+   void process(List<Event> events) throws PipetteExecutionException;
+   void stop() throws PipetteExecutionException;
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/IPipetteConsumerFactory.java b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/IPipetteConsumerFactory.java
new file mode 100644
index 00000000000..ec1dfb08ad0
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/IPipetteConsumerFactory.java
@@ -0,0 +1,8 @@
+package org.logstash.config.pipeline.pipette;
+
+/**
+ * Created by andrewvc on 10/18/16.
+ */
+public interface IPipetteConsumerFactory {
+       IPipetteConsumer make();
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/IPipetteProducer.java b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/IPipetteProducer.java
new file mode 100644
index 00000000000..b4a3b5fcfaf
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/IPipetteProducer.java
@@ -0,0 +1,12 @@
+package org.logstash.config.pipeline.pipette;
+
+/**
+ * Created by andrewvc on 9/30/16.
+ */
+public interface IPipetteProducer {
+    void start() throws PipetteExecutionException;
+
+    void stop();
+
+    void onEvents(PipetteSourceEmitter sourceEmitter);
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/IPipetteProducerFactory.java b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/IPipetteProducerFactory.java
new file mode 100644
index 00000000000..5cf62fce746
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/IPipetteProducerFactory.java
@@ -0,0 +1,8 @@
+package org.logstash.config.pipeline.pipette;
+
+/**
+ * Created by andrewvc on 10/18/16.
+ */
+public interface IPipetteProducerFactory {
+    IPipetteProducer make();
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/OrderedVertexPipetteConsumer.java b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/OrderedVertexPipetteConsumer.java
new file mode 100644
index 00000000000..b368243c40e
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/OrderedVertexPipetteConsumer.java
@@ -0,0 +1,101 @@
+package org.logstash.config.pipeline.pipette;
+
+import org.logstash.Event;
+import org.logstash.config.pipeline.PipelineRunnerObserver;
+import org.logstash.config.compiler.compiled.ICompiledProcessor;
+import org.logstash.config.ir.graph.Edge;
+import org.logstash.config.ir.graph.SpecialVertex;
+import org.logstash.config.ir.graph.Vertex;
+
+import java.util.*;
+
+/**
+ * Created by andrewvc on 10/11/16.
+ */
+public class OrderedVertexPipetteConsumer implements IPipetteConsumer {
+    private final List<Vertex> orderedVertices;
+    private final Map<Edge,List<Event>> edgesToEvents;
+    private final Map<Vertex, ICompiledProcessor> verticesToCompiled;
+    private final ArrayList<ExecutionStep> executionSteps;
+    private final SpecialVertex queueVertex;
+    private final PipelineRunnerObserver observer;
+
+    public class ExecutionStep {
+        public Collection<Edge> getOutgoingEdges() {
+            return outgoingEdges;
+        }
+
+        public Collection<Edge> getIncomingEdges() {
+            return incomingEdges;
+        }
+
+        public ICompiledProcessor getCompiledProcessor() {
+            return compiledProcessor;
+        }
+
+        public Vertex getVertex() {
+            return vertex;
+        }
+
+        private final Collection<Edge> outgoingEdges;
+        private final Collection<Edge> incomingEdges;
+        private final ICompiledProcessor compiledProcessor;
+        private final Vertex vertex;
+
+        public ExecutionStep(Vertex vertex, ICompiledProcessor compiledProcessor,
+                             Collection<Edge> incomingEdges, Collection<Edge> outgoingEdges) {
+            this.vertex = vertex;
+            this.compiledProcessor = compiledProcessor;
+            this.incomingEdges = incomingEdges;
+            this.outgoingEdges = outgoingEdges;
+        }
+
+        public String toString() {
+            return "[Execution Step] Vertex: " + vertex;
+        }
+    }
+
+    public OrderedVertexPipetteConsumer(List<Vertex> orderedVertices, Map<Vertex, ICompiledProcessor> processorVerticesToCompiled, SpecialVertex queueVertex, PipelineRunnerObserver observer) {
+        this.orderedVertices = orderedVertices;
+        this.verticesToCompiled = processorVerticesToCompiled;
+        this.executionSteps = new ArrayList<>(this.orderedVertices.size());
+        this.edgesToEvents = new HashMap<>();
+        this.queueVertex = queueVertex;
+        this.observer = observer;
+
+        for (Vertex vertex : this.orderedVertices) {
+            this.executionSteps.add(new ExecutionStep(vertex, verticesToCompiled.get(vertex), vertex.getIncomingEdges(), vertex.getOutgoingEdges()));
+        }
+    }
+
+    @Override
+    public void process(List<Event> events) throws PipetteExecutionException {
+        // Reset this on each run, probably cheaper than reallocating
+        edgesToEvents.clear();
+        // The queue edges just have the input set of events
+
+        queueVertex.getOutgoingEdges().forEach(e -> edgesToEvents.put(e, events));
+
+        for (ExecutionStep executionStep : executionSteps) {
+            List<Event> incomingEvents = new ArrayList<>();
+            for (Edge edge : executionStep.incomingEdges) {
+                List<Event> edgeEvents = edgesToEvents.get(edge);
+                if (edgeEvents != null) incomingEvents.addAll(edgeEvents);
+            }
+
+            // If there's nothing coming in, just skip execution
+            if (incomingEvents.isEmpty()) {
+                continue;
+            }
+
+            Map<Edge, List<Event>> outgoingEvents = executionStep.getCompiledProcessor().process(incomingEvents);
+            observer.postExecutionStep(executionStep, incomingEvents, outgoingEvents);
+            edgesToEvents.putAll(outgoingEvents);
+        }
+    }
+
+    @Override
+    public void stop() throws PipetteExecutionException {
+
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/Pipette.java b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/Pipette.java
new file mode 100644
index 00000000000..5f3be355f40
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/Pipette.java
@@ -0,0 +1,56 @@
+package org.logstash.config.pipeline.pipette;
+
+
+import org.logstash.Event;
+import org.logstash.config.pipeline.PipelineRunnerObserver;
+
+import java.util.List;
+
+/**
+ * Created by andrewvc on 9/30/16.
+ */
+public class Pipette {
+    public final IPipetteProducer producer;
+    public final IPipetteConsumer consumer;
+    private final String name;
+    private final PipelineRunnerObserver observer;
+
+
+    private final class OnWriteEmitter implements PipetteSourceEmitter {
+        @Override
+        public void emit(List<Event> events) throws PipetteExecutionException {
+            process(events);
+        }
+    }
+
+    public Pipette(String name, IPipetteProducer producer, IPipetteConsumer consumer, PipelineRunnerObserver observer) {
+        this.name = name;
+        this.producer = producer;
+        this.consumer = consumer;
+        this.observer = observer;
+        this.producer.onEvents(new OnWriteEmitter());
+    }
+
+
+    public Pipette(String name, IPipetteProducer producer, IPipetteConsumer consumer) {
+        this(name, producer, consumer, null);
+    }
+
+    public void start() throws PipetteExecutionException {
+        producer.start();
+    }
+
+    private void process(List<Event> events) throws PipetteExecutionException {
+        consumer.process(events);
+    }
+
+    public void stop() throws PipetteExecutionException {
+        producer.stop();
+        consumer.stop();
+    }
+
+    public String toString() {
+        return name;
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/PipetteExecutionException.java b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/PipetteExecutionException.java
new file mode 100644
index 00000000000..43b7dccecba
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/PipetteExecutionException.java
@@ -0,0 +1,15 @@
+package org.logstash.config.pipeline.pipette;
+
+
+/**
+ * Created by andrewvc on 9/30/16.
+ */
+public class PipetteExecutionException extends Exception {
+    public PipetteExecutionException(String s) {
+        super(s);
+    }
+
+    public PipetteExecutionException(String s, Exception e) {
+        super(s,e);
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/PipetteSourceEmitter.java b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/PipetteSourceEmitter.java
new file mode 100644
index 00000000000..01d0ff2c18e
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/PipetteSourceEmitter.java
@@ -0,0 +1,12 @@
+package org.logstash.config.pipeline.pipette;
+
+import org.logstash.Event;
+
+import java.util.List;
+
+/**
+ * Created by andrewvc on 10/11/16.
+ */
+public interface PipetteSourceEmitter {
+    void emit(List<Event> events) throws PipetteExecutionException;
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/QueueReadProducer.java b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/QueueReadProducer.java
new file mode 100644
index 00000000000..91c58c27897
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/QueueReadProducer.java
@@ -0,0 +1,46 @@
+package org.logstash.config.pipeline.pipette;
+
+import org.logstash.Event;
+
+import java.util.List;
+import java.util.concurrent.BlockingQueue;
+
+/**
+ * Created by andrewvc on 10/11/16.
+ */
+public class QueueReadProducer implements IPipetteProducer {
+    private final BlockingQueue<List<Event>> queue;
+    private volatile boolean running;
+    private PipetteSourceEmitter onEventsReader;
+
+    public QueueReadProducer(BlockingQueue<List<Event>> queue) {
+        this.queue = queue;
+        this.running = true;
+    }
+
+    @Override
+    public void start() throws PipetteExecutionException {
+        while (this.running) {
+            List<Event> events = null;
+            try {
+                events = queue.take();
+            } catch (InterruptedException e) {
+                throw new PipetteExecutionException("Unexpected Interruption!", e);
+            }
+
+            onEventsReader.emit(events);
+        }
+    }
+
+    @Override
+    public void stop() {
+        this.running = false;
+    }
+
+    @Override
+    public void onEvents(PipetteSourceEmitter onEvents) {
+       this.onEventsReader = onEvents;
+    }
+
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/QueueWriteConsumer.java b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/QueueWriteConsumer.java
new file mode 100644
index 00000000000..e4caafd1f0e
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/config/pipeline/pipette/QueueWriteConsumer.java
@@ -0,0 +1,31 @@
+package org.logstash.config.pipeline.pipette;
+
+import org.logstash.Event;
+
+import java.util.List;
+import java.util.concurrent.BlockingQueue;
+
+/**
+ * Created by andrewvc on 10/11/16.
+ */
+public class QueueWriteConsumer implements IPipetteConsumer {
+    private final BlockingQueue<List<Event>> queue;
+
+    public QueueWriteConsumer(BlockingQueue<List<Event>> queue) {
+        this.queue = queue;
+    }
+
+    @Override
+    public void process(List<Event> events) throws PipetteExecutionException {
+        try {
+            this.queue.put(events);
+        } catch (InterruptedException e) {
+            throw new PipetteExecutionException("Unexpected interruption!", e);
+        }
+    }
+
+    @Override
+    public void stop() throws PipetteExecutionException {
+
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/CheckpointTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/CheckpointTest.java
new file mode 100644
index 00000000000..83d1ac5aa49
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/CheckpointTest.java
@@ -0,0 +1,21 @@
+package org.logstash.ackedqueue;
+
+import org.junit.Test;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+public class CheckpointTest {
+
+    @Test
+    public void newInstance() {
+        Checkpoint checkpoint = new Checkpoint(1, 2, 3, 4, 5);
+
+        assertThat(checkpoint.getPageNum(), is(equalTo(1)));
+        assertThat(checkpoint.getFirstUnackedPageNum(), is(equalTo(2)));
+        assertThat(checkpoint.getFirstUnackedSeqNum(), is(equalTo(3L)));
+        assertThat(checkpoint.getMinSeqNum(), is(equalTo(4L)));
+        assertThat(checkpoint.getElementCount(), is(equalTo(5)));
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
new file mode 100644
index 00000000000..0477b1b209c
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
@@ -0,0 +1,116 @@
+package org.logstash.ackedqueue;
+
+import org.junit.Test;
+import org.logstash.common.io.ByteBufferPageIO;
+import org.logstash.common.io.FileCheckpointIOTest;
+import org.logstash.common.io.PageIO;
+
+import java.io.IOException;
+import java.net.URL;
+import java.nio.file.NoSuchFileException;
+import java.nio.file.Paths;
+
+import static org.hamcrest.CoreMatchers.containsString;
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+public class HeadPageTest {
+
+    @Test
+    public void newHeadPage() throws IOException {
+        Settings s = TestSettings.getSettings(100);
+        Queue q = new Queue(s);
+        PageIO pageIO = s.getPageIOFactory().build(0, 100, "dummy");
+        HeadPage p = new HeadPage(0, q, pageIO);
+
+        assertThat(p.getPageNum(), is(equalTo(0)));
+        assertThat(p.isFullyRead(), is(true));
+        assertThat(p.isFullyAcked(), is(false));
+        assertThat(p.hasSpace(10), is(true));
+        assertThat(p.hasSpace(100), is(false));
+    }
+
+    @Test
+    public void pageWrite() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+
+        Settings s = TestSettings.getSettings(singleElementCapacity);
+        Queue q = new Queue(s);
+        PageIO pageIO = s.getPageIOFactory().build(0, singleElementCapacity, "dummy");
+        HeadPage p = new HeadPage(0, q, pageIO);
+
+        assertThat(p.hasSpace(element.serialize().length), is(true));
+        p.write(element.serialize(), 0);
+
+        assertThat(p.hasSpace(element.serialize().length), is(false));
+        assertThat(p.isFullyRead(), is(false));
+        assertThat(p.isFullyAcked(), is(false));
+    }
+
+    @Test
+    public void pageWriteAndReadSingle() throws IOException {
+        long seqNum = 1L;
+        Queueable element = new StringElement("foobarbaz");
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+
+        Settings s = TestSettings.getSettings(singleElementCapacity);
+        Queue q = new Queue(s);
+        PageIO pageIO = s.getPageIOFactory().build(0, singleElementCapacity, "dummy");
+        HeadPage p = new HeadPage(0, q, pageIO);
+
+        assertThat(p.hasSpace(element.serialize().length), is(true));
+        p.write(element.serialize(), seqNum);
+
+        Batch b = p.readBatch(1);
+
+        assertThat(b.getElements().size(), is(equalTo(1)));
+        assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));
+
+        assertThat(p.hasSpace(element.serialize().length), is(false));
+        assertThat(p.isFullyRead(), is(true));
+        assertThat(p.isFullyAcked(), is(false));
+    }
+
+    @Test
+    public void pageWriteAndReadMulti() throws IOException {
+        long seqNum = 1L;
+        Queueable element = new StringElement("foobarbaz");
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+
+        Settings s = TestSettings.getSettings(singleElementCapacity);
+        Queue q = new Queue(s);
+        PageIO pageIO = s.getPageIOFactory().build(0, singleElementCapacity, "dummy");
+        HeadPage p = new HeadPage(0, q, pageIO);
+
+        assertThat(p.hasSpace(element.serialize().length), is(true));
+        p.write(element.serialize(), seqNum);
+
+        Batch b = p.readBatch(10);
+
+        assertThat(b.getElements().size(), is(equalTo(1)));
+        assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));
+
+        assertThat(p.hasSpace(element.serialize().length), is(false));
+        assertThat(p.isFullyRead(), is(true));
+        assertThat(p.isFullyAcked(), is(false));
+    }
+
+    @Test
+    public void pageViaQueueOpenForHeadCheckpointWithoutSupportingPageFiles() throws Exception {
+        URL url = FileCheckpointIOTest.class.getResource("checkpoint.head");
+        String dirPath = Paths.get(url.toURI()).getParent().toString();
+        Queueable element = new StringElement("foobarbaz");
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+        Settings s = TestSettings.getSettingsCheckpointFilePageMemory(singleElementCapacity, dirPath);
+        TestQueue q = new TestQueue(s);
+        try {
+            q.open();
+        } catch (NoSuchFileException e) {
+            assertThat(e.getMessage(), containsString("checkpoint.2"));
+        }
+        HeadPage p = q.getHeadPage();
+        assertThat(p, is(equalTo(null)));
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
new file mode 100644
index 00000000000..230f088bebc
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
@@ -0,0 +1,388 @@
+package org.logstash.ackedqueue;
+
+import org.junit.Test;
+import org.logstash.common.io.ByteBufferPageIO;
+
+import java.io.IOException;
+import java.util.*;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.CoreMatchers.nullValue;
+import static org.hamcrest.CoreMatchers.notNullValue;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+public class QueueTest {
+
+    @Test
+    public void newQueue() throws IOException {
+        Queue q = new TestQueue(TestSettings.getSettings(10));
+        q.open();
+
+        assertThat(q.nonBlockReadBatch(1), is(equalTo(null)));
+    }
+
+    @Test
+    public void singleWriteRead() throws IOException {
+        Queue q = new TestQueue(TestSettings.getSettings(100));
+        q.open();
+
+        Queueable element = new StringElement("foobarbaz");
+        q.write(element);
+
+        Batch b = q.nonBlockReadBatch(1);
+
+        assertThat(b.getElements().size(), is(equalTo(1)));
+        assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));
+        assertThat(q.nonBlockReadBatch(1), is(equalTo(null)));
+    }
+
+    @Test
+    public void singleWriteMultiRead() throws IOException {
+        Queue q = new TestQueue(TestSettings.getSettings(100));
+        q.open();
+
+        Queueable element = new StringElement("foobarbaz");
+        q.write(element);
+
+        Batch b = q.nonBlockReadBatch(2);
+
+        assertThat(b.getElements().size(), is(equalTo(1)));
+        assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));
+        assertThat(q.nonBlockReadBatch(2), is(equalTo(null)));
+    }
+
+    @Test
+    public void multiWriteSamePage() throws IOException {
+        Queue q = new TestQueue(TestSettings.getSettings(100));
+        q.open();
+
+        List<Queueable> elements = Arrays.asList(new StringElement("foobarbaz1"), new StringElement("foobarbaz2"), new StringElement("foobarbaz3"));
+
+        for (Queueable e : elements) {
+            q.write(e);
+        }
+
+        Batch b = q.nonBlockReadBatch(2);
+
+        assertThat(b.getElements().size(), is(equalTo(2)));
+        assertThat(b.getElements().get(0).toString(), is(equalTo(elements.get(0).toString())));
+        assertThat(b.getElements().get(1).toString(), is(equalTo(elements.get(1).toString())));
+
+        b = q.nonBlockReadBatch(2);
+
+        assertThat(b.getElements().size(), is(equalTo(1)));
+        assertThat(b.getElements().get(0).toString(), is(equalTo(elements.get(2).toString())));
+    }
+
+    @Test
+    public void writeMultiPage() throws IOException {
+        List<Queueable> elements = Arrays.asList(new StringElement("foobarbaz1"), new StringElement("foobarbaz2"), new StringElement("foobarbaz3"), new StringElement("foobarbaz4"));
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements.get(0).serialize().length);
+
+        TestQueue q = new TestQueue(TestSettings.getSettings(2 * singleElementCapacity));
+        q.open();
+
+        for (Queueable e : elements) {
+            q.write(e);
+        }
+
+        // total of 2 pages: 1 head and 1 tail
+        assertThat(q.getBeheadedPages().size(), is(equalTo(1)));
+
+        assertThat(q.getBeheadedPages().get(0).isFullyRead(), is(equalTo(false)));
+        assertThat(q.getBeheadedPages().get(0).isFullyAcked(), is(equalTo(false)));
+        assertThat(q.getHeadPage().isFullyRead(), is(equalTo(false)));
+        assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));
+
+        Batch b = q.nonBlockReadBatch(10);
+        assertThat(b.getElements().size(), is(equalTo(2)));
+
+        assertThat(q.getBeheadedPages().size(), is(equalTo(1)));
+
+        assertThat(q.getBeheadedPages().get(0).isFullyRead(), is(equalTo(true)));
+        assertThat(q.getBeheadedPages().get(0).isFullyAcked(), is(equalTo(false)));
+        assertThat(q.getHeadPage().isFullyRead(), is(equalTo(false)));
+        assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));
+
+        b = q.nonBlockReadBatch(10);
+        assertThat(b.getElements().size(), is(equalTo(2)));
+
+        assertThat(q.getBeheadedPages().get(0).isFullyRead(), is(equalTo(true)));
+        assertThat(q.getBeheadedPages().get(0).isFullyAcked(), is(equalTo(false)));
+        assertThat(q.getHeadPage().isFullyRead(), is(equalTo(true)));
+        assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));
+
+        b = q.nonBlockReadBatch(10);
+        assertThat(b, is(equalTo(null)));
+    }
+
+
+    @Test
+    public void writeMultiPageWithInOrderAcking() throws IOException {
+        List<Queueable> elements = Arrays.asList(new StringElement("foobarbaz1"), new StringElement("foobarbaz2"), new StringElement("foobarbaz3"), new StringElement("foobarbaz4"));
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements.get(0).serialize().length);
+
+        TestQueue q = new TestQueue(TestSettings.getSettings(2 * singleElementCapacity));
+        q.open();
+
+        for (Queueable e : elements) {
+            q.write(e);
+        }
+
+        Batch b = q.nonBlockReadBatch(10);
+
+        assertThat(b.getElements().size(), is(equalTo(2)));
+        assertThat(q.getBeheadedPages().size(), is(equalTo(1)));
+
+        // lets keep a ref to that tail page before acking
+        BeheadedPage beheadedPage = q.getBeheadedPages().get(0);
+
+        assertThat(beheadedPage.isFullyRead(), is(equalTo(true)));
+
+        // ack first batch which includes all elements from beheadedPages
+        b.close();
+
+        assertThat(q.getBeheadedPages().size(), is(equalTo(0)));
+        assertThat(beheadedPage.isFullyRead(), is(equalTo(true)));
+        assertThat(beheadedPage.isFullyAcked(), is(equalTo(true)));
+
+        b = q.nonBlockReadBatch(10);
+
+        assertThat(b.getElements().size(), is(equalTo(2)));
+        assertThat(q.getHeadPage().isFullyRead(), is(equalTo(true)));
+        assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));
+
+        b.close();
+
+        assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(true)));
+    }
+
+    @Test
+    public void writeMultiPageWithInOrderAckingCheckpoints() throws IOException {
+        List<Queueable> elements1 = Arrays.asList(new StringElement("foobarbaz1"), new StringElement("foobarbaz2"));
+        List<Queueable> elements2 = Arrays.asList(new StringElement("foobarbaz3"), new StringElement("foobarbaz4"));
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements1.get(0).serialize().length);
+
+        Settings settings = TestSettings.getSettings(2 * singleElementCapacity);
+        TestQueue q = new TestQueue(settings);
+        q.open();
+
+        assertThat(q.getHeadPage().getPageNum(), is(equalTo(0)));
+        Checkpoint c = q.getCheckpointIO().read("checkpoint.head");
+        assertThat(c.getPageNum(), is(equalTo(0)));
+        assertThat(c.getElementCount(), is(equalTo(0)));
+        assertThat(c.getMinSeqNum(), is(equalTo(0L)));
+        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(0L)));
+        assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));
+
+        for (Queueable e : elements1) {
+            q.write(e);
+        }
+
+        c = q.getCheckpointIO().read("checkpoint.head");
+        assertThat(c.getPageNum(), is(equalTo(0)));
+        assertThat(c.getElementCount(), is(equalTo(0)));
+        assertThat(c.getMinSeqNum(), is(equalTo(0L)));
+        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(0L)));
+        assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));
+
+//        assertThat(elements1.get(1).getSeqNum(), is(equalTo(2L)));
+        q.ensurePersistedUpto(2);
+
+        c = q.getCheckpointIO().read("checkpoint.head");
+        assertThat(c.getPageNum(), is(equalTo(0)));
+        assertThat(c.getElementCount(), is(equalTo(2)));
+        assertThat(c.getMinSeqNum(), is(equalTo(1L)));
+        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(1L)));
+        assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));
+
+        for (Queueable e : elements2) {
+            q.write(e);
+        }
+
+        c = q.getCheckpointIO().read("checkpoint.head");
+        assertThat(c.getPageNum(), is(equalTo(1)));
+        assertThat(c.getElementCount(), is(equalTo(0)));
+        assertThat(c.getMinSeqNum(), is(equalTo(0L)));
+        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(0L)));
+        assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));
+
+        c = q.getCheckpointIO().read("checkpoint.0");
+        assertThat(c.getPageNum(), is(equalTo(0)));
+        assertThat(c.getElementCount(), is(equalTo(2)));
+        assertThat(c.getMinSeqNum(), is(equalTo(1L)));
+        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(1L)));
+
+        Batch b = q.nonBlockReadBatch(10);
+        b.close();
+
+        assertThat(q.getCheckpointIO().read("checkpoint.0"), is(nullValue()));
+
+        c = q.getCheckpointIO().read("checkpoint.head");
+        assertThat(c.getPageNum(), is(equalTo(1)));
+        assertThat(c.getElementCount(), is(equalTo(2)));
+        assertThat(c.getMinSeqNum(), is(equalTo(3L)));
+        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(3L)));
+        assertThat(c.getFirstUnackedPageNum(), is(equalTo(1)));
+
+        b = q.nonBlockReadBatch(10);
+        b.close();
+
+        c = q.getCheckpointIO().read("checkpoint.head");
+        assertThat(c.getPageNum(), is(equalTo(1)));
+        assertThat(c.getElementCount(), is(equalTo(2)));
+        assertThat(c.getMinSeqNum(), is(equalTo(3L)));
+        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(5L)));
+        assertThat(c.getFirstUnackedPageNum(), is(equalTo(1)));
+    }
+
+    @Test
+    public void randomAcking() throws IOException {
+        Random random = new Random();
+
+        // 10 tests of random queue sizes
+        for (int loop = 0; loop < 10; loop++) {
+            int page_count = random.nextInt(10000) + 1;
+            int digits = new Double(Math.ceil(Math.log10(page_count))).intValue();
+
+            // create a queue with a single element per page
+            List<Queueable> elements = new ArrayList<>();
+            for (int i = 0; i < page_count; i++) {
+                elements.add(new StringElement(String.format("%0" + digits + "d", i)));
+            }
+            int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements.get(0).serialize().length);
+
+            TestQueue q = new TestQueue(TestSettings.getSettings(singleElementCapacity));
+            q.open();
+
+            for (Queueable e : elements) {
+                q.write(e);
+            }
+
+            assertThat(q.getBeheadedPages().size(), is(equalTo(page_count - 1)));
+
+            // first read all elements
+            List<Batch> batches = new ArrayList<>();
+            for (Batch b = q.nonBlockReadBatch(1); b != null; b = q.nonBlockReadBatch(1)) {
+                batches.add(b);
+            }
+            assertThat(batches.size(), is(equalTo(page_count)));
+
+            // then ack randomly
+            Collections.shuffle(batches);
+            for (Batch b : batches) {
+                b.close();
+            }
+
+            assertThat(q.getBeheadedPages().size(), is(equalTo(0)));
+        }
+    }
+
+    @Test(timeout = 5000)
+    public void reachMaxUnread() throws IOException, InterruptedException, ExecutionException {
+        Queueable element = new StringElement("foobarbaz");
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+
+        Settings settings = TestSettings.getSettings(singleElementCapacity);
+        settings.setMaxUnread(2); // 2 so we know the first write should not block and the second should
+        TestQueue q = new TestQueue(settings);
+        q.open();
+
+
+        long seqNum = q.write(element);
+        assertThat(seqNum, is(equalTo(1L)));
+        assertThat(q.isFull(), is(false));
+
+        int ELEMENT_COUNT = 1000;
+        for (int i = 0; i < ELEMENT_COUNT; i++) {
+
+            // we expect the next write call to block so let's wrap it in a Future
+            Callable<Long> write = () -> {
+                return q.write(element);
+            };
+
+            ExecutorService executor = Executors.newFixedThreadPool(1);
+            Future<Long> future = executor.submit(write);
+
+            while (!q.isFull()) {
+                // spin wait until data is written and write blocks
+                Thread.sleep(1);
+            }
+            assertThat(q.unreadCount, is(equalTo(2L)));
+            assertThat(future.isDone(), is(false));
+
+            // read one element, which will unblock the last write
+            Batch b = q.nonBlockReadBatch(1);
+            assertThat(b.getElements().size(), is(equalTo(1)));
+
+            // future result is the blocked write seqNum for the second element
+            assertThat(future.get(), is(equalTo(2L + i)));
+            assertThat(q.isFull(), is(false));
+
+            executor.shutdown();
+        }
+
+        // since we did not ack and pages hold a single item
+        assertThat(q.getBeheadedPages().size(), is(equalTo(ELEMENT_COUNT)));
+    }
+
+    @Test
+    public void reachMaxUnreadWithAcking() throws IOException, InterruptedException, ExecutionException {
+        Queueable element = new StringElement("foobarbaz");
+
+        // TODO: add randomized testing on the page size (but must be > single element size)
+        Settings settings = TestSettings.getSettings(256); // 256 is arbitrary, large enough to hold a few elements
+
+        settings.setMaxUnread(2); // 2 so we know the first write should not block and the second should
+        TestQueue q = new TestQueue(settings);
+        q.open();
+
+        // perform first non-blocking write
+        long seqNum = q.write(element);
+
+        assertThat(seqNum, is(equalTo(1L)));
+        assertThat(q.isFull(), is(false));
+
+        int ELEMENT_COUNT = 1000;
+        for (int i = 0; i < ELEMENT_COUNT; i++) {
+
+            // we expect this next write call to block so let's wrap it in a Future
+            Callable<Long> write = () -> {
+                return q.write(element);
+            };
+
+            ExecutorService executor = Executors.newFixedThreadPool(1);
+            Future<Long> future = executor.submit(write);
+
+            // spin wait until data is written and write blocks
+            while (!q.isFull()) { Thread.sleep(1); }
+
+            // read one element, which will unblock the last write
+            Batch b = q.nonBlockReadBatch(1);
+            assertThat(b, is(notNullValue()));
+            assertThat(b.getElements().size(), is(equalTo(1)));
+            b.close();
+
+            // future result is the blocked write seqNum for the second element
+            assertThat(future.get(), is(equalTo(2L + i)));
+            assertThat(q.isFull(), is(false));
+
+            executor.shutdown();
+        }
+
+        // all batches are acked, no tail pages should exist
+        assertThat(q.getBeheadedPages().size(), is(equalTo(0)));
+
+        // the last read unblocked the last write so some elements (1 unread and maybe some acked) should be in the head page
+        assertThat(q.getHeadPage().getElementCount() > 0L, is(true));
+        assertThat(q.getHeadPage().unreadCount(), is(equalTo(1L)));
+        assertThat(q.unreadCount, is(equalTo(1L)));
+    }
+
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/StringElement.java b/logstash-core/src/test/java/org/logstash/ackedqueue/StringElement.java
new file mode 100644
index 00000000000..99092a90884
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/StringElement.java
@@ -0,0 +1,58 @@
+package org.logstash.ackedqueue;
+
+import java.nio.ByteBuffer;
+
+public class StringElement implements Queueable {
+    private final String content;
+
+    public StringElement(String content) {
+        this.content = content;
+    }
+
+    @Override
+    public byte[] serialize() {
+        byte[] contentBytes = this.content.getBytes();
+        ByteBuffer buffer = ByteBuffer.allocate(contentBytes.length);
+        buffer.put(contentBytes);
+        return buffer.array();
+    }
+
+    public static StringElement deserialize(byte[] bytes) {
+        ByteBuffer buffer = ByteBuffer.allocate(bytes.length);
+        buffer.put(bytes);
+
+        buffer.position(0);
+        byte[] content = new byte[bytes.length];
+        buffer.get(content);
+        return new StringElement(new String(content));
+    }
+
+    @Override
+    public String toString() {
+        return content;
+    }
+
+
+    @Override
+    public boolean equals(Object other) {
+        if (other == null) {
+            return false;
+        }
+        if (!StringElement.class.isAssignableFrom(other.getClass())) {
+            return false;
+        }
+
+        final StringElement element = (StringElement)other;
+        if ((this.content == null) ? (element.content != null) : !this.content.equals(element.content)) {
+            return false;
+        }
+        return true;
+    }
+
+    @Override
+    public int hashCode() {
+        int hash = 13;
+        hash = 53 * hash + (this.content != null ? this.content.hashCode() : 0);
+        return hash;
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/TestQueue.java b/logstash-core/src/test/java/org/logstash/ackedqueue/TestQueue.java
new file mode 100644
index 00000000000..a150e36bce4
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/TestQueue.java
@@ -0,0 +1,17 @@
+package org.logstash.ackedqueue;
+
+import java.util.List;
+
+public class TestQueue extends Queue {
+    public TestQueue(Settings settings) {
+        super(settings);
+    }
+
+    public HeadPage getHeadPage() {
+        return this.headPage;
+    }
+
+    public List<BeheadedPage> getBeheadedPages() {
+        return this.beheadedPages;
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/TestSettings.java b/logstash-core/src/test/java/org/logstash/ackedqueue/TestSettings.java
new file mode 100644
index 00000000000..7478c2ec70f
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/TestSettings.java
@@ -0,0 +1,33 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.common.io.ByteBufferPageIO;
+import org.logstash.common.io.CheckpointIOFactory;
+import org.logstash.common.io.FileCheckpointIO;
+import org.logstash.common.io.MemoryCheckpointIO;
+import org.logstash.common.io.PageIOFactory;
+
+public class TestSettings {
+
+    public static Settings getSettings(int capacity) {
+        MemoryCheckpointIO.clearSources();
+        Settings s = new MemorySettings();
+        PageIOFactory pageIOFactory = (pageNum, size, path) -> new ByteBufferPageIO(pageNum, size, path);
+        CheckpointIOFactory checkpointIOFactory = (source) -> new MemoryCheckpointIO(source);
+        s.setCapacity(capacity);
+        s.setElementIOFactory(pageIOFactory);
+        s.setCheckpointIOFactory(checkpointIOFactory);
+        s.setElementClass(StringElement.class);
+        return s;
+    }
+
+    public static Settings getSettingsCheckpointFilePageMemory(int capacity, String folder) {
+        Settings s = new FileSettings(folder);
+        PageIOFactory pageIOFactory = (pageNum, size, path) -> new ByteBufferPageIO(pageNum, size, path);
+        CheckpointIOFactory checkpointIOFactory = (source) -> new FileCheckpointIO(source);
+        s.setCapacity(capacity);
+        s.setElementIOFactory(pageIOFactory);
+        s.setCheckpointIOFactory(checkpointIOFactory);
+        s.setElementClass(StringElement.class);
+        return s;
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/common/io/ByteBufferPageIOTest.java b/logstash-core/src/test/java/org/logstash/common/io/ByteBufferPageIOTest.java
new file mode 100644
index 00000000000..9edd134bbe2
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/common/io/ByteBufferPageIOTest.java
@@ -0,0 +1,157 @@
+package org.logstash.common.io;
+
+import org.junit.Test;
+import org.logstash.ackedqueue.Queueable;
+import org.logstash.ackedqueue.SequencedList;
+import org.logstash.ackedqueue.StringElement;
+
+import java.io.IOException;
+import java.util.List;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+
+public class ByteBufferPageIOTest {
+
+    private final int CAPACITY = 1024;
+    private int MIN_CAPACITY = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(0);
+
+    private ByteBufferPageIO subject() throws IOException {
+        return subject(CAPACITY);
+    }
+
+    private ByteBufferPageIO subject(int capacity) throws IOException {
+        ByteBufferPageIO io = new ByteBufferPageIO(capacity);
+        io.create();
+        return io;
+    }
+
+    private ByteBufferPageIO subject(int capacity, byte[] bytes) throws IOException {
+        return new ByteBufferPageIO(capacity, bytes);
+    }
+
+    private Queueable buildStringElement(String str) {
+        return new StringElement(str);
+    }
+
+    @Test
+    public void getWritePosition() throws IOException {
+        assertThat(subject().getWritePosition(), is(equalTo(1)));
+    }
+
+    @Test
+    public void getElementCount() throws IOException {
+        assertThat(subject().getElementCount(), is(equalTo(0)));
+    }
+
+    @Test
+    public void getStartSeqNum() throws IOException {
+        assertThat(subject().getMinSeqNum(), is(equalTo(0L)));
+    }
+
+    @Test
+    public void hasSpace() throws IOException {
+        assertThat(subject(MIN_CAPACITY).hasSpace(0), is(true));
+        assertThat(subject(MIN_CAPACITY).hasSpace(1), is(false));
+    }
+
+    @Test
+    public void hasSpaceAfterWrite() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+        long seqNum = 1L;
+
+        ByteBufferPageIO subject = subject(singleElementCapacity);
+
+        assertThat(subject.hasSpace(element.serialize().length), is(true));
+        subject.write(element.serialize(), seqNum);
+        assertThat(subject.hasSpace(element.serialize().length), is(false));
+        assertThat(subject.hasSpace(1), is(false));
+    }
+
+    @Test
+    public void write() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        long seqNum = 42L;
+        ByteBufferPageIO subj = subject();
+        subj.create();
+        subj.write(element.serialize(), seqNum);
+        assertThat(subj.getWritePosition(), is(equalTo(ByteBufferPageIO.HEADER_SIZE +  ByteBufferPageIO._persistedByteCount(element.serialize().length))));
+        assertThat(subj.getElementCount(), is(equalTo(1)));
+        assertThat(subj.getMinSeqNum(), is(equalTo(seqNum)));
+    }
+
+    @Test
+    public void recoversValidState() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        long seqNum = 42L;
+        ByteBufferPageIO subject = subject();
+        subject.create();
+        subject.write(element.serialize(), seqNum);
+
+        byte[] inititalState = subject.dump();
+        subject = subject(inititalState.length, inititalState);
+        subject.open(seqNum, 1);
+        assertThat(subject.getElementCount(), is(equalTo(1)));
+        assertThat(subject.getMinSeqNum(), is(equalTo(seqNum)));
+    }
+
+    @Test(expected = IOException.class)
+    public void recoversInvalidState() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        long seqNum = 42L;
+        ByteBufferPageIO subject = subject();
+        subject.create();
+        subject.write(element.serialize(), seqNum);
+
+        byte[] inititalState = subject.dump();
+        subject(inititalState.length, inititalState);
+        subject.open(1L, 1);
+    }
+
+    // TODO: add other invalid initial states
+
+    @Test
+    public void writeRead() throws IOException {
+        long seqNum = 42L;
+        Queueable element = buildStringElement("foobarbaz");
+        ByteBufferPageIO subj = subject();
+        subj.create();
+        subj.write(element.serialize(), seqNum);
+        SequencedList<byte[]> result = subj.read(seqNum, 1);
+        assertThat(result.getElements().size(), is(equalTo(1)));
+        Queueable readElement = StringElement.deserialize(result.getElements().get(0));
+        assertThat(result.getSeqNums().get(0), is(equalTo(seqNum)));
+        assertThat(readElement.toString(), is(equalTo(element.toString())));
+    }
+
+    @Test
+    public void writeReadMulti() throws IOException {
+        Queueable element1 = buildStringElement("foo");
+        Queueable element2 = buildStringElement("bar");
+        Queueable element3 = buildStringElement("baz");
+        Queueable element4 = buildStringElement("quux");
+        ByteBufferPageIO subj = subject();
+        subj.create();
+        subj.write(element1.serialize(), 40L);
+        subj.write(element2.serialize(), 41L);
+        subj.write(element3.serialize(), 42L);
+        subj.write(element4.serialize(), 43L);
+        int batchSize = 11;
+        SequencedList<byte[]> result = subj.read(40L, batchSize);
+        assertThat(result.getElements().size(), is(equalTo(4)));
+
+        assertThat(result.getSeqNums().get(0), is(equalTo(40L)));
+        assertThat(result.getSeqNums().get(1), is(equalTo(41L)));
+        assertThat(result.getSeqNums().get(2), is(equalTo(42L)));
+        assertThat(result.getSeqNums().get(3), is(equalTo(43L)));
+
+        assertThat(StringElement.deserialize(result.getElements().get(0)).toString(), is(equalTo(element1.toString())));
+        assertThat(StringElement.deserialize(result.getElements().get(1)).toString(), is(equalTo(element2.toString())));
+        assertThat(StringElement.deserialize(result.getElements().get(2)).toString(), is(equalTo(element3.toString())));
+        assertThat(StringElement.deserialize(result.getElements().get(3)).toString(), is(equalTo(element4.toString())));
+    }
+
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/common/io/FileCheckpointIOTest.java b/logstash-core/src/test/java/org/logstash/common/io/FileCheckpointIOTest.java
new file mode 100644
index 00000000000..86e860aad59
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/common/io/FileCheckpointIOTest.java
@@ -0,0 +1,53 @@
+package org.logstash.common.io;
+
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.logstash.ackedqueue.Checkpoint;
+
+import java.net.URL;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+public class FileCheckpointIOTest {
+    private String checkpointFolder;
+    private CheckpointIO io;
+
+    @Rule
+    public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+    @Before
+    public void setUp() throws Exception {
+        checkpointFolder = temporaryFolder
+                .newFolder("checkpoints")
+                .getPath();
+        io = new FileCheckpointIO(checkpointFolder);
+    }
+
+    @Test
+    public void read() throws Exception {
+        URL url = this.getClass().getResource("checkpoint.head");
+        String dirPath = Paths.get(url.toURI()).getParent().toString();
+        io = new FileCheckpointIO(dirPath);
+        Checkpoint chk = io.read("checkpoint.head");
+        assertThat(chk.getMinSeqNum(), is(8L));
+    }
+
+    @Test
+    public void write() throws Exception {
+        io.write("checkpoint.head", 6, 2, 10L, 8L, 200);
+        io.write("checkpoint.head", 6, 2, 10L, 8L, 200);
+        Path fullFileName = Paths.get(checkpointFolder, "checkpoint.head");
+        byte[] contents = Files.readAllBytes(fullFileName);
+        URL url = this.getClass().getResource("checkpoint.head");
+        Path path = Paths.get(url.getPath());
+        byte[] compare = Files.readAllBytes(path);
+        assertThat(contents, is(equalTo(compare)));
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/common/io/FileMmapIOTest.java b/logstash-core/src/test/java/org/logstash/common/io/FileMmapIOTest.java
new file mode 100644
index 00000000000..01bcfe6a46a
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/common/io/FileMmapIOTest.java
@@ -0,0 +1,55 @@
+package org.logstash.common.io;
+
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.logstash.ackedqueue.SequencedList;
+import org.logstash.ackedqueue.StringElement;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+public class FileMmapIOTest {
+    private String folder;
+    private MmapPageIO writeIo;
+    private MmapPageIO readIo;
+    private int pageNum;
+
+    @Rule
+    public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+    @Before
+    public void setUp() throws Exception {
+        pageNum = 0;
+        folder = temporaryFolder
+                .newFolder("pages")
+                .getPath();
+        writeIo = new MmapPageIO(pageNum, 1024, folder);
+        readIo = new MmapPageIO(pageNum, 1024, folder);
+    }
+
+    @Test
+    public void roundTrip() throws Exception {
+        List<StringElement> list = new ArrayList<>();
+        List<StringElement> readList = new ArrayList<>();
+        writeIo.create();
+        for (int i = 1; i < 17; i++) {
+            StringElement input = new StringElement("element-" + i);
+            list.add(input);
+            writeIo.write(input.serialize(), i);
+        }
+        writeIo.close();
+        readIo.open(1, 16);
+        SequencedList<byte[]> result = readIo.read(1, 16);
+        for (byte[] bytes : result.getElements()) {
+            StringElement element = StringElement.deserialize(bytes);
+            readList.add(element);
+        }
+        assertThat(readList, is(equalTo(list)));
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/common/io/MemoryCheckpointTest.java b/logstash-core/src/test/java/org/logstash/common/io/MemoryCheckpointTest.java
new file mode 100644
index 00000000000..d2629bd4ba1
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/common/io/MemoryCheckpointTest.java
@@ -0,0 +1,45 @@
+package org.logstash.common.io;
+
+import org.junit.Before;
+import org.junit.Test;
+import org.logstash.ackedqueue.Checkpoint;
+import org.logstash.ackedqueue.MemorySettings;
+import org.logstash.ackedqueue.Settings;
+
+import java.io.IOException;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+public class MemoryCheckpointTest {
+
+    private CheckpointIO io;
+
+    @Before
+    public void setUp() {
+        Settings settings = new MemorySettings();
+        CheckpointIOFactory factory = (dirPath) -> new MemoryCheckpointIO(dirPath);
+        settings.setCheckpointIOFactory(factory);
+        this.io = settings.getCheckpointIOFactory().build(settings.getDirPath());
+    }
+
+    @Test
+    public void writeNewReadExisting() throws IOException {
+        io.write("checkpoint.head", 1, 2, 3, 4, 5);
+
+        Checkpoint checkpoint = io.read("checkpoint.head");
+
+        assertThat(checkpoint.getPageNum(), is(equalTo(1)));
+        assertThat(checkpoint.getFirstUnackedPageNum(), is(equalTo(2)));
+        assertThat(checkpoint.getFirstUnackedSeqNum(), is(equalTo(3L)));
+        assertThat(checkpoint.getMinSeqNum(), is(equalTo(4L)));
+        assertThat(checkpoint.getElementCount(), is(equalTo(5)));
+    }
+
+    @Test
+    public void readInnexisting() throws IOException {
+        Checkpoint checkpoint = io.read("checkpoint.invalid");
+        assertThat(checkpoint, is(equalTo(null)));
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/common/io/MemoryPageIOStreamTest.java b/logstash-core/src/test/java/org/logstash/common/io/MemoryPageIOStreamTest.java
new file mode 100644
index 00000000000..f2b1ac6acdd
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/common/io/MemoryPageIOStreamTest.java
@@ -0,0 +1,188 @@
+package org.logstash.common.io;
+
+import org.junit.Test;
+import org.logstash.ackedqueue.Queueable;
+import org.logstash.ackedqueue.SequencedList;
+import org.logstash.ackedqueue.StringElement;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.List;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+public class MemoryPageIOStreamTest {
+
+    private final int CAPACITY = 1024;
+    private final int EMPTY_HEADER_SIZE = Integer.BYTES + Integer.BYTES;
+
+    private byte[] empty_page_with_header() {
+        byte[] result = new byte[CAPACITY];
+        // version = 1, details = ABC
+        ByteBuffer.wrap(result).put(new byte[]{0, 0, 0, 1, 0, 0, 0, 3, 65, 66, 67});
+        return result;
+    }
+
+    private MemoryPageIOStream subject() throws IOException {
+        return subject(CAPACITY);
+    }
+
+    private MemoryPageIOStream subject(int size) throws IOException {
+        MemoryPageIOStream io = new MemoryPageIOStream(size);
+        io.create();
+        return io;
+    }
+
+    private MemoryPageIOStream subject(byte[] bytes, long seqNum, int count) throws IOException {
+        MemoryPageIOStream io = new MemoryPageIOStream(bytes.length, bytes);
+        io.open(seqNum, count);
+        return io;
+    }
+
+    private Queueable buildStringElement(String str) {
+        return new StringElement(str);
+    }
+
+    @Test
+    public void getWritePosition() throws Exception {
+        assertThat(subject().getWritePosition(), is(equalTo(EMPTY_HEADER_SIZE)));
+        assertThat(subject(empty_page_with_header(), 1L, 0).getWritePosition(), is(equalTo(EMPTY_HEADER_SIZE + 3)));
+    }
+
+    @Test
+    public void getElementCount() throws Exception {
+        assertThat(subject().getElementCount(), is(equalTo(0)));
+        assertThat(subject(empty_page_with_header(), 1L, 0).getElementCount(), is(equalTo(0)));
+    }
+
+    @Test
+    public void getStartSeqNum() throws Exception {
+        assertThat(subject().getMinSeqNum(), is(equalTo(1L)));
+        assertThat(subject(empty_page_with_header(), 1L, 0).getMinSeqNum(), is(equalTo(1L)));
+    }
+
+    @Test
+    public void readHeaderDetails() throws Exception {
+        MemoryPageIOStream io = new MemoryPageIOStream(CAPACITY);
+        io.setPageHeaderDetails("ABC");
+        io.create();
+        assertThat(io.readHeaderDetails(), is(equalTo("ABC")));
+        assertThat(io.getWritePosition(), is(equalTo(EMPTY_HEADER_SIZE + 3)));
+    }
+
+    @Test
+    public void hasSpace() throws Exception {
+        assertThat(subject().hasSpace(10), is(true));
+    }
+
+    @Test
+    public void write() throws Exception {
+        long seqNum = 42L;
+        Queueable element = new StringElement("foobarbaz");
+        MemoryPageIOStream subj = subject();
+        subj.write(element.serialize(), seqNum);
+        assertThat(subj.getElementCount(), is(equalTo(1)));
+        assertThat(subj.getMinSeqNum(), is(equalTo(seqNum)));
+    }
+
+    @Test
+    public void writeUntilFull() throws Exception {
+        long seqNum = 42L;
+        Queueable element = new StringElement("foobarbaz");
+        byte[] data = element.serialize();
+        int bufferSize = 120;
+        MemoryPageIOStream subj = subject(bufferSize);
+        while (subj.hasSpace(data.length)) {
+            subj.write(data, seqNum);
+            seqNum++;
+        }
+        int recordSize = subj.persistedByteCount(data.length);
+        int remains = bufferSize - subj.getWritePosition();
+        assertThat(recordSize, is(equalTo(25))); // element=9 + seqnum=8 + length=4 + crc=4
+        assertThat(subj.getElementCount(), is(equalTo(4)));
+        boolean noSpaceLeft = remains < recordSize;
+        assertThat(noSpaceLeft, is(true));
+    }
+
+    @Test
+    public void read() throws Exception {
+        MemoryPageIOStream subj = subject();
+        SequencedList<byte[]> result = subj.read(1L, 1);
+        assertThat(result.getElements().isEmpty(), is(true));
+    }
+
+    @Test
+    public void writeRead() throws Exception {
+        long seqNum = 42L;
+        Queueable element = buildStringElement("foobarbaz");
+        MemoryPageIOStream subj = subject();
+        subj.write(element.serialize(), seqNum);
+        SequencedList<byte[]> result = subj.read(seqNum, 1);
+        assertThat(result.getElements().size(), is(equalTo(1)));
+        Queueable readElement = StringElement.deserialize(result.getElements().get(0));
+        assertThat(result.getSeqNums().get(0), is(equalTo(seqNum)));
+        assertThat(readElement.toString(), is(equalTo(element.toString())));
+    }
+
+    @Test
+    public void writeReadEmptyElement() throws Exception {
+        long seqNum = 1L;
+        Queueable element = buildStringElement("");
+        MemoryPageIOStream subj = subject();
+        subj.write(element.serialize(), seqNum);
+        SequencedList<byte[]> result = subj.read(seqNum, 1);
+        assertThat(result.getElements().size(), is(equalTo(1)));
+        Queueable readElement = StringElement.deserialize(result.getElements().get(0));
+        assertThat(result.getSeqNums().get(0), is(equalTo(seqNum)));
+        assertThat(readElement.toString(), is(equalTo(element.toString())));
+    }
+
+    @Test
+    public void writeReadMulti() throws Exception {
+        Queueable element1 = buildStringElement("foo");
+        Queueable element2 = buildStringElement("bar");
+        Queueable element3 = buildStringElement("baz");
+        Queueable element4 = buildStringElement("quux");
+        MemoryPageIOStream subj = subject();
+        subj.write(element1.serialize(), 40L);
+        subj.write(element2.serialize(), 42L);
+        subj.write(element3.serialize(), 44L);
+        subj.write(element4.serialize(), 46L);
+        int batchSize = 11;
+        SequencedList<byte[]> result = subj.read(40L, batchSize);
+        assertThat(result.getElements().size(), is(equalTo(4)));
+
+        assertThat(result.getSeqNums().get(0), is(equalTo(40L)));
+        assertThat(result.getSeqNums().get(1), is(equalTo(42L)));
+        assertThat(result.getSeqNums().get(2), is(equalTo(44L)));
+        assertThat(result.getSeqNums().get(3), is(equalTo(46L)));
+
+        assertThat(StringElement.deserialize(result.getElements().get(0)).toString(), is(equalTo(element1.toString())));
+        assertThat(StringElement.deserialize(result.getElements().get(1)).toString(), is(equalTo(element2.toString())));
+        assertThat(StringElement.deserialize(result.getElements().get(2)).toString(), is(equalTo(element3.toString())));
+        assertThat(StringElement.deserialize(result.getElements().get(3)).toString(), is(equalTo(element4.toString())));
+    }
+
+    @Test
+    public void readFromFirstUnackedSeqNum() throws Exception {
+        long seqNum = 10L;
+        String[] values = new String[]{"aaa", "bbb", "ccc", "ddd", "eee", "fff", "ggg", "hhh", "iii", "jjj"};
+        MemoryPageIOStream stream = subject(300);
+        for (String val : values) {
+            Queueable element = buildStringElement(val);
+            stream.write(element.serialize(), seqNum);
+            seqNum++;
+        }
+        MemoryPageIOStream subj = subject(stream.getBuffer(), 10L, 10);
+        int batchSize = 3;
+        seqNum = 13L;
+        SequencedList<byte[]> result = subj.read(seqNum, batchSize);
+        for (int i = 0; i < 3; i++) {
+            Queueable ele = StringElement.deserialize(result.getElements().get(i));
+            assertThat(result.getSeqNums().get(i), is(equalTo(seqNum + i)));
+            assertThat(ele.toString(), is(equalTo(values[i + 3])));
+        }
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/config/ir/IRHelpers.java b/logstash-core/src/test/java/org/logstash/config/ir/IRHelpers.java
new file mode 100644
index 00000000000..78622a8709a
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/config/ir/IRHelpers.java
@@ -0,0 +1,27 @@
+package org.logstash.config.ir;
+
+import org.hamcrest.MatcherAssert;
+import org.logstash.config.ir.graph.Edge;
+import org.logstash.config.ir.graph.Graph;
+
+import java.util.stream.Stream;
+
+/**
+ * Created by andrewvc on 9/19/16.
+ */
+public class IRHelpers {
+    public static void assertSyntaxEquals(ISourceComponent left, ISourceComponent right) {
+        String message = String.format("Expected '%s' to equal '%s'", left, right);
+        MatcherAssert.assertThat(message, left.sourceComponentEquals(right));
+    }
+
+    public static void assertGraphEquals(Graph left, Graph right) {
+        String message = String.format("Expected '%s' to equal '%s'\n%s", left, right, left.diff(right));
+
+        MatcherAssert.assertThat(message, left.sourceComponentEquals(right));
+    }
+
+
+
+
+}
diff --git a/logstash-core/src/test/java/org/logstash/config/ir/PipelineRunnerTest.java b/logstash-core/src/test/java/org/logstash/config/ir/PipelineRunnerTest.java
new file mode 100644
index 00000000000..0c731d335cd
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/config/ir/PipelineRunnerTest.java
@@ -0,0 +1,158 @@
+package org.logstash.config.ir;
+
+import org.junit.Test;
+import org.logstash.Event;
+import org.logstash.config.pipeline.PassthroughProcessor;
+import org.logstash.config.pipeline.Pipeline;
+import org.logstash.config.pipeline.PipelineRunner;
+import org.logstash.config.compiler.*;
+import org.logstash.config.compiler.compiled.ICompiledInputPlugin;
+import org.logstash.config.compiler.compiled.ICompiledProcessor;
+import org.logstash.config.pipeline.pipette.*;
+import org.logstash.config.ir.graph.Graph;
+import org.logstash.config.ir.graph.PluginVertex;
+
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.SynchronousQueue;
+import java.util.concurrent.ThreadLocalRandom;
+import java.util.stream.Collectors;
+
+import static org.logstash.config.ir.DSL.*;
+import static org.logstash.config.ir.PluginDefinition.Type.*;
+
+/**
+ * Created by andrewvc on 10/11/16.
+ */
+public class PipelineRunnerTest {
+    @Test
+    public void testPipelineRunner() throws InvalidIRException, CompilationError, PipetteExecutionException, InterruptedException {
+        Graph inputSection = iComposeParallel(iPlugin(INPUT, "generator"), iPlugin(INPUT, "stdin")).toGraph();
+        Graph filterSection = iIf(eEq(eEventValue("[foo]"), eEventValue("[bar]")),
+                iPlugin(FILTER, "grok"),
+                iPlugin(FILTER, "kv")).toGraph();
+        Graph outputSection = iIf(eGt(eEventValue("[baz]"), eValue(1000)),
+                iComposeParallel(
+                        iPlugin(OUTPUT, "s3"),
+                        iPlugin(OUTPUT, "elasticsearch")),
+                iPlugin(OUTPUT, "stdout")).toGraph();
+
+        Pipeline pipeline = new Pipeline(inputSection, filterSection, outputSection);
+
+        BlockingQueue<List<Event>> queue = new SynchronousQueue<>();
+
+        IExpressionCompiler expressionCompiler = new RubyExpressionCompiler();
+        IPluginCompiler pluginCompiler = new IPluginCompiler() {
+            @Override
+            public ICompiledInputPlugin compileInput(PluginVertex vertex) throws CompilationError {
+                return new ICompiledInputPlugin() {
+                    public PipetteSourceEmitter onWriteWriter;
+
+                    @Override
+                    public void register() {
+
+                    }
+
+                    public Event generateEvent() {
+                        Event e = new Event();
+                        int seed = ThreadLocalRandom.current().nextInt(1,100);
+                        e.setField("[foo]", (seed % 2 == 0 ? "String1" : "String2"));
+                        e.setField("[bar]", (seed % 2 == 0 ? "String1" : "String2"));
+                        e.setField("[baz]", (seed % 2 == 0 ? 500 : 5000));
+
+                        List<Event> events = Collections.singletonList(e);
+
+                        return e;
+                    }
+
+                    public List<Event> generateEvents() {
+                        return Collections.nCopies(2, ThreadLocalRandom.current().nextInt(5,10)).
+                                stream().
+                                map(i -> generateEvent()).
+                                collect(Collectors.toList());
+                    }
+
+                    @Override
+                    public void start() throws PipetteExecutionException {
+                        for (int i = 0; i < 10; i++) {
+                            onWriteWriter.emit(generateEvents());
+                        }
+                    }
+
+                    @Override
+                    public void stop() {
+
+                    }
+
+                    @Override
+                    public void onEvents(PipetteSourceEmitter sourceWriter) {
+                        this.onWriteWriter = sourceWriter;
+                    }
+                };
+            }
+
+            @Override
+            public ICompiledProcessor compileFilter(PluginVertex vertex) throws CompilationError {
+                return new PassthroughProcessor(vertex);
+            }
+
+            @Override
+            public ICompiledProcessor compileOutput(PluginVertex vertex) throws CompilationError {
+                return new PassthroughProcessor(vertex);
+            }
+        };
+
+        IPipetteConsumerFactory queueWriterFactory = () -> new IPipetteConsumer() {
+            @Override
+            public void process(List<Event> events) throws PipetteExecutionException {
+                try {
+                    System.out.println("WRITEQ");
+                    queue.put(events);
+                } catch (InterruptedException e) {
+                    throw new PipetteExecutionException(e.getMessage(), e);
+                }
+            }
+
+            @Override
+            public void stop() throws PipetteExecutionException {
+
+            }
+        };
+
+        IPipetteProducerFactory queueReaderFactory = new IPipetteProducerFactory() {
+            @Override
+            public IPipetteProducer make() {
+                return new IPipetteProducer() {
+                    public PipetteSourceEmitter sourceEmitter;
+
+                    @Override
+                    public void start() throws PipetteExecutionException {
+                        while (true) {
+                            try {
+                                List<Event> events = queue.take();
+                                System.out.println("READQ" + events);
+                                sourceEmitter.emit(events);
+                            } catch (InterruptedException e) {
+                                throw new PipetteExecutionException(e.getMessage(), e);
+                            }
+                        }
+                    }
+
+                    @Override
+                    public void stop() {
+                    }
+
+                    @Override
+                    public void onEvents(PipetteSourceEmitter sourceEmitter) {
+                        this.sourceEmitter = sourceEmitter;
+                    }
+                };
+            }
+        };
+
+        //PipelineRunner runner = new PipelineRunner(pipeline, queueWriterFactory, queueReaderFactory, expressionCompiler, pluginCompiler, null);
+        //runner.start(1);
+        //runner.join();
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/config/ir/PipelineTest.java b/logstash-core/src/test/java/org/logstash/config/ir/PipelineTest.java
new file mode 100644
index 00000000000..0104f753d10
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/config/ir/PipelineTest.java
@@ -0,0 +1,29 @@
+package org.logstash.config.ir;
+
+import org.junit.Test;
+import org.logstash.config.ir.graph.Graph;
+import org.logstash.config.pipeline.Pipeline;
+
+import static org.logstash.config.ir.DSL.*;
+import static org.logstash.config.ir.PluginDefinition.Type.*;
+
+/**
+ * Created by andrewvc on 9/20/16.
+ */
+public class PipelineTest {
+    @Test
+    public void testPipelineCreation() throws InvalidIRException {
+        Graph inputSection = iComposeParallel(iPlugin(INPUT, "generator"), iPlugin(INPUT, "stdin")).toGraph();
+        Graph filterSection = iIf(eEq(eEventValue("[foo]"), eEventValue("[bar]")),
+                                    iPlugin(FILTER, "grok"),
+                                    iPlugin(FILTER, "kv")).toGraph();
+        Graph outputSection = iIf(eGt(eEventValue("[baz]"), eValue(1000)),
+                                    iComposeParallel(
+                                            iPlugin(OUTPUT, "s3"),
+                                            iPlugin(OUTPUT, "elasticsearch")),
+                                    iPlugin(OUTPUT, "stdout")).toGraph();
+
+        Pipeline pipeline = new Pipeline(inputSection, filterSection, outputSection);
+        System.out.println(pipeline.toString());
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/config/ir/compiler/ExpressionCompilerTest.java b/logstash-core/src/test/java/org/logstash/config/ir/compiler/ExpressionCompilerTest.java
new file mode 100644
index 00000000000..881c4c09edf
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/config/ir/compiler/ExpressionCompilerTest.java
@@ -0,0 +1,67 @@
+package org.logstash.config.ir.compiler;
+
+import org.junit.Test;
+import org.logstash.Event;
+import org.logstash.config.compiler.CompilationError;
+import org.logstash.config.compiler.compiled.ICompiledExpression;
+import org.logstash.config.compiler.RubyExpressionCompiler;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.expression.Expression;
+
+import static junit.framework.TestCase.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.logstash.config.ir.DSL.*;
+
+
+/**
+ * Created by andrewvc on 10/11/16.
+ */
+public class ExpressionCompilerTest {
+    @Test
+    public void testSimpleEquality() throws InvalidIRException, CompilationError {
+        assertExpressionTrue(eEq(eValue(1), eValue(1)));
+        assertExpressionFalse(eEq(eValue(1), eValue(2)));
+    }
+
+    @Test
+    public void testFieldComparison() throws InvalidIRException, CompilationError {
+        Event event = new Event();
+        event.setField("[foo]", "bar");
+
+        assertExpressionTrue(eEq(eEventValue("foo"), eValue("bar")), event);
+        assertExpressionFalse(eEq(eEventValue("foo"), eValue("WRONG!")), event);
+    }
+
+    @Test
+    public void testNested() throws InvalidIRException, CompilationError {
+        assertExpressionTrue(eAnd(
+                eGt(eValue(2), eValue(1)),
+                eLt(eValue(100), eValue(1000))));
+
+        assertExpressionFalse(eOr(
+                eGt(eValue(-1), eValue(1)),
+                eLt(eValue(100000), eValue(1000))));
+    }
+
+    public void assertExpressionTrue(Expression expression) throws CompilationError {
+        assertExpressionTrue(expression, new Event());
+    }
+
+    public void assertExpressionTrue(Expression expression, Event event) throws CompilationError {
+        assertTrue("Expected expr to be true: " + expression.toRubyString() + " Event: " + event.getData(), runExpression(expression, event));
+    }
+
+    public void assertExpressionFalse(Expression expression) throws CompilationError {
+        assertExpressionFalse(expression, new Event());
+    }
+
+    public void assertExpressionFalse(Expression expression, Event event) throws CompilationError {
+       assertFalse("Expected expr to be false: " + expression.toRubyString() + " Event: " + event.getData(), runExpression(expression, event));
+    }
+
+    public boolean runExpression(Expression expression, Event event) throws CompilationError {
+        RubyExpressionCompiler expressionCompiler = new RubyExpressionCompiler();
+        ICompiledExpression compiled = expressionCompiler.compile(expression);
+        return compiled.execute(event);
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/config/ir/graph/GraphTest.java b/logstash-core/src/test/java/org/logstash/config/ir/graph/GraphTest.java
new file mode 100644
index 00000000000..f36fd803879
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/config/ir/graph/GraphTest.java
@@ -0,0 +1,7 @@
+package org.logstash.config.ir.graph;
+
+/**
+ * Created by andrewvc on 9/19/16.
+ */
+public class GraphTest {
+}
diff --git a/logstash-core/src/test/java/org/logstash/config/ir/imperative/DSLTest.java b/logstash-core/src/test/java/org/logstash/config/ir/imperative/DSLTest.java
new file mode 100644
index 00000000000..d23b10e4758
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/config/ir/imperative/DSLTest.java
@@ -0,0 +1,61 @@
+package org.logstash.config.ir.imperative;
+
+import org.junit.Test;
+import org.logstash.config.ir.SourceComponent;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.SourceMetadata;
+
+import java.util.ArrayList;
+
+import static org.logstash.config.ir.DSL.*;
+import static org.logstash.config.ir.IRHelpers.assertSyntaxEquals;
+import static org.logstash.config.ir.PluginDefinition.Type.*;
+
+/**
+ * Created by andrewvc on 9/13/16.
+ */
+public class DSLTest {
+    @Test
+    public void testDSLOnePluginEquality() {
+        assertSyntaxEquals(iPlugin(FILTER, "foo"), iPlugin(FILTER, "foo"));
+    }
+
+    @Test
+    public void testComposedPluginEquality() throws InvalidIRException {
+        assertSyntaxEquals(composedPlugins(), composedPlugins());
+    }
+
+    @Test
+    public void testDSLComplexEquality() throws InvalidIRException {
+        assertSyntaxEquals(complexExpression(), complexExpression());
+    }
+
+    @Test
+    public void testComposeSingle() throws InvalidIRException {
+        assertSyntaxEquals(iPlugin(FILTER, "grok"), iComposeSequence(iPlugin(FILTER, "grok")));
+    }
+
+    @Test
+    public void testComposeMulti() throws InvalidIRException {
+        Statement composed = iComposeSequence(iPlugin(FILTER, "grok"), iPlugin(FILTER, "foo"));
+        assertSyntaxEquals(iComposeSequence(iPlugin(FILTER, "grok"), iPlugin(FILTER, "foo")), composed);
+}
+
+
+    public SourceComponent composedPlugins() throws InvalidIRException {
+        return iComposeSequence(iPlugin(FILTER, "json"), iPlugin(FILTER, "stuff"));
+    }
+
+    public SourceComponent complexExpression() throws InvalidIRException {
+        return iComposeSequence(
+                iPlugin(FILTER, "grok"),
+                iPlugin(FILTER, "kv"),
+                iIf(eAnd(eTruthy(eValue(5l)), eTruthy(eValue(null))),
+                        iPlugin(FILTER, "grok"),
+                        iComposeSequence(iPlugin(FILTER, "json"), iPlugin(FILTER, "stuff"))
+                )
+        );
+    }
+
+
+}
diff --git a/logstash-core/src/test/java/org/logstash/config/ir/imperative/ImperativeToGraphtest.java b/logstash-core/src/test/java/org/logstash/config/ir/imperative/ImperativeToGraphtest.java
new file mode 100644
index 00000000000..b3e3ddd4dff
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/config/ir/imperative/ImperativeToGraphtest.java
@@ -0,0 +1,105 @@
+package org.logstash.config.ir.imperative;
+
+import org.junit.Test;
+import org.logstash.config.ir.InvalidIRException;
+import org.logstash.config.ir.graph.Graph;
+import org.logstash.config.ir.graph.IfVertex;
+import org.logstash.config.ir.graph.PluginVertex;
+
+import static org.logstash.config.ir.DSL.*;
+import static org.logstash.config.ir.IRHelpers.assertGraphEquals;
+import static org.logstash.config.ir.PluginDefinition.Type.*;
+
+/**
+ * Created by andrewvc on 9/15/16.
+ */
+public class ImperativeToGraphtest {
+
+    @Test
+    public void convertSimpleExpression() throws InvalidIRException {
+        Graph g =  iComposeSequence(iPlugin(FILTER, "json"), iPlugin(FILTER, "stuff")).toGraph();
+        Graph expected = graph().threadVertices(gPlugin(FILTER, "json"), gPlugin(FILTER, "stuff"));
+
+        assertGraphEquals(g, expected);
+    }
+
+    @Test
+    public void convertComplexExpression() throws InvalidIRException {
+        Graph generated = iComposeSequence(
+                iPlugin(FILTER, "p1"),
+                iPlugin(FILTER, "p2"),
+                iIf(eAnd(eTruthy(eValue(5l)), eTruthy(eValue(null))),
+                        iPlugin(FILTER, "p3"),
+                        iComposeSequence(iPlugin(FILTER, "p4"), iPlugin(FILTER, "p5"))
+                )
+        ).toGraph();
+
+        PluginVertex p1 = gPlugin(FILTER, "p1");
+        PluginVertex p2 = gPlugin(FILTER, "p2");
+        PluginVertex p3 = gPlugin(FILTER, "p3");
+        PluginVertex p4 = gPlugin(FILTER, "p4");
+        PluginVertex p5 = gPlugin(FILTER, "p5");
+        IfVertex testIf = gIf(eAnd(eTruthy(eValue(5l)), eTruthy(eValue(null))));
+
+        Graph expected = graph().threadVertices(p1,p2,testIf)
+            .threadVertices(true, testIf, p3)
+            .threadVertices(false, testIf, p4)
+            .threadVertices(p4, p5);
+
+        //PluginVertex p6 = gPlugin(FILTER, "p6");
+        //expected.threadVertices(p5,p6);
+
+        assertGraphEquals(generated, expected);
+    }
+
+    // This is a good test for what the filter block will do, where there
+    // will be a  composed set of ifs potentially, all of which must terminate at a
+    // single node
+    @Test
+    public void convertComplexExpressionWithTerminal() throws InvalidIRException {
+        Graph generated = iComposeSequence(
+            iPlugin(FILTER, "p1"),
+            iIf(eTruthy(eValue(1)),
+                iComposeSequence(
+                    iIf(eTruthy(eValue(2)), noop(), iPlugin(FILTER, "p2")),
+                    iIf(eTruthy(eValue(3)), iPlugin(FILTER, "p3"), noop())
+                ),
+                iComposeSequence(
+                    iIf(eTruthy(eValue(4)), iPlugin(FILTER, "p4")),
+                    iPlugin(FILTER, "p5")
+                )
+            ),
+            iPlugin(FILTER, "terminal")
+        ).toGraph();
+
+        PluginVertex p1 = gPlugin(FILTER,"p1");
+        PluginVertex p2 = gPlugin(FILTER, "p2");
+        PluginVertex p3 = gPlugin(FILTER, "p3");
+        PluginVertex p4 = gPlugin(FILTER, "p4");
+        PluginVertex p5 = gPlugin(FILTER, "p5");
+        PluginVertex terminal = gPlugin(FILTER, "terminal");
+
+        IfVertex if1 = gIf(eTruthy(eValue(1)));
+        IfVertex if2 = gIf(eTruthy(eValue(2)));
+        IfVertex if3 = gIf(eTruthy(eValue(3)));
+        IfVertex if4 = gIf(eTruthy(eValue(4)));
+
+        Graph expected = graph()
+                .threadVertices(p1, if1)
+                .threadVertices(true, if1, if2)
+                .threadVertices(false, if1, if4)
+                .threadVertices(true, if2, if3)
+                .threadVertices(false, if2, p2)
+                .threadVertices(p2, if3)
+                .threadVertices(true, if3, p3)
+                .threadVertices(false, if3, terminal)
+                .threadVertices(p3, terminal)
+                .threadVertices(true, if4, p4)
+                .threadVertices(false, if4, p5)
+                .threadVertices(p4, p5)
+                .threadVertices(p5, terminal);
+
+        assertGraphEquals(generated, expected);
+
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/stress/Concurent.java b/logstash-core/src/test/java/org/logstash/stress/Concurent.java
new file mode 100644
index 00000000000..48a4a5385a6
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/stress/Concurent.java
@@ -0,0 +1,183 @@
+package org.logstash.stress;
+
+import org.logstash.ackedqueue.*;
+import org.logstash.common.io.*;
+
+import java.io.IOException;
+import java.time.Duration;
+import java.time.Instant;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.ConcurrentLinkedQueue;
+import java.util.stream.Collectors;
+
+public class Concurent {
+    final static int ELEMENT_COUNT = 2000000;
+    final static int BATCH_SIZE = 1000;
+    static Settings settings;
+
+    public static Settings memorySettings(int capacity) {
+        Settings s = new MemorySettings();
+        PageIOFactory pageIOFactory = (pageNum, size, path) -> new ByteBufferPageIO(pageNum, size, path);
+        CheckpointIOFactory checkpointIOFactory = (source) -> new MemoryCheckpointIO(source);
+        s.setCapacity(capacity);
+        s.setElementIOFactory(pageIOFactory);
+        s.setCheckpointIOFactory(checkpointIOFactory);
+        s.setElementClass(StringElement.class);
+        return s;
+    }
+
+    public static Settings fileSettings(int capacity) {
+        Settings s = new MemorySettings("/tmp/queue");
+        PageIOFactory pageIOFactory = (pageNum, size, path) -> new MmapPageIO(pageNum, size, path);
+        CheckpointIOFactory checkpointIOFactory = (source) -> new FileCheckpointIO(source);
+        s.setCapacity(capacity);
+        s.setElementIOFactory(pageIOFactory);
+        s.setCheckpointIOFactory(checkpointIOFactory);
+        s.setElementClass(StringElement.class);
+        return s;
+    }
+
+    public static Thread producer(Queue q, List<StringElement> input) {
+        return new Thread(() -> {
+            try {
+                for (StringElement element : input) {
+                    q.write(element);
+                }
+            } catch (IOException e) {
+                throw new RuntimeException(e);
+            }
+        });
+
+    }
+
+    public static void oneProducersOneConsumer() throws IOException, InterruptedException {
+        List<StringElement> input = new ArrayList<>();
+        List<StringElement> output = new ArrayList<>();
+
+        Instant start = Instant.now();
+
+        Queue q = new Queue(settings);
+        q.getCheckpointIO().purge();
+        q.open();
+
+        System.out.print("stating single producers and single consumers stress test... ");
+
+        for (int i = 0; i < ELEMENT_COUNT; i++) {
+            input.add(new StringElement(new Integer(i).toString()));
+        }
+
+        Thread consumer = new Thread(() -> {
+            int consumedCount = 0;
+
+            try {
+                while (consumedCount < ELEMENT_COUNT) {
+                    Batch b = q.readBatch(BATCH_SIZE);
+//                    if (b.getElements().size() < BATCH_SIZE) {
+//                        System.out.println("read small batch=" + b.getElements().size());
+//                    } else {
+//                        System.out.println("read batch size=" + b.getElements().size());
+//                    }
+                    output.addAll((List<StringElement>) b.getElements());
+                    b.close();
+                    consumedCount += b.getElements().size();
+                }
+            } catch (IOException e) {
+                throw new RuntimeException(e);
+            }
+        });
+        consumer.start();
+
+        Thread producer = producer(q, input);
+        producer.start();
+
+        consumer.join();
+        q.close();
+
+        Instant end = Instant.now();
+
+        if (! input.equals(output)) {
+            System.out.println("ERROR: input and output are not equal");
+        } else {
+            System.out.println("SUCCESS, result size=" + output.size() + ", elapsed=" + Duration.between(start, end) + ", rate=" + (new Float(ELEMENT_COUNT) / Duration.between(start, end).toMillis()) * 1000);
+        }
+    }
+
+    public static void oneProducersOneMultipleConsumer() throws IOException, InterruptedException {
+        final List<StringElement> input = new ArrayList<>();
+        final Collection<StringElement> output = new ConcurrentLinkedQueue();
+        final int CONSUMERS = 5;
+        List<Thread> consumers = new ArrayList<>();
+
+        Instant start = Instant.now();
+
+        Queue q = new Queue(settings);
+        q.getCheckpointIO().purge();
+        q.open();
+
+        System.out.print("stating single producers and multiple consumers stress test... ");
+
+        for (int i = 0; i < ELEMENT_COUNT; i++) {
+            input.add(new StringElement(new Integer(i).toString()));
+        }
+
+        for (int i = 0; i < CONSUMERS; i++) {
+            consumers.add(new Thread(() -> {
+                try {
+                    while (output.size() < ELEMENT_COUNT) {
+                        Batch b = q.readBatch(BATCH_SIZE);
+//                        if (b.getElements().size() < BATCH_SIZE) {
+//                            System.out.println("read small batch=" + b.getElements().size());
+//                        } else {
+//                            System.out.println("read batch size=" + b.getElements().size());
+//                        }
+                        output.addAll((List<StringElement>) b.getElements());
+                        b.close();
+                    }
+                    // everything is read, close queue here since other consumers might be blocked trying to get next batch
+                    q.close();
+                } catch (IOException e) {
+                    throw new RuntimeException(e);
+                }
+            }));
+        }
+
+        consumers.forEach(c -> c.start());
+
+        Thread producer = producer(q, input);
+        producer.start();
+
+        // gotta hate exception handling in lambdas
+        consumers.forEach(c -> {try{c.join();} catch(InterruptedException e) {throw new RuntimeException(e);}});
+
+        Instant end = Instant.now();
+
+        List<StringElement> result = output.stream().collect(Collectors.toList());
+        Collections.sort(result, (p1, p2) -> Integer.valueOf(p1.toString()).compareTo(Integer.valueOf(p2.toString())));
+
+        if (! input.equals(result)) {
+            System.out.println("ERROR: input and output are not equal");
+        } else {
+            System.out.println("SUCCESS, result size=" + output.size() + ", elapsed=" + Duration.between(start, end) + ", rate=" + (new Float(ELEMENT_COUNT) / Duration.between(start, end).toMillis()) * 1000);
+        }
+    }
+
+
+    public static void main(String[] args) throws IOException, InterruptedException {
+        System.out.println(">>> starting in-memory stress test");
+
+        settings = memorySettings(1024 * 1024); // 1MB
+        oneProducersOneConsumer();
+        oneProducersOneMultipleConsumer();
+
+        System.out.println("\n>>> starting file-based stress test in /tmp/queue");
+
+        settings = fileSettings(1024 * 1024); // 1MB
+
+        oneProducersOneConsumer();
+        oneProducersOneMultipleConsumer();
+    }
+
+}
diff --git a/logstash-core/src/test/resources/org/logstash/common/io/checkpoint.head b/logstash-core/src/test/resources/org/logstash/common/io/checkpoint.head
new file mode 100644
index 00000000000..d189f0a7fde
Binary files /dev/null and b/logstash-core/src/test/resources/org/logstash/common/io/checkpoint.head differ
diff --git a/rakelib/artifacts.rake b/rakelib/artifacts.rake
index c00202a4d1e..8c33df07552 100644
--- a/rakelib/artifacts.rake
+++ b/rakelib/artifacts.rake
@@ -14,17 +14,28 @@ namespace "artifact" do
       "bin/**/*",
       "config/**/*",
       "data",
+
       "lib/bootstrap/**/*",
       "lib/pluginmanager/**/*",
       "lib/systeminstall/**/*",
+
       "logstash-core/lib/**/*",
       "logstash-core/locales/**/*",
       "logstash-core/vendor/**/*",
       "logstash-core/*.gemspec",
+      "logstash-core/gemspec_jars.rb",
+
       "logstash-core-event-java/lib/**/*",
       "logstash-core-event-java/*.gemspec",
+      "logstash-core-event-java/gemspec_jars.rb",
+
+      "logstash-core-queue-jruby/lib/**/*",
+      "logstash-core-queue-jruby/*.gemspec",
+      "logstash-core-queue-jruby/gemspec_jars.rb",
+
       "logstash-core-plugin-api/lib/**/*",
       "logstash-core-plugin-api/*.gemspec",
+
       "patterns/**/*",
       "vendor/??*/**/*",
       # To include ruby-maven's hidden ".mvn" directory, we need to
diff --git a/rakelib/compile.rake b/rakelib/compile.rake
index e5730125885..b2df88ec222 100644
--- a/rakelib/compile.rake
+++ b/rakelib/compile.rake
@@ -13,7 +13,7 @@ namespace "compile" do
 
   task "logstash-core-java" do
     puts("Building logstash-core using gradle")
-    system("./gradlew", "vendor", "-p", "./logstash-core")
+    system("./gradlew", "jar", "-p", "./logstash-core")
   end
 
   task "logstash-core-event-java" do
@@ -21,6 +21,11 @@ namespace "compile" do
     system("./gradlew", "jar", "-p", "./logstash-core-event-java")
   end
 
+  task "logstash-core-queue-jruby" do
+    puts("Building logstash-core-queue-jruby using gradle")
+    system("./gradlew", "jar", "-p", "./logstash-core-queue-jruby")
+  end
+
   desc "Build everything"
-  task "all" => ["grammar", "logstash-core-java", "logstash-core-event-java"]
+  task "all" => ["grammar", "logstash-core-java", "logstash-core-event-java", "logstash-core-queue-jruby"]
 end
diff --git a/rakelib/test.rake b/rakelib/test.rake
index a02a5c5d65a..503bec6c618 100644
--- a/rakelib/test.rake
+++ b/rakelib/test.rake
@@ -6,6 +6,12 @@ require "pluginmanager/util"
 namespace "test" do
 
   task "setup" do
+
+    # make sure we have a ./data/queue dir here
+    # temporary wiring until we figure proper queue initialization sequence and in test context etc.
+    mkdir "data" unless File.directory?("data")
+    mkdir "data/queue" unless File.directory?("data/queue")
+
     # Need to be run here as because if run aftewarse (after the bundler.setup task) then the report got wrong
     # numbers and misses files. There is an issue with our setup! method as this does not happen with the regular
     # bundler.setup used in regular bundler flows.
@@ -13,6 +19,7 @@ namespace "test" do
 
     require "bootstrap/environment"
     LogStash::Bundler.setup!({:without => [:build]})
+    require "logstash-core"
 
     require "rspec/core/runner"
     require "rspec"
diff --git a/spec/spec_helper.rb b/spec/spec_helper.rb
index 9bec90f640c..0fbe7f3b605 100644
--- a/spec/spec_helper.rb
+++ b/spec/spec_helper.rb
@@ -21,8 +21,43 @@ def puts(payload)
 
 RSpec.configure do |c|
   Flores::RSpec.configure(c)
+  c.before do
+    # TODO: commented out on post-merged in master - the logger has moved to log4j
+    #
+    #
+    # Force Cabin to always have a JSON subscriber.  The main purpose of this
+    # is to catch crashes in json serialization for our logs. JSONIOThingy
+    # exists to validate taht what LogStash::Logging::JSON emits is always
+    # valid JSON.
+    # jsonvalidator = JSONIOThingy.new
+    # allow(Cabin::Channel).to receive(:new).and_wrap_original do |m, *args|
+    #   logger = m.call(*args)
+    #   logger.level = :debug
+    #   logger.subscribe(LogStash::Logging::JSON.new(jsonvalidator))
+    #
+    #   logger
+    # end
+
+    LogStash::SETTINGS.set("queue.type", "memory_acked")
+    LogStash::SETTINGS.set("queue.page_capacity", 1024 * 1024)
+    LogStash::SETTINGS.set("queue.max_events", 250)
+  end
 end
 
 def installed_plugins
   Gem::Specification.find_all.select { |spec| spec.metadata["logstash_plugin"] }.map { |plugin| plugin.name }
 end
+
+RSpec::Matchers.define :ir_eql do |expected|
+  match do |actual|
+    if expected.java_kind_of?(org.logstash.config.ir.SourceComponent) && actual.java_kind_of?(org.logstash.config.ir.SourceComponent)
+      expected.sourceComponentEquals(actual)
+    else
+      return false
+    end    
+  end
+  
+  failure_message do |actual|
+    "actual value \n#{actual.to_s}\nis not .sourceComponentEquals to the expected value: \n#{expected.to_s}\n"
+  end
+end
diff --git a/spec/unit/license_spec.rb b/spec/unit/license_spec.rb
index 9425cfc9111..88fe74d4ba3 100644
--- a/spec/unit/license_spec.rb
+++ b/spec/unit/license_spec.rb
@@ -14,7 +14,8 @@
                    /bsd/,
                    /artistic 2.*/,
                    /ruby/,
-                   /lgpl/])
+                   /lgpl/,
+                   /epl/])
   }
 
   ##
diff --git a/versions.yml b/versions.yml
index 87670bec259..1c44f7e4c39 100644
--- a/versions.yml
+++ b/versions.yml
@@ -3,4 +3,5 @@ logstash: 6.0.0-alpha1
 logstash-core: 6.0.0-alpha1
 logstash-core-event: 6.0.0-alpha1
 logstash-core-event-java: 6.0.0-alpha1
+logstash-core-queue-jruby: 6.0.0-alpha1
 logstash-core-plugin-api: 2.1.16
