diff --git a/lib/logstash/outputs/s3.rb b/lib/logstash/outputs/s3.rb
index c493d29b526..eaaa7b91d2b 100644
--- a/lib/logstash/outputs/s3.rb
+++ b/lib/logstash/outputs/s3.rb
@@ -1,151 +1,127 @@
 # encoding: utf-8
+
+# TODO integrate aws_config in the future
+# require "logstash/plugin_mixins/aws_config"
 require "logstash/outputs/base"
 require "logstash/namespace"
 require "socket" # for Socket.gethostname
 
-# TODO integrate aws_config in the future
-#require "logstash/plugin_mixins/aws_config"
-
-# INFORMATION:
-
-# This plugin was created for store the logstash's events into Amazon Simple Storage Service (Amazon S3).
-# For use it you needs authentications and an s3 bucket.
-# Be careful to have the permission to write file on S3's bucket and run logstash with super user for establish connection.
-
-# S3 plugin allows you to do something complex, let's explain:)
-
-# S3 outputs create temporary files into "/opt/logstash/S3_temp/". If you want, you can change the path at the start of register method.
-# This files have a special name, for example:
-
-# ls.s3.ip-10-228-27-95.2013-04-18T10.00.tag_hello.part0.txt
-
-# ls.s3 : indicate logstash plugin s3
-
-# "ip-10-228-27-95" : indicate you ip machine, if you have more logstash and writing on the same bucket for example.
-# "2013-04-18T10.00" : represents the time whenever you specify time_file.
-# "tag_hello" : this indicate the event's tag, you can collect events with the same tag.
-# "part0" : this means if you indicate size_file then it will generate more parts if you file.size > size_file.
-#           When a file is full it will pushed on bucket and will be deleted in temporary directory.
-#           If a file is empty is not pushed, but deleted.
-
-# This plugin have a system to restore the previous temporary files if something crash.
-
-##[Note] :
-
-## If you specify size_file and time_file then it will create file for each tag (if specified), when time_file or
-## their size > size_file, it will be triggered then they will be pushed on s3's bucket and will delete from local disk.
-
-## If you don't specify size_file, but time_file then it will create only one file for each tag (if specified).
-## When time_file it will be triggered then the files will be pushed on s3's bucket and delete from local disk.
-
-## If you don't specify time_file, but size_file  then it will create files for each tag (if specified),
-## that will be triggered when their size > size_file, then they will be pushed on s3's bucket and will delete from local disk.
-
-## If you don't specific size_file and time_file you have a curios mode. It will create only one file for each tag (if specified).
-## Then the file will be rest on temporary directory and don't will be pushed on bucket until we will restart logstash.
-
-# INFORMATION ABOUT CLASS:
-
-# I tried to comment the class at best i could do.
-# I think there are much thing to improve, but if you want some points to develop here a list:
-
-# TODO Integrate aws_config in the future
-# TODO Find a method to push them all files when logtstash close the session.
-# TODO Integrate @field on the path file
-# TODO Permanent connection or on demand? For now on demand, but isn't a good implementation.
-#      Use a while or a thread to try the connection before break a time_out and signal an error.
-# TODO If you have bugs report or helpful advice contact me, but remember that this code is much mine as much as yours,
-#      try to work on it if you want :)
-
-
+# This plugin was created to store logstash events into Amazon Simple
+# Storage Service (Amazon S3). To use it, you need an AWS account with credentials to write to an S3 bucket.
+#
+# Output details:
+#
+# This output creates temporary files into `/opt/logstash/S3_temp/`. These files have a special name, for example:
+#     ls.s3.ip-10-228-27-95.2013-04-18T10.00.tag_hello.part0.txt
+#
+# * `ls.s3`: Prefix indicating Logstash S3 plugin.
+# * `ip-10-228-27-95`: The IP of the logstash machine. If you have multiple machines running logstash, this can help you distinguish which host sent the event.
+# `2013-04-18T10.00`: Time the file was created, for use with time_file.
+# `tag_hello`: Tag of the events contained in the file.
+# `part0`: The part of the file when `size_file` is used to split files based on size. When a temporary file reaches size of `size_file` it will pushed into the bucket and deleted from the temporary directory. Files empty at `time_file` interval are deleted.
+#
+# This plugin has a system to restore the previous temporary files if something crashes.
+#
+# `time_file` and `size_file` behavior:
+#
+# If you specify `size_file` and `time_file` then it will create `.partN` files for each tag (if
+# specified). When `time_file` is reached or their size > `size_file`, a new file will be created, and the existing files
+# will be uploaded to s3 and then deleted from disk.
+#
+# If you specify only `time_file` then it will create a file for each tag
+# (if specified). When `time_file` is reached, a new file will be created, and the existing files
+# will be uploaded to s3 and then deleted from disk.
+#
+# If you specify only `size_file` then it will create `.partN` files for each tag (if specified).
+# When their size > `size_file`, a new file will be created, and the existing files
+# will be uploaded to s3 and then deleted from disk.
+#
+# If you don't specify `size_file` or `time_file` you have a curious mode. It will create a file for each tag (if
+# specified). Then the file will stay in the temporary directory without being uploaded to s3 until logstash is restarted.
+#
+# Improvements welcome:
+#  * Integrate aws_config in the future
+#  * Find a method to upload all files when logtstash closes the session.
+#  * Integrate @field on the path file
+#  * Permanent connection or on demand? For now on demand, but isn't a good implementation.
+#  * Use a while or a thread to try the connection before break a time_out and signal an error.
+#
+#  If you have bugs report or helpful advice contact me, but remember that this
+#  code is much mine as much as yours, try to work on it if you want :)
+#
 # USAGE:
-
+#
 # This is an example of logstash config:
-
-# output {
-#    s3{
-#      access_key_id => "crazy_key"             (required)
-#      secret_access_key => "monkey_access_key" (required)
-#      endpoint_region => "eu-west-1"           (required)
-#      bucket => "boss_please_open_your_bucket" (required)
-#      size_file => 2048                        (optional)
-#      time_file => 5                           (optional)
-#      format => "plain"                        (optional)
-#      canned_acl => "private"                  (optional. Options are "private", "public_read", "public_read_write", "authenticated_read". Defaults to "private" )
-#    }
-# }
-
-# We analize this:
-
+#
+#     output {
+#        s3{
+#          access_key_id => "crazy_key"             (required)
+#          secret_access_key => "monkey_access_key" (required)
+#          endpoint_region => "eu-west-1"           (required)
+#          bucket => "boss_please_open_your_bucket" (required)
+#          size_file => 2048                        (optional)
+#          time_file => 5                           (optional)
+#          format => "plain"                        (optional)
+#          canned_acl => "private"                  (optional. Options are "private", "public_read", "public_read_write", "authenticated_read". Defaults to "private" )
+#        }
+#     }
+#
 # access_key_id => "crazy_key"
 # Amazon will give you the key for use their service if you buy it or try it. (not very much open source anyway)
-
 # secret_access_key => "monkey_access_key"
 # Amazon will give you the secret_access_key for use their service if you buy it or try it . (not very much open source anyway).
-
 # endpoint_region => "eu-west-1"
 # When you make a contract with Amazon, you should know where the services you use.
-
 # bucket => "boss_please_open_your_bucket"
 # Be careful you have the permission to write on bucket and know the name.
-
 # size_file => 2048
 # Means the size, in KB, of files who can store on temporary directory before you will be pushed on bucket.
 # Is useful if you have a little server with poor space on disk and you don't want blow up the server with unnecessary temporary log files.
-
 # time_file => 5
 # Means, in minutes, the time  before the files will be pushed on bucket. Is useful if you want to push the files every specific time.
-
 # format => "plain"
 # Means the format of events you want to store in the files
-
 # canned_acl => "private"
 # The S3 canned ACL to use when putting the file. Defaults to "private".
 
-# LET'S ROCK AND ROLL ON THE CODE!
-
 class LogStash::Outputs::S3 < LogStash::Outputs::Base
- #TODO integrate aws_config in the future
- #  include LogStash::PluginMixins::AwsConfig
+  # TODO integrate aws_config in the future
+  # include LogStash::PluginMixins::AwsConfig
 
  config_name "s3"
  milestone 1
 
- # Aws access_key.
+ # AWS access_key_id
  config :access_key_id, :validate => :string
 
- # Aws secret_access_key
+ # AWS secret_access_key
  config :secret_access_key, :validate => :string
 
- # S3 bucket
+ # S3 bucket to place files into.
  config :bucket, :validate => :string
 
- # Aws endpoint_region
+ # S3 region to use.
  config :endpoint_region, :validate => ["us-east-1", "us-west-1", "us-west-2",
                                         "eu-west-1", "ap-southeast-1", "ap-southeast-2",
                                         "ap-northeast-1", "sa-east-1", "us-gov-west-1"], :default => "us-east-1"
 
- # Set the size of file in KB, this means that files on bucket when have dimension > file_size, they are stored in two or more file.
- # If you have tags then it will generate a specific size file for every tags
- ##NOTE: define size of file is the better thing, because generate a local temporary file on disk and then put it in bucket.
+ # Set the size of file parts in KB. This means that when each file grows to this size, a new .part is created for upload. If you have tags then it will generate a specific size file for every tag.
+ # This is an optional field but setting is recommended.
  config :size_file, :validate => :number, :default => 0
 
- # Set the time, in minutes, to close the current sub_time_section of bucket.
- # If you define file_size you have a number of files in consideration of the section and the current tag.
- # 0 stay all time on listerner, beware if you specific 0 and size_file 0, because you will not put the file on bucket,
- # for now the only thing this plugin can do is to put the file when logstash restart.
+ # Set the time, in minutes, to upload each log file part. If you wanted to upload a new file every day, for example, set this to 1440.
+ # If you also set `size_file`, you may also have a group of .part files that are uploaded.
+ # Avoid setting both this and `size_file` to 0, because you will not upload anything to s3 until logstash is restarted.
  config :time_file, :validate => :number, :default => 0
 
  # The event format you want to store in files. Defaults to plain text.
  config :format, :validate => [ "json", "plain", "nil" ], :default => "plain"
 
- ## IMPORTANT: if you use multiple instance of s3, you should specify on one of them the "restore=> true" and on the others "restore => false".
- ## This is hack for not destroy the new files after restoring the initial files.
- ## If you do not specify "restore => true" when logstash crashes or is restarted, the files are not sent into the bucket,
- ## for example if you have single Instance.
+ # Whether to upload files on disk into the s3 bucket when logstash is restarted.
+ # IMPORTANT: if you use multiple instances of this output, only one of them should have this value set to true. This is why it defaults to false.
  config :restore, :validate => :boolean, :default => false
 
- # Aws canned ACL
+ # S3 canned_acl for new files.
  config :canned_acl, :validate => ["private", "public_read", "public_read_write", "authenticated_read"],
         :default => "private"
 
