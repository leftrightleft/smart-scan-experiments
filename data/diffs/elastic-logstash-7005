diff --git a/docs/index.asciidoc b/docs/index.asciidoc
index 73320e6278c..cdde5af6d85 100644
--- a/docs/index.asciidoc
+++ b/docs/index.asciidoc
@@ -19,6 +19,7 @@ release-state can be: released | prerelease | unreleased
 :logstash:              https://www.elastic.co/guide/en/logstash/current/
 :libbeat:               https://www.elastic.co/guide/en/beats/libbeat/current/
 :filebeat:              https://www.elastic.co/guide/en/beats/filebeat/current/
+:metricbeat:            https://www.elastic.co/guide/en/beats/metricbeat/current/          
 :lsissue:               https://github.com/elastic/logstash/issues/
 :security:              X-Pack Security
 :stack:                 https://www.elastic.co/guide/en/elastic-stack/current/
diff --git a/docs/static/deploying.asciidoc b/docs/static/deploying.asciidoc
index dc6fa74faf5..7d8e2867925 100644
--- a/docs/static/deploying.asciidoc
+++ b/docs/static/deploying.asciidoc
@@ -1,146 +1,254 @@
 [[deploying-and-scaling]]
 == Deploying and Scaling Logstash
 
-As your use case for Logstash evolves, the preferred architecture at a given scale will change. This section discusses
-a range of Logstash architectures in increasing order of complexity, starting from a minimal installation and adding
-elements to the system. The example deployments in this section write to an Elasticsearch cluster, but Logstash can
-write to a large variety of {logstash}output-plugins.html[endpoints].
+The Elastic Stack is used for tons of use cases, from operational log and
+metrics analytics, to enterprise and application search. Making sure your data
+gets scalably, durably, and securely transported to Elasticsearch is extremely
+important, especially for mission critical environments.
+
+The goal of this document is to highlight the most common architecture patterns
+for Logstash and how to effectively scale as your deployment grows. The focus
+will be around the operational log, metrics, and security analytics use cases
+because they tend to require larger scale deployments. The deploying and scaling
+recommendations provided here may vary based on your own requirements.
 
 [float]
-[[deploying-minimal-install]]
-=== The Minimal Installation
+[[deploying-getting-started]]
+=== Getting Started
 
-The minimal Logstash installation has one Logstash instance and one Elasticsearch instance. These instances are
-directly connected. Logstash uses an {logstash}input-plugins.html[_input plugin_] to ingest data and an
-Elasticsearch {logstash}output-plugins.html[_output plugin_] to index the data in Elasticsearch, following the Logstash
-{logstash}pipeline.html[_processing pipeline_]. A Logstash instance has a fixed pipeline constructed at startup,
-based on the instance’s configuration file. You must specify an input plugin. Output defaults to `stdout`, and the
-filtering section of the pipeline, which is discussed in the next section, is optional.
+For first time users, if you simply want to tail a log file to grasp the power
+of the Elastic Stack, we recommend trying
+{filebeat}filebeat-modules-overview.html[Filebeat Modules]. Filebeat Modules
+enable you to quickly collect, parse, and index popular log types and view
+pre-built Kibana dashboards within minutes.
+{metricbeat}metricbeat-modules.html[Metricbeat Modules] provide a similar
+experience, but with metrics data. In this context, Beats will ship data
+directly to Elasticsearch where {ref}ingest.html[Ingest Nodes] will process
+and index your data.
 
-image::static/images/deploy_1.png[]
+image::static/images/deploy1.png[]
 
 [float]
-[[deploying-filter-threads]]
-=== Using Filters
+==== Introducing Logstash
+What are the main benefits for integrating Logstash into your architecture?
+
+* Scale through ingestion spikes - Logstash has an adaptive disk-based
+buffering system that will absorb incoming throughput, therefore mitigating
+backpressure
+* Ingest from other data sources like databases, S3, or messaging queues
+* Emit data to multiple destinations like S3, HDFS, or write to a file
+* Compose more sophisticated processing pipelines with conditional dataflow logic
 
-Log data is typically unstructured, often contains extraneous information that isn’t relevant to your use case, and
-sometimes is missing relevant information that can be derived from the log contents. You can use a
-{logstash}filter-plugins.html[filter plugin] to parse the log into fields, remove unnecessary information, and derive
-additional information from the existing fields. For example, filters can derive geolocation information from an IP
-address and add that information to the logs, or parse and structure arbitrary text with the
-{logstash}plugins-filters-grok.html[grok] filter.
+[float]
+[[scaling-ingest]]
+=== Scaling Ingest
 
-Adding a filter plugin can significantly affect performance, depending on the amount of computation the filter plugin
-performs, as well as on the volume of the logs being processed. The `grok` filter’s regular expression computation is
-particularly resource-intensive. One way to address this increased demand for computing resources is to use
-parallel processing on multicore machines. Use the `-w` switch to set the number of execution threads for Logstash
-filtering tasks. For example the `bin/logstash -w 8` command uses eight different threads for filter processing.
+Beats and Logstash make ingest awesome. Together, they provide a comprehensive
+solution that is scalable and resilient. What can you expect?
 
-image::static/images/deploy_2.png[]
+* Horizontal scalability, high availability, and variable load handling
+* Message durability with at-least-once delivery guarantees
+* End-to-end secure transport with authentication and wire encryption
 
 [float]
-[[deploying-filebeat]]
-=== Using Filebeat
+==== Beats and Logstash
+
+Beats run across thousands of edge host servers, collecting, tailing, and
+shipping logs to Logstash. Logstash serves as the centralized streaming
+engine for data unification and enrichment. The
+<<plugins-inputs-beats,Beats input plugin>> exposes a secure,
+acknowledgement-based endpoint for Beats to send data to Logstash.
+
+image::static/images/deploy2.png[]
+
+NOTE: Enabling persistent queues is strongly recommended, and these
+architecture characteristics assume that they are enabled. We encourage you to
+review the <<persistent-queues>> documentation for feature benefits and more
+details on resiliency.
+
+[float]
+==== Scalability
+
+Logstash is horizontally scalable and can form groups of nodes running the same
+pipeline. Logstash’s adaptive buffering capabilities will facilitate smooth
+streaming even through variable throughput loads. If the Logstash layer becomes
+an ingestion bottleneck, simply add more nodes to scale out. Here are a few
+general recommendations:
+
+* Beats should {filebeat}load-balancing.html[load balance] across a group of
+Logstash nodes.
+* A minimum of two Logstash nodes are recommended for high availability.
+* It’s common to deploy just one Beats input per Logstash node, but multiple
+Beats inputs can also be deployed per Logstash node to expose independent
+endpoints for different data sources.
+
+[float]
+==== Resiliency
+
+When using https://www.elastic.co/products/beats/filebeat[Filebeat] or
+https://www.elastic.co/products/beats/winlogbeat[Winlogbeat] for log collection
+within this ingest flow, *at-least-once delivery* is guaranteed. Both the
+communication protocols, from Filebeat or Winlogbeat to Logstash, and from
+Logstash to Elasticsearch, are synchronous and support acknowledgements. The
+other Beats don’t yet have support for acknowledgements.
 
-https://www.elastic.co/guide/en/beats/filebeat/current/index.html[Filebeat] is a lightweight, resource-friendly tool
-written in Go that collects logs from files on the server and forwards these logs to other machines for processing.
-Filebeat uses the https://www.elastic.co/guide/en/beats/libbeat/current/index.html[Beats] protocol to communicate with a
-centralized Logstash instance. Configure the Logstash instances that receive Beats data to use the
-{logstash}plugins-inputs-beats.html[Beats input plugin].
+Logstash persistent queues provide protection across node failures. For
+disk-level resiliency in Logstash, it’s important to ensure disk redundancy.
+For on-premise deployments, it's recommended that you configure RAID. When
+running in the cloud or a containerized environment, it’s recommended that you
+use persistent disks with replication strategies that reflect your data SLAs.
 
-Filebeat uses the computing resources of the machine hosting the source data, and the Beats input plugin minimizes the
-resource demands on the Logstash instance, making this architecture attractive for use cases with resource constraints.
+NOTE: Make sure `queue.checkpoint.writes: 1` is set for at-least-once
+guarantees. For more details, see the
+<<durability-persistent-queues,persistent queue durability>> documentation.
 
-image::static/images/deploy_3.png[]
+[float]
+==== Processing
+
+Logstash will commonly extract fields with <<plugins-filters-grok,grok>> or
+<<plugins-filters-dissect,dissect>>, augment
+<<plugins-filters-geoip,geographical>> info, and can further enrich events with
+<<plugins-filters-translate,file>>, <<plugins-filters-jdbc_streaming,database>>,
+or <<plugins-filters-elasticsearch,Elasticsearch>> lookup datasets. Be aware
+that processing complexity can affect overall throughput and CPU utilization.
+Make sure to check out the other <<filter-plugins,available filter plugins>>.
 
 [float]
-[[deploying-larger-cluster]]
-=== Scaling to a Larger Elasticsearch Cluster
+==== Secure Transport
 
-Typically, Logstash does not communicate with a single Elasticsearch node, but with a cluster that comprises several
-nodes. By default, Logstash uses the HTTP protocol to move data into the cluster.
+Enterprise-grade security is available across the entire delivery chain.
 
-You can use the Elasticsearch HTTP REST APIs to index data into the Elasticsearch cluster. These APIs represent the
-indexed data in JSON. Using the REST APIs does not require the Java client classes or any additional JAR
-files and has no performance disadvantages compared to the transport or node protocols. You can secure communications
-that use the HTTP REST APIs by using {xpack}/xpack-security.html[{security}], which supports SSL and HTTP basic authentication.
+* Wire encryption is recommended for both the transport from
+{filebeat}configuring-ssl-logstash.html[Beats to Logstash] and from 
+{xpack}/logstash.html[Logstash to Elasticsearch].
+* There’s a wealth of security options when communicating with Elasticsearch
+including basic authentication, TLS, PKI, LDAP, AD, and other custom realms.
+To enable Elasticsearch security, consult the
+{xpack}/xpack-security.html[X-Pack documentation].
 
-When you use the HTTP protocol, you can configure the Logstash Elasticsearch output plugin to automatically
-load-balance indexing requests across a
-specified set of hosts in the Elasticsearch cluster. Specifying multiple Elasticsearch nodes also provides high availability for the Elasticsearch cluster by routing traffic to active Elasticsearch nodes.
+[float]
+==== Monitoring
 
-You can also use the Elasticsearch Java APIs to serialize the data into a binary representation, using
-the transport protocol. The transport protocol can sniff the endpoint of the request and select an
-arbitrary client or data node in the Elasticsearch cluster.
+When running Logstash 5.2 or greater,
+the https://www.elastic.co/products/x-pack/monitoring[Monitoring UI] provides
+deep visibility into your deployment metrics, helping observe performance and
+alleviate bottlenecks as you scale. Monitoring is an X-Pack feature under the
+Basic License and is therefore *free to use*. To get started, consult the
+{xpack}/monitoring-logstash.html[X-Pack Monitoring documentation].
 
-Using the HTTP or transport protocols keep your Logstash instances separate from the Elasticsearch cluster. The node
-protocol, by contrast, has the machine running the Logstash instance join the Elasticsearch cluster, running an
-Elasticsearch instance. The data that needs indexing propagates from this node to the rest of the cluster. Since the
-machine is part of the cluster, the cluster topology is available, making the node protocol a good fit for use cases
-that use a relatively small number of persistent connections.
+If external monitoring is preferred, there are <<monitoring,Monitoring APIs>>
+that return point-in-time metrics snapshots.
 
-You can also use a third-party hardware or software load balancer to handle connections between Logstash and
-external applications.
+[float]
+[[adding-other-sources]]
+=== Adding Other Popular Sources
 
-NOTE: Make sure that your Logstash configuration does not connect directly to Elasticsearch dedicated
-{ref}modules-node.html[master nodes], which perform dedicated cluster management. Connect Logstash to client or data
-nodes to protect the stability of your Elasticsearch cluster.
+Users may have other mechanisms of collecting logging data, and it’s easy to
+integrate and centralize them into the Elastic Stack. Let’s walk through a few
+scenarios:
 
-image::static/images/deploy_4.png[]
+image::static/images/deploy3.png[]
 
 [float]
-[[deploying-message-queueing]]
-=== Managing Throughput Spikes with Message Queueing
+==== TCP, UDP, and HTTP Protocols
 
-When the data coming into a Logstash pipeline exceeds the Elasticsearch cluster's ability to ingest the data, you can
-use a message broker as a buffer. By default, Logstash throttles incoming events when
-indexer consumption rates fall below incoming data rates. Since this throttling can lead to events being buffered at
-the data source, preventing back pressure with message brokers becomes an important part of managing your deployment.
+The TCP, UDP, and HTTP protocols are common ways to feed data into Logstash.
+Logstash can expose endpoint listeners with the respective
+<<plugins-inputs-tcp,TCP>>, <<plugins-inputs-udp,UDP>>, and
+<<plugins-inputs-http,HTTP>> input plugins. The data sources enumerated below
+are typically ingested through one of these three protocols.
 
-Adding a message broker to your Logstash deployment also provides a level of protection from data loss. When a Logstash
-instance that has consumed data from the message broker fails, the data can be replayed from the message broker to an
-active Logstash instance.
+NOTE: The TCP protocol does not support application-level acknowledgements, so
+connectivity issues may result in data loss.
 
-Several third-party message brokers exist, such as Redis, Kafka, or RabbitMQ. Logstash provides input and output plugins
-to integrate with several of these third-party message brokers. When your Logstash deployment has a message broker
-configured, Logstash functionally exists in two phases: shipping instances, which handles data ingestion and storage in
-the message broker, and indexing instances, which retrieve the data from the message broker, apply any configured
-filtering, and write the filtered data to an Elasticsearch index.
+For high availability scenarios, a third-party hardware or software load
+balancer, like HAProxy, should be added to fan out traffic to a group of
+Logstash nodes.
 
-image::static/images/deploy_5.png[]
+[float]
+==== Network and Security Data
+
+Although Beats may already satisfy your data ingest use case, network and
+security datasets come in a variety of forms. Let’s touch on a few other
+ingestion points.
+
+* Network wire data - collect and analyze network traffic with
+https://www.elastic.co/products/beats/packetbeat[Packetbeat].
+* Netflow v5/v9/v10 - Logstash understands data from Netflow/IPFIX exporters
+with the <<plugins-codecs-netflow,Netflow codec>>.
+* Nmap - Logstash accepts and parses Nmap XML data with the
+<<plugins-codecs-nmap,Nmap codec>>.
+* SNMP trap - Logstash has a native <<plugins-inputs-snmptrap,SNMP trap input>>.
+* CEF - Logstash accepts and parses CEF data from systems like Arcsight
+SmartConnectors with the <<plugins-codecs-cef,CEF codec>>. See this
+https://www.elastic.co/blog/integrating-elastic-stack-with-arcsight-siem-part-1[blog series]
+for more details.
 
 [float]
-[[deploying-logstash-ha]]
-=== Multiple Connections for Logstash High Availability
+==== Centralized Syslog Servers
 
-To make your Logstash deployment more resilient to individual instance failures, you can set up a load balancer between
-your data source machines and the Logstash cluster. The load balancer handles the individual connections to the
-Logstash instances to ensure continuity of data ingestion and processing even when an individual instance is unavailable.
+Existing syslog server technologies like rsyslog and syslog-ng generally send
+syslog over to Logstash TCP or UDP endpoints for extraction, processing, and
+persistence. If the data format conforms to RFC3164, it can be fed directly
+to the <<plugins-inputs-syslog,Logstash syslog input>>.
 
-image::static/images/deploy_6.png[]
+[float]
+==== Infrastructure & Application Data and IoT
 
-The architecture in the previous diagram is unable to process input from a specific type, such as an RSS feed or a
-file, if the Logstash instance dedicated to that input type becomes unavailable. For more robust input processing,
-configure each Logstash instance for multiple inputs, as in the following diagram:
+Infrastructure and application metrics can be collected with
+https://www.elastic.co/products/beats/metricbeat[Metricbeat], but applications
+can also send webhooks to a Logstash HTTP input or have metrics polled from an
+HTTP endpoint with the <<plugins-inputs-http_poller,HTTP poller input plugin>>.
 
-image::static/images/deploy_7.png[]
+For applications that log with log4j2, it’s recommended to use the
+SocketAppender to send JSON to the Logstash TCP input. Alternatively, log4j2
+can also log to a file for collection with FIlebeat. Usage of the log4j1
+SocketAppender is not recommended.
 
-This architecture parallelizes the Logstash workload based on the inputs you configure. With more inputs, you can add
-more Logstash instances to scale horizontally. Separate parallel pipelines also increases the reliability of your stack
-by eliminating single points of failure.
+IoT devices like Rasberry Pis, smartphones, and connected vehicles often send
+telemetry data through one of these protocols.
 
 [float]
-[[deploying-scaling]]
-=== Scaling Logstash
+[[integrating-with-messaging-queues]]
+=== Integrating with Messaging Queues
+
+If you are leveraging message queuing technologies as part of your existing
+infrastructure, getting that data into the Elastic Stack is easy. For existing
+users who are utilizing an external queuing layer like Redis or RabbitMQ just
+for data buffering with Logstash, it’s recommended to use Logstash persistent
+queues instead of an external queuing layer. This will help with overall ease
+of management by removing an unnecessary layer of complexity in your ingest
+architecture.
 
-A mature Logstash deployment typically has the following pipeline:
+For users who want to integrate data from existing Kafka deployments or require
+the underlying usage of ephemeral storage, Kafka can serve as a data hub where
+Beats can persist to and Logstash nodes can consume from.
 
-* The _input_ tier consumes data from the source, and consists of Logstash instances with the proper input plugins.
-* The _message broker_ serves as a buffer to hold ingested data and serve as failover protection.
-* The _filter_ tier applies parsing and other processing to the data consumed from the message broker.
-* The _indexing_ tier moves the processed data into Elasticsearch.
+image::static/images/deploy4.png[]
 
-Any of these layers can be scaled by adding computing resources. Examine the performance of these components regularly
-as your use case evolves and add resources as needed. When Logstash routinely throttles incoming events, consider
-adding storage for your message broker. Alternately, increase the Elasticsearch cluster's rate of data consumption by
-adding more Logstash indexing instances.
+The other TCP, UDP, and HTTP sources can persist to Kafka with Logstash as a
+conduit to achieve high availability in lieu of a load balancer. A group of
+Logstash nodes can then consume from topics with the
+<<plugins-inputs-kafka,Kafka input>> to further transform and enrich the data in
+transit.
+
+[float]
+==== Resiliency and Recovery
+
+When Logstash consumes from Kafka, persistent queues should be enabled and will
+add transport resiliency to mitigate the need for reprocessing during Logstash
+node failures. In this context, it’s recommended to use the default persistent
+queue disk allocation size `queue.max_bytes: 1GB`.
+
+If Kafka is configured to retain data for an extended period of time, data can
+be reprocessed from Kafka in the case of disaster recovery and reconciliation.
+
+[float]
+==== Other Messaging Queue Integrations
+
+Although an additional queuing layer is not required, Logstash can consume from
+a myriad of other message queuing technologies like
+<<plugins-inputs-rabbitmq,RabbitMQ>> and <<plugins-inputs-redis,Redis>>. It also
+supports ingestion from hosted queuing services like
+<<plugins-inputs-google_pubsub,Pub/Sub>>, <<plugins-inputs-kinesis,Kinesis>>, and
+<<plugins-inputs-sqs,SQS>>.
diff --git a/docs/static/images/deploy1.png b/docs/static/images/deploy1.png
new file mode 100644
index 00000000000..f6b97cb6547
Binary files /dev/null and b/docs/static/images/deploy1.png differ
diff --git a/docs/static/images/deploy2.png b/docs/static/images/deploy2.png
new file mode 100644
index 00000000000..2e7f48627ab
Binary files /dev/null and b/docs/static/images/deploy2.png differ
diff --git a/docs/static/images/deploy3.png b/docs/static/images/deploy3.png
new file mode 100644
index 00000000000..1173cf16237
Binary files /dev/null and b/docs/static/images/deploy3.png differ
diff --git a/docs/static/images/deploy4.png b/docs/static/images/deploy4.png
new file mode 100644
index 00000000000..20fca62da08
Binary files /dev/null and b/docs/static/images/deploy4.png differ
