diff --git a/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueRWBenchmark.java b/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueRWBenchmark.java
index 6cccdd76437..7af8ccbea89 100644
--- a/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueRWBenchmark.java
+++ b/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueRWBenchmark.java
@@ -100,7 +100,7 @@ public final void readFromPersistedQueue(final Blackhole blackhole) throws Excep
             }
         });
         for (int i = 0; i < EVENTS_PER_INVOCATION / BATCH_SIZE; ++i) {
-            try (Batch batch = queuePersisted.readBatch(BATCH_SIZE)) {
+            try (Batch batch = queuePersisted.readBatch(BATCH_SIZE, Queue.TIMEOUT_SECOND)) {
                 for (final Queueable elem : batch.getElements()) {
                     blackhole.consume(elem);
                 }
@@ -122,7 +122,7 @@ public final void readFromMemoryQueue(final Blackhole blackhole) throws Exceptio
             }
         });
         for (int i = 0; i < EVENTS_PER_INVOCATION / BATCH_SIZE; ++i) {
-            try (Batch batch = queueMemory.readBatch(BATCH_SIZE)) {
+            try (Batch batch = queueMemory.readBatch(BATCH_SIZE, Queue.TIMEOUT_SECOND)) {
                 for (final Queueable elem : batch.getElements()) {
                     blackhole.consume(elem);
                 }
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java
index b1c720f47e4..799b8145ac2 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java
@@ -15,17 +15,17 @@ public class Batch implements Closeable {
     private final Queue queue;
     private final AtomicBoolean closed;
 
-    public Batch(List<Queueable> elements, LongVector seqNums, Queue q) {
-        this.elements = elements;
+    public Batch(SequencedList<byte[]> serialized, Queue q) {
+        this(serialized.getElements(), serialized.getSeqNums(), q);
+    }
+
+    public Batch(List<byte[]> elements, LongVector seqNums, Queue q) {
+        this.elements = deserializeElements(elements, q);
         this.seqNums = seqNums;
         this.queue = q;
         this.closed = new AtomicBoolean(false);
     }
 
-    public Batch(SequencedList<byte[]> serialized, Queue q) {
-        this(deserializeElements(serialized.getElements(), q), serialized.getSeqNums(), q);
-    }
-
     // close acks the batch ackable events
     public void close() throws IOException {
         if (closed.getAndSet(true) == false) {
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
index f02f21fb608..cb3708970b1 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
@@ -202,21 +202,24 @@ public void forceCheckpoint() throws IOException {
     }
 
     public void behead() throws IOException {
-        checkpoint();
+        assert this.writable == true : "cannot behead a tail page";
+
+        headPageCheckpoint();
 
         this.writable = false;
         this.lastCheckpoint = new Checkpoint(0, 0, 0, 0, 0);
 
         // first thing that must be done after beheading is to create a new checkpoint for that new tail page
         // tail page checkpoint does NOT includes a fsync
-        checkpoint();
+        tailPageCheckpoint();
+    }
 
-        // TODO: should we have a better deactivation strategy to avoid too rapid reactivation scenario?
-        Page firstUnreadPage = queue.firstUnreadPage();
-        if (firstUnreadPage == null || (this.getPageNum() > firstUnreadPage.getPageNum())) {
-            // deactivate if this new tailPage is not where the read is occurring
-            this.getPageIO().deactivate();
-        }
+    /**
+     * signal that this page is not active and resources can be released
+     * @throws IOException
+     */
+    public void deactivate() throws IOException {
+        this.getPageIO().deactivate();
     }
 
     public boolean hasSpace(int byteSize) {
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
index 1682e885619..1325a08cce1 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
@@ -25,14 +25,6 @@
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
 
-// TODO: Notes
-//
-// - time-based fsync
-//
-// - tragic errors handling
-//   - what errors cause whole queue to be broken
-//   - where to put try/catch for these errors
-
 public final class Queue implements Closeable {
 
     private long seqNum;
@@ -78,6 +70,8 @@ public final class Queue implements Closeable {
     private FileLock dirLock;
     private final static String LOCK_NAME = ".lock";
 
+    public static final long TIMEOUT_SECOND = 1000; // 1s in millisec
+
     private static final Logger logger = LogManager.getLogger(Queue.class);
 
     public Queue(Settings settings) {
@@ -147,9 +141,10 @@ public long getUnreadCount() {
         return this.unreadCount;
     }
 
-    // moved queue opening logic in open() method until we have something in place to used in-memory checkpoints for testing
-    // because for now we need to pass a Queue instance to the Page and we don't want to trigger a Queue recovery when
-    // testing Page
+    /**
+     * Open an existing {@link Queue} or create a new one in the configured path.
+     * @throws IOException
+     */
     public void open() throws IOException {
         final int headPageNum;
 
@@ -252,10 +247,16 @@ public void open() throws IOException {
 
     // TODO: addIO and addPage are almost identical - we should refactor to DRY it up.
 
-    // addIO is basically the same as addPage except that it avoid calling PageIO.open
-    // before actually purging the page if it is fully acked. This avoid dealing with
-    // zero byte page files that are fully acked.
-    // see issue #7809
+    /**
+     * addIO is basically the same as addPage except that it avoid calling PageIO.open
+     * before actually purging the page if it is fully acked. This avoid dealing with
+     * zero byte page files that are fully acked.
+     * see issue #7809
+     *
+     * @param checkpoint the recovered {@link Checkpoint} of the {@link Page} to add
+     * @param pageIO the {@link PageIO} for the recovered {@link Page} data file
+     * @throws IOException
+     */
     private void addIO(Checkpoint checkpoint, PageIO pageIO) throws IOException {
         if (checkpoint.isFullyAcked()) {
             // first make sure any fully acked page per the checkpoint is purged if not already
@@ -294,8 +295,14 @@ private void addIO(Checkpoint checkpoint, PageIO pageIO) throws IOException {
         }
     }
 
-    // add a read tail page into this queue structures but also verify that this tail page
-    // is not fully acked in which case it will be purged
+    /**
+     * add a read tail page into this queue structures but also verify that this tail page
+     * is not fully acked in which case it will be purged
+     *
+     * @param checkpoint the recovered {@link Checkpoint} of the {@link Page} to add
+     * @param page the {@link Page} for the recovered data file
+     * @throws IOException
+     */
     private void addPage(Checkpoint checkpoint, Page page) throws IOException {
         if (checkpoint.isFullyAcked()) {
             // first make sure any fully acked page per the checkpoint is purged if not already
@@ -331,8 +338,12 @@ private void addPage(Checkpoint checkpoint, Page page) throws IOException {
         }
     }
 
-    // create a new empty headpage for the given pageNum and immediately checkpoint it
-    // @param pageNum the page number of the new head page
+    /**
+     * create a new empty headpage for the given pageNum and immediately checkpoint it
+     *
+     * @param pageNum the page number of the new head page
+     * @throws IOException
+     */
     private void newCheckpointedHeadpage(int pageNum) throws IOException {
         PageIO headPageIO = this.pageIOFactory.build(pageNum, this.pageCapacity, this.dirPath);
         headPageIO.create();
@@ -341,8 +352,14 @@ private void newCheckpointedHeadpage(int pageNum) throws IOException {
         this.currentByteSize += headPageIO.getCapacity();
     }
 
-    // @param element the Queueable object to write to the queue
-    // @return long written sequence number
+    /**
+     * write a {@link Queueable} element to the queue. Note that the element will always be written and the queue full
+     * condition will be checked and waited on **after** the write operation.
+     *
+     * @param element the {@link Queueable} element to write
+     * @return the written sequence number
+     * @throws IOException
+     */
     public long write(Queueable element) throws IOException {
         byte[] data = element.serialize();
 
@@ -367,18 +384,13 @@ public long write(Queueable element) throws IOException {
                 int newHeadPageNum = this.headPage.pageNum + 1;
 
                 if (this.headPage.isFullyAcked()) {
-                    // purge the old headPage because its full and fully acked
-                    // there is no checkpoint file to purge since just creating a new TailPage from a HeadPage does
-                    // not trigger a checkpoint creation in itself
+                    // here we can just purge the data file and avoid beheading since we do not need
+                    // to add this fully hacked page into tailPages. a new head page will just be created.
+                    // TODO: we could possibly reuse the same page file but just rename it?
                     this.headPage.purge();
                     currentByteSize -= this.headPage.getPageIO().getCapacity();
                 } else {
-                    // beheading includes checkpoint+fsync if required
-                    this.headPage.behead();
-                    this.tailPages.add(this.headPage);
-                    if (! this.headPage.isFullyRead()) {
-                        this.unreadTailPages.add(this.headPage);
-                    }
+                    behead();
                 }
 
                 // create new head page
@@ -418,6 +430,29 @@ public long write(Queueable element) throws IOException {
         }
     }
 
+    /**
+     * mark head page as read-only (behead) and add it to the tailPages and unreadTailPages collections accordingly
+     * also deactivate it if it's not next-in-line for reading
+     *
+     * @throws IOException
+     */
+    private void behead() throws IOException {
+        // beheading includes checkpoint+fsync if required
+        this.headPage.behead();
+        this.tailPages.add(this.headPage);
+
+        if (! this.headPage.isFullyRead()) {
+            if (!this.unreadTailPages.isEmpty()) {
+                // there are already other unread pages so this new one is not next in line and we can deactivate
+                this.headPage.deactivate();
+            }
+            this.unreadTailPages.add(this.headPage);
+        } else {
+            // it is fully read so we can deactivate
+            this.headPage.deactivate();
+        }
+    }
+
     /**
      * <p>Checks if the Queue is full, with "full" defined as either of:</p>
      * <p>Assuming a maximum size of the queue larger than 0 is defined:</p>
@@ -468,7 +503,9 @@ public boolean isEmpty() {
 
     }
 
-    // @return true if the queue is fully acked, which implies that it is fully read which works as an "empty" state.
+    /**
+     * @return true if the queue is fully acked, which implies that it is fully read which works as an "empty" state.
+     */
     public boolean isFullyAcked() {
         lock.lock();
         try {
@@ -478,7 +515,12 @@ public boolean isFullyAcked() {
         }
     }
 
-    // @param seqNum the element sequence number upper bound for which persistence should be guaranteed (by fsync'ing)
+    /**
+     * guarantee persistence up to a given sequence number.
+     *
+     * @param seqNum the element sequence number upper bound for which persistence should be guaranteed (by fsync'ing)
+     * @throws IOException
+     */
     public void ensurePersistedUpto(long seqNum) throws IOException{
         lock.lock();
         try {
@@ -488,100 +530,98 @@ public void ensurePersistedUpto(long seqNum) throws IOException{
         }
     }
 
-    // non-blockin queue read
-    // @param limit read the next bach of size up to this limit. the returned batch size can be smaller than than the requested limit if fewer elements are available
-    // @return Batch the batch containing 1 or more element up to the required limit or null of no elements were available
+    /**
+     * non-blocking queue read
+     *
+     * @param limit read the next bach of size up to this limit. the returned batch size can be smaller than than the requested limit if fewer elements are available
+     * @return {@link Batch} the batch containing 1 or more element up to the required limit or null of no elements were available
+     * @throws IOException
+     */
     public Batch nonBlockReadBatch(int limit) throws IOException {
         lock.lock();
         try {
-            Page p = firstUnreadPage();
-            return (p == null) ? null : _readPageBatch(p, limit);
+            Page p = nextReadPage();
+            return (isHeadPage(p) && p.isFullyRead()) ? null : _readPageBatch(p, limit, 0L);
         } finally {
             lock.unlock();
         }
     }
 
-    // blocking readBatch notes:
-    //   the queue close() notifies all pending blocking read so that they unblock if the queue is being closed.
-    //   this means that all blocking read methods need to verify for the queue close condition.
-    //
-    // blocking queue read until elements are available for read
-    // @param limit read the next bach of size up to this limit. the returned batch size can be smaller than than the requested limit if fewer elements are available
-    // @return Batch the batch containing 1 or more element up to the required limit or null if no elements were available or the blocking call was interrupted
-    public Batch readBatch(int limit) throws IOException {
-        Page p;
-
+    /**
+     *
+     * @param limit size limit of the batch to read. returned {@link Batch} can be smaller.
+     * @param timeout the maximum time to wait in milliseconds on write operations
+     * @return the read {@link Batch} or null if no element upon timeout
+     * @throws IOException
+     */
+    public Batch readBatch(int limit, long timeout) throws IOException {
         lock.lock();
         try {
-            while ((p = firstUnreadPage()) == null && !isClosed()) {
-                try {
-                    notEmpty.await();
-                } catch (InterruptedException e) {
-                    // the thread interrupt() has been called while in the await() blocking call.
-                    // at this point the interrupted flag is reset and Thread.interrupted() will return false
-                    // to any upstream calls on it. for now our choice is to simply return null and set back
-                    // the Thread.interrupted() flag so it can be checked upstream.
-
-                    // set back the interrupted flag
-                    Thread.currentThread().interrupt();
-
-                    return null;
-                }
-            }
-
-            // need to check for close since it is a condition for exiting the while loop
-            return (isClosed()) ? null : _readPageBatch(p, limit);
+            return _readPageBatch(nextReadPage(), limit, timeout);
         } finally {
             lock.unlock();
         }
     }
 
-    // blocking queue read until elements are available for read or the given timeout is reached.
-    // @param limit read the next batch of size up to this limit. the returned batch size can be smaller than than the requested limit if fewer elements are available
-    // @param timeout the maximum time to wait in milliseconds
-    // @return Batch the batch containing 1 or more element up to the required limit or null if no elements were available or the blocking call was interrupted
-    public Batch readBatch(int limit, long timeout) throws IOException {
-        Page p;
+    /**
+     * read a {@link Batch} from the given {@link Page}. If the page is a head page, try to maximize the
+     * batch size by waiting for writes.
+     * @param p the {@link Page} to read from.
+     * @param limit size limit of the batch to read.
+     * @param timeout  the maximum time to wait in milliseconds on write operations.
+     * @return {@link Batch} with read elements or null if nothing was read
+     * @throws IOException
+     */
+    private Batch _readPageBatch(Page p, int limit, long timeout) throws IOException {
+        int left = limit;
+        final List<byte[]> elements = new ArrayList<>(limit);
+        final LongVector seqNums = new LongVector(limit);
 
-        lock.lock();
-        try {
-            // wait only if queue is empty
-            if ((p = firstUnreadPage()) == null) {
+        // NOTE: the tricky thing here is that upon entering this method, if p is initially a head page
+        // it could become a tail page upon returning from the notEmpty.await call.
+
+        do {
+            if (isHeadPage(p) && p.isFullyRead()) {
+                // a head page is fully read but can be written to so let's wait for more data
                 try {
                     notEmpty.await(timeout, TimeUnit.MILLISECONDS);
                 } catch (InterruptedException e) {
-                    // the thread interrupt() has been called while in the await() blocking call.
-                    // at this point the interrupted flag is reset and Thread.interrupted() will return false
-                    // to any upstream calls on it. for now our choice is to simply return null and set back
-                    // the Thread.interrupted() flag so it can be checked upstream.
-
                     // set back the interrupted flag
                     Thread.currentThread().interrupt();
-
-                    return null;
+                    break;
                 }
 
-                // if after returning from wait queue is still empty, or the queue was closed return null
-                if ((p = firstUnreadPage()) == null || isClosed()) { return null; }
+                if ((p.isFullyRead()) || isClosed()) {
+                    // we either hit the timeout or the queue was closed
+                    break;
+                }
             }
 
-            return _readPageBatch(p, limit);
-        } finally {
-            lock.unlock();
-        }
-    }
+            boolean wasFull = isFull();
 
-    private Batch _readPageBatch(Page p, int limit) throws IOException {
-        boolean wasFull = isFull();
+            final SequencedList<byte[]> serialized = p.read(left);
+            int n = serialized.getElements().size();
+            assert n > 0 : "page read returned 0 elements";
+            elements.addAll(serialized.getElements());
+            seqNums.add(serialized.getSeqNums());
 
-        SequencedList<byte[]> serialized = p.read(limit);
+            this.unreadCount -= n;
+            left -= n;
 
-        this.unreadCount -= serialized.getElements().size();
+            if (wasFull) {
+                notFull.signalAll();
+            }
+
+            if (isTailPage(p) && p.isFullyRead()) {
+                break;
+            }
+        } while (left > 0);
 
-        if (p.isFullyRead()) { removeUnreadPage(p); }
-        if (wasFull) { notFull.signalAll(); }
+        if (isTailPage(p) && p.isFullyRead()) {
+            removeUnreadPage(p);
+        }
 
-        return new Batch(serialized, this);
+        return (left >= limit) ? null :  new Batch(elements, seqNums, this);
     }
 
     private static class TailPageResult {
@@ -594,7 +634,12 @@ public TailPageResult(Page page, int index) {
         }
     }
 
-    // perform a binary search through tail pages to find in which page this seqNum falls into
+    /**
+     * perform a binary search through tail pages to find in which page this seqNum falls into
+     *
+     * @param seqNum the sequence number to search for in the tail pages
+     * @return {@link TailPageResult}
+     */
     private TailPageResult binaryFindPageForSeqnum(long seqNum) {
         int lo = 0;
         int hi = this.tailPages.size() - 1;
@@ -613,7 +658,12 @@ private TailPageResult binaryFindPageForSeqnum(long seqNum) {
         return null;
     }
 
-    // perform a linear search through tail pages to find in which page this seqNum falls into
+    /**
+     * perform a linear search through tail pages to find in which page this seqNum falls into
+     *
+     * @param seqNum the sequence number to search for in the tail pages
+     * @return {@link TailPageResult}
+     */
     private TailPageResult linearFindPageForSeqnum(long seqNum) {
         for (int i = 0; i < this.tailPages.size(); i++) {
             Page p = this.tailPages.get(i);
@@ -624,10 +674,14 @@ private TailPageResult linearFindPageForSeqnum(long seqNum) {
         return null;
     }
 
-    // ack a list of seqNums that are assumed to be all part of the same page, leveraging the fact that batches are also created from
-    // same-page elements. A fully acked page will trigger a checkpoint for that page. Also if a page has more than checkpointMaxAcks
-    // acks since last checkpoint it will also trigger a checkpoint.
-    // @param seqNums the list of same-page sequence numbers to ack
+    /**
+     * ack a list of seqNums that are assumed to be all part of the same page, leveraging the fact that batches are also created from
+     * same-page elements. A fully acked page will trigger a checkpoint for that page. Also if a page has more than checkpointMaxAcks
+     * acks since last checkpoint it will also trigger a checkpoint.
+     *
+     * @param seqNums the list of same-page sequence numbers to ack
+     * @throws IOException
+     */
     public void ack(LongVector seqNums) throws IOException {
         // as a first implementation we assume that all batches are created from the same page
         // so we will avoid multi pages acking here for now
@@ -700,10 +754,13 @@ public void ack(LongVector seqNums) throws IOException {
     public CheckpointIO getCheckpointIO() {
         return this.checkpointIO;
     }
-
-    // deserialize a byte array into the required element class.
-    // @param bytes the byte array to deserialize
-    // @return Queueable the deserialized byte array into the required Queueable interface implementation concrete class
+    
+    /**
+     *  deserialize a byte array into the required element class.
+     *
+     * @param bytes the byte array to deserialize
+     * @return {@link Queueable} the deserialized byte array into the required Queueable interface implementation concrete class
+     */
     public Queueable deserialize(byte[] bytes) {
         try {
             return (Queueable)this.deserializeMethod.invoke(this.elementClass, bytes);
@@ -752,22 +809,28 @@ public void close() throws IOException {
         }
     }
 
-    public Page firstUnreadPage() {
+    /**
+     * return the {@link Page} for the next read operation.
+     * @return {@link Page} will be either a read-only tail page or the head page.
+     */
+    public Page nextReadPage() {
         lock.lock();
         try {
             // look at head page if no unreadTailPages
-            return (this.unreadTailPages.isEmpty()) ? (this.headPage.isFullyRead() ? null : this.headPage) : this.unreadTailPages.get(0);
+            return (this.unreadTailPages.isEmpty()) ?  this.headPage : this.unreadTailPages.get(0);
         } finally {
             lock.unlock();
         }
     }
 
     private void removeUnreadPage(Page p) {
-        // HeadPage is not part of the unreadTailPages, just ignore
-        if (p != this.headPage) {
-            // the page to remove should always be the first one
-            assert this.unreadTailPages.get(0) == p : String.format("unread page is not first in unreadTailPages list");
-            this.unreadTailPages.remove(0);
+        if (! this.unreadTailPages.isEmpty()) {
+            Page firstUnread = this.unreadTailPages.get(0);
+            assert p.pageNum <= firstUnread.pageNum : String.format("fully read pageNum=%d is greater than first unread pageNum=%d", p.pageNum, firstUnread.pageNum);
+            if (firstUnread == p) {
+                // it is possible that when starting to read from a head page which is beheaded will not be inserted in the unreadTailPages list
+                this.unreadTailPages.remove(0);
+            }
         }
     }
 
@@ -809,4 +872,20 @@ public long getUnackedCount() {
     private boolean isClosed() {
         return this.closed.get();
     }
+
+    /**
+     * @param p the {@link Page} to verify if it is the head page
+     * @return true if the given {@link Page} is the head page
+     */
+    private boolean isHeadPage(Page p) {
+        return p == this.headPage;
+    }
+
+    /**
+     * @param p the {@link Page} to verify if it is a tail page
+     * @return true if the given {@link Page} is a tail page
+     */
+    private boolean isTailPage(Page p) {
+        return !isHeadPage(p);
+    }
 }
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/LongVector.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/LongVector.java
index 601d66086d6..033bf9fe2f0 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/io/LongVector.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/LongVector.java
@@ -24,6 +24,22 @@ public void add(final long num) {
         data[count++] = num;
     }
 
+    /**
+     * Store the {@code long[]} to the underlying {@code long[]}, resizing it if necessary.
+     * @param nums {@code long[]} to store
+     */
+    public void add(final LongVector nums) {
+        if (data.length < count + nums.size()) {
+            final long[] old = data;
+            data = new long[(data.length << 1) + nums.size()];
+            System.arraycopy(old, 0, data, 0, old.length);
+        }
+        for (int i = 0; i < nums.size(); i++) {
+            data[count + i] = nums.get(i);
+        }
+        count += nums.size();
+    }
+
     /**
      * Get value stored at given index.
      * @param index Array index (only values smaller than {@link LongVector#count} are valid)
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
index e7a80cb67eb..df4882d55dd 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
@@ -84,7 +84,7 @@ public void inEmpty() throws IOException {
             assertThat(p.isEmpty(), is(true));
             p.write(element.serialize(), 1, 1);
             assertThat(p.isEmpty(), is(false));
-            Batch b = q.readBatch(1);
+            Batch b = q.readBatch(1, Queue.TIMEOUT_SECOND);
             assertThat(p.isEmpty(), is(false));
             b.close();
             assertThat(p.isEmpty(), is(true));
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
index 4754953f30e..88574f4857e 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
@@ -535,7 +535,7 @@ public void ackingMakesQueueNotFullAgainTest() throws IOException, InterruptedEx
             }
             assertThat(q.isFull(), is(true));
             
-            Batch b = q.readBatch(10); // read 1 page (10 events)
+            Batch b = q.readBatch(10, Queue.TIMEOUT_SECOND); // read 1 page (10 events)
             b.close();  // purge 1 page
             
             while (q.isFull()) { Thread.sleep(10); }
@@ -565,7 +565,7 @@ public void resumeWriteOnNoLongerFullQueueTest() throws IOException, Interrupted
 
             // read 1 page (10 events) here while not full yet so that the read will not singal the not full state
             // we want the batch closing below to signal the not full state
-            Batch b = q.readBatch(10);
+            Batch b = q.readBatch(10, Queue.TIMEOUT_SECOND);
 
             // we expect this next write call to block so let's wrap it in a Future
             Future<Long> future = executor.submit(() -> q.write(element));
@@ -608,7 +608,7 @@ public void queueStillFullAfterPartialPageAckTest() throws IOException, Interrup
             }
             assertThat(q.isFull(), is(true));
 
-            Batch b = q.readBatch(9); // read 90% of page (9 events)
+            Batch b = q.readBatch(9, Queue.TIMEOUT_SECOND); // read 90% of page (9 events)
             b.close();  // this should not purge a page
 
             assertThat(q.isFull(), is(true)); // queue should still be full
@@ -714,7 +714,7 @@ public void concurrentWritesTest() throws IOException, InterruptedException, Exe
             int read_count = 0;
 
             while (read_count < ELEMENT_COUNT * WRITER_COUNT) {
-                Batch b = q.readBatch(BATCH_SIZE);
+                Batch b = q.readBatch(BATCH_SIZE, Queue.TIMEOUT_SECOND);
                 read_count += b.size();
                 b.close();
             }
@@ -789,10 +789,12 @@ private void stableUnderStress(final int capacity) throws IOException {
                     int i = 0;
                     try {
                         while (i < count / concurrent) {
-                            final Batch batch = queue.readBatch(1);
-                            for (final Queueable elem : batch.getElements()) {
-                                if (elem != null) {
-                                    ++i;
+                            final Batch batch = queue.readBatch(1, Queue.TIMEOUT_SECOND);
+                            if (batch != null) {
+                                for (final Queueable elem : batch.getElements()) {
+                                    if (elem != null) {
+                                        ++i;
+                                    }
                                 }
                             }
                         }
@@ -832,7 +834,7 @@ public void inEmpty() throws IOException {
             q.write(new StringElement("foobarbaz"));
             assertThat(q.isEmpty(), is(false));
 
-            Batch b = q.readBatch(1);
+            Batch b = q.readBatch(1, Queue.TIMEOUT_SECOND);
             assertThat(q.isEmpty(), is(false));
 
             b.close();
@@ -917,16 +919,45 @@ public void pageCapacityChangeOnExistingQueue() throws IOException {
             assertThat(q.headPage.getPageIO().getCapacity(), is(NEW_CAPACITY));
 
             // will read only within a page boundary
-            Batch b1 = q.readBatch( 10);
+            Batch b1 = q.readBatch( 10, Queue.TIMEOUT_SECOND);
             assertThat(b1.size(), is(1));
             b1.close();
 
             // will read only within a page boundary
-            Batch b2 = q.readBatch( 10);
+            Batch b2 = q.readBatch( 10, Queue.TIMEOUT_SECOND);
             assertThat(b2.size(), is(1));
             b2.close();
 
             assertThat(q.tailPages.size(), is(0));
         }
     }
+
+
+    @Test(timeout = 5000)
+    public void maximizeBatch() throws IOException, InterruptedException, ExecutionException {
+
+        // very small pages to maximize page creation
+        Settings settings = TestSettings.persistedQueueSettings(1000, dataPath);
+        try (Queue q = new Queue(settings)) {
+            q.open();
+
+            Callable<Integer> writer = () -> {
+                Thread.sleep(500); // sleep 500 ms
+                q.write(new StringElement("E2"));
+                return 1;
+             };
+
+            // write one element now and schedule the 2nd write in 500ms
+            q.write(new StringElement("E1"));
+            Future<Integer> future = executor.submit(writer);
+
+            // issue a batch read with a 2s timeout, normally the batch should contain both element since
+            // the timeout is greater than the 2nd write delay
+            Batch b = q.readBatch(10, Queue.TIMEOUT_SECOND);
+
+            assertThat(b.size(), is(2));
+            future.get();
+        }
+    }
+
 }
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/io/LongVectorTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/io/LongVectorTest.java
index da0f7607c43..6655f47c670 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/io/LongVectorTest.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/io/LongVectorTest.java
@@ -19,4 +19,22 @@ public void storesAndResizes() {
             assertThat((long) i, is(vector.get(i)));
         }
     }
+
+    @Test
+    public void storesVecorAndResizes() {
+        final int count = 1000;
+        final LongVector vector1 = new LongVector(count);
+        for (long i = 0L; i < count; ++i) {
+            vector1.add(i);
+        }
+        final LongVector vector2 = new LongVector(count);
+        for (long i = 0L + count; i < 2 * count; ++i) {
+            vector2.add(i);
+        }
+        vector1.add(vector2);
+        assertThat(vector1.size(), is(2 * count));
+        for (int i = 0; i < 2 * count; ++i) {
+            assertThat((long) i, is(vector1.get(i)));
+        }
+    }
 }
diff --git a/logstash-core/src/test/java/org/logstash/stress/Concurrent.java b/logstash-core/src/test/java/org/logstash/stress/Concurrent.java
index 0093017c023..76504ee7afc 100644
--- a/logstash-core/src/test/java/org/logstash/stress/Concurrent.java
+++ b/logstash-core/src/test/java/org/logstash/stress/Concurrent.java
@@ -75,7 +75,7 @@ public static void oneProducersOneConsumer() throws IOException, InterruptedExce
 
             try {
                 while (consumedCount < ELEMENT_COUNT) {
-                    Batch b = q.readBatch(BATCH_SIZE);
+                    Batch b = q.readBatch(BATCH_SIZE, Queue.TIMEOUT_SECOND);
 //                    if (b.getElements().size() < BATCH_SIZE) {
 //                        System.out.println("read small batch=" + b.getElements().size());
 //                    } else {
@@ -129,7 +129,7 @@ public static void oneProducersOneMultipleConsumer() throws IOException, Interru
             consumers.add(new Thread(() -> {
                 try {
                     while (output.size() < ELEMENT_COUNT) {
-                        Batch b = q.readBatch(BATCH_SIZE);
+                        Batch b = q.readBatch(BATCH_SIZE, Queue.TIMEOUT_SECOND);
 //                        if (b.getElements().size() < BATCH_SIZE) {
 //                            System.out.println("read small batch=" + b.getElements().size());
 //                        } else {
