diff --git a/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueRWBenchmark.java b/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueRWBenchmark.java
index 316025371aa..4f331f30ebf 100644
--- a/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueRWBenchmark.java
+++ b/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueRWBenchmark.java
@@ -3,6 +3,7 @@
 import com.google.common.io.Files;
 import java.io.File;
 import java.io.IOException;
+import java.util.Collections;
 import java.util.concurrent.ArrayBlockingQueue;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
@@ -82,7 +83,7 @@ public final void readFromPersistedQueue(final Blackhole blackhole) throws Excep
         final Future<?> future = exec.submit(() -> {
             for (int i = 0; i < EVENTS_PER_INVOCATION; ++i) {
                 try {
-                    this.queuePersisted.write(EVENT);
+                    this.queuePersisted.write(Collections.singletonList(EVENT), -1);
                 } catch (final IOException ex) {
                     throw new IllegalStateException(ex);
                 }
diff --git a/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueWriteBenchmark.java b/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueWriteBenchmark.java
index c35c37f997c..d4c263a2224 100644
--- a/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueWriteBenchmark.java
+++ b/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueWriteBenchmark.java
@@ -3,6 +3,7 @@
 import com.google.common.io.Files;
 import java.io.File;
 import java.io.IOException;
+import java.util.Collections;
 import java.util.concurrent.TimeUnit;
 import org.apache.commons.io.FileUtils;
 import org.logstash.Event;
@@ -64,7 +65,7 @@ public final void pushToPersistedQueue() throws Exception {
         for (int i = 0; i < EVENTS_PER_INVOCATION; ++i) {
             final Event evnt = EVENT.clone();
             evnt.setTimestamp(Timestamp.now());
-            queue.write(evnt);
+            queue.write(Collections.singletonList(evnt),-1);
         }
     }
 
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
index 12b9040eebe..c0dda810012 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
@@ -3,6 +3,7 @@
 import java.io.Closeable;
 import java.io.IOException;
 import java.util.BitSet;
+import java.util.List;
 
 import org.codehaus.commons.nullanalysis.NotNull;
 import org.logstash.ackedqueue.io.CheckpointIO;
@@ -61,18 +62,20 @@ public SequencedList<byte[]> read(int limit) throws IOException {
         return serialized;
     }
 
-    public void write(byte[] bytes, long seqNum, int checkpointMaxWrites) throws IOException {
+    public void write(List<byte[]> bytesList, long seqNum, int checkpointMaxWrites) throws IOException {
         if (! this.writable) {
             throw new IllegalStateException(String.format("page=%d is not writable", this.pageNum));
         }
 
-        this.pageIO.write(bytes, seqNum);
+        this.pageIO.write(bytesList, seqNum);
 
         if (this.minSeqNum <= 0) {
             this.minSeqNum = seqNum;
             this.firstUnreadSeqNum = seqNum;
         }
-        this.elementCount++;
+        this.elementCount+=bytesList.size();
+
+        seqNum = seqNum + bytesList.size()-1;
 
         // force a checkpoint if we wrote checkpointMaxWrites elements since last checkpoint
         // the initial condition of an "empty" checkpoint, maxSeqNum() will return -1
@@ -235,8 +238,8 @@ public boolean hasSpace(int byteSize) {
      * @param byteSize the date size to verify
      * @return true if data plus overhead fit in page
      */
-    public boolean hasCapacity(int byteSize) {
-        return this.pageIO.persistedByteCount(byteSize) <= this.pageIO.getCapacity();
+    public boolean hasCapacity(int byteSize, int numElements) {
+        return this.pageIO.persistedByteCount(byteSize, numElements) <= this.pageIO.getCapacity();
     }
 
     public void close() throws IOException {
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
index db467d109d5..2177f07d97f 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
@@ -1,5 +1,16 @@
 package org.logstash.ackedqueue;
 
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+import org.logstash.FileLockFactory;
+import org.logstash.LockException;
+import org.logstash.ackedqueue.io.CheckpointIO;
+import org.logstash.ackedqueue.io.FileCheckpointIO;
+import org.logstash.ackedqueue.io.LongVector;
+import org.logstash.ackedqueue.io.MmapPageIO;
+import org.logstash.ackedqueue.io.PageIO;
+import org.logstash.common.FsUtil;
+
 import java.io.Closeable;
 import java.io.IOException;
 import java.lang.reflect.InvocationTargetException;
@@ -10,6 +21,7 @@
 import java.nio.file.Path;
 import java.nio.file.Paths;
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
@@ -18,19 +30,8 @@
 import java.util.concurrent.locks.Condition;
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
-import org.apache.logging.log4j.LogManager;
-import org.apache.logging.log4j.Logger;
-import org.logstash.FileLockFactory;
-import org.logstash.LockException;
-import org.logstash.ackedqueue.io.CheckpointIO;
-import org.logstash.ackedqueue.io.FileCheckpointIO;
-import org.logstash.ackedqueue.io.LongVector;
-import org.logstash.ackedqueue.io.MmapPageIO;
-import org.logstash.ackedqueue.io.PageIO;
-import org.logstash.common.FsUtil;
 
 public final class Queue implements Closeable {
-
     private long seqNum;
 
     protected Page headPage;
@@ -319,16 +320,64 @@ private void newCheckpointedHeadpage(int pageNum) throws IOException {
         this.headPage.forceCheckpoint();
     }
 
+    /**
+     * Write a {@link Queueable} element to the queue. This will block until the write succeeds
+     * @param queueable
+     * @return the written sequence number
+     * @throws IOException
+     */
+    public long write(Queueable queueable) throws IOException {
+        return write(queueable, -1);
+    }
+
+    /**
+     * Write a {@link Queueable} element to the queue. This will block until the write succeeds or timeout occurs
+     * @param queueable
+     * @param timeoutMillis maximum time to block before returning. -1 for infinity
+     * @return the written sequence number, or -1 if timed out
+     * @throws IOException
+     */
+    public long write(Queueable queueable, long timeoutMillis)  throws IOException {
+        return write(Collections.singletonList(queueable), timeoutMillis);
+    }
+
     /**
      * write a {@link Queueable} element to the queue. Note that the element will always be written and the queue full
-     * condition will be checked and waited on **after** the write operation.
+     condition will be checked and waited on **after** the write operation.
      *
-     * @param element the {@link Queueable} element to write
-     * @return the written sequence number
+     * @param queueables list of events
+     * @return the maximum written sequence number
+     * @throws IOException
+     */
+    public long write(List<Queueable> queueables) throws IOException {
+        return write(queueables, -1);
+    }
+
+    /**
+     * Write a {@link Queueable} element to the queue. This will block until the write succeeds or a timeout occurs
+     *
+     * @param queueables list of events
+     * @param timeoutMillis maximum time to block before returning. -1 for infinity
+     * @return the maximum written sequence number, or -1 if timed out
      * @throws IOException
      */
-    public long write(Queueable element) throws IOException {
-        byte[] data = element.serialize();
+    public long write(List<Queueable> queueables, long timeoutMillis) throws IOException {
+        if (queueables.isEmpty()) return -1;
+
+        try {
+            timeoutMillis -= optionallyLockForTimeout(timeoutMillis);
+        } catch (InterruptedException e) {
+            Thread.currentThread().interrupt();
+            return -1;
+        }
+
+        List<byte[]> serializedQueueables = new ArrayList<>(queueables.size());
+        int dataSize = 0;
+        for (Queueable queueable : queueables) {
+            byte[] serialized = queueable.serialize();
+            serializedQueueables.add(serialized);
+            dataSize += serialized.length;
+        }
 
         // the write strategy with regard to the isFull() state is to assume there is space for this element
         // and write it, then after write verify if we just filled the queue and wait on the notFull condition
@@ -337,63 +386,117 @@ public long write(Queueable element) throws IOException {
         // element at risk in the always-full queue state. In the later, when closing a full queue, it would be impossible
         // to write the current element.
 
-        lock.lock();
-        try {
-            if (! this.headPage.hasCapacity(data.length)) {
-                throw new IOException("data to be written is bigger than page capacity");
-            }
+        final int headPageCapacity = this.headPage.getPageIO().getCapacity();
+        // Our batch might span multiple pages, so we have to break it up if its too big
 
-            // create a new head page if the current does not have sufficient space left for data to be written
-            if (! this.headPage.hasSpace(data.length)) {
+        final int persistedBatchSize = this.headPage.getPageIO().persistedByteCount(dataSize, serializedQueueables.size());
 
-                // TODO: verify queue state integrity WRT Queue.open()/recover() at each step of this process
+        if (persistedBatchSize <= headPageCapacity) {
 
-                int newHeadPageNum = this.headPage.pageNum + 1;
 
-                if (this.headPage.isFullyAcked()) {
-                    // here we can just purge the data file and avoid beheading since we do not need
-                    // to add this fully hacked page into tailPages. a new head page will just be created.
-                    // TODO: we could possibly reuse the same page file but just rename it?
-                    this.headPage.purge();
+            try {
+                return unsafeQueueWrite(serializedQueueables, timeoutMillis, persistedBatchSize);
+            } finally {
+                lock.unlock();
+            }
+        } else {
+            // Batch is larger than a page, we can't do an atomic write
+            throw new IOException("data to be written is bigger than page capacity");
+        }
+    }
+
+    /**
+     * Locks blocking, waiting on the lock for the given timeout. -1 is an infinite timeout
+     *
+     * @param timeoutMillis millis to wait for a timeout
+     * @return  If a lock was not acquired, -1.
+     *          If a positive timeout value was given this returns the time spent waiting.
+     *          If no value was given, zero.
+     * @throws InterruptedException when the timeout is exceeded
+     */
+    private long optionallyLockForTimeout(final long timeoutMillis) throws InterruptedException {
+        if (timeoutMillis <= -1) {
+                lock.lock();
+                return 0;
+            } else {
+                // Try to avoid the nanotime calculation with a simple tryLock
+                if (lock.tryLock()) {
+                    return 0;
                 } else {
-                    behead();
+                    final long start = System.nanoTime();
+                    if (lock.tryLock(timeoutMillis, TimeUnit.MILLISECONDS)) {
+                        return timeoutMillis - ((System.nanoTime() - start) / 1000000);
+                    } else {
+                        return -1;
+                    }
                 }
+            }
+    }
 
-                // create new head page
-                newCheckpointedHeadpage(newHeadPageNum);
+    /**
+     * Encapsulates the logic
+     * @param serializedQueueables
+     * @param timeoutMillis
+     * @return
+     * @throws IOException
+     */
+    private long unsafeQueueWrite(List<byte[]> serializedQueueables, long timeoutMillis, int totalPersistedSize) throws IOException {
+        // create a new head page if the current does not have sufficient space left for data to be written
+        if (! this.headPage.hasSpace(totalPersistedSize)) {
+
+            // TODO: verify queue state integrity WRT Queue.open()/recover() at each step of this process
+
+            int newHeadPageNum = this.headPage.pageNum + 1;
+
+            if (this.headPage.isFullyAcked()) {
+                // here we can just purge the data file and avoid beheading since we do not need
+                // to add this fully hacked page into tailPages. a new head page will just be created.
+                // TODO: we could possibly reuse the same page file but just rename it?
+                this.headPage.purge();
+            } else {
+                behead();
             }
 
-            long seqNum = this.seqNum += 1;
-            this.headPage.write(data, seqNum, this.checkpointMaxWrites);
-            this.unreadCount++;
+            // create new head page
+            newCheckpointedHeadpage(newHeadPageNum);
+        }
 
-            notEmpty.signal();
+        long minWrittenSeqNum = this.seqNum + 1;
+        long maxWrittenSeqNum = minWrittenSeqNum + (serializedQueueables.size() - 1);
+        this.headPage.write(serializedQueueables, minWrittenSeqNum, this.checkpointMaxWrites);
+        this.seqNum = maxWrittenSeqNum;
+        this.unreadCount += serializedQueueables.size();
 
-            // now check if we reached a queue full state and block here until it is not full
-            // for the next write or the queue was closed.
-            while (isFull() && !isClosed()) {
-                try {
+        notEmpty.signal();
+
+        // now check if we reached a queue full state and block here until it is not full
+        // for the next write or the queue was closed.
+        while (isFull() && !isClosed()) {
+            try {
+                if (timeoutMillis > 0) {
+                  if (!notFull.await(timeoutMillis, TimeUnit.MILLISECONDS)) {
+                      return -1;
+                  }
+                } else {
                     notFull.await();
-                } catch (InterruptedException e) {
-                    // the thread interrupt() has been called while in the await() blocking call.
-                    // at this point the interrupted flag is reset and Thread.interrupted() will return false
-                    // to any upstream calls on it. for now our choice is to return normally and set back
-                    // the Thread.interrupted() flag so it can be checked upstream.
+                }
+            } catch (InterruptedException e) {
+                // the thread interrupt() has been called while in the await() blocking call.
+                // at this point the interrupted flag is reset and Thread.interrupted() will return false
+                // to any upstream calls on it. for now our choice is to return normally and set back
+                // the Thread.interrupted() flag so it can be checked upstream.
 
-                    // this is a bit tricky in the case of the queue full condition blocking state.
-                    // TODO: we will want to avoid initiating a new write operation if Thread.interrupted() was called.
+                // this is a bit tricky in the case of the queue full condition blocking state.
+                // TODO: we will want to avoid initiating a new write operation if Thread.interrupted() was called.
 
-                    // set back the interrupted flag
-                    Thread.currentThread().interrupt();
+                // set back the interrupted flag
+                Thread.currentThread().interrupt();
 
-                    return seqNum;
-                }
+                return maxWrittenSeqNum;
             }
-
-            return seqNum;
-        } finally {
-            lock.unlock();
         }
+
+        return maxWrittenSeqNum;
     }
 
     /**
@@ -499,7 +602,7 @@ public void ensurePersistedUpto(long seqNum) throws IOException{
     }
 
     /**
-     * non-blocking queue read
+     * non-blocking queue read. This may return fewer items if the requested number span multiple pages
      *
      * @param limit read the next batch of size up to this limit. the returned batch size can be smaller than the requested limit if fewer elements are available
      * @return {@link Batch} the batch containing 1 or more element up to the required limit or null of no elements were available
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JRubyAckedQueueExt.java b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JRubyAckedQueueExt.java
index bbd3b8aab8c..16f31ccab47 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JRubyAckedQueueExt.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JRubyAckedQueueExt.java
@@ -1,6 +1,8 @@
 package org.logstash.ackedqueue.ext;
 
 import java.io.IOException;
+import java.util.List;
+
 import org.jruby.Ruby;
 import org.jruby.RubyBoolean;
 import org.jruby.RubyClass;
@@ -16,6 +18,7 @@
 import org.logstash.ackedqueue.AckedBatch;
 import org.logstash.ackedqueue.Batch;
 import org.logstash.ackedqueue.Queue;
+import org.logstash.ackedqueue.Queueable;
 import org.logstash.ackedqueue.SettingsImpl;
 
 @JRubyClass(name = "AckedQueue")
@@ -96,9 +99,9 @@ public void open() throws IOException {
         queue.open();
     }
 
-    public void rubyWrite(ThreadContext context, Event event) {
+    public void rubyWrite(ThreadContext context, List<Queueable> events, long timeout) {
         try {
-            this.queue.write(event);
+            this.queue.write(events, timeout);
         } catch (IOException e) {
             throw RubyUtil.newRubyIOError(context.runtime, e);
         }
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JRubyWrappedAckedQueueExt.java b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JRubyWrappedAckedQueueExt.java
index 55c3a04c064..405a80d69f1 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JRubyWrappedAckedQueueExt.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JRubyWrappedAckedQueueExt.java
@@ -1,6 +1,7 @@
 package org.logstash.ackedqueue.ext;
 
 import java.io.IOException;
+import java.util.Collections;
 import java.util.concurrent.atomic.AtomicBoolean;
 import org.jruby.Ruby;
 import org.jruby.RubyBoolean;
@@ -66,7 +67,7 @@ public IRubyObject rubyClose(ThreadContext context) {
     @JRubyMethod(name = {"push", "<<"})
     public void rubyPush(ThreadContext context, IRubyObject event) {
         checkIfClosed("write");
-        queue.rubyWrite(context, ((JrubyEventExtLibrary.RubyEvent) event).getEvent());
+        queue.rubyWrite(context, Collections.singletonList(((JrubyEventExtLibrary.RubyEvent) event).getEvent()),-1);
     }
 
     @JRubyMethod(name = "read_batch")
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/MmapPageIO.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/MmapPageIO.java
index 3a3d8051019..119a9f4c375 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/io/MmapPageIO.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/MmapPageIO.java
@@ -3,11 +3,13 @@
 import java.io.File;
 import java.io.IOException;
 import java.io.RandomAccessFile;
+import java.nio.ByteBuffer;
 import java.nio.MappedByteBuffer;
 import java.nio.channels.FileChannel;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.List;
 import java.util.zip.CRC32;
 import org.apache.logging.log4j.LogManager;
@@ -204,10 +206,75 @@ public void purge() throws IOException {
     }
 
     @Override
-    public void write(byte[] bytes, long seqNum) {
-        write(bytes, seqNum, bytes.length, checksum(bytes));
+    public void write(final List<byte[]> bytesList, final long minSeqNum) {
+        int persistedSize = bytesList.stream().mapToInt(b -> b.length).sum();
+
+        // We want to make our writes atomic, hence this buffer
+        int writeSize = persistedByteCount(persistedSize, bytesList.size());
+
+        final int initialHead = this.head;
+        int mainBufferOffset = initialHead;
+        final int[] newOffsets = new int[bytesList.size()];
+
+        buffer.position(initialHead);
+
+        for (int i = 0; i < bytesList.size(); i++) {
+            byte[] bytes = bytesList.get(i);
+            long seqNum = minSeqNum + i;
+            // since writes always happen at head, we can just append head to the offsetMap
+            assert this.offsetMap.size() == this.elementCount :
+                String.format("offsetMap size=%d != elementCount=%d", this.offsetMap.size(), this.elementCount);
+
+            newOffsets[i] = mainBufferOffset;
+
+            buffer.putLong(seqNum);
+            buffer.putInt(bytes.length);
+            buffer.put(bytes);
+            buffer.putInt(checksum(bytes));
+
+            mainBufferOffset += persistedByteCount(bytes.length, 1);
+        }
+
+        this.head += writeSize;
+
+        assert this.head == buffer.position() :
+                String.format("head=%d != buffer position=%d", this.head, buffer.position());
+
+        if (this.elementCount <= 0) {
+            this.minSeqNum = minSeqNum;
+        }
+        for (int o : newOffsets) {
+            this.offsetMap.add(o);
+        }
+        this.elementCount+=bytesList.size();
     }
 
+    private int write(byte[] bytes, long seqNum, int length, int checksum) {
+        // since writes always happen at head, we can just append head to the offsetMap
+        assert this.offsetMap.size() == this.elementCount :
+            String.format("offsetMap size=%d != elementCount=%d", this.offsetMap.size(), this.elementCount);
+
+        int initialHead = this.head;
+        buffer.position(this.head);
+        buffer.putLong(seqNum);
+        buffer.putInt(length);
+        buffer.put(bytes);
+        buffer.putInt(checksum);
+        this.head += persistedByteCount(bytes.length, 1);
+
+        assert this.head == buffer.position() :
+            String.format("head=%d != buffer position=%d", this.head, buffer.position());
+
+        if (this.elementCount <= 0) {
+            this.minSeqNum = seqNum;
+        }
+        this.offsetMap.add(initialHead);
+        this.elementCount++;
+
+        return initialHead;
+    }
+
+
     @Override
     public void close() {
         if (this.buffer != null) {
@@ -236,12 +303,13 @@ public int getElementCount() {
     @Override
     public boolean hasSpace(int bytes) {
         int bytesLeft = this.capacity - this.head;
-        return persistedByteCount(bytes) <= bytesLeft;
+        return bytes <= bytesLeft;
     }
 
     @Override
-    public int persistedByteCount(int byteCount) {
-        return SEQNUM_SIZE + LENGTH_SIZE + byteCount + CHECKSUM_SIZE;
+    public int persistedByteCount(int byteCount, int numElements) {
+        int metadataBytes = (SEQNUM_SIZE + LENGTH_SIZE + CHECKSUM_SIZE) * numElements;
+        return byteCount + metadataBytes;
     }
 
     @Override
@@ -336,30 +404,6 @@ private void readNextElement(long expectedSeqNum, boolean verifyChecksum) throws
         buffer.position(this.head);
     }
 
-    private int write(byte[] bytes, long seqNum, int length, int checksum) {
-        // since writes always happen at head, we can just append head to the offsetMap
-        assert this.offsetMap.size() == this.elementCount :
-            String.format("offsetMap size=%d != elementCount=%d", this.offsetMap.size(), this.elementCount);
-
-        int initialHead = this.head;
-        buffer.position(this.head);
-        buffer.putLong(seqNum);
-        buffer.putInt(length);
-        buffer.put(bytes);
-        buffer.putInt(checksum);
-        this.head += persistedByteCount(bytes.length);
-
-        assert this.head == buffer.position() :
-            String.format("head=%d != buffer position=%d", this.head, buffer.position());
-
-        if (this.elementCount <= 0) {
-            this.minSeqNum = seqNum;
-        }
-        this.offsetMap.add(initialHead);
-        this.elementCount++;
-
-        return initialHead;
-    }
 
     // we don't have different versions yet so simply check if the version is VERSION_ONE for basic integrity check
     // and if an unexpected version byte is read throw PageIOInvalidVersionException
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/PageIO.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/PageIO.java
index 6560b5a4c31..afb6aa1d704 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/io/PageIO.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/PageIO.java
@@ -4,6 +4,7 @@
 
 import java.io.Closeable;
 import java.io.IOException;
+import java.util.List;
 
 public interface PageIO extends Closeable {
 
@@ -29,7 +30,7 @@ public interface PageIO extends Closeable {
     boolean hasSpace(int bytes);
 
     // write the given bytes to the data container
-    void write(byte[] bytes, long seqNum) throws IOException;
+    void write(List<byte[]> bytesList, long seqNum) throws IOException;
 
     // read up to limit number of items starting at give seqNum
     SequencedList<byte[]> read(long seqNum, int limit) throws IOException;
@@ -41,7 +42,7 @@ public interface PageIO extends Closeable {
     int getHead();
 
     // @return the actual persisted byte count (with overhead) for the given data bytes
-    int persistedByteCount(int bytes);
+    int persistedByteCount(int bytes, int numElements);
 
     // signal that this data page is not active and resources can be released
     void deactivate() throws IOException;
diff --git a/logstash-core/src/main/java/org/logstash/ext/JrubyAckedWriteClientExt.java b/logstash-core/src/main/java/org/logstash/ext/JrubyAckedWriteClientExt.java
index ebe14b681d5..6d771d06767 100644
--- a/logstash-core/src/main/java/org/logstash/ext/JrubyAckedWriteClientExt.java
+++ b/logstash-core/src/main/java/org/logstash/ext/JrubyAckedWriteClientExt.java
@@ -1,7 +1,12 @@
 package org.logstash.ext;
 
+import java.util.ArrayList;
 import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
 import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.stream.Collectors;
+
 import org.jruby.Ruby;
 import org.jruby.RubyClass;
 import org.jruby.RubyObject;
@@ -10,6 +15,8 @@
 import org.jruby.runtime.ThreadContext;
 import org.jruby.runtime.builtin.IRubyObject;
 import org.logstash.RubyUtil;
+import org.logstash.ackedqueue.Batch;
+import org.logstash.ackedqueue.Queueable;
 import org.logstash.ackedqueue.ext.JRubyAckedQueueExt;
 
 @JRubyClass(name = "AckedWriteClient")
@@ -50,16 +57,17 @@ private JrubyAckedWriteClientExt(final Ruby runtime, final RubyClass metaClass,
     @JRubyMethod(name = {"push", "<<"}, required = 1)
     public IRubyObject rubyPush(final ThreadContext context, IRubyObject event) {
         ensureOpen();
-        queue.rubyWrite(context, ((JrubyEventExtLibrary.RubyEvent) event).getEvent());
+        queue.rubyWrite(context, Collections.singletonList(((JrubyEventExtLibrary.RubyEvent) event).getEvent()), -1);
         return this;
     }
 
     @JRubyMethod(name = "push_batch", required = 1)
-    public IRubyObject rubyPushBatch(final ThreadContext context, IRubyObject batch) {
+    public IRubyObject rubyPushBatch(final ThreadContext context, final IRubyObject batch) {
         ensureOpen();
-        for (final IRubyObject event : (Collection<JrubyEventExtLibrary.RubyEvent>) batch) {
-            queue.rubyWrite(context, ((JrubyEventExtLibrary.RubyEvent) event).getEvent());
-        }
+        final List<JrubyEventExtLibrary.RubyEvent> rubyEvents = (List<JrubyEventExtLibrary.RubyEvent>) batch;
+        final List<Queueable> events = new ArrayList<>(rubyEvents.size());
+        for (JrubyEventExtLibrary.RubyEvent rubyEvent : rubyEvents) events.add(rubyEvent.getEvent());
+        queue.rubyWrite(context, events, -1);
         return this;
     }
 
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
index 2af5e3c784a..5d9d5d98a45 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
@@ -2,6 +2,7 @@
 
 import java.io.IOException;
 import java.nio.file.Paths;
+import java.util.Collections;
 import java.util.concurrent.TimeUnit;
 import org.junit.Before;
 import org.junit.Rule;
@@ -57,7 +58,7 @@ public void pageWrite() throws IOException {
             Page p = q.headPage;
 
             assertThat(p.hasSpace(element.serialize().length), is(true));
-            p.write(element.serialize(), 0, 1);
+            p.write(Collections.singletonList(element.serialize()), 0, 1);
 
             assertThat(p.hasSpace(element.serialize().length), is(false));
             assertThat(p.isFullyRead(), is(false));
@@ -76,7 +77,7 @@ public void pageWriteAndReadSingle() throws IOException {
             Page p = q.headPage;
 
             assertThat(p.hasSpace(element.serialize().length), is(true));
-            p.write(element.serialize(), seqNum, 1);
+            p.write(Collections.singletonList(element.serialize()), seqNum, 1);
 
             Batch b = new Batch(p.read(1), q);
 
@@ -99,7 +100,7 @@ public void inEmpty() throws IOException {
             Page p = q.headPage;
 
             assertThat(p.isEmpty(), is(true));
-            p.write(element.serialize(), 1, 1);
+            p.write(Collections.singletonList(element.serialize()), 1, 1);
             assertThat(p.isEmpty(), is(false));
             Batch b = q.readBatch(1, TimeUnit.SECONDS.toMillis(1));
             assertThat(p.isEmpty(), is(false));
@@ -121,7 +122,7 @@ public void pageWriteAndReadMulti() throws IOException {
             Page p = q.headPage;
 
             assertThat(p.hasSpace(element.serialize().length), is(true));
-            p.write(element.serialize(), seqNum, 1);
+            p.write(Collections.singletonList(element.serialize()), seqNum, 1);
 
             Batch b = new Batch(p.read(10), q);
 
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
index 03d6fb78049..a6a507def11 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
@@ -8,7 +8,9 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
 import java.util.Random;
 import java.util.concurrent.Callable;
 import java.util.concurrent.ExecutionException;
@@ -23,6 +25,7 @@
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
+import org.logstash.Event;
 import org.logstash.ackedqueue.io.LongVector;
 import org.logstash.ackedqueue.io.MmapPageIO;
 
@@ -43,10 +46,17 @@ public class QueueTest {
 
     private String dataPath;
 
+    private List<Queueable> batch;
+
     @Before
     public void setUp() throws Exception {
         dataPath = temporaryFolder.newFolder("data").getPath();
         executor = Executors.newSingleThreadExecutor();
+        int batchSize = 10;
+        batch = new ArrayList<>(batchSize);
+        for (int i = 0; i < batchSize; i++) {
+            batch.add(new StringElement("foo " + i));
+        }
     }
 
     @After
@@ -82,6 +92,24 @@ public void singleWriteRead() throws IOException {
         }
     }
 
+    @Test
+    public void batchWriteRead() throws IOException {
+        // We need a fairly large size because reads don't span pages
+        try (Queue q = new Queue(TestSettings.persistedQueueSettings(1000, dataPath))) {
+            q.open();
+
+            long res = q.write(batch);
+
+            Batch b = q.nonBlockReadBatch(batch.size());
+
+            assertThat(b.getElements().size(), is(batch.size()));
+            for (int i = 0; i < batch.size(); i++) {
+                assertThat(b.getElements().get(i).toString(), is(batch.get(i).toString()));
+            }
+            assertThat(q.nonBlockReadBatch(1), nullValue());
+        }
+    }
+
     /**
      * This test guards against issue https://github.com/elastic/logstash/pull/8186 by ensuring
      * that repeated writes to an already fully acknowledged headpage do not corrupt the queue's
@@ -132,7 +160,7 @@ public void writeWhenPageEqualsQueueSize() throws IOException {
             TestSettings.persistedQueueSettings(1024, 1024L, dataPath))) {
             q.open();
             for (int i = 0; i < 3; ++i) {
-                q.write(element);
+                long res = q.write(element);
                 try (Batch b = q.readBatch(1, 500L)) {
                     assertThat(b.getElements().size(), is(1));
                     assertThat(b.getElements().get(0).toString(), is(element.toString()));
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileMmapIOTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileMmapIOTest.java
index d8e6d61c197..e3c830ad4de 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileMmapIOTest.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileMmapIOTest.java
@@ -9,6 +9,7 @@
 import org.logstash.ackedqueue.StringElement;
 
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.List;
 
 import static org.hamcrest.CoreMatchers.equalTo;
@@ -42,7 +43,7 @@ public void roundTrip() throws Exception {
         for (int i = 1; i < 17; i++) {
             StringElement input = new StringElement("element-" + i);
             list.add(input);
-            writeIo.write(input.serialize(), i);
+            writeIo.write(Collections.singletonList(input.serialize()), i);
         }
         writeIo.close();
         readIo.open(1, 16);
