diff --git a/docs/index.asciidoc b/docs/index.asciidoc
index ead78d2ae22..b785ae12d14 100644
--- a/docs/index.asciidoc
+++ b/docs/index.asciidoc
@@ -74,11 +74,9 @@ include::static/shutdown.asciidoc[]
 // Upgrading Logstash
 include::static/upgrading.asciidoc[]
 
-// Configuring Logstash
+// Configuring pipelines
 include::static/pipeline-configuration.asciidoc[]
 
-include::static/ls-to-cloud.asciidoc[]
-
 // Security
 include::static/security/logstash.asciidoc[]
 
diff --git a/docs/static/env-vars.asciidoc b/docs/static/env-vars.asciidoc
index 6d550a01d96..91604861c42 100644
--- a/docs/static/env-vars.asciidoc
+++ b/docs/static/env-vars.asciidoc
@@ -1,20 +1,20 @@
 [[environment-variables]]
-=== Using environment variables in the configuration
+=== Using environment variables
 
 ==== Overview
 
 * You can set environment variable references in the configuration for Logstash plugins by using `${var}`.
-* At Logstash startup, each reference will be replaced by the value of the environment variable.
+* At Logstash startup, each reference is replaced by the value of the environment variable.
 * The replacement is case-sensitive.
 * References to undefined variables raise a Logstash configuration error.
 * You can give a default value by using the form `${var:default value}`. Logstash uses the default value if the
 environment variable is undefined.
-* You can add environment variable references in any plugin option type : string, number, boolean, array, or hash.
+* You can add environment variable references in any plugin option type: string, number, boolean, array, or hash.
 * Environment variables are immutable. If you update the environment variable, you'll have to restart Logstash to pick up the updated value.
 
 ==== Examples
 
-The following examples show you how to use environment variables to set the values of some commonly used
+These examples show you how to use environment variables to set the values of some commonly used
 configuration options.
 
 ===== Setting the TCP port
@@ -37,7 +37,7 @@ Now let's set the value of `TCP_PORT`:
 export TCP_PORT=12345
 ----
 
-At startup, Logstash uses the following configuration:
+At startup, Logstash uses this configuration:
 
 [source,ruby]
 ----------------------------------
@@ -94,7 +94,7 @@ Let's set the value of `ENV_TAG`:
 export ENV_TAG="tag2"
 ----
 
-At startup, Logstash uses the following configuration:
+At startup, Logstash uses this configuration:
 
 [source,ruby]
 ----
diff --git a/docs/static/event-data.asciidoc b/docs/static/event-data.asciidoc
index 1a958ff5137..78713346f56 100644
--- a/docs/static/event-data.asciidoc
+++ b/docs/static/event-data.asciidoc
@@ -1,38 +1,37 @@
 [[event-dependent-configuration]]
-=== Accessing event data and fields in the configuration
+=== Accessing event data and fields
 
-The logstash agent is a processing pipeline with 3 stages: inputs -> filters ->
-outputs. Inputs generate events, filters modify them, outputs ship them
-elsewhere.
+A Logstash pipeline usually has three stages: inputs -> filters -> outputs. 
+Inputs generate events, filters modify them, and outputs ship them elsewhere.
 
-All events have properties. For example, an apache access log would have things
+All events have properties. 
+For example, an Apache access log has properties
 like status code (200, 404), request path ("/", "index.html"), HTTP verb
-(GET, POST), client IP address, etc. Logstash calls these properties "fields."
+(GET, POST), client IP address, and so forth. 
+Logstash calls these properties "fields".
 
-Some of the configuration options in Logstash require the existence of fields in
-order to function.  Because inputs generate events, there are no fields to
+Some configuration options in Logstash require the existence of fields in
+order to function.  
+Because inputs generate events, there are no fields to
 evaluate within the input block--they do not exist yet!
 
-Because of their dependency on events and fields, the following configuration
-options will only work within filter and output blocks.
-
-IMPORTANT: Field references, sprintf format and conditionals, described below,
-do not work in an input block.
+IMPORTANT: <<logstash-config-field-references>>, <<sprintf>>, and <<conditionals,conditionals>> do not work in input blocks.
+These configuration options depend on events and fields, and therefore, work only within filter and output blocks.
 
 [discrete]
 [[logstash-config-field-references]]
 ==== Field references
 
-It is often useful to be able to refer to a field by name. To do this,
-you can use the Logstash <<field-references-deepdive,field reference syntax>>.
+When you need to refer to a field by name, you can use the Logstash <<field-references-deepdive,field reference syntax>>. 
 
-The basic syntax to access a field is `[fieldname]`. If you are referring to a
-**top-level field**, you can omit the `[]` and simply use `fieldname`.
-To refer to a **nested field**, you specify
-the full path to that field: `[top-level field][nested field]`.
+The basic syntax to access a field is `[fieldname]`. 
+If you are referring to a **top-level field**, you can omit the `[]` and simply
+use `fieldname`.
+To refer to a **nested field**, specify the full path to that field:
+`[top-level field][nested field]`.
 
-For example, the following event has five top-level fields (agent, ip, request, response,
-ua) and three nested fields (status, bytes, os).
+For example, this event has five top-level fields (agent, ip, request,
+response, ua) and three nested fields (status, bytes, os).
 
 [source,js]
 ----------------------------------
@@ -51,8 +50,9 @@ ua) and three nested fields (status, bytes, os).
 
 ----------------------------------
 
-To reference the `os` field, you specify `[ua][os]`. To reference a top-level
-field such as `request`, you can simply specify the field name.
+To reference the `os` field, specify `[ua][os]`. 
+To reference a top-level field such as `request`, you can simply specify the
+field name.
 
 For more detailed information, see <<field-references-deepdive>>.
 
@@ -60,10 +60,10 @@ For more detailed information, see <<field-references-deepdive>>.
 [[sprintf]]
 ==== sprintf format
 
-The field reference format is also used in what Logstash calls 'sprintf format'. This format
-enables you to refer to field values from within other strings. For example, the
-statsd output has an 'increment' setting that enables you to keep a count of
-apache logs by status code:
+The field reference format is also used in what Logstash calls 'sprintf format'. 
+This format enables you to embed field values in other strings. 
+For example, the statsd output has an 'increment' setting that enables you to
+keep a count of apache logs by status code:
 
 [source,js]
 ----------------------------------
@@ -90,7 +90,7 @@ output {
 ----------------------------------
 
 NOTE: The sprintf format continues to support http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html[deprecated joda time format] strings as well using the `%{+FORMAT}` syntax.
-      These formats are not directly interchangeable, and we advise you to begin using the more modern Java Time format.
+These formats are not directly interchangeable, and we advise you to begin using the more modern Java Time format.
 
 NOTE: A Logstash timestamp represents an instant on the UTC-timeline, so using sprintf formatters will produce results that may not align with your machine-local timezone.
 
@@ -120,7 +120,7 @@ if EXPRESSION {
 
 What's an expression? Comparison tests, boolean logic, and so on!
 
-You can use the following comparison operators:
+You can use these comparison operators:
 
 * equality: `==`,  `!=`,  `<`,  `>`,  `<=`, `>=`
 * regexp: `=~`, `!~` (checks a pattern on the right against a string value on the left)
@@ -137,7 +137,7 @@ Supported unary operators are:
 Expressions can be long and complex. Expressions can contain other expressions,
 you can negate expressions with `!`, and you can group them with parentheses `(...)`.
 
-For example, the following conditional uses the mutate filter to remove the field `secret` if the field
+For example, this conditional uses the mutate filter to remove the field `secret` if the field
 `action` has a value of `login`:
 
 [source,js]
@@ -413,4 +413,4 @@ filter {
     }
   }
 }
-----------------------------------
\ No newline at end of file
+----------------------------------
diff --git a/docs/static/ls-to-cloud.asciidoc b/docs/static/ls-to-cloud.asciidoc
index 9c6b2752406..541a29ebb71 100644
--- a/docs/static/ls-to-cloud.asciidoc
+++ b/docs/static/ls-to-cloud.asciidoc
@@ -1,12 +1,15 @@
 [[connecting-to-cloud]]
 === Sending data to Elastic Cloud (hosted Elasticsearch Service)
 
-Elastic Cloud is our our hosted Elasticsearch Service, and it is
-available on AWS, GCP, and Azure.
-{ess-trial}[You can try the Elasticsearch Service for free].
+Our hosted {ess} on https://cloud.elastic.co/[Elastic Cloud] simplifies safe, secure communication between {ls} and {es}. 
+When you configure the Elasticsearch output plugin to use <<plugins-outputs-elasticsearch-cloud_id,`cloud_id`>> with either the <<plugins-outputs-elasticsearch-cloud_auth,`cloud_auth` option>> or the <<plugins-outputs-elasticsearch-api_key,`api_key` option>>, no additional SSL configuration is needed.
 
-Logstash comes with two settings that simplify sending data to
-https://cloud.elastic.co/[Elastic Cloud]: Cloud ID and Cloud Auth. 
+Examples:
+
+* `output {elasticsearch { cloud_id => "<cloud id>" cloud_auth => "<cloud auth>" } }`
+* `output {elasticsearch { cloud_id => "<cloud id>" api_key => "<api key>" } }``
+
+{ess-leadin-short}
 
 [[cloud-id]]
 ==== Cloud ID
diff --git a/docs/static/pipeline-config-exps.asciidoc b/docs/static/pipeline-config-exps.asciidoc
index bb4926bf28d..e1364965929 100644
--- a/docs/static/pipeline-config-exps.asciidoc
+++ b/docs/static/pipeline-config-exps.asciidoc
@@ -4,8 +4,7 @@
 These examples illustrate how you can configure Logstash to filter events, process Apache logs and syslog messages, and use conditionals to control what events are processed by a filter or output.
 
 TIP: If you need help building grok patterns, try out the
-{kibana-ref}/xpack-grokdebugger.html[Grok Debugger]. The Grok Debugger is an
-{xpack} feature under the Basic License and is therefore *free to use*.
+{kibana-ref}/xpack-grokdebugger.html[Grok Debugger]. 
 
 [discrete]
 [[filter-example]]
diff --git a/docs/static/pipeline-configuration.asciidoc b/docs/static/pipeline-configuration.asciidoc
index 85e3b8f9cd2..07c3e5edaf1 100644
--- a/docs/static/pipeline-configuration.asciidoc
+++ b/docs/static/pipeline-configuration.asciidoc
@@ -1,38 +1,43 @@
 [[configuration]]
-== Configuring Logstash
+== Creating a {ls} pipeline
 
-To configure Logstash, you create a config file that specifies which plugins you want to use and settings for each plugin.
-You can reference event fields in a configuration and use conditionals to process events when they meet certain
-criteria. When you run logstash, you use the `-f` to specify your config file.
+You can create a pipeline by stringing together plugins--<<input-plugins,inputs>>, <<output-plugins,outputs>>, <<filter-plugins,filters>>, and sometimes <<codec-plugins,codecs>>--in order to process data. 
+To build a Logstash pipeline, create a config file to specify which plugins you want to use and the settings for each plugin.
 
-Let's step through creating a simple config file and using it to run Logstash. Create a file named "logstash-simple.conf" and save it in the same directory as Logstash.
+A very basic pipeline might contain only an input and an output. 
+Most pipelines include at least one filter plugin because that's where the "transform" part of the ETL (extract, transform, load) magic happens. 
+You can reference event fields in a pipeline and use conditionals to process events when they meet certain criteria. 
+
+Let's step through creating a simple pipeline config on your local machine and then using it to run Logstash. 
+Create a file named "logstash-simple.conf" and save it in the same directory as Logstash.
 
 [source,ruby]
 ----------------------------------
 input { stdin { } }
 output {
-  elasticsearch { hosts => ["localhost:9200"] }
+  elasticsearch { cloud_id => "<cloud id>" api_key => "<api key>" }  
   stdout { codec => rubydebug }
 }
 ----------------------------------
 
-Then, run logstash and specify the configuration file with the `-f` flag.
+Then, run {ls} and specify the configuration file with the `-f` flag.
 
 [source,ruby]
-----------------------------------
+-----
 bin/logstash -f logstash-simple.conf
-----------------------------------
+-----
 
-Et voilà! Logstash reads  the specified configuration file and outputs to both Elasticsearch and stdout. Note that if you see a message in stdout that reads "Elasticsearch Unreachable" that you will need to make sure Elasticsearch is installed and up and reachable on port 9200. Before we
-move on to some <<config-examples,more complex examples>>, let's take a closer look at what's in a config file.
+Et voilà! Logstash reads the specified configuration file and outputs to both Elasticsearch and stdout. 
+Before we move on to <<config-examples,more complex examples>>, let's take a look at what's in a pipeline config file.
 
 [[configuration-file-structure]]
-=== Structure of a config file
+=== Structure of a pipeline
 
-A Logstash config file has a separate section for each type of plugin you want to add to the event processing pipeline. For example:
+A {ls} pipeline config file has a separate section for each type of plugin you want to add to the event processing pipeline. 
+For example:
 
 [source,js]
-----------------------------------
+-----
 # This is a comment. You should use comments to describe
 # parts of your configuration.
 input {
@@ -46,37 +51,42 @@ filter {
 output {
   ...
 }
-----------------------------------
+-----
 
-Each section contains the configuration options for one or more plugins. If you specify
-multiple filters, they are applied in the order of their appearance in the configuration file.
+Each section contains configuration options for one or more plugins. 
+If you specify multiple filters, they are applied in the order they appear in the configuration file.
+If you specify multiple outputs, events are sent to each destination sequentially, in the order they appear in the configuration file.
+
+TIP: When you are ready to deploy a pipeline beyond your local machine, add the pipeline config file to <<logstash-settings-file,`logstash.yml`>> using the `pipeline.id` setting. 
+When you are ready to deploy <<multiple-pipelines,multiple pipelines>>, set up and configure your pipelines in the `pipelines.yml` file. 
 
 
 [discrete]
 [[plugin_configuration]]
 === Plugin configuration
 
-The configuration of a plugin consists of the plugin name followed
-by a block of settings for that plugin. For example, this input section configures two file inputs:
+A plugin configuration consists of the plugin name followed by a block of settings for that plugin. 
+For example, this input section configures two file inputs:
 
 [source,js]
-----------------------------------
+-----
 input {
-  file {
-    path => "/var/log/messages"
-    type => "syslog"
+  http {
+    port => 3333
+    tags => gateway
   }
-
-  file {
-    path => "/var/log/apache/access.log"
-    type => "apache"
+  http {
+    port => 4444
+    tags => billing
   }
 }
-----------------------------------
+-----
+
 
-In this example, two settings are configured for each of the file inputs: 'path' and 'type'.
+In this example, two settings are configured for each of the file inputs: 'port' and 'tags'.
 
-The settings you can configure vary according to the plugin type. For information about each plugin, see <<input-plugins,Input Plugins>>, <<output-plugins, Output Plugins>>, <<filter-plugins,Filter Plugins>>, and <<codec-plugins,Codec Plugins>>.
+The settings you can configure vary according to the plugin type. 
+For information about each plugin, see <<input-plugins,Input Plugins>>, <<output-plugins, Output Plugins>>, <<filter-plugins,Filter Plugins>>, and <<codec-plugins,Codec Plugins>>.
 
 [discrete]
 [[plugin-value-types]]
@@ -304,4 +314,5 @@ input { # comments can appear at the end of a line, too
 
 include::event-data.asciidoc[]
 include::env-vars.asciidoc[]
+include::ls-to-cloud.asciidoc[]
 include::pipeline-config-exps.asciidoc[]
