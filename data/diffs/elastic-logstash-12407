diff --git a/x-pack/lib/config_management/bootstrap_check.rb b/x-pack/lib/config_management/bootstrap_check.rb
index 432dd910db0..1729873e03b 100644
--- a/x-pack/lib/config_management/bootstrap_check.rb
+++ b/x-pack/lib/config_management/bootstrap_check.rb
@@ -14,6 +14,10 @@ module ConfigManagement
     class BootstrapCheck
       include LogStash::Util::Loggable
 
+      # pipeline ID must begin with a letter or underscore and contain only letters, underscores, dashes, and numbers
+      # wildcard character `*` is also acceptable and follows globbing rules
+      PIPELINE_ID_PATTERN = %r{\A[a-z_*][a-z_\-0-9*]*\Z}i
+
       def self.check(settings)
         check_path_config(settings)
 
@@ -42,6 +46,11 @@ def self.check(settings)
           raise LogStash::BootstrapCheckError, "You need to specify the ID of the pipelines with the `xpack.management.pipeline.id` options in your logstash.yml"
         end
 
+        invalid_patterns =  pipeline_ids.reject { |entry| PIPELINE_ID_PATTERN =~ entry }
+        if invalid_patterns.any?
+          raise LogStash::BootstrapCheckError, "Pipeline id in `xpack.management.pipeline.id` must begin with a letter or underscore and contain only letters, underscores, dashes, and numbers. The asterisk wildcard `*` can also be used. Invalid ids: #{invalid_patterns.join(', ')}"
+        end
+
         duplicate_ids = find_duplicate_ids(pipeline_ids)
         if duplicate_ids.size > 0
           raise LogStash::BootstrapCheckError, "Duplicate pipeline ids found in `xpack.management.pipeline.id`, defined IDs must be unique, Duplicated ids: #{duplicate_ids.join(', ')}"
diff --git a/x-pack/lib/config_management/elasticsearch_source.rb b/x-pack/lib/config_management/elasticsearch_source.rb
index fa5c3736cff..50f4597e7f7 100644
--- a/x-pack/lib/config_management/elasticsearch_source.rb
+++ b/x-pack/lib/config_management/elasticsearch_source.rb
@@ -80,17 +80,12 @@ def pipeline_configs
         fetcher = get_pipeline_fetcher
         fetcher.fetch_config(pipeline_ids, client)
 
-        @cached_pipelines = pipeline_ids.collect do |pid|
+        @cached_pipelines = fetcher.get_pipeline_ids.collect do |pid|
           get_pipeline(pid, fetcher)
         end.compact
       end
 
       def get_pipeline(pipeline_id, fetcher)
-        unless fetcher.config_exist?(pipeline_id)
-          logger.debug("Could not find a remote configuration for a specific `pipeline_id`", :pipeline_id => pipeline_id)
-          return nil
-        end
-
         config_string = fetcher.get_single_pipeline_setting(pipeline_id)["pipeline"]
 
         raise RemoteConfigError, "Empty configuration for pipeline_id: #{pipeline_id}" if config_string.nil? || config_string.empty?
@@ -188,12 +183,18 @@ def client
     end
 
     module Fetcher
-      def config_exist?(pipeline_id)
-        @response.has_key?(pipeline_id)
+      include LogStash::Util::Loggable
+
+      def get_pipeline_ids
+        @pipelines.keys
       end
 
       def fetch_config(pipeline_ids, client) end
       def get_single_pipeline_setting(pipeline_id) end
+
+      def log_pipeline_not_found(pipeline_ids)
+        logger.debug("Could not find a remote configuration for specific `pipeline_id`", :pipeline_ids => pipeline_ids) if pipeline_ids.any?
+      end
     end
 
     class SystemIndicesFetcher
@@ -202,18 +203,44 @@ class SystemIndicesFetcher
       SYSTEM_INDICES_API_PATH = "_logstash/pipeline"
 
       def fetch_config(pipeline_ids, client)
-        path_ids = pipeline_ids.join(",")
-        response = client.get("#{SYSTEM_INDICES_API_PATH}/#{path_ids}")
+        response = client.get("#{SYSTEM_INDICES_API_PATH}/")
 
         if response["error"]
           raise ElasticsearchSource::RemoteConfigError, "Cannot find find configuration for pipeline_id: #{pipeline_ids}, server returned status: `#{response["status"]}`, message: `#{response["error"]}`"
         end
 
-        @response = response
+        @pipelines = get_wildcard_pipelines(pipeline_ids, response)
       end
 
       def get_single_pipeline_setting(pipeline_id)
-        @response.fetch(pipeline_id, {})
+        @pipelines.fetch(pipeline_id, {})
+      end
+
+      private
+      # get pipelines if pipeline_ids match wildcard patterns
+      # split user pipeline id setting into wildcard and non wildcard pattern
+      # take the non wildcard pipelines. take the wildcard pipelines by matching with glob pattern
+      def get_wildcard_pipelines(pipeline_ids, response)
+        wildcard_patterns, fix_pids = pipeline_ids.partition { |pattern| pattern.include?("*")}
+
+        fix_id_pipelines = fix_pids.map { |id|
+          response.has_key?(id) ? {id => response[id]}: {}
+        }.reduce({}, :merge)
+        fix_id_pipelines.keys.map { |id| response.delete(id)}
+
+        wildcard_matched_patterns = Set.new
+        wildcard_pipelines = response.keys.map { |id|
+          found_pattern = wildcard_patterns.any? { |pattern|
+            matched = ::File::fnmatch?(pattern, id)
+            wildcard_matched_patterns << pattern if matched
+            matched
+          }
+          found_pattern ? {id => response[id]}: {}
+        }.reduce({}, :merge)
+
+        log_pipeline_not_found((fix_pids - fix_id_pipelines.keys) + (wildcard_patterns - wildcard_matched_patterns.to_a))
+
+        fix_id_pipelines.merge(wildcard_pipelines)
       end
     end
 
@@ -236,11 +263,16 @@ def fetch_config(pipeline_ids, client)
           raise ElasticsearchSource::RemoteConfigError, "Elasticsearch returned an unknown or malformed document structure"
         end
 
-        @response = format_response(response)
+        @pipelines = format_response(response)
+
+        log_wildcard_unsupported(pipeline_ids)
+        log_pipeline_not_found(pipeline_ids - @pipelines.keys)
+
+        @pipelines
       end
 
       def get_single_pipeline_setting(pipeline_id)
-        @response.fetch(pipeline_id, {}).fetch("_source", {})
+        @pipelines.fetch(pipeline_id, {}).fetch("_source", {})
       end
 
       private
@@ -251,6 +283,13 @@ def format_response(response)
         }.compact
         .reduce({}, :merge)
       end
+
+      def log_wildcard_unsupported(pipeline_ids)
+        has_wildcard = pipeline_ids.any? { |id| id.include?("*") }
+        if has_wildcard
+          logger.warn("wildcard '*' in xpack.management.pipeline.id is not supported in Elasticsearch version < 7.10")
+        end
+      end
     end
 
   end
diff --git a/x-pack/qa/integration/management/multiple_pipelines_spec.rb b/x-pack/qa/integration/management/multiple_pipelines_spec.rb
index 680f04976cf..51507223cb0 100644
--- a/x-pack/qa/integration/management/multiple_pipelines_spec.rb
+++ b/x-pack/qa/integration/management/multiple_pipelines_spec.rb
@@ -27,7 +27,7 @@
     @logstash_service = logstash("bin/logstash -w 1", {
       :settings => {
         "xpack.management.enabled" => true,
-        "xpack.management.pipeline.id" => @pipelines.keys,
+        "xpack.management.pipeline.id" => @pipelines.keys + ["*"],
         "xpack.management.logstash.poll_interval" => "1s",
         "xpack.management.elasticsearch.hosts" => ["http://localhost:9200"],
         "xpack.management.elasticsearch.username" => "elastic",
@@ -89,6 +89,19 @@
     end.to eq(4)
   end
 
+  it "add new pipelines" do
+    temporary_file = File.join(Stud::Temporary.directory, "wildcard_pipeline.log")
+    new_config = "input { generator { count => 10000 } tcp { port => 6008 }} output { file { path => '#{temporary_file}' } }"
+
+    expect(File.exist?(temporary_file)).to be_falsey
+    push_elasticsearch_config("wildcard_pipeline", new_config)
+    elasticsearch_client.indices.refresh
+
+    Stud.try(max_retry.times, [RSpec::Expectations::ExpectationNotMetError]) do
+      expect(File.exist?(temporary_file)).to be_truthy
+    end
+  end
+
   # Returns the number of hashes for the list of pipelines
   # Returns nil if the response is bad
   # This can happen if ES is not yet up or if the data is not yet in ES
diff --git a/x-pack/spec/config_management/bootstrap_check_spec.rb b/x-pack/spec/config_management/bootstrap_check_spec.rb
index fdfc345137a..ba0ea22cbbb 100644
--- a/x-pack/spec/config_management/bootstrap_check_spec.rb
+++ b/x-pack/spec/config_management/bootstrap_check_spec.rb
@@ -177,6 +177,40 @@
         end
       end
 
+      context "when defining invalid patterns" do
+        let(:pipeline_ids) { ["pipeline1", "pipeline2", "@o@"] }
+        let(:settings) do
+          apply_settings(
+            {
+              "xpack.management.enabled" => true,
+              "xpack.management.pipeline.id" => pipeline_ids
+            },
+            system_settings
+          )
+        end
+
+        it "raises a `LogStash::BootstrapCheckError` with the invalid patterns" do
+          expect { subject.check(settings) }.to raise_error LogStash::BootstrapCheckError, /@o@/
+        end
+      end
+
+      context "when defining wildcard patterns" do
+        let(:pipeline_ids) { ["pipeline1", "pipeline2", "*pipeline*"] }
+        let(:settings) do
+          apply_settings(
+              {
+                  "xpack.management.enabled" => true,
+                  "xpack.management.pipeline.id" => pipeline_ids
+              },
+              system_settings
+          )
+        end
+
+        it "does not raise a `LogStash::BootstrapCheckError` error" do
+          expect { subject.check(settings) }.to_not raise_error
+        end
+      end
+
       context "when defining duplicate ids" do
         let(:pipeline_ids) { ["pipeline1", "pipeline2", "pipeline1"] }
         let(:settings) do
diff --git a/x-pack/spec/config_management/elasticsearch_source_spec.rb b/x-pack/spec/config_management/elasticsearch_source_spec.rb
index 28d10593a33..87d8ad957b4 100644
--- a/x-pack/spec/config_management/elasticsearch_source_spec.rb
+++ b/x-pack/spec/config_management/elasticsearch_source_spec.rb
@@ -202,17 +202,58 @@
       let(:config) { "input { generator { count => 100 } tcp { port => 6005 } } output { }}" }
       let(:pipeline_id) { "super_generator" }
       let(:elasticsearch_response) { {"#{pipeline_id}"=> {"pipeline"=> "#{config}"}} }
+      let(:all_pipelines) { JSON.parse(::File.read(::File.join(::File.dirname(__FILE__), "fixtures", "pipelines.json"))) }
 
       it "#fetch_config" do
-        expect(mock_client).to receive(:get).with("#{described_class::SYSTEM_INDICES_API_PATH}/#{pipeline_id}").and_return(elasticsearch_response)
+        expect(mock_client).to receive(:get).with("#{described_class::SYSTEM_INDICES_API_PATH}/").and_return(elasticsearch_response.clone)
         expect(subject.fetch_config([pipeline_id], mock_client)).to eq(elasticsearch_response)
-        expect(subject.get_single_pipeline_setting(pipeline_id)).to eq({"pipeline"=> "#{config}"})
+        expect(subject.get_single_pipeline_setting(pipeline_id)).to eq({"pipeline"=>"#{config}"})
       end
 
       it "#fetch_config should raise error" do
-        expect(mock_client).to receive(:get).with("#{described_class::SYSTEM_INDICES_API_PATH}/apache,nginx").and_return(elasticsearch_8_err_response)
+        expect(mock_client).to receive(:get).with("#{described_class::SYSTEM_INDICES_API_PATH}/").and_return(elasticsearch_8_err_response.clone)
         expect{ subject.fetch_config(["apache", "nginx"], mock_client) }.to raise_error(LogStash::ConfigManagement::ElasticsearchSource::RemoteConfigError)
       end
+
+      describe "wildcard" do
+        it "should accept * " do
+          expect(mock_client).to receive(:get).with("#{described_class::SYSTEM_INDICES_API_PATH}/").and_return(all_pipelines.clone)
+          expect(subject).to receive(:logger).never
+          expect(subject.fetch_config(["*"], mock_client).keys.length).to eq(all_pipelines.keys.length)
+        end
+
+        it "should accept multiple * in one pattern " do
+          expect(mock_client).to receive(:get).with("#{described_class::SYSTEM_INDICES_API_PATH}/").and_return(all_pipelines.clone)
+          expect(subject).to receive(:logger).never
+          expect(subject.fetch_config(["host*_pipeline*"], mock_client).keys).to eq(["host1_pipeline1", "host1_pipeline2", "host2_pipeline1", "host2_pipeline2"])
+        end
+
+        it "should give unique pipeline with multiple wildcard patterns" do
+          expect(mock_client).to receive(:get).with("#{described_class::SYSTEM_INDICES_API_PATH}/").and_return(all_pipelines.clone)
+          expect(subject).to receive(:log_pipeline_not_found).with(["*pipeline*"]).exactly(1)
+          expect(subject.fetch_config(["host1_pipeline*", "host2_pipeline*","*pipeline*"], mock_client).keys).to eq(["host1_pipeline1", "host1_pipeline2", "host2_pipeline1", "host2_pipeline2"])
+        end
+
+        it "should accept a mix of wildcard and non wildcard pattern" do
+          expect(mock_client).to receive(:get).with("#{described_class::SYSTEM_INDICES_API_PATH}/").and_return(all_pipelines.clone)
+          expect(subject).to receive(:logger).never
+          expect(subject.fetch_config(["host1_pipeline*", "host2_pipeline*","super_generator"], mock_client).keys).to eq(["super_generator", "host1_pipeline1", "host1_pipeline2", "host2_pipeline1", "host2_pipeline2"])
+        end
+
+        it "should log unmatched pattern" do
+          pipeline_ids = ["very_awesome_pipeline", "*whatever*"]
+          expect(mock_client).to receive(:get).with("#{described_class::SYSTEM_INDICES_API_PATH}/").and_return(all_pipelines.clone)
+          expect(subject).to receive(:log_pipeline_not_found).with(pipeline_ids).exactly(1)
+          expect(subject.fetch_config(pipeline_ids, mock_client)).to eq({})
+        end
+
+        it "should log unmatched pattern and return matched pipeline" do
+          pipeline_ids = ["very_awesome_pipeline", "*whatever*"]
+          expect(mock_client).to receive(:get).with("#{described_class::SYSTEM_INDICES_API_PATH}/").and_return(all_pipelines.clone)
+          expect(subject).to receive(:log_pipeline_not_found).with(pipeline_ids).exactly(1)
+          expect(subject.fetch_config(pipeline_ids + [pipeline_id], mock_client)).to eq(elasticsearch_response)
+        end
+      end
     end
   end
 
@@ -252,6 +293,7 @@
 
       it "#fetch_config" do
         expect(mock_client).to receive(:post).with("#{described_class::PIPELINE_INDEX}/_mget", {}, "{\"docs\":[{\"_id\":\"#{pipeline_id}\"},{\"_id\":\"#{another_pipeline_id}\"}]}").and_return(elasticsearch_response)
+        expect(subject).to receive(:logger).never
         expect(subject.fetch_config([pipeline_id, another_pipeline_id], mock_client).size).to eq(2)
         expect(subject.get_single_pipeline_setting(pipeline_id)).to eq({"pipeline" => "#{config}"})
         expect(subject.get_single_pipeline_setting(another_pipeline_id)).to eq({"pipeline" => "#{another_config}"})
@@ -259,6 +301,7 @@
 
       it "#fetch_config should raise error" do
         expect(mock_client).to receive(:post).with("#{described_class::PIPELINE_INDEX}/_mget", {}, "{\"docs\":[{\"_id\":\"#{pipeline_id}\"},{\"_id\":\"#{another_pipeline_id}\"}]}").and_return(elasticsearch_7_9_err_response)
+        expect(subject).to receive(:logger).never
         expect{ subject.fetch_config([pipeline_id, another_pipeline_id], mock_client) }.to raise_error(LogStash::ConfigManagement::ElasticsearchSource::RemoteConfigError)
       end
 
@@ -267,12 +310,25 @@
         expect{ subject.fetch_config([pipeline_id, another_pipeline_id], mock_client) }.to raise_error(LogStash::ConfigManagement::ElasticsearchSource::RemoteConfigError)
       end
 
+      it "#fetch_config should log unmatched pipeline id" do
+        expect(mock_client).to receive(:post).with("#{described_class::PIPELINE_INDEX}/_mget", {}, "{\"docs\":[{\"_id\":\"#{pipeline_id}\"},{\"_id\":\"#{another_pipeline_id}\"},{\"_id\":\"*\"}]}").and_return(elasticsearch_response)
+        expect(subject).to receive(:log_pipeline_not_found).with(["*"]).exactly(1)
+        expect(subject.fetch_config([pipeline_id, another_pipeline_id, "*"], mock_client).size).to eq(2)
+        expect(subject.get_single_pipeline_setting(pipeline_id)).to eq({"pipeline" => "#{config}"})
+        expect(subject.get_single_pipeline_setting(another_pipeline_id)).to eq({"pipeline" => "#{another_config}"})
+      end
+
       it "#format_response should return pipelines" do
         result = subject.send(:format_response, elasticsearch_response)
         expect(result.size).to eq(2)
         expect(result.has_key?(pipeline_id)).to be_truthy
         expect(result.has_key?(another_pipeline_id)).to be_truthy
       end
+
+      it "should log wildcard warning" do
+        result = subject.send(:log_wildcard_unsupported, [pipeline_id, another_pipeline_id, "*"])
+        expect(result).not_to be_nil
+      end
     end
   end
 
diff --git a/x-pack/spec/config_management/fixtures/pipelines.json b/x-pack/spec/config_management/fixtures/pipelines.json
new file mode 100644
index 00000000000..8ed5fe8f6a0
--- /dev/null
+++ b/x-pack/spec/config_management/fixtures/pipelines.json
@@ -0,0 +1,60 @@
+{
+  "super_generator": {
+    "pipeline": "input { generator { count => 100 } tcp { port => 6005 } } output { }}"
+  },
+  "another_generator": {
+    "pipeline": "input { generator { count => 100 } tcp { port => 6006 } } output { file { path => '/var/folders/8g/415b_scd6ql1vxc2p128kfn40000gn/T/studtmp-b0f4079ab3df20f7eb2389958a6568b6614b38fffec1fcbf1627c4e5edc3/another_generator_new' }}",
+    "username": "log.stash",
+    "pipeline_metadata": {
+      "version": "1"
+    },
+    "pipeline_settings": {
+      "pipeline.batch.delay": "50"
+    },
+    "last_modified": "2020-10-06T23:11:38Z"
+  },
+  "host1_pipeline1": {
+    "pipeline": "input { generator { count => 10000000 }} filter { sleep { time => \"1\" every => 10}} output { file { path  => '/var/folders/8g/415b_scd6ql1vxc2p128kfn40000gn/T/studtmp-b0f4079ab3df20f7eb2389958a6568b6614b38fffec1fcbf1627c4e5edc3/host1_generator1'}}",
+    "username": "log.stash",
+    "pipeline_metadata": {
+      "version": "3"
+    },
+    "pipeline_settings": {
+      "pipeline.batch.delay": "50"
+    },
+    "last_modified": "2020-10-01T15:42:30.229Z"
+  },
+  "host1_pipeline2": {
+    "pipeline": "input { generator { count => 10000000 }} filter { sleep { time => \"1\" every => 10}} output { file { path  => '/var/folders/8g/415b_scd6ql1vxc2p128kfn40000gn/T/studtmp-b0f4079ab3df20f7eb2389958a6568b6614b38fffec1fcbf1627c4e5edc3/host1_generator2'}}",
+    "username": "log.stash",
+    "pipeline_metadata": {
+      "version": "3"
+    },
+    "pipeline_settings": {
+      "pipeline.batch.delay": "50"
+    },
+    "last_modified": "2020-10-01T15:42:30.229Z"
+  },
+  "host2_pipeline1": {
+    "pipeline": "input { generator { count => 10000000 }} filter { sleep { time => \"1\" every => 10}} output { file { path  => '/var/folders/8g/415b_scd6ql1vxc2p128kfn40000gn/T/studtmp-b0f4079ab3df20f7eb2389958a6568b6614b38fffec1fcbf1627c4e5edc3/host2_generator1'}}",
+    "username": "log.stash",
+    "pipeline_metadata": {
+      "version": "3"
+    },
+    "pipeline_settings": {
+      "pipeline.batch.delay": "50"
+    },
+    "last_modified": "2020-10-01T15:42:30.229Z"
+  },
+  "host2_pipeline2": {
+    "pipeline": "input { generator { count => 10000000 }} filter { sleep { time => \"1\" every => 10}} output { file { path  => '/var/folders/8g/415b_scd6ql1vxc2p128kfn40000gn/T/studtmp-b0f4079ab3df20f7eb2389958a6568b6614b38fffec1fcbf1627c4e5edc3/host2_generator2'}}",
+    "username": "log.stash",
+    "pipeline_metadata": {
+      "version": "3"
+    },
+    "pipeline_settings": {
+      "pipeline.batch.delay": "50"
+    },
+    "last_modified": "2020-10-01T15:42:30.229Z"
+  }
+}
\ No newline at end of file
