diff --git a/apache.yml b/apache.yml
new file mode 100644
index 00000000000..4e0123f4801
--- /dev/null
+++ b/apache.yml
@@ -0,0 +1,36 @@
+---
+graph:
+  log-reader:
+    component: input-stdin
+    to: [main-queue]
+  main-queue:
+    component: queue-synchronous
+    to: [apache-grok]
+  apache-grok:
+    component: filter-grok
+    options:
+      match:
+        message: '%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}'
+    to: [apache-geoip]
+  apache-geoip:
+    component: filter-geoip
+    options:
+      source: clientip
+      target: geoip
+    to: [apache-ua]
+  apache-ua:
+    component: filter-useragent
+    options:
+      source: agent
+      target: useragent
+    to: [apache-date]
+  apache-date:
+    component: filter-date
+    options:
+      match: [ "timestamp", "dd/MMM/YYYY:HH:mm:ss Z" ]
+      locale: en
+    to: [main-out]
+  main-out:
+    component: output-stdout
+    options:
+      codec: dots
\ No newline at end of file
diff --git a/conditional-apache.cfg b/conditional-apache.cfg
new file mode 100644
index 00000000000..0869895ebd5
--- /dev/null
+++ b/conditional-apache.cfg
@@ -0,0 +1,39 @@
+input {
+  stdin {}
+}
+
+filter {
+  grok {
+    match => {
+      "message" => '%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}'
+    }
+  }
+
+  if ['response'] and ['response'] > 399 {
+    mutate {
+      add_tag => weird_error
+    }
+  }
+
+  geoip {
+    source => clientip
+    target => geoip
+  }
+
+  useragent {
+    source => agent
+    target => useragent
+  }
+
+  date {
+    match => [ "timestamp", "dd/MMM/YYYY:HH:mm:ss Z" ]
+    locale => en
+  }
+}
+
+output {
+  file {
+    path => "/tmp/ls-out.json"
+    codec => json_lines
+  }
+}
diff --git a/conditional-apache.yml b/conditional-apache.yml
new file mode 100644
index 00000000000..893d0045845
--- /dev/null
+++ b/conditional-apache.yml
@@ -0,0 +1,48 @@
+---
+graph:
+  log-reader:
+    component: input-stdin
+    to: [main-queue]
+  main-queue:
+    component: queue-synchronous
+    to: [apache-grok]
+  apache-grok:
+    component: filter-grok
+    options:
+      match:
+        message: '%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}'
+    to: [code-splitter]
+  code-splitter:
+    component: predicate-ifelse-ruby
+    to:
+      - ["value = event['[response]'] && value ? value > 399 : false", [error-tagger]]
+      - ["__ELSE__", [apache-geoip]]
+  error-tagger:
+    component: filter-mutate
+    options:
+      add_tag:
+        fb: weird_error
+    to: [apache-geoip]
+  apache-geoip:
+    component: filter-geoip
+    options:
+      source: geoip
+      target: geoip
+    to: [apache-ua]
+  apache-ua:
+    component: filter-useragent
+    options:
+      source: agent
+      target: useragent
+    to: [apache-date]
+  apache-date:
+    component: filter-date
+    options:
+      match: [ "timestamp", "dd/MMM/YYYY:HH:mm:ss Z" ]
+      locale: en
+    to: [main-out]
+  main-out:
+    component: output-file
+    options:
+      path: /tmp/graph-out.json
+      codec: json_lines
\ No newline at end of file
diff --git a/conditional-graph-pipeline.yml b/conditional-graph-pipeline.yml
new file mode 100644
index 00000000000..18499e20c84
--- /dev/null
+++ b/conditional-graph-pipeline.yml
@@ -0,0 +1,44 @@
+---
+graph:
+  log-reader:
+    component: input-generator
+    options: {"message": "Hello 123", "count": 50}
+    to: [main-queue]
+  main-queue:
+    component: queue-synchronous
+    to: [hello-grok]
+  hello-grok:
+    component: filter-grok
+    options:
+      match:
+        message: "%{WORD:greeting} %{NUMBER:greetno}"
+    to: [fizzbuzz]
+  fizzbuzz:
+    component: predicate-ifelse-ruby
+    to:
+      - ["event['[sequence]'] % 15 == 0", [fizzbuzzer]]
+      - ["event['[sequence]'] % 5 == 0", [fizzer]]
+      - ["event['[sequence]'] % 3 == 0", [buzzer]]
+      - ["__ELSE__", [main-out]]
+  fizzbuzzer:
+    component: filter-mutate
+    options:
+      add_field:
+        fb: fizzbuzz
+    to: [main-out]
+  fizzer:
+    component: filter-mutate
+    options:
+      add_field:
+        fb: fizz
+    to: [main-out]
+  buzzer:
+    component: filter-mutate
+    options:
+      add_field:
+        fb: buzz
+    to: [main-out]
+  main-out:
+    component: output-stdout
+    options:
+      codec: json_lines
\ No newline at end of file
diff --git a/demo.conf b/demo.conf
new file mode 100644
index 00000000000..e845d8f1af3
--- /dev/null
+++ b/demo.conf
@@ -0,0 +1,26 @@
+input {
+  generator {
+    count => 500000
+    message => "stdin=yes blah=baz timestamp=2016-02-02"
+  }
+}
+
+filter {
+kv {}
+date {
+  match => ["timestamp", "ISO8601"]
+}
+mutate {
+  add_field => { mutated => "true" }
+}
+}
+
+output {
+  file {
+    path => "/tmp/fileout"
+  }
+
+  stdout {
+    codec => json_lines
+  }
+}
\ No newline at end of file
diff --git a/logstash-core-event-java/build.gradle b/logstash-core-event-java/build.gradle
index b2a4a55ec43..5f991221602 100644
--- a/logstash-core-event-java/build.gradle
+++ b/logstash-core-event-java/build.gradle
@@ -9,6 +9,11 @@ buildscript {
     }
 }
 
+plugins {
+    id 'java' // or 'groovy' Must be explicitly applied
+    id 'com.github.johnrengelman.shadow' version '1.2.3'
+}
+
 repositories {
     mavenLocal()
     mavenCentral()
@@ -27,7 +32,7 @@ apply plugin: 'idea'
 
 group = 'org.logstash'
 
-project.sourceCompatibility = 1.7
+project.sourceCompatibility = 1.8
 
 task sourcesJar(type: Jar, dependsOn: classes) {
     from sourceSets.main.allSource
@@ -94,11 +99,20 @@ idea {
 dependencies {
     compile 'com.fasterxml.jackson.core:jackson-core:2.7.1'
     compile 'com.fasterxml.jackson.core:jackson-databind:2.7.1-1'
+    compile 'com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.7.1'
+    compile 'commons-io:commons-io:2.4'
     provided 'org.jruby:jruby-core:1.7.22'
     testCompile 'junit:junit:4.12'
     testCompile 'net.javacrumbs.json-unit:json-unit:1.9.0'
 }
 
+shadowJar {
+    dependencies {
+        exclude(dependency('org.jruby:jruby-core:1.7.22'))
+    }
+}
+
+
 // See http://www.gradle.org/docs/current/userguide/gradle_wrapper.html
 task wrapper(type: Wrapper) {
     description = 'Install Gradle wrapper'
diff --git a/logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties b/logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties
index 25611753f15..e8a46a32d1b 100644
--- a/logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties
+++ b/logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties
@@ -1,6 +1,6 @@
-#Fri Jan 22 14:29:02 EST 2016
+#Sat Feb 20 21:23:41 PST 2016
 distributionBase=GRADLE_USER_HOME
 distributionPath=wrapper/dists
 zipStoreBase=GRADLE_USER_HOME
 zipStorePath=wrapper/dists
-distributionUrl=https\://services.gradle.org/distributions/gradle-2.8-bin.zip
+distributionUrl=https\://services.gradle.org/distributions/gradle-2.8-all.zip
diff --git a/logstash-core-event-java/lib/logstash-core-event-java.rb b/logstash-core-event-java/lib/logstash-core-event-java.rb
index 29b487aa192..2a5736c567e 100644
--- a/logstash-core-event-java/lib/logstash-core-event-java.rb
+++ b/logstash-core-event-java/lib/logstash-core-event-java.rb
@@ -1 +1,3 @@
-require "logstash-core-event-java/logstash-core-event-java"
\ No newline at end of file
+require "logstash-core-event-java/logstash-core-event-java"
+
+java_import com.logstash.pipeline.Constants
\ No newline at end of file
diff --git a/logstash-core-event-java/lib/logstash-core-event-java_jars.rb b/logstash-core-event-java/lib/logstash-core-event-java_jars.rb
index 143d7a3e068..42fd975a90e 100644
--- a/logstash-core-event-java/lib/logstash-core-event-java_jars.rb
+++ b/logstash-core-event-java/lib/logstash-core-event-java_jars.rb
@@ -2,5 +2,7 @@
 require 'jar_dependencies'
 
 require_jar( 'com.fasterxml.jackson.core', 'jackson-core', '2.7.1' )
+require_jar( 'org.yaml', 'snakeyaml', '1.15' )
 require_jar( 'com.fasterxml.jackson.core', 'jackson-annotations', '2.7.0' )
+require_jar( 'com.fasterxml.jackson.dataformat', 'jackson-dataformat-yaml', '2.7.1' )
 require_jar( 'com.fasterxml.jackson.core', 'jackson-databind', '2.7.1-1' )
diff --git a/logstash-core-event-java/lib/logstash/event.rb b/logstash-core-event-java/lib/logstash/event.rb
index 8f6a1908901..d33b103575e 100644
--- a/logstash-core-event-java/lib/logstash/event.rb
+++ b/logstash-core-event-java/lib/logstash/event.rb
@@ -5,22 +5,12 @@
 require "logstash/string_interpolation"
 require "cabin"
 
-# transcient pipeline events for normal in-flow signaling as opposed to
-# flow altering exceptions. for now having base classes is adequate and
-# in the future it might be necessary to refactor using like a BaseEvent
-# class to have a common interface for all pileline events to support
-# eventual queueing persistence for example, TBD.
-class LogStash::ShutdownEvent; end
-class LogStash::FlushEvent; end
-
-module LogStash
-  FLUSH = LogStash::FlushEvent.new
-
-  # LogStash::SHUTDOWN is used by plugins
-  SHUTDOWN = LogStash::ShutdownEvent.new
-end
-
 # for backward compatibility, require "logstash/event" is used a lots of places so let's bootstrap the
 # Java code loading from here.
 # TODO: (colin) I think we should mass replace require "logstash/event" with require "logstash-core-event"
-require "logstash-core-event"
\ No newline at end of file
+require "logstash-core-event"
+
+module LogStash
+  SHUTDOWN = com.logstash.pipeline.Constants.shutdownEvent;
+  FLUSH = com.logstash.pipeline.Constants.flushEvent;
+end
\ No newline at end of file
diff --git a/logstash-core-event-java/logstash-core-event-java.gemspec b/logstash-core-event-java/logstash-core-event-java.gemspec
index 10beff6d4bb..9eb0a95fe64 100644
--- a/logstash-core-event-java/logstash-core-event-java.gemspec
+++ b/logstash-core-event-java/logstash-core-event-java.gemspec
@@ -28,4 +28,5 @@ Gem::Specification.new do |gem|
 
   gem.requirements << "jar com.fasterxml.jackson.core:jackson-core, 2.7.1"
   gem.requirements << "jar com.fasterxml.jackson.core:jackson-databind, 2.7.1-1"
-end
+  gem.requirements << 'jar com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.7.1'
+end
\ No newline at end of file
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Event.java b/logstash-core-event-java/src/main/java/com/logstash/Event.java
index bf62eb4ea3b..935a979575e 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/Event.java
+++ b/logstash-core-event-java/src/main/java/com/logstash/Event.java
@@ -33,6 +33,7 @@ public class Event implements Cloneable, Serializable {
     // logger is static since once set there is no point in changing it at runtime
     // for other reasons than in tests/specs.
     private transient static Logger logger = DEFAULT_LOGGER;
+    private int batchSequence;
 
     public Event()
     {
@@ -293,4 +294,12 @@ public void tag(String tag) {
     public static void setLogger(Logger logger) {
         Event.logger = logger;
     }
+
+    public void setBatchSequence(int batchSequence) {
+        this.batchSequence = batchSequence;
+    }
+
+    public int getBatchSequence() {
+        return batchSequence;
+    }
 }
diff --git a/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyEventExtLibrary.java b/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyEventExtLibrary.java
index 1cb630d5a75..dfc04c833f4 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyEventExtLibrary.java
+++ b/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyEventExtLibrary.java
@@ -20,6 +20,7 @@
 import org.jruby.anno.JRubyClass;
 import org.jruby.anno.JRubyMethod;
 import org.jruby.exceptions.RaiseException;
+import org.jruby.java.proxies.ConcreteJavaProxy;
 import org.jruby.javasupport.JavaUtil;
 import org.jruby.runtime.Arity;
 import org.jruby.runtime.ObjectAllocator;
@@ -113,8 +114,8 @@ public Event getEvent() {
             return event;
         }
 
-        public void setEvent(Event event) {
-            this.event = event;
+        public void setEvent(IRubyObject event) {
+            this.event = (Event) event;
         }
 
         // def initialize(data = {})
@@ -276,6 +277,13 @@ public IRubyObject ruby_to_json(ThreadContext context, IRubyObject[] args)
             }
         }
 
+        @JRubyMethod(name = "from_java", required = 1, meta = true)
+        public static IRubyObject ruby_from_java(ThreadContext context, IRubyObject recv, IRubyObject event) {
+            ConcreteJavaProxy eventProxy = (ConcreteJavaProxy) event;
+            Event converted = (Event) eventProxy.toJava(Event.class);
+            return RubyEvent.newRubyEvent(context.runtime, converted);
+        }
+
         // @param value [String] the json string. A json object/map will convert to an array containing a single Event.
         // and a json array will convert each element into individual Event
         // @return Array<Event> array of events
diff --git a/logstash-core-event-java/src/main/java/com/logstash/pipeline/Batch.java b/logstash-core-event-java/src/main/java/com/logstash/pipeline/Batch.java
new file mode 100644
index 00000000000..1a9149a606a
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/pipeline/Batch.java
@@ -0,0 +1,33 @@
+package com.logstash.pipeline;
+
+import com.logstash.Event;
+
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * Created by andrewvc on 2/20/16.
+ */
+public class Batch {
+    private final boolean flush;
+    private final boolean shutdown;
+    private final List<Event> events;
+
+    Batch(List events, boolean flush, boolean shutdown) {
+        this.flush = flush;
+        this.shutdown = shutdown;
+        this.events = events;
+    }
+
+    public List<Event> getEvents() {
+        return events;
+    }
+
+    public boolean isFlush() {
+        return flush;
+    }
+
+    public boolean isShutdown() {
+        return shutdown;
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/pipeline/BooleanEventsResult.java b/logstash-core-event-java/src/main/java/com/logstash/pipeline/BooleanEventsResult.java
new file mode 100644
index 00000000000..5f5145cb35a
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/pipeline/BooleanEventsResult.java
@@ -0,0 +1,32 @@
+package com.logstash.pipeline;
+import com.logstash.Event;
+import java.util.LinkedList;
+import java.util.List;
+
+/**
+ * Created by andrewvc on 2/26/16.
+ */
+public class BooleanEventsResult {
+    private final List<Event> trueEvents;
+    private final List<Event> falseEvents;
+
+    public BooleanEventsResult() {
+        this.trueEvents = new LinkedList<>();
+        this.falseEvents = new LinkedList<>();
+    }
+
+    public BooleanEventsResult(List<Event> trueEvents, List<Event> falseEvents) {
+        this.trueEvents = trueEvents;
+        this.falseEvents = falseEvents;
+    }
+
+    public List<Event> getTrueEvents() {
+        return trueEvents;
+    }
+
+    public List<Event> getFalseEvents() {
+        return falseEvents;
+    }
+
+
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/pipeline/Component.java b/logstash-core-event-java/src/main/java/com/logstash/pipeline/Component.java
new file mode 100644
index 00000000000..8457caef0ba
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/pipeline/Component.java
@@ -0,0 +1,43 @@
+package com.logstash.pipeline;
+
+/**
+ * Created by andrewvc on 2/22/16.
+ */
+public class Component {
+    public enum Type { INPUT, QUEUE, FILTER, OUTPUT, PREDICATE }
+
+    private final Type type;
+    private final String id;
+    private final String componentName;
+    private final String optionsStr;
+
+    public Component(String id, String componentName, String optionsStr) {
+        this.id = id;
+        this.componentName = componentName;
+        this.type = extractTypeFromComponentName(componentName);
+        this.optionsStr = optionsStr;
+    }
+
+    private Type extractTypeFromComponentName(String componentName) {
+        String[] componentParts = componentName.split("-", 2);
+        return Type.valueOf(componentParts[0].toUpperCase());
+    }
+
+    public Type getType() {
+        return type;
+    }
+
+    public String getId() {
+        return id;
+    }
+
+    public String getComponentName() {
+        return componentName;
+    }
+
+    public String getOptionsStr() { return optionsStr; }
+
+    public String toString() {
+        return this.getId();
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/pipeline/ComponentProcessor.java b/logstash-core-event-java/src/main/java/com/logstash/pipeline/ComponentProcessor.java
new file mode 100644
index 00000000000..44c6f3c0a5a
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/pipeline/ComponentProcessor.java
@@ -0,0 +1,16 @@
+package com.logstash.pipeline;
+
+import com.logstash.Event;
+import com.logstash.pipeline.graph.Condition;
+
+import java.util.*;
+
+/**
+ * Created by andrewvc on 2/22/16.
+ */
+public interface ComponentProcessor {
+    List<Event> process(Component component, List<Event> events);
+    BooleanEventsResult processCondition(Condition condition, List<Event> events);
+    void flush(Component c, boolean shutdown);
+    void setup(Component component);
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/pipeline/Constants.java b/logstash-core-event-java/src/main/java/com/logstash/pipeline/Constants.java
new file mode 100644
index 00000000000..7cdbefef3c6
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/pipeline/Constants.java
@@ -0,0 +1,11 @@
+package com.logstash.pipeline;
+
+import com.logstash.Event;
+
+/**
+ * Created by andrewvc on 2/20/16.
+ */
+public class Constants {
+    public final static Event shutdownEvent = new Event();
+    public final static Event flushEvent = new Event();
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/pipeline/PipelineGraph.java b/logstash-core-event-java/src/main/java/com/logstash/pipeline/PipelineGraph.java
new file mode 100644
index 00000000000..59c1e3c624a
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/pipeline/PipelineGraph.java
@@ -0,0 +1,167 @@
+package com.logstash.pipeline;
+
+
+import com.logstash.Event;
+import com.logstash.pipeline.graph.Condition;
+import com.logstash.pipeline.graph.Edge;
+import com.logstash.pipeline.graph.Vertex;
+
+import java.util.*;
+import java.util.stream.Collectors;
+import java.util.stream.Stream;
+
+/**
+ * Created by andrewvc on 2/20/16.
+ */
+public class PipelineGraph {
+    private final Set<Vertex> sources;
+    private final List<Vertex> sortedVertices;
+    private final List<Vertex> postQueueVertices;
+    private Set<Edge> edges;
+    private final Map<String, Vertex> vertices;
+    private final ComponentProcessor componentProcessor;
+
+    public static class InvalidGraphError extends Throwable {
+        InvalidGraphError(String message) {
+            super(message);
+        }
+    }
+
+    public PipelineGraph(Map<String, Vertex> vertices, ComponentProcessor componentProcessor) throws InvalidGraphError {
+        this.vertices = vertices;
+        this.componentProcessor = componentProcessor;
+        this.edges = getVertices().stream().flatMap(v -> v.getOutEdges().stream()).collect(Collectors.toSet());
+        // Precalculate source elements (those with no incoming edges)
+        this.sources = getVertices().stream().filter(Vertex::isSource).collect(Collectors.toSet());
+
+        // Setup each vertex with the component processor
+        Arrays.stream(this.getComponents()).forEach(componentProcessor::setup);
+
+        this.sortedVertices = topologicalSort();
+        int queueIndex = this.sortedVertices.indexOf(this.queueVertex());
+        // Workers only process stuff after the queue, so this list becomes valuable
+        this.postQueueVertices = this.sortedVertices.subList(queueIndex+1,this.sortedVertices.size());
+    }
+
+    // Uses Kahn's algorithm to do a topological sort and detect cycles
+    public List<Vertex> topologicalSort() throws InvalidGraphError {
+        List<Vertex> sorted = new ArrayList<>(this.vertices.size());
+
+        Deque<Vertex> pending = new LinkedList<>();
+        pending.addAll(sources);
+
+        Set<Edge> traversedEdges = new HashSet<>();
+
+        while (!pending.isEmpty()) {
+            Vertex currentVertex = pending.removeFirst();
+            sorted.add(currentVertex);
+            currentVertex.getOutEdges().forEach(edge -> {
+                traversedEdges.add(edge);
+                Vertex toVertex = edge.getTo();
+                if (toVertex.getInEdges().stream().allMatch(traversedEdges::contains)) {
+                    pending.add(toVertex);
+                }
+            });
+        }
+
+        // Check for cycles
+        if (this.edges.stream().noneMatch(traversedEdges::contains)) {
+            throw new InvalidGraphError("Graph has cycles, is not a DAG!");
+        }
+
+        return sorted;
+    }
+
+    public void processWorker(Batch batch) {
+        Map<Edge, List<Event>> edgePayloads = new HashMap<>(edges.size());
+
+        Set<Vertex> workerRootVertices = this.queueVertex().getOutVertices().collect(Collectors.toSet());
+
+        for (Vertex vertex : postQueueVertices) {
+            // Root elements get the input batch directly
+            // We should probably consider cloning these at some point
+            // because if we truly have multiple roots that would be problematic
+            List<Event> incoming;
+            if (workerRootVertices.contains(vertex)) {
+                incoming = batch.getEvents();
+            } else {
+                incoming = vertex.getInEdges().stream().
+                        flatMap(e -> edgePayloads.get(e).stream()).
+                        collect(Collectors.toList());
+            }
+
+            // Sort incoming events if we're sending to an output, this gives us strict ordering
+            // if a single worker is used and there is only one path through the pipeline
+            if (vertex.getComponent().getType() == Component.Type.OUTPUT) {
+                incoming.sort((e1, e2) -> {
+                    // Nulls go last!
+                    if (e1 == null && e2 == null) return 0;
+                    else if (e1 == null) return 1;
+                    else if (e2 == null) return -1;
+                    else return Integer.compare(e1.getBatchSequence(), e2.getBatchSequence());
+                });
+            }
+
+            edgePayloads.putAll(processVertex(vertex, incoming));
+        }
+    }
+
+    public LinkedHashMap<Edge, List<Event>> processVertex(Vertex v, List<Event> inEvents) {
+        Component component = v.getComponent();
+
+        LinkedHashMap<Edge, List<Event>> vMapping = new LinkedHashMap<>();
+        if (component.getType() == Component.Type.PREDICATE) {
+            // Edges are in order of branching
+            List<Event> remainingEvents = inEvents;
+            for (Edge edge : v.getOutEdges()) {
+                Condition condition = edge.getCondition();
+
+                BooleanEventsResult results;
+                if (condition != Condition.elseCondition) {
+                    results = componentProcessor.processCondition(condition, remainingEvents);
+                } else {
+                    results = new BooleanEventsResult(remainingEvents, new LinkedList<>());
+                }
+                remainingEvents = results.getFalseEvents();
+
+                vMapping.put(edge, results.getTrueEvents());
+            }
+        } else {
+            List<Event> outEvents = componentProcessor.process(component, inEvents);
+            v.getOutEdges().forEach(outE -> vMapping.put(outE, outEvents));
+        }
+
+        return vMapping;
+    }
+
+    // Vertices that occur after the queue
+    // This is a bit hacky and only supports one queue at the moment
+    // for our current pipeline
+    public Stream<Vertex> workerVertices() {
+        return queueVertex().getOutVertices();
+    }
+
+    public Vertex queueVertex() {
+        return this.vertices.get("main-queue");
+    }
+
+    public Component[] getComponents() {
+        return this.vertices.values().stream().map(Vertex::getComponent).toArray(Component[]::new);
+    }
+
+    public Stream<Component> componentStream() {
+        return this.vertices.values().stream().map(Vertex::getComponent);
+    }
+
+    public Map<String, Vertex> getVertexMapping() {
+        return this.vertices;
+    }
+
+    public Collection<Vertex> getVertices() {
+        return this.vertices.values();
+    }
+
+    public void flush(boolean shutdown) {
+        this.componentStream().forEach(c -> componentProcessor.flush(c, shutdown));
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/pipeline/Worker.java b/logstash-core-event-java/src/main/java/com/logstash/pipeline/Worker.java
new file mode 100644
index 00000000000..83c5a515082
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/pipeline/Worker.java
@@ -0,0 +1,116 @@
+package com.logstash.pipeline;
+
+import com.logstash.Event;
+import com.logstash.ext.JrubyEventExtLibrary;
+
+import java.io.InputStream;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.CopyOnWriteArraySet;
+import java.util.concurrent.SynchronousQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.function.IntConsumer;
+import java.util.stream.IntStream;
+
+/**
+ * Created by andrewvc on 2/20/16.
+ */
+public class Worker implements Runnable {
+    private final int batchDelay;
+    private final int batchSize;
+    private final SynchronousQueue<Event> queue;
+    private final PipelineGraph graph;
+    private final String name;
+    private volatile Thread thread;
+
+    public static List<Worker> startWorkers(int count, PipelineGraph graph, SynchronousQueue<Event> queue, int batchSize, int batchDelayMs) {
+        List<Worker> workers = new ArrayList(count);
+        for (int i=0; i < count; i++ ) {
+            String name = String.format("[main]>worker%d", i);
+            Worker worker = new Worker(name, graph, queue, batchSize, batchDelayMs);
+            worker.start();
+            workers.add(worker);
+        }
+        return workers;
+    }
+
+
+    Worker(String name, PipelineGraph graph, SynchronousQueue<Event> queue, int batchSize, int batchDelayMs) {
+        this.name = name;
+        this.graph = graph;
+        this.queue = queue;
+        this.batchSize = batchSize;
+        this.batchDelay = batchDelayMs;
+    }
+
+    public Thread start() {
+        Thread thread = new Thread(this);
+        thread.setName(this.name);
+        this.thread = thread;
+        thread.start();
+        return thread;
+    }
+
+    @Override
+    public void run() {
+        boolean isShutdown = false;
+        while (!isShutdown) {
+            Batch batch = takeBatch();
+            batch = processBatch(batch);
+            if (batch.isFlush()) graph.flush(batch.isShutdown());
+            isShutdown = batch.isShutdown(); // Stop the loop
+        }
+    }
+
+    public void join() throws InterruptedException {
+        this.thread.join();
+    }
+
+    public Batch takeBatch() {
+        boolean flush = false;
+        boolean shutdown = false;
+        int batchSequence = 0;
+
+        final List<Event> events = new ArrayList<>(batchSize);
+
+        for (int i = 0; i < batchSize; i++) {
+            Event event;
+
+            try {
+                if (i == 0) {
+                    event = queue.take();
+                } else {
+                    event = queue.poll(batchDelay, TimeUnit.MILLISECONDS);
+                }
+            } catch (InterruptedException e) {
+                break;
+            }
+
+            if (event == Constants.flushEvent) {
+                flush = true;
+                break; // Don't delay when its time to flush!
+            } else if (event == Constants.shutdownEvent) {
+                shutdown = true;
+                break; // Terminate the batch early. Required because only num_workers shutdown events are injected!
+            } else if (event != null) {
+                batchSequence++;
+                event.setBatchSequence(batchSequence);
+                events.add(event);
+            }
+        }
+
+        return new Batch(events, flush, shutdown);
+    }
+
+    public Batch processBatch(Batch batch) {
+        graph.processWorker(batch);
+        return batch;
+    }
+
+    public Thread getThread() {
+        return thread;
+    }
+
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/pipeline/graph/Condition.java b/logstash-core-event-java/src/main/java/com/logstash/pipeline/graph/Condition.java
new file mode 100644
index 00000000000..32bb80e531c
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/pipeline/graph/Condition.java
@@ -0,0 +1,40 @@
+package com.logstash.pipeline.graph;
+
+import org.jruby.runtime.builtin.IRubyObject;
+
+/**
+ * Created by andrewvc on 2/24/16.
+ */
+public class Condition {
+    public static final Condition elseCondition = new Condition("__ELSE__");
+
+    final String source;
+
+    final static Condition fromSource(String source) {
+        if (source.equals(elseCondition.source)) {
+            return elseCondition;
+        } else {
+            return new Condition(source);
+        }
+    }
+
+    public Condition(String source) {
+        this.source = source;
+    }
+
+    public String getSource() {
+        return source;
+    }
+
+    public boolean equals(Condition other) {
+        if (this.source == null) {
+            if (other.source == null) {
+                return true;
+            } else {
+                return false;
+            }
+        } else {
+            return other.source.equals(this.source);
+        }
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/pipeline/graph/ConfigFile.java b/logstash-core-event-java/src/main/java/com/logstash/pipeline/graph/ConfigFile.java
new file mode 100644
index 00000000000..80e6ffe35ea
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/pipeline/graph/ConfigFile.java
@@ -0,0 +1,175 @@
+package com.logstash.pipeline.graph;
+
+import com.fasterxml.jackson.databind.JsonNode;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.databind.node.JsonNodeType;
+import com.fasterxml.jackson.dataformat.yaml.YAMLFactory;
+import com.logstash.pipeline.Component;
+import com.logstash.pipeline.ComponentProcessor;
+import com.logstash.pipeline.PipelineGraph;
+
+import java.io.IOException;
+import java.util.*;
+
+/**
+ * Created by andrewvc on 2/20/16.
+ */
+public class ConfigFile {
+    private final JsonNode graphElement;
+    private final String source;
+    private final JsonNode tree;
+    private final Map<String, Vertex> vertices = new HashMap<>();
+
+    private final PipelineGraph pipelineGraph;
+
+    public static class InvalidGraphConfigFile extends Throwable {
+        InvalidGraphConfigFile(String message) {
+            super(message);
+        }
+    }
+
+    public static ConfigFile fromString(String source, ComponentProcessor componentProcessor) throws IOException, InvalidGraphConfigFile {
+        ObjectMapper mapper = new ObjectMapper(new YAMLFactory());
+        JsonNode tree = mapper.readTree(source);
+        return new ConfigFile(source, tree, componentProcessor);
+    }
+
+    public ConfigFile(String source, JsonNode tree, ComponentProcessor componentProcessor) throws InvalidGraphConfigFile {
+        this.source = source;
+        this.tree = tree;
+
+        this.graphElement = tree.get("graph");
+        buildVertices();
+        connectVertices();
+        try {
+            this.pipelineGraph = new PipelineGraph(vertices, componentProcessor);
+        } catch (PipelineGraph.InvalidGraphError invalidGraphError) {
+            throw new InvalidGraphConfigFile("Cycle detected in graph!");
+        }
+    }
+
+    public PipelineGraph getPipelineGraph() {
+        return pipelineGraph;
+    }
+
+    public String source() {
+        return source;
+    }
+
+    public void buildVertices() throws InvalidGraphConfigFile {
+        if (graphElement == null) {
+            throw new InvalidGraphConfigFile("Missing vertices element in config: " + source);
+        }
+
+        // Use a for loop here since it's a little tricky with lambdas + exceptions
+        for (Iterator<Map.Entry<String, JsonNode>> geFields = graphElement.fields(); geFields.hasNext();) {
+            Map.Entry<String, JsonNode> e = geFields.next();
+
+            JsonNode propsNode = e.getValue();
+            String id = e.getKey();
+
+            JsonNode componentNameNode = propsNode.get("component");
+            if (componentNameNode == null) {
+                throw new InvalidGraphConfigFile("Missing component declaration for: " + id);
+            }
+            String componentNameText = componentNameNode.asText();
+
+            JsonNode optionsNode = propsNode.get("options");
+
+            String optionsStr;
+            if (optionsNode == null) {
+                optionsStr = null;
+            } else {
+                optionsStr = optionsNode.toString();
+            }
+
+            Component component = new Component(id, componentNameText, optionsStr);
+            vertices.put(id, new Vertex(id, component));
+        }
+    }
+
+    private void connectVertices() throws InvalidGraphConfigFile {
+        Iterator<Map.Entry<String, JsonNode>> geFields = graphElement.fields();
+        while(geFields.hasNext()) {
+            Map.Entry<String, JsonNode> field = geFields.next();
+            String name = field.getKey();
+
+            JsonNode propsNode = field.getValue();
+            JsonNode toNode = propsNode.get("to");
+
+            // blank to nodes are fine (that's a terminal node)
+            if (toNode == null) {
+                continue;
+            } else {
+                // non-array non-null nodes are a problem!
+                checkNodeType(toNode, JsonNodeType.ARRAY, "The 'to' field must be a list of vertex names if specified!");
+            }
+
+            Vertex v = vertices.get(name);
+            if (v == null) throw new IllegalArgumentException("Could not connect unknown vertex: " + name);
+
+            Iterator<JsonNode> toNodeElements = toNode.elements();
+            while (toNodeElements.hasNext()) {
+                JsonNode toElem = toNodeElements.next();
+                if (v.getComponent().getType() == Component.Type.PREDICATE) {
+                    createPredicateToEdges(v, toElem);
+                } else if (toElem.isTextual()) {
+                    createStandardEdge(v, toElem);
+                } else {
+                    throw new IllegalArgumentException(("Non-textual 'out' vertex"));
+                }
+            };
+        };
+    }
+
+    private void createStandardEdge(Vertex v, JsonNode toElem) throws InvalidGraphConfigFile {
+        String toElemVertexName = toElem.asText();
+        Vertex toElemVertex = vertices.get(toElemVertexName);
+        if (toElemVertex == null) {
+            throw new InvalidGraphConfigFile("Could not find vertex: " + toElemVertexName);
+        }
+
+        Vertex.linkVertices(v, toElemVertex);
+    }
+
+    private void createPredicateToEdges(Vertex v, JsonNode clauseNode) throws InvalidGraphConfigFile {
+        checkNodeType(clauseNode, JsonNodeType.ARRAY, "Expected predicate clause to be an array!");
+
+        Condition currentCondition;
+        Iterator<JsonNode> cnElems = clauseNode.elements();
+
+        if (!cnElems.hasNext()) {
+            throw new InvalidGraphConfigFile("Expected predicate clause to have at least one element! Got: " + clauseNode);
+        }
+
+        JsonNode condElem = cnElems.next();
+        checkNodeType(condElem, JsonNodeType.STRING, "Expected a textual condition element!");
+        currentCondition = Condition.fromSource(condElem.asText());
+
+        if (!cnElems.hasNext()) {
+            throw new InvalidGraphConfigFile("Expected a list of vertices following the predicate clause!");
+        }
+
+        JsonNode condToElem = cnElems.next();
+        checkNodeType(condToElem, JsonNodeType.ARRAY, "Predicate 'to' list must be a list of vertex names!");
+
+        Iterator<JsonNode> condToElemNameElems = condToElem.elements();
+        while(condToElemNameElems.hasNext()) {
+            JsonNode condtoNameElem = condToElemNameElems.next();
+            String condToVertexName = condtoNameElem.asText();
+            Vertex condToVertex = vertices.get(condToVertexName);
+            if (condToVertex == null) {
+                throw new InvalidGraphConfigFile("Could not find vertex: " + condToVertexName);
+            }
+            Vertex.linkVertices(v, condToVertex, currentCondition);
+        }
+    }
+
+    private void checkNodeType(JsonNode node, JsonNodeType type, String message) throws InvalidGraphConfigFile {
+        JsonNodeType actualNodeType = node.getNodeType();
+        if (actualNodeType != type) {
+            message = String.format("%s / Expected a %s, got a %s (%s)!", message, type, actualNodeType, node.asText());
+            throw new InvalidGraphConfigFile(message);
+        }
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/pipeline/graph/Edge.java b/logstash-core-event-java/src/main/java/com/logstash/pipeline/graph/Edge.java
new file mode 100644
index 00000000000..6a88778e41a
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/pipeline/graph/Edge.java
@@ -0,0 +1,36 @@
+package com.logstash.pipeline.graph;
+
+import java.util.Optional;
+import java.util.function.Predicate;
+
+/**
+ * Created by andrewvc on 2/24/16.
+ */
+public class Edge {
+    private final Vertex from;
+    private final Vertex to;
+    private final Condition condition;
+
+
+    public Edge(Vertex from, Vertex to, Condition condition) {
+        this.from = from;
+        this.to = to;
+        this.condition = condition;
+    }
+
+    public Vertex getFrom() {
+        return from;
+    }
+
+    public Vertex getTo() {
+        return to;
+    }
+
+    public boolean isCondition() {
+        return this.condition != null;
+    }
+
+    public Condition getCondition() {
+        return this.condition;
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/pipeline/graph/Vertex.java b/logstash-core-event-java/src/main/java/com/logstash/pipeline/graph/Vertex.java
new file mode 100644
index 00000000000..d306e5cdc11
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/pipeline/graph/Vertex.java
@@ -0,0 +1,103 @@
+package com.logstash.pipeline.graph;
+
+import com.logstash.pipeline.Component;
+
+import java.util.*;
+import java.util.stream.Stream;
+
+/**
+ * Created by andrewvc on 2/20/16.
+ */
+public class Vertex {
+    private final List<Edge> outEdges;
+    private final Component component;
+    private final List<Edge> inEdges;
+    private final String id;
+
+    Vertex(String id, Component component) {
+        this.id = id;
+        this.component = component;
+        this.inEdges = new ArrayList<>();
+        this.outEdges = new ArrayList<>();
+    }
+
+    public static Edge linkVertices(Vertex from, Vertex to) {
+        return linkVertices(from, to, null);
+    }
+
+    public static Edge linkVertices(Vertex from, Vertex to, Condition condition) {
+        Optional<Edge> existingEdge = getVertexLinkingEdge(from, to, condition);
+
+        if (existingEdge.isPresent()) {
+            return existingEdge.get();
+        } else {
+            Edge edge = new Edge(from, to, condition);
+            from.addOutEdge(edge);
+            to.addInEdge(edge);
+
+            return edge;
+        }
+    }
+
+    public static Optional<Edge> getVertexLinkingEdge(Vertex from, Vertex to, Condition condition) {
+        return from.getOutEdges().stream().filter(edge -> {
+            Condition edgeCondition = edge.getCondition();
+            if (edgeCondition == null) {
+                return condition == null;
+            } else {
+                return edgeCondition.equals(condition);
+            }
+        }).findFirst();
+    }
+
+    public Component getComponent() {
+        return component;
+    }
+
+    public Stream<Vertex> getOutVertices() {
+        return this.outEdges.stream().map(Edge::getTo);
+    }
+
+    public Stream<Vertex> getInVertices() { return this.inEdges.stream().map(Edge::getTo); }
+
+    public boolean hasOutVertex(Vertex v) {
+        return this.getOutVertices().anyMatch(ov -> v == ov);
+    }
+
+    public boolean hasInVertex(Vertex v) { return this.getInVertices().anyMatch(iv -> v == iv ); }
+
+
+    protected void addOutEdge(Edge e) {
+        if (!this.outEdges.contains(e)) {
+            this.outEdges.add(e);
+        }
+    }
+
+    protected void addInEdge(Edge e) {
+        if (!this.inEdges.contains(e)) {
+            this.inEdges.add(e);
+        }
+    }
+
+    public List<Edge> getOutEdges() {
+        return this.outEdges;
+    }
+
+    public List<Edge> getInEdges() {
+        return this.inEdges;
+    }
+
+    public boolean isSource() { return this.inEdges.size() == 0; }
+
+    public boolean isTerminal() {
+        return this.outEdges.size() == 0;
+    }
+
+    public String toString() {
+       return  String.format("<Vertex (%s)>", this.component);
+    }
+
+    public String getId() {
+        return id;
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/pipeline/PipelineGraphTest.java b/logstash-core-event-java/src/test/java/com/logstash/pipeline/PipelineGraphTest.java
new file mode 100644
index 00000000000..5a6655ba429
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/com/logstash/pipeline/PipelineGraphTest.java
@@ -0,0 +1,57 @@
+package com.logstash.pipeline;
+import com.logstash.pipeline.graph.ConfigFile;
+import com.logstash.pipeline.graph.Vertex;
+import org.apache.commons.io.IOUtils;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.io.InputStream;
+
+import static org.junit.Assert.*;
+
+/**
+ * Created by andrewvc on 2/20/16.
+ */
+public class PipelineGraphTest {
+    public static PipelineGraph loadGraph(String configName) throws IOException, ConfigFile.InvalidGraphConfigFile {
+        InputStream ymlStream =  ConfigFile.class.getResourceAsStream(configName);
+        String ymlString = IOUtils.toString(ymlStream, "UTF-8");
+        IOUtils.closeQuietly(ymlStream);
+
+        return ConfigFile.fromString(ymlString, new TestComponentProcessor()).getPipelineGraph();
+    }
+    public static PipelineGraph loadSimpleGraph() throws IOException, ConfigFile.InvalidGraphConfigFile {
+        return loadGraph("simple-graph-pipeline.yml");
+    }
+
+    public static PipelineGraph loadConditionalGraph() throws IOException, ConfigFile.InvalidGraphConfigFile {
+        return loadGraph("conditional-graph-pipeline.yml");
+    }
+
+    @Test
+    public void testGraphLoad() throws IOException, ConfigFile.InvalidGraphConfigFile {
+        loadSimpleGraph();
+    }
+
+    @Test
+    public void testConditionalGraphLoad() throws IOException, ConfigFile.InvalidGraphConfigFile {
+        loadConditionalGraph();
+    }
+
+    @Test
+    public void testGraphQueueGetReturnsQueue() throws IOException, ConfigFile.InvalidGraphConfigFile {
+        assertEquals(loadSimpleGraph().queueVertex().getComponent().getType(), Component.Type.QUEUE);
+    }
+
+    @Test
+    public void testPullingComponents() throws IOException, ConfigFile.InvalidGraphConfigFile {
+        Component[] components = loadSimpleGraph().getComponents();
+        assertEquals(components.length, loadSimpleGraph().getVertices().size());
+    }
+
+    @Test
+    public void testQueueConnectedToOneComponent() throws IOException, ConfigFile.InvalidGraphConfigFile {
+        Vertex qv = loadSimpleGraph().queueVertex();
+        assertEquals(qv.getOutVertices().count(), 1);
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/pipeline/TestComponentProcessor.java b/logstash-core-event-java/src/test/java/com/logstash/pipeline/TestComponentProcessor.java
new file mode 100644
index 00000000000..115463a019c
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/com/logstash/pipeline/TestComponentProcessor.java
@@ -0,0 +1,33 @@
+package com.logstash.pipeline;
+
+import com.logstash.Event;
+import com.logstash.pipeline.graph.Condition;
+import com.logstash.pipeline.graph.Vertex;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Created by andrewvc on 2/22/16.
+ */
+public class TestComponentProcessor implements ComponentProcessor {
+    @Override
+    public ArrayList<Event> process(Component component, List<Event> events) {
+        return new ArrayList<Event>();
+    }
+
+    @Override
+    public BooleanEventsResult processCondition(Condition condition, List<Event> events) {
+        return new BooleanEventsResult();
+    }
+
+    @Override
+    public void flush(Component c, boolean shutdown) {
+
+    }
+    @Override
+    public void setup(Component component) {
+
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/pipeline/graph/ConfigFileTest.java b/logstash-core-event-java/src/test/java/com/logstash/pipeline/graph/ConfigFileTest.java
new file mode 100644
index 00000000000..c10c5c3f87c
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/com/logstash/pipeline/graph/ConfigFileTest.java
@@ -0,0 +1,27 @@
+package com.logstash.pipeline.graph;
+
+import com.logstash.pipeline.PipelineGraph;
+import com.logstash.pipeline.TestComponentProcessor;
+import org.apache.commons.io.IOUtils;
+import org.junit.Test;
+
+import java.io.InputStream;
+import java.net.URL;
+
+import static org.junit.Assert.assertEquals;
+
+/**
+ * Created by andrewvc on 2/21/16.
+ */
+public class ConfigFileTest {
+    @Test
+    public void testSimpleParse() throws Exception, ConfigFile.InvalidGraphConfigFile {
+        InputStream ymlStream =  this.getClass().getResourceAsStream("simple-graph-pipeline.yml");
+        String ymlString = IOUtils.toString(ymlStream, "UTF-8");
+        IOUtils.closeQuietly(ymlStream);
+
+        ConfigFile f = ConfigFile.fromString(ymlString, new TestComponentProcessor());
+        assertEquals(f, f);
+    }
+
+}
diff --git a/logstash-core-event-java/src/test/resources/com/logstash/pipeline/graph/conditional-graph-pipeline.yml b/logstash-core-event-java/src/test/resources/com/logstash/pipeline/graph/conditional-graph-pipeline.yml
new file mode 100644
index 00000000000..27f90757b90
--- /dev/null
+++ b/logstash-core-event-java/src/test/resources/com/logstash/pipeline/graph/conditional-graph-pipeline.yml
@@ -0,0 +1,41 @@
+---
+graph:
+  log-reader:
+    component: input-generator
+    options: {"message": "Hello 50", "count": 5000000}
+    to: [main-queue]
+  main-queue:
+    component: queue-synchronous
+    to: [hello-grok]
+  hello-grok:
+    component: filter-grok
+    options:
+      match:
+        message: "%{WORD:greeting} %{NUMBER:greetno}"
+    to: [fizzbuzz]
+  fizzbuzz:
+    component: predicate-ifelse-ruby
+    to:
+      - ["event['[sequence]'] % 15 == 0", [fizzbuzzer]]
+      - ["event['[sequence]'] % 5 == 0", [fizzer]]
+      - ["event['[sequence]'] % 3 == 0", [buzzer]]
+      - ["__ELSE__", [main-out]]
+  fizzbuzzer:
+    component: filter-mutate
+    options:
+      add_field:
+        fb: fizzbuzz
+  fizzer:
+    component: filter-mutate
+    options:
+      add_field:
+        fb: fizz
+  buzzer:
+    component: filter-mutate
+    options:
+      add_field:
+        fb: buzz
+  main-out:
+    component: output-stdout
+    options:
+      codec: rubydebug
\ No newline at end of file
diff --git a/logstash-core-event-java/src/test/resources/com/logstash/pipeline/graph/simple-graph-pipeline.yml b/logstash-core-event-java/src/test/resources/com/logstash/pipeline/graph/simple-graph-pipeline.yml
new file mode 100644
index 00000000000..31bc246efb2
--- /dev/null
+++ b/logstash-core-event-java/src/test/resources/com/logstash/pipeline/graph/simple-graph-pipeline.yml
@@ -0,0 +1,16 @@
+---
+graph:
+  log-reader:
+    component: input-generator
+    options: {"message": "Hello world!", "count": 3}
+    to: [main-queue]
+  main-queue:
+    component: queue-synchronous
+    to: [hello-grok]
+  hello-grok:
+    component: filter-grok
+    options:
+      match: "%{WORD} %{NUMBER}"
+    to: [main-out]
+  main-out:
+    component: output-stdout
\ No newline at end of file
diff --git a/logstash-core/lib/logstash-core.rb b/logstash-core/lib/logstash-core.rb
index c2e4557afa8..aa8a270ea3c 100644
--- a/logstash-core/lib/logstash-core.rb
+++ b/logstash-core/lib/logstash-core.rb
@@ -1 +1 @@
-require "logstash-core/logstash-core"
+require "logstash-core/logstash-core"
\ No newline at end of file
diff --git a/logstash-core/lib/logstash/config/file.rb b/logstash-core/lib/logstash/config/file.rb
index fb0d939dfde..4c4e60790b6 100644
--- a/logstash-core/lib/logstash/config/file.rb
+++ b/logstash-core/lib/logstash/config/file.rb
@@ -5,6 +5,7 @@
 require "logstash/config/registry"
 require "logstash/errors"
 require "logger"
+java_import 'com.logstash.pipeline.graph.ConfigFile'
 
 class LogStash::Config::File
   include Enumerable
@@ -18,12 +19,7 @@ def initialize(text)
   end # def initialize
 
   def parse(text)
-    grammar = LogStashConfigParser.new
-    result = grammar.parse(text)
-    if result.nil?
-      raise LogStash::ConfigurationError, grammar.failure_reason
-    end
-    return result
+    com.logstash.pipeline.graph.ConfigFile.fromString(config_str, LogStash::Pipeline::PipelineComponentProcessor)
   end # def parse
 
   def plugin(plugin_type, name, *args)
diff --git a/logstash-core/lib/logstash/pipeline.rb b/logstash-core/lib/logstash/pipeline.rb
index 0eef3bdb501..073fa405183 100644
--- a/logstash-core/lib/logstash/pipeline.rb
+++ b/logstash-core/lib/logstash/pipeline.rb
@@ -20,6 +20,12 @@
 require "logstash/instrument/collector"
 require "logstash/output_delegator"
 require "logstash/filter_delegator"
+require "logstash/pipeline/ruby_component_processor"
+
+java_import 'com.logstash.pipeline.Worker'
+java_import 'com.logstash.pipeline.graph.ConfigFile'
+java_import 'com.logstash.pipeline.Constants'
+
 
 module LogStash; class Pipeline
  attr_reader :inputs,
@@ -64,9 +70,9 @@ def initialize(config_str, settings = {})
     settings.each {|setting, value| configure(setting, value) }
     @reporter = LogStash::PipelineReporter.new(@logger, self)
 
-    @inputs = nil
-    @filters = nil
-    @outputs = nil
+    @inputs = []
+    @filters = []
+    @outputs = []
 
     @worker_threads = []
 
@@ -80,26 +86,18 @@ def initialize(config_str, settings = {})
     # sure the metric instance is correctly send to the plugin.
     @metric = settings.fetch(:metric, Instrument::NullMetric.new)
 
-    grammar = LogStashConfigParser.new
-    @config = grammar.parse(config_str)
-    if @config.nil?
-      raise LogStash::ConfigurationError, grammar.failure_reason
-    end
-    # This will compile the config to ruby and evaluate the resulting code.
-    # The code will initialize all the plugins and define the
-    # filter and output methods.
-    code = @config.compile
-    @code = code
-
-    # The config code is hard to represent as a log message...
-    # So just print it.
-    @logger.debug? && @logger.debug("Compiled pipeline code:\n#{code}")
-
-    begin
-      eval(code)
-    rescue => e
-      raise
+    component_processor = ::LogStash::Pipeline::RubyComponentProcessor.new(self) do |component, plugin|
+      case plugin
+        when LogStash::Inputs::Base
+          @inputs << plugin
+        when LogStash::FilterDelegator
+          @filters << plugin
+        when LogStash::OutputDelegator
+          @outputs << plugin
+      end
     end
+    @config_file = com.logstash.pipeline.graph.ConfigFile.fromString(config_str, component_processor)
+    @graph = @config_file.getPipelineGraph()
 
     @input_queue = LogStash::Util::WrappedSynchronousQueue.new
     @events_filtered = Concurrent::AtomicFixnum.new(0)
@@ -229,12 +227,8 @@ def start_workers
         @logger.warn "CAUTION: Recommended inflight events max exceeded! Logstash will run with up to #{max_inflight} events in memory in your current configuration. If your message sizes are large this may cause instability with the default heap size. Please consider setting a non-standard heap size, changing the batch size (currently #{batch_size}), or changing the number of pipeline workers (currently #{pipeline_workers})"
       end
 
-      pipeline_workers.times do |t|
-        @worker_threads << Thread.new do
-          LogStash::Util.set_thread_name("[#{pipeline_id}]>worker#{t}")
-          worker_loop(batch_size, batch_delay)
-        end
-      end
+      workers = com.logstash.pipeline.Worker.startWorkers(pipeline_workers, @graph, @input_queue.queue, batch_size, batch_delay)
+      @worker_threads = workers.map(&:getThread)
     ensure
       # it is important to garantee @ready to be true after the startup sequence has been completed
       # to potentially unblock the shutdown method which may be waiting on @ready to proceed
@@ -242,110 +236,6 @@ def start_workers
     end
   end
 
-  # Main body of what a worker thread does
-  # Repeatedly takes batches off the queue, filters, then outputs them
-  def worker_loop(batch_size, batch_delay)
-    running = true
-
-    namespace_events = metric.namespace([:stats, :events])
-    namespace_pipeline = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :events])
-
-    while running
-      # To understand the purpose behind this synchronize please read the body of take_batch
-      input_batch, signal = @input_queue_pop_mutex.synchronize { take_batch(batch_size, batch_delay) }
-      running = false if signal == LogStash::SHUTDOWN
-
-      @events_consumed.increment(input_batch.size)
-      namespace_events.increment(:in, input_batch.size)
-      namespace_pipeline.increment(:in, input_batch.size)
-
-      filtered_batch = filter_batch(input_batch)
-
-      if signal # Flush on SHUTDOWN or FLUSH
-        flush_options = (signal == LogStash::SHUTDOWN) ? {:final => true} : {}
-        flush_filters_to_batch(filtered_batch, flush_options)
-      end
-
-      @events_filtered.increment(filtered_batch.size)
-
-      namespace_events.increment(:filtered, filtered_batch.size)
-      namespace_pipeline.increment(:filtered, filtered_batch.size)
-
-      output_batch(filtered_batch)
-
-      namespace_events.increment(:out, filtered_batch.size)
-      namespace_pipeline.increment(:out, filtered_batch.size)
-
-      inflight_batches_synchronize { set_current_thread_inflight_batch(nil) }
-    end
-  end
-
-  def take_batch(batch_size, batch_delay)
-    batch = []
-    # Since this is externally synchronized in `worker_look` wec can guarantee that the visibility of an insight batch
-    # guaranteed to be a full batch not a partial batch
-    set_current_thread_inflight_batch(batch)
-
-    signal = false
-    batch_size.times do |t|
-      event = (t == 0) ? @input_queue.take : @input_queue.poll(batch_delay)
-
-      if event.nil?
-        next
-      elsif event == LogStash::SHUTDOWN || event == LogStash::FLUSH
-        # We MUST break here. If a batch consumes two SHUTDOWN events
-        # then another worker may have its SHUTDOWN 'stolen', thus blocking
-        # the pipeline. We should stop doing work after flush as well.
-        signal = event
-        break
-      else
-        batch << event
-      end
-    end
-
-    [batch, signal]
-  end
-
-  def filter_batch(batch)
-    batch.reduce([]) do |acc,e|
-      if e.is_a?(LogStash::Event)
-        filtered = filter_func(e)
-        filtered.each {|fe| acc << fe unless fe.cancelled?}
-      end
-      acc
-    end
-  rescue Exception => e
-    # Plugins authors should manage their own exceptions in the plugin code
-    # but if an exception is raised up to the worker thread they are considered
-    # fatal and logstash will not recover from this situation.
-    #
-    # Users need to check their configuration or see if there is a bug in the
-    # plugin.
-    @logger.error("Exception in pipelineworker, the pipeline stopped processing new events, please check your filter configuration and restart Logstash.",
-                  "exception" => e, "backtrace" => e.backtrace)
-    raise
-  end
-
-  # Take an array of events and send them to the correct output
-  def output_batch(batch)
-    # Build a mapping of { output_plugin => [events...]}
-    outputs_events = batch.reduce(Hash.new { |h, k| h[k] = [] }) do |acc, event|
-      # We ask the AST to tell us which outputs to send each event to
-      # Then, we stick it in the correct bin
-
-      # output_func should never return anything other than an Array but we have lots of legacy specs
-      # that monkeypatch it and return nil. We can deprecate  "|| []" after fixing these specs
-      outputs_for_event = output_func(event) || []
-
-      outputs_for_event.each { |output| acc[output] << event }
-      acc
-    end
-
-    # Now that we have our output to event mapping we can just invoke each output
-    # once with its list of events
-    outputs_events.each { |output, events| output.multi_receive(events) }
-  end
-
   def set_current_thread_inflight_batch(batch)
     @inflight_batches[Thread.current] = batch
   end
@@ -426,9 +316,7 @@ def shutdown(&before_stop)
 
     before_stop.call if block_given?
 
-    @logger.info "Closing inputs"
     @inputs.each(&:do_stop)
-    @logger.info "Closed inputs"
   end # def shutdown
 
   # After `shutdown` is called from an external thread this is called from the main thread to
@@ -438,7 +326,7 @@ def shutdown_workers
     # Each worker thread will receive this exactly once!
     @worker_threads.each do |t|
       @logger.debug("Pushing shutdown", :thread => t)
-      @input_queue.push(LogStash::SHUTDOWN)
+      @input_queue.queue.put(com.logstash.pipeline.Constants.shutdownEvent)
     end
 
     @worker_threads.each do |t|
@@ -446,6 +334,7 @@ def shutdown_workers
       t.join
     end
 
+
     @filters.each(&:do_close)
     @outputs.each(&:do_close)
   end
@@ -508,7 +397,7 @@ def shutdown_flusher
   def flush
     if @flushing.compare_and_set(false, true)
       @logger.debug? && @logger.debug("Pushing flush onto pipeline")
-      @input_queue.push(LogStash::FLUSH)
+      @input_queue.push(com.logstash.pipeline.Constants.flushEvent)
     end
   end
 
diff --git a/logstash-core/lib/logstash/pipeline/ruby_component_processor.rb b/logstash-core/lib/logstash/pipeline/ruby_component_processor.rb
new file mode 100644
index 00000000000..5c43ed9562d
--- /dev/null
+++ b/logstash-core/lib/logstash/pipeline/ruby_component_processor.rb
@@ -0,0 +1,71 @@
+java_import 'com.logstash.pipeline.ComponentProcessor'
+java_import 'com.logstash.pipeline.BooleanEventsResult'
+
+module LogStash; class Pipeline
+  class RubyComponentProcessor
+    include com.logstash.pipeline.ComponentProcessor
+
+    def initialize(pipeline,&block)
+      @lookup = {}
+      @execution_lookup = {}
+      @pipeline = pipeline
+      @on_setup = block
+      @compiled_conditions = {}
+    end
+
+    def setup(component)
+      type, name = component.getComponentName.split("-", 2)
+      options = component.getOptionsStr ? LogStash::Json.load(component.getOptionsStr) : {}
+      return if ["queue","predicate"].include?(type) # TODO: One day this will do something...
+      plugin = @pipeline.plugin(type, name, options)
+      @on_setup.call(component, plugin) if @on_setup
+      @lookup[component] = plugin
+      @execution_lookup[component] = if plugin.is_a? LogStash::FilterDelegator
+                                       proc do |events|
+                                         out = plugin.multi_filter(events)
+                                         @pipeline.events_filtered.increment(out.length)
+                                         out
+                                       end
+                                       plugin.method(:multi_filter)
+                                     elsif plugin.is_a? LogStash::OutputDelegator
+                                       proc do |events|
+                                         out = plugin.multi_receive(events)
+                                         @pipeline.events_consumed.increment(out.length)
+                                         out
+                                       end
+                                     end
+    end
+
+    def process(component, events)
+      rubyified_events = rubyify_events(events.compact)
+      res = @execution_lookup[component].call(rubyified_events)
+
+      javaify_events(res)
+    end
+
+    def rubyify_events(java_events)
+      java_events.map {|e| Event.from_java(e)}
+    end
+
+    def javaify_events(ruby_events)
+      ruby_events.map {|e| e.to_java}
+    end
+
+    def flush(component, shutdown)
+      component = @lookup[component]
+      if component.respond_to?(:flush)
+        component.flush()
+      end
+    end
+
+    def process_condition(condition, events)
+      ber = com.logstash.pipeline.BooleanEventsResult.new
+      compiled_condition(condition).call(events,ber.getTrueEvents, ber.getFalseEvents)
+      ber
+    end
+
+    def compiled_condition(condition)
+      @compiled_conditions[condition] ||= eval("proc {|events,t,f| events.each {|event_j| event = Event.from_java(event_j); (#{condition.source}) ? t.add(event_j) : f.add(event_j) } }")
+    end
+  end
+end end
\ No newline at end of file
diff --git a/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
index a8822ca0af5..834fd0df15f 100644
--- a/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
+++ b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
@@ -2,6 +2,8 @@
 
 module LogStash; module Util
   class WrappedSynchronousQueue
+    attr_reader :queue
+
     java_import java.util.concurrent.SynchronousQueue
     java_import java.util.concurrent.TimeUnit
 
@@ -13,8 +15,8 @@ def initialize()
     # it will block until the object can be added to the queue.
     #
     # @param [Object] Object to add to the queue
-    def push(obj)
-      @queue.put(obj)
+    def push(event)
+      @queue.put(event.to_java)
     end
     alias_method(:<<, :push)
 
diff --git a/simple-graph-pipeline.yml b/simple-graph-pipeline.yml
new file mode 100644
index 00000000000..eba4062f729
--- /dev/null
+++ b/simple-graph-pipeline.yml
@@ -0,0 +1,24 @@
+---
+graph:
+  log-reader:
+    component: input-generator
+    options: {"message": "Hello 50", "count": 5000000}
+    to: [main-queue]
+  main-queue:
+    component: queue-synchronous
+    to: [hello-grok]
+  hello-grok:
+    component: filter-grok
+    options:
+      match:
+        message: "%{WORD:greeting} %{NUMBER:greetno}"
+    to: [main-out]
+  field-adder:
+    component: filter-mutate
+    options:
+      add_field:
+        foo: bar
+  main-out:
+    component: output-stdout
+    options:
+      codec: dots
\ No newline at end of file
diff --git a/simple-kv.yml b/simple-kv.yml
new file mode 100644
index 00000000000..18f37922e63
--- /dev/null
+++ b/simple-kv.yml
@@ -0,0 +1,21 @@
+---
+graph:
+  log-reader:
+    component: input-generator
+    options: {"message": "baz=bot blah=bar", "count": 10000000}
+    to: [main-queue]
+  main-queue:
+    component: queue-synchronous
+    to: [hello-kv]
+  hello-kv:
+    component: filter-kv
+    to: [main-out]
+  field-adder:
+    component: filter-mutate
+    options:
+      add_field:
+        foo: bar
+  main-out:
+    component: output-stdout
+    options:
+      codec: dots
