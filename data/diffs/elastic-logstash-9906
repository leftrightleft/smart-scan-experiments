diff --git a/docs/index-shared3.asciidoc b/docs/index-shared3.asciidoc
index 9454487961b..f5e06e1d5b7 100644
--- a/docs/index-shared3.asciidoc
+++ b/docs/index-shared3.asciidoc
@@ -126,6 +126,9 @@ include::static/maintainer-guide.asciidoc[]
 :edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/submitting-a-plugin.asciidoc
 include::static/submitting-a-plugin.asciidoc[]
 
+// :edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/more-resources.asciidoc
+include::static/more-resources.asciidoc[]
+
 // Glossary of Terms
 
 :edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/glossary.asciidoc
diff --git a/docs/index.asciidoc b/docs/index.asciidoc
index 6680d9da85b..35ceda3935c 100644
--- a/docs/index.asciidoc
+++ b/docs/index.asciidoc
@@ -3,6 +3,258 @@
 
 :plugins-repo-dir:  {docdir}/../../logstash-docs/docs
 
+<<<<<<< Updated upstream
 include::index-shared1.asciidoc[]
 include::index-shared2.asciidoc[]
 include::index-shared3.asciidoc[]
+=======
+
+
+:lsissue: https://github.com/elastic/logstash/issues
+:lspull: https://github.com/elastic/logstash/pull/
+
+
+//////////
+release-state can be: released | prerelease | unreleased
+//////////
+:release-state:  unreleased
+:versioned_docs: false
+
+:jdk:                   1.8.0
+:lsissue:               https://github.com/elastic/logstash/issues
+:lsplugindocs:          https://www.elastic.co/guide/en/logstash-versioned-plugins/current
+
+include::{asciidoc-dir}/../../shared/attributes.asciidoc[]
+
+[[introduction]]
+== Logstash Introduction
+
+Logstash is an open source data collection engine with real-time pipelining capabilities. Logstash can dynamically
+unify data from disparate sources and normalize the data into destinations of your choice. Cleanse and democratize all
+your data for diverse advanced downstream analytics and visualization use cases.
+
+While Logstash originally drove innovation in log collection, its capabilities extend well beyond that use case. Any
+type of event can be enriched and transformed with a broad array of input, filter, and output plugins, with many
+native codecs further simplifying the ingestion process. Logstash accelerates your insights by harnessing a greater
+volume and variety of data.
+
+// The pass blocks here point to the correct repository for the edit links in the guide.
+
+// Introduction
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/introduction.asciidoc
+include::static/introduction.asciidoc[]
+
+// Glossary and core concepts go here
+
+// Getting Started with Logstash
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/getting-started-with-logstash.asciidoc
+include::static/getting-started-with-logstash.asciidoc[]
+
+// Advanced LS Pipelines
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/advanced-pipeline.asciidoc
+include::static/advanced-pipeline.asciidoc[]
+
+// Processing Pipeline
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/life-of-an-event.asciidoc
+include::static/life-of-an-event.asciidoc[]
+
+// Lostash setup
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/setting-up-logstash.asciidoc
+include::static/setting-up-logstash.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/settings-file.asciidoc
+include::static/settings-file.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/keystore.asciidoc
+include::static/keystore.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/running-logstash-command-line.asciidoc
+include::static/running-logstash-command-line.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/running-logstash.asciidoc
+include::static/running-logstash.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/docker.asciidoc
+include::static/docker.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/logging.asciidoc
+include::static/logging.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/shutdown.asciidoc
+include::static/shutdown.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/installing-xls.asciidoc
+include::static/setup/installing-xls.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/setting-up-xpack.asciidoc
+include::static/setup/setting-up-xpack.asciidoc[]
+
+// Breaking Changes
+
+:edit_url: https://github.com/elastic/logstash/edit/master/docs/static/breaking-changes.asciidoc
+include::static/breaking-changes.asciidoc[]
+
+
+// Upgrading Logstash
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/upgrading.asciidoc
+include::static/upgrading.asciidoc[]
+
+// Configuring Logstash
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/configuration.asciidoc
+include::static/configuration.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/multiple-pipelines.asciidoc
+include::static/multiple-pipelines.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/pipeline-pipeline-config.asciidoc
+include::static/pipeline-pipeline-config.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/reloading-config.asciidoc
+include::static/reloading-config.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/managing-multiline-events.asciidoc
+include::static/managing-multiline-events.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/glob-support.asciidoc
+include::static/glob-support.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/ingest-convert.asciidoc
+include::static/ingest-convert.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/ls-ls-config.asciidoc
+include::static/ls-ls-config.asciidoc[]
+
+ifdef::include-xpack[]
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/management/configuring-centralized-pipelines.asciidoc
+include::static/management/configuring-centralized-pipelines.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/monitoring/configuring-logstash.asciidoc
+include::static/monitoring/configuring-logstash.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/security/logstash.asciidoc
+include::static/security/logstash.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/setup/configuring-xls.asciidoc
+include::static/setup/configuring-xls.asciidoc[]
+endif::include-xpack[]
+
+// Centralized configuration managements
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/config-management.asciidoc
+include::static/config-management.asciidoc[]
+
+// Working with Logstash Modules
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/modules.asciidoc
+include::static/modules.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/arcsight-module.asciidoc
+include::static/arcsight-module.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/netflow-module.asciidoc
+include::static/netflow-module.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/azure-module.asciidoc
+include::static/azure-module.asciidoc[]
+
+// Working with Filebeat Modules
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/filebeat-modules.asciidoc
+include::static/filebeat-modules.asciidoc[]
+
+// Data resiliency
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/resiliency.asciidoc
+include::static/resiliency.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/persistent-queues.asciidoc
+include::static/persistent-queues.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/dead-letter-queues.asciidoc
+include::static/dead-letter-queues.asciidoc[]
+
+// Transforming Data
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/transforming-data.asciidoc
+include::static/transforming-data.asciidoc[]
+
+// Deploying & Scaling
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/deploying.asciidoc
+include::static/deploying.asciidoc[]
+
+// Troubleshooting performance
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/performance-checklist.asciidoc
+include::static/performance-checklist.asciidoc[]
+
+// Monitoring overview
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/monitoring.asciidoc
+include::static/monitoring.asciidoc[]
+
+// Monitoring APIs
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/monitoring-apis.asciidoc
+include::static/monitoring-apis.asciidoc[]
+
+// Working with Plugins
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/plugin-manager.asciidoc
+include::static/plugin-manager.asciidoc[]
+
+// These files do their own pass blocks
+
+include::{plugins-repo-dir}/plugins/inputs.asciidoc[]
+include::{plugins-repo-dir}/plugins/outputs.asciidoc[]
+include::{plugins-repo-dir}/plugins/filters.asciidoc[]
+include::{plugins-repo-dir}/plugins/codecs.asciidoc[]
+
+:edit_url:
+
+// Contributing to Logstash
+
+include::static/contributing-to-logstash.asciidoc[]
+
+include::static/input.asciidoc[]
+include::static/codec.asciidoc[]
+include::static/filter.asciidoc[]
+include::static/output.asciidoc[]
+
+// Contributing a Patch to a Logstash Plugin
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/contributing-patch.asciidoc
+include::static/contributing-patch.asciidoc[]
+
+// Logstash Community Maintainer Guide
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/maintainer-guide.asciidoc
+include::static/maintainer-guide.asciidoc[]
+
+// A space is necessary here ^^^
+
+
+// Submitting a Plugin
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/submitting-a-plugin.asciidoc
+include::static/submitting-a-plugin.asciidoc[]
+
+// Glossary of Terms
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/glossary.asciidoc
+include::static/glossary.asciidoc[]
+
+// This is in the pluginbody.asciidoc itself
+//
+
+// Release Notes
+
+:edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/releasenotes.asciidoc
+include::static/releasenotes.asciidoc[]
+>>>>>>> Stashed changes
diff --git a/docs/static/faq.asciidoc b/docs/static/faq.asciidoc
new file mode 100644
index 00000000000..37ec8cf95f8
--- /dev/null
+++ b/docs/static/faq.asciidoc
@@ -0,0 +1,174 @@
+[[faq]]
+== Frequently asked questions
+
+This section contains frequently asked questions about Logstash. 
+//Also check out the
+https://discuss.elastic.co/c/beats/filebeat[Filebeat discussion forum].
+
+[float]
+[[filebeat-network-volumes]]
+=== Can't read log files from network volumes?
+
+We do not recommend reading log files from network volumes. Whenever possible, install Filebeat on the host machine and
+send the log files directly from there. Reading files from network volumes (especially on Windows) can have unexpected side
+effects. For example, changed file identifiers may result in Filebeat reading a log file from scratch again.
+
+[float]
+[[filebeat-not-collecting-lines]]
+=== Filebeat isn't collecting lines from a file?
+
+Filebeat might be incorrectly configured or unable to send events to the output. To resolve the issue:
+
+* Make sure the config file specifies the correct path to the file that you are collecting. See <<filebeat-configuration>>
+for more information.
+* Verify that the file is not older than the value specified by <<ignore-older,`ignore_older`>>. ignore_older is disable by
+default so this depends on the value you have set. You can change this behavior by specifying a different value for
+<<ignore-older,`ignore_older`>>.
+* Make sure that Filebeat is able to send events to the configured output. Run Filebeat in debug mode to determine whether
+it's publishing events successfully:
++
+["source","sh",subs="attributes,callouts"]
+----------------------------------------------------------------------
+./filebeat -c config.yml -e -d "*"
+----------------------------------------------------------------------
+
+[float]
+[[open-file-handlers]]
+=== Too many open file handlers?
+
+Filebeat keeps the file handler open in case it reaches the end of a file so that it can read new log lines in near real time. If Filebeat is harvesting a large number of files, the number of open files can become an issue. In most environments, the number of files that are actively updated is low. The `close_inactive` configuration option should be set accordingly to close files that are no longer active.
+
+There are additional configuration options that you can use to close file handlers, but all of them should be used carefully because they can have side effects. The options are:
+
+* <<close-renamed,`close_renamed`>>
+* <<close-removed,`close_removed`>>
+* <<close-eof,`close_eof`>>
+* <<close-timeout,`close_timeout`>>
+* <<harvester-limit,`harvester_limit`>>
+
+The `close_renamed` and `close_removed` options can be useful on Windows to resolve issues related to file rotation. See <<windows-file-rotation>>. The `close_eof` option can be useful in environments with a large number of files that have only very few entries. The `close_timeout` option is useful in environments where closing file handlers is more important than sending all log lines. For more details, see <<configuration-filebeat-options>>.
+
+Make sure that you read the documentation for these configuration options before using any of them.
+
+[float]
+[[reduce-registry-size]]
+=== Registry file is too large?
+
+Filebeat keeps the state of each file and persists the state to disk in the `registry_file`. The file state is used to continue file reading at a previous position when Filebeat is restarted. If a large number of new files are produced every day, the registry file might grow to be too large. To reduce the size of the registry file, there are two configuration options available: <<clean-removed,`clean_removed`>> and <<clean-inactive,`clean_inactive`>>.
+
+For old files that you no longer touch and are ignored (see <<ignore-older,`ignore_older`>>), we recommended that you use `clean_inactive`. If old files get removed from disk, then use the `clean_removed` option.
+
+
+[float]
+[[inode-reuse-issue]]
+=== Inode reuse causes Filebeat to skip lines?
+
+On Linux file systems, Filebeat uses the inode and device to identify files. When a file is removed from disk, the inode may be assigned to a new file. In use cases involving file rotation, if an old file is removed and a new one is created immediately afterwards, the new file may have the exact same inode as the file that was removed. In this case, Filebeat assumes that the new file is the same as the old and tries to continue reading at the old position, which is not correct.
+
+By default states are never removed from the registry file. To resolve the inode reuse issue, we recommend that you use the <<clean-options,`clean_*`>> options, especially <<clean-inactive,`clean_inactive`>>, to remove the state of inactive files. For example, if your files get rotated every 24 hours, and the rotated files are not updated anymore, you can set <<ignore-older,`ignore_older`>> to 48 hours and <<clean-inactive,`clean_inactive`>> to 72 hours.
+
+You can use <<clean-removed,`clean_removed`>> for files that are removed from disk. Be aware that `clean_removed` cleans the file state from the registry whenever a file cannot be found during a scan. If the file shows up again later, it will be sent again from scratch.
+
+[float]
+[[windows-file-rotation]]
+=== Open file handlers cause issues with Windows file rotation?
+
+On Windows, you might have problems renaming or removing files because Filebeat keeps the file handlers open. This can lead to issues with the file rotating system. To avoid this issue, you can use the <<close-removed,`close_removed`>> and <<close-renamed,`close_renamed`>> options together.
+
+IMPORTANT: When you configure these options, files may be closed before the harvester has finished reading the files. If the file cannot be picked up again by the prospector and the harvester hasn't finish reading the file, the missing lines will never be sent to the output.
+
+
+[float]
+[[filebeat-cpu]]
+=== Filebeat is using too much CPU?
+
+Filebeat might be configured to scan for files too frequently. Check the setting for `scan_frequency` in the `filebeat.yml`
+config file. Setting `scan_frequency` to less than 1s may cause Filebeat to scan the disk in a tight loop.
+
+[float]
+[[dashboard-fields-incorrect-filebeat]]
+=== Dashboard in Kibana is breaking up data fields incorrectly?
+
+The index template might not be loaded correctly. See <<filebeat-template>>.
+
+[float]
+[[fields-not-indexed]]
+=== Fields are not indexed or usable in Kibana visualizations?
+
+If you have recently performed an operation that loads or parses custom, structured logs,
+you might need to refresh the index to make the fields available in Kibana. To refresh
+the index, use the {elasticsearch}/indices-refresh.html[refresh API]. For example:
+
+["source","sh"]
+----------------------------------------------------------------------
+curl -XPOST 'http://localhost:9200/filebeat-2016.08.09/_refresh'
+----------------------------------------------------------------------
+
+[float]
+[[newline-character-required-eof]]
+=== Filebeat isn't shipping the last line of a file?
+
+Filebeat uses a newline character to detect the end of an event. If lines are added incrementally to a file that's being
+harvested, a newline character is required after the last line, or Filebeat will not read the last line of
+the file.
+
+[float]
+[[faq-deleted-files-are-not-freed]]
+=== Filebeat keeps open file handlers of deleted files for a long time?
+
+In the default behaviour, Filebeat opens the files and keeps them open until it
+reaches the end of them.  In situations when the configured output is blocked
+(e.g. Elasticsearch or Logstash is unavailable) for a long time, this can cause
+Filebeat to keep file handlers to files that were deleted from the file system
+in the mean time. As long as Filebeat keeps the deleted files open, the
+operating system doesn't free up the space on disk, which can lead to increase
+disk utilisation or even out of disk situations.
+
+To mitigate this issue, you can set the <<close-timeout>> setting to `5m`. This
+will ensure every file handler is closed once every 5 minutes, regardless of
+whether it reached EOF or not. Note that this option can lead to data loss if the
+file is deleted before Filebeat reaches the end of the file.
+
+================
+[float]
+[[faq-kafka]]
+=== Kafka
+
+This section is just a quick summary the most common  Kafka questions I answered on Github and Slack over the last few months:
+
+[float]
+[[faq-kafka-partitions]]
+==== How many partitions should be used per topic?
+
+At least: Number of LS nodes x consumer threads per node
+Better yet: Use a multiple of the above number. Increasing the number of partitions for an existing topic is extremely complicated. Partitions have a very low overhead. Using 5 to 10 times the number of partitions suggested by the first point is generally fine so long as the overall partition count does not exceed 2k (err on the side of over-partitioning 10x when for less than 1k partitions overall, over-partition less liberally if it makes you exceed 1k partitions).
+
+[float]
+[[faq-kafka-threads]]
+==== How many consumer threads should I configure?
+
+Lower values tend to be more efficient and have less memory overhead. Try a value of 1 then iterate your way up. The value should in general be lower than the number of pipeline workers. Values larger than 4 will rarely result in a performance improvement.
+
+
+[float]
+[[faq-kafka-pq-persist]]
+==== Does Kafka Input when used with the LS PQ only commit offsets once the event has been safely persisted to the PQ.
+
+No, we can’t make the guarantee. Offsets are committed to Kafka periodically. If writes to the PQ are slow/blocked offsets for events that haven’t yet safely reached the PQ can be committed.
+
+
+[float]
+[[faq-kafka-offset-commit]]
+==== Does Kafa Input only commit offsets for events that have passed the pipeline fully?
+No, we can’t make the guarantee. Offsets are committed to Kafka periodically. If writes to the PQ are slow/blocked offsets for events that haven’t yet safely reached the PQ can be committed.
+
+
+
+
+
+Common Kafka Support Issues and their Solutions
+
+
+
+//include::../../libbeat/docs/faq-limit-bandwidth.asciidoc[]
+//include::../../libbeat/docs/shared-faq.asciidoc[]
diff --git a/docs/static/more-resources.asciidoc b/docs/static/more-resources.asciidoc
new file mode 100644
index 00000000000..c7b3f09ac54
--- /dev/null
+++ b/docs/static/more-resources.asciidoc
@@ -0,0 +1,5 @@
+[[troubleshooting]]
+=== Additional resources
+
+
+Some text
diff --git a/docs/static/releasenotes.asciidoc b/docs/static/releasenotes.asciidoc
index 10b61eaa32d..712dc6214fc 100644
--- a/docs/static/releasenotes.asciidoc
+++ b/docs/static/releasenotes.asciidoc
@@ -5,6 +5,7 @@ This section summarizes the changes in the following releases:
 
 * <<logstash-7-0-0-alpha1,Logstash 7.0.0-alpha1>>
 
+<<<<<<< Updated upstream
 ifdef::include-xpack[]
 See also:
 
@@ -15,3 +16,655 @@ endif::include-xpack[]
 === Logstash 7.0.0-alpha1 Release Notes
 
 Placeholder for alpha1 release notes
+=======
+// Format an issue like this:
+// Format a pull like this:  {lspull}9877[#9877]
+
+[[logstash-6-4-0]]
+=== Logstash 6.4.0 Release Notes
+
+* Adds the Azure Module for integrating Azure activity logs and SQL diagnostic logs with the Elastic Stack.
+* Adds the [Azure event hub input]{logstash-ref}plugins-input-azure_event_hubs.html as a default plugin.
+* Adds support for port customization in cloud id ({lsissue}9877[Issue 9877]).
+* Adds opt-in strict-mode for field reference ({lsissue}9591[Issue 9591]).
+* Fixes incorrect pipeline shutdown logging ({lsissue}9688[Issue 9688]).
+* Fixes incorrect type handling between Java pipeline and Ruby pipeline ({lsissue}9671[Issue 9671]).
+* Fixes possible where Ensure separate output streams to avoid keystore corruption issue by ensuring separate output streams ({lsissue}9582[Issue 9582]).
+* Javafication to continue moving parts of Logstash core from Ruby to Java and some general code cleanup ({lsissue}9414[Issue 9414], {lsissue}9415[Issue 9415], {lsissue}9416[Issue 9416], {lsissue}9422[Issue 9422], {lsissue}9482[Issue 9482], {lsissue}9486[Issue 9486], {lsissue}9489[Issue 9489], {lsissue}9490[Issue 9490], {lsissue}9491[Issue 9491], {lsissue}9496[Issue 9496], {lsissue}9520[Issue 9520], {lsissue}9587[Issue 9587], {lsissue}9574[Issue 9574], {lsissue}9610[Issue 9610], {lsissue}9620[Issue 9620], {lsissue}9631[Issue 9631], {lsissue}9632[Issue 9632], {lsissue}9633[Issue 9633], {lsissue}9661[Issue 9661], {lsissue}9662[Issue 9662], {lsissue}9665[Issue 9665], {lsissue}9667[Issue 9667], {lsissue}9668[Issue 9668], {lsissue}9670[Issue 9670], {lsissue}9676[Issue 9676], {lsissue}9687[Issue 9687], {lsissue}9693[Issue 9693], {lsissue}9697[Issue 9697], {lsissue}9699[Issue 9699], {lsissue}9717[Issue 9717], {lsissue}9723[Issue 9723], {lsissue}9731[Issue 9731], {lsissue}9740[Issue 9740], {lsissue}9742[Issue 9742], {lsissue}9743[Issue 9743], {lsissue}9751[Issue 9751], {lsissue}9752[Issue 9752], {lsissue}9765[Issue 9765]).
+
+
+* TEST THIS ONE  {lsissue}9877[#9877]
+* TEST THIS ONE  {lsissue}9870[#9870]
+* TEST THIS ONE  {lspull}9870[#9870]
+
+What got deleted:  :lsissue: https://github.com/elastic/logstash/issues`
+
+``:logstash-ref:         http://www.elastic.co/guide/en/logstash/{branch}``
+
+
+[float]
+==== Plugins
+
+*Rubydebug Codec*
+
+* Fixes crash that could occur on startup if `$HOME` was unset or if `${HOME}/.aprc` was unreadable by pinning awesome_print dependency to a release before the bug was introduced. https://github.com/logstash-plugins/logstash-codec-rubydebug/pull/5[#5]
+
+*Fingerprint Filter*
+
+* Adds support for non-keyed, regular hash functions. https://github.com/logstash-plugins/logstash-filter-fingerprint/issues/18[#18]
+
+*KV Filter*
+
+* Adds `whitespace => strict` mode, which allows the parser to behave more predictably when input is known to avoid unnecessary whitespace. https://github.com/logstash-plugins/logstash-filter-kv/pull/67[#67]
+* Adds error handling, which tags the event with `_kv_filter_error` if an exception is raised while handling an event instead of allowing the plugin to crash. https://github.com/logstash-plugins/logstash-filter-kv/pull/68[#68]
+
+*Azure Event Hubs Input*
+
+* Initial version of the {logstash-ref}/plugins-inputs-azure_event_hubs.html[azure_event_hubs input plugin], which supersedes logstash-input-azureeventhub.
+
+*Beats Input*
+
+* Adds `add_hostname` flag to enable/disable the population of the `host` field from the beats.hostname. field https://github.com/logstash-plugins/logstash-input-beats/pull/340[#340]
+* Fixes handling of batches where the sequence numbers do not start with 1. https://github.com/logstash-plugins/logstash-input-beats/pull/342[#342]
+* Changes project to use gradle version 4.8.1. https://github.com/logstash-plugins/logstash-input-beats/pull/334[#334]
+* Adds `ssl_peer_metadata` option. https://github.com/logstash-plugins/logstash-input-beats/pull/327[#327]
+* Fixes `ssl_verify_mode => peer`. https://github.com/logstash-plugins/logstash-input-beats/pull/326[#326]
+
+*Exec Input*
+
+* Fixes issue where certain log entries were incorrectly writing 'jdbc input' instead of 'exec input'. https://github.com/logstash-plugins/logstash-input-exec/pull/21[#21]
+
+*File Input*
+
+* Adds new feature: `mode` setting. Introduces two modes, `tail` mode is the existing behaviour for tailing, `read` mode is new behaviour that is optimized for the read complete content scenario. Please read the docs to fully appreciate the benefits of `read` mode.
+* Adds new feature: File completion actions. Settings `file_completed_action` and `file_completed_log_path` control what actions to do after a file is completely read. Applicable: `read` mode only.
+* Adds new feature: in `read` mode, compressed files can be processed, GZIP only.
+* Adds new feature: Files are sorted after being discovered. Settings `file_sort_by` and `file_sort_direction` control the sort order. Applicable: any mode.
+* Adds new feature: Banded or striped file processing. Settings: `file_chunk_size` and `file_chunk_count` control banded or striped processing. Applicable: any mode.
+* Adds new feature: `sincedb_clean_after` setting. Introduces expiry of sincedb records. The default is 14 days. If, after `sincedb_clean_after` days, no activity has been detected on a file (inode) the record expires and is not written to disk. The persisted record now includes the "last activity seen" timestamp. Applicable: any mode.
+* Moves Filewatch code into the plugin folder, rework Filewatch code to use Logstash facilities like logging and environment.
+* Adds much better support for file rotation schemes of copy/truncate and rename cascading. Applies to tail mode only.
+* Adds support for processing files over remote mounts e.g. NFS. Before, it was possible to read into memory allocated but not filled with data resulting in ASCII NUL (0) bytes in the message field. Now, files are read up to the size as given by the remote filesystem client. Applies to tail and read modes.
+* Fixes `read` mode of regular files sincedb write is requested in each read loop iteration rather than waiting for the end-of-file to be reached. Note: for gz files, the sincedb entry can only be updated at the end of the file as it is not possible to seek into a compressed file and begin reading from that position. https://github.com/logstash-plugins/logstash-input-file/pull/196[#196]
+* Adds support for String Durations in some settings e.g. `stat_interval => "750 ms"`. https://github.com/logstash-plugins/logstash-input-file/pull/194[#194]
+* Fixes `require winhelper` error in WINDOWS. https://github.com/logstash-plugins/logstash-input-file/issues/184[#184]
+* Fixes issue, where when no delimiter is found in a chunk, the chunk is reread - no forward progress is made in the file. https://github.com/logstash-plugins/logstash-input-file/issues/185[#185]
+* Fixes JAR_VERSION read problem, prevented Logstash from starting. https://github.com/logstash-plugins/logstash-input-file/issues/180[#180]
+* Fixes sincedb write error when using /dev/null, repeatedly causes a plugin restart. https://github.com/logstash-plugins/logstash-input-file/issues/182[#182]
+* Fixes a regression where files discovered after first discovery were not always read from the beginning. Applies to tail mode only. https://github.com/logstash-plugins/logstash-input-file/issues/198[#198]
+
+
+*Http Input*
+
+* Replaces Puma web server with Netty. https://github.com/logstash-plugins/logstash-input-http/pull/73[#73]
+* Adds `request_headers_target_field` and `remote_host_target_field` configuration options with default to host and headers respectively. https://github.com/logstash-plugins/logstash-input-http/pull/68[#68]
+* Sanitizes content-type header with getMimeType. https://github.com/logstash-plugins/logstash-input-http/pull/87[#87]
+* Moves most message handling code to Java. https://github.com/logstash-plugins/logstash-input-http/pull/85[#85]
+* Fixes issue to respond with correct http protocol version. https://github.com/logstash-plugins/logstash-input-http/pull/84[#84]
+* Adds support for crt/key certificates.
+* Deprecates jks support.
+
+*Jdbc Input*
+
+* Fixes crash that occurs when receiving string input that cannot be coerced to UTF-8 (such as BLOB data). https://github.com/logstash-plugins/logstash-input-jdbc/pull/291[#291]
+
+*S3 Input*
+
+* Adds ability to optionally include S3 object properties inside `@metadata`. https://github.com/logstash-plugins/logstash-input-s3/pull/155[#155]
+
+*Kafka Output*
+
+* Fixes handling of two settings that weren't wired to the kafka client. https://github.com/logstash-plugins/logstash-output-kafka/pull/198[#198]
+
+
+[[logstash-6-3-2]]
+=== Logstash 6.3.2 Release Notes
+
+* Fixes a dependency issue with the Guava library ({lsissue}9836[Issue 9836]).
+* Fixes issue when launching logstash from a path that contains white spaces ({lsissue}9832[Issue 9832]).
+* Fixes issue with non-unicode event keys in serialization ({lsissue}9821[Issue 9821]).
+* Fixes jruby-openssl conflict after running bin/logstash-plugin update ({lsissue}9817[Issue 9817]).
+* Fixes development environment jruby artifact downloading ({lsissue}9807[Issue 9807]).
+
+
+
+[float]
+==== Plugins
+
+*Dissect Filter*
+
+* Fix Trailing Delimiters requires a false field https://github.com/logstash-plugins/logstash-filter-dissect/pull/57[#57].
+
+*Graphite Output*
+
+* Fixes exception handling during socket writing to prevent logstash termination https://github.com/logstash-plugins/logstash-output-graphite/pull/33[#33].
+
+*Http Output*
+
+* Fixes high CPU usage on retries in json_batch mode https://github.com/logstash-plugins/logstash-output-http/pull/89[#89].
+* Adds compression in json_batch mode https://github.com/logstash-plugins/logstash-output-http/pull/89[#89].
+
+[[logstash-6-3-1]]
+=== Logstash 6.3.1 Release Notes
+
+* Adds a Persistent Queue repair utility, enabling self-recovery of corrupted PQs ({lsissue}9710[Issue 9710]).
+* Fixes two separate issues in Experimental Java Execution mode where complex pipeline configurations could fail to compile ({lsissue}9747[Issue 9747], {lsissue}9745[Issue 9745]).
+* Fixes issue when running Logstash inside of a Docker container with Persistent Queue enabled where we incorrectly reported that there was insufficient space to allocate for the queue ({lsissue}9766[Issue 9766]).
+* Fixes issue in x-pack monitoring where `queue_push_duration_in_millis` was incorrectly reporting _nanoseconds_ ({lsissue}9744[Issue 9744]).
+* Fixes an issue where Logstash could fail to start when its `path.data` is a symlink ({lsissue}9706[Issue 9706]).
+* Fixes issue with Netflow module where it could fail to populate `[geoip_dst][autonomous_system]` ({lsissue}9638[Issue 9638]).
+* Fixes a potential conflict with plugin depenencies that require Guava by explicitly loading Guava 22 ({lsissue}9592[Issue 9592]).
+
+[float]
+==== Plugins
+
+*Netflow Codec*
+
+* Fixes exception when receiving Netflow 9 from H3C devices.
+* Added support for Netflow 9 from H3C devices.
+* Fixes incorrect definitions of IE 231 and IE 232.
+* Fixes exceptions due to concurrent access of IPFIX templates.
+* Added support for Netflow 9 reduced-size encoding support.
+* Added support for Barracuda IPFIX Extended Uniflow.
+
+*Beats Input*
+
+* Fixes an issue that prevented auto-recovery in certain failure modes.
+* Fixes an issue where trace-level logging omitted helpful context.
+
+*Kafka Input*
+
+* Fix race-condition where shutting down a Kafka Input before it completes startup could cause Logstash to crash.
+* Upgrade Kafka client to version 1.1.0.
+
+*S3 Input*
+
+* Avoid plugin crashes when encountering 'bad' files in S3 buckets.
+* Log entry when bucket is empty.
+* Fixes `additional_settings` configuration option to properly symbolize keys for downstream library.
+
+*TCP Input*
+
+* New configuration option to set TCP keep-alive.
+* Fixes an issue where the input could crash during shutdown, affecting pipeline reloads.
+
+*UDP Input*
+
+* Mitigate memory leak in JRuby's UDP implementation.
+
+*DNS Filter*
+
+* Log timeouts as warn instead of error.
+* Allow concurrent queries when cache enabled.
+
+*Elasticsearch Filter*
+
+* Fix: The filter now only calls filter_matched on events that actually matched. This fixes issues where all events would have success-related actions happened when no match had actually happened (`add_tag`, `add_field`, `remove_tag`, `remove_field`).
+
+*JDBC Static Filter*
+
+* Fixed an issue where failing to specify `index_columns` would result in an obscure error message.
+
+*KV Filter*
+
+* improves `trim_key` and `trim_value` to trim any _sequence_ of matching characters from the beginning and ends of the corresponding keys and values; a previous implementation limited trim to a single character from each end, which was surprising.
+* fixes issue where we can fail to correctly break up a sequence that includes a partially-quoted value followed by another fully-quoted value by slightly reducing greediness of quoted-value captures.
+
+*Mutate Filter*
+
+* Fix: when converting to `float` and `float_eu`, explicitly support same range of inputs as their integer counterparts.
+
+*Elasticsearch Output*
+
+* Added support for customizing HTTP headers.
+* Log an error -- not a warning -- when ES raises an invalid_index_name_exception.
+* Improve plugin behavior when Elasticsearch is down on startup.
+
+*File Output*
+
+* Fix a bug where flush interval was being called for each event when enabled
+
+*Kafka Output*
+
+* Changed Kafka send errors to log as warn.
+* Upgrade Kafka client to version 1.1.0.
+
+*S3 Output*
+
+* Fixes `additional_settings` configuration option to properly symbolize keys for downstream library.
+
+*SQS Output*
+
+* Added the ability to send to a different account id's queue.
+
+[[logstash-6-3-0]]
+=== Logstash 6.3.0 Release Notes
+
+[IMPORTANT]
+--
+Persistent Queue users must upgrade. Old data will not be compatible with 6.3.0, and must be migrated or deleted. Read
+{logstash-ref}/upgrading-logstash-pqs.html[Upgrading Persistent Queue from Logstash 6.2.x and Earlier]
+for more information.
+--
+
+* BUGFIX: Fix race condition in shutdown of pipelines https://github.com/elastic/logstash/pull/9285[#9285]
+* BUGFIX: Ensure atomic creation of persistent queue checkpoints https://github.com/elastic/logstash/pull/9303[#9303]
+* BUGFIX: Fixed issue where events containing non-ASCII characters were getting encoded incorrectly after passing through the persistent queue https://github.com/elastic/logstash/pull/9307[#9307]
+* BUGFIX: Fixes incorrect serialization of strings extracted from other strings via substring, regex matching, etc. https://github.com/elastic/logstash/pull/9308[#9308]
+* BUGFIX: Fixes nested metadata field lookup in Java execution https://github.com/elastic/logstash/pull/9297[#9297]
+* BUGFIX: Persistent queue must allow reading empty batches https://github.com/elastic/logstash/pull/9328[#9328]
+* BUGFIX: Prevents pipelines.yml from being overwritten during RPM/DEB package upgrade https://github.com/elastic/logstash/pull/9130[#9130]
+* BUGFIX: Different types of values for the `ssl.enabled` module option are now tolerated https://github.com/elastic/logstash/pull/8600[#8600]
+* BUGFIX: Detect invalid proxy and raise error https://github.com/elastic/logstash/pull/9230[#9230]
+* BUGFIX: Fix `Logstash::Util.deep_clone` for `LogStash::Timestamp` https://github.com/elastic/logstash/pull/9405[#9405]
+* BUGFIX: Better error message for temp directory errors https://github.com/elastic/logstash/pull/9293[#9293]
+* BUGFIX: Better error message when `Event#set` is called on non-collection nested field https://github.com/elastic/logstash/pull/9298[#9298]
+* Implemented upgrade to persistent queues v2 https://github.com/elastic/logstash/pull/9538[#9538]
+* Inter-pipeline communication (within multiple pipelines on a single Logstash node) https://github.com/elastic/logstash/pull/9225[#9225]
+* Speed up pipeline compilation https://github.com/elastic/logstash/pull/9278[#9278]
+* Added bootstrap checks for available disk space when persistent queue is enabled https://github.com/elastic/logstash/pull/8978[#8978]
+* Made `-V`/`--version` fast on Windows https://github.com/elastic/logstash/pull/8508[#8508]
+* Start web server after pipeline https://github.com/elastic/logstash/pull/9398[#9398]
+* Optimize out empty `if` conditions from execution graph https://github.com/elastic/logstash/pull/9314[#9314]
+
+==== Plugins
+*Netflow Codec*
+
+* Added support for IPFIX from Procera/NetIntact/Sandvine 15.1 https://github.com/logstash-plugins/logstash-codec-netflow/pull/131[#131]
+
+*JDBC_static Filter*
+
+* Support multiple driver libraries https://github.com/logstash-plugins/logstash-filter-jdbc_static/issues/22[#22]
+* Use Java classloader to load driver jar. Use system import from file to loader local database. Prevent locking errors when no records returned. https://github.com/logstash-plugins/logstash-filter-jdbc_static/issues/18[#18], https://github.com/logstash-plugins/logstash-filter-jdbc_static/issues/17[#17], https://github.com/logstash-plugins/logstash-filter-jdbc_static/issues/12[#12]
+* `loader_schedule` now works as designed https://github.com/logstash-plugins/logstash-filter-jdbc_static/issues/8[#8]
+
+*UDP Input*
+
+* Fix missing require for the ipaddr library https://github.com/logstash-plugins/logstash-input-udp/pull/37[#37]
+
+[[logstash-6-2-4]]
+=== Logstash 6.2.4 Release Notes
+
+* Fixed an issue where events passing through the persistent queue had the contents of string type fields corrupted for values containing non-ASCII characters. The solution involved serializing the RubyString as Java String UTF-16 encoded https://github.com/elastic/logstash/pull/9167[#9167]
+* Fixed serialization bug when a RubyString that comes out of a matching, substring or similar operation may not have offset 0 pointing at the underlying BytesList. Solved by serializing the correct part of the BytesList https://github.com/elastic/logstash/pull/9308[#9308]
+* Improved performance of Event#cancel, where each operation would unnecessarily generate a new object. In configurations that use plugins like the `drop filter` throughput may increase up to 5x https://github.com/elastic/logstash/pull/9284[#9284]
+* Fixed an issue with type handling in metadata fields. Before this a plugin could trigger a Java exception when placing data into an event's metadata https://github.com/elastic/logstash/pull/9299[#9299]
+* Fixed a race condition in shutdown of pipelines where not all workers would consume a single SHUTDOWN signal https://github.com/elastic/logstash/pull/9285[#9285]
+* Multiple documentation improvements relating to configuration files, JVM options, default plugin codecs, Logstash-to-Logstash communication and Keystore.
+
+==== Plugins
+
+*Json_lines Codec*
+
+* Support flush method, see https://github.com/logstash-plugins/logstash-codec-json_lines/pull/35
+
+*Netflow Codec*
+
+* Workaround for breaking change in Netflow-Input-UDP > 3.2.0, see issue https://github.com/logstash-plugins/logstash-codec-netflow/issues/122[#122]
+* Renamed some unknown VMware VDS fields
+
+*Aggregate Filter*
+
+* new feature: add 'timeout_timestamp_field' option.
+  When set, this option computes timeout based on event timestamp field (and not system time). It's particularly useful when processing old logs.
+* new feature: add 'inactivity_timeout' option.
+  Events for a given `task_id` will be aggregated for as long as they keep arriving within the defined `inactivity_timeout` option - the inactivity timeout is reset each time a new event happens. On the contrary, `timeout` is never reset and happens after `timeout` seconds since aggregation map creation.
+
+*Dns Filter*
+
+* Logging improvement to include DNS resolution failure reason https://github.com/logstash-plugins/logstash-filter-dns/issues/36[#36]
+* Fix bug where forward lookups would not cache timeout errors
+
+*Jdbc_streaming Filter*
+
+* Load the driver with the system class loader. Fixes issue loading some JDBC drivers in Logstash 6.2+ https://github.com/logstash-plugins/logstash-input-jdbc/issues/263[#263]
+
+*Kv Filter*
+
+* Correctly handle empty values between value separator and field separator https://github.com/logstash-plugins/logstash-filter-kv/issues/58[#58]
+
+*Ruby Filter*
+
+* Fix return of multiple events when using file based scripts https://github.com/logstash-plugins/logstash-filter-ruby/issues/41[#41]
+
+*Translate Filter*
+
+* Add 'refresh_behaviour' to either 'merge' or 'replace' during a refresh https://github.com/logstash-plugins/logstash-filter-translate/issues/57[#57]
+
+*Beats Input*
+
+* Ensure that the keep-alive is sent for ALL pending batches when the pipeline is blocked, not only the batches attempting to write to the queue. https://github.com/logstash-plugins/logstash-input-beats/issues/310[#310]
+
+*Exec Input*
+
+* Add metadata data to the event wrt execution duration and exit status
+* Add 'schedule' option to schedule the command to run, using a cron expression
+
+*Http Input*
+
+* Make sure default codec is also cloned for thread safety. https://github.com/logstash-plugins/logstash-input-http/pull/80[#80]
+* Always flush codec after each request and codec decoding. https://github.com/logstash-plugins/logstash-input-http/pull/81[#81]
+
+*Jdbc Input*
+
+* Clarify use of use_column_value. Make last_run_metadata_path reference in record_last_run entry clickable. https://github.com/logstash-plugins/logstash-input-jdbc/issues/273[#273]
+* Load the driver with the system class loader. Fixes issue loading some JDBC drivers in Logstash 6.2+ https://github.com/logstash-plugins/logstash-input-jdbc/issues/263[#263]
+* Fix regression with 4.3.5 that can result in NULL :sql_last_value depending on timestamp format https://github.com/logstash-plugins/logstash-input-jdbc/issues/274[#274]
+
+*Redis Input*
+
+* Add support for SSL https://github.com/logstash-plugins/logstash-input-redis/issues/61[#61]
+* Add support for Redis unix sockets https://github.com/logstash-plugins/logstash-input-redis/issues/64[#64]
+
+*S3 Input*
+
+* Improve error handling when listing/downloading from S3 https://github.com/logstash-plugins/logstash-input-s3/issues/144[#144]
+* Add documentation for endpoint, role_arn and role_session_name https://github.com/logstash-plugins/logstash-input-s3/issues/142[#142]
+* Add support for additional_settings option https://github.com/logstash-plugins/logstash-input-s3/issues/141[#141]
+
+*Sqs Input*
+
+* Add documentation for endpoint, role_arn and role_session_name https://github.com/logstash-plugins/logstash-input-sqs/issues/46[#46]
+* Fix sample IAM policy to match to match the documentation https://github.com/logstash-plugins/logstash-input-sqs/issues/32[#32]
+
+*Tcp Input*
+
+* Restore SSLSUBJECT field when ssl_verify is enabled. https://github.com/logstash-plugins/logstash-input-tcp/issues/115[#115]
+* Update Netty/tc-native versions to match those in beats input https://github.com/logstash-plugins/logstash-input-tcp/issues/113[#113]
+
+*Udp Input*
+
+* Add metrics support for events, operations, connections and errors produced during execution. https://github.com/logstash-plugins/logstash-input-udp/issues/34[#34]
+* Fix support for IPv6 https://github.com/logstash-plugins/logstash-input-udp/issues/31[#31]
+
+*Aws Mixin*
+
+* Drop strict value validation for region option https://github.com/logstash-plugins/logstash-mixin-aws/issues/36[#36]
+* Add endpoint option to customize the endpoint uri https://github.com/logstash-plugins/logstash-mixin-aws/issues/32[#32]
+* Allow user to provide a role to assume https://github.com/logstash-plugins/logstash-mixin-aws/issues/27[#27]
+* Update aws-sdk dependency to '~> 2'
+
+*Elasticsearch Output*
+
+* Set number_of_shards to 1 and document_type to '_doc' for es 7.x clusters #741 https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/747[#747]
+* Fix usage of upsert and script when update action is interpolated https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/239[#239]
+* Add metrics to track bulk level and document level responses https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/585[#585]
+
+*File Output*
+
+* Add feature `write_behavior` to the documentation https://github.com/logstash-plugins/logstash-output-file/issues/58[#58]
+
+*S3 Output*
+
+* Add documentation for endpoint, role_arn and role_session_name https://github.com/logstash-plugins/logstash-output-s3/issues/174[#174]
+* Add option for additional settings https://github.com/logstash-plugins/logstash-output-s3/issues/173[#173]
+* Add more S3 bucket ACLs https://github.com/logstash-plugins/logstash-output-s3/issues/158[#158]
+* Handle file not found exception on S3 upload https://github.com/logstash-plugins/logstash-output-s3/issues/144[#144]
+* Document prefix interpolation https://github.com/logstash-plugins/logstash-output-s3/issues/154[#154]
+
+*Sqs Output*
+
+* Add documentation for endpoint, role_arn and role_session_name https://github.com/logstash-plugins/logstash-output-sqs/issues/29[#29]
+
+[[logstash-6-2-3]]
+=== Logstash 6.2.3 Release Notes
+
+* There are no changes to Logstash core in this release
+
+==== Plugins
+
+*Fluent codec*
+
+* Added ability to encode tags as fluent forward protocol tags
+
+*Netflow codec*
+
+* Improved decoding performance of ASA ACL ids, MAC addresses and IPv4 addresses
+
+*KV Filter*
+
+* Added option to split fields and values using a regex pattern (#55)
+
+*Mutate Filter*
+
+* Introduced support for number strings using a decimal comma (e.g. 1,23), added convert support to specify integer_eu and float_eu
+
+*Beats Input*
+
+* Improved memory management and back pressure handling
+
+*JDBC Input*
+
+* Added fixes for thread and memory leak
+
+*Syslog Input*
+
+* Changed syslog field to be a configurable option. This is useful for when codecs change the field containing the syslog data.
+
+*Elasticsearch Output*
+
+* Changed sniffing behavior to connect only to `http.enabled` nodes that serve data for Elasticsearch 5.x and 6.x. Master-only nodes are ignored.
+  (For Elasticsearch 1.x and 2.x, any nodes with `http.enabled` are added to the hosts lists, including master-only nodes.)
+
+[[logstash-6-2-2]]
+=== Logstash 6.2.2 Release Notes
+
+* Fix issue introduced in 6.2.1 where `bin/logstash-plugin` could not install or upgrade plugins
+
+[[logstash-6-2-1]]
+=== Logstash 6.2.1 Release Notes
+
+* There are no user facing changes in this release
+
+
+[[logstash-6-2-0]]
+=== Logstash 6.2.0 Release Notes
+
+* Added support to protect sensitive settings and configuration in a {logstash-ref}/keystore.html[keystore].
+* Added the {logstash-ref}/plugins-filters-jdbc_static.html[jdbc_static filter] as a default plugin.
+* Set better defaults to allow for higher throughput under load. (https://github.com/elastic/logstash/issues/8707[#8707] and https://github.com/elastic/logstash/issues/8702[#8702])
+* Set the default configuration for RPM/DEB/Docker installations to use {logstash-ref}/multiple-pipelines.html[Multiple pipelines].
+* Added a default max size value (100MB) for log files.
+* Added compression when log files are rolled (for ZIP-based installs).
+* Added the ability to specify `--pipeline.id` from the command line. (https://github.com/elastic/logstash/issues/8868[#8868])
+* Implemented continued improvements to the next generation of execution. Give it a try with the command line switch `--experimental-java-execution`.
+
+==== Plugins
+
+*Jdbc_static Filter*
+
+* Released the initial version the {logstash-ref}/plugins-filters-jdbc_static.html[jdbc_static filter], which enriches events with data pre-loaded from a remote database.
+
+*Dissect Filter*
+
+* Fixed multiple bugs. See the plugin release notes for https://github.com/logstash-plugins/logstash-filter-dissect/blob/master/CHANGELOG.md#113[1.1.3].
+
+*Grok Filter*
+
+* Fixed a thread leak that occurred when Logstash was reloaded.
+
+*Kafka Output*
+
+* Improved error logging for when a producer cannot be created.
+
+[[logstash-6-1-3]]
+=== Logstash 6.1.3 Release Notes
+
+* Fix bug where with terminating input plugins in-memory queue might not be drained. This could happen in some situations with inputs like the stdin input or the Elasticsearch input. This could result in some messages not being processed.
+* Correctly handle paths with spaces on Windows. See https://github.com/elastic/logstash/pull/8931[#8931] for details.
+
+==== Plugins
+
+*Multiline Codec*
+
+* Fixed concurrency issue causing random failures when multiline codec was used together with a multi-threaded input plugin
+
+*CSV Filter*
+
+* Added support for tagging empty rows which users can reference to conditionally drop events
+
+*Elasticsearch Filter*
+
+* If elasticsearch response contains a shard failure, then tag_on_failure tags are added to Logstash event
+* Enhancement : add support for nested fields
+* Enhancement : add 'docinfo_fields' option
+* Enhancement : add 'aggregation_fields' option
+
+*Elasticsearch Input*
+
+* Add support for scheduling periodic execution of the query
+
+*RabbitMQ Input/Output*
+
+* Bug Fix: undefined method `value' for nil:NilClass with SSL enabled, but no certificates provided
+* Output Only: Use shared concurrency / multiple channels for performance
+
+*HTTP Output*
+
+* Added json_batch format
+* Make 429 responses log at debug, not error level. They are really just flow control
+
+
+[[logstash-6-1-2]]
+=== Logstash 6.1.2 Release Notes
+* Fixed a bug that caused empty objects when cloning Logstash Timestamp instances
+* Changed the way pipeline configurations are hashed to ensure consistence (not user facing)
+
+[float]
+==== Input Plugins
+
+*`Beats`*:
+
+* Re-order Netty pipeline to avoid NullPointerExceptions in KeepAliveHandler when Logstash is under load
+* Improve exception logging
+* Upgrade to Netty 4.1.18 with tcnative 2.0.7
+* Better handle case when remoteAddress is nil to reduce amount of warning messages in logs
+
+*`Jdbc`*:
+
+* Fix thread and memory leak. See (https://github.com/logstash-plugins/logstash-input-jdbc/issues/255[#255])
+
+*`Kafka`*:
+
+* Upgrade Kafka client to version 1.0.0
+
+*`S3`*:
+
+* Add support for auto-detecting gzip files with .gzip extension, in addition to existing support for *.gz
+* Improve performance of gzip decoding by 10x by using Java's Zlib
+* Change default sincedb path to live in `{path.data}/plugins/inputs/s3` instead of $HOME. Prior Logstash installations (using $HOME default) are automatically migrated.
+* Don't download the file if the length is 0
+
+*`Tcp`*:
+
+* Fix bug where codec was not flushed when client disconnected
+* Restore INFO logging statement on startup
+* Fixed typo in @metadata tag
+
+[float]
+==== Filter Plugins
+
+*`Geoip`*:
+
+* Skip lookup operation if source field contains an empty string
+* Update of the GeoIP2 DB
+
+*`Grok`*:
+
+* Fix potential race condition. see (https://github.com/logstash-plugins/logstash-filter-grok/pull/131[#131])
+
+[float]
+==== Output Plugins
+
+*`Kafka`*:
+
+* bump kafka dependency to 1.0.0
+
+[float]
+==== Codecs
+
+*`Line`*:
+
+* Reverted thread safety fix and instead fixed udp input codec per worker. See (https://github.com/logstash-plugins/logstash-codec-line/pull/14[#14])
+
+*`Netflow`*:
+
+* Added support for Nokia BRAS
+* Added Netflow v9 IE150 IE151, IE154, IE155
+
+*`Plain`*:
+
+* Code cleanup. See (https://github.com/logstash-plugins/logstash-codec-plain/pull/6[#6])
+
+[[logstash-6-1-1]]
+=== Logstash 6.1.1 Release Notes
+*  There are no user-facing changes in Logstash core in this release.
+
+[float]
+==== Input Plugins
+
+*`Beats`*:
+
+* Fixed issue with close_wait connections to make sure that keep alive is sent back to the client. (https://github.com/logstash-plugins/logstash-input-beats/pull/272[#272])
+
+*`HTTP`*:
+
+* If all webserver threads are busy, the plugin now returns status code 429. (https://github.com/logstash-plugins/logstash-input-http/pull/75[#75])
+
+*`JDBC`*:
+
+* Fixed connection and memory leak. (https://github.com/logstash-plugins/logstash-input-jdbc/issues/251[#251])
+
+*`Syslog`*:
+
+* Fixed issue where stopping a pipeline with active inbound syslog connections (for example, while reloading the configuration) could cause Logstash to crash. (https://github.com/logstash-plugins/logstash-input-syslog/issues/40[#40])
+
+[float]
+==== Filter Plugins
+
+*`Split`*:
+
+* Fixed crash on arrays with null values. (https://github.com/logstash-plugins/logstash-filter-split#31[#31])
+
+[float]
+==== Codecs
+
+*`Line`*:
+
+* Fixed thread safety issue. (https://github.com/logstash-plugins/logstash-codec-line/pull/13[#13])
+
+*`Netflow`*:
+
+* Added vIPtela support.
+* Added fields for Cisco ASR1k.
+
+
+[[logstash-6-1-0]]
+=== Logstash 6.1.0 Release Notes
+* Implemented a new experimental Java execution engine for Logstash pipelines. The Java engine is off by default, but can be enabled with --experimental-java-execution ({lsissue}/7950[Issue 7950]).
+* Added support for changing the <<configuring-persistent-queues,page capacity>> for an existing queue ({lsissue}/8628[Issue 8628]).
+* Made extensive improvements to pipeline execution performance and memory efficiency ({lsissue}/7692[Issue 7692], {lsissue}/8776[8776], {lsissue}/8577[8577], {lsissue}/8446[8446], {lsissue}/8333[8333], {lsissue}/8163[8163], {lsissue}/8103[8103], {lsissue}/8087[8087], and {lsissue}/7691[7691]).
+
+[float]
+==== Filter Plugins
+
+*`Grok`*:
+
+* Fixed slow metric invocation and needless locking on timeout enforcer (https://github.com/logstash-plugins/logstash-filter-grok/pull/125[#125]).
+
+*`Mutate`*:
+
+* Added support for boolean-to-integer conversion (https://github.com/logstash-plugins/logstash-filter-mutate/pull/108[#108]).
+
+*`Ruby`*:
+
+* Fixed concurrency issues with multiple worker threads that was caused by a (https://github.com/jruby/jruby/issues/4868[JRuby issue]).
+* Added file-based Ruby script support as an alternative to the existing inline option (https://github.com/logstash-plugins/logstash-filter-ruby/pull/35[#35]).
+
+[float]
+==== Output Plugins
+
+*`Elasticsearch`*:
+
+* When indexing to Elasticsearch 6.x or above, Logstash ignores the event's `type` field and no longer uses it to set the document's `_type` (https://github.com/logstash-plugins/logstash-filter-elasticsearch/pull/712[#712]).
+>>>>>>> Stashed changes
diff --git a/docs/static/troubleshooting.asciidoc b/docs/static/troubleshooting.asciidoc
new file mode 100644
index 00000000000..b4c7c1a124a
--- /dev/null
+++ b/docs/static/troubleshooting.asciidoc
@@ -0,0 +1,156 @@
+[[troubleshooting]] 
+= Troubleshooting
+
+If you have issues installing or running Logstash, read the
+following tips.
+
+[float] 
+[[ts-temp-dir]] 
+== Installation and setup
+
+
+[float] 
+[[ts-temp-dir]] 
+=== Inaccessible temp directory
+
+Certain versions of the JRuby runtime and libraries
+in certain plugins (e.g., the Netty network library in the TCP input) copy
+executable files to the temp directory which causes subsequent failures when
+/tmp is mounted noexec. 
+
+Possible solutions:
+. Change setting to mount /tmp with exec.
+. Specify an alternate directory using the `-Djava.io.tmpdir` setting in the jvm.options file.
+ 
+
+
+
+
+
+[float] 
+[[ts-kafka]] 
+== Kafka
+
+This section contains a list of the most common Kafka related support issues of
+the last few months.  
+
+[float] 
+[[ts-kafka-timeout]] 
+== Kafka Session Timeout Issues (Input Side)
+
+This is a very common problem. 
+
+Symptoms: Throughput issues and duplicate event
+processing LS logs warnings:
+[2017-10-18T03:37:59,302][WARN][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
+Auto offset commit failed for group clap_tx1: Commit cannot be completed since
+the group has already rebalanced and assigned the partitions to another member.
+This means that the time between subsequent calls to poll() was longer than the
+configured session.timeout.ms, which typically implies that the poll loop is
+spending too much time message processing. You can address this either by
+increasing the session timeout or by reducing the maximum size of batches
+returned in poll() with max.poll.records. [INFO
+][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] Revoking
+previously assigned partitions [] for group log-ronline-node09
+[2018-01-29T14:54:06,485][INFO][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
+Setting newly assigned partitions [elk-pmbr-9] for group log-pmbr 
+
+Example: https://github.com/elastic/support-dev-help/issues/3319
+
+Background:
+
+Kafka tracks the individual consumers in a consumer group (i.e. a number of LS
+instances) and tries to give each consumer one or more specific partitions of
+the data in the topic they’re consuming.  In order to achieve this, Kafka has to
+also track whether or not a consumer (LS Kafka input thread) is making any
+progress on their assigned partition and reassign partitions that have not seen
+progress in a set timeframe. This causes a problem when Logstash is requesting
+more events from the Kafka Broker than it can process within the timeout because
+it triggers reassignment of partitions. Reassignment of partitions can cause
+duplicate processing of events and significant throughput problems because of
+the time the reassignment takes. Solution:
+
+Solution:
+Fixing the problem is easy by reducing the number of records per request that LS
+polls from the Kafka Broker in on request, reducing the number of Kafka input
+threads and/or increasing the relevant timeouts in the Kafka Consumer
+configuration.
+
+The number of records to pull in one request is set by the option
+`max_poll_records`.  If it exceeds the default value of 500, reducing this
+should be the first thing to try. The number of input threads is given by the
+option `consumer_threads`.  If it exceeds the number of pipeline workers
+configured in the `logstash.yml` it should certainly be reduced.  If it is a
+large value (> 4), it likely makes sense to reduce it to 4 (if the client has
+the time/resources for it, it would be ideal to start with a value of 1 and then
+increment from there to find the optimal performance). The relevant timeout is
+set via `session_timeout_ms`. It should be set to a value that ensures that the
+number of events in `max_poll_records` can be safely processed within. Example:
+pipeline throughput is 10k/s and `max_poll_records` is set to 1k => the value
+must be at least 100ms if `consumer_threads` is set to `1`. If it is set to a
+higher value n, then the minimum session timeout increases proportionally to `n *
+100ms`. In practice the value must be set much larger than the theoretical value
+because the behaviour of the outputs and filters in a pipeline follows a
+distribution. It should also be larger than the maximum time you expect your
+outputs to stall for. The default setting is 10s == `10000ms`. If a user is
+experiencing periodic problems with an output like Elasticsearch output that
+could stall because of load or similar effects, there is little downside to
+increasing this value significantly to say 60s. Note: Decreasing the
+`max_poll_records` is preferable to increasing this timeout from the performance
+perspective. Increasing this timeout is your only option if the client’s issues
+are caused by periodically stalling outputs. Check logs for evidence of stalling
+outputs (e.g. ES output logging status `429`).
+
+[float] 
+[[ts-kafka-many-offset-commits]] 
+== Large Number of Offset Commits (Input Side)
+
+Symptoms: Logstash’s Kafka Input is causing a much higher number of commits to
+the offset topic than expected. Often the complaint also mentions redundant
+offset commits where the same offset is committed repeatedly.
+
+Examples: https://github.com/elastic/support-dev-help/issues/3702
+https://github.com/elastic/support-dev-help/issues/3060 Solution:
+
+For Kafka Broker versions 0.10.2.1 to 1.0.x: The problem is caused by a bug in
+Kafka. https://issues.apache.org/jira/browse/KAFKA-6362 The client’s best option
+is upgrading their Kafka Brokers to version 1.1 or newer. For older versions of
+Kafka or if the above does not fully resolve the issue: The problem can also be
+caused by setting too low of a value for `poll_timeout_ms` relative to the rate
+at which the Kafka Brokers receive events themselves (or if Brokers periodically
+idle between receiving bursts of events). Increasing the value set for
+`poll_timeout_ms` will proportionally decrease the number of offsets commits in
+this scenario (i.e. raising it by 10x will lead to 10x fewer offset commits).
+
+
+[float] 
+[[ts-kafka-codec-errors-input]] 
+== Codec Errors in Kafka Input (before Plugin Version 6.3.4 only) 
+
+Symptoms:
+Logstash Kafka input randomly logs errors from the configured codec and/or reads
+events incorrectly (partial reads, mixing data between multiple events etc.).
+
+Log example:  [2018-02-05T13:51:25,773][FATAL][logstash.runner          ] An
+unexpected error occurred! {:error=>#<TypeError: can't convert nil into String>,
+:backtrace=>["org/jruby/RubyArray.java:1892:in `join'",
+"org/jruby/RubyArray.java:1898:in `join'",
+"/usr/share/logstash/logstash-core/lib/logstash/util/buftok.rb:87:in `extract'",
+"/usr/share/logstash/vendor/bundle/jruby/1.9/gems/logstash-codec-line-3.0.8/lib/logstash/codecs/line.rb:38:in
+`decode'",
+"/usr/share/logstash/vendor/bundle/jruby/1.9/gems/logstash-input-kafka-5.1.11/lib/logstash/inputs/kafka.rb:241:in
+`thread_runner'",
+"file:/usr/share/logstash/vendor/jruby/lib/jruby.jar!/jruby/java/java_ext/java.lang.rb:12:in
+`each'",
+"/usr/share/logstash/vendor/bundle/jruby/1.9/gems/logstash-input-kafka-5.1.11/lib/logstash/inputs/kafka.rb:240:in
+`thread_runner'"]} 
+
+Examples: https://github.com/elastic/support-dev-help/issues/3308
+https://github.com/elastic/support-dev-help/issues/2107 Background:
+
+There was a bug in the way the Kafka Input plugin was handling codec instances
+when running on multiple threads (`consumer_threads` set to > 1).
+https://github.com/logstash-plugins/logstash-input-kafka/issues/210 Solution:
+
+Ideally: Upgrade Kafka Input plugin to v. 6.3.4 or later. If (and only if)
+upgrading is impossible: Set `consumer_threads` to `1`.
